mimic locating and understanding bugs by analyzing mimicked executions daniele zuddas wei jin fabrizio pastore leonardo mariani alessandro orso university of milano bicocca georgia institute of technology milano ital y atlanta ga usa zuddas pastore mariani disco.unimib.it weijin orso gatech.edu abstract automated debugging techniques aim to help developers locate and understand the cause of a failure an extremely challenging yet fundamental task.
most state of the art approaches suffer from two problems they require a large number of passing and failing tests and report possible faulty code with no explanation.
to mitigate these issues we present mimic a novel automated debugging technique that combines and extends our previous input generation and anomaly detection techniques.
mimic synthesizes multiple passing and failing executions similar to an observed failure and uses these executions to detect anomalies in behavior that may explain the failure.
we evaluated mimic on six failures of realworld programs with promising results for five of these failures mimic identified their root causes while producing a limited number of false positives.
most importantly the anomalies identified by mimic provided information that may help developers understand and ultimately eliminate such root causes.
categories and subject descriptors d. .
testing and debugging general terms reliability experimentation keywords debugging execution synthesis anomaly detection .
introduction because software debugging is an extremely expensive and human intensive activity researchers and practitioners alike have put a great deal of effort into developing automated debugging techniques and tools that can help developers reduce the cost of debugging.
one debugging task in particular fault localization has been intensively investigated during the last decade.
among the fault localization techniques developed to date the ones based on statistical analysis are particularly popular e.g.
.
although statistical fault localization techniques can be effective in guiding the developer towards parts of the code that are likely to be responsible for an observed failure most of these approaches have two serious limitations.
first in order to perform their statistical analysis they require a large number of suitable permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september vasteras sweden.
copyright acm ... .
.
and failing test cases .
unfortunately this ideal set of tests is rarely available in practice.
second these approaches normally return a list of statements ranked according to their likelihood of being faulty with no additional information.
as the study by parnin and orso has shown developers usually need context and some form of explanation to understand a bug.
to address and mitigate these issues we present mimic a novel automated debugging technique that combines and extends two techniques developed in previous work by the authors.
the first one is an input generation technique that can synthesize executions similar to an observed failure .
the second technique performs anomaly detection by identifying the violations of a previously built model of the normal behavior of an application .
given a programpand a failure fforp mimic performs the following steps.
first it generates a set of inputs that when run against p result in both passing and failing executions that are similar to f. second mimic leverages these synthesized executions and the structure of the program to automatically compute suitable points where to monitor program behavior.
third mimic uses the passing inputs to build a model of p s normal behavior and the failing inputs to identify violations of this model at the monitoring points computed in the previous step.
finally mimic reports the discovered violations suitably processed as potential explanations for f. to evaluate mimic we implemented it in a prototype tool and used the tool on six failures of real world programs.
the results of our evaluation provide initial evidence of the usefulness of our approach.
in particular the results show that mimic can report anomalies that are closely related to the root cause s of fand can potentially help understand such cause s .
in addition the results show that both using the synthesized executions generated by mimic instead of p s existing test cases and monitoring program behavior at the program points selected by mimic rather than at arbitrary program points can considerably improve the effectiveness of the approach.
the main contributions of this work are the definition of mimic a new highly automated technique for discovering failure causes by leveraging only failure data from a single failing execution.
an approach for identifying effective monitoring points within a program for anomaly detection.
an optimization heuristic for filtering out spurious anomalies that are unlikely to be related to failure causes.
the development of a publicly available prototype tool that implements our technique available at unimib.it tools mimic.
an empirical study that shows initial yet clear evidence of the usefulness of our approach.
failure data step execution synthesis synthesizedexecutionsstep monitoring points detection monitoredexecutions models step anomaly detectionsed.c sed.c 8958sed.c sed.c 9011sed.c sed.c 8786sed.c sed.c 8786sed.c sed.c 990sed.c sed.c 8958sed.c sed.c 9011sed.c sed.c 8786sed.c sed.c 8786sed.c sed.c 990sed.c sed.c 8958sed.c sed.c 9011sed.c sed.c 8786sed.c sed.c 8786sed.c sed.c 990sed.c sed.c 8958sed.c sed.c 9011sed.c sed.c 8786sed.c sed.c 8786sed.c sed.c 990anomalies application step filtering sed.c sed.c 8958sed.c sed.c 9011sed.c sed.c 8786sed.c sed.c 8786sed.c sed.c 990failurecausesmimicfigure overview of the mimic approach.
the rest of the paper is organized as follows.
section provides background information that is necessary to make the paper self contained.
section describes our approach in detail.
section presents our empirical evaluation.
finally sections and discuss related work and conclude the paper.
.
background before discussing mimic in detail we concisely describe the techniques that we leverage and extend in this work f3and radar.
.
f3 f3is a general approach for reproducing and debugging field failures that we developed in previous work .
the inputs to f3 are a program pand failure data dfor an execution eofpthat resulted in a field failure f. given this input f3 synthesizes multiple passing and failing executions similar to eand applies a customized statistical fault localization approach to identify and rank statements in pthat are likely to be responsible for f. the failure data used by f3 d is an ordered list of intermediate program locations that can be seen as breadcrumbs to be followed or subgoals to be reached to get to the failure.
the last goal in the list is the actual failure point.
given d f3uses our bugredux algorithm to perform an optimized guided forward symbolic execution that aims to synthesize multiple passing and failing executions of pthat reach the goals in the same order as e. to generate passing execution the algorithm may remove some intermediate goals from d which increases the degrees of freedom of the synthesis and thus the chances of generating such executions.
in mimic we leverage f3in two ways.
first we use the execution generator in f3to generate multiple passing and failing executions similar to the observed in house in this case failure f. as failure data mimic uses a partial call sequence for f which in previous work we found to offer a good tradeoff between amount of information collected and usefulness of the information in guiding execution synthesis and fault localization .
second mimic leverages the suspiciousness values generated by f3 s customized fault localization techniques to select suitable points where to monitor program behavior for anomaly detection.
.
radar radar is a technique for automatically identifying the likely causes of a regression failure .
radar relies on the availabil ity of a regression test suite such that all tests in the suite pass when executed on the original version of the software and some tests fail when executed on the modified software.
the basic idea behind radar is to identify the behavioral differences between the failing executions of the modified program and the passing executions of the original program.
the behavior of the original and modified programs is observed at a set of code locations determined according to the differences between the two programs.
when running the programs radar collects two pieces of dynamic information the sequence of statements executed and the values of the program variables in scope at the monitoring points.
the information collected from the passing tests is used to generate models that capture the correct behavior of the application.
to do so radar leverages daikon to generate expressions that model values that can be assigned to program variables and kbehavior to generate finite state automata fsa that represent the sequences of statements that can be executed without observing any failure.
radar checks the traces recorded for the failing executions agains the models derived from the passing executions to identify behavioral differences between original and modified programs that may indicate anomalies and that may be the causes of an observed failure.
mimic leverages radar s ability to monitor programs and generate and check models for program variables whereas it does not use radar s fsa models.
the monitoring model generation and model checking capabilities of radar have been adapted to fit mimic s purpose as we describe in section .
.
the mimic approach mimic is a failure analysis technique whose goal is to automatically identify the likely causes of failures from a set of failure data i.e.
runtime information about a failure .
mimic starts from a failure e.g.
a failing test and produces a list of events that likely explain the cause s of the failure.
as illustrated in figure mimic works in four steps execution synthesis monitoring points detection anomaly detection and filtering.
we first provide an overview of these steps and then discuss them in detail.
theexecution synthesis step starts from the failure data and generates multiple executions that mimic the corresponding failure.
as we quickly discussed in section .
in this step mimic leverages f3 which is based on symbolic execution to synthesize both failing and passing executions.
when the approach is successful de ne tab width c h c h c ... clump bu xmalloc max chars per input tab ... char s clump bu ... 2690if c input tab char chars per c chars per input tab 2691if c input tab char c nt width tab width chars per c input position if untabify input for i width i i s chars width figure code example bug13272 in pr.
passing input b t v v failing input b d b b b b b t v v figure examples of passing and failing inputs generated for program pr.
the failing executions mimic the behavior of the original failure intuitively they violate the same assertion.
the passing executions conversely are executions that are similar to the falling ones in the sense that they follow similar paths and typically reach the point of failure but do not result in a failure.
in its subsequent steps mimic uses these sets of similar executions to determine the events that can discriminate passing from failing executions.
themonitoring points detection step determines using a combination of several heuristics a set of code locations that are particularly suitable for detecting behavioral differences between passing and failing executions.
because collecting precise behavioral data from all code locations of a program is prohibitively expensive for any non trivial system mimic selects a small but meaningful set of code locations to be used as monitoring points.
theanomaly detection step collects data from the passing executions at the monitoring points determined in the previous step.
mimic uses the collected data to generate behavioral models that capture the supposedly correct behavior of the application.
the same monitoring points are then used to collect data from the failing executions.
mimic uses the data collected from the failing executions and the previously generated behavioral models to identify violations of the models that can indicate an anomaly i.e.
a likely failure cause .
as we discussed in section .
mimic leverages radar to monitor programs generate models and check models.
in principle the anomalies detected in the previous step could be already reported as likely causes of the failure being investigated.
however some anomalous events could result from spurious behaviors unrelated to the failure.
mimic addresses these cases in its filtering step which removes from consideration every anomalous event that has not been observed in every failing execution.
the rationale for this step is that since all the failing executions fail for the same reason a valid explanation for the failure should be present in every failing execution that has been analyzed.
at the end of this step the filtered anomalies are the likely failure causes that are reported to the developer to help him debug the application.
running example.
to demonstrate how mimic works we use a running example that is also one of the case studies considered in our empirical evaluation.
the example consists of a fault affecting pr a unix utilitytable excerpt of potential failure causes produced by mimic for pr.
source model actual values line pr.c input position input position pr.c input position input position pr.c i i pr.c c i c i pr.c c i c i pr.c chars per c i chars per c i pr.c chars per input tab ichars per input tab i program for paginating text files before printing.
pris part of coreutils .
the considered fault is difficult to trigger and in fact it affected many versions of prbefore being revealed and fixed.
figure shows an excerpt of pr.
the code in the example fails when before a tab character the number of backspaces in the input string exceeds the number of characters that can be removed.
figure provides an illustrative example in which the input string contains a tab nt at position seven and six out of the seven characters before the tab are backspaces nb .
the actual failure in the program is triggered when variable input position is assigned a negative value.
this variable represents the position in which the next character must be written.
when too many backspaces occur in the input string the program assigns a negative value to input position intuitively representing the case of a cursor that has been moved backward up to a negative position.
processing a tab character causes the program to enter the ifbranch at line .
when the value of width is computed at line if input position is negative macro tab width returns a value that is larger than the size of array clump buff.
this macro is apparently designed to work on non negative values only.
since width exceeds the size of the buffer the program causes a buffer overflow while iteratively executing line to map the tab character into a over length sequence of spaces.
mimic is able to capture the causes of this failure.
for instance table shows an excerpt of the failure causes automatically determined by our technique.
column source line indicates the line of code used as observation point.
column model indicates the property that is systematically violated by all the failing executions.
column actual values shows a sample value extracted from one of the failing executions.
mimic suitably identifies the trigger of the failure negative values assigned to input position see the first two rows in table .
this result might be already sufficient to debug the program as it is enough to notice that the unexpected negative value assigned toinput position causes the assignment of a too large value towidth which in turn produces the buffer overflow in the for loop at lines .
however mimic provides additional assistance to developers because it also identifies the unusually high values assigned to variable i which counts the number of iterations that are executed in the for loop.
this anomalous value explicitly points at the location where the buffer overflow happens demonstrating that a consequence of the negative value assigned to input position is an excessive number of loop iterations.
in the following sections we discuss how each individual step of the technique works in detail using our running example for illustrative purposes when possible.
.
execution synthesis in the execution synthesis step mimic generates a set of passing and failing executions similar to the falling execution that is being debugged.
to synthesizes these executions as we discussed earlier mimic leverages the execution generator in f3 which can generate executions starting from the sequence of calls performed in the failing execution.
it is worth noting that although we are presenting mimic as a technique mainly meant to be applied during in house debugging it could also be applied to crash reports from the field as long as they contain the right kind of failure data.
as we discussed in section .
the execution synthesis in mimic is based on a guided symbolic execution that can generate both failing and passing executions that mimic the failure at hand.
specifically the failing executions would perform the same calls and fail in the same way as the original failure the passing executions conversely would perform as many as possible of the calls in the call sequence and would not fail.
performing all calls may not necessarily result in a failure so in some cases f3has to skip calls to increase the set of program behaviors it can explore and be able to actually generate passing executions.
to properly distinguish between failing and passing executions mimic relies on an external oracle.
when used for in house debugging this is not an issue as it can simply reuse the oracle associated with the failing test case that is being debugged.
even if we wanted to use mimic on crash reports from the field however it would be possible to define a pseudo oracle that recognizes the reported field failure and can check whether a synthesized execution fails in the same way.
the facts that the pseudo oracle does not have to distinguish failing and passing executions in general but must simply be able to recognize the observed failure and that most reports from the field are about program crashes makes this feasible in many practical cases .
figure shows examples of a passing execution and a failing execution synthesized by mimic for the prprogram.
in the figure nbis a backspace char ndis a delete char ntis a tabulation character andnvis a vertical tab character.
the two inputs differ in the number of backspace and printable characters that occur before the tab character which are exactly the input elements that make the program fail.
the synthesis of many passing and failing executions that represent similar program behaviors is of crucial importance for mimic because its capability to detect failure causes depends on the differences that exist between the analyzed passing and failing executions.
usually the smaller these differences the more accurate is the detection of the failure causes.
recall that the synthesized failing and passing executions share the majority of the function calls.
in our empirical evaluation see section .
.
we provide evidence that mimic produces definitely better results when working with the synthesized executions than when using the test suites available with the programs.
.
monitoring points detection mimic identifies failure causes by comparing the behavior of the program in the failing executions to the models that represent the behavior of the program in the passing executions.
since collecting data and generating models is an expensive operation mimic automatically identifies a small but effective set of program points that can be used to observe the behavior of the system during failing and passing executions and determine the causes of the failures.
to select such proper monitoring points we defined asuitability formula that associates a value in the range to each line in the program.
the higher the suitability value the most likely the program point is a useful observation point.given a source line l the suitability of lis calculated as follows suitability l kpr passratio l l executed by all ks suspiciousness l failing executions kvf vicinityfailing l kvpof vicinitypof l otherwise since the formula is applied to the generated failing executions which are all caused by the same fault the anomalies that are reported to the developer must occur in every failing execution.
therefore the suitability formula always assigns to the lines of code that are not covered by every failing execution.
if a linelis executed by every failing execution its suitability is determined as a weighted sum of four terms.
the four constants kpr ks kvf andkvpof represent the four weights which have to sum to .
in the following paragraphs we explain the meaning and rationale for each term.
passratio l the detection of the illegal values assigned to program variables during failing executions requires good models that suitably generalize the behavior sampled with passing executions.
if a line is executed by several passing executions mimic can generate good models that capture the correct behavior of the application.
while if the line is executed by few passing executions mimic might generate models that overfit the executions failing to fully capture the legal behavior of the application.
passratio represents the ratio of passing executions that cover l that is the more passing executions cover lthe more likely lis selected as a monitoring point.
for example if out of passing executions coverlthepassratio l is .
.
suspiciousness l to suitably observe the trigger and the effect of a fault it is important to select monitoring points that are likely to be related to the fault.
to this end mimic computes the suspiciousness score of a line of code using fault localization.
the higher the suspiciousness score is the more likely the line of code is faulty or closely related to the fault.
in particular mimic leverages the customized fault localization technique in f3 which has been shown to work particularly well for synthesized executions to compute the suspiciousness score.
vicinityfailing l the code locations executed by failing executions only are likely to process illegal variable values that result from the failure.
to timely capture these anomalous values it is important to have monitoring points that are close to the lines executed by failing executions only.
mimic does not directly monitor the lines executed by failing executions only because no passing execution reaches these lines and thus no model of the correct behavior of the application could be generated for these locations.
vicinityfailing measures how close a line of code is to a statement executed by failures only.
in particular if lonlyis the set of lines executed by failing executions only vicinityfailing l is defined as vicinityfailing l max lonly2l onlyvicinitysourcefile l l only where vicinitysourcefile l l2 7jl1 l2j 50l1andl2 in same file otherwise 818the vicinity between two lines of code is if the two lines of code are in two different files.
otherwise the distance is computed using the difference between line numbers.
the vicinity is when the two line numbers are the same.
although this simple metric has worked well in the cases we studied more sophisticated metrics might be needed in the future.
one possible solution if such need were to arise when performing additional experimentation would be to compute vicinity based on proximity in the control flow graph of the program.
currently we compute vicinity using a formula based on an exponential growth to strongly penalize the lines of code that are far from any statement executed by failing executions only.
the constants in the formula are chosen to prevent values from dropping too abruptly the vicinity drops by every time the distance between the lines of code increases by a factor of .
vicintypof l a line of code which is near the point of failure is likely to capture the illegal values that are responsible for the actual failure.
to this end we compute the vicinity of a line of code lto the point of failure namely lpof using the following formula vicinitypof l vicinitysourcefile l l pof we did not systematically investigate different combinations of weights to discover the optimal formula for computing the suitability of a line of code l. however we empirically observed that thesuspiciousness and the passratio terms are the most important ones with the former term contributing more than the latter in the identification of good monitoring points.
we thus defined the following values for the four weights kpr ks kvf kvpof .
the suitability formula produces a ranking of the statements in the program.
the statements at the top of the ranking are those that are likely to represent the best observation points for discovering the causes of the failure according to the set of executions available.
from the ranking mimic selects the first nnon consecutive statements for monitoring.
mimic does not select consecutive statements because they are likely to give similar opportunities in terms of their ability to monitor values and discover failure causes.
therefore if consecutive statements occur among the first n statements only the one with the highest suitability is selected the choice is non deterministic when the suitability is the same .
in practice the exact value of ndepends on the resources that are available e.g.
time available for the analysis computational power of the analysis infrastructure .
in most of our experiments we usedn .
mimic uses the monitoring points to extensively observe the behavior of the program.
in practice it records the values of all the variables including fields and elements of data structures that are visible from the monitoring points.
as we discussed in section .
mimic uses the monitoring infrastructure available in radar to collect the data .
in our running example mimic automatically selected line among the top lines which resulted to be an effective choice for determining the causes of the failure the failure causes reported in table have been detected using line as observation point.
.
anomaly detection in the anomaly detection step mimic runs the passing and failing executions and collects information about program behavior at the monitoring points determined in the previous step.
the data collected from the passing executions are used to distill models that capture the values that can be legally assigned to program variables.
to this end mimic leverages daikon whichis an inference engine that generates boolean expressions for a set of variables whose values have been observed in multiple executions.
in our case for each individual monitoring point daikon generates a set of boolean expressions that hold for the variables that can be accessed from the monitoring point.
for the variable values collected from line in our running example mimic generates both useful expressions such as input position which captures an important characteristic of passing executions and spurious expressions such as clump buff s which does not capture any relevant information about the failure .
to discover anomalous events that might reveal the causes of failures mimic checks the data collected while running the failing executions with the models generated from passing executions.
each anomalous event is composed of three fields the source line where the event has been detected the model that has been violated by the anomalous event and the actual values that violated the model.
for instance h2697 input position input position 5i andh2697 clump buff s clump buff in001 s in001 iare two anomalous events detected by mimic for the pr case study.
regarding the classes of faults that can be addressed this anomaly detection strategy is general.
in fact the actual possibility to address a fault does not depend on the type of fault rather depends on the impact of the fault on the program variables.
in particular the faults that can be addressed with mimic are all the faults that can cause anomalous variable values that can be detected with daikon invariants or other invariant generators .
.
filtering although mimic generates passing and failing executions that are similar by construction the differences between them might be related to spurious events rather than to events that caused failures.
to filter out such spurious events mimic implements a simple heuristic that has shown to be effective in practice it considers the set of anomalous events detected for each failing execution and filters out the ones that do not occur in every failure.
since the same model might be violated in many different ways and the way the model is violated is usually irrelevant for identifying the failure causes mimic filters anomalies considering only the models and ignoring the actual values that violate the models.
for instance the model input position 0at line ofpr.c might be violated in many different ways.
for a failing execution input position may be equal to while for a different failing execution its value may be .
the fact that the two executions witness different values for input position is irrelevant.
the important information is that for all the failing executions input position has a negative value.
for this reason an anomaly a hloc model valuesi is common to all the failing executions if every failing execution produces an anomaly a0 hloc model values i with loc loc andmodel model .
in our running example mimic successfully preserved the anomalyh2697 input position input position 5i which is important to understand the fault whereas it eliminated the anomalyh2697 clump buff s clump buff in001 s in001 i which is irrelevant.
the anomalies that are not filtered out are presented to the developer as set of possible failure causes that might explain the reason of the failure reported in the crash report.
since each anomaly that is presented to the developer occurs as many times as the number of failing executions that have been synthesized there are multiple values that can be presented to the tester to show the way the models have been violated.
mimic selects one of these values non819table benchmark programs and faults used in our study.
name version size fault type fault id loc grep .
10k injected dg 4 sed .
14k injected ag 20 pr .
3k real mknod .
.3k real od .
2k real xmail .
1k real cve deterministically from the set of available ones while keeping the whole set of values that violate a model available for inspection.
table shows some of the failure causes automatically reported by mimic for our running example.
.
empirical evaluation to assess the effectiveness of mimic we implemented a prototype tool and used it to investigate the following research questions rq1 can mimic report anomalies that are related to faults?
rq2 does filtering increase the quality of the results?
rq3 is the automatic selection of monitoring points effective?
rq4 do synthesized executions produce better results than the test suites of the analyzed applications?
rq5 can the detected violations help understand the causes of a failure?
in the following we describe our prototype implementation the objects of study the experiment setup and the results of the evaluation.
.
implementation we implemented mimic in a prototype tool written in java.
the mimic prototype can analyze failures that affect programs written in c c .
our implementation integrates several other tools.
in particular mimic uses f3to synthesize passing and failing executions from failure data that consist of partial call sequences and radar to generate and check behavioral models .
radar monitors applications using the gdb debugger and generates program properties using daikon .
mimic collects coverage data using gcov .
mimic uses daikon with the suppression of redundant models disabled.
otherwise daikon might incidentally suppress a model that perfectly captures the reasons of the failure when it is implied by other models that do not represent the reason of the failure as clearly as the suppressed one.
.
objects of study table presents the programs that we used to evaluate mimic.
we selected six real world open source programs written in c and c grep a well known command line utility for searching the lines that match a given pattern within textual files sed a stream editor xmail a mail server and three programs from the coreutils file manipulation package pr mknod and od .
we selected these programs because they are well known unix utilities whose size ranges from loc to kloc.
we considered one fault per program.
in two cases grep and sed the faults have been defined by a third party we used the faulty versions of grep andsed available in the public repositorysir .
in all other cases the faults are real faults discovered by end users in the field.
in table column fault type indicates the type of fault while column fault id provides the exact reference to the fault that has been analyzed.
each fault is associated with a test case that triggers it and produces the corresponding failure.
for real faults we considered the test case implemented from the report provided by the end users.
for the injected faults we considered a failing test case randomly selected from the test suites provided with the programs.
the failure data have been obtained from running these tests against instrumented versions of the benchmark programs.
.
experiment setup in this section we present the design of the empirical studies aimed to answer our research questions.
rq1 can mimic report anomalies that are related to faults?
this research question investigates whether mimic can detect the causes of failures that is if it can detect anomalies related to the fault under analysis.
to answer this research question we executed the mimic prototype on each case study and manually classified the reported anomalies as either true or false positives.
we classified as true positives only those anomalies that either represent the condition that triggers the failure or capture erroneous variable values produced by the faulty code.
an effective analysis should return few anomalies with a high density of true positives.
rq2 does filtering increase the quality of the results?
this research question investigates the effectiveness of the filtering step.
to answer this research question we manually inspected the anomalies that mimic automatically filtered out and classified them as either true or false positives.
an effective filtering should eliminate most of the false positives without eliminating the true positives.
rq3 is the automatic selection of monitoring points effective?
mimic heuristically defines monitoring points taking into consideration the structure of the program and the set of executions that have been synthesized.
in principle the selected monitoring points should guarantee the best observation capabilities for the fault under analysis.
however their effectiveness must be empirically assessed.
to answer this research question we measured the number of true and false positives obtained by using the observation points determined by mimic and by following the common practice of observing the behavior of a program at the entry and exit points of its functions .
a good set of observation points should generate more true positives and less false positives.
rq4 do synthesized executions produce better results than the test suites of the analyzed applications?
this research question investigates the benefit of using a set of synthesized executions that accurately samples the behavior of the application around the failure compared to using the test suites available with the programs.
to answer this research question we compared mimic as defined in the paper to two alternative configurations.
the first configuration mimic orig applies mimic using the original test suite instead of synthesized executions but uses the same observation points computed for the synthesized executions.
in this way the differences on the results could be uniquely due to the different test cases used for the analysis.
however using the observation points computed from the synthesized executions for both configurations might penalize the analysis based on the original test suite.
we thus consider a third option mimic orig obs which uses the original test suite of the program and the observation points computed for the original test suite instead of the synthesized test suite i.e.
we applied the suitability formula to the tests available with the program instead of the synthesized executions .
since 820table results for rq1.
namepassing failing monitoring failure causes tests tests points tp fp grep sed sed od mknod xmail pr pr the test suites available with the programs do not include multiple failing test cases we used the synthesized failing executions for the purpose of the experiment.
rq5 can the detected violations help understand the causes of a failure?
this research question investigates whether the results obtained with mimic are not only related to the fault i.e.
they are true positives but are also practically useful to identify the faults.
the quantitative investigation of this research question would require a large study based on human subjects.
so far we focused on the technical contribution and such a study which is motivated by the promising empirical results reported in the next section is part of our future work.
here we try to address this research question qualitatively by shortly presenting the individual cases that have been investigated and providing qualitative arguments that show how the results returned by mimic can help identifying failure causes.
since mimic leverages several computationally expensive techniques e.g.
symbolic execution and dynamic invariants detection techniques and more importantly the goal of the empirical evaluation is to assess the effectiveness of the technique we did not consider any particular time limit for the analysis.
all the cases reported in this paper have been analyzed in few hours which is compatible with a common test overnight scenario.
the only exception is the fault in pr which needed one day of processing time due to the complexity of the symbolic states involved in the analysis.
.
results and discussion .
.
rq1 can mimic report anomalies that are related to faults?
table shows the results for rq1.
columns passing tests and failing tests report the number of passing and failing executions automatically synthesized by mimic.
column monitoring points reports the number of program points monitored for each case study.
we generally used the top 10locations.
in case 10was not enough to get any result see sed andpr we tried with a higher number of locations.
columns tpandfpreport the number of true and false positives among the set of the likely failure causes identified by mimic.
when using 10monitoring points mimic detected the failure causes i.e.
tp for 4out of 6cases.
in two cases sed andpr mimic detected no failure causes.
to investigate if the problem was related to the number of monitoring points we repeated the analysis with an increased number of points 20monitoring points were enough to identify the failure causes for pr while even 30monitoring points were insufficient for sed.
we carefully inspected the fault in sed and found that the corresponding fault could not be addressed with mimic.
this is due to thefact that the faulty code region always fails when executed.
there is thus no way to generate passing executions that cover code regions close enough to produce models useful to debug the fault.
it is worth noting that in the cases where the failure cause was not detected mimic generated no false positives either thus preventing the developer from inspecting useless anomalies.
in some cases mimic generated a perfect outcome such as for grep andxmail where it reported only true positives.
for three cases od mknod and pr conversely the results included both true and false positives.
however mimic always returned a small number of quick to inspect anomalies for the cases considered so we do not expect false positives to be a major issue in these cases.
in summary our results show that mimic has the potential to report the causes of failures automatically.
.
.
rq2 does filtering increase the quality of the results?
table shows the results for rq2.
for each case identified by the program name column name and the number of monitoring points used column monitoring points table reports the number of unique anomalies discovered by mimic that is multiple anomalies that violate the same model in different executions are counted as one.
column false positives shows the total number of anomalies unrelated to the fault detected by mimic before applying the filtering step column all the number of false positives successfully filtered out column filtered and the number of false positives that have not been filtered out automatically column reported .
column true positives shows the total number of anomalies related to the faults detected by mimic before applying the filtering step column all the number of true positives erroneously filtered out column filtered and the number of true positives successfully reported to developers column reported .
the last row of the table reports average values across all cases.
the results show that the filtering step can effectively remove many false positives.
in fact it eliminated around of the false positives on average although with varying performance among the individual case studies.
in particular for 4out of 7cases one case produced no anomalies filtering eliminated of the false positives thus returning only useful anomalies to the developers.
in the three remaining cases filtering was less effective eliminating from to12 of the false positives.
this result suggests that depending on the characteristics of the failure filtering could have varying effectiveness.
in particular we noticed that filtering was less effective when either mimic generated few failing executions such as for odandmknod or it generated many failing executions but the failure was triggered only through a very specific path with little possibility to obtain diverse failing executions such as for pr .
ideally mimic needs multiple diverse failing executions to effectively filter false positives and both lack of failing executions and lack of diversity are factors that clearly affect the performance of the filter.
finally filtering demonstrated to be conservative suggesting that the assumptions behind it tend to hold in practice.
in fact filtering never dropped any true positive as shown in table .
.
.
rq3 is the automatic selection of monitoring points effective?
table shows the results for rq3.
column mimic shows the number of true positives column tp and false positives column fp returned by mimic when using the top 10monitoring points 20in the case of pr identified by the suitability formula presented in section .
.
column entry exit points shows the number of true positives column tp and false positives column fp returned by 821table results for rq2.
namemonitoring true positives false positives points all filtered reported all filtered reported grep sed sed od mknod xmail pr pr avg table results for rq3.
namemimic entry exit points monitoring failure causes failure causes points tp fp tp fp grep sed od mknod xmail pr mimic when using the function entry and exit points to observe the program behavior.
the results show that the monitoring points identified with the suitability formula can be used to detect failure causes more effectively than a standard set of monitoring points such as the entry and exit points of program functions.
in fact when using entry exit points mimic missed several code locations important for detecting failure causes and the number of successful cases i.e.
cases with at least a true positive dropped to two.
monitoring entry and exit points might imply monitoring code locations that are both not well covered by passing executions and unrelated to the problem under investigation causing the generation of poor models that introduce many false positives.
in fact for three cases out of six sed od and xmail the strategy based on entry exit points returned significantly more false positives than mimic.
only in two cases the use of entry exit points resulted in less false positives.
in one of these two cases mknod detecting less false positives also implied detecting no true positives mimic conversely successfully discovered the causes of the fault in this case.
in the remaining case the entry exit points strategy produced slightly better results than the strategy implemented in mimic but both approaches were successful.
overall this empirical results demonstrate the effectiveness of the strategy defined in this paper which detects the monitoring points taking into consideration the specific characteristics of the case under analysis including the failure the set of synthesized executions and the structure of the program.
in particular the strategy based on the suitability formula has been able to identify an effective set of monitoring points for all the faults that could be analyzed with mimic recall that the fault in sed cannot be addressed with mimic .
.
.
rq4 do synthesized executions produce better results than the test suites of the analyzed applications?
mimic discovers failure causes exploiting its unique capability of synthesizing passing and failing executions that are similar to the failing execution under analysis.
one key question is to what extent this capability impacts the results.
to answer this research question we compared the results obtained with mimic to two alternative configurations mimic orig which consists of applying mimic to the test suite of the application using the monitoring points computed from the synthesized executions and mimic orig obs which consists of applying mimic to the test suite of the application using the monitoring points computed from these same tests.
we compare the results in terms of false and true positives.
figure shows a histogram that illustrates the number of false positives generated by the three configurations for five cases.
we did not consider xmail in this case because it is not distributed with a test suite and thus only mimic can be applied to it .
to keep the histogram readable we cut the bars at .
for sed od andpr however the configurations using the test suite distributed with the applications generated thousands of anomalies.
based on these results we can conclude that the original test suite cannot be used to analyze these three cases due to the excessive number of reported false positives.
on the contrary mimic generated a limited number of false positives for odandmknod and no false positives forsed showing the higher effectiveness of the synthesized tests compared to the original test suites in the context of our failure analysis.
in the case of grep the test suite of the program generated again a higher number of false positives compared to mimic which generated no false positives.
however in this case the number of false positives generated by mimic orig and mimic orig obs is manageable.
the failure in mknod is the only case in which using the original test suite the number of false positives that are generated is smaller than but comparable to the number of false positives generated by mimic.
the results about false positives already show the clear advantage of synthesized executions over the original test suites.
the superiority of synthesized executions is confirmed by the data about true positives shown in figure .
in terms of true positives mimic orig obs produced the worst results and detected useful anomalies only for grep.
when considering mimic orig the results slightly improved with the number of cases in which it reported at least a true positive raising to three.
in practice however only two of these three cases can be practically addressed with mimic orig as it returned thousands of false grep sed od mknod pr false posi ves miimc mimic orig mimic orig obs figure false positives reported by different versions of mimic to answer rq4.
grep sed od mknod pr true posi ves miimc mimic orig mimic orig obs figure true positives reported by different versions of mimic to answer rq4.
positives for pr which makes the result virtually useless for developers.
mimic conversely returned effective and concise results for four of these five cases.
in summary when using the original test suite instead of synthesized executions the effectiveness of the technique decreases significantly.
finally we were not able to apply mimic orig and mimic orig obs to xmail because xmail is not distributed with a test suite as discussed above but mimic was applicable and effective also in this case.
.
.
rq5 can the detected violations help understand the causes of a failure?
to demonstrate that mimic can be practically useful to help developers understand failure causes and debug program failures we shortly describe the output returned by mimic for each analyzed failure discussing how this output can be used to understand the fault in the program.
we will not discuss prbecause we already presented it as the running example in section .
because of space limitations we will keep the discussion short and focused on a subset of the anomalies detected by mimic.ingrep mimic discovered that all the failing executions violate the property end p. in the violated model pis a pointer used to iterate through the locations of a buffer endis also a pointer but it is used to point at the last location available in the same buffer.
this violation indicates that the failure is a buffer overflow caused by a wrong value assigned to p thus effectively focusing the attention of the developers on the statements that update the value of p. the fault in the program actually consists of a wrong assignment to p. inod mimic discovers that failures happen when the model file stats.st size !
n skip is violated that is when file stats.st size is equal to n skip.
this violation is detected just before the predicate uintmax t file stats.st size n skip is evaluated in a conditional statement.
developers are thus directly pointed at the implementation of the two branches following the conditional statement.
exploiting this contextual information it is fairly easy to recognize that when file stats.st size is equal to n skip the wrong branch is taken by the program a problem that can be fixed by replacing with in the conditional statement.
inmknod mimic discovers that failures happen when arg !
nullis violated that is when argis equal to null.
this anomaly exactly captures the failure cause because the program crashes when this null value is passed as a parameter to function quote which is invoked just after the statement that produces the violation.
inxmail mimic discovers that the failure happens when predicate iaddrlength 9is violated.
this happens for instance when iaddrlength a variable whose value is determined by user input is assigned the value .
because variable iaddrlength is used to copy chars into a buffer of fixed size this anomaly indicates a possible stack overflow that happens when iaddrlength is assigned a value that is too large which is exactly the cause of the observed failure.
.
threats to validity even if our results are positive for the cases considered they might not generalize beyond the systems and failures analyzed in this paper.
to mitigate this issue we considered failures in different applications both standalone applications and coreutils and of different nature both injected and real faults .
however additional experiments are necessary to assess the generality of the results.
part of the evaluation exploits the classification of the anomalies as true and false positives.
this classification has been the result of a partially subjective work and there is a risk that different developers might classify the same anomalies in different ways.
to mitigate this issue we classified as true positives only the anomalies that either represent the condition that triggers the failure or capture erroneous variable values produced by the faulty code we classified as false positives all remaining cases including the ambiguous ones.
to further address this issue we reported qualitative results that show case by case how the output generated by mimic has been useful to locate and understand the faults considered.
a last threat to validity is the possible presence of faults in our tools.
to address this threat we carefully inspected all the anomalies reported by mimic for each case considered in the empirical evaluation.
by so we gained confidence that our implementation and results were correct.
.
related work in this section we discuss existing techniques that are closely related to mimic.
in particular we discuss statistical fault localization anomaly detection experimental debugging and techniques exploiting synthesized executions.
823statistical fault localization techniques localize faults based on the intuition that code elements executed more frequently by failing executions are more likely to be faulty e.g.
.
these techniques differ mainly in the statistical analysis used to compute such likelihood.
most statistical fault localization techniques suffer from two main limitations.
the first limitation is that they rely on the existence of a large number of passing and failing test cases which are rarely available in practice.
the second limitation is that these techniques provide suspicious code locations without any further explanation which has been shown to be of limited helpfulness to developers .
mimic addresses both of these limitations.
first it does not rely on an existing test suite as it leverages f3to automatically generate multiple passing and failing executions from a single failure.
second it leverages radar to infer from the so generated passing and failing executions models that capture correct program behavior and violations of such models that can be used to understand the context of a failure and investigate its causes.
anomaly detection techniques can be used to identify the behavioral anomalies that occur in failing executions .
these techniques infer models that capture the valid behavior of a program from a set of passing executions and identify the anomalous events responsible for the failure by checking the failing executions using the inferred models.
the effectiveness of anomaly detection techniques depends on two main aspects the quality of the passing executions and the strategy used to monitor the program.
most state of the art techniques rely on existing test cases and monitor programs at predefined code locations e.g.
method entry and exit points .
a notable exception is radar which can determine the monitoring points dynamically according to the characteristics of the change that is being analyzed.
unfortunately radar can be applied to regression faults only in the sense that it relies on the existence of an extensive regression test suite .
mimic is the first anomaly detection technique that generates synthetic executions instead of relying on existing tests to explore the execution space close to the original failing execution and selects monitoring points based on the characteristics of both the program and the synthesized executions rather than a priori.
results from our empirical studies show that these two capabilities are of fundamental importance in determining the effectiveness of our approach.
experimental debugging approaches such as delta debugging and predicate switching can also be used to perform fault localization.
delta debugging e.g.
is based on a divide and conqueror algorithm that given one passing execution and one failing execution identifies a minimal set of circumstances e.g.
inputs or program states that can distinguish the two executions and can thus be considered causes for the failure.
techniques based on predicate switching conversely aim to identify the predicates in the code that if flipped can transform a failing execution into a passing one .
both delta debugging and predicate switching alter executions in a possibly unsound way which often results in infeasibility issues that negatively affect the diagnosis ability of these techniques.
mimic conversely always identifies anomalies that differentiate real i.e.
feasible failing executions from real passing executions.
other techniques share with mimic the idea of applying anomaly detection to automatically synthesized executions e.g.
and differ in both the kind of output they produce and the execution synthesis strategy they use.
sahoo and colleagues use dynamic data dependence information to locate the statements that may have generated anomalous values in failing executions that is values that violate invariants derived from synthesized passingexecutions .
the primary purpose of this technique is different from mimic s as it focuses on fault localization and does not aim to detect the causes of failures as mimic does.
most importantly this technique relies on the availability of a grammar based specification for the generation of test inputs while mimic generates test cases using guided symbolic execution.
these two alternative techniques have somehow complementary strengths and weaknesses as shown in related work by kifetew and colleagues .
bugex shares with mimic the goal of helping software developers understand the failure context .
specifically bugex produces as output a set of predicates that are most likely to occur in failing executions.
bugex and mimic differ fundamentally in the way they generate program inputs and the way they observe program executions.
bugex generates inputs from a failing unit test using a search based approach whereas mimic uses guided symbolic execution to systematically generate synthetic executions from a set of executions data .
as for observing program executions bugex monitors a set of state predicates at predetermined points in the program whereas mimic monitors predicates at code locations determined according to the characteristic of both the program and the executions of synthesized tests.
our empirical evaluation shows that this latter approach can be more effective than one based on observing program executions at fixed locations.
.
conclusion most automated techniques that aim to help developers localize and understand the causes of software failures suffer from two main limitations.
first they can be applied only when a high number of passing and failing tests are available which rarely happens in practice.
second they provide little to no information about the possible causes of a failure which makes them less useful to developers .
to address these limitations researchers have recently defined techniques that generate passing and failing executions automatically and analyze the differences between passing and failing executions.
mimic the approach that we presented in this paper operates in this space and is the first technique that combines the following unique capabilities debugging of individual program failures even when a test suite is not available generation of passing and failing executions similar to an observed failure and particularly suitable for debugging and differential behavioral analysis identification of suitable monitoring points for the comparison of passing and failing executions and effective filtering of false positives by leveraging multiple failing executions.
the results of our empirical evaluation performed on six faults of several real world applications show that mimic can effectively detect failure causes.
they also show that mimic s unique capabilities and in particular the ability to synthesize passing and failing executions and identify effective monitoring points are crucial for the success of the technique.
these promising empirical results motivate the design of a user study with the goal of confirming that the reports produced by mimic can actually help developers locate understand and fix faults.
in addition to performing this user study in future work we will also investigate techniques for systematically analyzing each individual anomaly identified by mimic to empirically confirm or disprove the correlation between the anomaly and the failure and further improve the effectiveness and accuracy of our approach.