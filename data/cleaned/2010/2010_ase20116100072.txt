local vs. global models for effort estimation and defect prediction tim menzies andrew butcher cs ee wvu morgantown usa tim menzies.us abutcher afrolegs.comandrian marcus computer science wayne state university usa amarcus wayne.eduthomas zimmermann microsoft research redmond usa tzimmer microsoft.comdavid cok grammatech inc. ithaca new york dcok grammatech.com abstract data miners can infer rules showing how to improve either a the effort estimates of a project or b the defect predictions of a software module.
such studies often exhibit conclusion instability regarding what is the most effective action for different projects or modules.
this instability can be explained by data heterogeneity.
we show that effort and defect data contain many local regions with markedly different properties to the global space.
in other words what appears to be useful in a global context is often irrelevant for particular local contexts .
this result raises questions about the generality of conclusions from empirical se.
at the very least se researchers should test if their supposedly general conclusions are valid within subsets of their data.
at the very most empirical se should become a search for local regions with similar properties and conclusions should be constrained to just those regions .
index terms data mining defect effort estimation validation empirical se.
i. introduction a repeated pattern in software engineering research is conclusion instability i.e.
the finding that some effect xis not generally true.
for example in the field of software development effort estimation mair and shepperd compared regression to analogy methods.
from empirical studies they found no conclusion regarding which methods were best seven favored regression four were indifferent and nine favored analogy .
a similar pattern of conclusion instability can be found in module defect predictors learned from static code features .
zimmermann et al.
learned defect predictors from pairs of projects angbracketleftproject project angbracketright.
in only of pairs did defect predictors learned in project 1work in project .
other conclusion instability results in effort defect estimation are reported in .
the work was partially funded by nsf grant ccf and the qatar west virginia university research grant nprp .
also the research reported in this document presentation was performed in connection with contract instrument w911qx c with the u.s. army research laboratory.
the views and conclusions contained in this document presentation are those of the authors and should not be interpreted as presenting the official policies or position either expressed or implied of the u.s. army research laboratory or the u.s. government unless so designated by other authorized documents.
citation of manufacturers or trade names does not constitute an official endorsement or approval of the use thereof.
the u.s. government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright notation hereon.one explanation for conclusion instability is data heterogeneity .
if data sets contain local regions with very different properties then an induction cross validation study would exhibit high variance when tests are conducted on stratifications with unusual properties.
to test this conjecture this paper compares the learning of treatments changes that are intended to improve some quality measure using just the local data in adjacent clusters to all the global data found in all clusters based on the literature on outlier removal in software engineering e.g.
we expected to find a few heterogeneous clusters spread around other clusters that were mostly homogeneous.
to our surprise we found the reverse.
usually every cluster was different to the others and in those clusters the treatments from local regions were different and superior to the global treatments.
it is neither novel nor interesting to say that particular cases cansometimes contradict general principles.
what was surprising however is that in these results the global treatments were nearly always inferior to the local treatments.
note that such data heterogeneity explains the prevalence of conclusion instability.
it also suggests a change of focus in se research rather than focus on generalities that may be irrelevant to any particular project empirical se should focus more on context specific principles.
the rest of this paper is structured as follows.
first we motivate this work with a small case study on local learning.
then we discuss techniques for localized reasoning the where clustering algorithm that divides the data the which learner that finds treatments in clusters.
we use those techniques in an experiment that compares the treatments learned from global or local contexts.
this is followed by notes on validity and related work.
ii.
l ocal learning motivations to motivate this paper we offer the following simple example of the value of local learning.
there are many general truisms in the field of software engineering.
our results are only interesting if our treatments could978 .
c ieee ase lawrence ks usa343 not have been generated in a much simpler way just by applying the truisms.
for example the cocomo coqualmo effort defect predictors assume that defects and efforts are mostly reduced by decreasing functionality measured either in function points or lines of code .
according to this assumption the best thing a manager can do to control defects and cost is to discard needless functionality.
this process of discarding requirements is also one of brooks key recommendations in the mythical man month .
in the experiments section of this paper which learns treatments i.e.
changes from four data sets.
some of those treatments agree with this truism of make it smaller .
for example which can learn this treatment loc to read this note that we discretize variables min..max to .. .
hence loc 1means set lines of code to minimum .
significantly of the treatments learned in the experiments section the make it smaller treatment appears far more often in the global treatments than in the local treatments as we expected .
our experiments used four data sets two on effort estimation and two on defect prediction and in two of those data sets the learned global treatment recommends minimizing the function points or lines of code of that system.
however in the local treatments learned by our experiments only of them recommend make it smaller .
that is what seems to be a good idea overall e.g.
make it smaller is actually irrelevant to sub groups within the data i.e.
local contexts .
notice that our results do not disagree with boehm and brooks.
in general make it smaller is a valid method of reducing the effort and defects associated with a software system.
indeed our global analysis reaches the same conclusion in half the experiments we present below.
however we would add that for particular kinds of projects other factors may be more important than just size.
for example for one data set explored below we learn a treatment from a local region of the form pcap nominal which in cocomo speak means avoid programmers with poor or very poor programming capability.
note that this treatment makes no reference to the sizeof the system since this particular data set the size effects were dominated by the impact of poorly trained programmers.
examples like this motivate our research into local lesson learning in software engineering.
our preferred method of learning those treatments is to combine two tools which and where.
iii.
a lgorithms which and where in order to conduct the experiments of this paper we need one tool that can learn local lessons from each cluster and a second tool that can find each cluster.
this section describe two such tools the which contrast set learner and the where clusterer.we currently favor the two tools since they are based on years of our research and incorporate the best practices we have found so far in our work.
also they scale to large data sets.
this does not mean that the community should uncritically accept them.
like any learner where and which rely on certain tuning parameters to control their operation.
we have used our best engineering judgment to set those parameters but it is possible that other settings or indeed other algorithms are better suited to this task.
a challenge problem we offer other researchers is to review our our methods to propose refinements alternatives.
but the details of tuning parameters for where and which are orthogonal to this discussion.
to defend our conclusions this paper shows that when the same analysis method is applied globally to all data or locally to just some intra cluster data then different and better treatments are found from the local analysis.
the rest of this paper presents that demonstrations.
a. contrast set learning with which when we show data mining output to business users their first question is usually what does this say about how to improve a project?
.
to answer this question we use contrast set learning to infer rules describing differences between a current context called the baseline and a better context called the target .
a contrast set rule takes the form if r xthen change 1 0 support we say this rule selects some support of the data that contains a different and hopefully better distribution of the dependent quality variables and by select we mean it finds all rows consistent with rx .
here rxis atreatment containing a set of attribute value pairs av 0is the median score of all instances in the baseline and target and 1is the median score in the selected subset of baseline and target.
for effort and defect prediction where lessisbetter then the ratio 1 0is smaller if the treatment selects for better instances.
it turns out that the minimal description of the differences between two things is often much smaller than a full description of both things.
for example in the experiments shown below we generate treatments that reference only one attribute.
as a result we can show our users succinct rules describing what needs to change in order to select for certain desired classes.
our which contrast set learner loops over attribute values combinations combining those that look most promising continuous attributes are discretized to values.
a stack is created one item for every attribute value.
the items in that stack are sorted using 1 0 support .
number of times do generate number of new items as follows.
pick two items at random favoring those with better 1 0 support .
combine the pair into a new item.
score it.344technique name notes default which number of bins for descretizing number attributes equal frequency bins generates performance score for a set of instancese.g.
median effort number of loops number of pairs to be picked and combined maximum acceptable height of which s stack minimum acceptable i 0 support score where stopping rule for quadtree tree recursion n stopping rule for clustering .
fig.
.
default settings nrefers to the number of instances .
sort the new items rules into the stack.
repeat step until no new improvements seen in the best score.
return the item with best score.
which is controlled by the settings of figure .
two settings change the maximum size of which s stack and the minimum acceptable 1 0 support score .
which runs fastest when and only allow for small stacks processing rules with largest 1 support scores.
however this fast version of which can miss rules which in isolation are not promising but when combined are useful.
to avoid that issue this study uses and i.e.
do not prune the stack and all rules are acceptable .
in the following section we will use which on pairs of neighboring clusters found by where.
in those experiments where clusters range in size from to instances which runs very quickly indeed on a 4gb machine with a .5ghz processor a python version of which terminates in under a second excluding time to read any data from disk .
b. using where to find similar projects this section describes where a fast clustering algorithm for finding software artifacts with similar attributes.
this process is controlled by the settings of figure .
where clusters data on dimensions synthesized along the axis of greatest variability in the data.
one way to find such dimensions is via methods such as principal component analysis pca that transform dbasic dimensions that might be correlated into a fewer number of uncorrelated orthogonal components.
in pca component iaccounts for as much variability as possible in the data and an orthogonal component i 1tries to account for the remaining.
matrix factoring methods like pca take polynomial time to execute .
faloutsos lin offer a linear time heuristic for generating these dimensions.
given ninstances their fastmap heuristic finds the dimension of greatest variability to a line drawn between the two furthest points.
these two points are found in linear time as follows pick any instance zat random find the instance xthat is furthest away from z find the instance ythat is furthest away from x fig.
.
each dot is an d dimensional instance mapped into d using equation .
one dimension is the line between x at the origin and the most remote instance y at0 c .
each dot has distance afrom the origin and bfrom the most remote point.
the median point on the xandyaxis are x and y respectively.
these median points divide the space into four quadrants.
the line xy is an approximation to the first component found by pca and is computed using 2ndistance calculations i.e.
faster than pca s polynomial time inference .
as shown in figure an orthogonal dimension to xy can be found by declaring that the line xy is of length cand runs from point to c .
each instance now has a distance ato the origin instance x and distance bto most remote point instance y .
from the pythagoras and cosine rule each instance is at the point x y x a2 c2 b2 2c y a2 x2 figure shows four quadrants defined by the median values of each dimension x y northwest northeast southwest southeast .
where recurses on each quadrant to generate a balanced tree of quadrants stopping when a sub quadrant has less than instances .
that is after an o n process that generates the quadtrees where can use the quadtrees as an index that maps test instances to related instances in time o log4n .
schikuta warns that quadtrees needlessly sub divide data when neighboring leaf quadrants have similar properties.
hence as a post processor to quadtree generation where combines similar leaf quadrants as follows create a list of leaf quadrants sorted by their density number of instances divided by cluster size .
set stop to maximum density of items in that list.
starting with the densest cluster perform a geometric search through immediate neighbors of this first quadrant.
remove all quadrants connected in this way from the list and added into their own separate cluster.
if the list is not empty find next cluster goto step .345initial two dimensions.
after quadtree generation.
after clustering of neighboring leaf quadrants.
fig.
.
recursive dimensionality synthesis.
each d dots represents descriptors of a software project.
colors on the right hand side show median intra cluster development effort green lowest effort red highest effort .
white denotes a region too sparse to cluster.
generated from the nasa93 effort estimation data set using the default parameters of figure .
figure shows the results of running where on the nasa93 effort estimation dataset from the promise repository see each dot describes one project using independent attributes and one dependent attribute showing the development effort in months .
the left hand side of that figure places the data within the top two dimensions learned by fastmap.
the middle figure shows the leaf quadrants found after where recursively explored the northwest northeast southwest southeast quadrants.
the right hand side figure shows the results of leaf quadrant clustering.
each cluster has been colored to show the median intra cluster development effort.
the colors range from dark red highest effort to dark green lowest effort .
the white clusters contain less than members this occurs when a parent cluster has less than items .
one inference supported by figure is what to change .
consider the three clusters in figure labelled c c c .
suppose a manager of a project in the pink cluster cis considering how to decrease the development effort of that project of all the neighbors of that cluster the green cluster c has the lowest development effort .
accordingly that manager would run which over the c data to learn treatments that convert projects of type ctoc .
note that such a strategy is notavailable to the manager projects in the dark green cluster c .
no neighbor of c has a shorter development effort so in that cluster we would advise to just maintain the status quo.
one advantage of where is that it scales to large data sets.
where always recurses on two dimensions synthesized in linear time via fastmap .
this approach scales linearly on the number of attributes.
where also scales very well on the number of instances.
figure shows runtimes after applying where to data sets from the promise repository cm1 kb2 mw1 kc3 pc1 kc1 from where the instances are copied once twice four or eight times.
note that the runtimes scale linearly with data set size.
one potential drawback with where is that since it uses the fastmap heuristic it may not find the points that best .
.
.
10000runtime secs of rows fig.
.
where s runtimes scale linearly on number of rows.
generated using the defaults on figure .
represent the dimensions of greatest variability in the data.
to check this if this heuristic generates inaccurate dimensions elsewhere we have conducted extensive experiments with the fastmap heuristic versus other more considered clustering methods such as k means.
our results agree with those of faloutsos lin in practice the approximate dimensions found by fastmap does not degrade inferencing compared to other more complete and slower approaches .
iv.
e xperiments the goal of our experiment is to test locally learned treatments are better and different to global treatments.
a. data this study used data from china is software development projects tagged with the development effort in months .
each project is described in terms of function points i.e.
number of highlevel operations within the system shown in figure .
nasacoc are the cost estimation instances in the combined nasa93 and coc81 datasets converted to coc ii via reifer et al.
s rosettastone algorithm .
lucene2.
is a defect log on oo classes in a ja v a search engine optimized for text mining.
xalan2.
is a defect log on the classes of an oo java implementation of an xlst processor.
adjusted function points adjusted size by the standard value adjustment factor vaf input function points ufp of input output function points ufp of external output enquiry function points ufp of external enquiry file function points ufp of internal logical files or entity