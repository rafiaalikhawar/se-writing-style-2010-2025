techniques for improving regression testing in continuous integration development environments sebastian elbaumy gregg rothermely john penixz yuniversity of nebraska lincoln lincoln ne usa elbaum grother cse.unl.eduzgoogle inc. mountain view ca usa jpenix google.com abstract in continuous integration development environments software engineers frequently integrate new or changed code with the mainline codebase.
this can reduce the amount of code rework that is needed as systems evolve and speed up development time.
while continuous integration processes traditionally require that extensive testing be performed following the actual submission of code to the codebase it is also important to ensure that enough testing is performed prior to code submission to avoid breaking builds and delaying the fast feedback that makes continuous integration desirable.
in this work we present algorithms that make continuous integration processes more cost effective.
in an initial presubmit phase of testing developers specify modules to be tested and we use regression test selection techniques to select a subset of the test suites for those modules that render that phase more costeffective.
in a subsequent post submit phase of testing where dependent modules as well as changed modules are tested we use test case prioritization techniques to ensure that failures are reported more quickly.
in both cases the techniques we utilize are novel involving algorithms that are relatively inexpensive and do not rely on code coverage information two requirements for conducting testing cost effectively in this context.
to evaluate our approach we conducted an empirical study on a large data set from google that we make publicly available.
the results of our study show that our selection and prioritization techniques can each lead to costeffectiveness improvements in the continuous integration process.
categories and subject descriptors d. .
testing and debugging testing tools tracing general terms reliability experimentation keywords continuous integration regression testing regression test selection test case prioritization permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
fse november hong kong china copyright acm ... .
.
.
introduction in continuous integration development environments engineers merge code that is under development or maintenance with the mainline codebase at frequent time intervals .
merged code is then regression tested to help ensure that the codebase remains stable and that continuing engineering efforts can be performed more reliably.
this approach is advantageous because it can reduce the amount of code rework that is needed in later phases of development and speed up overall development time.
as a result increasingly organizations that create software are using continuous integration processes to improve their product development and tools for supporting these processes are increasingly common e.g.
.
there are several challenges inherent in supporting continuous integration development environments.
developers must conform to the expectation that they will commit changes frequently typically at a minimum of once per day but usually more often.
version control systems must support atomic commits in which sets of related changes are treated as a single commit operation to prevent builds from being attempted on partial commits.
testing must be automated and test infrastructure must be robust enough to continue to function in the presence of significant levels of code churn.
the challenges facing continuous integration do not end there.
in continuous integration development environments testing must be conducted cost effectively and organizations can address this need in various ways.
when code submitted to the codebase needs to be tested organizations can restrict the test cases that must be executed to those that are most capable of providing useful information by for example using dependency analysis to determine test cases that are potentially affected by changes.
by structuring builds in such a way that they can be performed concurrently on different portions of a system s code e.g.
modules or sub systems organizations can focus on testing changes related to specific portions of code and then conduct the testing of those portions in parallel.
in this paper we refer to this type of testing performed following code submission as post submit testing .
post submit testing can still be inordinately expensive because it focuses on large amounts of dependent code.
it can also involve relatively large numbers of other modules which themselves are undergoing code churn and may fail to function for reasons other than those involving integration with modules of direct concern.
organizations interested in continuous integration have thus learned and other researchers have noted that it is essential that developers first test their code prior to submission to detect as many integration errors as possible before they can enter the codebase break builds and delay the fast feedback that makes continuous integration desirable.
organizations thus require developers to conduct some initial testing of new or changed modules prior to submittingthose modules to the codebase and subsequent post submit testing .
in this paper we refer to this pre submission phase of testing as pre submit testing .
despite efforts such as the foregoing continuous integration development environments face further problems of scale as systems grow larger and engineering teams make changes more frequently.
even when organizations utilize farms of servers to run tests in parallel or execute tests in the cloud test suites tend to expand to utilize all available resources and then continue to expand beyond that.
developers preferring not to break builds may over rely on pre submit testing causing that testing to consume excessive resources and become a bottleneck.
at the same time test suites requiring execution during more rigorous post submit testing may expand requiring substantial machine resources to perform testing quickly enough to be useful for the developers.
in response to these problems we have been investigating strategies for applying regression testing in continuous integration development environments more cost effectively.
we have created an approach that relies on two well researched tactics for improving the cost effectiveness of regression testing regression test selection rts and test case prioritization tcp .
rts techniques attempt to select test cases that are important to execute and tcp techniques attempt to put test cases in useful orders section provides details .
in the continuous integration context however traditional rts and tcp techniques are difficult to apply.
traditional techniques tend to rely on code instrumentation and be applicable only to discrete complete sets of test cases.
in continuous integration however testing requests arrive at frequent intervals rendering techniques that require significant analysis time overly expensive and rendering techniques that must be applied to complete sets of test cases overly constrained.
furthermore the degree of code churn caused by continuous integration quickly renders data gathered by code instrumentation imprecise or even obsolete.
in this work therefore we utilize new rts and tcp techniques.
we have created rts techniques that use time windows to track how recently test suites1have been executed and revealed failures to select test suites to be applied during pre submit testing.
we then utilize tcp techniques based on such windows to prioritize test suites that must be performed during subsequent post submit testing.
the approaches we utilize can be applied to individual test suites as their execution is requested.
they also utilize relatively lightweight analysis and do not require code instrumentation rendering them appropriate for use within the continuous integration process.
to evaluate our approach we conducted an empirical study on a large data set obtained from google a data set that allows us to simulate and track the results of various regression testing approaches on a substantial codebase.
the results of our study show that our rts technique can greatly improve the cost effectiveness of pre submit testing that is it produced large reductions in test case execution costs while deferring relatively few faults to postsubmit testing.
we also find that our tcp technique can substantially reduce the time required to provide feedback on failing test cases during post submit testing allowing developers to address problems more quickly.
thus overall our techniques contribute directly to the goals of the continuous integration process.
1traditionally rts and tcp techniques have been applied to test cases.
in this work we apply them to test suites primarily because the datasets we use to study our approach include test suites and analysis at the test suite level is more efficient.
the approaches could also however be performed at the level of test cases.in summary the contributions of this work are a characterization of how regression testing is performed in a continuous integration development environment and the identification of key challenges faced by that practice that cannot be addressed with existing techniques.
a definition and assessment of new regression testing techniques tailored to operate in continuous integration development environments.
the insight underlying these techniques is that past test results can serve as lightweight and effective predictors of future test results.
a sanitized dataset collected at google who supported and helped sponsor this effort containing over .5m records of test suite executions.
this dataset provides a glimpse into what fast and large scale software development environments are and also lets us assess the proposed techniques while google is investigating how to integrate them into their overall development workflow.
the remainder of this paper is organized as follows.
section provides background and related work.
section discusses the continuous integration process used at google and the google dataset we use in the evaluation.
section presents our rts and tcp techniques.
section presents the design and results of our study.
section discusses the findings and section concludes.
.
background and related work letpbe a program let p0be a modified version of p and let t be a test suite for p. regression testing is concerned with validatingp0.
to facilitate this engineers often begin by reusing t but reusing all of t the retest all approach can be inordinately expensive.
thus a wide variety of approaches have been developed for rendering reuse more cost effective via regression test selection e.g.
and test case prioritization e.g.
.
yoo and harman provide a recent survey.
regression test selection rts techniques select from test suite t a subset t0that contains test cases that are important to re run.
when certain conditions are met rts techniques can be safe i.e.
they will not omit test cases which if executed on p0 would reveal faults in p0due to code modifications .
test case prioritization tcp techniques reorder the test cases intsuch that testing objectives can be met more quickly.
one potential objective involves revealing faults and tcp techniques have been shown to be capable of revealing faults more quickly .
because tcp techniques do not themselves discard test cases they can avoid the drawbacks that can occur when regression test selection cannot achieve safety.
alternatively in cases where discarding test cases is acceptable test case prioritization can be used in conjunction with regression test selection to prioritize the test cases in the selected test suite.
further test case prioritization can increase the likelihood that if regression testing activities are unexpectedly terminated testing time will have been spent more beneficially than if test cases were not prioritized.
a key insight behind most of the rts and tcp techniques studied to date is that certain testing related tasks such as gathering code coverage data can be performed in the preliminary period of testing before changes to a new version are complete.
the information derived from these tasks can then be used during the critical period of testing after changes are complete and when time is more limited.
this insight however applies only in cases where sufficiently long preliminary periods are available and this is not typically the case in continuous integration development environments.while most work on regression testing has focused on development processes that fit the foregoing description there has been some work considering more incremental processes.
do et al.
study test case prioritization in the presence of time constraints such as those that arise when faster development and test cycles are used.
walcott et al.
zhang et al.
and alspaugh et al.
present prioritization techniques that operate in the presence of time constraints.
this work however does not specifically consider continuous integration processes.
there has been some recent work on techniques for testing programs on large farms of test servers or in the cloud e.g.
.
this work however does not specifically consider continuous integration processes or regression testing.
a few pieces of work do address continuous integration and testing.
saff and ernst consider a form of continuous testing in which regression tests are run continuously as developers write code.
this work however focuses on testing during the development effort itself not at the testing periods typically utilized in continuous integration processes.
jiang et al.
consider continuous integration environments and discuss the use of test case prioritization following code commits to help organizations reveal failures faster but their work focuses on the ability to use the failures thus revealed in statistical fault localization techniques.
kim and porter use history data on test cases to prioritize them as do we but they do not consider the continuous integration process.
in the work most closely related to ours marijan et al.
briefly present a prioritization algorithm for continuous regression testing.
the algorithm also utilizes the time since the last test cases failed but does not consider the notions of an execution or a prioritization window and it assumes a limit on time allotted for test execution.
in the work most closely related to ours yoo et al.
also working with google data sets describe a search based approach for using tcp techniques to help developers perform pre submit testing more cost effectively.
their study of the approach while modest in scale does suggest that it can function cost effectively in the google context.
they do not however consider the use of rts techniques or consider the application of their technique to post submit testing.
.
regression testing at google continuous integration development environments are employed by various organizations in various forms.
because we are familiar with the use of continuous integration at google and because we have access to a large collection of data relevant to the processes employed there we describe the ways in which continuous integration is used there in relation to other important aspects of testing.
note that our description which is based on materials available at is necessarily somewhat simplified but is sufficient to support the presentation of our approach and of our empirical study.
in section we provide further discussion of several complexities that arise when continuous integration is employed in practice.
google s developers create test suites utilizing specialized xunitlike frameworks e.g.
the google c testing framework and extensive execution and monitoring support.
this holds for test cases at the unit integration and system levels.
in this context test cases are composed of both inputs and oracles and thus most test execution and oracle checking is automated.
there are exceptions such as tests that require human judgment but they are relatively infrequent.
this enables the execution of test suites on a common test suite execution infrastructure that can leverage the massive computational power of server farms or the cloud to obtain test results faster.while developers create the test suites used at google an independent test infrastructure team manages the infrastructure used in testing and takes steps necessary to support fully automated test execution.
one element of this effort involves keeping track of dependencies between code modules 2this supports efforts to focus testing resources on changed and dependent modules during postsubmit testing as suggested in section .
given the scale of testing requirements google s test execution infrastructure operates at the level of entire test suites.
google s process includes both pre submit and post submit testing as described in section .
when a developer completes their coding activities on a module m the developer presents mfor presubmit testing.
in this phase the developer provides a change list that indicates modules directly relevant to building or testing m. pre submit testing requests are queued for processing and the test infrastructure performs them as resources become available this testing includes to different extents the execution of all test suites relevant to all of the modules listed in the change list.
if failures are encountered the developer is informed and must correct the faults responsible and repeat the process.
when pre submit testing succeeds for m a developer submits mto source code control a process that causes it to be considered for post submit testing.
at this point tools are used to determine using module dependency graphs modules that are globally relevant tom.
this includes modules on which mdepends as well as modules that depend on m. the dependence analysis relied on in this step is coarse and the dependencies calculated often form an over approximation this is necessary in order to ensure that the process is sufficiently fast .
all of the test suites relevant to these modules are queued for processing.
the foregoing processes also apply to new code.
new modules are submitted for pre submit testing with change lists along with new test suites.
new modules are then submitted to source code control and subsequent post submit testing where their dependence information is calculated and used to conduct testing of dependent modules.
the google shared dataset of test suite results.
in conducting this work we have been able not only to use real industry data but also to sanitize and package a large dataset of test suite execution results that google has willingly made available for use by the software engineering research community .
it is this data that we rely on in evaluating the techniques presented in this paper and because this data also helps illustrate the continuous integration process we present basic information on it here.
the google shared dataset of test suite results gsdtsr contains information on a sample of over .
million test suite executions gathered over a period of days applied to a sample of google products.
the dataset includes information such as anonymized test suite identifiers change requests phases presubmit and post submit at which test suites were executed outcome statuses of test suite executions and time required to execute test suites.
note however that the dataset contains only a small sample of observations relative to to the entire pantheon of observations that could be accrued at google.
table provides data on the test suites in gsdtsr focusing on the relative sizes of test suites utilized in the two phases the percentages of these test suites that have been observed across each phase and size and the percentages of test suite executions that 2we use the term code modules in this paper for simplicity to refer to any element of the codebase that can be modified and passed to a build process as well as larger code units for which the build processes involve multiple independently built modules.
hour day test suite execu ons figure flow of incoming test suites for pre submit testing over three days.
failed.
size classifications of test suites are meant to serve as qualitative indicators of their scope and of the resources required to execute them .
in general small test suites are those that focus on a class medium test suites are those that target a class and its neighboring classes and large test suites are those that most closely represent field scenarios.
additionally with each size there are also rules that enforce duration and resource usage limits.
table google dataset distribution of test suite executions per testing phase and test suite size phase size total failing pre submit small .
pre submit medium .
pre submit large .
post submit small .
post submit medium .
post submit large .
table provides a different view of the test suites in gsdtsr showing the percentages of test suites of different sizes that exist for each of the major programming languages utilized.
this table illustrates the diversity of languages that a large continuous building and testing process must accommodate and how the language choice varies depending on the testing scope.
table google dataset distribution of test suite executions per language across test suite sizes language total small medium large config .
.
.
c .
.
.
.
go .
.
.
gwt .
java .
.
.
python .
.
.
.
shell .
.
.
.
web .
.
.
.
to further illustrate the type of information present in gsdtsr as well as to further illustrate the effects that continuous integration has on test suite execution at google figure presents data on test suites executed during pre submit testing over a three day period.
the horizontal axis depicts hourly time periods labels correspond gmt the vertical axis depicts the number of test suite executions occurring.
note that requests for test executions arrive continuously this is because engineers are working in various locationsaround the globe.
the heaviest periods of test execution however occur in the time periods corresponding to lunch time in mountain view california where engineers tend to submit modules for testing prior to taking their breaks.
.
approach the foregoing process employed in a continuous integration context has many advantages.
frequent system builds and regression testing of new and changed modules ensure faster feedback on potential problems.
post submit testing reduces the number of problems that slip through into the codebase and affect future builds while pre submit testing helps prevent an excess of problems during post submit testing.
the process also faces other challenges.
as noted in section these include developers listing too many or too few modules on change lists resulting in bottlenecks in testing or excessive faults being left undetected until post submit testing.
furthermore even when executing test suites on larger server farms or in the cloud the rapid pace at which code is changed and submitted for testing in either phase and the sheer numbers of changes being made by developers can lead to bottlenecks in either testing phase.
we now describe two approaches that we use to address these challenges involving regression test selection rts and test case prioritization tcp techniques.
.
continuous regression test selection a natural question that arises in the context of pre submit testing is whether rts techniques can be used to select a subset of the test suites related to the modules listed in a change list rendering that testing more cost effective without unduly harming the overall pre submit together with post submit testing process.
as noted in section a wide range of rts techniques have been developed and studied and could potentially be applied in this context.
in practice however existing techniques will not suffice.
google s codebase undergoes tens of changes per minute .
most existing rts techniques utilize instrumentation to track the code executed by test cases and then analyze code changes and relate them to these test executions.
the rate of code churn in the google codebase however is quite large and this can cause code instrumentation results to quickly become inaccurate.
in such situations keeping coverage data up to date is not feasible .
we therefore sought an alternative approach.
it has long been suggested in the testing literature that some test cases or test suites are inherently better than others at revealing failures .
in an evolving system test suites that have failed in a recent ver sion are in some ways proxies for code change they target code that is churning.
we conjectured that an rts approach that selects test suites based on some failure window might be cost effective in pre submit testing.
the foregoing approach also has the effect of ignoring test suites that have not in recent builds revealed faults.
we conjecture that this should have relatively minor deleterious effects initially but in time as test suites are repeatedly ignored the effectiveness of regression testing might be reduced.
thus a second issue to consider when selecting test suites for execution in pre submit testing involves the use of some execution window where test suites not executed within that window are also selected.
a third issue in regression testing relates not to existing test suites but to new test suites created to exercise new or modified system code or functionality.
such test suites are clear candidates for execution in pre submit testing on a modified system version.
the foregoing issues provide motivation for algorithm selectpretests.
to help explain the algorithm we provide figure .
in the figure each rectangle represents a set of test suites that have been submitted for execution.
the grey rectangle in the figure represents a set of test suites at the point at which they are about to be considered for execution.
rectangles to the right of the grey rectangle represent sets of test suites that have been executed in the past from most recently executed to least recently moving from left to right.
rectangles to the left of the grey rectangle represent sets of test suites that have been queued for future execution from most recently queued to least recently moving from left to right.
test suites queued for future execution are not considered in algorithm we include them in the figure because we consider them in our second algorithm discussed in section .
.
let wfbe a failure window and let webe an execution window as discussed above.
in practice wfandwecan indicate lengths of time or numbers of test suites executed in this work we rely on the former.
the figure depicts wfandweprojected over the sets of test suites that have been executed in the past.
lettbe the set of test suites for modules submitted by a developer or required by a developer at the point at which they are about to be considered again test suites represented by the grey rectangle in figure .
selectpretests selects from t for execution all test suites that have been observed to fail within window wf have not been executed within window we or are new.
note that in the case of algorithm failure and execution windows for test suites are assessed relative to both prior pre submit and prior post submit testing results.
algorithm selectpretests parameters test suites t failure window wf execution window we for all ti2tdo iftimesincelastfailure ti wfor timesincelastexecution ti w eor tiis new then t0 t0 ti end if end for return t0 .
continuous test suite prioritization algorithm selectpretests is meant to reduce testing effort in pre submit testing while preserving much of its effectiveness and to be executed suites execu.on history suites submi4ed for execu.on wp we wf figure test suite execution windows the extent to which it may do so needs to be studied empirically.
however selectpretests can have a secondary effect it can shift the execution of failing test suites from pre submit testing to postsubmit testing.
skipping passing test suites during pre submit testing does not cause an extra load during post submit testing as they will be executed then anyway.
test suites that would have failed during pre submit testing are skipped however can cause delays in failure finding that may slow down development.
thus we wished to find an approach by which to reduce such delays in post submit testing.
to do this we looked to test prioritization.
just as test suites selected for pre submit testing can be selected based on failure and execution windows so also the test suites that must be utilized in post submit testing can be prioritized based on those windows.
we conjecture that by using this approach test suites that reveal failures can be executed earlier than they otherwise would.
engineers can then use information on failures to determine whether to continue a system build or to return to the drawing board.
in the context of prioritization however there is one additional issue to consider.
prioritization techniques have traditionally operated on entire sets of test cases or test suites.
in continuous integration development environments test suites arrive for post submit testing continuously in relatively small batches or in bursts related to code submits and these test suites are placed in a dispatch queue.
in figure this queue is represented by all of the rectangles to the left of and including the grey rectangle .
as resources become available test infrastructure removes test suites from this queue and launches them.
assigning priorities to test suites or small batches of suites as they reach the dispatch queue may cause lower priority test suites to remain unexecuted for longer than desirable periods of time.
it may also cause test suites that have relatively similar behavior to all be assigned high priorities and run earlier than other test suites with dissimilar behavior lowering the effectiveness of prioritization.
finally prioritizing test suites as they arrive is likely to focus on test suites related to single submissions.
we conjecture that a vast majority of the test suites related to specific submissions do not fail since they were already validated by pre submit testing and grouping all of the test suites related to a single submission ahead of those related to later submissions may have the effect of queuing a number of likely to pass test suites ahead of more likely to fail test suites.
to address these issues as well as to gather data that might help us evaluate whether the foregoing effects can be significant in practice we rely on an additional form of window referred to as a prioritization window wp and illustrated in figure .
like weand wf wpcan either be a number of test suites or a time in this work we utilize time.
as test suites arrive they are placed in the dispatch queue and when wphas been exceeded we prioritize the test suites in the queue that have not yet been prioritized.
in this approach setting wpto if wpis a number of test suites or to atime window less than that required by any test suite if wpis an amount of time is equivalent to not prioritizing.
algorithm prioritizeposttests performs the foregoing actions.
prioritizeposttests is triggered when prioritization window wphas been met and operates on the set of test suites that has been submitted for post submit testing denoted here by postqueue .
recall that these test suites include test suites for modules submitted by developers to source code control along with test suites for all modules assessed to depend on or be depended on by the submitted modules.
again let wfbe a failure window and let webe an execution window.
let ppbe a pointer to the last test suite in the queue that was prioritized during the prior prioritization window.
algorithm prioritizeposttests parameters postqueue failure window wf execution window we starting point ppin postqueue for all ti2postqueue after ppto lastentry.postqueue do iftimesincelastfailure ti wfor timesincelastexecution ti w eor tiis new then ti.priority else ti.priority end if end for sort bypriority postqueue pp lastentry.postqueue pp lastentry.postqueue prioritizeposttests begins at location ppin postqueue.
the algorithm considers each test suite tiin the queue from the suite immediately after ppto the end of the queue and assigns a priority to each of these test suites.
priority assignment involves applying a function that seeks to capture the relationship between the test suite and weandwf.
there are various functions that might be utilized in our current implementation we use a simple and easy to compute function that assigns one of two priority levels high or low to test suites as follows.
for each test suite t that has been observed to fail within window wf has not been executed within window we or is new tis assigned priority level .
all other test suites are assigned priority level .
note that in the case of prioritizeposttests we consider failure and execution data relative only to prior executions in post submit testing for reasons discussed in section .
after all test suites have been assigned priority levels the algorithm sorts them such that higher priority test suites appear first in the queue beginning at point pp .
finally ppis updated to the last prioritized test suite in the queue.
note that as defined prioritizeposttests assigns test suites to priority levels in the order in which it extracts them from postqueue.
in theory a secondary prioritization criterion might be applied to the suites assigned to the two queues at additional costs.
for example traditional coverage based prioritization techniques often randomly sort test cases that are judged to have equivalent priorities other approaches apply secondary characteristics of tests.
in this work we employ no such secondary criterion thus the orders in which test cases are executed within each priority level are fully deterministic.
3to simplify the presentation we assume that access to the postqueue or queue section to be sorted is atomic.there are several factors that may affect the cost effectiveness of the foregoing approach.
we have already discussed a few issues related to the choice of wpthat suggest the use of a window size promoting the prioritization of batches of test suites rather than of single test suites.
given that choice however other issues arise.
first the rate at which test requests arrive and the expense of executing test suites play a role because together they determine the rate at which test suites accumulate and can be optimally prioritized.
second the resources available for test execution i.e.
machines on which to run test suites also play a role.
for example if the number of machines available for testing exceeds wp then partitioning wptest suites into high or low priority suites is not likely to have any effect on testing outcomes since all test suites may then be executed at once.
third the relationship between wp wfandwemay play a role if wpis too close to wforwe then prioritization may miss important information.
we explore several of these issues further in our empirical study the other factors merit further study as well.
.
empirical study we wish to evaluate the cost effectiveness of our approach and also to assess the effects that result from the use of different window sizes.
to do this we consider the two components of the approach rts and tcp independently this is reasonable because each of these components is used in a separate phase of testing and in a practical setting an organization might choose to employ either or both depending on cost effectiveness.
as baseline approaches we consider the case in which rts and tcp techniques are not applied.
this yields the following research questions.
rq1 how cost effective is the rts technique during pre submit testing and how does its cost effectiveness vary with different settings of wfandwe?
rq2 how cost effective is the tcp technique during post submit testing and how does its cost effectiveness vary with different settings of wp?
.
objects of analysis as objects of analysis we use the google shared dataset of test suite results gsdtsr described in section .
this dataset contains a sample of the results of test suites executed by google.
the dataset is provided as comma delimited file.
the dataset includes a summary of the data and a per field description .
.
variables and measures .
.
independent variables our independent variables involve the techniques and windows used.
for rq1 we employ three techniques the technique presented in section .
which selects a subset of the test suites in the change list for a module m a baseline approach in which all test suites in the change list are utilized and a random rts approach that selects a percentage of test suites matching that of the proposed approach to enable their comparison averaged over runs .
we utilize three execution window sizes we f1 48g and nine failure window sizes wf f0 96g each representing different numbers of hours.
for rq2 we employ two techniques the technique presented in section .
and a baseline approach that does not prioritize test suite execution.
we arbitrarily fix wf andwe the median values for those windows and we explore seven values for the prioritization window representing different numbers of hours wp f0 12g.
.
.
dependent variables as dependent variables for rq1 we measure the percentages of test suites that are selected the percentage of execution time required and the percentages of failures detected by our technique relative to the percentages required or detected by the baseline technique.
we do this for each combination of weandwf.
for rq2 we measure the time it takes for the test suites to exhibit a failure.
note that this measure differs from the apfd measure typically used in studies of test case prioritization.
apfd considers cumulative fault detection over time which is reasonable in a situation in which batches of test cases are being employed but not as applicable in continuous integration environments where the focus is on obtaining feedback on individual test suites.
.
study operation to study the proposed techniques we implemented the algorithms described in section using approximately lines of python.
we used the gsdtsr dataset to simulate a continuous testing environment.
the simulation performs a walk through the data file assuming that the test suites are executed by the computing infrastructure at the time for the duration and with the result reported in the dataset.
the selectpretests implementation utilizes the gsdtsr data a failure window size and an execution window size and reports the number of test suites selected the time required to execute those suites and the number of failures they detected.
it does this by reading each line in gsdtsr determining whether the test suite in the line would be executed given the failure and execution windows and updating the the latest failure and execution information for the test suite.
if the test suite is to be executed the implementation updates the test suite counter the test suite execution time accumulator with the time recorded in the line for that test suite and if the test suite resulted in a failure the failure accumulator.
prioritizeposttests takes the same inputs plus a prioritization window and produces a list of test suites prioritized within each specified window size.
our implementation of the technique operates similar to our implementation of selectpretests except for the manipulation of a moving prioritization window.
note that we apply regression test selection only to test suite executions related to pre submit testing and prioritization only to data related to test suite executions related to post submit testing see table for information on the sizes of those data sets .
.
threats to validity where external validity is concerned we have applied our techniques to an extensive dataset but that dataset represents testing processes conducted only in a small section of one industrial setting.
we have compared our techniques to baseline approaches in which no rts or tcp techniques are used and compared our rts technique to an approach using random test selection but we have not considered other alternative rts or tcp techniques from the literature although most would need significant changes to work under the continuous integration settings we are intending to operate .
we have utilized various window sizes but have necessarily limited our choices to a finite set of possible sizes.
we have not considered factors related to the availability of computing infrastructure such as variance in numbers of platforms available for use in testing.
these threats must be addressed through further study.
where internal validity is concerned faults in the tools used to simulate our techniques on the gsdtsr objects could cause problems in our results.
to guard against these we carefully tested our tools against small portions of the dataset on which results could be verified.
further we have not considered possible variations intesting results that may occur when test results are inconsistent see the discussion of flaky test suites in section such variations if present in large numbers could potentially alter our results.
where construct validity is concerned our measures include costs in terms of numbers of test suites executed and testing time and effectiveness in terms of numbers of failing test suites detected.
other factors such as whether the failure is new costs in engineer time and costs of delaying fault detection are not considered and may be relevant.
.
results and analysis we now analyze the results of our study relative to each of our research questions.
section provides further discussion.
.
.
rq1 regression test selection results of applying the continuous rts technique for the three wesizes are depicted in figures and .
each figure shows for each wf on the x axis the percentage of test suites selected their execution time and the failures detected which corresponds to the set of failing test suites selected compared with the baseline case in which all test suites are run.
across all three figures we observe similar trends.
the numbers of failing suites increase rapidly initially as wfincreases and then begin to level off at around wf .
at that point the percentage of failing test suites selected is at least three times greater than the percentage of test suites selected for execution and the percentage of test suite execution time.
the gains as measured by the detection of failing test suites reach between and when wf .
the other to of failing test suites were not selected for execution by our algorithm as they were executed recently but did not have a recent failure history.
random test suite selection with the same number of test suites selected for execution performed approximately six times worse in terms of failure detection.
this illustrates the ability of continuous rts to substantially reduce the testing load in pre submit testing at the cost of delaying the execution of a certain number but much smaller percentage relative to the costs saved of failing test suites to post submit testing.
as expected using larger values of weled to more aggressive test suite selection.
for example for a failure window of wf hours the percentage of selected test suites decreases by almost from we in figure to we in figure .
this is because as weincreases the number of non failing test suites that fall within that window and thus that are notselected increases.
note that for lower values of wf the percentage of test suites executed is greater than the percentage of time used for test execution but this changes as wfincreases.
for example for we for values of wfless than the percentage of test suites executed is smaller than the percentage of time used but above the percentage of time is greater.
the primary reason for this is that larger test suites tend to have greater failure detection rates than smaller test suites see table and as wfincreases greater numbers of larger test suites are selected.
overall skipping tests during pre submit testing ends up imposing only a small additional load on post submit testing.
assuming that test suite execution failures rates are .
as in our dataset selecting of the test suites the maximum in our simulation forwe andwf implies that the test executions added to post submit testing form less than .
of the pre submit testing load and even a smaller percentage of the post submit testing load.
however delaying the execution of test suites that will fail will cause an increase in the feedback latency to developers.
this is the challenge we address with prioritization within post submit testing in the next section.
w f test suites execu7on time failing test suites failing test suites with random selec7on figure test suite selection we w f test suites execu7on time failing test suites failing test suites with random selec7on figure test suite selection we w f test suites execu7on time failing test suites failing test suites with random selec7on figure test suite selection we time detected in hours failures w p 0h w p 1h w p 2h w p 4h w p 8h w p 12h figure sample of prioritization results for failures .
.
rq2 test case prioritization figure presents a small sample of the results of applying test suite prioritization with windows wf we andwp f0 12g.
the x axis corresponds to of the more than failing test suites and the y axis denotes the times at which these test suites were executed.
thus for example the first failure shown number is detected at hour of testing when test suites are prioritized with wp and at hour of testing when test suites are not prioritized wp .
although all instances of prioritization techniques appear to perform better than no prioritization this sample illustrates a fact evident across the entire data set namely that there is large variance in prioritization technique performance across window sizes.
for some failures this difference can be measured in tens of hours.
to understand the trends in the data given this high level of variation for each test suite execution resulting in a failure we computed the differences in detection time between each instantiation of the prioritization technique and no prioritization.
this is equivalent to the vertical distance in figure between the value of each technique and wp at each failure point for example for failure the difference in detection time when wp and wp 0is hours .
the results of this analysis are shown in figure .
we use a box plot to show the distribution of the differences.
in the figure a positive difference indicates that prioritization outperformed the no prioritization baseline.
the x axis corresponds to the different values of wp and for each of these we show the corresponding median thick black line within a box with a number next to it the first and third quartiles box bounds the min and max whiskers and the outliers black circles .
overall we observe that all instantiations of the prioritization technique perform better than no prioritization.
increasing wpled to a greater number of requested test suite executions marked as high priority ranging from to because larger windows0.
.
wpgained hours over no prioritization 183140figure boxplots comparing prioritization techniques with wp f0 12gagainst no prioritization meant more stale data.
the techniques with the best median performance however were also those that exhibited the greatest variation.
for example for the smallest window wp the median was hours better than the median for no prioritization but the second quartile begins at hours.
the largest window ofwp hours had a median of hours but the third quartile ended at hours.
the prioritization windows in between wp andwp result in smaller medians and variances.
forwp the median is the lowest at hours and the second and third quartiles end at and respectively.
we conjecture that given an overly small prioritization window the prioritization algorithm operates with only very recent data.
in some cases as when there are many instances of the same test suite awaiting execution in the pending queue this may be detrimental as the prediction signal may be overly strong.
on the other hand given too large a prioritization window the prioritization algorithm may not have enough recent test execution data to make good predictions about which tests suites should be given higher priority.
.
discussion in this section we summarize our findings and discuss several additional issues related to rts and tcp techniques that must be considered when operating in large and continuous integration development environments.
summary and implications.
our results show that our continuous rts technique can lower the testing load in pre submit testing.
in so it can delay the detection of faults but it does so at a far lower rate than its rate of savings in terms of testing execution cost.
since delayed test suites are eventually executed during post submit testing delayed faults are still detected later thus this process offers potential cost benefit tradeoffs that can be beneficial when pre submit testing is acting as a bottleneck.
our results also show that our continuous tcp technique can reduce delays in fault detection during post submit testing.
utilizing both techniques together during continuous integration then can improve the cost effectiveness of the continuous integration process overall.
our results also show that selections of window sizes we wf andwpaffect the effectiveness of our algorithms.
organizations will need to investigate the effects of choices of these parametersin their particular process settings in order to determine what values to use.
we also believe however that the parameters can be dynamically tuned based on data gathered while testing is ongoing a process that fits well with the notion of continuous integration.
further research is needed however to determine how such dynamic adjustments could be performed.
integration of approaches at google.
google who supported and partially sponsored this effort is investigating how to integrate the proposed techniques into their overall development workflow.
one challenge in that context is that in spite of using the same testing infrastructure different teams have different tolerances for delaying failure detection in pre submit testing or for changing the test suite execution order in post submit testing due to their specific testing and release practices.
the proposed approaches simplify the problem by assuming that a single set of parameters is sufficient but clearly more sophisticated levels of configuration are necessary.
reducing test execution time vs. delaying failure detection.
one of the impacts of utilizing continuous rts in pre submit testing is the potential delay in detecting failing test suites until postsubmit testing.
in practice this chance of delay may be compensated for by faster pre submit test execution however organizations may wish to assess the tradeoffs more quantitatively.
to obtain an initial notion about what this delay may involve we further analyzed our data.
note that the cost of the delay depends in part on the latency between pre submit and post submit testing.
for passing pre submit test suites the average latency between pre submit and post submit testing in the best case comparing only the last test suite execution in pre submit testing to the first in post submit testing is .
hours and in the worst case comparing only the last test suite execution in pre submit testing to the last in post submit testing is .
hours.
clearly this is just a coarse approximation of the costs of delays in fault detection and one that accounts only for time.
future studies could evaluate this cost more carefully perhaps by employing rts for only some of the systems under test while collecting not just failure result data but also failure detection latency.
specifying windows using time versus numbers of events.
as noted in section our rts and tcp techniques can be used with window sizes specified in terms of time or numbers of events where an event can be for example a test suite execution.
our experience indicates that specifying windows in terms of time is more intuitive for managers of test execution pipelines and this is one reason we chose to use time.
however using numbers of events to define window sizes would allow our techniques to adapt more readily to fluctuations in the inflow of test execution requests.
we conjecture that such adaptability may be desirable in some instances but not others.
for example setting an event based window may be beneficial if test suites fail friday afternoon when the inflow of test suites slows down such that the failing test suites should be considered high priority on monday morning even though there is a large window of time in between.
on the other hand when the inflow of test suites is high an event based window may cause test suites utilized just minutes but many hundreds of test suites ago to be discarded.
we leave the comparison of different specification mechanisms as future work.
adding pre submit test data to post submit prioritization.
in our study we used failure and execution data from previous postsubmit test suite executions to drive current post submit test suite prioritization.
one could also consider including failure and execution data from pre submit test executions to prioritize post submit testing.
we investigated this possibility on our data set and discov ered that the addition of pre submit test data did not improve prioritization.
for example when prioritizing with wf we andwp 1hours the median gain in fault detection time for prioritization with respect to no prioritization was with a variance of whether pre submit test data was included or not.
for the same setting but with wp the median gain in fault detection time when using just post submit test data was and the variance was and when adding pre submit test data the median was and the variance was .
we conjecture that the reason for this lack of additional effectiveness may involve the different roles that pre submit test data plays in development.
for example it is relatively common for developers to use pre submit testing to explore the robustness of a certain piece of code.
failure and execution data collected through such explorations can mislead the prioritization technique during post submit testing resulting in mixed effectiveness gains.
running on a large computing infrastructure.
when applied at large scales continuous integration processes often rely on large computing infrastructure to increase execution efficiency reducing the amount of time required to provide feedback on test execution to developers.
the rts and tcp techniques we propose however are not cognizant of the capabilities of or the variability that can be introduced by the underlying computing infrastructure.
this means that for example for a fixed wf a test suite that has previously failed may or may not be selected for execution depending on how quickly the infrastructure can schedule it to run.
the parameters provided by our techniques in terms of failure and executions windows can help to compensate for some of the variation in the computing infrastructure.
still more resourcecognizant techniques could identify ways to cluster and allocate test suites more effectively.
there is a clear a tension between designing techniques that are general enough to be applicable to many settings and specialized enough to leverage the available computational resources.
development organizations operating with such infrastructure must deal with such decisions frequently.
evolving test suites.
test suites are not stagnant they evolve with code.
our algorithms give new test suites higher value but they are not cognizant of whether a test suite is obsolete or has changed.
obsolete failing test suites are eventually removed or deprecated by developers so we estimate that their influence in the overall test suite selection or prioritization performance is going to be steady but rather small.
obsolete passing suites will also have a steady effect on the algorithms depending primarily on the value of we but they do not seem to provide a clear and safe signal for our approach to discard them automatically.
changed test suites however may offer a refinement opportunity for our approach.
considering that changes in test suites may be meant to validate new and potentially faulty program behavior and changes in suites may be faulty or at least inconsistent with the software under test it seems that the act of changing in itself may be a valuable signal for triggering test suite execution.
hence a natural step in the evolution of the proposed algorithms will be to consider the notion of a test suite change window .
dealing with non deterministic test suites.
another interesting side effect of continuously running large numbers of test suites is that there may be large numbers of test suites for which results are inconsistent these are often also referred to as flaky test suites .
flaky test suites may fail intermittently without any variations occurring in the module under test or in the test suite itself.
in some cases the cause for this lies in the test suite design due to failures to check assumptions or control environmental factors.
in othercases these variations may be so rare and difficult to control that it may not be cost effective for developers to address them.
for our rts and tcp techniques to be effective they must distinguish between true failures and flaky failures or risk the chance of actually magnifying the effect of flaky test suites by promoting their execution.
one common practice is to deflake failing test suites by rerunning them e.g.
and computing a flakiness index for each test suite in the process.
we anticipate that this index could be used as a threshold to determine whether a failing test suite should be considered as such in a wf.
.
conclusion just as software integration has become a continuous process so has the testing performed during that process.
integration and testing are increasingly intertwined as software moves closer to deployment.
this is why to be cost effective regression testing techniques must operate effectively within continuous integration development environments.
in this work we described the challenges associated with performing regression testing under continuous integration introduced two new regression testing techniques that use readily available test suite execution history data to determine what tests are worth executing and executing with higher priority and we shared a sanitized industry dataset with millions of test suite executions that we used to assess the techniques presented.
we anticipate that follow up work will include the refinement of rts techniques under continuous integration and testing particularly through the incorporation of other light weight sources of data.
for example we are exploring whether the transitions from a pass to a failure during test suite execution may indicate a new breakage instead of ongoing broken modules as this differentiation may trigger different follow up processes and toolsets.
similarly we would like to explore mechanisms for incorporating a test suite changed window and also for adjusting window sizes dynamically so that we can consider more of the potential performance factors we have identified.
we will also extend our simulation to use multiple test execution resources that better match the cloud setting used to obtain the dataset.
last we wish to extend our studies to other datasets and contexts to better understand the limitations of rts techniques as the scale of code and change increases.
.