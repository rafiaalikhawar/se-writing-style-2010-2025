deviance from perfection is a better criterion than closeness to evil when identifying risky code marouane kessentini st phane vaucher houari sahraoui diro universit de montr al canada kessentm vauchers sahraouh iro.umontreal.ca abstract we propose an approach for the automatic detection of potential design defects in code.
the detection is based on the notion that the more code deviates from good practices the more likely it is bad.
taking inspiration from artificial immune systems we generated a set of detectors that characterize different ways that a code can diverge from good practices.
we then used these detectors to measure how far code in assessed systems deviates from normality.
we evaluated our approach by finding potential defects in two open source systems xerces j and gantt .
we used the library jhotdraw as the code base representing good design programming practices.
in both systems we found that of the riskiest classes were defects a precision far superiour to state of the art rule based approaches.
categories and subject descriptors d. .
maintenance restructuring reverse engineering and reengineering general terms design keywords maintenance design defects artificial immune systems .
introduction in order to limit maintenance costs and improve the quality of their software systems companies try to both enforce good design development practices and prevent bad practices.
as a result these practices have been studied by professionals and researchers alike with a special attention given to design level problems.
there has been much research focusing on the study of bad design practices sometimes called defects antipatterns permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
ase september antwerp belgium.
copyright acm ... .
.
smells or anomalies in the literature1.
although these bad practices are sometimes unavoidable in most cases development teams should try to prevent them and remove them from their code base as early as possible.
hence many fully automated detection techniques have been proposed .
several problems limit the effectiveness of existing techniques.
indeed the vast majority of existing work relies on rule based detection .
different rules identify key symptoms that characterize a defect using combinations of mainly quantitative metrics structural and or lexical information.
therefore to identify and remove defects in a system all possible defects should be known and their symptoms characterized with rules.
moreover the rules must to be applied equally to any system in any context.
this is not reasonable considering the variety of software systems and the difficulty of expressing some types of symptoms.
these difficulties explain a large portion of the high false positive rates mentioned in existing research .
in this article we propose an automated detection approach that is completely different from the state of art.
instead of characterizing each symptom of each possible defect type we apply the principle of negative selection the process used by biological immune systems to identify antigens.
an immune system does not try to detect specific bacteria and viruses.
rather it starts by detecting what is abnormal i.e.
what is different from the healthy cells of the body.
the more something is different the more it is considered risky.
we apply the same principle to the detection of design defects by first defining what is normal.
normality is defined using a code base containing examples of well designed and implemented software elements.
then we create a set of detectors that represent different ways that a code can diverge from the good code.
finally elements of assessed systems that are similar to detectors are considered as risky.
to evaluate our approach we used classes from the jhotdraw library as our examples of well designed and implemented code.
two systems xerces j and gantt were then analyzed using our approach.
almost all the identified riskiest classes with levels were found in a list of classes tagged as defects blobs spaghetti code and functional decomposition in another project .
our contributions to automation are as follows.
first our technique is fully automatable from the creation of detectors to the evaluation of classes.
second our technique 1in the remainder of this article we use the generic term defect to refer to an occurrence of a bad practice in the code does not require an expert to write rules for every defect type and adapt them to different systems.
finally using only standard algorithms to measure similarity our technique not only outperforms rule based techniques in terms of precision but we are also able to find a good mix of defects types.
the major limitation of the approach is that we require a code base representing of good design practices.
our results indicate however that jhotdraw seems to be usable and could serve as a starting point for a company wishing to use our approach.. the remainder of this paper is structured as follows.
section is dedicated to the problem statement.
in section we describe the principles of the artificial immune system that inspires our approach and the adaptations of these principles to the detection of design defects.
section presents and discusses the validation results.
a summary of the related work in defect detection is given in section .
we conclude and suggest future research directions in section .
.
problem statement in this section we describe the problem of defect detection.
we start by defining important concepts.
then we detail the specific problems that are addressed by our approach.
.
basic concepts design defects also called design anomalies refer to design situations that adversely affect the development of a software.
in general they make a system difficult to change which may in turn introduce bugs.
different types of defects presenting a variety of symptoms have been studied with the intent of improving their detection and suggesting improvements paths.
the two following types of defects are commonly mentioned.
in beck defines sets of symptoms of common defects named code smells .
these include large classes feature envy long parameter lists and lazy classes.
each defect type is accompanied by some refactoring suggestions to remove them.
brown et al.
define another category of design defects named anti patterns which includes blob classes spaghetti code and cut paste programming.
in both books the authors focus on describing the symptoms to look for in order to identify specific defects.
regarding our detection approach we use the following concepts acode fragment represents a software element that is evaluated.
this could be a class method or package in an object oriented code.
although our approach could be applied to evaluate any of these entities in this paper we use code fragment to refer essentially to a class.
adesign risk is a code fragment that is dissimilar unusual from known good code.
it could be a design defect or simply an unusual design developpement practice.
the process of discovering design defects consists of finding high risk code fragments in the system without relying on specific knowledge on the known defect types.
.
problem statement any technique to detect design defect should address circumvent many difficulties inherent to the nature of defects.
here is the description of the most important difficulties and how they affect an automation process.
there is no exhaustive list of all possible types of design defects.
although there has been significant work to classify defect types programming practices paradigms and languages evolve making unrealistic to support the detection of all possible defect types.
furthermore there might be company or application specific bad design practices.
for those design defects that are documented there is no consensual definition of symptom detections .
defects are generally described using natural language and their detection relies on the interpretation of the developers.
this limits the automation of the detection.
the majority of detection methods do not provide an efficient manner to guide the manual inspection of the candidate list .
potential defects are generally not listed in an order that helps developers addressing in priority the most severe ones.
there is little work such as the one of khomh et al where probabilities are used to order the results.
.
ais based detection algorithm our approach is based on the metaphor of biological immune systems.
in this section we present the principles of this metaphor and our adaptation to the problem of detecting design defects.
.
principles of artificial immune systems the role of a biological immune system is is to protect its host organism against foreign elements such as pathogens e.g.
bacteria and viruses and or malfunctioning cells e.g.
cancerous cells .
this is performed following three phases discovery identification and elimination of foreign elements.
discovery is the phase that interests us in particular for our work.
therefore we explain its principle in the following paragraphs.
there is no central organ that fully controls the is.
instead detectors wander in the body searching for harmful elements.
any element that can be recognised by the immune system is called an antigen .
the cells that originally belong to our body and are harmless to its functioning are termed self for self antigens while the disease causing elements are named nonself for nonself antigens .
the is classifies cells that are present in the body as self and nonself cells.
the immune system produces a large number of randomly created detectors.
a negative selection mechanism eliminates detectors that match cells present in a protected environment bone marrow and the thymus where only self cells are assumed to be present.
non eliminated ones become naive detectors they die after some time unless they match an element assumed to be a pathogen.
detectors that do match a pathogen are quickly cloned this is used to accelerate the response to future attacks.
since the clones are not exact replicates they are mutated this provides a more 114focused response to pathogens.
this process called affinity maturation provides an efficient adaptation to a changing non self environment.
a detailed presentation of the biological immune system can be found in books such as .
the success of immune systems at keeping a living organism healthy inspired the emergence of artificial immune systems ais as a generic solution to problems in several domains such as scheduling computer security optimization or robotics .
ais can be adapted to the problem of defect detection.
the following mappings shows the similarity between our problem and the ais concepts.
body the evaluated system more precisely its code detector an artificial code fragment that is very different from a well designed code base self cells well designed code fragments in the system to evaluate without design defects non self cells code fragments in the system to evaluate that present a risk of being design defects affinity the similarity between detectors and code fragments to evaluate.
.
approach overview figure gives an overview of our approach.
the detection process has two main steps detector generation and risk estimation.
detectors are generated from a collection of code fragments coming from one or more well designed systems.
these code fragments define the reference of what is considered normal code.
the generation process of detectors is performed using a heuristic search that maximizes on one hand the distance between detectors and normal code and on the other hand the distance between the detectors themselves.
the same set of detectors could be used to evaluate many systems and it could be updated as the normal code base or development practices evolves.
the second step of the detection process consists of comparing the code to evaluate to the detectors.
a code fragment that exhibits a similarity with a detector is considered as a risky element.
the higher the similarity the more a code fragment is considered risky.
both the detector generation and risk estimation steps use similarity scores.
before detailing the two steps we first describe the similarity functions used in this work.
figure approach overview .
similarity between code fragments to calculate the similarity between two code fragments we adapted the needleman wunsch alignment algorithmto our context.
it is a dynamic programing algorithm used in bioinformatics to efficiently find similar regions between two sequences of dna rna or protein .
an example of the algorithm is presented in figure .
figure global alignment of two strings as we are manipulating code elements and not sequences strings we represent these elements by sets of predicates.
each predicate type corresponds to a construct type of an object oriented system class c attribute a method m parameter p generalization g and method invocation relationship between classes r .
for example in figure the sequence of predicates cgaamppm corresponds to a class with a generalization link containing two attributes and two methods.
the first method has two parameters.
figure encoding predicates include details about the associated constructs visibility types etc.
.
these details thereafter called parameters determine ways a code fragment can deviate from a notion of normality.
the example of figure is a representation of class rangeexceptionimpl from xerces j. the corresponding predicate set extracted using our inhouse eclipse plugin is as follows class rangeexceptionimpl public generalisation rangeexceptionimpl rangeexception attribute rangeexceptionimpl serialversionuid long static attribute rangeexceptionimpl serialimpluid short static method rangeexceptionimpl rangeexceptionimpl void y public parameter rangeexceptionimpl rangeexceptionimpl code short parameter rangeexceptionimpl rangeexceptionimpl message string method rangeexceptionimpl implserial void y private as described below the needleman wunsch global alignment algorithm is described recursively.
.
when aligning two sequences a1 ... an and b1 ... bm .
each position si jin the matrix corresponds to the best score of alignment considering the previously aligned elements of the sequences.
the algorithm can introduce gaps represented by to improve the matching of subsequences.
115si j max8 si j g insert gap for bj si j g insert gap for ai si j sim i j match where si g iands0 j g j at any given point algorithm considers two possibilities.
first it considers the case when a gap should be inserted.
when a gap is inserted for either aorb the algorithm applies a penalty of g. second it tries to match predicates.
the similarity function sim i jreturns the reward or cost of matching aitobj.
the final similarity is contained in sn m. our adaptation of the algorithm is straightforward.
we define the gap penalty gand the similary function to match individual predicates sim .
we do not seek perfect matches in terms of number of predicates.
a class with is not necessarily different from one with methods if the methods are similar.
to eliminate the sensitivity of the algorithm to size we thus set the gap penalty to .
we define a predicate specific function to measure the similarity.
first if the types differ the similarity is .
as we manipulate sequences of complex predicates and not strings sim i jis defined as a predicate matching function pm ij.
pm ij measures the similarity in terms of the elements of the predicates associated to aiandbj.
this similarity is the ratio of common parameters in both predicates.
pm ij p ai q bi p q max ai bj where aiandbjare treated as sets of predicates.
the equivalence between predicate parameters depends on each type of parameter.
for visibility and element types it means equality.
specific names are not considered.
instead they are used to indicate a common reference by other predicates.
for example if a class defines an attribute and its related getter method.
they will both share the same class name.
to illustrate an example for the local alignment algorithm let us consider the class rangeexceptionimpl described previously as a code fragment c32andoptions c152as a second code fragment to compare with c32.
the code fragments are sequentially numbered.
c152is defined as follows class options public method options isfractionalmetrics boolean n public method options istextantialiased boolean n private parameter options istextantialiased id string relation abstractfigure getfontrendercontext isfractionalmetrics options n according to the coding mentioned previously the predicate sequence for c32is cgaamppm and one of c152is cmmpr.
the alignment algorithm finds the best alignment sequence as shown in figure .
there are three matched predicates between c32andc152 one class one method and one method parameter.
if we consider the second matched predicates p15 method rangeexceptionimpl rangeexceptionimpl void y public from c32andp22 method options isfractionalmetrics boolean n public from c152.
the predicates have two common parameters out of a possible five.
the resulting similarity is consequently .
we normalize this absolute similarity figure best alignment sequence between c32and c152 measure sn m by the maximum number of predicates to produce our similarity measure sim a b sn m max n m .
detectors generation this section describes how a set of detectors is produced starting from the reference code.
the generation inspired by the work of gonzalez and dasgupta follows a genetic algorithm .
the idea is to produce a set of detectors that best covers the possible deviations from the reference code.
as the set of possible deviations is very large its coverage may require a huge number of detectors which is infeasible in practice.
for example pure random generation was shown to be infeasible in for performance reasons.
we therefore consider the detector generation as a search problem.
a generation algorithm should seek to optimize the following two objectives maximize the generality of the detector to cover the non self by minimizing the similarity with the self minimize the overlap similarity between detectors.
these two objectives define the cost function that evaluates the quality of a solution and then guides the search.
the cost of a solution d set of detectors is evaluated as the average costs of the included detectors.
we derive the cost of a detector dias a weighted average between the scores of respectively the lack of generality and the overlap.
formally cost di lg di o di here we give equal weight to both scores.
the lack of generality is measured by a matching score lg di between the predicate sequence of a detector diand those of all the classes sjin the reference code call it s .
it is defined as the average value of the alignment scores sim di sj between di and classes sjins.
formally lgdi p sj ssim di sj s similarly the overlap oi is measured by the average value of the individual sim di dj between the detector diand all the other detectors djin the solution d. formally 116odi p dj j negationslash isim di dj d the cost function defined above is used in our geneticbased search algorithm.
genetic algorithms ga implement the principle of natural selection .
roughly speaking a ga is an iterative procedure that generates a population of individuals from the previous generation using two operators crossover and mutation.
individuals having a high fitness have higher chances to reproduce themselves by crossover which improves the global quality of the population.
to avoid falling in local optima mutation is used to randomly change individuals.
individuals are represented by chromosomes containing a set of genes.
for the particular case of detector generation we reuse the predicate sequences as chromosomes.
each predicate represents a gene.
we start by randomly generating an initial population of detectors.
the size of this population is a parameter that will be discussed later in section .
this size is maintained constant during the evolution.
the fitness of each detector is evaluated by the inverse function of cost.
the fitness determines the probability of being selected for the crossover.
this process is called a wheel selection strategy .
in fact for each crossover two detectors are selected by applying twice the wheel selection.
even though detector are selected the crossover happens only with a certain probability.
the crossover operator allows to create two offspring o1 ando2from the two selected parents p1andp2.
it is defined as follows a random position k is selected in the predicate sequences.
the first kelements of p1become the first kelements ofo1.
similarly he first kelements of p2become the firstkelements of o2.
the remaining elements of respectively p1andp2are added as second parts of respectively o2ando1.
for instance if k and p1 cammppp and p2 cmprmpp then o1 camrmpp and o2 cmpmppp.
the mutation operator operator consists of randomly changing a predicate.
.
risk estimation the second step of our defect discovery is the assessment of risk for the different code fragments evaluated.
these are also represented by predicate sequences.
each sequence is compared using the alignment algorithm to the detectors obtained in the previous step.
the risk of being a defect associated to a code fragment eiis defined as the average value of the alignment scores sim ei dj obtained by comparing eito respectively all the detectors of a set d. formally risk ei p dj dsim l ei dj d the code fragments can then be ranked according to their risks to be inspected by the maintainers.
.
evaluation to test our approach we studied its usefulness to guide quality assurance efforts on two open source programs.
in this section we describe our experimental setup and present the results of an exploratory study.
.
goals and objectives the goal of the study is to evaluate the efficiency of our ais approach for the discovery of design defects from the perspective of a software maintainer conducting a quality audit.
we present the results of the experiment aimed at answering the following research questions rq1 to what extent can the proposed approach discover design defects?
rq2 what types of defects does it locate?
to answer rq1 we used an existing corpus of known design defects to evaluate the precision of our approach.
we ranked classes in order of decreasing risk and compared results to produced by a rule based strategy .
to answer rq2 we investigated the type of defects that were found.
.
system studied we used three open source java projects to perform our experiments ganttproject v1.
.
xerces v2.
.
and jhotdraw v7.
.
table summarizes facts on these programs.
ganttproject2is a tool for creating project schedules by means of gantt charts and resource load charts.
ganttproject enables breaking down projects into tasks and establishing dependencies between these tasks.
xerces3is a family of software packages for parsing and manipulating xml.
it implements a number of standard apis for xml parsing.
jhotdraw v7.14is a framework used to build graphic editors.
it was first built as an example of the use of design patterns.
jhotdraw was chosen because it contains very few known design defects.
in fact previous work could not find any blob defects.
in our experiments we used all of the classes in jhotdraw as our example set of good code.
we chose the xerces and gantt libraries because they are medium sized open source projects and were analysed in related work.
the version of gantt studied was known to be of poor quality which lead to a new major version.
xercesj on the other hand has been actively developed over the past years and its design has not been responsible for a slowdown of its development.
in moha et al.
asked three groups of students to analyse the libraries to tag instances of specific antipatterns to validate their detection technique decor.
for replication purposes they provided5a corpus of describing instances of different antipatters including blob classes spaghetti code and functional decompositions .
blobs are classes that do or know too much.
spaghetti code sc is code that does not use appropriate structuring mechanisms.
functional decomposition fd is code that is structured as a series of function calls.
these represent different types of design risks.
in our study we verified the capacity of our 117approach to locate classes that corresponded to instances of these antipatterns.
systems sharpclasses sharppredicates kloc ganttprojectv1.
.
xerces v2.
.
jhotdraw v7.
table program statistics .
experimental setting for our experiment we randomly generated detectors for jhotdraw about a quarter of the number of examples with a maximum size of characters.
the same set of detectors was used on both xerces and gantt.
the obtained results6were compared to those of decor a state of the art rule based detection technique.
for every antipattern in xerces and gantt they published the number of antipatterns detected the number of true positives and the precision ratio of true positives over the number detected .
our comparison is consequently done using precision.
we would have liked to consider recall but they did not publish clean complete data describing all existing antipatterns.
we therefore could not perform systematic comparisons of the recall of our approach.
instead we discuss the proportion of known antipatterns detected.
.
results tables and summarize our findings.
each class is presented with its risk its size and the associated defect types.
we only presented classes with a risk level of this corresponds to about of the classes in the system.
for gantt our precision over the top classes is with the eight riskiest classes being true positives.
decor on the other hand has a combined precision of for its detection on the same set of antipatterns.
for xerces our precision is of with the top classes correctly identified as defects.
for the same dataset decor had a precision of .
in the context of this experiment we can conclude that our technique is able to accurately identify design anomalies more accurately than decor rq1 .
we noticed that our technique does not have a bias towards the detection of specific anomaly types.
in xerces we had an almost equal distibution of each antipattern scs blobs and fds .
on gantt the distribution is not as balanced.
this is principally due to the number of actual antipatterns in the system.
we found all four known blobs and all scs in the system.
we found fds two more than decor.
having a relatively good distribution of antipatterns is useful for a quality engineer as he can focus on the notion of riskiest classes regardless of the type.
furthermore since the results are ranked he can efficiently use his time unlike decor.
this ability to identify different types of antipatterns underlines a key strength to our approach the similarity function is able to abstract out the importance of size.
most other tools and techniques rely heavily on the notion of size to detect defects.
this is reasonable considering that some antipatterns like the blob are associated to a notion of size.
ase2010 class risk s.c. blob f.d.
ganttoptions .
check gantttree .
check gregoriantimeunitstack .
check ganttdialogperson .
check csvsettingspanel .
check ganttproject .
check check gantttaskpropertiesbean .
check newprojectwizard .
check timeunitgraph .
resourceloadgraphicarea .
check check ganttcsvexport .
check ganttgraphicarea .
check check findpossibledependeesalgo... .
check ganttxfigsaver .
check ganttapplet .
check graphicprimitivecontainer .
check shape .
ganttxmlsaver .
check recalculatetaskcompletion... .
check taskhierarchymanagerimpl .
check precision table results for gantt for antipatterns like fds however the notion of size is irrelevant and this makes this type of anomaly hard to detect using structural information.
this difficulty is why decor includes an analysis of naming conventions to perform its detection.
using naming convention means that their results depend on the coding practices of a development team.
our results are however comparable to theirs while we do not leverage lexical information.
.
discussion in this section we discuss different issues concerning the detection of design risks.
number of detectors.
an important factor to our detection technique is the number of detectors generated.
in figure we present the precision of our approach when varying the number of detectors nd with nd .
the figure shows that the performance of our approach improves as we consider more detectors.
when we use detectors of the total number of cases in jhotdraw our performance is over for both systems.
our technique requires the comparison of every class to every detector this improved performance is at a negligible cost in terms of execution time.
indeed the execution time for applying the detection on each system varies between minutes for detectors and minutes for .
variability in detector generation.
another issue is our selection of interesting detectors.
the detection results might vary depending on the detectors which are generated randomly though guided by a meta heuristic .
to ensure that our results are relatively stable we compared the results of multiple executions for detector generation.
when we consider results up to of risk we observed an average precision of for gantt and for xerces.
furthermore we found that the majority of defects detected are found in every execution and respectively for gantt and xerces .
these unanimously detected defects were systematically the riskiest classes in every execution in gantt the top classes were common to all executions.
.
.
.
.
.
.
classes inspectedprecisionl l l l l l l l llll l l l l l l l l l ll ll l l l l llllllllll l l l l l l l l ll l l l l l l l l l l l l l l l l l l l l l l l l l l detectors a gantt .
.
.
.
.
.
classes inspectedprecisionl l l l l l l l llllllllllllllllllllllllllllllllllllllllll l l l l l l l l lllllllllllllllllllllllllllllllllllllllll l l l l l l l l llllllllllllllllllllllllllllllllllllllll l l l detectors b xerces figure effect of the number of detectors on detection precision vs. classes inspected in xerces there was only one non unanimous class in the top ten classes returned which was a false positive.
the average rank for a class detected by a single execution was and for gantt and xerces respectively.
we consequently believe that since the variability comes from the least risky classes and that our technique is stable.
metric based detection vs. similarity based detection.
our approach is significantly different from existing work that are rule based.
a key problem with these approaches is that these rules simplify the different notions that are useful for the detection of certain antipatterns.
in particular to detect blobs the notion of size is important.
most size metrics are highly correlated with one another and the best measure of size can depend on the system itself.
our use of predicates allows for complex structures to be detected.
for example we correctly detected taskhierarchymanagerimpl in gantt.
it holds a reference to the root of the hierarchy and controls creations of new children to the root.
public class taskhierarchymanagerimpl private taskhierarchyitem myrootitem new taskhierarchyitem null null public taskhierarchyitem getrootitem return myrootitem public taskhierarchyitem createitem task task taskhierarchyitem result new taskhierarchyitem task myrootitem return result public class taskmanagerimpl implements taskmanager ... private final taskhierarchymanagerimpl myhierarchymanager new taskhierarchymanagerimpl public taskhierarchymanagerimpl gethierarchymanager return myhierarchymanager ... it is detected for three reasons.
first it declares one attribute type taskhierarchyitem on which it never invokes any methods.
second it is used in a similar manner by taskmanagerimpl.
finally apart from creating objects it never uses any methods.
it is consequently a datastructure.these types of relationships are hard to detect using metrics.
on the other hand our technique produced a detector that was almost a complete match except the final parameter attribute x aaaa aa n private myrootitem attribute x aa x n private myhierarchymanager class x n n public taskhierarchymanagerimpl method x z x y n n public createitem method x zzzz aa n n n public getrootitem method x zxzzz x n n n public gethierarchymanager parameter x z zuwe gsfg declaration task parameter x z xuqye fzfgg local result parameter x zzzz jdajg gffgs declaration match error decor also successfully identified this class.
however it did so not because of metrics but because the name of the class contains the term manager .
building an example data set.
the reliability of the proposed approach requires an example set of good code.
it can be argued that constituting such a set might require more work than identifying and adapting rules.
in our study we showed that by using jhotdraw directly without any adaptation the technique can be used out of the box and this will produce good detection results for the detection of antipatterns for the two systems studied.
the performance of this detection in terms of precision was superiour to that of decor.
in an industrial setting we could expect a company to start with jhotdraw and gradually migrate its set of good code examples to include context specific data.
this might be essential if we consider that different languages and software infrastructures have different best worst practices.
.
related work several studies have recently focused on detecting design defects in software using different techniques.
these techniques range from fully automatic detection to guided manual inspection.
the related work can be classified into three broad categories metric based detection detection of refactoring opportunities visual based detection.
119class risk s.c. blob f.d.
dfacontentmodel .
check xsfacets .
check xmlserializer .
check xmlversiondetector .
check xml11entityscanner .
check xsdhandler .
check check token .
check xmlentitymanager .
check xsdabstracttraverser .
check xml11dtdvalidator .
check domnormalizer .
check xmlnsdtdvalidator .
check parserconfigurationsettings .
check saxparser .
check dtdgrammar .
check xml11nonvalidatingconfiguration .
check xmldtdvalidator .
check xmlentityscanner .
check xsattributegroupdecl .
check abstractdomparser .
check schemadom .
check xml11dtdconfiguration .
check xsdattributetraverser .
check objectfactory .
check xincludehandler .
check xsdfacm .
check nonvalidatingconfiguration .
check xmlschemavalidator .
check dtdconfiguration .
check coredocumentimpl .
check check xsattributechecker .
check cmnodefactory .
regexparser .
check timedv .
xml11configuration .
check xmlfilterimpl .
domserializerimpl .
check xsfacets .
check basemarkupserializer .
elementschemepointer .
check xmlparser .
check xpathmatcher .
check precision table results for xerces in first category marinescu defined a list of rules relying on metrics to detect what he calls design flaws of oo design at method class and subsystem levels.
erni et al.
use metrics to evaluate frameworks with the goal of improving them.
they introduce the concept of multi metrics as an n tuple of metrics expressing a quality criterion e.g.
modularity .
the main limitation of the two previous contribution is the difficulty to define threshold values for metrics in the rules.
to circumvent this problem alikacem et al.
express defect detection as fuzzy rules with fuzzy label for metrics e.g.
small medium large .
when evaluating the rules actual metric value are mapped to truth value for the labels by means of membership functions.
although no thresholds have to be defined still it is not obvious to decide for membership functions.
the previous approaches start from the hypothesis that all defect symptoms could be expressed in terms of metrics.
actually many defects involve notions that could not quantified.
this observation was the foudation of the work of moha et al.
.
in their approach named decor they start by describing defect symptoms using an abstract rule language.
these descriptions involve different notions such as class roles and structures.
the descriptions are later mapped to detection algorithms.
in addition to the threshold problem this approach uses heuristics to approximate some notions with results in an important rate of false positives.
another limitation of decor is that all the detected defect candidate are listed without any rank that help the maintainers checking addressing in priority the most severe ones.
khomh et al.
extended decor to support uncertainty and to sort the defect candidates accordingly.
uncertainty is managed by bayesian belief networks that implement the detection rules of decor.
the detection outputs are probabilities that a class is an occurrence of a defect type.
in our approach all the above mentioned problems related to the use of rules and metrics do not arise.
indeed the symptoms are not explicitly used which reduces the adaptation calibration effort.
in the second category of work defects are not detected explicitly.
they are implicitly because the approaches refactor a system by detecting elements to change to improve he global quality.
for example in defect detection is considered as an optimization problem.
the authors use a combination of metrics to measure the improvements achieved when sequences of simple refactorings are applied such as moving methods between classes.
the goal of the optimization is to determine the sequence that maximize a function which captures the variations of a set of metrics .
the fact that the quality in terms of metrics is improved does not necessary means that the changes make sense.
the link between defect and correction is not obvious which make the inspection difficult for the maintainers.
in our case we separate the detection and correction phase.
the high rate of false positives generated by the automatic approaches encouraged other teams to explore semiautomatic solutions.
these solutions took the form of visualization based environments.
the primary goal is to take advantage of the human ability to integrate complex contextual information in the detection.
kothari et al.
present a pattern based framework for developing tool support to detect software anomalies by representing potentials defects with different colors.
later dhambri et al.
propose a visualization based approach to detect design anomalies by automatically detecting some symptoms and letting others to the human analyst.
the visualization metaphor was chosen specifically to reduce the complexity of dealing with a large amount of data.
still the visualization approach is not obvious when evaluating large scale systems.
moreover the information visualized is for the most part metric based meaning that complex relationships can still be difficult to detect.
in our case the human intervention is needed for the inspection of the candidate only.
this inspection is made easier because the candidates are ranked by risk and also because by analysing the most similar detectors it is possible to identify what part of the element was problematic.
the work that is closest to ours is by catal and diri .
the authors use a machine learning version of ais called an artificial immune recognition system airs to learn a prediction model for defect prone modules.
the airs used was a generic package implemented in the machine learning package weka.
this package cannot handle complex structures like predicates and does not implement the negative selection algorithm.
.
conclusion in this article we presented a new approach to problem of detecting design defects.
typically researchers and practitioners try to characterize different types of common design defects and present symptoms to use in order to locate them in a system.
in our work we show that we do not need this knowledge to perform a detection.
instead all we need is a clear notion of what is good.
what significantly diverges is often a defect.
interestingly enough our study shows that our technique outperforms an decor a state of the art rule based approach on its test corpus.
by ignoring the detection of specific defect types we avoid problems with existing detection techniques.
first the detection of most defect is difficult to automate because their definitions are expressed informally.
second even with a precise definition some symptoms are context specific and might or not be useful for a given system.
there is consequently a non negligeable effort to test and adapt a detection process to another system.
finally by presenting all defects regardless of types in order of risk a development team can focus on the most urgent problems first.
this technique was tested on two open source systems and the results were promising.
the discovery process uncovered different types of design defects was more efficiently than decor.
in fact for gantt our precision is with the eight riskiest classes being true positives.
decor on the other hand has a combined precision of for its detection of the same set of antipatterns.
for xerces our precision is of with the top classes correctly identified as defects.
for the same dataset decor had a precision of .
furthermore as decor needed an expert to define rules our results were achieved without any expert knowledge relying only on the good structure of jhotdraw to guide the detection process.
in this work we only looked at the first step of an immune systems the discovery of risk.
as part of our future work we plan to explore the other two steps identification and correction of detected design defects refactoring .
furthermore we need to extend our reference code base with other well designed code in order to take into consideration different programming contexts.
specifically we plan on adapting the ais metaphor to identify discovered defects using immune memory and danger theory .
adapting the colonal selection algorithm to find the best immune response that correspond the optimal refactorings sequence to apply.
using our approach for defect prediction using the estimation risk score.
acknowledgment this work has been partly funded by the natural sciences and engineering research council of canada nserc and the tunisian ministry of higher education and scientific research.
.