automated oracles an empirical study on cost and effectiveness cu d. nguyen alessandro marchetto and paolo tonella fondazione bruno kessler trento italy cunduy marchetto tonella fbk.eu abstract software testing is an e ective yet expensive method to improve software quality.
test automation a potential way to reduce testing cost has received enormous research attention recently but the so called oracle problem how to decide the pass fail outcome of a test execution is still a major obstacle to such cost reduction.
we have extensively investigated state of the art works that contribute to address this problem from areas such as speci cation mining and model inference.
in this paper we compare three types of automated oracles data invariants temporal invariants and finite state automata .
more speci cally we study the training cost and the false positive rate we evaluate also their fault detection capability.
seven medium to large industrial application subjects and real faults have been used in our empirical investigation.
categories and subject descriptors d. d. .
general terms experimentation reliability keywords speci cation mining automated testing oracles empirical study .
introduction nowadays the internet is connecting devices systems and people at an unprecedented speed.
software systems have to evolve at a similar pace to deal with new use cases new categories of users and business changes.
taking some of today s social network systems as examples we can see that new features are being released in a matter of weeks.
new user interfaces and new supported devices are also introduced permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse august saint petersburg russia copyright acm ... .
.continuously.
software testing is involved in every change hence its cost is increasingly high.
automated techniques that are able to generate e ective test cases and judge the test outcome for software testing are needed to deal with the testing cost.
the automated oracle problem how to judge the behaviors of a system under test and how to decide the pass fail outcome of a test execution is still a major obstacle to such cost reduction.
in the literature there has been a large amount of work investigating software speci cation mining .
mined speci cations can be used on one hand in software maintenance.
they help understand software systems including legacy systems when speci cation documents are missing or outdated.
on the other hand mined speci cations can be used to nd software faults by acting as guards or criteria to evaluate behaviors of a system under test sut .
in other words they are software oracles which are automatically learned or mined.
we name them automated oracles .
the principal idea underpinning automated oracles is that when a sut is running its execution traces are collected.
from such traces which are assumed to be non faulty oracles of di erent types are learned and as long as the sut operates properly the learned oracles can be continuously re ned and updated with new incoming traces.
these oracles report problems if any new trace violates them.
automated oracles are particularly useful in regression testing where good oracles that are learned from a version can be used to check the execution traces of the next new version.
they are also useful in systems that continuously evolve or systems that can exhibit new behaviors at runtime.
in these systems automated oracles will be learned and used for checking continuously.
we have investigated the literature including the main approaches that can be used as automated oracles and we have selected three types or oracles based on their characteristics popularity and supporting tools finite state automata fsa data invariants and temporal invariants .
in this paper we extensively evaluate these oracles in terms of cost and e ectiveness in an industrial context where they are applied to medium and large subject applications.
the key contributions of the paper are twofold this is the rst independent large scale study on the cost bene ts involved in oracle learning in terms of false positive rate fault detection and needed resources conducted on real world systems.
to the best of our knowledge the size of the analyzed traces more than events on average exceeds by a large amount the trace size considered in all existing empirical studies in the area of speci cation mining.permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august saint petersburg russia copyright acm ... .
the empirical study provided us with valuable insight about the practical applications of automated oracles speci cally the trade o between cost and capability to detect real faults.
the remainder of the paper is organized as follows.
section gives a pragmatic background on the selected oracle types and introduces related work.
section discusses in detailed the design of our study and the selected subject applications.
section and show the results that we obtained and discuss them.
we discuss the relevant threats to the validity of our study in section .
finally section concludes the paper.
.
background and related work .
a pragmatic background di erent types of automated oracles have been proposed finite state automata fsa which model the behaviors of a sut by means of nite state machines data invariants which pose constraints on software variables at speci c program locations and temporal invariants which guard the occurrence of events during the execution of the sut according to speci c patterns.
all these types of oracle have received substantial research attention and supporting tools are available to infer and exploit such automated oracles while in their original proposals some of the automated oracle inference tools that we consider have not been evaluated empirically for their support to automated fault anomaly detection in this work we focus on their e ectiveness when used as automated oracles for seven real world subject applications.
we have selected klfa1 for fsa oracles daikon2 for data invariants as oracles and synoptic3 for temporal invariants as oracles.
these are quite popular and active tools to date.
in the case of synoptic we use only the temporal invariant inference module although its main feature is the generation of automata which are however supposed to be used mainly for software understanding.
to illustrate the functionality of those tools we introduce some examples of traces of a simple shopping cart system simplecart and show how automated oracles can be inferred from them.
simplecart is a simple shopping system that sells a single subject its user can add items to a cart remove items from it and check out when she is happy with the number of items.
table shows traces obtained from the execution of simplecart.
the rst column shows the events that were red the second column shows the values of the global variable itemincart that was changed after each event.
the rst trace contains events hadd add remove checkout i the second trace contains events hadd add add remove remove add checkout i and the last trace contains events hadd remove add checkout i the value of variable itemincart changes as the consequence of some events when add occurs itemincart is increased by when remove occurs itemincart is decreased by .
from the traces in table klfa infers the nite state machine shown in figure .
this automaton can be used as example of traces of simplecart n itemincart trace event n add add remove checkout 1trace event n add add add remove remove add checkout 2trace event n add remove add checkout table examples of inferred temporal invariants produced by synoptic alwaysfollowedby !
initial!add initial!checkout initial!remove add!checkout remove!checkout alwaysprecedes add checkout add remove remove checkoutneverfollowedby checkout 9add checkout 9checkout checkout 9remove a software oracle to check if future traces are recognized aka accepted by the automaton otherwise the system exhibits an anomalous behavior either the manifestation of a fault or a false positive of the automated oracle.
s0s1s3s4s2s5s6addaddaddaddaddcheckoutcheckoutcheckoutcheckoutremoveremoveremove figure an example of inferred automata produced by klfa synoptic considers currently three types of temporal invariants alwaysprecedes alwaysfollowedby and neverfollowedby .
they capture the occurrence relationship between pairs of events.
in our running example with the same input traces synoptic infers temporal invariants as shown in table .
daikon infers invariants at program locations of interest.
they are often before and after method invocations and can be used as pre post conditions.
for instance for the event add actually the corresponding method addin the source code daikon infers that itemincart is always greater than zero itemincart after method invocation.
it infers the same invariant before the invocation of checkout .
the invariants produced by synoptic and daikon can be used as oracles to check the occurrence of events or the value of variables at runtime.
any violation of such invariants will raise an alarm for investigation.
as with klfa an invariant violation may point to a software fault or may be a false positive of the automated oracle.137as a concrete example assuming a fault in simplecart that allows removing items even when the cart is empty the following new trace may be produced trace that exhibits the fault event itemincart remove remove add checkout all three types of oracles can detect this fault the fsa produced by klfa in figure rejects this trace meaning that klfa detects the fault the temporal invariant add remove produced by synoptic is violated by this trace because remove cannot occur before add thus synoptic detects the fault nally the negative values of itemincart also violate the invariants produced by daikon hence it can also detect the fault.
.
related work there is a large body of work on dynamic model inference and speci cation mining .
the approaches described in these papers have been shown to be able to e ectively detect software faults and anomalies.
however the empirical validation reported in these papers was conducted by the authors themselves usually on small scale benchmarks that were often known to exhibit some of the problems speci cally addressed by the proposed technique.
our work is the rst one that compares three di erent types of oracle on a common benchmark consisting of medium and large size systems for which long execution traces are available.
since we have not been involved in the development of any of the evaluated tools our study is an independent evaluation.
we focus on the practical use of automated oracles using the best available tools out of the box to see the potential obstacles that a software practitioner might encounter if she plans to adopt automated oracles.
staats et al.
studied the understandability of invariants when they are used as test oracles.
their results indicate that users have problems in understanding those oracles.
our study shares the same topic of automated oracles.
however we investigate two completely di erent aspects false positive rate and fault detection.
in this section we summarize the related work that shows the application of automated oracles in detecting faults and anomalies.
mariani et al.
presented an approach that combines classic dynamic analysis with incremental nite state generation.
the generated model can e ectively detect faults and their causes of failures when it is trained by a diverse set of tests that cover well the execution space.
experiment results obtained through a set of small size trace samples up to have demonstrated the viability and e ectiveness of the approach.
moreover the authors implemented a heuristic in their tool bct to lter out false alarms.
bct is a core component of klfa the tool that we used in this study.
a markov model of the system behavior is learned by means of the active learning algorithm proposed by bowring et al.
but their aim is not anomaly detection and automated oracle creation.
rather the authors propose to use the learned model for the identi cation of new behaviors that require the extension of the existing test plan with additional test cases.sekar et al.
proposed to focus only on system calls to learn a fsa.
in fact the approach deals with system security and aims at detecting anomalous sequences of system calls which are likely to point to intrusion attempts and malware.
since this approach is speci cally tailored for security testing it cannot be included in our study which instead considers general purpose oracles.
the empirical validation reported by the authors takes into account only the occurrence and the actual detection of security issues.
while one of the envisaged uses of daikon is bug avoidance this tool has not undergone systematic empirical evaluation along this speci c dimension.
a small scale evaluation of daikon in the context of program changes and regression testing is reported in another paper about daikon .
the program used for this evaluation is replace a small program from the siemens test suite.
eclat is a junit test case generator that tries to produce candidate oracles from passing runs.
the operational model used as unit test case oracle is produced by daikon.
the tool was evaluated on a set of small java programs the largest of which consists of less than 2k lines of code.
diduce a data invariant mining tool that set the foundations for successive developments in the eld among which daikon was originally evaluated on four java programs.
the experiences reported by the authors about the use of diduce are very encouraging since the tool was able to reveal interesting faults that may have gone otherwise unnoticed.
the kind of empirical evaluation conducted in this work can be classi ed as an experience report which is fundamental in the early stages of a new research but should be complemented by more systematic studies such as the one presented in this paper when the discipline matures.
synoptic has also been evaluated only in small scale studies in the experience report category with quite promising results both in terms of bug nding and of increasing the developers con dence in the implementation.
.
experiment design while automated oracles have the potential to reveal software faults they come with some cons however.
the biggest issue with automated oracles is the rate of false alarms or false positives.
when such rate is intolerably high any problem reported by automated oracles will be unreliable hence developers will just ignore it.
in this section we describe the design of the empirical evaluation used to assess the e ectiveness of the automated oracles produced by klfa synoptic and daikon on a set of medium to large industrial subject systems.
the choice of these three tools is motivated as follows klfa is a good representative for fsa oracles synoptic represents temporal invariants and daikon data invariants.
the following are the research questions that we have investigated rq1 what is the rate of false positives associated with the use of automated oracles?
which one triggers more false alarms?
rq2 once trained can automated oracles reveal new faults occurring in running applications?
which one is more e ective?
rq3 how long does it take for training?
once oracles are trained how much time does it take for checking a new trace?138we adopt a dynamic analysis assessment procedure for each subject simulating the real use of the automated oracles a subject system pis running its execution is monitored to obtained traces and di erent automated oracles are inferred from those traces.
then due to new usages new traces the automated oracles may report alarms when the execution violates them.
alarms might due to a fault that has been triggered or they may be wrong false positives .
figure illustrates this procedure.
the training traces in tare used to infer oraclesfo1 o2 ongof di erent types in our case n the oracles being fsa temporal invariants or data invariants .
given the new traces in f the inferred oracles may raise alarms which could be true faults or false positives.
p t fx o1 o2 ... on figure oracle learning and fault reporting the subject systems used in our study are shown in table .
they are java applications from di erent domains and of di erent sizes up to nclocs non comment lines of code .
they have been selected so as to maximize diversity of features and representativeness of relevant application domains.
all applications come with a large manual junit test suite up to more than test cases we consider each junit test method as one test case .
we execute these test cases to get traces from the subjects.
each test case produces one trace if the test succeeds we discard failed tests actually there was only a small number of failed tests from all the subjects and failures were due to concurrency issues which cannot be controlled from the test execution environment .
during test execution daikon was used to obtain the traces because the daikon trace format is general enough to be compatible with the input required by the other two tools klfa and synoptic.
the daikon trace format consists of program point de nitions i. e. identi er variable names and types at executed program locations and the concrete values of the variables during test execution.
daikon traces can be used directly by daikon and from such traces we extract sequences of method invocations enter and method exits exit in the exact order as they occur during test execution.
these sequences are the input traces for synoptic and klfa we name these traces as sequence traces .
sequence traces are often very long the average length of the sequences for our subjects is and each entry in a sequence is often a full java method signature including package name class name method name and then the parameters long signatures.
this slows down the performance of synoptic and klfa and requires more ram during trace processing.
therefore we developed a trace compression algorithm that replaces sequence entry values with their hash keys consisting usually of just or characters.
the algorithm takes into account the occurrence frequency ofsequence entries to assign shorter hash keys for entries that are most frequent.
as a result the size in bytes of the traces is reduced dramatically up to from .9gb to 224mb.
this helps synoptic and klfa deal with more and with longer traces.
.
rq1 false positives to study the false positive rate we use only the traces in the training set t under the assumption that all of them are the result of normal accepted behaviors of p. we sample n traces from tand divide the sampled set into two disjoint sub sets t1 and t2 containing and of the traces int accordingly.
t1is used to infer automate oracles while t2is used for cross validation.
similar to the folds cross validation technique in machine learning we repeat this process to change the selection of traces for training and validation times so that every trace in twill appear at least once in t2.
for each automated type of oracle fsa temporal and data oracles for each sample set tof size nof training traces the size of the sub set t2isn2 n and the false positive rate is calculated as follows fpositive number of rejected traces from t n2 a trace from t2is rejected if the automated oracles learned from t1do not accept it.
in other words the learned oracles report an alarm when processing the trace.
this is a false alarm because all the traces in tare assumed to be fault free ones hence no alarm should be raised.
.
rq2 fault detection investigation of the fault detection capability of automated oracles requires substantial manual work.
in fact faults reported in bug tracking systems and later xed in the code have to be manually re injected in the code under analysis.
for this reason we focus this part of our empirical investigation on a single subject commons collections .
commons collections has a rich and large set of test cases.
such a set of tests generates many traces that can be used for learning the automated oracles.
the key factor for this choice is that the apache software foundation4has a very good issue tracking system where we could nd detailed information about faults at which version a fault is detected and by which tests.
we have selected real faults of commonscollections for this study.
more details about them are provided in section .
.
to evaluate the fault detection capability of an oracle type we work with one single real fault at a time.
we consider one test case that reveals the fault this test produces a faulty trace.
all other non failing tests are used to produce training traces to infer the oracles.
for each oracle type we say that it might detect the fault candidate true positive if at least one of its learned invariants rejects the faulty trace for temporal and data oracles or the trace is not accepted by the fsa for fsa oracle .
to con rm that an oracle actually detects a fault true positive we manually inspect the output reports produced by the oracle to nd if there are any direct link between the reports i.e.
the violated invariants or the path taken in the fsa and the fault.
subject systems used in our study their description size in terms of number of non comment lines of code ncloc and their test suite size name size ncloc test size description commons collections apache collection library for java commons math3 apache mathematic library for java xstream library for serializing objects to xml and back jgap genetic programming library openmrs api core api of openmrs a medical record system jfreechart library to create charts in java joda time date and time library .
rq3 training and checking cost training and checking cost is measured as the amount of execution time real time each tool needs to process respectively all the training traces or the trace to be checked.
since during the evaluation of rq1 false positive rate we found a linear relationship between the number of traces processed and the amount of time needed to process them to answer rq3 we only need to estimate the per unit processing time of each tool.
speci cally we measure the time needed to process a unit consisting of traces.
.
results in this section we present the results obtained from running daikon synoptic and klfa on application subjects.
we traced only the application code excluding the libraries that the applications use.
for daikon and synoptic we use their default con guration options that have been ne tuned in previous studies.
for klfa we limit its minimization step to minimizationlimit because otherwise klfa cannot nish its processing even on a small number of traces.
this is one suggestion of klfa s authors to increase klfa s performance and the parameter does not a ect the generalization of klfa output models.
this means that it does not in uence the acceptability of the models with respect to new traces.
in other words it does not a ect the results of klfa in our study.
.
false positive rate .
.
daikon data invariants figure depicts the box plot of false positive rates of daikon on one of our subjects openmrs api with respect to di erent training sample sizes.
for a sample of traces runs are performed.
in each run traces are used for training the oracles and traces are used for cross validation.
the false positive rate is then calculated based on how many traces among the cross validation traces are rejected by the learned oracles.
the data in the gure show that the false positive rate reduces when the size of the training sample increases .
when the sample size is the false positive rate falls in the range of to while at the sample size of the rate is in between to .
figure shows also the average false positive rate as the line that connects the average values at di erent sample sizes.
we also see that on average the false positive rate reduces when the sample size increases.
in figure we put together the average false positive rate of daikon on all studied subjects.
we ran the experifigure false positive rate of daikon on openmrsapi .
.
.
average of false posi.ve rate sample size daikon summary commons0math303.
commons0collec6ons03.
.
xstream joda06me jfreechart openmrs0api jgap figure average false positive rate of daikon on the subjects140ments based on the traces available from the subjects and for each subject the sample size was increased from to the maximum number of traces that the subject has.
for example jfreechart was run from to traces while commons collections was run from to traces.
overall with daikon we observe the decrease of false positives when the size of the training set increases.
this means that the more we train the data invariants the less they over t the training data.
hence they better generalize to new data accepting more legal behaviors.
as a result the false positive rate is reduced.
the decrease of the false positive rate is not monotonic for subjects joda time and commons collections .
we speculate that this is speci cally due to trace sampling particular sample sizes induce a sample in which cross validation traces are similar to the training traces.
hence the former are likely accepted by the learned oracles.
as a result the false positive rate reduces quicker than usual at such sample sizes.
regarding the rate of false positive with respect to the training sample size as depicted in the gure we can notice that when the training sample size is smaller than traces for most of the subjects the false positive rate is higher than which is a relatively quite high rate.
the best and smallest false positive rate obtained with daikon is .
withcommons collections with the highest sample size of .
.
.
synoptic temporal invariants .
.
.
average of f posi.ve sample size synoptic summary commons0math303.
commons0collec6ons03.
.
xstream joda06me jfreechart openmrs0api jgap figure the average of false positive rate of synoptic on the subjects figure puts together the average false positive rate of synoptic on the application subjects.
here on all subjects we also observe that the average false positive rate reduces when the sample size increases.
this means that the more we train the temporal invariants the smaller false positive rate they report.
overall when the sample size is smaller than on all cases except joda time the false positive rate is greater than higher than the rate obtained by daikon even with traces daikon s rate is .
with joda time synoptic is able to lower the false positive rate to less than when the sample size is greater than .
the best rate that synoptic achieved is .
with joda time when the sample size is .
commons collections has more traces but we could not increase the sample size due to the associated training time which becomes unmanageable even on a cluster .
we discuss this in section .
.
.
.
klfa finite state automata .
.
.
average of f posi.ve sample size klfa summary commons0math303.
commons0collec6ons03.
.
xstream joda06me jfreechart openmrs0api jgap figure the average of false positive rate of klfa on the subject among the three tools klfa is the worst performer in terms of false positive rate see figure .
in all cases it can only slightly reduce the rate when we increase the sample size.
klfa performs the best with openmrs api where it is able to reduce the false positive rate from to .
which is still too high to be used in practice.
the reason behind this poor performance is that we considered system level traces that are quite long their average length is events trace .
when checking a new trace the automaton inferred by klfa has to accept every single event in the exact sequential order of occurrence for an entire system level trace.
this is quite unlikely to happen at the system level while results may be substantially di erent at the component unit level where shorter traces are produced.
our recommendation for this kind of oracle is to use it within a small scope such as a single unit or component.
the false positive rate is expected to be much smaller in such a scope.
.
fault detection in this study we selected real faults of commonscollections from its issue repository .
table shows the ids of the faults and their descriptions.
detailed information for each fault is available from https issues.apache.org jira browse .
these faults were selected based on their diversity and severity.
they are all major faults and they range from missing exception handling to logical faults like wrong method invocation collections or wrong implementation collections .
we ran our experiments on each fault individually so that in each run only one single fault is present.
each fault is revealed by one unique test case.
the fault will manifest itself in the corresponding faulty trace when its revealing test case is executed.
because of the faults the faulty traces might contain some sequences of events or data values that violate the learned oracles.
all other non faulty traces obtained from pass tests are used for oracle training because as the results on the false positive rate indicate the more we train the oracles the less false positive rate we obtain in most cases.141table fault descriptions fault id description collections indexoutofboundsexception in listorderedmap.putall int index map ?
extends k ?
extends v map .
collections the collectionutils.removeall method calls the listutils.retainall method instead of the listutils.removeall method.
collections missing a nullpointerexception check.
collections class extendedproperties has problem with a key composing of only space character.
collections method convertproperties of the class extendedproperties loses non string values.
collections when duplicates are present in a list listutils.intersection the intersection of two lists should give the same result regardless of which list comes rst.
collections nullpointerexception at the collatingiterator class when comparator is null.
table fault detection results by daikon data invariants synoptic temporal invariants and klfa fsa .
each tool has two corresponding columns reported r if the tool reports at least an alarm and true positive tp if the reported alarms are veri ed to point to the corresponding fault.
fault id daikon synoptic klfa r tp r tp r tp collections collections collections collections collections collections collections total table summarizes the fault detection results of daikon data invariants synoptic temporal invariants and klfa fsa .
as expected given its very high false positive rate klfa raises alarms on all the faults meaning that its learned oracles report problems with respect to all the faulty traces column klfa r .
synoptic and daikon also report many faults.
daikon reports out of faults column daikon r while synoptic reports out of column synoptic r .
we investigated the fault reports provided by the tools to see if the invariant fsa violations are actually due to to the faults.
in other words we manually veri ed whether the reported faults are truly detected by the automated oracles.
the second forth and last columns of table show the faults that were truly revealed by daikon synoptic and klfa.
daikon revealed only fault while synoptic revealed and klfa revealed faults.
on one hand this showsthat automated oracles can indeed detect faults.
on the other hand what is also very interesting is that daikon and synoptic result to be complementary of each other all the true positives are exclusively detected by either daikon only or by synoptic only.
this nding suggests that in practice we should combine these two types of oracle to maximize the probability of detecting faults.
in addition the set of faults revealed by synoptic subsumes those detected by klfa.
this can be explained by the fact that both tools depend on the temporal occurrence of events in sequential traces.
in practice both daikon and synoptic should be used to maximize the fault detection capability of automated oracles while according to our empirical results klfa does not need to be used if synoptic is also used.
in what follows we give some examples of fault reports and how they relate to the corresponding faults.
collections daikon report at ppt org.apache.commons.collections.collectionutils .addignorenull java.util.collection java.lang.object enter invariant collection !
null invalidated by sample collection null this is a daikon report saying that at the entry of the method addignorenull daikon has learned an invariant stating that the collection parameter must be non null.
however when executing the fault revealing test org.apache.commons .collections.testcollectionutils.addignorenullbug400 a null pointer has been observed which violates the invariant.
such null pointer is a consequence of the bug hence in this case daikon s automated oracle violation is a direct manifestation of the bug.
collections synoptic report invariant 24r alwaysfollowedby t 1ua invalidated by this counter example events are encoded as follows 1ua org.apache.commons.collections.listutils .retainall java.util.collection java.util .collection enter 24r org.apache.commons.collections.listutils .retainall java.util.collection java.util .collection exit253 2t8 org.apache.commons.collections .collectionutils.removeall java.util .collection java.util.collection exit1251 from the report we learn that synoptic has inferred this invariant 24r alwaysfollowedby t 1ua meaning that the event org.apache.commons.collections.listutils.retainall java.util.collection java.util.collection exit253 is always followed by a org.apache.commons.collections.listutils .retainall java.util.collection java.util.collection enter .
however from the counter example trace after the very last event 24r there is no 1ua.
thus the invariant is violated.
this is because of a fault in the code that calls retainall instead of removeall .
this happens in the body of org.apache .commons.collections.collectionutils.removeall ... .
as a consequence of this fault the output event sequence changes142and the last three events which violate the invariant and their order point directly to the fault.
the same fault collections is revealed also by klfa after inferring a model and checking the same counter example.
in fact klfa expects to see either a branch going to 1ua or another one going to 2w8 from 2t8 according to the learned model.
however 2t8 is the last event in the trace and no nal state is reached in the klfa model.
hence klfa rejects the trace and reveals a true fault.
.
training and checking cost the input traces for synoptic and klfa are sequences of events.
their size is measured in terms of the number of events.
since these traces are composed of method invocations and exits extracted from daikon traces the size of the corresponding daikon traces can be standardized to the same measurement as the number of events or daikon data points in daikon s language .
among the subject applications xstream produces very long traces the average size of an xstream trace is events much higher than the average trace size of all other subjects events.
hence the cost to process xstreams traces is of a di erent category.
we decided to exclude xstream from this part of the study.
in fact in this regard xstream is an outlier not meaningful when estimating the average training and checking cost.
table shows the average clock time needed for daikon synoptic and klfa to learn oracles for a unit of traces and events on average.
these data were measured on a cluster system in which the cpu speed of each node is .2ghz and the amount of ram available is up to 256gb.
the cluster system has a high speed i o storage therefore the data shown in the table might be smaller than the actual amount of time needed on a hardware not equipped with high speed i o storage in fact trace processing involves substantial i o .
as we can observe from the data the cost is higher for klfa and daikon from .
to .
minutes for a set of traces.
from the values we can infer that the amount of time needed to process traces as the case ofcommons collections will exceed hour.
for klfa and daikon we also observed that the amount of time for checking a single trace is very small compared to the time required for training the oracles.
table average wall clock time in minutes to complete the training and checking of a unit of traces tool time minutes daikon .
synoptic .
klfa .
the training cost is smaller for synoptic .
minutes for processing traces thanks to its parallel execution.
although synoptic learns a huge amount of temporal invariants e.g.
more than thousand invariants are learned from traces of openmrs api considering the relationships among all events appearing in all traces it partitions learned invariants into groups and it runs invariant checking in parallel for new traces up to parallel processes.
as a result the total amount of time taken by synoptic is dramatically reduced.in terms of the amount of memory required by the tools on average klfa and daikon require up to 8gb of ram to be able to process all the traces.
synoptic is again an exception it requires about 26gb of ram to process all the traces.
.
discussion we organize the discussion of the experimental results along the following dimensions practical usefulness adoption costs and barriers implications for the research in the eld.
.
practical benefits automated oracles have a moderate fault detection capability see table .
this represents potentially a substantial practical bene t because these oracles are inferred without any manual intervention aimed at re ning or ne tuning them.
they have been used out of the box as reported by the tools which means developers can expose some faults not all of them even without manually specifying any oracle.
the downside of this positive result is that the false positive rate of these tools is quite high.
even excluding klfa which exhibits an intolerably high false positive rate the other two tools daikon and synoptic are still around see figure which means that on a test suite consisting of test cases developers would have to manually classify on average fail outcomes as false alarms.
each such assessment might require substantial e ort because the developer has to carefully examine the execution and the violated invariant to decide whether a real fault was exposed or the inferred invariant is wrong.
since invariants are produced automatically they are not easy to understand and validate.
whether the balance between false positive rate and fault detection capability is acceptable or not for practical purposes is quite di cult to assess.
in practice one can expect that a test suite will expose few faults.
still as discussed above it would require substantial e ort to recognize the false alarms.
a quite representative example could be one where the test suites consist of test cases of which raise false alarms.
we may also assume that at most test case exposes a real fault.
for the developer the cost bene t balance is between manually assessing failed test cases and manually tagging of them as false alarms due to incorrect invariants as compared to manually de ning an oracle capable of exposing the automatically detected fault.
both processes are complex.
manual examination of the false alarms involves deep understanding of automatically generated oracles a di cult task.
manual de nition of a fault exposing oracle might be easy or di cult depending on the kind of fault.
in our example the ratio between the two costs manual oracle de nition vs. failed test case assessment should be or higher to justify automated oracles.
in the general case the cost for the manual assessment of the failed test cases grows linearly with the test suite size being on average of the number of test cases .
on the other hand the cost of manual oracle de nition grows with the complexity and size of the program.
in turn a larger and more complex program requires larger test suites for testing.
assuming a linear proportion and taking our example as representative we can state that on average a cost ratio above between manual oracle de nition and failed test case assessment may justify the adoption of automated oracles.
however project speci c estimates should be carried143out to make any practical decision.
another factor to consider about the practical bene ts is that automated oracles are not going to expose all faults see table and that they are complementary in fault detection.
this means that multiple oracle inference tools should be used hence increasing the cost for the manual assessment of the false positives and that some manual oracle de nition would be still required.
as part of our future work we intend to investigate whether manual oracles are complementary or overlap with automated oracles.
in fact if automated oracles were capable to detect faults that tend to go unnoticed when manual oracles are de ned the bene ts of automated oracles would be strengthened and the associated costs might be regarded as more acceptable.
.
adoptions costs adoption costs are mostly associated with the training phase which requires substantial machine time and resources.
we used a cluster to train the automated oracles and the training time was non negligible.
it involved substantial computational resources.
however we think that nowadays such resources can be obtained at reasonable costs and that such costs do not represent the major barrier to adoption provided the practical bene ts discussed above are delivered.
additional adoption costs are associated with the generation of traces from the test case executions and with the con guration of the tools.
tracing was easily achieved thanks to the functionalities provided by daikon.
tool con guration was also relatively easy.
a few interactions with the tool developers were necessary.
the considered tools are research prototypes not commercial tools which means they tend to be less engineered for easy of use and robustness.
on the other hand we could use them without any major problem.
these tools work for java programs.
when di erent programming languages are targeted the adoption barrier might become insurmountable since to the best of our knowledge no porting to programming languages other than java is available.
.
future research although an informed decision on the practical bene ts would require a careful estimation of the involved costs the high false positive rates of the existing automated oracles make them cost e ective only when manual oracle de nition costs are very high more than times higher in our rough estimate as compared to the manual assessment of a failed test case.
in practice this might prevent any industrial adoption of automated oracles unless their false positive rate is dramatically reduced.
we think a major research direction for this area should target the reduction of the false positive rate while preserving the fault detection capability of the automated oracles.
a fault detection rate of or less at equal fault detection rate would make the cost bene t balance more favorable for the automated oracles in many practical cases.
this paper represent the rst attempt to empirically validate automatically inferred oracles in an unbiased way by adopting the standard procedures used in machine learning and empirical studies and by selecting the subjects their test cases and the faults to be revealed without any in uence from the techniques being validated.
we hope that the research community will use this experiment as the baseline for further experimentation aiming at acquiringa comprehensive body of knowledge on automated oracles and their practical e ectiveness.
we make our experimental material subjects test cases and faults publicly available here for replication and for further experimentation with other tools.
.
threats to v alidity the main threats to the validity of this experiment belong to the internal construct and external validity threat categories.
internal validity threats concern external factors that may a ect the independent variable.
since the authors of this paper have not been involved in the development of any of the tools evaluated in the study there is no bias in favor or against any of the three considered tools.
in contrast with the empirical evaluations published in the respective papers associated with the tools evaluated in our study this is the rst empirical investigation conducted by a third party which is completely unrelated with the development of the tools under evaluation.
this greatly increases the internal validity of the study reported in this paper as compared to the self evaluations available in the literature.
another threat to the internal validity concerns the conguration of the three tools under evaluation.
whenever possible we adopted the default con gurations or the congurations suggested in the papers describing the tools and their validation.
occasionally we also contacted the tools authors asking for suggestions about speci c con gurations.
while a deeper knowledge of the tools might have produced a better tuned con guration to minimize this threat to validity we executed a set of preliminary runs under di erent congurations and we took advantage of all available hints and suggestions to ensure an optimal tool con guration before executing the actual experiment.
construct validity threats concern the relationship between theory and observation.
they are mainly due to how we measured the performance of the three tools under evaluation.
to estimate the false positive rate we adopted the crossvalidation procedure a pretty standard way for assessing the performance after training in machine learning.
the fault detection rate was measured by carefully selecting real faults reported in issue tracking systems and by manually re injecting them into the code.
we put our best e ort in carrying out this task so as to ensure that each bug was faithfully reproduced in the manually mutated code.
for the cost measurement we considered the wall clock time measured directly on the cluster where the training process was executed.
while this might expose us to some variability intrinsic with process execution on cluster nodes the actually measured times are at a granularity minutes that make such uctuations the order of seconds not very important.
external validity concerns the generalization of the ndings.
we considered subjects real faults of one subject and automated oracle mining tools.
while we tried to select the subjects of our experiment so as to ensure maximum diversity and representativeness further replication of the study on additional subjects is essential to be able to generalize our ndings and to reduce the external validity threat.
generalization on tools di erent from the three considered in this study should be also done with care.
while tools in the same categories data invariant inference temporal invariant inference and fsa model inference might produce similar results it is de nitely not possible to extrapolate our144 ndings to tools that fall in other categories.
the problem of extending our empirical evaluation to other automated oracle mining tools is that often such tools are just described in papers but their authors do not make them available for third party experimentation.
.
conclusion several attempts to address the oracle problem in software testing have been recently proposed in the literature.
automated oracles are inferred from execution traces by mining data invariants temporal invariants or fsa behavioral models.
the work presented in this paper is the rst large scale empirical investigation of the e ectiveness of automated oracles conducted by a third party not involved in any of the automated oracle tools being evaluated.
results show that automated oracles can detect several real faults but such fault detection capability comes at the price of a quite high false positive rate on average .
the high false positive rate makes the balance between practical bene ts revealed faults and costs for the manual assessment of the false alarms unclear.
in our rough estimates a software project where manual assessment of a false alarm incurs a cost which is or more times higher than the cost for manual oracle de nition could bene t from the proposed approach.
however a detailed estimate should be carried out on a project speci c basis taking into account several factors discussed in our paper among which the faults that would be anyway missed by automated oracles the need for multiple kinds of automated oracles and the degree of complementarity between manual and automated oracles.
the current performance of available automated oracle learning tools do not support any easy or obvious decision about tools adoption despite their overall acceptable adoption barrier only for java programs associated with the computational resources and the con guration e ort involved.
key to the success of automated oracle mining is the capability to infer oracles that exhibit a substantially lower false positive rate as compared to the available state of the art tools while keeping the same or o ering an increased fault detection capability.
this demands for a major advancement of the research in the area.
our future research will be devoted to further empirical investigation of the e ectiveness of automated oracles to corroborate our ndings and to increase their external validity.
we plan also to compare manually de ned oracles with automated oracles so as to have a direct measurement of the involved costs and to determine whether they target complementary or overlapping classes of faults.
speci cally we intend to conduct a study in which the ratio between the cost for manually de ning the oracle and the cost for manually classifying a failed test case as a false alarm or a real fault are measured directly.
.
acknowledgement this work has been funded by the european union fp7 project fittest grant agreement n. .
.