cachetor detecting cacheable data to remove bloat khanh nguyen and guoqing xu university of california irvine ca usa khanhtn1 guoqingx ics.uci.edu abstract modern object oriented software commonly suffers from run time bloat that significantly affects its performance and scalab ility.
studies have shown that one important pattern of bloat is the work repeatedly done to compute the same data values.
very often the cost of computation is very high and it is thus beneficial to me moize the invariant data values for later use.
while this is a c ommon practice in real world development manually finding in variant data values is a daunting task during development and tuning .
to help the developers quickly find such optimization opportun ities for performance improvement we propose a novel run time profil ing tool called cachetor which uses a combination of dynamic dependence profiling and value profiling to identify and report operations that keep generating identical data values.
the major chall enge in the design of cachetor is that both dependence and value profi ling are extremely expensive techniques that cannot scale to large real world applications for which optimizations are impor tant.
to overcome this challenge we propose a series of novel abstra ctions that are applied to run time instruction instances during p rofiling yielding significantly improved analysis time and scalabil ity.
we have implemented cachetor in jikes research virtual machin e and evaluated it on a set of large java applications.
our exper imental results suggest that cachetor is effective in exposing c aching opportunities and substantial performance gains can be ach ieved by modifying a program to cache the reported data.
categories and subject descriptors d. .
metrics performance measures d. .
testing and debugging debugging aids general terms performance reliability experimentation keywords runtime bloat performance optimization cacheable data dynamic dependence analysis permission to make digital or hard copies of all or part of thi s work for personal or classroom use is granted without fee provided th at copies are not made or distributed for profit or commercial advantage an d that copies bear this notice and the full citation on the first page.
to cop y otherwise to republish to post on servers or to redistribute to lists re quires prior specific permission and or a fee.
esec fse august saint petersburg russia copyright acm ... .
.
.
introduction many applications suffer from chronic runtime bloat exces sive memory usage and run time work to accomplish simple tasks that significantly affects scalability and performance.
ou r experience with dozens of large scale real world applications shows that a very important source of runtime bloat is the work repeatedly done to compute identical data values if th e computation is expensive significant performance improvemen t can be achieved by memoizing1these values and avoiding computing them many times.
in fact caching important data instead of recomputing them is already a well known programming practi ce.
for example in the white paper websphere application serv er development best practices for performance and scalabilit y four of the eighteen best practices are instructions to avoi d repeated creation of identical objects.
while finding and caching ide ntical data values is critical to the performance of many large sca le software systems the task is notoriously challenging for progr ammers to achieve during development and tuning.
a large long run ning program may contain millions of instructions and each inst ruction may be executed for an extremely large number of times and pro duce a sea of data values.
it would be extremely difficult if n ot impossible to find identical run time data values and under stand how to cache them without appropriate tool support.
motivation to illustrate consider the following code example adapted from sunflow2 an open source image rendering system.
float fvalues .
.
.
.
.
.
.
.
.
.
.
int ivalues new int for int i i fvalues .length i ivalues float.floattointbits fvalues this simple program encodes each float value in array fvalues using a bit array represented by an integer which can then be stored in an integer array.
in this example most of the value s in fvalues are1.
and it is unnecessary to invoke method float .
floattointbits which is quite expensive to compute the bit array for each of them.
the program would run more efficiently iffloat.floattointbits can be invoked the first time .
is seen and the result can be cached and reused for its future occurrences.
however this information may not be availabl e to the programmer during development as fvalues may be a dynamically computed array whose content is unknown at compile ti me or the fact that most of its elements are the same is specific to a certain kind of input image being processed.
as a result it i s necessary to develop techniques and tools that can help the progra mmer 1terms memoize and cache are used interchangeably.
to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august saint petersburg russia copyright acm ... .
268figure an overview of cachetor.
find such missed optimization opportunities e.g.
report m ethod floattointbits is frequently executed with the same input and produces the same output especially in a situation whe re a significant performance issue is observed and tuning must be undertaken to make the application reach its performance goal .
our proposal in this paper we propose a dynamic analysis tool called cachetor that profiles large scale applications to pinpoint cacheable data values and operations producing these values.
cachetor has three major components which are illustr ated in figure .
at the lowest level of the tool is an instruction level cacheable value detector called i cachetor which identifies bytecode instructions whose executions produce identical pr imitivetyped values.
finding only instructions that always produ ce the same values may significantly limit the amount of optimizati on opportunities that can be detected.
to improve usefulness we propose to compute a cacheability measurement for each instruction that captures the percentage of the most frequently occurr ing value among all values produced by the instruction.
for example a lthough the call instruction that invokes floattointbits does not always return the same value this instruction has a high cm and will thus be recognized by the developer during inspecti on.
i cachetor is of limited usefulness by itself it is often no t possible to cache values produced by specific instructions.
we d evelop two higher level detectors namely d cachetor andm cachetor that detect data structures containing identical values and method calls producing identical values respectively to help develop ers understand and fix inefficiencies at a logical level.
d cacheto r queries i cachetor for the cms of the heap store instructions that wr ite into a data structure and aggregate these instruction level cms to compute the cm of the data structure.
m cachetor focuses on the v alue returned from each call site it queries i cachetor for the c m of the call instruction if the return value is of primitive type or otherwise queries d cachetor for the cm of the returned object s o as to compute the cm of the call site.
eventually allocation si tes and method calls are ranked based on their respective cms and the lists are reported to the developer for manual inspection.
fixing problems reported by cachetor because cachetor relates optimization opportunities with high level program entities the reported problems can be easily understood and fixed.
for example d cachetor reports allocation sites that create ide ntical objects and data structures.
to fix the reported problems one m ay create a singleton pattern for such an allocation site to enf orce the use of one single instance throughout the execution or deve lop a clone method in its class to directly copy values between the objects instead of re computing the same values from the scr atch.
as another example m cachetor reports call sites whose exe cutions always produce the same results.
one may easily create a static or instance field and cache the result of such a call s ite in the field so that the frequent invocation of the method can be avo ided.technical challenges and our solution the biggest challenge that stands in the way of implementing cachetor is how to find these identical data values in a scalable way so that cachetor can be applied to large real world applications.
in particula r the implementation of i cachetor requires the comparison of run time values produced by different executions of the same instruc tion.
to do this a natural idea is to perform whole program value profiling which records run time values for all instruction exe cutions.
these values are compared offline to identify cache able instructions.
in addition in order to compute data struct ure level cms a dynamic dependence analysis may be needed to understand which data structure an instruction may write into at run tim e. however both whole program value profiling and dependence ana lysis are extremely expensive techniques that cannot scale to rea l world applications.
to improve the practicality of our analysis we propose a novel approach that combines value profiling with dyna mic dependence analysis in a way so that they mutually improve th e scalability of each other.
specifically we use distinct val ues produced at run time to abstract dependence graph nodes yieldi ng a value abstracted dynamic dependence graph .
this graph contains the value information necessary for our analysis and yet is much smaller and easier to compute than a regular dynamic depende nce graph.
we then propose a series of further abstractions base d on this dependence analysis to scale cachetor to the real world .
we have implemented this combined dependence and value profiling technique in jikes research virtual machine and then built the three detectors based on the abstract representat ion.
an evaluation of the tool on a set of large scale java applica tions shows that cachetor incurs an overall .
running time overhead and .
space overhead.
while these overheads are very large they have not prevented us from collecting any run ti me data for real world applications.
in fact it could have been imp ossible to implement such a heavyweight analysis on real world appl ications such as those in the dacapo benchmark set without the proposed abstractions.
we have carefully inspected the ana lysis reports fixing the reported problems has led to significant p erformance improvements for many large scale applications.
thi s experience is described in the five case studies in section .
the major contributions of the paper are a novel approach that uses distinct values to abstract instr uction instances and dependence relationships leading to si gnificantly increased efficiency three cacheability detectors that are built on top of the val ueabstracted dependence graph to find caching opportunities an implementation of cachetor in jikes research virtual machine an evaluation of cachetor on a set of large java applications that shows cachetor incurs a high but acceptable overhead and large optimization opportunities can be quickl y found from cachetor s reports.
these initial results sugge st that cachetor is useful in practice in helping developers fin d caching opportunities and our combined dependence and value profiling may be employed to improve the efficiency of a variety of dynamic analysis techniques.
.
v alue abstracted dynamic dependence analysis the naive implementation of either dynamic dependence anal ysis or value profiling cannot scale to large real world appl ications for which optimizations are important.
to overcome this sca lability challenge we propose a novel technique called value abstracted int f int i int arr int j int k int p arr if k j j p k k return j int input new int input input input input input input int i if i intput.length int result f i input print result i i goto a a simple program b part of the value abstracted dynamic dependence graph figure an example of value abstracted data dependence gr aph.
dependence analysis that uses distinct run time values an instruction produces to define a set of equivalence classes to abstra ct dependence graph nodes i.e.
instruction instances leadi ng to significantly reduced analysis time and space consumption.
.
value abstracted dependence graph to formally define the abstraction we first give our definitio n of dynamic dependence graph.
since our goal is to detect cach ing opportunities we are interested only in data dependence.
definition dynamic data dependence graph a dynamic data dependence graph n e has node set n s i where each node is a static instruction s annotated with a natural number i i representing the i th execution of this instruction.
an edge sj sk s1 s2 s andj k i shows that the j th execution of instruction s1writes a heap or stack location that is then used by the k th execution of s2 without an intervening write to that location.
if an instruction accesses a heap location through v.f the reference value in stack location vis also considered to be used.
while cachetor works on the low level jvm intermediate representation the discussion of the algorithms uses a three addresscode representation of the program e.g.
an assignment a bor a computation a b c .
we will use terms statement and instruction interchangeably both meaning a statement in the three addresscode representation.
we divide the static instruction set sintosp andsr which contain instructions that process primitive typed and reference typed data respectively.
the first step of ou r analysis targets sp because we are interested in finding instructions that produce identical primitive typed values.
srwill be considered in the next stage when the cacheability information of instruc tions in spneeds to be aggregated to compute the cacheability informat ion for objects and data structures in d cachetor .
for an instruction s sp we use vsto denote the set of distinct values that the instances of sproduce at run time.
we use each value v v sas an identifier to determine an equivalence class including a set of instances of sthat produce the same value v. the definition of the value abstracted dependence graph is g iven as follows definition value abstracted data dependence graph a value abstracted data dependence graph n e has node set n sp v where each node is a pair of a static instruction s sp and a value v v s represented as sv denoting the set of instances of sthat produce the same value v. an edge sw sx s1 s2 sp andw x v shows that an instance of s1thatproduces a value wwrites a location that is used by an instance of s2that produces a value x without an intervening write to that location.
if an instruction accesses a heap location through v.f the reference value in stack location vis also considered to be used.
in the value abstracted dependence graph instances of an i nstruction that produce the same run time value are merged an d represented by a single node.
the advantage of developing such a n abstraction is two fold the number of distinct values p roduced by an instruction is often much smaller than the total number of executions of the instruction this is especially true for cacheable instructions that frequently produce identical values le ading to reduced dependence graph size and profiling cost the merge d nodes and edges are very likely to represent computation pat hs that ultimately produce the same results maintaining one singl e copy of the path would often suffice to help us understand how these results are computed computation paths leading to different results are still distinguished.
example to illustrate consider the example in figure .
part a shows a simple program where each integer in the array input is passed into function f which simply computes the integer to the power of i.e.
arr .
note that among the six integers ininput four of them are and hence it is highly beneficial to cache and reuse the result of function ffor the specific input value .
shown in figure b is an important part of the value abstracted dependence graph for the program execution.
eac h edge in the graph is annotated with the number of occurrences of th e dependence relationship the edge represents during executio n. each node in figure b is a static instruction annotated with a distinct value it produces.
the static instruction is represe nted by its line number in the program.
the left part of the value abstra cted dependence graph combines the computations for i and i.e.
input for these four input values all instructions except those at line and produce the same output value .
edg es annotated with high frequencies represent common computat ions that may be reused to improve performance.
dependence relat ionships for i i.e.
input are shown in the right part where each dependence edge occurs only once.
dependence relation ships fori are similar to those for i and are thus omitted from the figure.
while using distinct values to abstract dynamic dependence graph can significantly reduce the graph size and the profiling cost the numbers of distinct values can be very large for some instruc tions such as those that increase loop variables e.g.
line in f igure .
because each instance of such an instruction produces a diff erent 270value its instances can never be merged.
in addition the ex ecution frequency of the instruction depends completely on the numb er of loop iterations which is input sensitive andunbounded and thus it can still be difficult to collect the value abstracted dep endence graph for large scale long running applications.
our experience shows that key to developing a scalable dynam ic analysis is to statically bound the amount of information to be collected dynamically .
in other words the size of the profile should have an upper bound before the execution it cannot depend on dynamic behaviors of the program.
in cachetor we propose to further limit the number of equivalence classes for each instru ction to a fixed number k so that at most kdependence graph nodes can be recorded regardless of how many times the instruction is e xecuted.
to do this we define a hash function h v v kthat uses a simple modulo operation to map each distinct run time valu ev produced by the instruction into an integer in the set k .
ifvis a floating point value it is cast to an integer before the hash function is performed.
this abstraction results in a hash value based dependence graph defined as follows definition hash value abstracted data dependence graph a hash value abstracted data dependence graph n e has node setn sp k where each node is a pair of a static instruction s sp and an integer m k represented as sm denoting the set of instances of swhose results are mapped to the same number mby the hash function h. an edge sm1 sm2 s1 s2 sp andm1 m2 k shows that an instance of s1 whose result is mapped to m1writes a location that is used by an instance of s2whose result is mapped to m2without an intervening write to that location.
if an instruction accesses a heap location through v.f the reference value in stack location vis also considered to be used.
note that the hash value abstracted dependence graph is a lossy representation where the parameter kdefines a tradeoff framework between analysis precision and scalability.
kcan be provided by the user as a tuning parameter to find the sweetspot for a parti cular program.
we associate a frequency count with each graph node representing the number of instruction instances that are m apped to the node.
it is clear to see that instructions whose execut ions are dominated by one graph node are more likely to create iden tical values than those whose execution frequencies are thinl y spread among multiple nodes.
in our experiments we have evaluated cachetor using different k s. we find that a prime number preserves more information than a composite number and a rel atively small number can often be very effective to distingui sh truly cacheable instructions from those that are not.
details of o ur evaluation can be found in section .
.
adding calling context abstraction in order to compute cacheability for an object in d cacheto r we need to aggregate the cacheability information for instr uctions that write into the object.
a common abstraction for modelin g a heap object is its allocation site.
however using only allo cation sites to aggregate run time information can cause significa nt imprecision leading to reduced usefulness of the tool.
this is es pecially true for large scale object oriented applications that ma ke heavy use of data structures.
for example each hashmap has an inte rnal entry array and all these array objects are created by the same allocation site.
failing to distinguish array objects base d on the hashmap objects they belong to would cause all hashmaps to ha ve similar cacheability measurements.
object contexts have been widely used in static analysi s to distinguish objects that belong to different data structur es.
an ob ject context is represented by a chain of allocation sites of the receiver objects for the method invocations on the call stack.
we propose to add object contexts into the dependence graph so t hat instructions that write into different data structures can be distinguished and the cacheability of an instruction can be approp riately attributed to the cacheability of the data structure that th e instruction writes into.
details about the cacheability computati on will be discussed shortly in the next section.
it can be extremely expensive to record a chain of allocation sites for each dependence graph node.
to solve the problem we enco de an object context into a probabilistic unique value .
an encoding function proposed in is adapted to perform this computat ion ci ci ai where aiis the i th allocation site id in the chain and ci 1is the probabilistic context value computed for the chain prefix with length i .
while simple this function exhibits very small context conflict rate as demonstrated in .
sim ilarly to the handling of distinct values we bound the number of obj ect contexts allowed for each instruction with a user defined pa rameter k and map each context cito a number in k using the same hash function ci k .
note that this modeling is performed for both instructions that manipulate primitive typed values i.e .
sp and those that manipulate objects i.e.
sr .
the addition of object contexts results in a new dependence graph which we refer to as value and context vc abstracted data dependence graph each instruction s sp has a pair annotation angbracketleftm n angbracketright where mis a hash value k andnis a hash context k each instruction s sr has only one context annotation n k .
the vcabstracted data dependence graph is defined as follows definition vc abstracted data dependence graph a vcabstracted data dependence graph n e has node set n sp k k sr k where each node is either a triple s angbracketleftm n angbracketright s1 sp m k n k denoting the set of instances of s1whose results are mapped to the same number mand whose object contexts are mapped to the same numbern or a pair sn s2 sr n k denoting the set of instances of s2whose object contexts are mapped to the same numbern.
each edge can have one of the three forms sn1 sn2 s angbracketleftm1 n1 angbracketright s angbracketleftm2 n2 angbracketright orsn1 s angbracketleftm n2 angbracketright .
the first two forms represent the propagation of a reference typed and a primitive typed value respectively.
the third form represents a pointer de referencing operation.
cachetor profiles a program execution to compute a vc abstra cted data dependence graph.
this graph will be used by a series of offline analyses discussed in the next section to compute cacheability measurements and rank data structures call sites.
the profi ling details can be found in section .
.
cacheability computation this section presents three offline analyses that take a vcabstracted dependence graph as input and compute cacheabil ity measurements cm for instructions data structures and c all sites.
these measurements are subsequently used to rank the corres ponding program entities to facilitate user inspection.
.
computing instruction cacheability measurements the first analysis i cachetor computes cms for static inst ructions.
the higher cm an instruction has the more identical v alues the instruction produces during execution.
as discussed in section .
each dependence graph node s angbracketleftm n angbracketrightis associated with an execution frequency count recording the number of instruc tion instances that are merged into this node.
using freqm n sto represent 271the frequency associated with node s angbracketleftm n angbracketright we give the definition of the instruction cm as follows definition instruction cacheability measurement icm for each static instruction s sp its icm is defined as icm s avg n k max0 m k freq angbracketleftm n angbracketright s sum0 m k freq angbracketleftm n angbracketright s over all vc abstracted data dependence graph nodes of the fo rm s angbracketleftm n angbracketright.
icm is computed only for instructions that manipulate primi tivetyped data.
for each vc abstracted dependence graph node s angbracketleftm n angbracketright we first fix its context slot n and compute a ratio between the maximum of the frequencies and their sum over the nodes with diffe rent hash values m. the icm of sis finally computed as the average of these ratios for all contexts n. it is clear to see that1 k icm s .
if the instruction always produces the same value during ex ecution its icm is .
on the other hand if the values the instr uction produces are spread evenly in the khash value slots its icm is1 k. icm is not particularly useful by itself because it can be ve ry difficult if not impossible for the developer to cache the v alue of a particular instruction in the program.
to further impro ve cachetor s usefulness we develop two high level cacheabili ty detectors to help the developer make sense of the heap and executio n information at a high logical level.
icm will be used later b y the two detectors to compute high level cms.
.
computing data structure cacheability measurements the second offline analysis d cachetor aggregates icms fo r instructions that write into a data structure to compute dat a structure cacheability measurements dcms .
specifically we co mpute a dcm for each allocation site summarizing the likelihood of the run time data structures created by the allocation si te containing identical data values.
we find that focusing on alloc ation sites achieves the right balance between the amount of optim ization opportunities that can be detected and the difficulty of deve loping fixes.
for example if an allocation site has a cm we may simply cache and reuse one single instance for it.
optimizat ion opportunities can still be found for allocation sites with s maller cms although their objects are not entirely identical the y may contain identical fields which may be cached for improved pe rformance.
note that simply ranking allocation sites based on their exe cution frequencies cannot reveal caching opportunities.
for exam ple in a typical large application the most frequently executed al location site is the one in hashmap.put that keeps creating map entry objects to store newly added keys and values.
objects creat ed by this allocation site are not reusable at all.
hence it is nec essary to develop new metrics for allocation sites in our framework.
a data structure often contains multiple levels of objects and hence we first consider the computation of cacheability mea surements for individual objects i.e.
ocms .
ocms are aggrega ted later based on the reference relationships among objects to form the dcm of a data structure.
to compute the ocm for an allocation siteo we focus on heap store instructions that access primitivetyped data only.
reference typed stores will be considered later when ocms are aggregated.
the ocm computation starts with inspecting each allocation site of the form o a newa.
as the allocation site accesses a reference typed variable it belongs to the instruction se t sr and has a total of k nodes in the vc abstracted dependence graph.
each node is of the form on where nis a hash context valuea.f b a c2 a a simple program b d ... pointer dependence value dependenceh c3 h.g h.t p5p new p2 p.q c c new c c a c int b d a.f b p p new p p.q c h c h.g h.t p c new c1 b its dependence graph figure an example program and its value and pointer dependence.
k .
the analysis traverses the vc abstracted dependence graph starting from each such node find a set of nodes of the for m s a.f b angbracketleftm n angbracketrightsuch that s sp andapoints to an object created at on.
this can be done by distinguishing pointer dependence andvalue dependence in the dependence graph.
a pointer dependence relationship occurs between an instruction insta nce that defines a pointer variable and a subsequent load or store in struction instance that dereferences this pointer.
a value depen dence relationship occurs between an instruction instance that w rites a value into a stack or heap location and a subsequent instru ction instance that reads the value from the location.
figure sho ws a simple program and its dependence graph.
these two dependen ce relationships are represented by dashed arrows and solid ar rows respectively.
note that nodes that have pair annotations and t hat have single context annotations in figure b represent inst ruction instances manipulating primitive typed and reference type d values respectively.
suppose figure a shows an inlined program those statements are originally located in different metho ds.
hence different statements may have different context encoding i n figure b .
in order to find the set of nodes that write into an object created by an allocation site we perform a depth first traversa l from each node onrepresenting the allocation site.
during the traversal we are interested in such a pointer dependence edge ethateis reachable from ononly through value dependence edges.
in other words no other pointer dependence edge exists between eand the root node on.
clearly the target node of erepresents an instruction instance that reads writes an object of the allocation siteon.
among these target nodes we are interested only in those t hat write primitive typed values into the object.
this subset o f nodes is referred to as the object writing instruction set owis for the allocation site node on.
the owis for node c newc1in figure includes for example a.f b angbracketleft3 angbracketrightandh.g angbracketleft4 angbracketright.
note that node h.t p5writes a reference into an object created by c newc1 i.e.
it has a single context annotation and thus is not considered in the ocm computation.
now we give the definition ofon s object cacheability measurement definition object cacheability measurement ocm for each allocation site node on o sr its ocm is defined as ocm on avg s owisicm s owis 272algorithm finding allocation site nodes that belong to the same data structure.
input vc abstracted data dependence graph g selection ratio r output map node set node dsmap 1set node visited alloc site nodes that have been visited 2foreach alloc site node oningdo list node alloclist on set node ds on 5dsmap dsmap an bracketle ton ds an bracketri ht whilealloclist ne ationslash do alloc site node pc removetop alloclist ifpc visited then continue visited visited pc list node reachednodes pc whilereachednodes ne ationslash do nodeqd removetop reachednodes foreach outgoing edge eof node qddo ifeis a pointer dep edge and target e writes a reference value then nodere target e foreach incoming edge e of node redo ife is a value dependence edge then traverse backward alloc site node o n backwardtraverse e if the difference between the ocms of on ando n are r if ocmo n ocmon ocmon rthen ds ds o n alloclist alloclist o n else if eis a value dep edge and target e writes a reference value then reachednodes reachednodes target e 27returndsmap where owis is the object writing instruction set for node on and icm s owis denotes the icm of the instruction scomputed over the nodes in owis.
the ocm of an allocation site node is determined by the icms of the instructions that write primitive typed values into the objects created by the allocation site.
however the icm for an instr uction used here is computed over the nodes in the owis of on while the icm in definition is computed over all nodes for the instr uction.
because a static instruction may write into objects cr eated by different allocation sites during execution its graph nod es that are unreachable from onare not considered towards the computation ofon s ocm.
dcm computation the ocm computation considers only the primitive typed values contained in an object.
in order to fi nd large optimization opportunities we compute dcms for data struc tures by considering reference typed store instructions and agg regating ocms of the objects connected by such instructions.
for each allocation site node on the dcm computation first identifies other allocation site nodes pcthat belong to the same logical data structure rooted at on.
this can be done by traversing the dependence graph from each node onand transitively following pointer dependence edges.
algorithm describes the details of such a traversal .
the algorithm takes as input a vc abstracted dependence graph a nd a selection ratio r and eventually computes a map dsmap that contains for each allocation site node in the graph e. g. on a set of allocation site nodes e.g.
dsat line that may belong to the same logical data structure.
initially dscontains one single element on line and more allocation site nodes will be gradually added into dsas the data structure is being discovered by the analysis.
alloclist is a list of allocation site nodes that have been identified as part of the data structure.
these nodes need further inspection to find those that are transitively reachable from them.
in the beginning of the an alysis alloclist has one node on line .
lines show a worklistbased iterative process to discover the data structure.
eac h iteration of the process retrieves an allocation site node pcfromalloclist and attempts to find allocation site nodes that are reference d bypc.
this is achieved by performing a breath first traversal of th e graph starting from node pc lines .
for each node qdreached during the traversal line the algorithm inspects each of its outgoing edges.
if an outgoing edge eis a value dependence edge that propagates a reference value lines the target node ofeis added into list reachednodes for further inspection.
we are particularly interested in pointer dependence edges whose tar get is a store node writing a reference value lines such as h.t p5 in figure because such an edge can lead the analysis to find a llocation sites that are referenced bypc.
once such a pointer dependence edge is found lines tra verse backward the dependence graph starting from the targ et re of the edge.
this backward traversal follows only value depe ndence edges until it reaches an allocation site node o n line which is the creation point of the object that flows to re.
it is important to note that we add o n into the data structure set dsonly when the ocm of o n and the ocm of the root of the data structure on are close enough i.e.
their difference is the given selection ratio r .
in other words we select objects that have similar cachea bility measurements to form a data structure so that the develop er can easily find and fix problems related to the data structure.
example we use the simple example in figure to illustrate how the algorithm works.
in order to identify the data struct ure rooted at the node c newc1 our analysis traverses the graph until it reaches a pointer dependence edge whose target node i.e.
h.t p5 is a heap store writing a reference value.
next lines in algorithm traverse backward the dependence graph sta rting from h.t p5 following only value dependence edges.
this traversal leads up to the allocation site node p newp2.
there exists a reference relationship between this node and node c new c1.
the ocms of these two nodes are then compared against the selection ratio r to determine whether p newp2should be included in the data structure set of c newc1.
using ds onto represent the set of allocation nodes discovered by algorithm for the root note on we give the definition of dcm as follows.
note that onitself is also included in ds on.
definition data structure cacheability measurement dcm for each allocation site node on its dcm is defined as dcm on avgpd dsonocmpd whereocmpdis the object cacheability measurement for the allocation site node pd as defined in definition .
eventually allocation site nodes are ranked based on their dcms and the ranked list is reported to the user for manual inspect ion.
when cachetor reports an allocation site node on it reports not onlyonitself but also the data structure ds oncomputed by algorithm .
this would make it easier for the developer to unders tand the problem and develop the fix to cache the invariant part o f the 273data structure.
note that different graph nodes for the same static allocation site are reported separately based on their dcms .
the allocation site may be cacheable only under certain calling co ntexts reporting them separately instead of combining them using an average would potentially reveal more optimization opportu nities.
.
computing call site cacheability measurements the third offline analysis m cachetor computes cacheabil ity measurements for call sites.
our target is call sites that ha ve values returned from the callees.
we are not interested in those that do not bring values back because it is often unclear how to av oid re executing such calls.
given a call site of the form a f .
.
.
its call site cacheability measurement ccm is determined by w hether aalways receives identical values.
it is computed by either q uerying i cachetor for the icm of the instruction if ais a primitivetyped variable or querying d cachetor for the dcm s of the allocation site s that create the objects acan point to.
formally we give the definition of ccm as follows.
definition call site cacheability measurement ccm for each call site c a f .
.
.
its ccm is defined as ccm c icm c a is a primitive typed var avgon alloc a dcm onotherwise where alloc a is a set of allocation site nodes such that the objects created at these allocation sites may flow to a. the set of allocation site nodes alloc a can be obtained by traversing backward the dependence graph from the call site into the callee and following only value dependence edges.
thi s process is similar to what lines do in algorithm .
note tha t to compute ccm for a call site we consider only whether the ca ll returns identical values regardless of its arguments.
thi s definition may potentially expose more optimization opportuniti es if the call produces the same output under different inputs th ere may be some repeated computation inside the method that can be re used for increased efficiency.
eventually call sites are ranked based on their ccms and then reported to the user.
another important optimizability measurement is the execu tion frequency of each allocation call site.
to take this into ac count we take the top allocation call sites from their respect ive lists ranked based on the dcms and ccms and re rank them based on their frequencies.
these newly ranked lists contain info rmation regarding not only cacheability but also execution hotnes s the developer could thus focus her effort on fixing problems with allocation call sites that are both cacheable and frequently ex ecuted.
.
dependence graph profiling we have implemented cachetor in the jikes research virtual m achine rvm version .
.
.
jikesrvm contains two just intime jit compilers a baseline compiler that directly tra nslates java bytecode into assembly code and an optimizing compile r that recompiles and optimizes hot methods for improved performa nce.
the cachetor instrumentation is performed on the high leve l intermediate representation hir generated by the optimizing c ompiler and thus it runs in the optimizing compiler only mode.
as the parameters k i.e.
the number of value slots and k i.e.
the number of context slots are determined by the user befor e the execution all dependence graph nodes can be created at the c ompile time.
specifically we inspect each hir instruction dur ing the compilation if the instruction manipulates primitive ty ped data weallocate an k k array and each slot of the array represents a graph node for the instruction if the instruction manipulates re ferences we only need to create an array of k slots.
we use a shadow memoryto perform the dependence graph profiling.
for each memory location lin the program we maintain a shadow location l that keeps track of the address of the graph node that represents t he instruction instance that last writes into l. iflis a stack variable l is simply a new bit variable in the same method stack.
to shadow heap locations we introduce an additional bit field for each existing field in each class.
to shadow an array with sslots we modify the object allocator in the jikesrvm in a way so that an additional space of s 4bytes is allocated and appended to the space of the array upon its creation.
at each instruction that reads locations l1 l2 .
.
.
l nand writes location l0 our instrumentation code adds dependence graph edges in the following three steps the current encoded call ing context and the value in l0are retrieved.
these values are used to determine which dependence graph node nthis particular instance of the instruction should be mapped to values contained in the shadow locations of l l .
.
.
l nare retrieved.
these values correspond to the addresses of the dependence graph nodes that last writ e into l1 l2 .
.
.
l n respectively we add a dependence graph edge between each node contained in l iand node n representing a dependence relationship.
the address of nis then written into the shadow location l this address will be retrieved later when another instruction uses l0.
eventually if the instruction does a pointer deference on li the edge connecting l iandnis annotated with pointer dependence otherwise it is annotated with value dependen ce.
we create a tracking stack to propagate tracking informatio n between callers and callees.
in addition instrumentation co de is inserted before each call site and in the beginning of each meth od in order to calculate object contexts.
when a method is execu ted its current object context is stored in a local variable whi ch will be retrieved and used later to determine dependence graph no des.
it is important to note that cachetor is thread safe.
we coll ect a vc abstracted dependence graph per thread and eventually c ombine these graphs to compute various cms.
.
ev aluation we have applied cachetor to a set of real world programs from both the dacapo benchmark set and the java grande benchmark set .
we are not aware of any publicly available tool that can provide similar diagnostic information to serve as basis for comparison.
therefore in this section we describe the ove rhead measurements and our experiences with finding and fixing perf ormance problems using cachetor.
all programs were executed with their large workloads.
expe rimental results were obtained on a quad core machine with in tel xeon e5620 .
ghz processor running linux .
.
.
the max imal heap size was 4gb.
.
overhead measurements table shows our overhead measurements.
the parameters k andk used to obtain the data are both .
it appears that is the largest prime number to which our benchmarks can scale.
i ncreasing either kork to causes some programs to run out of memory.
due to space limitations we report only the worst c ase performance measurements for k k .
using a smaller k ork will significantly reduce the time and space overheads.
user may use kandk as tuning parameters to find the balance point between the precision of the report and the scalability of the a nalysis.
section a reports the characteristics of benchmarks in term of the size of their vc abstracted graphs i.e.
number of node s and 274table our benchmarks the characteristics of their vc ab stracted graphs and the overhead measurements.
program a graph characteristics b time overhead c space overhead nodes edges classes methods t0 s t1 s to s0 mb s1 mb so antlr .
.
.
.
.
.
bloat .
.
.
.
.
.
fop .
.
.
.
.
.
hsqldb .
.
.
.
.
.
luindex .
.
.
.
.
.
lusearch .
.
.
.
.
.
pmd .
.
.
.
.
.
xalan .
.
.
.
.
.
avrora .
.
.
.
.
.
sunflow .
.
.
.
.
.
euler .
.
.
.
.
.
moldyn .
.
.
.
.
.
montecarlo .
.
.
.
.
.
raytracer .
.
.
.
.
.
geomean .
.
edges and source code i.e.
number of classes and methods t hat are executed and instrumented by cachetor .
section b and c of the table report the overheads of cache tor.
the running time measured for our tool column t includes both the actual execution time and the time for the offline analyse s which occur before the jvm exits.
on average the tool incurs a .
overhead in execution time.
the additional peak memory cons umption is .
larger than that of the original program across all benchmarks.
this is due to the extra space for memory shadowi ng as well as the vc abstracted graph.
while both time and space overhead of our tool are too high for production executions they may be acceptable for performance tuning and debugging.
in additi on the high overhead has not prevented us from collecting data from realworld applications.
this paper focuses on the demonstratio n of the usefulness of the technique and future work may consider va rious optimization techniques such as sampling and selective pr ofiling of certain suspicious areas to reduce cachetor s overhead s. .
case studies we have inspected the analysis reports for all the benchmark s in table .
this section describes our studies on five of them montecarlo raytracer euler bloat andxalan .
it takes us weeks to conduct the studies.
we choose to report our experience with these benchmarks partly because they contain interesting repre sentative coding patterns leading to repeated computations of identi cal data values and partly because large performance improvements have been seen after the implementation of fixes e.g.
.
spac e reduction for montecarlo and .
running time reduction for euler .
these reports were generated under the same analysis confi guration and was chosen to be the selection ratio r described in algorithm .
we have compared the analysis reports gener ated under three different r i.e.
and and find that r appears to be a balance point between the amount of optimization opportunities that can be found and the difficulty o f developing fixes.
while r identifies truly optimizable data structures the size of each reported data structure is very small .
on the other hand r identifies large data structures which are often mixes of optimizable and non optimizable parts of the heap.
even though jikes rvm is the platform on which cachetor was implemented and the reports were generated the performance stat istics before and after the fixes were collected using java hotspot bit server vm build .
.0 27. hence the performance improveme nts we have achieved are beyond the compiler optimizations even in a commercial jvm.
in order to avoid compilation costs and exe cution noises each application is run times and the median s are compared and reported.
montecarlo is a financial simulation tool included in the java grande benchmark suite .
it uses monte carlo techniques t o price products derived from the price of an underlying asset .
the simulation generates sample time series with the sam e mean and fluctuation as a series of historical data.
the top two all ocation sites from d cachetor s report are located at lines and of classappdemo .
these allocation sites create seed objects and header objects respectively in order to construct a task which is then saved into a list.
after carefully inspecting the code we find that we are actually able to implement a singleton pattern fo r these allocation sites the list contains identical tasks and he nce one single instance of the task would suffice for the execution.
e ven though the dcms are relatively low because only part of the ob jects contains identical values the next three allocation sites are located inside loops and their executions create objects wi th completely disjoint lifetimes.
to optimize we hoist these sit es out of the loops and create a clone when required.
many call sites reported by m cachetor have high ccms .
.
among the top are call sites at line of method getresult and line of setinitalltasks in the class pricestock .
these calls return exactly the same objects every time they a re executed.
in addition the returned objects are used only as tem porary objects that carry multiple values from the callees to the ca llers.
we cache these objects into static fields to avoid re invokin g these calls.
further investigation of method getresult reveals that there is even no need for this method to create an object to hol d data.
originally the program stores all the returned data in to a list and wait to compute the final price.
we eliminate the list and c ompute the final price on the fly every time a new result is returne d. after implementing these fixes we have seen a .
reductio n in the peak memory consumption from 507268kb to 6320kb a .
reduction in the number of garbage collection runs fr om to a .
reduction in the time spent on gc from 461ms to 50ms and a .
reduction in the total running time fro m .146s to .678s .
each java grande benchmark reports an efficiency measurement based on a certain performance metric .
for montecarlo the fixed version has gained a .
improvement according to its own efficiency measurement.
raytracer is a java grande benchmark that measures the performance of a 3d ray tracer.
the scene contains spheres and is rendered at a resolution of pixels.
out of the top four allocation sites reported by d cachetor two sites are loca ted in 275methodshade .
we inspect the code and find that objects created at the allocation site at line always contain the same dat a and are discarded after the method returns.
the other allocatio n site is located at line even though its dcm is we cannot develo p a fix for it objects created by this site are returned by metho d shade which is a recursive method.
the recursion prevents us from caching an instance of the allocation site in a field.
we additionally find that many allocation sites in the report create large numbers of objects of type vec each of which stores the results of certain computations in a method and is then retur ned to the caller of the method.
while the dcms of these allocatio n sites are not very high i.e.
only certain fields of the objec ts contain identical values we manage to reuse one vec instance across all these allocation sites and reset its content if necessar y. we also find that in class sphere objects referenced by its field vec b always have the same data content because the field is comple tely unused after being initialized.
thus we remove this field fr om the class.
all these fixes have led to a .
reduction in the runn ing time from .595s to .034s a .
reduction in the numb er of gc runs to and a .
reduction in the gc time 39ms to 27ms .
the benchmark specific efficiency i.e.
the num ber of pixels per second is improved by .
from .
to .
.
only a .
reduction is seen in the peak memory con sumption.
euler is a java grande benchmark that solves the time dependent euler equations for the flow from a channel employing a structured irregular mesh.
the top allocation site reported by d cachetor is at line of class tunnel initializing a matrix with elements typed statevector .
this allocation site has a .
dcm.
after inspecting the code we realize that immediately after this initialization point the element of this matrix is rep laced by anotherstatevector object created in method calculatedamping .
hence we can safely remove this allocation site.
the matrix element replacement leads us to inspect the method calculatedamping .
in this method three statevector objects are used as value containers for computations and die a fter the method returns.
while these three allocations sites do n ot have very high dcms i.e.
only around .
we come up with a fix by creating three static fields one for each allocation site to cache its instance.
a similar fix is employed for three vector2 objects in methodcalculatedummycells .
the report of m cachetor reveals more optimization opportu nities.
among the top call sites are those that invoke method svect of classstatevector .svect returns astatevector object that may be saved in the matrix created in class tunnel mentioned above.
these calls are located inside a loop and thus they cr eate large numbers of objects at run time.
we develop a fix that make s svect return one single statevector object and create a clone only when it receives different values or is assigned to the m atrix.
altogether these fixes have reduced the running time from .
44s to .246s .
number of gc runs from to .
and the gc time from 46ms to 25ms .
.
no significant reduction is seen in the peak memory consumption.
there is a .
efficiency improvement after the fixes are implemented.
bloat is a java byte code optimizer and analysis tool.
the report of d cachetor points to the heavy use of the visitor patt ern many of the top allocation sites are related to visitor class es such astreevisitor andcomponentvisitor .bloat declares a visitor class for each program entity and creates an object of the class to visit each entity object.
we find that these visit or objects contain the same auxiliary data and their lifetimes ar e completely disjoint.
to optimize we manually implement the si ngleton pattern for each visitor class and use a single visitor ob jectto visit all program entities of the same type.
cachetor also reports that many allocation sites frequently create constan t arrays such as those at lines and of class tree lines and of class codegenerator lines and of class typeinferencevisitor and line of class ssagraph .
these arrays contain offset values and serve no other purpos es.
hence we can safely cache these arrays into static fields.
th e running time and the peak memory consumption are reduced by .
from 26239ms to 22809ms and .
from 177286kb to 154933kb respectively.
no significant reduction is see n in the number of gc runs and total gc time.
xalan is an xslt processor for transforming xml documents.
the top allocation sites on d cachetor s report are at lines and of class nodesequence where the content of a newlycreatednodevector object is only used to initialize another object.
lines and of class xpathcontext createnodevector objects whose content are always the same are also among the top results.
we fix the problems by using static fields to ca che the instances of nodevector created by these allocation sites.
the same fix can be applied to the allocation site at line of classsax2dtm .
we also inspect call sites that have high ccms from m cachetor s report.
as these calls have very simple in puts we perform lightweight profiling to understand what the freq uent inputs and outputs are for them.
then we cache these frequent inputs and outputs into a hashmap upon the execution of the cal l site we first query the hashmap to see if the input of the call i s in the map.
if it is the result is directly retrieved and used o therwise the call still needs to be executed.
these fixes have resulted in a .
reduction in the running time from 8321ms to 7889ms .
n o reduction is seen in the memory consumption.
table numbers of false positives identified in the reports of d cachetor andm cachetor .
program d cachetor m cachetor bloat xalan euler montecarlo raytracer .
false positives table lists the numbers of false positives identified among the top allocations sites in the reports of d cachetor and m c achetor.
if the total number of reported allocation call sites is sma ller than that number is shown in parentheses.
an allocation call site is classified as a false positive if either a it is clearly not c acheable or b we could not develop a fix to cache the data.
based on our studies we have identified the following four sources of false positives.
the first source is the handling of floating point v alues.
casting floating point values into integers before applying the hash function causes cachetor to mistakenly classify different run time values into the same slot.
future work may develop a lossless encoding for floating point values using for example the iee e floating point single format bit layout .
the second source of false positives is the context sensiti ve reporting of allocation sites.
associating calling contexts with instructions is extremely important for cachetor to distingu ish objects based on their logical data structures.
however repo rting cacheability of allocation sites separately for different contexts makes it difficult for the developer to understand and fix problems.
this is the case especially if the dcms of an allocation site differ s ignificantly under different contexts.
future work may address th is prob276lem by recording richer calling context information which w ould allow the user to recover a context from the encoded number.
the third source is the missing of the actual values that are f requently produced in our analysis reports.
for example m ca chetor reports call sites with high ccms but fails to provide infor mation regarding what the frequent inputs and outputs are for the ca ll sites.
the developer has to do additional profiling to understand th ese values in order to develop fixes.
while some extra effort is ne eded to find such frequent input and output values this profiling i s very lightweight and can be quickly implemented.
we implement su ch profiling for almost every call site we inspect and the burde n of time is negligible.
eventually value hashing may give rise to false positives.
in d cachetor s report for montecarlo we have found a few allocation sites that have high dcms but do not really contain ident ical values.
one example is an allocation site that creates seed o bjects.
our code inspection reveals that a seed object is created bas ed on a sequence of numbers each of which is a multiple of .
this hard coded number is coincidentally the same as the value of kwe chose causing instruction instances producing differe nt values to be mapped into the same slot.
however hashing induce d false positives are not common and montecarlo is the only program where such misclassification was found.
future work may alle viate the problem by comparing the reports generated under differ entks and selecting only the common top allocation sites.
.
related work software bloat analysis dufour et al.
propose dynamic metrics for java which provide insights by quantifying ru ntime bloat.
mitchell et al.
structure behavior according to the flow of information though using a manual technique.
their aim i s to allow programmers to place judgments on whether certain cla sses of computations are excessive.
their follow up work in troduces a way to find data structures that consume excessive amo unts of memory.
work by dufour et al.
finds excessive use of temporary data structures and summarizes the shape of the se structures.
in contrast to the purely dynamic approximatio n introduced in our work they employ a blended escape analysis whi ch applies static analysis to a region of dynamically collecte d calling structure with observed performance problem.
by approxima ting object effective lifetimes the analysis has been shown to b e useful in classifying the usage of newly created objects in the prob lematic program region.
object equality profiling oep is a run time technique that discovers opportunities for replacing a set of equivalent o bject instances with a single representative object to save space.
j olt is a vm based tool that uses a new metric to quantify object churn and identify regions that make heavy use of temporary object s in order to guide aggressive method inlining.
work by xu et al.
finds copy and container related inefficienc ies.
jin et al.
from studies performance bugs in real world software systems.
these bugs are analyzed to extract efficiency rules which are then applied to detect problems in other applications.
r ecent work by nistor et al.
detects performance problems using similar memory access patterns.
xu proposes a technique t o find reusable data structures.
the technique gives a three leve l reusability definition and encodes instances shapes and data cont ent of run time data structures to find optimization opportunitie s. unlike all the existing work on performance problem detection cac hetor is the first attempt to use an abstracted dynamic dependence g raph to find caching opportunities.
control and data based profiling profiling techniques have been proposed for various optimization and software engine eringtasks these techniques include dynamic dependence profile s control flow profiles and value profiles .
research fr om studies the compressed representations of control flow t races.
value predictors are proposed to compress value profiles which can be used to perform various kinds of tasks such as code spec ialization data compression value encoding an d value speculation .
research from proposes a technique t o compress an address profile which is used to help prefetch data and to find cache conscious data layouts .
zhang and gupta propose whole execution traces that include complete data information of an execution to enable the mining of behavior t hat requires understanding of relationships among various profil es.
ammons et al.
develops a dynamic analysis tool to explore calling context trees in order to find performance bottlenecks.
srin ivas et al.
use a dynamic analysis technique that identifies important program components also by inspecting calling contex t trees.
chameleon is a dynamic analysis tool that profiles conta iner behaviors to provide advice as to the choices of appropriate containers.
the work in proposes object ownership profilin g to detect memory leaks in java programs.
dynamic dependence analysis and slicing since first being proposed by korel and laski dynamic slicing has inspir ed a large body of work on efficiently computing slices and on appl ications to a variety of software engineering tasks.
a general d escription of slicing technology and challenges can be found in tip s survey and krinke s thesis .
the work by zhang et al.
has considerably improved the state of the art in dyna mic slicing.
the work from is more related to our work in that the proposed event based slicing approach uses pre defined eve nts to merge dependence graph nodes.
however this work targets au tomated program debugging whereas the goal of the proposed wo rk is to find caching opportunities to improve performance.
.
conclusions this paper presents a novel dynamic analysis tool called ca chetor that profiles a program to help the developer find cach ing opportunities for improved performance.
cachetor contain s three different cacheable data detectors i cachetor d cachet or and mcachetor that find cacheable data at the instruction data structure and call site level respectively.
to make cachetor scal e to realworld programs we develop a novel dynamic technique that co mbines value profiling and dynamic dependence analysis in a no vel way so that distinct values are used to abstract instruction instances.
based on the abstracted dependence graph we develop three o ffline analyses to compute cacheability measurements that wi ll be used by the three detectors to generate analysis report.
we h ave implemented cachetor on jikesrvm and evaluated it on a set of large scale real world applications.
our experimental r esults show that the overhead of cachetor is large but acceptable and la rge optimization opportunities can be quickly found by inspectin g its reports.
.