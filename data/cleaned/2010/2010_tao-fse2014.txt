automatically generated patches as debugging aids a human study yida tao1 jindae kim1 sunghun kim and chang xu 1dept.
of computer science and engineering the hong kong university of science and technology hong kong china 2state key lab for novel software technology and dept.
of computer sci.
and tech.
nanjing university nanjing china fidagoo jdkim hunkim g cse.ust.hk changxu nju.edu.cn abstract recent research has made signi cant progress in automatic patch generation an approach to repair programs with less or no manual intervention.
however direct deployment of auto generated patches remains di cult for reasons such as patch quality variations and developers intrinsic resistance.
in this study we take one step back and investigate a more feasible application scenario of automatic patch generation that is using generated patches as debugging aids.
we recruited participants for a controlled experiment in which they performed debugging tasks with the aid of either buggy locations i.e.
the control group or generated patches of varied qualities.
we observe that a high quality patches signi cantly improve debugging correctness b such improvements are more obvious for di cult bugs c when using low quality patches participants debugging correctness drops to an even lower point than that of the control group d debugging time is signi cantly a ected not by debugging aids but by participant type and the speci c bug to x. these results highlight that the bene ts of using generated patches as debugging aids are contingent upon the quality of the patches.
our qualitative analysis of participants feedback further sheds light on how generated patches can be improved and better utilized as debugging aids.
categories and subject descriptors d. .
debugging aids general terms experimentation human factors keywords debugging automatic patch generation human study corresponding authors permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
fse november hong kong china copyright acm ... .
.
.
introduction debugging is of paramount importance in the process of software development and maintenance given tens or even hundreds of bugs reported daily from individual projects .
however debugging is dauntingly challenging as bugs can take days or even months to resolve .
to alleviate developers burden of xing bugs automatic patch generation is proposed to repair programs with less or no manual intervention.
recent research in this area has made signi cant progress .
le goues et al.
proposed a technique based on evolutionary computation and used it to successfully repair out of real world bugs .
other advanced techniques such as runtime modi cations program synthesis using code contracts and formal speci cations and pattern adaptation have also produced promising results in terms of the repair success rate.
despite these fruitful research outcomes direct deployment of auto generated patches in production code seems unrealistic at this point.
kim et al.
suggested that generated patches are sometimes non sensical and thus less likely to be accepted by developers .
research has also identied other reasons why developers are not comfortable with blindly trusting auto generated code.
for example generated code might be less readable and maintainable .
rather than direct deployment a more feasible application of auto generated patches would be to aid debugging since they not only pinpoint buggy locations but also suggest candidate xes.
yet developers can still judge whether candidate xes are correct and whether to adapt or discard generated patches.
a natural question thus arises from the above scenario are auto generated patches useful for debugging ?
in addition generated patches vary in quality which a ects both the functional correctness and future maintainability of the program being patched .
research has also found that the quality of automated diagnostic aids a ects users reliance and usages of them .
as such does a patch s quality a ect its usefulness as a debugging aid ?
in this paper we conduct a large scale human study to address these two questions.
we recruited participants including graduate students software engineers and amazon mechanical turk mturk users to individually x ve real bugs.
each bug was aided by one of the three hints its low quality orhigh quality generated patch or its buggy location method name as the control condition.
in total we collected patches submitted by the participants.grad engr mturk section .
bug selection highquality generated patch lowquality generated patch location the control group section .
debugging aids section .
participants patches section .
evaluation measures section .
study design feedback debugging time ground truth patches figure overview of the experimental process.
we create debugging tasks using ve real bugs.
each bug is aided by one of the three hints its generated patch of low or high quality or its buggy location.
our participants include computer science graduate students software engineers and amazon mechanical turk mturk users.
graduate students debug using the eclipse ide in onsite sessions while software engineers and mturk participants debug using our web based debugging system.
we evaluate participants patch correctness debugging time and exit survey feedback.
we observe a signi cant increase in the debugging correctness when participants were aided by the high quality generated patches and such an improvement was more obvious for di cult bugs.
nevertheless participants were adversely in uenced by the low quality generated patches as their debugging correctness dropped surprisingly to an even lower point than the control group.
this nding urges a strict quality control for generated patches if we were to use them as debugging aids.
otherwise incorrect or unmaintainable patches can indeed cloud developers judgement and deteriorate their debugging correctness.
interestingly the types of debugging aid have only marginal in uence on participants debugging time while the types of bugs and participant populations instead have greater in uence.
for example we observe a signi cant slowdown in debugging time for the most di cult bug and a signi cant speed up for software engineers and mturk participants compared to graduate students.
we also qualitatively analyze participants attitudes toward using auto generated patches.
participants gave generated patches credit for quick problem identi cation and simpli cation.
however they remained doubtful about whether generated patches can x complicated bugs and address root causes.
participants also claimed that they could have been more con dent and smooth in using generated patches if they had deeper understanding of the buggy code its context and its test cases.
overall this paper makes the following contributions.
a large scale human study for assessing the usefulness of auto generated patches as debugging aids.
an in depth quantitative evaluation on how debugging is in uenced by the quality of generated patches and other factors such as the types of bugs and participants.
a qualitative analysis of patch usages which sheds light on potential directions of automatic patch generation research.
the remainder of this paper is organized as follows.
section introduces our experimental process.
section and section report and discuss the study results.
section presents threats to validity followed by a survey of related work in section .
section concludes the paper.
.
experimental process this section presents detailed experimental process following the overview of figure .
.
bug selection and debugging aids to evaluate the usefulness of auto generated patches in debugging we select bugs that satisfy the following criteria to simulate real debugging scenarios real world bugs are favored over seeded ones and the diversity of bug types is preferred.
patches of varied qualities can be automatically generated for the chosen bugs.
patches written and accepted by the original developers should be available as the ground truth to evaluate participants patches.
a proper number of bugs should be selected to control the length of the human study .
accordingly we selected ve bugs reported by kim et al.
and summarize them in table .
four of the bugs are from mozilla rhino1and one is from apache commons math2.
these bugs manifest di erent symptoms and cover various defect types and all of them have been xed by developer written and veri ed patches.
kim et al.
reported that all ve bugs can be xed by two state of the art program repair techniques genprog and par such that their generated patches can pass all the corresponding test cases .
kim et al.
launched a survey in which developers and cs students were asked to rank the acceptability of the generated patches for each bug .
table reports these ten generated patches two patches for each bug along with their average acceptability rankings reported by kim et al.
.
while the generated patches are equally quali ed in terms of passing test cases humans rank patch acceptability by further judging their x semantics and understandability which are the primary concerns of patch quality .
hence we inherited this acceptability ranking reported by kim et al.
as an indicator for patch quality.
speci cally for the two patches generated for each bug we labeled the one with a higher ranking as high quality and the other one with a lower ranking as low quality .
since generated patches already reveal buggy locations and suggest candidate xes for fair comparison we provide names of buggy methods to the control group instead of leaving it completely unaided.
this design conforms to real debugging scenarios where developers are aware of basic faulty regions for example from detailed bug reports .
description of the ve bugs used in the study.
we list semantics of developers original xes which are used to evaluate the correctness of participants patches.
bug buggy code fragment and symptom semantics of original developers patchmath public static double bracket ... double lowerbound double upperbound ... if fa fb .
throw new convergenceexception ... convergenceexception is thrown at line .
iffa fb .
then the method bracket should terminate without convergenceexception regardless of given lowerbound and upperbound values.
iffa fb .
then the method bracket should throw convergenceexception .rhino if lhs undefined lhs strings arrayindexoutofboundsexception is thrown at line .
ifgetshort icode pc returns a valid index then strings should be assigned to lhs.
ifgetshort icode pc returns the program should return normally without aiobe and lhsshould beundefined .rhino 1922261private void visitregularcall node node int type node child boolean firstargdone ... string simplecallname null if type !
tokenstream .
new simplecallname getsimplecallname node ... child child .
getnext .
getnext nullpointerexception is thrown at line .
statements in the block of ifstatement is executed only if firstargdone isfalse .
simplecallname should be initialized with a return value of getsimplecallname node .rhino for int i i parencount i substring sub substring parens .get i args sub.
tostring nullpointerexception is thrown at line .
ifparens.get i is not null assign it to args for all i ...parencount .
the program should terminate normally without nullpointerexception when subis null.rhino 766831for int i num i state .
parencount i state .
parens .
length 3state .
parencount num nullpointerexception is thrown at line .
the program should terminate without nullpointerexception even if state.parens is equal to null.
ifforstatement is processing the array state.parens it should be processed from index num to state.parencount .
for simplicity we refer to participants given di erent aids as the lowq low quality patch aided highq high quality patch aided and location buggy location aided group.
.
participants grad we recruited cs graduate students from the hong kong university of science and technology and from nanjing university.
graduate students typically have some programming experience but in general they are still in an active learning phase.
in this sense they resemble novice developers.
engr we also invited industrial software engineers to represent the population of experienced developers.
in this study we recruited software engineers from companies via email invitations.
mturk we further recruited participants from amazon mechanical turk a crowdsourcing website for requesters to post tasks and for workers to earn money by completing tasks .
the mturk platform potentially widens the variety of our participants since its workers have diverse demographics such as nationality age gender education and income .
to safeguard worker quality we prepared a buggy code revised from apache commons collection issue and asked workers to describe how to x it.
onlythose who passed this qualifying test could proceed to our debugging tasks.
we nally recruited mturk workers including developers two undergraduate students and one it manager with on average .
years of java programming experience.
.
study design we provided detailed bug descriptions from mozilla and apache bug reports to participants as their starting point.
we also provided developer written test cases which included the failed ones that reproduced the bugs.
although participants were encouraged to x as many bugs as they could they were free to skip any bug as the completion of all ve tasks was not mandatory.
to conform to participants di erent demographics we adopted the onsite and online settings as introduced below.
onsite setting applies to the graduate students.
we adopted a between group design by dividing them into three groups each group was exposed to one of the three debugging aids.
prior to the study we asked students to selfreport their java programming experience and familiarity with eclipse on a point likert scale.
we used this information to evenly divide them into three groups with members having similar levels of expertise.
finally the lowq andtable the two generated patches for each bug.
the middle column shows the patches acceptability ranking reported by kim et al.
.
the patch with a higher ranking or lower bar is labeled as high quality and its counterpart for the same bug is labeled as low quality .
bug high quality patch rank low quality patchmath 280public static double bracket ... double lowerbound double upperbound ... if fa fb .
fa fb .
throw new convergenceexception ... high q low qpublic static double bracket ... double lowerbound double upperbound ... if function null throw mathruntimeexception .
createillegalargumentexception ...rhino 114493if lhs undefined if getshort icode pc strings .
length getshort icode pc lhs strings getshort icode pc high q low qif lhs undefined lhs scriptable lhs .
getdefaultvalue null rhino 192226private void visitregularcall node node int type node child boolean firstargdone ... string simplecallname null if type !
tokenstream .
new if !
firstargdone simplecallname getsimplecallname node ... high q low qprivate void visitregularcall node node int type node child boolean firstargdone ... string simplecallname null visitstatement node rhino 217379for int i i parencount i substring sub substring parens .get i if sub !
null args sub.
tostring high q low qfor int i i parencount i substring sub substring parens .get i args new integer reimpl .
leftcontext .
length rhino 76683for int i num i state .
parencount i if state !
null state .
parens !
null state .
parens .
length state .
parencount num high q low qfor int i num i state .
parencount i state .
parencount num highq groups each had participants and the location group had participants.
the graduate students used the eclipse ide figure 2a .
we piloted the study with cs undergraduate students and recorded their entire eclipse usage with screen capturing software.
from this pilot study we determined that two hours should be adequate for participants to complete all ve tasks at a reasonably comfortable pace.
the formal study session was supervised by one of the authors and two other helpers.
at the beginning of the session we gave a minute tutorial introducing the tasks.
then within a maximum of two hours participants completed the ve debugging tasks in their preferred order.
online setting applies to the software engineers and mturk participants who are geographically unavailable.
we created a web based online debugging system3that provided similar features to the eclipse workbench figure 2b .
unlike grad it was unlikely to determine beforehand the total number of these online participants and their expertise.
hence to minimize individual di erences we adopted within group design such that participants can be exposed to di erent debugging aids .
to balance the experimental conditions we assigned the type of aids to each selected bug in a round robin fashion such that each aid was equally likely to be given to each bug.
we piloted the study with cs graduate students.
they were asked to use our debugging system and report any encountered problems.
we resolved these problems such as browser incompatibility before formally inviting software engineers and mturk users.
an exit survey is administered to all the participants upon their task completion.
in the survey we asked the participants to rate the di culty of each bug and the helpfulness of the provided aids on a point likert scale.
also we asked them to share their opinions on using auto generated patches in a free textual form.
we asked engr and mturk participants to self report their java programming experience and debugging time for each task since the online system cannot monitor the event when they were away from the keyboard discussed in section .
in addition we asked mturk participants to report their occupations.
we summarize our experimental settings in table .a b c d e a the eclipse workbench.
d b c e a b the online debugging system.
figure the onsite and online debugging environments which allow participants to browse a bug s description a edit the buggy source code b view the debugging aids c run the test cases d and submit their patches e .
.
evaluation measures we measure the participants patch correctness debugging time and survey feedback as described below.
correctness indicates whether a patch submitted by a participant is correct or not.
we considered a patch correct only if it passed all the given test cases and also matched the semantics of the original developers xes as described in table .
two of the authors and one cs graduate student with years of java programming experience individually evaluated all patches.
initially the three evaluators agreed on of patches.
the disagreement mainly resulted from misunderstanding of patch semantics.
after clari cation the three evaluators carried out another round of individual evaluations and this time agreed on out of patches.
this was considered to be near perfect agreement as its fleiss kappa value equals to .
.
for the remaining patches the majority s opinion i.e.
two of the three evaluators was adopted as the nal evaluation result.
debugging time was recorded automatically.
for the onsite session we created an eclipse plug in to sum up the elapsed time of all the activities e.g.
opening a le modifying the code and running the tests related to each bug.
for online sessions our system computes participants debugging time as the time elapsed from them entering the website until the patch submission.
feedback was collected from the exit survey section .
.
we particularly analyzed participants perception about the helpfulness of generated patches and their free form answers.
these two items allowed us to quantitatively and qualitatively evaluate participants opinions on using generated patches as debugging aids section .
and section .table the study settings and designs for di erent types of participants along with their population size and number of submitted patches.
in cases where participants skipped or gave up some tasks we excluded patches with no modication to the original buggy code and nally collected patches for analysis.
settings designs size patches grad onsite eclipse between group engr online website within group mturk online website within group total .
results we report participants debugging performance with respect to di erent debugging aids section .
.
we then investigate debugging performance on di erent participant types section .
bug di culties section .
and programming experience section .
.
we use regression models to statistically analyze the relations between these factors and the debugging performance section .
and nally summarize our observations section .
.
.
debugging aids among the patches we collected and were aided by buggy locations lowq and highq patches respectively.
figure shows an overview of the results in terms of the three evaluation measures.
correctness figure 3a patches submitted by the location group is correct.
the percentage of correct patches dramatically increases to for the highq group.
lowq patches do not improve debugging correctness.
on the contrary the lowq group performs even worse than the location group with only patches being correct.
debugging time figure 3b the highq group has an average debugging time of .
minutes which is slightly faster than the .
and .
minutes of the location and lowq groups.
feedback figure 3c highq patches are considered much more helpful for debugging compared to lowq patches.
mann whitney u test suggests that this di erence is statistically signi cant p value .
.
.
to understand the reasons behind this result we further analyze participants textual answers and discuss the details in section .
.
participant types our three types of participants engr grad and mturk have di erent demographics and are exposed to di erent experimental settings.
for this reason we investigate the debugging performance by participant types.
figure 5a shows a consistent trend across all three participant types the highq group makes the highest percentage of correct patches followed by the location group while the lowq group makes the lowest.
note that grad has a lower correctness compared to engr and mturk possibly due to their relatively short programming experience.
figure 5b reveals that engr debugs faster than grad and mturk participants.
yet among the participants from the same population their debugging time does not vary too much between the location lowq and highq groups.
location lowq highq a of correct patches location lowq highq b debugging time min not helpful slightly helpful medium helpful very helpful lowq highq c figure a the percentage of correct patches by di erent debugging aids b debugging time by di erent aids c participants perception about the helpfulness of lowq and highq patches.
.
bug difficulty since the ve bugs vary in symptoms and defect types they possibly pose di erent levels of challenges to the participants.
research also shows that task di culty potentially correlates to users perception about automated diagnostic aids .
for this reason we investigate participants debugging performance with respect to di erent bugs.
we obtained the di culty ratings of bugs from the exit survey.
note that we adopt the di culty ratings from only the location group since other participants perceptions could have been in uenced by the presence of generated patches.
as shown in figure rhino is considered to be the most di cult bug while rhino is considered to be the easiest.
in particular anova with tukey hsd post hoc test shows that the di culty rating of rhino 1922264is signi cantly higher than that of the remaining four bugs.
figure 5c shows that for bug3 the highq group made a landslide victory with of the submitted patches being correct while no one from the location or the lowq group was able to x it correctly.
one primary reason could be the complex nature of this bug which is caused by a missingif construct .
this type of bug also known as the code omission fault can be hard to identify .
fry and weimer also empirically showed that the accuracy of human fault localization is relatively low for missingconditional faults .
in addition the x location of this bug is not the statement that throws the exception but ve lines above it.
the code itself is also complex for involving lexical parsing and dynamic compilation.
hence both the location and lowq groups stumbled on this bug.
however the highq group indeed bene ted from the generated patch as one participant explained without deep knowledge of the data structure representing the code being optimized the problem would have taken quite a long time to determine by manually.
the patch precisely points to the problem and provided a good hint and my debugging time was cut down to about minutes.
for bug4 interestingly the location group performs better than the lowq and even the highq group figure 5c .
one possible explanation is that the code of this bug requires less project speci c knowledge and its x 4for simplicity in the remaining of the paper we refer to each bug using its order appearing in table .
very easy2 easy3 medium4 difficult5 very difficult bug1 math 280bug2 rhino 114493bug3 rhino 192226bug4 rhino 217379bug5 rhino 76683figure the average di culty rating for each bug.
by adding a null checker is a common pattern .
therefore even without the aid of generated patches participants were likely to resolve this bug solely by themselves and they indeed considered it to be the easiest bug figure .
for bug5 the lowq patch deletes the entire buggy block inside the for loop which might raise suspicion for being too straightforward.
this possibly explains why the lowq and location groups have the same correctness for this bug.
in general the highq group has made the highest percentage of correct patches for all bugs except bug4.
the lowq group on the other hand has comparable or even lower correctness than the location group.
as for debugging time participants in general are slower for the rst three bugs as shown in figure 5d.
we speculate that this also relates to the di culty of the bugs as these three bugs are considered to be more di cult than the other two figure .
.
programming experience research has found that expert and novice developers are di erent in terms of debugging strategies and performance .
in this study we also explore whether the usefulness of generated patches as debugging aids is a ected by participants programming experience.
among all participants reported their java programming experience.
they have up to and on average .
years of java programming experience.
accordingly we divided these participants into two groups experts with above average and novices with below average programming experience.
as shown in figure 5e experts are correct when aided by location.
this number increases to when they are aided by highq patches and decreases to when they0 engr grad mturk a of correct patches bug1 bug2 bug3 bug4 bug5 c experts novices e debugging aid location lowq highq engr grad mturk b debugging time min bug1 bug2 bug3 bug4 bug5 d experts novices f debugging aid location lowq highqfigure percentage of correct patches and debugging time by participants bugs and programming experience.
are aided by lowq patches.
novices manifest a similar yet sharper trend their percentage of correct patches increases substantially from to when they are aided by location and highq patches respectively.
interestingly when aided by highq patches the debugging correctness of novices is almost as good as that of the experts.
figure 5f shows that novices also spend less time debugging with highq patches than with location and lowq patches.
.
regression analysis prior subsections observe that changes in debugging correctness and time are attributable to debugging aids bugs participant types programming experience and their joint e ects.
to quantify the e ects of these factors on the debugging performance we perform multiple regression analysis.
multiple regression analysis is a statistical process for quantifying relations between a dependent variable and a number of independent variables which ts naturally to our problem.
multiple regression analysis also quanti es the statistical signi cance of the estimated relations .
this type of analysis has been applied in other similar research for example to analyze how con gurational characteristics of software teams a ect project productivity .
debugging aids participant types and bugs are all categorical variables with di erent values respectively.
since categorical variables cannot be directly used in regression models we apply dummy coding to transform a categorical variable with kvalues intok binary variables .
for example for the debugging aids we create two binary variables highq and lowq one of them being if the corresponding patch is given.
when both variables are the location aid is given and serves as the basis to which the other two categories are compared.
in the same manner we dummy code participant types and bugs with grad and bug4 as the basis categories5.
below shows the equations that form our regression models.
since patch correctness 5the selection of basis category is essentially arbitrary and does not a ect the regression results .is binary data while debugging time is continuous data we estimate equation using logistic regression and equation using linear regression .
note that we log transform debugging time to ensure its normality .
logit correctness highq lowq engr mturk bug1 bug2 bug3 bug5 javayears ln time highq lowq engr mturk bug1 bug2 bug3 bug5 javayears table shows the regression results with the rst number as the variable coe cient and the second number in the parenthesis as its p value.
in general a positive coe cient means the variable row name improves the debugging performance column name compared to the basis category while a negative coe cient means the opposite.
since the logistic regression computes the log of odds for correctness and we log transform debugging time in the linear regression both computed coe cients need to be anti logged to interpret the results.
we highlight our ndings below.
highq patches signi cantly improve debugging correctness with coef .
and p value .
.
.
to put this into perspective holding other variables xed the odds of the highq group making correct patches is e1 .
times that of the location group.
the lowq group on the other hand is less likely to make correct patches though this trend is not signi cant coef .
p .
.
the aid of highq coef .
p .
and lowq patches coef .
p .
both slightly reduce the debugging time but not signi cantly.
di cult bugs signi cantly slow down debugging as bug1 all have signi cant positive coe cients or longer debugging time.
for the most di cult bug3 its debuggingtable regression results.
arrows indicate the direction of impact on debugging performance.
double arrows indicate that the coe cient is statistically signi cant at level.
correctness debugging time coef p value coef p value highq .
.
.
.
lowq .
.
.
.
bug1 .
.
.
.
bug2 .
.
.
.
bug3 .
.
.
.
bug5 .
.
.
.
engr .
.
.
.
mturk .
.
.
.
java years .
.
.
.
time ise0 .
times that of bug4 yet the odds of correctly xing it is only e of the odds of correctly xing bug4.
participant type and experience signi cantly affect debugging performance compared to grad engr and mturk are much more likely to make correct patches coef .
and .
with p .
.
engr is also signi cantly faster spending approximately e time debugging.
also more java programming experience signi cantly improves debugging correctness coef .
p .
.
our regression analysis thus far investigates the individual impact of debugging aids and other variables on the debugging performance.
however the impact of debugging aids also possibly depends on the participants who use the aids or the bugs to be xed.
to explore such joint e ects we add interaction variables to the regression model and use anova to analyze their explanatory powers .
below are our ndings.
highq patches are more useful for di cult bugs as shown in table in addition to the four single variables the debugging aids and bugs also jointly explain large variances in the patch correctness deviance reduction .
p .
.
hence we add this interaction variable aid bug to the logistic regression for estimating correctness.
table shows that highq bug3 has a relatively large positive coe cient coef .
indicating that the highq patch is particularly useful for xing this bug.
the type of aid does not a ect debugging time none of the interaction variables signi cantly contributes to the model .
interestingly neither does the debugging aid itself which explains only r2 .
of the variance in the debugging time.
instead debugging time is much more susceptible to participant types and bugs which further explain and of its variance respectively.
these regression results are consistent with those observed from figures and in the preceding subsections.
due to space limitation we elaborate more details of the regression analysis at x regression analysis.html which reports the descriptive statistics and correlation matrix for the independent variables the hierarchical regression results and how table and are derived.
.
summary based on our results we draw the following conclusions high quality generated patches signi cantly improve debugging correctness and are particularly bene cialtable anova for the regression models with interaction variables.
it shows how much variance in correctness debugging time is explained as each interaction variable is added to the model.
the p value in parenthesis indicates whether adding the variable signi cantly improves the model.
correctness debugging time variable resid.
deviance r2 variance reductions explained debuggingaid .
.
.
.
participant .
.
.
.
bug .
.
.
.
javayears .
.
.
.
aid participant .
.
.
.
aid bug .
.
.
.
aid javayears .
.
.
.
table coe cients of the interaction variable debugaid bug for estimating patch correctness.
the bold value is statistically signi cant at level.
bug1 bug2 bug3 bug4 bug5 highq .
.
.
.
.
lowq .
.
.
.
.
for di cult bugs.
low quality generated patches slightly undermine debugging correctness.
participants debugging time is not a ected by the debugging aids they use.
however their debugging takes a signi cantly longer time for di cult bugs.
participant type and experience also signi cantly affect debugging performance.
engr mturk or the participants with more java programming experience are more likely to make correct patches.
.
qualitative analysis we used the survey feedback to qualitatively investigate participants opinions on using auto generated patches in debugging.
in total we received textual answers.
after an open coding phase we identi ed twelve common reasons why participants are positive or negative about using autogenerated patches as debugging aids.
table summarizes these reasons along with the participants original answers.
participants from both the highq and lowq groups acknowledge generated patches to provide quick starting points by pinpointing the general buggy area p1 p3 .
highq patches in particular simplify debugging and speed up the debugging process p4 p6 .
however a quick starting point does not guarantee that the debugging is going down the right path since generated patches can be confusing and misleading n1 n2 .
even highq patches may not completely solve the problem and thus need further human perfection n3 .
participants regardless of using highq or lowq patches have a general concern that generated patches may overcomplicate debugging or over simplify it by not addressing root causes n4 n5 .
in other words participants are concerned about whether machines are making random or educated guesses when generating patches n6 .
another shared concern is that the usage of generated patches should be based on a good understanding to the target programs n7 n8 .table participants positive and negative opinions on using auto generated patches as debugging aids.
all of these sentences are the participants original feedback.
we organize them into di erent categories with the summary written in italic .
positive negative it provides a quick starting point .
p1 it usually provides a good starting point.
highq p2 it did point to the general area of code.
lowq p3 i have followed the patch to identify the exact place where the issue occurred.
lowq it can be confusing misleading or incomplete .
n1 the generated patch was confusing.
lowq n2 for developers with less experience it can give them the wrong lead.
lowq n3 it s not a complete solution.
the patch tested for two null