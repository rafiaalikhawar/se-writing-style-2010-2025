does bug prediction support human developers?
findings from a google case study chris lewis1 zhongpeng lin1 caitlin sadowski2 xiaoyan zhu3 rong ou2 e. james whitehead jr. 1university of california santa cruz 2google inc. 3xi an jiaotong university email fcflewis linzhp ejw g soe.ucsc.edu fsupertri rongou g google.com xyxyzh gmail.com abstract while many bug prediction algorithms have been developed by academia they re often only tested and verified in the lab using automated means.
we do not have a strong idea about whether such algorithms are useful to guide human developers.
we deployed a bug prediction algorithm across google and found no identifiable change in developer behavior.
using our experience we provide several characteristics that bug prediction algorithms need to meet in order to be accepted by human developers and truly change how developers evaluate their code.
i. i ntroduction the growth of empirical software engineering techniques has led to increased interest in bug prediction algorithms.
these algorithms predict areas of software projects that are likely to be bug prone areas where there tend to be a high incidence of bugs appearing these are also sometimes called fault prone areas .
the overall approach is statistical bug prediction algorithms are based on aspects of how the code was developed and various metrics that the code exhibits rather than traditional static or dynamic analysis approaches.
one common strategy is to use code metrics while another has focused on extracting features from a source file s change history relying on the intuition that bugs tend to cluster around difficult pieces of code .
evaluations of these algorithms have confirmed that they can successfully predict bug prone code areas.
such evaluations have typically involved the use of the information retrieval and machine learning metrics of accuracy precision recall f measure and roc auc the underlying rationale has been that high values for these metrics indicate a wellperforming algorithm.
the follow on assumption is that a wellperforming bug prediction algorithm provides useful information to developers.
since this is a foundational assumption for bug prediction one would expect that it has been well tested by multiple empirical studies of the use of bug prediction in software development settings.
it has not.
there is very little empirical data validating that areas predicted to be bug prone match the expectations of expert developers nor is there data showing whether the information provided by bug prediction algorithms leads to modification of developer behavior.
in this paper we focus on predictions at the granularity of files.
the goal of this paper is to investigate how developers respond to bug prediction algorithms via the following research questions rq according to expert opinion given a collection ofbug prediction algorithms how many bug prone files do they find and which algorithm is preferred?
rq what are the desirable characteristics a bug prediction algorithm should have?
rq using the knowledge gained from the other two questions to design a likely algorithm do developers modify their behavior when presented with bug prediction results?
to answer these questions we partnered with google s engineering tools department to evaluate various bug prediction algorithms and develop one for use at google.
we address rq using formal user studies rq through informal discussion with developers and rq through quantitative data collected from google s code review system.
we find that developers prefer bug prediction algorithms that expose files with large numbers of closed bugs and present a number of desirable characteristics that a bug prediction algorithm should have.
we also find that there was no significant change in developer behavior after our deployment of a bug prediction algorithm.
ii.
b ugprediction a. algorithm choice one of the most popular academic bug prediction algorithms at the time of writing is fixcache which won an acm sigsoft distinguished paper award at the international conference of software engineering .
fixcache is an algorithm which uses the idea of bug locality when a bug is found it is likely there will be more in that area of the code base.
using various localities fixcache creates a cache of files that are predicted to be bug prone at a particular commit.
when a file meets one of these locality criterions it enters the cache and an old file is replaced via a selectable policy commonly least recently used lru .
fixcache uses three localities if a file is recently changed added it is likely to contain faults churn locality if a file contains a fault it is likely to contain more faults temporal locality and files that change alongside faulty files are more likely to contain faults spatial locality .
fixcache s characteristics were extensively surveyed by rahman et al.
investigating how it can aid human inspection.
they concluded that the bug density of files in the cache is generally higher than outside the cache and that the temporal locality feature is what mostly contributes to fixcache s effectiveness.
they present findings that when bugprediction is used to support human inspection such as code review using the number of closed bugs to rank files from most bug prone to least bug prone works almost as well as fixcache.
we call this the rahman algorithm.
we decided that the fixcache and rahman algorithms were ideal for our study.
fixcache as well as performing well in the lab has been used with success in industry at ericsson .
the simplicity of the rahman algorithm provides an excellent counterpoint to the more complex fixcache and so these two algorithms complement each other well.
fixcache can be run with various parameters which were evaluated in the original paper to find optimal settings but we found that the cache size metric would not be usable in its current form.
b. fixcache cache size fixcache is found to work best when the cache size is set to store about of the files in the code base.
however after consultation with google engineers this number was deemed too large for most google projects.
mid size projects can run into order thousands of source files meaning hundreds of files will be flagged by fixcache as bug prone.
this load is too high for developers to properly process and would lead to the majority of files being worked on at any given time being flagged.
such exposure to the flag will lessen its impact and we believed developers would quickly learn to ignore it.
to combat this we settled on showing just the top files.
showing the top files is less trivial than it sounds.
fixcache has a binary understanding of bug propensity either files are bug prone or they are not.
there is no weighting of severity once files are in the cache they are not easily reordered to rank the most bug prone.
we identified two possible solutions reduce the cache size the first option is to reduce the cache size to only store files.
with such a small cache size it is conceivable that the number of files created in a day could fill it entirely with new files.
another issue with making the cache so small is that the cache hit rate decreases dramatically as it is highly likely that a file will not be in the cache.
this is not in and of itself negative as hit rate as a metric is not perfect but this has been the primary method of evaluation of fixcache in previous literature and could indicate that this solution is destined to perform poorly.
reducing the cache size biases the results towards newer files as older files are pushed out of the cache.
order the cache by some metric the second option is to keep the cache size at and then order the cache by a reasonable metric to ascertain bug propensity severity.
the chosen metric for this task was the duration which is the measure of how many total commits the file has been in the cache for.
files that leave the cache and then re enter it do not have their durations reset but continue where they left off.
ordering the cache biases the results towards older files as they have more time to increase their duration.project language source lines of code files interviewees a java b c table i characteristics of the investigated projects.
the source lines of code sloc were counted using the sloccount tool which ignores comments and blank lines.
both sloc and files have been rounded to one significant figure.
both options match two possible intuitions for which areas of a code base are more bug prone reducing the cache size highlights areas of immaturity whereas ordering the cache highlights areas which have dogged the project for a long period of time.
as neither presented a strong case over the other we chose to use both in our studies.
iii.
u ser studies a. overview to evaluate rq we decided to perform user studies in order to utilize developer knowledge in finding out whether files flagged as bug prone by an algorithm match developer intuition.
developers those who work on a project daily are the authoritative source on a code base.
while it is possible to produce and use metrics such as bug density we felt that going directly to developers would help us gain a holistic view of the code base that can sometimes be missing from data mining.
to do this we pitted the three algorithms fixcache with a cache set to which we will refer to as cache fixcache with its output ranked by duration duration cache and rahman against each other.
we performed developer studies with two anonymous google projects which we ll refer to as project a and project b. the characteristics of the projects can be seen in table i. each project has run for a number of years and these two were chosen based on their differing sizes and language.
in order to track which files are involved in bug fixes each algorithm requires knowledge of closed bugs to be extracted from bug tracking software.
to do this we extracted data from google s internal bug tracker.
however google does not mandate use of its internal tracker and teams may use any means of bug tracking that they wish.
this means projects that were not using the tracker would not be visible to these algorithms.
further there is evidence that bug trackers are biased and google s is likely no different .
there was no other discoverable means of finding this data so we had to rely on this source.
the bug tracker allows developers to classify tickets as bugs feature requests or a number of other classifications.
we only used tickets which were classified as a bug.
b. methodology interviews we selected two projects a and b and set out about recruiting interviewees from each of these projects.
projects a and b were chosen to provide a spread of variables a is in java b is in c .
a has fewer source lines than b butmore files overall.
both projects are relatively mature.
maturity meant that we had a greater amount of source history to feed each algorithm.
interviewees were recruited by emailing project leads or those interested in code health who were then responsible for disseminating news of the study to the other developers on their project.
v olunteers would contact us to say they were willing to perform in the study.
we recruited interviewees with varied experience levels ranging from between one to six years of experience on the projects studied.
each interviewee was met in private and then shown three lists of twenty files from the project that he or she worked on each list generated by one of the different algorithms.
each interviewee for a particular project saw the same lists but the lists were shuffled and handed out during the interview and the study was performed double blind neither the interviewee nor the interviewer knew which list was which.
interviewees were told that the lists were generated by bug prediction algorithms with varying parameters but were given no further details.
each interview took around minutes and interviewees were instructed to not discuss the experiment with others.
during the interviews interviewees were asked to mark each file as either a file that they did consider to be bug prone that they didn t consider to be bug prone that they had no strong feeling either way about which we will refer to as ambivalence or that they didn t have enough experience with to make a choice which we will refer to as unknown .
interviewees were told that bug prone means that the file may or may not have bugs now but that it has a certain set of characteristics or history that indicates that the file has a tendency to contain faults.
we very specifically avoided using a term that indicated the file had bugs currently like buggy as we felt many developers may feel unable to answer this question if they do not have a very strong current understanding of the file.
as the lists were not exclusive a file could appear in different lists interviewees were instructed to ensure they gave consistent answers across lists although interviewees were allowed to pick whichever method of ensuring this they felt most comfortable with.
most chose to do this by filling out the first list and then checking the other lists for duplicates.
others would mark a file and then immediately look to see if it appeared elsewhere.
finally each interviewee was asked to rank the entire lists themselves from the list they think most accurately reports the bug prone areas of the code to the list that is the least accurate.
as there were three lists this created three classifications that a list could have most bug prone middle bug prone and least bug prone.
interviewees were instructed that they did not have to take into account how they had marked the files on each list and were informed that they should look at the list holistically.
for example a file that a developer strongly feels should be included may be more important than three or four files that were marked bug prone but were less problematic in practice.
an example result would be that the developer marked duration cache as the file list that is most bug prone cache in the middle and rahman as the least bug prone list.
common questions interviewees raised two common questions what is a bug?
the most common question asked by interviewees was what bug actually meant with a significant minority requesting guidance on this issue.
the interviewer responded i want you to define the term for yourself.
self definition more specifically reflects the operation of the bug prediction algorithms we chose they do not give reasoning behind what a bug is or isn t instead using closed bug tickets as a way of inducing that a bug was present in the code beforehand.
some felt that certain classes of files such as templates or configuration files could not even contain bugs by their own definition although a well regarded taxonomy of bugs does include a classification for configuration faults .
can i mark files i don t work on?
the second most common question asked by interviewees was whether they could mark files based on feelings alone even if they had not worked on or even seen the given file.
many of these interviewees indicated that they had gathered from colleagues that a certain set of files was particularly difficult to work with.
as previously mentioned some classes of files were deemed unable to contain bugs at all and some interviewees felt they were able to confidently mark not bug prone on these files.
when asked if a file can be marked without direct interaction the interviewer responded that the interviewee should mark a file however he she felt comfortable to do so.
for the most part the interviewee would then continue to mark the file as bug prone or not bugprone rather than leave it as an unknown.
this led us to theorize that many developers on a large code base gain opinions of it via developer folklore and gut instinct rather than direct interaction and that they feel confident that these opinions are valid.
classification of responses we aimed to specifically analyze the bug prone and not bug prone responses.
ambivalence is treated as a non answer the interviewee could have marked bug prone or not bug prone so it is essentially a split decision between both.
we chose not to factor this answer into our results.
we use the unknown classification to differentiate between knowledge of the file but no answer ambivalence and having no knowledge of the file at all unknown .
file classification was performed as follows for a bug prone classification at least respondents must have indicated the file is bug prone and the bugprone responses must be at least double that of the not bug prone responses e.g.
if respondents mark a file as bug prone at most respondents can answer not bugprone .
for a not bug prone classification the same method as bug prone is used in reverse.
if the file is neither classified bug prone nor not bugprone and if at least respondents indicated they did not know the file the file is classified as unknown.
otherwise the file is marked as being disagreed upon.
while this represents only one of a multitude of classification possibilities we found this most accurately represented our instinctual interpretation of the raw survey data.
a simpler classifier such as majority wins will select more files as unknown as few developers have a strong understanding across the entire code base.
this would nullify signals given by the developers that are actually experienced enough with the files to give a bug prone or not bug prone answer.
c. results list quality figure display the results of each file classified in projects a and b. preferred list the preferred lists in figure show how developers ranked each list against each other from most bugprone to least bug prone.
notes on project a interviewee comments from project a indicated that one possible reason for the duration cache list unknown responses is because a number of files in the list that were old enough that most team members had never worked on them.
however developers did comment that they were aware of the files existence they just did not need to work with them.
two of the files marked as not bug prone in the rahman list one of which appeared in the cache list were files containing constants.
these files would need to be changed alongside various feature releases so would be included in bug fixing changes causing them to be flagged.
the developers we interviewed felt that these files could not be buggy by definition and marked them not bug prone.
for example one constants file resulted in a report of developer marking it bug prone marking it not bug prone and having no feeling either way.
four of the files in the cache list did not garner a single response about whether the file was or was not bug prone only recording a small number of ambivalent responses with the rest being given an unknown response.
this caused one developer to comment that the cache list looked random.
two of these files were pinpointed as prototype code.
when developers create small prototypes to perform experiments they are encouraged to commit these prototypes to source control so that any useful ideas generated are not lost.
however our algorithms had no means of identifying and filtering these files so they were pulled into the cache.
when participants discussed the different project a lists they would mention that the rahman algorithm seemed to be pulling out older files to which developers assigned terms such as monolithic has many dependents and hard to maintain .
project a had one file that appeared in two lists and one file that appeared in all three lists.
notes on project b project b responses were largely similar to those of project a and project b interviewees commented on the age of the files in the rahman list just as project a developers did these are old files that have gained a lot of cruft lots of people have changed these over the years we have to work carefully with these because of their dependents .
again as with project a neither fixcache list seemed to perform much better than the other but they both shared a large number of unknown responses.
the unknown files in the duration cache may have been marked as such for the same reason that project a identified the files were so old that developers simply didn t work with them anymore the duration cache is a list of old files that represents project a five years ago that is not relevant to today and it s like just a worse job .
it is worth noting that we did not identify any prototype code in any list for project b. project b had more files that appeared in more than one list than project a. eight files appeared in two lists and three files appeared in all three lists.
interpretation the rahman algorithm performed significantly better than the fixcache algorithms at identifying bugprone files.
both the duration cache and cache lists mostly contained unknown files.
only the rahman algorithm resulted in a list where interviewees both knew the files on the list and felt those files to be bug prone.
this result does not necessarily mean that the files identified in the duration cache and cache20 lists are not bug prone but that developers did not have enough experience with those files to make any comment.
the overall rankings show that most interviewees preferred the rahman algorithm.
this is unsurprising given the quality of the results we saw in its ability to match developer intuition at predicting bug prone files.
comments from interviewees did not provide any clear insight into why the duration cache list was the middle preference in project a and why the cache list was the middle preference in project b. iv.
d esirable algorithm characteristics a. overview in addition to our formal user study we had informal discussions with many developers about what they would like to see from a bug prediction algorithm to answer rq .
these discussions were typical watercooler conversations that arose naturally in places such as on mailing lists an internal google deployment in corridors at lunch and on the bus.
while these conversations were not formalized or coded they are typical of the google working environment and represent a common means of ascertaining developer feedback.
using input from these discussions we identified three main algorithm characteristics that a bug prediction algorithm should have in order to be useful when aiding developers during code review.
these three algorithm characteristics were mentioned in a majority of developer discussions and are supported by prior research on static analysis adoption and error messages.
dura on cache cache rahman bug prone not bug prone disagreement unknown a project a dura.on cache cache rahman bug prone not bug prone disagreement unknown b project b fig.
charts showing the number of files of a certain classification from project a and project b. files can be classified as bug prone or not as a disagreement between whether the file was or was not bug prone or the file was not known to interviewees.
a higher bug prone rating and lower not bug prone or disagreement ratings is better.
dura.on cache cache rahman most bug prone middle bug prone least bug prone a project a dura0on cache cache rahman most bug prone middle bug prone least bug prone b project b fig.
charts showing the rankings of the lists from participants in project a and project b. each bar shows the number of times the list was given a certain rank so in project a one developer chose the duration cache as the most bug prone two developers chose it as the middle and six developers chose it as the least bug prone list.
a higher most bug prone rating and a lower least bug prone rating is better.
we also identified two required algorithm scaling characteristics from our experiences deploying bug prediction at google.
to our knowledge there have been no articles on bug prediction enumerating these five characteristics although they are essential to ensure developer adoption.
b. algorithm characteristics actionable messages given the information that a code area is bug prone it may not be clear how to improve the software.
in other words there is a potential gulf of execution between bug prediction results and improving software artifacts based on those results.
the importance of suggesting solutions to problems or errors has also long been emphasized for usability .
prior research has also found that the description of a fault is important in deciding whether to fix it .
by far the most desired characteristic is that the outputfrom a bug prediction algorithm is actionable.
once a bugprone area has been identified the development team should be able to take clear steps that will result in the area no longer being flagged.
this matches developer intuition about how programming support tools such as type checkers or static analysis tools like findbugs should be integrated into a workflow.
for example findbugs is already in use at google and most categories of warnings cause over of users to investigate high priority warnings .
unfortunately fixcache s reliance on source history means there is nothing that can be done by a team to immediately unflag a file they must try to improve the health of the file then wait weeks or months for it to no longer collect bug reports.
this is particularly frustrating if the warning appears frequently which is possible if a bug prone piece of code is changed multiple times a week.
neither rahman nor fixcache has actionable messages but a related problem is that both will always flag a certain percentage of files so the code can never be given a clean bill of health.
this means that these algorithms never provide developers with any reward just a revolving door of problems which is likely very disheartening.
obvious reasoning when an area is flagged as bugprone there must be a strong visible and obvious reason why the flagging took place allaying any fear that the flag is a false positive that will only waste developer time.
if a developer is convinced that a tool outputs false positives he or she will ignore it from that point on.
the burden of proof is always on the tool to precisely elucidate why it has come to the conclusion it has.
this largely goes hand in hand with actionable messages a message is only actionable if there is a clear and obvious reason why the message appeared at all.
prior research also has emphasized that programmers must develop trust for analysis tools .
rahman s reasoning is clear and obvious any developer can quickly believe and verify the number of closed bugs on any given file.
in contract fixcache does not easily offer its reasoning it is often opaque as to why fixcache has pulled in a file even with a strong understanding of the algorithm and the input it has been given.
inspecting log messages will eventually allow a human to build a mental model of what happened and presumably this indicates that a tool could be written to explain why a file was in the cache but such a tool does not currently exist.
obvious reasoning does not necessarily require the developer to completely agree with the assessment only that he or she would at least be able to understand the tool s reasoning.
bias towards the new although in the user test users noted that they believe older files to be more bug prone a number of developers mentioned that they aren t actually worried about the files they know have technical debt.
instead developers were more concerned about files that are currently causing problems which may or may not include those older files .
if an algorithm doesn t have actionable messages it should probably bias itself towards newer issues rather than taking into account older ones which could be a number of years old and not indicative of the current state of the code .
prior research has identified the importance of this characteristic when using static analysis to find bugs .
fixcache s least recently used replacement policy creates a bias towards the new and it is a helpful coincidence that this replacement policy was also found to be the most optimal replacement policy in the original work.
c. scaling as a pre requisite for deploying a bug prediction algorithm at google we had to ensure that the bug prediction algorithm would scale to google level infrastructure.
we ran into two scaling issues which blocked deployment of fixcache.
parallelizable the modern architectures of large technology companies like google yahoo or amazon now revolve around parallelized computations on large computer clusters.
ideally we want response from a bug predictionalgorithm to happen as quickly as possible.
in order to meet such a goal with the large source repositories that these large companies have the algorithm must also be able to be run in a parallelized fashion a single cpu and data store is too slow to process an entire repository efficiently.
rahman is easily parallelized and could run on the entire code base quickly.
fixcache is less amenable to parallelization as it is not clear how one could reconcile each fixcache worker s individual cache into a single cache on the reduction step.
if one was interested in just a per project output fixcache does run quite quickly on this smaller set of data so multiple fixcache workers could run on separate cpu cores and complete within a number of hours.
effectiveness scaling as mentioned previously fixcache s assumption of of files is not feasible for human consumption in almost all cases.
in a modest file project files will be flagged.
worse it is likely that figure is the superset of all files being actively worked on in the project as they are the ones that are currently being created and changed.
fixcache likely flags too much in this case and almost every file a developer submits for review will have the fixcache warning attached decreasing its impact dramatically.
however fixcache doesn t perform as well at least with the currently agreed upon metrics by peer review when the cache size is reduced.
any good bug prediction algorithm for humans must be effective whether it is to flag one file or one hundred.
in contrast rahman can scale to any effectiveness level and still operate similarly.
v. t ime weighted risk algorithm a. description given what we learnt from our derived desirable characteristics we set about modifying the rahman algorithm to achieve our goals.
rahman already embodies the obvious reasoning parallelizable and effectiveness scaling traits.
we modified it to bias towards the new resulting in a new algorithm for scoring the bug propensity of files which we call time weighted risk twr .
it is defined as follows nx i e 12ti !
where iis a bug fixing commit nis the number of bugfixing commits and tiis the normalized time of the current bug fixing commit.
tiruns between and with being the earliest commit and being the last submitted commit in practice this value is just the time when the script is run as something is committed to the google code base every day .
!defines how strong the decay should be it shifts the scoring curve along the x axis .
this equation sums up all the bug fixing commits of a file weighting the commits based on how old they are with older commits tending to zero.
note that twr only iterates through bug fixing commits commits that do not have any bug fixes are scored zero and so are not considered.
mathematically this has no distinction but computationally being able to quicklyfilter these inconsequential commits lowers memory usage and improves runtime.
the weighting function is a modified logistic curve where the x axis is the normalized time and the y axis is the score.
the score is not normalized as it is designed to only be used as a comparator between commits.
of note is that the score for a file changes every day as the time normalization will alter the weighting of a commit.
this is by design as older files that go unchanged should begin to decay.
we experimented with various x axis placements of the curve but found that a very strong decay seemed to be of most use based on our domain knowledge of projects a and b. currently the strength of decay in the algorithm represented by!
is hard coded to .
ideally this shifting should be dynamically calculated over time to keep the decay window about the same as time goes on the window expands.
at the time of writing the window counts commits for about to months before they start to become inconsequential.
this levels the field between old files and new files which are scored based on their most recent issues rather than taking into account problems in the past that may have been fixed.
the value of !is deployment dependent and will vary depending on what decay window makes intuitive sense for the given environment.
happened to be the value that worked for our purposes at google.
b. step by step example to illustrate how twr works in practice consider an imaginary file.
the first commit places the file in a brand new repository so this is the earliest commit for the normalization calculation.
the example will use three differences from a practical deployment to make things clearer.
each commit added will move time forward one unit.
in reality commits are very unlikely to be equally spaced.
also recall that the algorithm twr only iterates through bug fixing commits as commits that don t add any bug fixes are scored and not worth considering.
normal commits will be included here to better illustrate how the twr calculation changes over time.
finally !will be set to as the used in the google deployment decays twr too rapidly.
commit bug fixing?
no twr .
the developer finds a mistake and makes a second commit that fixes the first one.
commit bug fixing?
yes twr .
at this point commit s normalized time ti is .
and the initial commit s tiis .
.
twr calculates to .
at three significant figures at a greater level of accuracy we would see that a positive value of !causes a slight amount of decay even to new bug fixing changes .
everything is now fine with the file and the developer makes another change without issue.
commit 3bug fixing?
no twr .
commit with ti the value of twr is now recalculated.
the bug fixing commit in commit is weighted to the normalized time.
as there are three commits commit s time remains .
but now commit normalizes to a tiof .
and commit s time is .
.
with this smaller normalized time the bugfixing commit has decayed to be worth much less and twr calculates to .
.
the developer adds another non bug fixing commit.
commit bug fixing?
no twr .
commit with ti commits to are also non bug fixing.
commit is the next bug fixing change.
commit bug fixing?
yes twr .
commit with ti .
commit with ti .
note how the decay function causes commit to be almost inconsequential in the calculation of twr at commit .
this is intentional.
new commits are given much stronger favor over old commits.
in this example even with the reduced value for !
the decay appears to be very fast.
as mentioned previously in our deployment the decay window was about to months so commits don t tail off as quickly as one might infer here.
vi.
b ug prediction deployment a. overview having settled on twr we set out to find a suitable place in the developer workflow for the results to be displayed where we could reasonably expect to adjust developer behavior and encourage developers to show more attention to files which are flagged as bug prone.
embedding the results in an ide proved impractical as google does not centrally mandate any particular development environment so a number of plugins would need to be developed.
however all developers that commit to google s main source repository must have their code peer reviewed and the majority of reviews are performed through google s code review software mondrian.
when a developer wishes to commit code she must first submit that code for review and nominate reviewers to perform the review.
those reviewers should be knowledgeable about the code that is being changed and will be able to spot any bugs that may have evaded the original developer.
comments can be added at each line by reviewers and the submitter leading to brief conversations about bugs possible refactorings or missing unit tests.
the submitter can then submit new code with the requested changes.
this cycle can happen any number of times.
once the review team is happy the submission is marked looks good to me and is committed to the source repository.
to help aid reviewers mondrian offers a report of which unit tests passed and failed and runs a lint checker to helpensure that code meets the company standard.
the lint checker runs as a process which attaches comments line by line using the same mechanism that human reviewers do.
the various lint warning levels are attached to the comments and reviewers can change the warning levels to show or hide the comments.
as most developers use mondrian and are already used to seeing code health being reported in the interface we decided that the best insertion point for bug prediction results would be to attach a comment to line of any file that was flagged as bug prone.
the exact message used was this file has been flagged as bug prone by based on the number of changelists it has been in with a bug attached.
for this warning please fix means please review carefully .
the substituted word is the project codename that we call twr here.
changelists refers to perforce changelists which are commits in other source code vocabularies.
please fix is a reference to a shortcut that mondrian offers on lint warnings to allow reviewers to indicate to others that the lint warning should be dealt with.
as please fix was hard coded into mondrian as a possible response to lint warnings we needed to show that the link had a different meaning in this case.
twr was run as a nightly job across the entire code base and was set to flag the top .
of the google code base.
note that twr was not run at a project level but using google s entire source history.
this decision was made in order to flag only what twr considers the very worst files at the company which we hoped would filter false positives from the list.
this meant that some projects had more files flagged than others.
running the algorithm across the entire code base did not affect the output at a ui level a code reviewer would see a file was flagged in mondrian but was not told where it was ranked.
only those who have experience with the portion of the code base in question should be used as code reviewers so the flag was expected to be meaningful to those who were presented with it.
b. methodology to evaluate rq we deployed our solution in september .
bug prone files began to have comments attached in mondrian and all developers could see it during code review.
after four weeks the comments were given the same priority as a high level lint warning developers could see it if warnings were enabled but otherwise the comment was not visible.
this was largely done to bring the project into a consistent state with other warnings but reduced visibility of our comments.
three months later we investigated whether there was any quantitative change in developer behavior.
three months should have been a sufficient time for developers to have either taken to the tool or chosen to ignore it.
we were looking for measurable means to identify whether flagging a file resulted in greater developer engagement with the review something that indicates they are thinking about a review more deeply than they otherwise would have.
we identified two metrics for study the average time a review containing a bug prone file takes from submission to approval.
the average number of comments on a review that contains a bug prone file.
to measure this we generated a list of all the files that had been flagged as bug prone at any point since launch.
we then took reviews from three months either side of the launch date that included these files.
we analyzed the time a code review took from submission to approval and the number of comments that the code review gained.
c. results developer behavior we first looked at how the overall mean averages changed before and after launch as displayed in table ii.
for this test outliers that were lower than the 5th percentile and higher than the 95th percentile of each data set were filtered to prevent extreme outliers from skewing the results.
we then looked at whether the means for each individual file increased or decreased as displayed in table iii.
student s t tests show that neither change was significant supporting a null hypothesis that the bug prediction deployment had no effect on developers.
developer feedback initial developer feedback was mixed but leaning towards the negative.
we monitored feature requests sent directly to us as bug tickets and also internal mailing lists where the project had been mentioned.
common complaints included no opt out function confusion from both submitters and reviewers about how to fix the file before the submission would be approved as the algorithm is non actionable there was no means to do this technical debt files would be flagged again and again with developers feeling they were in no position to make any positive change auto generated files were not filtered and would often be changed so would end up attached to bug fixing changes and get flagged teams that were using google s bug tracking software felt unfairly flagged versus teams that used a different bug tracking method there was some indication that developers may have tried to sanitize their tracked bug tickets by correctly flagging them with the bug or feature request flag only bugs not feature requests are tracked by the project but our investigation of the data did not find any significant change in the ratio of bugs versus feature requests being reported as shown in figure .
if there was any change in reporting so that only bugs are characterized as so we would expect the number of bugs reported to go down and the number of feature requests to go up.
as the ratio didn t change we found no evidence that there was any significant change in behavior.
some developers did see value in the project and requested means to run the tool only for their own workspaces.
however by and large qualitative feedback did not have the positive responses that we had hoped for.metric before deployment after deployment change mean time to review days .
.
.
mean number of comments .
.
.
table ii a table showing how the mean time to review and mean number of comments changed before and after the deployment of twr.
metric increase decrease no change increase improvement mean time to review mean number of comments table iii a table showing the number of files that saw an increase or decrease in their mean time to review and their number of comments since deployment of the twr.
fig.
a chart showing the ratio of bug tickets classified as bugs vs bug tickets classified as feature requests submitted on a given day.
the project launched on september 16th and this shows one month of data either side of that date.
there was no significant change in the ratio.
interpretation we interpret these results not as a failure of developer focused bug prediction as a whole but largely as a failure of twr.
the feedback from developers was clear unless there was an actionable means of removing the flag and fixing the file developers did not find value in the bug prediction and ignored it.
in hindsight this is not surprising given the characteristics that developers requested during our informal discussions.
we do not believe fixcache would have fared any better.
we were at least expecting to see a small but statistically significant increase in the number of comments left on a file as we anticipated reviewers asking questions such as are we sure we should be editing this file at all?
while we see a small increase in the number of comments the increase is not significant.
vii.
t hreats to validity a. assumption of expert primacy in our evaluation of fixcache and rahman we test how well the bug prediction matches the intuition of expert developers.
we were interested in whether we could use these algorithms to help convey implicit knowledge about bug pronefiles to new developers on a project.
however bug prediction may also help help identify hotspots that may have been missed by every developer not just those new to the project.
one possible experiment design to take this into account would be to deploy each algorithm to two groups and have a signal back from developers which indicated both the developer s initial assumption about whether the algorithm was correct or not and then how the developer felt about the file after investigating the warning.
we could then measure which algorithm performed the best after developer investigation and also see whether the algorithm helped to shift developer views about certain files.
b. deployment by conducting this research exclusively at google the results may not be generalizable to other companies with other development processes.
within google we identified mondrian as the best place for deployment but there were limitations.
ideally there would have been a better injection point for showing flagged files than an annotation at line on a file.
one quirk of the annotations is that developers were used to only seeing them when there was some action that should be taken at that line which likely created confusion as twr doesn t provide actionable messages.
one developer suggested that instead of being used for code review bug prediction could have been used as a tool to help guide iteration planning to select cleanup work.
in this way the lack of directly actionable messages is lessened.
other deployment methodologies were discussed after the initial launch of twr.
one possibility is for results to be part of the ide perhaps as part of a system like the code orb .
moving the interaction with twr to the beginning of the code change workflow where a developer may have more leeway to modify their approach could yield significantly different results to those presented here.
c. interviewee pool we surveyed developers from project a and from project b. although we selected these two projects as examples of typical google projects our results could be biased by any unique characteristics of these specific projects.
another potential bias is in the interviewee pool.
while this pool had a good mix of experience levels we would havepreferred to have more interviewees.
in particular we would have liked to reduce the number of unknown responses for the duration cache and cache .
were it not for the performance of the rahman algorithm it is possible that our request for developers to comment across the project was unreasonable and should have perhaps been focused on smaller subsystems.
however the strong performance of the rahman algorithm shows that developers did seem to have a working knowledge of at least the portion of the code base rahman presented.
as our interviewees were volunteers there is the possibility of selection bias in our population.
further study with only developers with exceptionally high experience may yield more accurate results.
d. metric choice for deployment evaluation the two chosen metrics time to review and number of comments were metrics that were both available and descriptive of the problem at hand.
however it is possible that more suitable metrics could exist such as checking the number of bugs fixed against a file before and after deployment or code churn before or after deployment.
further research should be performed into methods of identifying reception to a tool.
in particular we found many developers talking about the algorithm on mailing lists a sentiment analysis approach using such discussions may yield interesting insights into tool reception.
we also considered modifying the comment in mondrian to include questions such as was this useful to you?
or do you agree with this assessment?
with yes no buttons so we could more directly get feedback from developers.
viii.
c onclusion in this paper we addressed three different research questions.
we found that developers preferred the rahman algorithm over the fixcache algorithms although there were still some files developers thought were not bug prone.
we enumerated various characteristics that a bug prediction algorithm should ideally have.
we then used this knowledge to alter rahman and create a new algorithm but did not observe any change in developer behavior.
we believe our findings do not point to bug prediction being a failure at helping developers.
we found developers to be excited about adding a new tool to help aid them in the never ending code health battle but these findings provide an illustration that what has currently been developed is not yet useful to them and hopefully provide a better insight into what might be required for bug prediction to be helpful.
we hope that future work will be able to incorporate the missing pieces particularly actionable messages and lead to widespread developer adoption.
we would like to caution that our work only pertains to an analysis of human factors when relating to bug prediction and our study does not analyze the suitability of any algorithm for automated uses such as test case optimization.
it is also possible that front line developers are the wrong audience for bug prediction.
instead software quality personnel might bea better target as they could use the results of bug prediction to focus quality improving resources on bug prone areas.
ix.
a cknowledgments the authors would like to acknowledge the contribution of googlers vasily koroslev jeanie light jennifer bevan jorg brown adam haberlach and john penix.