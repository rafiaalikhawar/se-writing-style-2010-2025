improving bug localization using structured information retrieval ripon k. saha matthew leaseysarfraz khurshid dewayne e. perry department of electrical and computer engineering yschool of information the university of texas at austin usa ripon utexas.edu ml ischool.utexas.edu khurshid ece.utexas.edu perry mail.utexas.edu abstract locating bugs is important difficult and expensive particularly for large scale systems.
to address this natural language information retrieval techniques are increasingly being used to suggest potential faulty source files given bug reports.
while these techniques are very scalable in practice their effectiveness remains low in accurately localizing bugs to a small number of files.
our key insight is that structured information retrieval based on code constructs such as class and method names enables more accurate bug localization.
we present bluir which embodies this insight requires only the source code and bug reports and takes advantage of bug similarity data if available.
we build bluir on a proven open source ir toolkit that anyone can use.
our work provides a thorough grounding of ir based bug localization research in fundamental ir theoretical and empirical knowledge and practice.
we evaluate bluir on four open source projects with approximately bugs.
results show that bluir matches or outperforms a current state of theart tool across applications considered even when bluir does not use bug similarity data used by the other tool.
index terms bug localization information retrieval search i. i ntroduction frederick brooks wrote that software entities are more complex for their size than perhaps any other human construct because no two parts are alike at least above the statement level .
due to this inherent complexity of software construction software bugs remain frequent.
for a large software system the number of bugs may range from hundreds to thousands.
generally bug fixing starts with finding relevant buggy source code.
i.e.
bug localization .
however performing this process manually for many bugs is time consuming and expensive.
therefore effective methods for locating bugs automatically from bug reports are highly desirable.
there are two general approaches for bug localization i dynamically locating the bug via program execution together with such technologies as execution and data monitoring breakpoints etc.
and ii statically locating bugs via various forms of analyses using the bug reports together with the code .
the dynamic approach is often time consuming and expensive.
the ease of the static approach together with its immediate recommendation make it appealing.
in recent years information retrieval ir based bug localization techniques have gained significant attention due to their relatively low computational cost and minimal external dependencies e.g.
requiring only source code and bug report in order to operate .
in these ir approaches each bugreport is treated as a query and the source files to be searched comprise the document collection .
ir techniques then rank the documents by predicted relevance returning a ranked list of candidate source files which may contain the bug.
lukins et al.
proposed a latent dirichlet allocation lda approach while rao et al.
compared a range of ir techniques unigram vector space latent semantic analysis lsa lda cluster based and various combinations.
both used a relatively small number of bugs in evaluation.
ngyuen et al.
proposed bugscout which customized lda for bug localization.
results on several large scale datasets showed good performance.
recently zhou et al.
proposed buglocator which combined a sophisticated tf.idf formulation a modeling heuristic for file length and knowledge of previously fixed similar bugs.
in a large scale evaluation of approximately bugs over four open source projects buglocator showed even stronger performance than bugscout.
moreover datasets and buglocator s executable were made available providing an invaluable benchmark for testing and comparing alternative ir approaches to bug localization.
despite the empirical success of prior work we perceive a gap today between ir community practices and techniques being applied to bug localization.
for example existing irbased bug localization treats source code as flat text lacking structure.
in fact source code s rich structure distinguishes code constructs such as comments names of classes methods and variables etc.
while ignoring such code structure simplifies the system it also sacrifices an opportunity to exploit this structural information to improve localization accuracy.
while we believe modeling source code structure is novel for bug localization we also note that the concept of modeling document structure in ir is quite old e.g.
google in and more recent bm25f .
whereas recent prior work devised a heuristic to model program length we discuss how the importance of length normalization was actually recognized in ir two decades ago and is built into today s baseline ir models.
in the same vein we discuss how the use of bug similarity data to improve localization is closely related to the established ir use of relevance feedback data .
generalizing from this we suspect our idea for modeling code structure is only one of the many ways in which ir based bug location could benefit from greater interaction with the ir community.
beyond our978 c ieee ase palo alto usa accepted for publication by ieee.
c ieee.
personal use of this material is permitted.
permission from ieee must be obtained for all other uses in any current or future media including reprinting republishing this material for advertising or promotional purposes creating new collective works for resale or redistribution to servers or lists or reuse of any copyrighted component of this work in other works.345technical contributions our approach strives to forge stronger conceptual ties between ongoing work in bug localization and proven practices from the ir community.
we introduce bluir bug localization using information retrieval an automatic bug localization tool based on the concept of structured information retrieval.
rather than build bluir s ir indexing and retrieval system from scratch we instead build upon an existing highly tuned open source ir toolkit indri .
while we use an off the shelf ir tool we simultaneously stress the importance of using it effectively i.e.
recognizing and addressing domain specific particulars of bug localization.
in our work we extract and model code constructs like structured documents and we show how a seemingly trivial change to how camel case identifiers are indexed yields significantly improved localization accuracy.
we evaluate bluir using the same large scale benchmark on which buglocator was evaluated.
when bug similarity data is not used the off the shelf ir tookit unmodified already exceeds buglocator s accuracy.
with our enhancements e.g.
structural modeling and indexing camel case identifiers as is accuracy is significantly improved further.
modeling additional bug similarity data provides yet a further gain.
finally even if buglocator is given bug similarity data and bluir is not bluir still outperforms buglocator on three of the four code repositories and matches its accuracy on the fourth.
for reproducibility data from our experiments is available online.
contributions.
we present new techniques for increasing localization accuracy particularly modeling of source code structure new state of the art accuracy for bug localization on a public community benchmark built on a proven open source ir toolkit anyone can use and thorough grounding of ir based bug localization research in fundamental ir theoretical and empirical knowledge and practice.
ii.
b ackground we begin this section with a demonstrative example of irbased bug localization.
the fundamental assumption underlying these techniques is that some terms in a given bug report will be found in source files needing to be fixed for that bug.
figure presents a real world bug report from eclipse .
and corresponding source code fix taken from .
the figure shows matching words in bold font found in both the bug report and one of the corresponding source code files that was ultimately fixed for that bug.
in ir based bug localization a software system s source code files represent the document collection to search with each bug report being a search query .
finding candidate files to fix is then reduced to standard ir ranking of documents source files based on estimated relevance to each query bug report .
the better an ir system can interpret the bug report and source files the more accurately it is expected to highly rank the source files needing to be fixed.
while deep semantics remain elusive shallow matching often works quite well.
fig.
.
an example of a bug localization a. information retrieval ir for a broad overview of ir see online.
an ir system typically begins with three step preprocessing text normalization stopword removal and stemming .
normalization involves removing punctuation performing case folding tokenizing terms etc ultimately defining the initial vocabulary in which queries and documents will be represented.
next a set of extraneous terms identified in a stopword list e.g.
to the be etc.
are filtered out in order to improve efficiency and reduce spurious matches.
finally stemming conflates variants of the same underlying term e.g.
ran running run to improve term matching between query and document.
while these three preprocessing steps are often given short shrift in describing ir approaches they embody important tradeoffs that can significantly influence the ultimate success or failure of the retrieval model.
for example normalization can increase matches between query and document by casefolding improving recall but this can also introduce spurious matches as well hurting precision .
similarly while stopword removal can reduce unhelpful term matching e.g.
to any stopword removed is almost certain to hurt matching for some particular query e.g.
to be or not to be .
finally stemming will similarly increase recall by conflating variants of the same underlying term but this may also introduce false matches.
for reproducible experimentation preprocessing methods should be fully described along with other details of ir model.
once queries and documents have been pre processed documents are indexed by collecting and storing various statistics such as term frequency tf the number of times a term occurs in a given document and document frequency df the number of documents in which the term appears .
idf refers to inverse dampened df most simply formulated as log n df wherenis the number of documents in the colleciton.
a widespread misconception about tf.idf merits particular attention.
specifically the tf.idf model is often used as a baseline model for comparison with new retrieval models.
however it is not actually a well defined model in the sense that there are several heuristic components in the model that can affect performance significantly.
.
for greater detail 346see .
many studies have claimed improvement over tf.idf using only the most naive version of the model or simply report a tf.idf baseline without fully specifying what tf.idf model was actually used.
in contrast buglocator fully specified the tf.idf formulation they defined .
however this raises a related issue it is not clear how their formulation differed conceptually or empirically to other existing state of the art tf.idf variants.
we advocate always trying simple well tuned existing models first then fully describing and motivating whatever tf.idf variant is used.
iii.
p resence of source code terms in bug reports anempirical study the success of ir based bug localization is dependent on effectively matching the bug report to source files needing to be fixed.
as discussed in section ii even preprocessing issues can significantly impact ir accuracy.
the classic ir challenge lies in effectively recognizing important terms in the query and document and assigning each greater weight for matching.
with regard to text length long queries e.g.
when using the bug report s description field can obscure key search terms .
document length also merits special attention .
both topics are further discussed in section ix c. another classic ir approach distinguishes and separately models different fields when text is structured .
for example while searching documents google considers page title different anchor texts and body separately .
we investigate this structured approach to ir based bug localization.
with queries a bug report contains separate summary and description fields whereas the summary provides essential keywords the description is more verbose with additional terms.
as discussed in the next section source code files are even more structured.
we perform preliminary analysis here to assess the degree to which source code terms appear in bug reports potentially providing an opportunity for better ir.
we distinguish six types of terms.
query terms come from different bug report fields the concise summary and verbose description .
parsing source code structure also lets us distinguish four different document fields class method variable and comments .
these fields are extracted by constructing and traversing the abstract syntax tree ast of the subject program section iv a .
for each bug report we separately search for terms from each document field in source files that were fixed for the corresponding bug.
we collect two separate sets of statistics matching terms as is in their original form vs. splitting identifier names based on the camel case heuristics and searching for each token.
to illustrate for the given example in figure first we search consoleview and then the separate terms console andview in both the bug summary and bug description.
for each search we exclude those tokens that either are stop words or have less than three characters.
for example if a variable name is isbalancedtree we do not search for is .
table i provides empirical evidence that terms from source files to fix in aspectj are present in the corresponding bug reports.
each entry represents the number of bug reports in which different term types class method variable ortable i presence of different term types in bugreports for aspect j term type summary description exact match token match exact mach token match class .
.
.
.
method .
.
.
.
variable .
.
.
.
comments n a .
n a .
comment were found.
for each bug report section summary vs.description we count the number of bug reports containing an exact match or token match for at least one of the files to be fixed.
for example the first two numbers in the class row of table i represent that in bug reports in aspectj at least one of the class names of the fixed files was present as is in the bug summary whereas in bug reports at least one of the class name s split tokens was present.
from the table we see that although summary contained only the of total terms in the bug report at least one of the class method variable comment terms was found in of the bug summaries respectively.
similarly although class name is typically a combination of terms per source code file they are present in more than of the bug summaries and of the bug descriptions.
furthermore the exact class name is present in more than of the bug descriptions.
we can observe a similar phenomena for method names as well.
while the bug report description vs. summary has many more matches the more verbose description likely matches many irrelevant terms as well.
similarly table i only shows matches from the source files needing to be fixed.
the bug reports also include terms matching many other source files not needing to be fixed.
consequently this table provides suggestive rather than conclusive evidence for our approach evaluation later in the paper will demonstrate the empirical effectiveness of modeling this information.
we intentionally restrict the analysis here to aspectj only reserving the other three source code repositories for later blind evaluation to maximize generality of our findings.
iv.
a pproach in the previous section we showed that important program constructs such as class names and method names are present in many bug reports and thus might be effectively used to improve bug localization.
this section describes our structured ir based approach for localizing bugs.
a. bluir architecture figure shows the overall architecture of bluir.
first bluir takes as input the source code files in which we would like to localize the bugs.
next it builds the abstract syntax tree ast of each source code file using eclipse java development tools jdt and traverses the ast to extract different program constructs such as class names method names variable names and comments.
then bluir tokenizes all the identifier names and comment words as described in section iv b. this information for each source file is then stored as a structured xml document.347fig.
.
bluir architecture reducing bug localization to a standard ir task enables us to exploit a wealth of prior theoretical and empirical ir methodology providing a robust foundation for tackling bug localization.
we adopt the indri toolkit for efficient indexing and developing our retrieval model.
after xml documents are created above they are handed off to indri for stopword removal stemming and indexing.
note that we used the default stopword list provided with indri.
each bug report is similarly tokenized then handed off to indri for stopping stemming and retrieval section iv c .
b. source code parsing term indexing in comparison to prior approaches we make two improvements in our preprocessing.
first prior work has indexed all source code terms except english stopwords and programming language keywords.
however some keywords like string or class are used in identifier names and may be found in bug reports.
for example in aspectj many identifiers use java language keywords e.g.
if else etc.
therefore instead of pruning all language keywords we instead build the abstract syntax tree ast of each source file and extract all identifier names class name method name variable name etc.
.
in this way we exclude language keywords without losing their presence in identifiers.
secondly identifiers are typically split into tokens for indexing to improve recall.
dit et al.
compared simple camel case splitting to the more sophisticated samurai system and found that both performed comparably in concept location.
we therefore adopt camel case splitting for its simplicity.
however since our analysis in table i reveals that full identifiers are often present in bug reports in the form of execution traces of exceptions test cases or code snippets we index full identifiers as well as split tokens.
although it is a very simple extension we will see that it yields significant improvement.
c. retrieval model as discussed in section ii a tf.idf is not actually a well defined model and different tf.idf variants can achieve vastly different empirical performance in practice.
we adopt indri s built in tf.idf formulation from its parent projectlemur based upon the well established bm25 okapi model .
this tf.idf model has been rigorously evaluated over a decade of widespread use in ir.
we elaborate below.
assume that a document and a query are represented by a weighted term frequency vector dand qrespectively of length n the total number of terms or the size of vocabulary .
d x1 x2 x n q y1 y2 y n each element of xiof drepresents the frequency count of termtiin document d similarly yiin query q .
generally in a vector space model query and document terms are weighted by a heuristic tf.idf weighting formula instead of only their raw frequencies.
inverse document frequency idf diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.
weighted vectors for dand qare thus dw tfd x1 id f t1 tfd x2 id f t2 tf d xn id f tn qw tfq y1 id f t1 tfq y2 id f t2 tf q yn id f tn given a collection cof source files the simplest classic idf formulation for term tis given byidf ti logn nt where nis the total number of documents in candntis the number of documents with term t. in the simplest tf idf model we would simply multiply this value by the term s frequency in documentdto compute the tf idf score for t d then sum over all terms in the query to arrive at the d s tf idf score.
as mentioned above however actual tf idf models used in practice differ greatly from this for improved accuracy .
we adopt indri s tf.idf model which is summarized below.
to begin the idf value is smoothed as follows to avoid division by zero which would otherwise occur whenever a particular term appears in all documents idf ti logn nt .
the document stffunction is computed by okapi tfd x k1x x k1 b bld lc wherek1is a tuning parameter that calibrates document term frequency scaling.
the term frequency value quickly saturates for a small value of k1 whereas a large value corresponds to using raw term frequency.
bis another tuning parameter between and which is the document scaling factor.
recall our earlier discussion of buglocator introducing a heuristic for modeling document length whereas this is already built into ir models today section i .
here when the value ofbis the term weight is fully scaled by the document length.
for a zero value of b no length normalization is applied.ldandlcrepresents the document length and average document length for the collection respectively.
thequery s tf function tfqis defined similarly tfdthough b is fixed since the query is fixed across documents being compared and thus normalization of query length is unnecessary tfq y k3y x k3 in equation the value of k3is fixed to to obtain almost the raw query term frequency because in query the348probability of having a term many times is rare.
now the similarity score of document dagainst query qis given by equation .
s d q nx i 1tfd xi tfq yi id f ti d. incorporating structural information the tf.idf model presented in equation does not consider source code structure program construct i.e.
each term in a source code file is considered having the same relevance with respect to the given query.
therefore important information like class names and method names often get lost in the relatively large number of variable names and comments terms due to the term weighting function equation .
therefore if a source code file with class name a also contains other variable names having the term a then the class name a does not add much weight.
thus if there is a bug report related to class a it will rank another file higher if the file has the term a more than times even in the local variable names or comments.
our proposed model distinguishes different code constructs to overcome this problem.
as we described in section iii we distinguish two alternative query representations coming from different fields of the bug report the summary and the more verbose description .
parsing source code structure also lets us distinguish four different document fields class method variable comments .
to exploit all of these different types of query and document representations we perform a separate search for each of the eight query represent document field combinations and then sum document scores across all eight searches.
s0 d q x r2qx f2ds df qr whereris a particular query representation and fis a particular document field.
the benefit of this model is that terms appearing in multiple document fields are implicitly assigned greater weight since the contribution from each term is summed over all fields in which it appears.
while our method of integrating structural information is quite simple more sophisticated methods for integrating structural information exist and could be explored in future work e.g.
a weighted combination rather than a simple sum or better yet weighting term frequencies rather than document fields to better control for term frequency saturation .
v. e valuation setup a. data set we have used the same dataset that zhou et al.
used to evaluate buglocator.
this dataset contains bug reports in total from four popular open source projects eclipse aspectj swt and zxing along with the information of fixed files for those bugs.
table ii describes the dataset in more detail.
since we would like to compare bluir with buglocator the same dataset allows us to get comparable results.
among the four subject systems in the dataset we always use aspectj for learning purposes to tune the parameters so that we do not overfit our retrieval model.
we chose aspectj system astable ii details of benchmark project description period bugs files swt .
widget toolkit for java eclipse .
popular ide for java aspectj aspect oriented extension to java07 zxing barcode image processing library for android03 training dataset because it has bugs which is neither too large nor too small.
we have also compared our results with a similar version of dataset for aspectj and eclipse that were used in evaluating bugscout table vii .
b. evaluation metrics since an ir system s value is in direct proportion to how well it serves its users the design and selection of appropriate evaluation metrics has been a topic of considerable study in ir.
we should select a sufficient yet minimal set of metrics to ensure what we measure provides an appropriate and comprehensible yardstick for assessing the most pertinent aspects of performance.
we err on the side of excess and comparative evaluation including all five metrics as zhou et al.
other systems we compare to use a subset of these metrics.
all metrics are based on gain rather than loss larger values indicate better performance .
recall at top n this metric reports the number of bugs with at least one buggy source file found in the top n ranked results once the first buggy file is located it may become easier for the developer to find the rest .
since we are only considering the top few ranks and only requires finding one of the buggy files per bug this metric emphasizes early precision over total recall.
mean reciprocal rank mrr like recall at top n mrr emphasizes early precision over recall.
the reciprocal rank for a query is the inverse rank of the first relevant document found.
mrr is the reciprocal rank averaged over all queries mrr jqjjqjx i rank i mean average precision map map is by far the most commonly used traditional ir metric.
it takes all the faulty files into account with their ranks.
therefore map emphasizes recall over precision and is favored in scenarios in which users will go deep in a ranked list to find many relevant results.
the average precision of a single query is computed as ap mx k 1p k pos k number of positive instances wherekis the rank mis the number of retrieved source files andpos k is a binary indicator of whether or not the item at rank is a buggy file.
p k is the precision at the given cut off rankk.
the map for a set of queries is simply the mean of the average precision values for all queries.
note that all metrics above compute an arithmetic mean over the query set to measure average performance.
this may not349table iii effect of different stemmers and parameters on aspect j term weighting stemmer top top top map mrr none .
.
k1 b krovetz .
.
porter .
.
k1 b 75krovetz .
.
k1 b 3krovetz .
.
be appropriate if developer satisfaction is driven by worst case performance rather than average performance in which case a geometric mean may be more appropriate.
figure presents a per query analysis of results inspecting the performance of each query instead of only the average.
c. system tuning as we described in section v a we use aspectj as training dataset to choose stemmer and tune two parameters of our model the term weight scaling parameter k1and the document normalization parameter b. table iii compares system performance with no stemmer vs. using two popular stemmers krovetz and porter.
for this experiment only we use approximately raw tf.idf with k1 andb .
we observe no significant difference among the three methods.
in prior work hill et al.
also observed that no single stemmer is better for all kinds of queries.
while we choose krovetz somewhat arbitrarily as the more conservative of the two stemming algorithms closer analysis here appears to be warranted to provide a fuller explanation.
table iii shows results of tuning k1 and b. these experiments exclude modeling of source code structure.
traditional wisdom is to set k1 2andb .
however since bug localization is different from traditional text retrieval we did a linear sweep of all values between for k1and for b with step size .
selecting k1 0andb 3as optimal.
vi.
r esults this section presents the evaluation results of bluir while performing bug localization on the four subject systems described in table ii.
we mainly answer five research questions that show the effectiveness of different improvements that we made in developing bluir and compare the results of bluir with other information retrieval models and tools.
rq1 does indexing the exact identifier names improve bug localization?
in section iii we observed that in many bug reports of aspectj different kinds of source code entity names e.g.
class name method name are present exactly asis.
in this research question we investigate the effectiveness of adding full identifier names as well as tokenized identifiers to the index.
our experiments in this section exclude source code structure.
results are reported for all four subject systems first with only the tokenized identifier names and then with the combination of tokenized and full identifier names.
table iv presents the result of indexing both.
the evaluation results show that the addition of exact identifier names improved the accuracy for three of four subject systems.
in eclipse using exact identifier names bluir localized .
more bugs in top file whereas the increases aretable iv effect of indexing full idenifier names system indexed top top top map mrr swt tokens .
.
both .
.
eclipse tokens .
.
both .
.
aspectj tokens .
.
both .
.
zxing tokens .
.
both .
.
table v effect of modeling source code structure system structure top top top map mrr et qi s swt n .
.
.
y .
.
.
eclipse n .
.
.
y .
.
.
aspectj n .
.
.
y .
.
.
zxing n .
.
.
y .
.
.
.
and .
for swt and aspectj respectively.
the consistently higher map and mrr for the first three subject systems show the overall improvements of the ranking due to adding exact identifiers.
in zxing the top and mrr metrics were a little bit lower than the traditional one while other metrics were exactly the same.
however it is very difficult to derive any useful conclusions from zxing because the bug dataset has only bugs for this subject system.
rq2 does modeling source code structure help improve accuracy?
in section iii we argued that source code structure i.e.
distinguishing different code constructs could be effectively used to find more important terms in both source code and bug report and thus improve the overall bug localization accuracy.
in this research question we investigate whether this improves the accuracy of bug localization and if so how much.
to this end we ran bluir on all the subject systems to localize bugs with and without modeling source code structure.
table v results show that in most cases bluir performed better in terms of all the metrics when it considered different program constructs.
more specifically structured retrieval is more effective for top .
using structured retrieval bluir localized .
.
.
and more bugs in swt eclipse aspectj and zxing respectively within top file.
in aspectj for top and top bluir localized a few less bugs using structured retrieval.
however the high map and mrr shows that the overall ranking is much better when bluir used structured retrieval.
further qualitative analysis is presented in section vii.
runtime overhead since structured information retrieval involves more computation than the normal text retrieval the runtime overhead of structured information retrieval is expected to be higher.
to investigate this overhead we computed the average execution time per query et qi of bluir both for traditional retrieval and structured retrieval table v .
the results show that structured retrieval is more costly and the overhead depends on the size of document collection .
the350specific cost varies from about 3x to 12x.
however since all the execution times are less than seconds the added cost from the developers perspective is negligible.
moreover structured information retrieval due to its higher accuracy may save quite a bit of the developers time overall.
rq3 does bluir outperform other bug localization tools and models?
while evaluating buglocator zhou et al.
compared their model with other prior work and showed that buglocator consistently performed best.
we therefore compare bluir to with buglocator which is to the best of our knowledge the most accurate tool presently.
table vi shows results of bluir and buglocator for the given dataset using and without using similar bug report data.
buglocator results are copied verbatim from .
it should be noted that we use the same datasets used to evaluate buglocator.
in this section we restrict our discussion to results without using bug similarity data.
comparing each metric of each system we can see that for zxing the results produced by both tools are almost the same.
as we explained earlier it is very difficult to derive any useful conclusions from zxing because the bug dataset has only bugs for this subject system.
however looking into the results of other systems which have more bug reports for swt for eclipse and for aspectj we can clearly see the that bluir outperformed buglocator by a great margin.
bluir localized .
more bugs in swt .
more bugs in eclipse and .
more bugs in aspectj ranked within the top file.
the same trend is observed for other metrics as well.
the consistently higher map and mrr for bluir also suggest that the overall ranking of the buggy files produced by bluir are better than that of buglocator.
now we investigate the number of queries for which bluir actually performed better than buglocator because the higher number of bugs located in top and files retrieved by bluir than that of buglocator does not necessarily mean that bluir performed well for all queries.
figure shows per query performance of bluir compared to buglocator on swt where x axis represents the query number and y axis represents the difference between the best rank of the buggy files by bluir and that of buglocator.
the negative value represents the query where buglocator performs better than bluir.
we can see that for queries bluir performed better for queries buglocator performed better and for queries both tools perform exactly the same.
this results suggest that bluir performed better than buglocator for most of the queries.
interestingly we also observe that for bug reports bluir improved the rank of buggy files by more than positions whereas it is only two where buglocator improved the rank by more than positions and positions .
as a result bluir places more buggy files within top and files in the rank list than buglocator.
this analysis required access to per query results from buglocator made possible by its executable being publicly available.
unfortunately it crashed when run on the other three collections and we could not reach the authors for assistance in time for this submission.
this analysis is therefore limited to swt only.table vi blu irvsbuglocator system method sb top top top map mrr buglocator n .
.
swt bluir n .
.
buglocator y .
.
bluir y .
.
buglocator n .
.
eclipse bluir n .
.
buglocator y .
.
bluir y .
.
buglocator n .
.
aspectj bluir n .
.
buglocator y .
.
bluir y .
.
buglocator n .
.
zxing bluir n .
.
buglocator y .
.
bluir y .
.
fig.
.
query wise comparison of buglocator and bluir for swt we also compare our results to bugscout and buglocator in table vii .
to evaluate bugscout nguyen et al.
used aspectj and eclipse as their subject systems.
since our datasets are not exactly the same as theirs we present the differences between two datasets in terms of number of bugs in the table.
we have also interpreted the recall at top top and top of bugscout results from a figure in their paper which may slightly differ from their actual value.
results show that bluir outperformed bugscout consistently.
recently sisman and kak incorporated version histories in ir based bug localization.
they proposed two models namely the modification history based prior and the defect history based prior models to estimate a prior probability for each file in a project having bugs.
they used these priors to rank documents in addition to different models.
based on table vii comparison of bugscout and buglocator with blu ir system description bugscout buglocator bluir number of bug reports aspectj top top top number of bug reports eclipse top top top 351a case study on aspectj they showed that version histories improved the map as much as .
however without considering any version history information bluir performed map .
better than their best results map .
.
rq4 does our approach compensate for the lack of similar bug information?
although the performance of buglocator improved a lot after using similar bug fixes information one of our main objectives is improving bug localization without using the similar bug information since most real world projects do not explicitly have that information.
in addition reconstructing the similar bug fix information is also not a trivial task.
therefore we investigate how bluir performs in locating bugs compared to buglocator even when buglocator uses similar bug information.
the comparative results presented in figure vi show that in most cases the map and mrr of bluir are higher than that of buglocator which indicates that bluir overall performed better even when buglocator considered similar bug information.
for example bluir localized more bugs in swt more bugs in eclipse and more bugs in aspectj than buglocator within top file.
we were also curious to see if we could capture those bugs that were localized by buglocator using similar bug information.
to this end we ran buglocator on swt using and not using similar bug information.
we found such bugs in total that have been placed within top or files by buglocator after using similar bug information.
interestingly we found that bluir could localize all the bugs without using similarity information.
rq5 does similar bug fix information further improve our model?
we implemented the same technique for incorporating similar bug information to bluir that zhou et al.
did in developing buglocator.
by comparing the results of bluir in table vi we can see that the similar bug information further improved our results.
for example it helps bluir localize and more bugs ranked in the top and files respectively for eclipse project.
it also improved bug localization in swt and in aspectj within top file.
however the overall improvement due to using similar bug information was not as large as that of buglocator.
therefore here we can conclude that bluir can compensate for the lack of similar bug information partially because we already localized many bugs without using similar bug fix information which were only localized by buglocator using similar bug information.
bluir can also make use of the similar bug fix information to improve the model further if it is available.
other results.
we briefly report preliminary experiments with pseudo relevance feedback prf section ix b using indri.
the primary advantage of using prf is that we do not need to know any prior information e.g.
similar bugs about relevant documents while running query.
in prf mode indri basically performs the general retrieval first and then augments the original query by taking the mmost frequent words from top rdocuments.
there is also a tuning parameter for weighting original query and augmented terms.
finally the augmented query is run again to get the final rank list.
we experimented with different values of m n and but did notobserve improved accuracy.
in future work we would like to explore this idea further.
vii.
q ualitative analysis the previous section presented quantitative results showing bluir s improvement on average over buglocator.
in this section we dig into several queries in detail to better understand why bluir performs better in most cases.
consider swt bug report summary double click only works on a tree s column0 description using the log view as an example doubleclicking on column0 brings up the event dialog as it should.
double clicking on column1 column2 results in no notification to our double click listener.
for this bug only one file was fixed and that is org.eclipse.swt.widgets.tree.java .
by reading the bug report and seeing the file name of the fixed file one might think at a glance that this file can be identified easily.
however identifying a buggy file in a real world project is not that easy especially where there are many other such similar files.
for example in swt there are at least other files treecolumn.java treeevent.java treelistener.java treeadapter.java treeitem.java etc.
that deal with tree and have this word in their file names.
thus for a developer who did not originally implement the functionality of tree might think treecolumn and treelistener are more important because the bug report contains the words column andlistener .
furthermore the bug report has some other words such as double click event dialog which are contained many times in more than other files such as text.java widget.java button.java and so on.
therefore finding the desired files from the ir perspective is also very challenging.
relying on only length of the files is certainly not the solution of this problem.
as a result buglocator placed the file tree.java at 50th position in the rank list.
fortunately bluir first performs all the field retrievals using both the bug summary and the bug description and then aggregates all the scores to finally rank all the source code files.
this results in the summary words e.g.
tree being used more advantageously.
furthermore documents have search words e.g.
column double click spread over more fields produce better results than documents having search words found in one field.
in this way bluir emphasizes on more important words in the documents.
as a result bluir placed tree.java at 3rd position in the rank list.
in this way bluir improved the rank of buggy files by more than positions for bug reports e.g.
bugs and so on .
viii.
t hreats to validity this section discusses the validity and generalizability of our findings.
in particular we discuss construct validity internal validity and external validity.
construct validity .
we used two artifacts of a software repository source code and bug reports which are generally well understood.
our evaluation uses the same benchmark dataset of bug reports and source code shared by zhou at352al.
enabling fair comparison and reproducible findings.
metrics used for evaluation match those of zhou at al.
and other prior work are standard in ir and are straightforward to compute.
therefore we argue for a strong construct validity.
internal validity .
we utilize program constructs to rank source code documents with respect to a given bug.
since we have used all the subject systems written in java these are mainly object oriented oo constructs such as class names method names and so on.
in this sense our approach is language dependent.
we expect our system could be easily adapted to other oo languages future work .
since we are matching terms between bug reports and source code we assume meaningful identifier names and inclusion of comments consistent with programming best practices.
that said poorly written source code would make bug localization more difficult for both ir or non ir approaches .
similarly we also depend upon the quality of bug report and poorly written reports would likely also hurt ir and non ir methods.
our structural modeling approach matching source code terms in bug reports likely benefits significantly from the bug reports having been written by developers knowledgeable of the underlying source code.
bug reports written by endusers would likely show a far less pronounced effect.
we have used the same dataset as zhou et al.
.
while the possibility exists of errors in their data this seems quite low since they have manually validated the dataset.
also three of our four subject systems represent system specific projects.
as hyrum et al.
noted system domain software may have its own set of development biases.
therefore we may not capture some unique concerns which are only present in the software development targeted toward other domains.
external validity .
we have used only four subject systems in our experiment and all of them are open source projects.
although they are very popular projects our findings may not be generalizable to other open source projects or industrial projects.
however to maximize generalizability of findings and minimize risk of over fitting we developed and tuned bluir on only one subject system aspectj reserving the remaining three systems for final blind evaluation.
this risk of insufficient generalization could be mitigated by expanding the benchmark to include more subject systems both open source and industrial .
this will be explored in our future work.
ix.
r elated work a. automatic bug localization automatic bug localization or automatic debugging has been an active research area for over two decades .
existing techniques can be broadly categorized into two categories dynamic and static .
generally dynamic fault localization techniques can localize bugs very precisely such as at statement level .
however they require a test case suite and need to execute the program for gathering passing and failing execution traces.
furthermore the approaches are computationally expensive.
spectrum based fault localizations dynamic slicing delta debugging are some of the well known techniques in this category.static approaches on the other hand do not require any program test cases or execution traces.
in most cases they need only program source code and bug reports.
they are also computationally efficient.
the static approaches can be also divided into two categories i program analysis based approaches ii ir based approaches.
findbug is a popular bug localization tool based on static program analysis that can detect bugs by identifying buggy patterns frequently happened in practice.
therefore findbug does not even need a bug report.
however it cannot detect semantic bugs.
b. information retrieval ir while historical arguments debated which of three traditionally dominant ir paradigms was best tf.idf the probabilistic approach known as bm25 okapi or more recent language modeling all three approaches have been shown theoretically to utilize the same underlying textual features and empirically to perform comparably when well tuned .
consequently while one formalism or another might make it easier to integrate useful additional features use of one formalism or another is not particularly important when it simply comes to baseline ir performance.
in contrast with shallow bag of words models research has also explored deeper methods matching concepts often poorly defined .
while latent semantic indexing lsi induces latent concepts it is rarely used in practice today due to errors in induced concepts introducing more harm than good.
while a probabilistic variant of lsi has been devised its probability model was found to be deficient.
this led to now ubiquitous latent dirichlet allocation lda modeling .
while many studies have shown lda can usefully infer latent topics underlying a document collection lda is both computationally expensive and operates without reference to the input query.
it has been shown that far simpler ir models based on pseudo relevance feedback prf can efficiently induce better topics on the fly for each query tailored to the query vs. query independent lda topics .
consequently lda models appear less useful for ir than simpler models until this fundamental problem can be meaningfully addressed.
one issue considered in this paper was how to best utilize multiple representations of the same bug report i.e.
its summary anddescription .
while the summary is very succinct and likely provides the most important keywords it may lack other terms useful for matching suggesting high precision but possibly low recall .
in contrast the more verbose description may contain many other useful terms to match it likely contains a variety of distracting terms as well.
this is a very well known problem in traditional ir .
for over two decades data from the text retrieval conference trec has provided queries at three levels of verbosity with researchers devising various methods to maximally exploit these different representations.
for example simply concatenating the two representations together provides an easy way to emphasize keywords while also including more verbose terms as well.
in this paper our method of performing separate summary and description searches and summing results is roughly equivalent to such concatenation.
future work could explore a wide353variety of more sophisticated ir methods for exploiting these alternative query representations with varying verbosity.
c. ir based bug localization the value of document length normalization was recognized in ir nearly two decades ago .
empirical data compared the length of documents predicted relevant by tf.idf vs. the length of actual relevant documents showing that traditional ir models are actually biased against longer documents.
an empirical correction for this bias was developed it was realized that this correction was already built into bm25 and it has been further shown that ir s language modeling paradigm performs implicit length normalization as well.
several prior studies have investigated use of bug similarity data in order to improve localization accuracy .
this idea can be seen as a close cousin to long established methods for incorporating relevance feedback rf data in ir .
while rf exploits the fact that knowing one or more documents relevant to the current query makes it much easier to find other relevant documents this knowledge is often seldom available in practice.
a trinity of related variants has been theoretically and empirically established showing that similar queries should retrieve similar documents and vice versa and that similar documents should receive similar relevance scores for the same query score regularization .
in fact the idea that the same documents should be relevant for similar queries provides the foundation for search community question and answer forums today .
consequently while use of bug similarity data for localization represents a very valuable adaptation of rf methods from traditional ir there is a wide spectrum of similar techniques and existing methodology that might be further explored as well e.g.
the aforementioned prf which infers relevant documents for feedback rather requiring the user to supply them explicitly .
concept location or feature location represents another task closely related to bug localization.
generally concept location or feature location aims to identify the relevant parts of a software system that implement a specific concept or functionality.
thus it is one of the most common activities in program comprehension.
researchers have used a variety of information retrieval techniques in feature location and concept location as well.
marcus et al.
used lsi to find modules related to a given feature in form of a user query.
poshyvanyk et al.
used lsi first to rank source code elements based on a given feature or bug reports and then used a formal concept analysis to cluster the results.
in another work poshyvanyk et al.
formulated the feature location problem as a decision making problem in the presence of uncertainty.
the decision is taken based on the opinions from two experts.
the first expert is lsi which enables users to search static documents relevant to a feature.
the second expert is the scenario based probabilistic ranking which helps user rank a list of entities given a feature of interest by analyzing dynamic traces from the execution of different scenarios.
gay et al.
incorporated rf in ir based concept location.
although bug reports were used as a concept feature in some of these studies they were few in number.x.
c onclusion locating bugs is important difficult and expensive particularly for large scale software projects.
to address this natural language information retrieval ir techniques are increasingly being used to suggest potential faulty source files given bug reports.
while these techniques are very scalable in practice their effectiveness remains low in accurately localizing bugs to a small number of files.
our key insight is that structured ir based on code constructs such as class and method names enables more accurate bug localization.
we present bluir which embodies this insight builds on an open source ir toolkit requires only the source code and bug reports and takes advantage of bug similarity data if available.
we evaluate bluir on four open source projects with approximately bugs.
when bug similarity data is not used the off the shelf ir tookit unmodified already exceeds state of the art tool buglocator s accuracy.
with our enhancements e.g.
structural modeling and camel case indexing accuracy is significantly improved further.
modeling additional bug similarity data provides yet a further gain.
finally even if buglocator is given bug similarity data and bluir is not bluir still outperforms buglocator on three of the four code repositories in the benchmark.
beyond our technical contributions our presentation also strives to forge stronger conceptual ties between ongoing work in bug localization and proven practices from the ir community via a thorough discussion of ir based bug localization research in relation to fundamental ir theoretical and empirical knowledge and practice.
in our future research we would like to explore the following areas to further improve our model bug report summarization and learning parameters.
bug report summarization.
in this paper we showed how the performance of bug localization improves by focusing on condensed information such as bug summaries class names or method names.
however we still used exactly the same long bug descriptions from bug reports.
there are some automatic techniques that can condense bug descriptions up to of its original size.
such summarized bug descriptions may further improve the performance of bug localization.
learning to rank.
to tune the value of k1andbin our model we ran bluir on aspectj using a range of values at a fixed interval length and took the pair for which we got the best result for other subject systems.
however the best values may be different for different subject systems.
finding a globally optimal weights is still an open problem in ir research community.
recent work in ir is using machine learning methods to automatically optimize ranking parameters for more sophisticated ranking functions.
this would provide another interesting direction for future studies.
we also plan to utilize other datasets e.g.
morebugs and perform function method level bug localization .
acknowledgment we thank dawn lawrie for her helpful feedback.
this research was supported in part by nsf grant shf srs ccf and temple fellowship.354references r. abreu p. zoeteweij r. golsteijn and a. j. c. van gemund.
a practical evaluation of spectrum based fault localization.
j. syst.
softw.
nov. .
d. binkley and d. lawrie.
information retrieval applications in software maintenance and evolution.
encyclopedia of software engineering .
d. m. blei a. y .
ng and m. i. jordan.
latent dirichlet allocation.
the journal of machine learning research .
s. brin and l. page.
the anatomy of a large scale hypertextual web search engine.
in proceedings of the 7th international conference on the world wide web www pages .
f. p. brooks jr. no silver bullet essence and accidents of software engineering.
computer apr.
.
s. davies m. roper and m. wood.
using bug report similarity to enhance bug localisation.
in proceedings of the 19th working conference on reverse engineering wcre pages washington dc usa .
ieee computer society.
f. diaz.
regularizing query based retrieval scores.
information retrieval .
b. dit l. guerrouj d. poshyvanyk and g. antoniol.
can better identifier splitting techniques help feature location?
in proceedings of the ieee 19th international conference on program comprehension icpc pages washington dc usa .
ieee computer society.
b. dit m. revelle m. gethers and d. poshyvanyk.
feature location in source code a taxonomy and survey.
journal of software evolution and process .
e. enslen e. hill l. pollock and k. vijay shanker.
mining source code to automatically split identifiers for software analysis.
in proceedings of the 6th ieee international working conference on mining software repositories msr pages washington dc usa .
ieee computer society.
h. fang t. tao and c. zhai.
a formal study of information retrieval heuristics.
in proc.
of the acm sigir conference pages .
g. gay s. haiduc a. marcus and t. menzies.
on the use of relevance feedback in ir based concept location.
in proceedings of the ieee international conference on software maintenance pages .
e. hill s. rao and a. kak.
on the use of stemming for concern location and bug localization in java.
in proceedings of the12th ieee international working conference on source code analysis and manipulation scam .
t. hofmann.
probabilistic latent semantic indexing.
in proceedings of the 22nd annual international acm sigir conference on research and development in information retrieval pages .
acm .
d. hovemeyer and w. pugh.
finding bugs is easy.
sigplan not.
dec. .
j. jeon w. b. croft and j. h. lee.
finding similar questions in large question and answer archives.
in proc.
of the 14th acm conference on information and knowledge management pages .
j. a. jones and m. j. harrold.
empirical evaluation of the tarantula automatic fault localization technique.
in proceedings of the 20th ieee acm international conference on automated software engineering ase pages new york ny usa .
acm.
m. lease j. allan and w. b. croft.
regression rank learning to meet the opportunity of descriptive queries.
in proceedings of the european conference on information retrieval pages .
b. liblit m. naik a. x. zheng a. aiken and m. i. jordan.
scalable statistical bug isolation.
in proceedings of the acm sigplan conference on programming language design and implementation pldi pages new york ny usa .
acm.
t. y .
liu j. xu t. qin w. xiong and h. li.
letor benchmark dataset for research on learning to rank for information retrieval.
in proceedings of sigir workshop on learning to rank for information retrieval pages .
s. k. lukins n. a. kraft and l. h. etzkorn.
bug localization using latent dirichlet allocation.
information and software technology .
s. mani r. catherine v .
s. sinha and a. dubey.
ausum approach for unsupervised bug report summarization.
in proceedings of the acm sigsoft 20th international symposium on the foundations of software engineering fse pages .
c. d. manning p. raghavan and h. sch utze.
introduction to information retrieval .
cambridge university press .
a. marcus a. sergeyev v .
rajlich and j. i. maletic.
an information retrieval approach to concept location in source code.
in proceedings of the 11th working conference on reverse engineering wcre pages washington dc usa .
ieee computer society.
a. t. nguyen t. t. nguyen j. al kofahi h. v .
nguyen and t. nguyen.
a topic based approach for narrowing the search space of buggy files from a bug report.
in proceedings of the 26th ieee acm international conference on automated software engineering pages .
j. m. ponte and w. b. croft.
a language modeling approach to information retrieval.
in proceedings of the 21st annual acm sigir conference pages .
d. poshyvanyk y .
g. gueheneuc a. marcus g. antoniol and v .
rajlich.
feature location using probabilistic ranking of methods based on execution scenarios and information retrieval.
ieee transactions on software engineering june .
d. poshyvanyk and a. marcus.
combining formal concept analysis with information retrieval for concept location in source code.
in proceedings of the 15th ieee international conference on program comprehension icpc pages .
s. rao and a. kak.
retrieval from software libraries for bug localization a comparative study of generic and composite text models.
in proceedings of the 8th working conference on mining software repositories msr pages .
s. rao and a. kak.
morebugs a new dataset for benchmarking algorithms for information retrieval from software repositories trece .
technical report purdue university school of electrical and computer engineering april .
s. robertson h. zaragoza and m. taylor.
simple bm25 extension to multiple weighted fields.
in proc.
of the 13th acm conference on information and knowledge management cikm pages .
s. e. robertson s. walker and m. beaulieu.
experimentation as a way of life okapi at trec.
information processing management .
j. rocchio.
relevance feedback in information retrieval.
smart retrieval system experiments in automatic document processing .
g. salton and c. buckley.
term weighting approaches in automatic text retrieval.
information processing management .
n. shahmehri m. kamkar and p. fritzson.
semi automatic bug localization in software maintenance.
in proceedings of the international conference on software maintenance pages .
e. y .
shapiro.
algorithmic program debugging .
mit press cambridge ma usa .
a. singhal.
modern information retrieval a brief overview.
ieee data engineering bulletin .
a. singhal c. buckley and m. mitra.
pivoted document length normalization.
in proceedings of the 19th annual international acm sigir conference on research and development in information retrieval pages .
acm .
b. sisman and a. kak.
incorporating version histories in information retrieval based bug localization.
in proceedings of the 9th ieee working conference on mining software repositories pages .
t. strohman d. metzler h. turtle and w. b. croft.
indri a language model based search engine for complex queries.
in proceedings of the international conference on intelligent analysis pages .
h. k. wright m. kim and d. e. perry.
validity concerns in software engineering research.
in proc.
of the fse sdp workshop on future of software engineering eesearch foser pages .
acm .
x. yi and j. allan.
a comparative study of utilizing topic models for information retrieval.
in proceedings of the 31st european conference on information retrieval ecir pages .
springer verlag .
a. zeller and r. hildebrandt.
simplifying and isolating failure inducing input.
ieee trans.
softw.
eng.
feb. .
c. zhai.
notes on the lemur tfidf model unpublished work .
technical report carnegie mellon university .
x. zhang h. he n. gupta and r. gupta.
experimental evaluation of using dynamic slices for fault location.
in proceedings of the 6th international symposium on automated analysis driven debugging aadebug pages new york ny usa .
acm.
j. zhou h. zhang and d. lo.
where should the bugs be fixed?
more accurate information retrieval based bug localization based on bug reports.
in proceedings of the international conference on software engineering icse pages piscataway nj usa .
ieee press.