a search engine for finding highly relevant applications mark grechanik chen fu qing xie accenture technology labs chicago il mark.grechanik chen.fu qing.xie accenture.comcollin mcmillan denys poshyvanyk the college of william and mary williamsburg va cmc denys cs.wm.educhad cumby accenture technology labs chicago il chad.c.cumby accenture.com abstract a fundamental problem of finding applications that are highly relevant to development tasks is the mismatch between the high level intent reflected in the descriptions of these tasks and low level implementation details of applications.
to reduce this mismatch we created an approach called exemplar executable examples archive for finding highly relevant software projects from large archives of applications.
after a programmer enters a naturallanguage query that contains high level concepts e.g.
mime data sets exemplar uses information retrieval and program analysis techniques to retrieve applications that implement these concepts.
our case study with professional java programmers shows that exemplar is more effective than sourceforge in helping programmers to quickly find highly relevant applications.
.
introduction creating software from existing components is a fundamental challenge of software reuse.
naturally when programmers develop software they instinctively sense that there are fragments of code that other programmers wrote and these fragments can be reused.
reusing fragments of existing applications is beneficial because complete applications provide programmers with the contexts in which these fragments exist.
unfortunately few major challenges make it difficult to locate existing applications that contain relevant code fragments.
a fundamental problem of finding relevant applications is the mismatch between the high level intent reflected in the descriptions of these applications and low level implementation details.
this problem is known as the concept assignment problem .
many search engines match keywords in queries to words in the descriptions of the applications comments in their source code and the names of program variables and types.
if no match is found then potentially relevant applications are never retrieved from repositories.
this situation is aggravated by the fact that many application repositories are polluted with poorly functioning projects a match between a keyword from the query with a word in the description or in the source code of an application does not guarantee that this application is relevant to the query.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may cape town south africa copyright acm ... .
.currently a prevalent way for programmers to determine if an application is relevant to their task s is to download the application locate and examine fragments of the code that implement features of interest and observe and analyze the runtime behavior to ensure that the features behave as desired.
this process is manual and laborious programmers study the source code and executions profiles of the retrieved applications in order to determine whether they match task descriptions.
typically search engines do little to ensure that retrieved applications contain code fragments that are relevant to requirements that developers need to implement.
short code snippets that are returned as results to user queries do not give enough background or context to help programmers determine how to reuse these snippets and programmers typically invest a significant intellectual effort i.e.
they need to overcome a high cognitive distance to understand how to use these code snippets in larger scopes.
on the other hand if code fragments are retrieved in the contexts of applications it makes it easier for programmers to understand how to reuse these code fragments.
a majority of code search engines treat code as plain text where all words have unknown semantics.
however applications contain functional abstractions in a form of api calls whose semantics are defined precisely.
the idea of using api calls to improve code search was proposed and implemented elsewhere however it was not evaluated with statistical significance over a large codebase using a standard information retrieval methodology pages .
we created an application search system called exemplar executable examples archive that helps users find highly relevant executable applications for reuse.
exemplar combines information retrieval and program analysis techniques to reliably link high level concepts to the source code of the applications via standard thirdparty application programming interface api calls that these applications use.
we have built exemplar as part of our s3architecture and conducted a case study with professional java programmers to evaluate this search engine.
the results show with strong statistical significance that users find more relevant applications with higher precision with exemplar than those with sourceforge.
exemplar is available for public use1.
.
our approach in this section we describe the key ideas and give intuition about why and how our approach works.
.
the problem a straightforward approach for finding highly relevant applications is to search through the source code of applications to match from queries to the names of program variables and types.
the precision of this approach depends highly on programmers choosing meaningful names but their compliance is generally difficult to enforce .
this problem is partially addressed by programmers who create meaningful descriptions of the applications that they deposit into software repositories.
however state of the art search engines use exact matches between the keywords from queries the words in the descriptions and the source code of applications making it difficult for users to guess exact keywords to find relevant applications.
this is known as the vocabulary problem which states that no single word can be chosen to describe a programming concept in the best way .
this problem is general to all search engines but somewhat mitigated by the fact that different programmers who participate in the projects use their vocabularies to write code comments and descriptions of these projects.
modern search engines do little to ensure that retrieved applications are highly relevant to tasks or requirements that are described using high level queries.
to ensure relevancy a code mining system should take high level queries and return executable applications whose functionality is described by high level requirements thereby solving an instance of the concept assignment problem.
we believe that the rich context provided by whole applications make it easier for programmers to reuse code fragments from these applications.
.
key ideas our goal is to automate parts of the human driven procedure of searching for relevant applications.
suppose that requirements specify that a program should encrypt and compress data.
when retrieving sample applications from sourceforge2using the keywordsencrypt andcompress programmers look at the source code to check to see if some api calls from third party packages are used to encrypt and compress data.
even though the presence of these api calls does not guarantee that the applications are relevant it is a good starting point for deciding whether to check these applications further.
what we seek is to augment standard code search to include api documentations of widely used libraries such as standard java development kit jdk .
of course existing engines allow users to search for specific api calls but knowing in advance what calls to search for is hard.
our idea is to match keywords from queries to words in help documentation for api calls in addition to finding keyword matches in the descriptions and the source code of applications.
when programmers read these help documents about api calls that implement certain high level concepts they trust these documents because they come from known and respected vendors were written by different people reviewed multiple times and have been used by other programmers who report their experience at different forums.
help documents are more verbose and accurate and consequently trusted more than the descriptions of applications from repositories .
in addition we observe that relations between concepts entered in queries are often preserved as dataflow links between api calls that implement these concepts in the program code.
this observation is closely related to the concept of the software reflexion models formulated by murphy notkin and sullivan where relations between elements of high level models e.g.
processing elements of software architectures are preserved in their implementations in source code .
for example if the user enters keywords secure andsend and the corresponding api calls encrypt as of september .andemail are connected via some dataflow then an application with these connected api calls are more relevant to the query than ones where these calls are not connected.
consider for example two api calls string encrypt andvoid email string .
after the call encrypt is invoked it returns a string that is stored in some variable.
at some later point a call to the function email is made and the variable is passed as the input parameter.
in this case we say that these functions are connected using a dataflow link which reflects the implicit logical connection between keywords in queries specifically the data should be encrypted and then sent to some destination.
to improve the precision of our approach our idea is to determine relations between api calls in retrieved applications.
all things equal if a dataflow link is present between two api calls in the program code of one application and there is no link between the same api calls in some other application then the former application should have a higher ranking than the latter.
in addition knowing how api calls are connected using dataflows enables programmers to better understand the contexts of the code fragments that contain these api calls.
finally it is possible to utilize dataflow connections to extract code fragments which is a subject of our future work on our s3architecture .
.
our goal our goal is to develop a search engine that is most effective in the solution domain i.e.
the domain in which engineers use their ingenuity to solve problems .
in the problem domain requirements are expressed using vague objectives or wish lists.
conversely in the solution domain engineers go into implementation details.
to realize requirements in the solution domain engineers look for reusable abstractions that are often implemented using third party api calls.
thus exemplar should be most effective when keywords reflect the reality of the solution domain.
consider a situation in which engineers develop a matchmaking application.
using keywords such as sweet andlove to describe requirements from the problem domain it is possible to find a variety of applications in sourceforge with according descriptions.
however it is unlikely that these keywords are used to describe api calls in java documentation so it is up to engineers to investigate retrieved applications to determine if any code can be reused.
since exemplar uses basic word matches in addition to locating api calls it performs equally well in this situation when compared with existing search engines.
exemplar is more effective than existing code search engines when keywords come from the solution domain.
consider the following task find an application for sharing viewing and exploring large data sets that are encoded using mime and the data can be stored using key value pairs.
using the following keywords mime share view data sets key value pairs an unlikely candidate application called biolap is retrieved using exemplar with a high ranking score.
the description of this application matches only the keywords data sets and yet this application made it to the top five of the list.
the reason is that biolap uses the class mimetype specifically its method getparametermap that deals with mimeencoded data.
the descriptions of this class and this method contain the desired keywords and these implementation details are highly relevant to the solution domain for the given task.
it is needless to say that biolap which is contained in the sourceforge repository does not show on the top list of retrieved applications when the search is performed with the sourceforge search engine.
the same situation happens when users enter other queries that contain matching keywords in the descriptions of api calls.keywordapp1 appn...descriptions of apps a standard search engines.keywordapp1 appn...descriptions of api callsapi call1 api call3api call2 b exemplar search engine.
figure illustrations of the processes for standard and exemplar search engines.
the idea behind exemplar is to enable engineers to retrieve hidden and highly relevant applications for their solution domains.
.
our approach we describe our approach using an illustration of differences between the process for standard search engines shown in figure a and the exemplar process shown in figure b .
consider the process for standard search engines e.g.
sourceforge google code search shown in figure a .
a keyword from the query is matched against words in the descriptions of the applications in some repository sourceforge krugle or words in the entire corpus of source code google code search .
when a match is found applications app 1toapp nare returned.
consider the process for exemplar shown in figure b .
a keyword from the query is matched against the descriptions of different documents that describe api calls of widely used software packages.
when a match is found the names of the api calls call 1to call kare returned.
these names are matched against the names of the functions invoked in these applications.
when a match is found applications app 1toapp nare returned.
a fundamental difference between these search schemes is that exemplar uses help documents to obtain the names of the api calls in response to user queries.
so can be viewed as instances of the query expansion concept in information retrieval systems andconcept location .
the aim of query expansion is to reduce this query document mismatch by expanding the query with concepts that have similar meanings to the set of relevant documents.
using help documents the initial query is expanded to include the names of the api calls whose semantics unequivocally reflects specific behavior of the matched applications.
in addition to the keyword matching functionality of standard search engines exemplar matches keywords with the descriptions of the various api calls in help documents.
since a typical application invokes api calls from several different libraries the help documents associated with these api calls are usually written by different people who use different vocabularies.
the richness of these vocabularies makes it more likely to find matches and produce api calls api call 1toapi call k. if some help document does not contain a desired match some other document may yield a match.
this is how we address the vocabulary problem .
as it is shown in figure b api calls api call api call andapi call 3are invoked in the app .
it is less probable that the search engine fails to find matches in help documents for all three api calls and therefore the application app 1will be retrieved from the repository.
searching help documents produces additional benefits.
api calls from help documents are linked to locations in the project source code where these api calls are used thereby allowing programmers to navigate directly to these locations and see how highlevel concepts from queries are implemented in the source code.
so solves an instance of the concept assignment problem .
.
exemplar architecture the architecture for exemplar is shown in figure .
the main elements of the exemplar architecture are the database holding applications i.e.
the apps archive the search and ranking engines and the api call lookup.
applications metadata describes dataflow links between different api calls invoked in the applications.
exemplar is being built on an internal extensible database of help documents that come from the jdk api documentation.
it is easy to extend exemplar by plugging in different help documents for other widely used third party libraries.
the inputs to exemplar are shown in figure with thick solid arrows labeled and .
the output is shown with the thick dashed arrow labeled .
exemplar works as follows.
the input to the system are help documents describing various api calls .
the help page processor indexes the description of the api calls in these help documents and outputs the api calls dictionary which is the set of tuples word ... word n api call linking selected words from the descriptions of the api calls to the names of these api calls .
our approach for mapping words in queries to api calls is different from the keyword programming technique since we derive mappings between words and apis from external documentation rather than source code.
when the user enters a query it is passed to the api call lookup component along with the api calls dictionary .
the lookup engine searches the dictionary using the words in the query as keys and outputs the set of the names of the api calls whose descriptions contain words that match the words from the query .
these api calls serve as an input to the search engine along with the apps archive .
the engine searches the archive and retrieves applications that contain input api calls .
the analyzer pre computes the applications metadata that contains dataflow links between different api calls from the applications source code .
since this is done offline precise program analysis can be accommodated in this framework to achieve better results in dataflow ranking.
this metadata is supplied to the ranking engine along with the retrieved applications and the ranking engine combines keyword matching score with api call scores to produce a unified rank for each retrieved help pagesapi call lookupapi calls search engine apps archive analyzer applications metadataretrieved applications ranking enginerelevant applicationshelp page processorapi calls dictionary figure exemplar architecture.application.
finally the engine sorts applications using their ranks and it outputs relevant applications which are returned to the user .
.
ranking in this section we discuss our ranking algorithm and its components such as dataflow computations.
.
components of ranking there are three components that compute different scores in the exemplar ranking mechanism a component that computes a score based on word occurrences wos a component that computes a score based on the number of relevant api calls ras and a component that computes a score based on dataflow connections between these calls dcs .
the total ranking score is the weighted sum of these three ranking scores.
each component produces results of different perspectives i.e.
word matches api calls dataflow connections .
our goal is to produce a unified ranking by putting these different rankings together in a single score.
the purpose of wos is to enable exemplar to retrieve applications based on matches between words in queries and words in the descriptions of applications in repositories.
this is a baseline exemplar search that should be as effective as the one of sourceforge.
this approach may be useful for a small subset of applications that do not use any third party api calls to implement different functions.
in this case exemplar s ranking engine should rely on word matches to return relevant applications.
however our investigation of available projects in sourceforge shows that a majority of applications use third party api calls and some of these calls implement functionality that is typically referred in keywords from user queries.
simply put the more relevant api calls are found in an application the higher its rank should be.
the component ras computes the api call based ranking score.
finally the component dcs computes a score based on weighted dataflow links.
we detect different ways of passing data between api calls and assign weights differently to these dataflow links.
we discuss these ranking components in depth below.
.
wos ranking wos component uses okapi bm25 which is a ranking function that is typically used by search engines to rank matching documents according to their relevance to a given search query.
this function is implemented in the lucene java framework which is used in exemplar and it is distinguished by trec for its performance and considered as state of the art in the ir community .
bm25 is a standard bag of words retrieval function that ranks a set of documents based on the relative proximity of query terms e.g.
without dependencies appearing in each document.
bm25 score is computed as swos n i 1idf qi f qi d k f qi d k b b d d where f qi d is the qi s term frequency in the document dwith the length i.e.
the number of words d d is the average document length in the text collection from which documents are drawn kandbare parameters whose values are usually chosen .
and .
respectively and finally the idf qi is the inverse document frequency weight of the query term qi.
.
ras ranking we consider each section in the library documentation that describes different api calls as a separate document.
the collection of api documents is defined as dapi d1 api d2 api ... dk api .
a corpus is created from dapiand represented as the term by document m kmatrix m where mis the number of terms and kis the number of api documents in the collection.
a generic entrya in this matrix denotes a measure of the weight of the ith term in the jthapi document .
api calls that are relevant to the user query are obtained by ranking documents dapithat describe these calls as relevant to the query q. this relevance is computed as a conceptual similarity c i.e.
the length normalized inner product between the user query q and each api document dapi.
as a result the set of triples angbracketlefta c n angbracketrightis returned where ais the api call nis the number of occurrences of this api call in the application with the conceptual similarity c of the api call documentation to query terms.
the api call based ranking score for the application j is computed as sj ras p i 1nj i cj i a j where a jis the total number of api calls in the application j. .
dcs ranking to improve the precision of ranking we derive the structure of connections between api calls and use this structure as an important component in computing rankings.
the standard syntax for invoking an api call is t var o.callname p1 ... pn .
the structural relations between api calls reflect compositional properties between these calls.
specifically it means that api calls access and manipulate data at the same memory locations.
there are four types of dependencies between api calls input output true and anti dependence .
true dependence occurs when the api call fwrite a memory location that the api callglater reads e.g.
var f ... ... g var ... .
antidependence occurs when the api call freads a memory location that the api call glater writes e.g.
f var ... ... var g ... .
output dependence occurs when the api calls fandgwrite the same memory location.
finally input dependence occurs when the api callsfandgread the same memory location.
consider an all connected graph i.e.
a clique where nodes are api calls and the edges represent dependencies between these calls.
the absence of an edge means that there is no dependency between two api calls.
let the total number of connections between nrelevant api calls be less or equal to n n .
let a connection between two distinct api calls in the application be defined as link we assign some weight wto this link based on the strength of the dataflow or control flow dependency type.
the ranking is normalized to be between and .
the api call connectivity based ranking score for the application j is computed as sj dcs n n i 1wj i linkj i n n where wiis the weight to each type of flow dependency for the given link link i such that wtrue i wanti i wout put i winput i .
the intuition behind using this order is that these dependencies contribute differently to ranking heuristics.
specifically using the values of the same variable in two api calls introduces a weaker link as compared to the true dependency where one api call produces a value that is used in some other api call.
.
combined ranking the final ranking score is computed as s wosswos rassras dcssdcs where is the interpolation weight for each type of the score.
these weights are determined independently of queries unlike the scores which are query dependent.
adjusting these weights enables experimentation with how underlying structural and textual information in application affects resulting ranking scores.
.
implementation in this section we describe how we implemented exemplar.
.
crawlers exemplar consists of two crawlers archiver andwalker .archiver populated exemplar s repository by retrieving from sourceforge more than java projects that contain close to submitted archive files which comprise the total of files.
walker traverses exemplar s repository opens each project by extracting its source code from zipped archive and applies a dataflow computation utility to the extracted source code.
in addition the archiver regularly checks sourceforge to see if there are new updates and it downloads these updates into the exemplar repository.
to extract all occurrences of invocations of jdk api calls in all available java projects we ran threads for over hours on five servers and workstations three of these servers have two dual core .8ghz em64t xeon processors with 8gb ram each and two have four .0ghz em64t xeon cpus with 32gb ram each.
the workstations uniformly had one .83ghz quad core cpu and 2gb ram.
this job resulted in finding close to twelve million invocations of these api calls from jdk .
across all projects.
the next item was to compute dataflow connections between these calls in all java applications in the exemplar s repository.
.
dataflow computation our approach relies on the tool pmd3for computing approximate dataflow links which are based on patterns of dataflow dependencies.
using these patterns it is possible to recover a large number of possible dataflow links between api calls however some of these recovered links can be false positives.
in addition we currently recover links among api calls within files intraprocedurally hence it is likely that some intraprocedural links are missed and no interprocedural analyses are performed.
.
computing rankings we use the lucene search engine4to implement the core retrieval based on keyword matches.
we indexed descriptions and titles of java applications and independently we indexed java api call documentation by duplicating descriptions about the classes and packages in each methods.
thus when users enter keywords they are matched separately using the index for titles and descriptions and the index for api call documents.
as a result two lists are retrieved the list of applications and the list of api calls.
each entry in these lists are accompanied by a rank i.e.
conceptual similarity c a number between and .
the next step is to locate retrieved api calls in the retrieved applications.
to improve the performance we configure exemplar to use the positions of the top two hundred api calls in the retrieved list.
these api calls are crosschecked against api calls invoked in the retrieved applications and the combined ranking score is computed for each application.
the list of applications is sorted using the computed ranks and returned to the user.
.
case study design typically search engines are evaluated using manual relevance judgments by experts .
to determine how effective exemplar is we conducted a case study with participants who are java programmers.
we gave a list of tasks described in english.
our goal is to evaluate how well these participants can find applications that match given tasks using three different search as of september .
as of september .engines sourceforge sf and exemplar with ewd and without end dataflow links as part of the ranking mechanism.
we chose to compare exemplar with sourceforge because the latter has a popular search engine with the largest open source java project repository and exemplar is populated with java projects from this repository.
.
methodology we used a cross validation study design in a cohort of participants who were randomly divided into three groups.
the study was sectioned in three experiments in which each group was given a different search engine i.e.
sf ewd or end to find applications for given tasks.
each group used a different task in each experiment.
thus each participant used each search engine on different tasks in this case study.
before the study we gave a one hour tutorial on using these search engines to find applications for tasks.
each experiment consisted of three steps.
first participants translated tasks into a sequence of keywords that described key concepts of applications that they needed to find.
then participants entered these keywords as queries into the search engines the order of these keywords does not matter and obtained lists of applications that were ranked in descending order.
the next step was to examine the returned applications and to determine if they matched the tasks.
each participant accomplished this step individually assigning a confidence level c to the examined applications using a four level likert scale.
we asked participants to examine only top ten applications that resulted from their searches.
the guidelines for assigning confidence levels are the following.
.
completely irrelevant there is absolutely nothing that the participant can use from this retrieved project nothing in it is related to your keywords.
.
mostly irrelevant only few remotely relevant code snippets or api calls are located in the project.
.
mostly relevant a somewhat large number of relevant code snippets or api calls in the project.
.
highly relevant the participant is confident that code snippets or api calls in the project can be reused.
twenty six participants are accenture employees who work on consulting engagements as professional java programmers for different client companies.
remaining participants are graduate students from the university of illinois at chicago who have at least six months of java experience.
accenture participants have different backgrounds experience and belong to different groups of the total accenture workforce of approximately employees.
out of participants had programming experience with java ranging from one to three years and participants reported more than three years of experience writing programs in java.
eleven participants reported prior experience with sourceforge which is used in this case study participants reported prior experience with other search engines and said that they never used code search engines.
twenty six participants have bachelor degrees and thirteen have master degrees in different technical disciplines.
.
precision two main measures for evaluating the effectiveness of retrieval are precision and recall .
the precision is calculated as pr of retrieved applications that are relevant total of retrieved applications i.e.
the precision of a ranking method is the fraction of the top r ranked documents that are relevant to the query where r inthis case study.
relevant applications are counted only if they are ranked with the confidence levels 4or3.
the precision metrics reflects the accuracy of the search.
since we limit the investigation of the retrieved applications to top ten the recall is not measured in this study.
.
hypotheses we introduce the following null and alternative hypotheses to evaluate how close the means are for the cs and ps for control and treatment groups.
unless we specify otherwise participants of the treatment group use either end or ewd and participants of the control group use sf.
we seek to evaluate the following hypotheses at a .
level of significance.
h0the primary null hypothesis is that there is no difference in the values of confidence level and precision per task between participants who use sf ewd and end.
h1an alternative hypothesis to h0is that there is statistically significant difference in the values of confidence level and precision between participants who use sf ewd and end.
once we test the null hypothesis h0 we are interested in the directionality of means of the results of control and treatment groups.
we are interested to compare the effectiveness of ewd versus the end and sf with respect to the values of confidence level c and precision p. h1 c of ewd versus sf the effective null hypothesis is that ewd c sf c while the true null hypothesis is that ewd c sf c. conversely the alternative hypothesis is ewd c sf c. h2 p of ewd versus sf the effective null hypothesis is that ewd p sf p while the true null hypothesis is that ewd p sf p. conversely the alternative hypothesis is ewd p sf p. h3 c of ewd versus end the effective null hypothesis is that ewd c end c while the true null hypothesis is that ewd c end c. conversely the alternative is ewd c end c. h4 p of ewd versus end the effective null hypothesis is that ewd p end p while the true null hypothesis is that ewd p end p. conversely the alternative is ewd p end p. h5 c of end versus sf the effective null hypothesis is that end c sf c while the true null hypothesis is that end c sf c. conversely the alternative hypothesis is end c sf c. h6 p of end versus sf the effective null hypothesis is that end p sf p while the true null hypothesis is that end p sf p. conversely the alternative hypothesis is end p sf p. the rationale behind the alternative hypotheses to h1andh2is that exemplar allows users to quickly understand how keywords in queries are related to implementations using api calls in retrieved applications.
the alternative hypotheses to h3 and h4 are motivated by the fact that if users see dataflow connections between api calls they can make better decisions about how closely retrieved applications match given tasks.
finally having the alternative hypotheses to h5andh6ensures that exemplar without dataflow links still allows users to quickly understand how keywords in queries are related to implementations using api calls in retrieved applications.
.
task design we designed tasks that participants work on during experiments in a way that these tasks belong to domains that are easy to understand and they have similar complexity.
a sample task for instance asks a user to design a java applications for sharing viewing and exploring large data sets that are encoded using mime.
additional criteria for these tasks is that they should represent real world programming tasks and should not be biased towards any of the search engines that are used in this experiment.
descriptions of these tasks should be flexible enough to allow participants to suggest different keywords for searching.
this criteria significantly reduces any bias towards evaluated search engines.
.
normalizing sources of variations sources of variation are all issues that could cause an observation to have a different value from another observation.
we identify sources of variation as the prior experience of the participants with specific applications retrieved by the search engines in this study the amount of time they spend on learning how to use search engines and different computing environments which they use to evaluate retrieved applications.
the first point is sensitive since some participants who already know how some retrieved applications behave are likely to be much more effective than other participants who know nothing of these applications.
we design this experiment to drastically reduce the effects of covariates i.e.
nuisance factors in order to normalize sources of variations.
using the cross validation design we normalize variations to a certain degree since each participant uses all three search engines on different tasks.
.
tests and the normality assumption we use one way anov a t tests for paired two sample for means and 2to evaluate the hypotheses.
these tests are based on an assumption that the population is normally distributed.
the law of large numbers states that if the population sample is sufficiently large between to participants then the central limit theorem applies even if the population is not normally distributed pages .
since we have participants the central limit theorem applies and the above mentioned tests have statistical significance.
.
threats to validity in this section we discuss threats to the validity of this case study and how we address these threats.
.
.
internal validity internal validity refers to the degree of validity of statements about cause effect inferences.
in the context of our experiment threats to internal validity come from confounding the effects of differences among participants tasks and time pressure.
participants.
since evaluating hypotheses is based on the data collected from participants we identify two threats to internal validity java proficiency and motivation of participants.
even though we selected participants who have working knowledge of java as it was documented by human resources we did not conduct an independent assessment of how proficient these participants are in java.
the danger of having poor java programmers as participants of our case study is that they can make poor choices of which retrieved applications better match their queries.
this threat is mitigated by the fact that all participants from accenture worked on successful commercial projects as java programmers.
the other threat to validity is that not all participants could be motivated sufficiently to evaluate retrieved applications.
we addressed this threat by asking participants to explain in a couple of a confidence level c. b precision p. figure statistical summary of the results of the case study for cand p.the central box represents the values from the lower to upper quartile to percentile .
the middle line represents the median.
the thicker vertical line extends from the minimum to the maximum value.
the filled out box represents the values from the minimum to the mean and the thinner vertical line extends from the quarter below the mean to the quarter above the mean.
sentences why they chose to assign certain confidence level to applications and based on their results we financially awarded top five performers.
tasks.
improper tasks pose a big threat to validity.
if tasks are too general or trivial e.g.
open a file and read its data into memory then every application that has file related api calls will be retrieved thus creating bias towards exemplar.
on the other hand if application and domain specific keywords describe task e.g.
genealogy andgentech only a few applications will be retrieved whose descriptions contain these keywords thus creating a bias towards sourceforge.
to avoid this threat we based the task descriptions on a dozen of specifications of different software systems that were written by different people for different companies.
time pressure.
each experiment lasted for two hours and for some participants it was not enough time to explore all retrieved applications for each of eight tasks.
it is a threat to validity that some participants could try to accomplish more tasks by shallowly evaluating retrieved applications.
to counter this threat we notified participants that their results would be discarded if we did not see sufficient reported evidence of why they evaluated retrieved applications with certain confidence levels.
.
.
external validity to make results of this case study generalizable we must address threats to external validity which refer to the generalizability of a casual relationship beyond the circumstances of our case study.
the fact that supports the validity of the case study design is that the participants are highly representative of professional java programmers.
however a threat to external validity concerns the usage of search tools in the industrial settings where requirements are updated on a regular basis.
programmers use these updated requirements to refine their queries and locate relevant applications using multiple iterations of working with search engines.
we addressed this threat only partially by allowing programmers to refinetheir queries multiple times.
in addition it is sometimes the case when engineers perform multiple searches using different combinations of keywords and they select certain retrieved applications from each of these search results.
we believe that the results produced by asking participants to decide on keywords and then perform a single search and rank applications do not deviate significantly from the situation where searches using multiple refined queries are performed.
the other threat to external validity comes from different sizes of software repositories.
we populated exemplar s repository with all java projects from the sourceforge repository to address this threat to external validity.
.
results in this section we report the results of the case study and evaluate the null hypotheses.
.
case study results we use one way anov a t tests for paired two sample for means and 2to evaluate the hypotheses that we stated in section .
.
.
.
variables a main independent variable is the search engine sf ewd end that participants use to find relevant java applications.
the other independent variable is participants java experience.
dependent variables are the values of confidence level c and precision p. we report these variables in this section.
the effect of other variables task description length prior knowledge is minimized by the design of this case study.
.
.
testing the null hypothesis we used anov a to evaluate the null hypothesis h0that the variation in an experiment is no greater than that due to normal variation of individuals characteristics and error in their measurement.
the results of anov a confirm that there are large differ h var approach samples min max median 2df c p t tcrit h1 cewd .
.
.
.
.
.96sf .
.
h2 pewd .
.
.
.
.
.
.
.
.03sf .
.
.
.
.
h3 cewd .
.
.
.
.
.96end .
.
h4 pewd .
.
.
.
.
.
.
.
.03end .
.
.
.
.
h5 cend .
.
.
.
.
.96sf .
.
h6 pend .
.
.
.
.
.
.
.
2sf .
.
.
.
table results of t tests of hypotheses h for paired two sample for means for two tail distribution for dependent variable specified in the column var either corp whose measurements are reported in the following columns.
extremal values median means variance 2 degrees of freedom df and the pearson correlation coefficient c are reported along with the results of the evaluation of the hypotheses i.e.
statistical significance p and the tstatistics.
java cper par for relev scores p average expert sf end ewd sf end ewd yes .
.
.
.
no .
.
.
.
.
.
summary .
.
.
.
.
.
table contingency table shows relationship between cs per participant for relevant scores and ps for participants with and without expert java experience.
ences between the groups for cwith f fcrit with p .
55which is strongly statistically significant.
the mean cfor the sf approach is .
with the variance .
which is smaller than the mean cfor end .
with the variance .
and it is smaller than the mean cfor ewd .
with the variance .
.
also the results of anov a confirm that there are large differences between the groups for pwith f fcrit .
with p 6which is strongly statistically significant.
the mean p for the sf approach is .
with the variance .
which is smaller than the mean pfor end .
with the variance .
and it is smaller than the mean pfor ewd .
with the variance .
.
based on these results we reject the null hypothesis and we accept the alternative hypothesis h1.
a statistical summary of the results of the case study for cand t median quartiles range and extreme values are shown as boxand whisker plots in figure a and figure b correspondingly with confidence interval for the mean.
.
.
comparing sourceforge with exemplar to test the null hypothesis h1 h2 h5 and h6 we applied four t tests for paired two sample for means for candpfor participants who used sf and both variants of exemplar.
the results of this test forcand for pare shown in table .
the column samples shows that out of a total of participants participated in all experiments two participants missed one experiment .
based on these results we reject the null hypotheses h1 h2 h5 and h6 and we accept the alternative hypotheses that states that participants who use exemplar report higher relevance and precision on finding relevant applications than those who use sourceforge .
.
.
comparing ewd with end to test the null hypotheses h3 and h4 we applied two t tests for paired two sample for means for candpfor participants who used the baseline end and ewd.
the results of this test for c and for pare shown in table .
based on these results we accept the null hypotheses h3 and h4 that say that participants who use ewd do not report higher relevance and precision on finding relevant applications than those who use end .
there are several explanations for this result.
first given that our dataflow analysis is imperfect some links are missed and subsequently the remaining links cannot affect the ranking score significantly.
second it is possible that our dataflow connectivitybased ranking mechanism needs fine tuning and it is a subject of our future work.
finally after the case study a few participants questioned the idea of dataflow connections between api calls.
as it turns out a few participants had vague ideas as to what dataflow connections meant and how to incorporate them into the evaluation process.
at this point it is a subject of our future work to investigate this phenomenon in more detail.
.
.
experience relationships we construct a contingency table to establish a relationship between candpfor participants with years and without less than years expert java experience as shown in table .
to test the null hypotheses that the categorical variables candpare independent from the categorical variable java experience we apply two 2 tests 2 cand 2 pforcandprespectively.
we obtain 2 c .
for p .
and 2 p .
for p .
.
the small values of 2allow us to reject these null hypotheses in favor of the alternative hypotheses suggesting that there is no statistically strong relationship between expert java programming experiences of participants and the values of reported cs and ps.
that is participants performed better with exemplar than with sourceforge independently of their java experience.
.
related work different code mining techniques and tools have been proposed to retrieve relevant software components from different repositories as it is shown in table .
codefinder iteratively refines code repositories in order to improve the precision of returned software components .
like exemplar codefinder reformulates queriesapproach granularity corpora query search input expansion codefinder m c d yes codebroker m c d yes mica f c c yes prospector f a c yes hipikat a c d c yes xsnippet f a d yes strathcona f c c yes amc f c c no google code f m a c a d c no sourceforge a c d no spars j m c c no sourcerer a c c no codegenie a c c no spotweb m c c yes parseweb f a c yes s6 f c a t c manual krugle f m a c a d c no koders f m a c a d c no sniff f m c a d c yes exemplar f m a c a d c yes table comparison of exemplar with other related approaches.
columngranularity specifies how search results are returned by each approach f ragment of code m odule or a pplication and how users specify queries c oncept a pi call or t est case .
the columncorpora specifies the scope of search i.e.
c ode or d ocuments followed by the column query expansion that specifies if an approach uses this technique to improve the precision of search queries.
in order to expand the search scope.
unlike exemplar codefinder heavily depends on the descriptions often incomplete of software components while exemplar links api help pages whose information is of higher quality than ad hoc descriptions of components.
codebroker system uses source code and comments written by programmers to query code repositories to find relevant artifacts .
unlike exemplar codebroker is dependent upon the descriptions of documents and meaningful names of program variables and types and this dependency often leads to lower precision of returned projects.
even though it returns code snippets rather than applications mica is similar to exemplar since it uses help pages to find relevant api calls to guide code search .
however mica uses help documentation to refine the results of the search while exemplar uses help pages as an integral instrument in order to expand the range of the query.
in addition exemplar returns executable projects while mica returns code snippets as well as non code artifacts.
sniff extends the idea of using documentation for api calls for query expansion in several ways .
after retrieving code fragments sniff then performs intersection of types in these code chunks to retain the most relevant and common part of the code chunks.
sniff also ranks these pruned chunks using the frequency of their occurrence in the indexed code base.
in contrast to sniff mica and our original msr idea we evaluated exemplar using a large scale case study with programmers to obtain statistically significant results we followed a standard ir methodology for comparing search engines and we return fully executable applications.
exemplar s internals differ substantiallyfrom previous attempts to use api calls for searching including sniff our search results contain multiple levels of granularity we conduct a thorough comparison with the state of art search engine using a large body of java application code and we are not tied to a specific ide.
prospector is a tool that synthesizes fragments of code in response to user queries that contain input types and desired output types .
prospector is an effective tool to assist programmers in writing complicated code however it falls short of providing support for a full fledged code search engine.
the hipikat tool recommends relevant development artifacts i.e.
source revisions associated with a past change task from a project s history to a developer .
unlike exemplar hipikat is a programming task oriented tool that does not recommend applications whose functionalities match high level requirements.
strathcona is a tool that heuristically matches the structure of the code under development to the example code .
strathcona is beneficial when assisting programmers while working with existing code however its utility is not applicable when searching for relevant projects given a query containing high level concepts with no source code.
robillard proposed an algorithm for calculating program elements of likely interest to a developer .
exemplar is similar to this algorithm in that it uses relations between api calls in the retrieved projects to compute the level of interest ranking of the project however it does not find relevant applications.
xsnippet is a context sensitive tool that allows developers to query a sample repository for code snippets that are relevant to the programming task at hand .
it remains to be seen how xsnippet can be applicable to finding relevant projects since its goal is to return code fragments based on existing code context.
existing work on ranking mechanisms for retrieving source code are centered on locating components of source code that match other components.
quality of match qom ranking measures the overall goodness of match between two given components which is different from exemplar which retrieves applications based on high level concepts that users specify in queries.
component rank model crm is based on analyzing actual usage relations of the components and propagating the significance through the usage relations .
unlike crm exemplar ranking mechanism is based on a combination of query expansion and relations between api calls that implement high level concepts in queries.
s6is a code search engine that uses a set of user guided program transformations to map high level queries into a subset of relevant code fragments not complete applications.
like exemplar s6uses query expansion however it requires additional low level details from the user such as data types of test cases.
.
conclusion we created an approach called exemplar for finding highly relevant software projects from a large archive of executable examples.
in exemplar we combined program analysis techniques with information retrieval to convert high level user queries to basic functional abstractions that are used automatically in code search engines to retrieve highly relevant applications.
we evaluated exemplar with professional java programmers and found with strong statistical significance that it performed better than sourceforge in terms of reporting higher confidence levels and precisions for retrieved java applications.
in addition participants expressed strong satisfaction with using exemplar since it enabled them to assess retrieved applications using well documented api calls from thirdparty trusted java libraries.