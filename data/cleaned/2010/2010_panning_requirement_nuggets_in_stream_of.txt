panning requirement nuggets in stream of software maintenance tickets senthil mani karthik sankaranarayanan vibha singhal sinha ibm research india sentmani kartsank vibha.sinha in.ibm.com and premkumar devanbu university of california davis ptdevanbu ucdavis.edu abstract there is an increasing trend to outsource maintenance of large applications and application portfolios of a business to third parties specialising in application maintenance who are incented to deliver the best possible maintenance at the lowest cost.
to do so they need to identify repeat problem areas which cause more maintenance grief and seek a unified remedy to avoid the costs spent on fixing these individually.
these repeat areas in a sense represent major evolving areas of need or requirements for the customer.
the information about the repeating problem is typically embedded in the unstructured text of multiple tickets waiting to be found and addressed.
currently repeat problems are found by manual analysis effective solutions depend on the collective experience of the team solving them.
in this paper we propose an approach to automatically analyze problem tickets to discover groups of problems being reported in them and provide meaningful descriptive labels to help interpret these groups.
our approach incorporates a cleansing phase to handle the high level of noise observed in problem tickets and a method to incorporate multiple text clustering techniques and merge their results in a meaningful manner.
we provide detailed experiments to quantitatively and qualitatively evaluate our approach.
categories and subject descriptors k. .
software maintenance keywords text clustering requirements mining software repositories .
introduction software maintenance is a crucial phase of any software project that continues long after the product has been developed.
there have been a number of estimates of effort spent in software maintenance and enhancement.
figures varying from percent of total systems and programming resources have been cited .
in some market segments the maintenance phase of the software permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
fse november hong kong china copyright acm ... .
.lifecycle has undergone dramatic changes.
rather than undertake this costly task in house many large and small businesses have taken to outsourcing their maintenance activities.
market research firms estimate the outsourced software maintenance business to be around billion a year business as of .the software is turned over complete with source code manuals data etc to a maintenance service provider such as ibm accenture cognizant infosys tcs inter alia .
the practice of outsourced software maintenance differs substantially from the traditional in house version.
the outsourced service provider typically operates the system in addition to providing maintenance.
maintenance in this setting consists of answering support calls and issuing and handling trouble tickets.
this is a challenging and dynamic business.
service providers compete on the basis of low costs and negotiated service level agreements slas which impose volumes and deadlines on the resolution of trouble tickets.
in a typical setting a service provider handles hundreds of calls a day hundreds or thousands of trouble tickets are issued and resolved on a weekly basis.
cost and the honouring of slas are of paramount importance.
tickets are typically handled one by one however groups of tickets often follow common patterns examples below .
recognising these patterns and exploiting common synergies with a unified response can lead to substantial cost quality and interval benefits.
the response could include process changes that alter the way certain tickets are handled changes to documentation automating a frequently used service and also changes to code to add new features.
the recognition of common patterns is both important and very challenging.
in practice managers are dependent on experience of individuals resolving these tickets to recognize and report these patterns on an ad hoc basis.
in a sense the emerging patterns in the tickets indicate a kind of requirements drift that is latent in the otherwise undifferentiated mass of individually generated tickets these patterns must be extracted somehow to allow efficient and co ordinated responses.
consider the example of a specific e commerce web site1 maintained by ibm.
it allows users to log in using a username and password.
the site did not provide a feature to reset the password and hence anytime a user forgot his her password they would open a problem ticket.
someone in the maintenance team would then reset the password.
in the first year of the web site there were only tickets related to password reset .
however as the user base increased the number of password resets increased to .
this was noticed informally by alert personnel.
the cost of enhancing the application to provide a self password reset option was .
however the cost of manual password reset was only .
hence the team 1identifying information omitted for confidentiality.permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
fse november hong kong china copyright acm ... .
678decided to not implement the feature.
in the subsequent year the password resets increased to and now the work around was not cost effective anymore and hence the feature was supported.
consider another example from a supply chain application portfolio maintained by ibm.
a client owned several warehouses and trucks delivering goods between them.
each truck was fitted with a gps device which on returning to its base warehouse triggered an alert that updated a database record marking the truck as available.
during the day to day operations the application maintenance team were receiving about tickets per month each asking for a manual release of trucks.
consultants were able to respond to this quickly in less than mins by manually running a simple query to update the database record and thus close these tickets.
however considering that an automatic gps based solution was already in place these tickets should not have arisen in the first place.
the repetitive pattern of problem tickets asking for a vehicle release pointed towards a bug in the application that should have been spotted investigated and resolved.
in first example to reduce maintenance cost the management team needs to be able to identify password reset as a common pattern because as we saw such commonly occurring problem patterns are likely to translate into requirements later on.
in the second example the management team needs to be able to identify vehicle release as a common pattern as this problem indicated a systematic application fault.
our research goal is to build tools that assist managers in identifying these emerging common patterns from the corpus of past tickets.
towards this end we make the following contributions to help outsourced maintenance software projects manage large volumes of trouble tickets tickets contain information about the problem in its title and description as natural language text.
we develop a text clustering method that identifies emerging patterns of tickets that can be addressed together in a unified manner.
it is imperative that maintenance teams are able to quickly validate whether the identified patterns indicate valid requirements for the maintained system.
hence the labels used for summarizing the patterns become very important.
we build a phrase based approach to assign labels to the identified ticket clusters in a way that would be strongly suggestive of unified responses and thus be salient to practitioners.
we evaluate the quality of the ticket clusters we find using an entropy based measure of assigning tickets to people using historical data and using qualitative methods on large application maintenance projects within ibm.
this need to mine common problem patterns to identify feature requests has also been seen in open source projects.
in authors have presented approaches based on text mining to identify problem patterns from support forums.
in authors apply their proposed technique on firefox forum entries and found problem clusters indicative of technical challenges users are facing that could be addressed with enhancements.
they report that firefox principal engineer felt that this kind of approach would help their open source community to prioritise their efforts around commonly reported challenges.
in the authors have applied an existing clustering algorithm to group forum entries requesting for same similar feature.
they present a prototype of their approach and evaluate it on support forum for an open source project called sugarcrm.
they also report that this approach helps the technical team easily identify 2note that in this paper we are notproposing a new general purpose text clustering algorithm but rather a method to cluster trouble tickets using multiple existing ideas from text mining so as to effectively identify emergent patterns of requirements from them.new feature requests being asked for by user community.
similar to the need in outsourced maintenance projects both these works underscore the need of a technique that can help technical team easily identify requirements for feature requests and enhancement from textual data.
however both of these papers present very limited evaluation.
we build upon approaches suggested in these papers to build a system that uses a combination of different text clustering techniques to identify cohesive problem clusters.
as far as we know our work is the first one that presents detailed quantitative and qualitative experiments that help evaluate the efficacy of the approach on large outsourced maintenance projects.
software engineering community has experimented with use of text analysis for multiple other use cases.
for example duplicate bug report detection bug assignment finding similar bugs to help in fault localization.
the underlying approach here is to identify past bug reports that are textually similar to the given bug report.
however because the end use case is different these approaches focus on different design considerations than our approach.
for e.g.
in the case of duplicate bug report detection it is imperative that there exist a very high textual similarity between old and the new bug report.
hence precision of analysis is of paramount importance.
also finding one past similar bug report is sufficient rather than a collection of similar reports.
in case of bug assignment rather than text clustering classification based approach works well where each developer becomes a class that can be learnt based on historical data.
a new bug report is auto assigned to one of these classes.
classification approaches do not work in our use case as the class of problem categories is not known upfront and need to be discovered from ticket data.
rest of the paper is organized as follows.
in next section we present important design points for the approach based on the usecase need and nature of enterprise problem ticket data.
in section we present our approach followed by experiments that measure different aspects of the approach in section .
multiple teams within ibm are using our approach.
in we present examples from two ibm teams that have successfully used our system to identify actionable requirements.
finally we present related work in section and conclusions in section .
.
design considerations when compared to typical product maintenance problem ticket repositories in outsourced maintenance projects contain data corresponding to both so called bugs managed in open source projects using bug tracking systems such as bugzilla jira etc.
and support requests managed using discussion forums in open source .
the nature of unstructured text in problem tickets could be automatic machine generated text this type of text is generally well formatted arising from alerts put in applications to indicate issues human generated free text directly typed into these unstructured fields their informativeness could vary from being terse to being very detailed copy pasted text from emails threads and other sources of discussion or any combination of the above types.
see table for examples.
we now identify a couple of major issues that must be addressed when grouping these problem tickets into unified meaningful groups or clusters .
readable cluster labels application maintenance practitioners need a quick interpretable way to understand what each cluster represents as they do not have enough time to peruse each individual ticket in a group.
in table we show some examples of readable labels that are easier to comprehend versus labels that are not self explanatory.
later in the paper in experiment we present using a user study the importance of this design consideration in overall usefulness of the approach.
679table examples show variance in nature of ticket text.
machine generated well formatted us5163webp apdreports system.nullreferenceexception object reference not set to an instance of an object.
us0894srap r3abap critical r3abap shortdumps frequency frequency min min no.
of messages per minute above threshold us0726bizp btsapshr document s have failed.
please use the bizadmin application for more information.
errmonitor natural language conversational and noisy to support company.com subject please help john doe and i upgraded our nvs portal to version and we are unable to open the company daily v olumes app.
we get the message showing a backend connection error which we have never seen before in this context.
... i removed the qv plugin from memory and tried again but received the same message connection error and this time with an empty response.
i m not sure how to proceed.
thanks jane doe variance in unstructured nature of text the overall clustering approach should be able to handle the variance in unstructured nature of the text from machine generated to human entered free text and maximize the number of tickets clustered without necessarily having to know beforehand the nature of data in each ticket.
later in the paper in experiments and we evaluate the individual steps used in our approach that were developed to handle the nature of text and how they significantly improved the problem cluster quality.
having characterized the nature of the unstructured data and the corresponding requirements we next provide a detailed description of the approach we developed for our task.
.
problem ticket clustering ptc we call our approach problem ticket clustering ptc .
the first phase of this approach is data cleansing sect.
.
to prepare the data for the remaining phases.
this phase takes the raw unstructured data and removes unnecessary information such as email headers signature blocks greetings etc.
such noise severely impacts the performance of the following phases and hence to obtain meaningful results we define certain pre processing rules for data cleansing specific to the nature of problem tickets.
the second phase of our approach is the grouping phase sect.
.
which takes as input the cleansed data and produces as output clusters of problem tickets based on similarity of problems faced along with meaningful labels describing these clusters.
we employ two distinct clustering algorithms with complementary capabilities i a latent semantic indexing based technique called lingo which works well with large non regular unstructured text and is known to provide good quality meaningful cluster labels and ii a novel hierarchical n gram based technique which is designed to work well with short snippets of well formatted text.
we then develop a cluster merging technique to merge the results from these two complementary algorithms.
we next describe each of these phases in detail.
.
data cleansing and noise removal typically raw data in unstructured fields contain conversations email threads etc.
and therefore can have a lot of noise which hamper text clustering.
to handle this noise we employ the fol lowing heuristics and cleanse the data before presenting it for the grouping phase.
heurisitic removal of email specific noise.
since the data can contain copy pasted email threads we employ regular expressions to remove noise arising from them.
we remove email headers f rom to c c b cc subject etc.
commonly used salutations h i hello dear hey signature blocks that usually occur at the end of email messages and email formats such as abc foo.com regular type name country company enterprise type.
this is similar to heuristic proposed in .
heuristic removal of context specific stop words.
we use a list of words provided by user based on domain knowledge as stop words over and above the generic english stop words list that many other techniques also employ.
for example in a problem ticket collection dealing with sap issues the word sap occurs very frequently but is uninformative.
heuristic removal of names.
people names in text typically do not help discern the technical problems but their frequent occurrence can hinder clustering.
hence we remove them by providing the ability to specify the names if available in the ticket dump in columns such as r esolution owner customer etc.
as these are the names commonly observed in the ticket descriptions.
heuristic misc.
regular expression based heuristics.
on top of the above mentioned heuristics we also employ regular expressions to remove entities such as urls dates and numbers.
.
grouping tickets into labeled clusters the grouping phase is divided into the following steps.
in the first step we extract frequently occurring phrases in the ticket text and then filter them to derive a pool of candidate cluster labels sect.
.
.
.
next to group the tickets we employ multiple clustering techniques to handle the variance in the unstructured nature of the data and build a robust clustering method sect.
.
.
.
in addition to a standard off the shelf vector space clustering technique we introduce a novel hierarchical n gram technique to deal with the presence of machine generated trouble tickets along with human produced ones.
finally we apply a merging algorithm to combine the results of these multiple techniques to present meaningful well labeled groupings of tickets which are agnostic to the nature of ticket data present in the data set sect.
.
.
.
in the following sections we describe each of the above steps.
.
.
candidate label generation the first step is to extract frequently appearing phrases in the document set which can later be used to induce labels for the clusters formed.
a phrase is a sequence of words occuring together commonly refered to as n gram where nindicates length of phrase.
to obtain readable labels we seek to detect complete phrases in the input text which i are shorter than a specified maximum term length ii occur at least a specified number of times in the document set iii do not cross sentence boundaries such as periods and iv discount the presence of certain words in the subsequence words that occur too frequently .
these filtered set of phrases constitute thecandidate set of labels for the following clustering step.
to illustrate this technique consider the following example sentence.
example sentence inbound glmast idoc failed bigrams word phrases inbound glmast glmast idoc idoc failed inbound idoc trigrams word phrases inbound glmast idoc glmast idoc failed inbound idoc failed gram word phrases inbound glmast idoc failed 680table comparison of labels from ptc and lda.
labels from ptc labels from lda change master workflow errors master amd workflow wifi removal of duplicate accounts ms unable account error role authorization issues run auth max role daily monitoring errors netscape tools error monitor batch job failures job service failed software note that here the word glmast is known as a context specific stop word and hence its presence is considered optional when calculating candidate labels.
assuming that the word idoc occurs too frequently in the tickets collection the only label remaining in the candidate set would be inbound failed .
we use a minimum phrase length of and maximum of .
one can argue that we can group the problem tickets simply by using these candidate labels and present the groups to the end users.
however there are two main problems i frequent phrases based grouping will result in large number of groups which will be laborious and frustrating for the practitioners to act upon and ii there will be a large overlap of problem tickets across groups and therefore the very purpose of grouping them is then lost.
text clustering seeks precisely to address these problems by considering the text as whole and leverages the notions of similarity and distance and groups tickets into clusters with minimal overlap.
.
.
text clustering text clustering is essentially an unsupervised learning approach.
it is well understood in the text mining community that no single learning technique can work well for all types of data but rather individual methods have to be picked and tuned to meet specific requirements .
the k means clustering algorithm which was popular for its simplicity a decade ago has been superseded by methods which capture the semantic relations between words in a document collection by performing latent semantic analysis .
approaches based on topic modelling that are commonly used these days such as latent dirichlet allocation lda tackle the labelling requirement post facto by generating high probability terms associated with each cluster and use them to assign labels to clusters.
but such labels are hard to interpret and lead to poorlylabeled clusters see table .
keeping in mind the limitations of these techniques and considering our requirements we employ the following two complementary clustering techniques i lingo a popular technique that combines the results of frequent phrase finding with the strength of latent semantic analysis to identify best labels representative of the document set involving long non regular text in natural language well suited for human authored tickets and ii a novel hierarchical technique based on identifying frequently occurring n grams sequences of tokens of length n in the data set which is particularly suited for short well formatted text such as machine generated error logs .
humans could have used different words and sentence formations to describe similar problems.
so a clustering technique that converts the problem text into bag of words such as lingo is likely to work well.
when the problem ticket is generated by a machine the sentences are very regular so a language model based approach that focuses on sequence of words such as n grams is likely to give better results.
as a common pre processing step for both techniques we run the entire ticket text through a stemming and stop word removal operation which is a standard linguistic technique applied in most text processing tasks.
we next describe the two methods and use thetable examples to highlight the benefits of complementary clustering techniques example set t11 my annual leave verification.
t12 unable to apply leave.
leave forecast balance is zero t13 i can t apply for leave and why i cant find annual leave balance in pso t14 annual leave application error t15 discrepancy in my leave balance.
for the above tickets we would expect to get the following clusters cluster t11 t13 t14 regarding annual leave cluster t12 t13 t15 regarding leave balance cluster t12 t13 regarding apply leave example set t21 gmp failed abc aaa document t22 impl failed def ddd document t23 blast failed xyz xxx document t24 gmpl failed xyz document we would expect to get the following clusters cluster t21 t22 t23 t24 regarding failed document cluster t23 t24 regarding failed xyz document two example ticket sets to illustrate the complementary strengths of each of the techniques.
lingo clustering in what follows we use the term document to describe the unstructured text data from a single problem ticket anddocument set to describe the entire ticket dump.
first the document set is represented as a term document matrix a where the columns of the matrix correspond to terms appearing in the documents and rows of the matrix correspond to the documents.
the value in each cell is proportional to the frequency of a term in that particular document and inversely proportional to its frequency in the entire document set tf idf .
this matrix ais now decomposed using singular value decomposition or any other method such as non negative matrix factorization to obtain an orthogonal basis of vectors in the feature space as specified in eqn.
a u s vt wheresis the diagonal matrix of singular values and uandvare the left and right matrices of orthogonal bases vectors.
next the candidate set of labels discovered previously sect.
.
.
are expressed in the same term document space by considering them as tiny documents themselves with each candidate label represented as a document constituted by the terms in the label.
after this the similarity of each such candidate label is calculated with respect to the firstkorthogonal basis vectors as shown in eqn.
m ut k p wherepis matrix of documents formed by the candidate set of labels andmis the matrix of similarity values between each candidate label inpand each of the top kbasis vectors in ucalculated using the cosine similarity function as sim u p u p bardblu bardbl bardblp bardbl summationtextn i 1wi uwi p radicalbig summationtextn i 1w2 i u radicalbig summationtextn i 1w2 i p wherewi uandwi prepresent the weights of base vector uand candidate label vector prespectively.
now using these cosine similarity values a label is assigned to each of the top kbasis vectors based on the one that is most similar to them from the candidate set .
the final step is to assign documents 681to these basis vectors by calculating their cosine similarity with these vectors and assigning them to those vectors and labels which fall within a threshold.
note that this could result in some tickets being assigned to multiple vectors but this is preferred over picking only the closest one which would be sensitive to relative similarities between documents .
therefore this entire process results in grouping the documents according to orthogonal basis vectors that guarantee diversity of topics covering the term document space and are also attached with most frequently occurring phrases that are most similar to these document clusters thus resulting in well labeled clusters.
applying this clustering technique on example set yields the following labeled clusters cluster label annual leave tickets t11 t13 t14.
cluster label leave balance tickets t12 t13 t15 cluster label apply leave tickets t12 t13 which is in line with what we would like to obtain.
however on example set it yields only the following cluster cluster t23 t24 with label failed xyz document it fails to identify cluster since failed document occurs in all the tickets.
therefore each document s column entries corresponding these terms receive low weights due to the idf factor in tf idf weighting explained above and this results in this cluster not being identified.
we therefore introduce a new clustering technique based on n grams hierarchical n gram clustering hierarchical n gram clustering technique as mentioned before to handle machine generated text which are well formatted arising from templates and therefore exhibit almost exactly repeating phrases we require a technique that focuses on extracting frequently appearing n grams.
for this we develop a suitable hierarchical n gram clustering technique as follows.
the first step in this technique is to filter the set of phrases that will be used to create the clusters.
to do so we first identify all bigrams extracted in the candidate label generation phase and then for each bigram form a cluster from all the tickets where this bigram appears.
this could result in a cluster of tickets being associated with multiple bigrams due to multiple common phrases .
therefore to decide which of the associated bigrams to choose as the label for such a cluster we pick the bigram having a higher log likelihood ratio .
the log likelihood ratio measures the deviation between the observed data word1 word2 and what would be expected if word1 andword2 were statistically independent.
the higher the score the less evidence there is in favour of concluding that the words are independent.
it is calculated as follows ll n11logn11 m11 n12logn12 m12 n21logn21 m21 n22logn22 m22 wheren11is the number of times word1 word2 occur together and n12is the number of times word1 occurs with some word other than word2 and so on.
here the mijvalues are the expected values of the words occurring together and are calculated by taking the product of their associated marginals and dividing by the sample size as follows m11 n1pnp1 npp wheren1pis the number of times in total that word1 occurs as the first word in a bigram and so on.
the formula is similarly extended for tri grams grams grams and so on.with this each cluster now has exactly one label.
we then put each ticket in the clusters whose labels it contains.
similarly we repeat the above process using the tri grams as the candidate phrase set grams and so on.
at the end of this process we get a hierarchy of clusters where each ticket would belong to a bigram cluster and could further also belong to child trigram gram and so on clusters.
to illustrate the working of this technique consider the tickets inexample set .
here the two bigrams in the candidate set are failed document andfailed xyz but the only cluster generated from bigrams is failed document because all tickets in failed xyz are subsumed in it.
the tri gram step generates a cluster containing tickets t23 t24 with the label failed xyz document .
cluster t21 t22 t23 t24 with label failed document cluster .
t23 t24 with label failed xyz document thus conforming to what we would like to obtain.
applying the same technique on example set yields the following labeled clusters cluster label annual leave tickets t11 t13 t14.
cluster label leave balance tickets t13 t15 cluster label apply tickets t12 t13 it can be observed that t12 which is also about leave balance is missing from cluster because the actual phrase occurring here is leave forecast balance .
this shows that this technique works well when the text to be clustered is concise and template based.
hence in our overall approach we only employ it when the text length is restricted to a few sentences.
looking at the results with the two examples sets we observe the complementary advantages of the two clustering techniques which together provide us with the necessary robustness to deal with the variance in the unstructured nature of the ticket text.
both our clustering techniques require a minimum cluster size to be specified as an input.
in practise we observe that projects choose to take an action only when a problem has repeated a minimum number of times and therefore we allow project teams to specify this value based on their specific needs.
for purposes of experiments in this paper we fix this value to be .
also we employed the opensource implementation of lingo from carrot2 project.carrot2.org and retained the default value of k .
no calibration was needed.
.
.
cluster merging in this phase we combine the outputs of the two clustering techniques to present a unified clustering result by merging the clusters and their corresponding labels using the following heuristics minimum cluster merge threshold if the percentage of overlap in tickets across clusters from the two clustering techniques is high enough employing a threshold of we merge the clusters into a single cluster by forming a union set of tickets from the two clusters.
to pick the label of the new merged cluster we simply pick the cluster label with higher word count.
high similarity between labels it is quite commonly observed in our ticket data that same typographical errors repeated by the same practitioners lead to distinct clusters varying only by the difference of the typo.
another reason could also be due to variations in spellings employed.
for example a cluster labeled as job canceled and another labeled as job cancelled essentially represent the same underlying problem.
in such cases we find the levenshtein distance between the labels normalized by the length of the longer of the two labels and check if it lies within a threshold we pick .
.
if so we collapse the two clusters into a single cluster and pick one of the two labels as the label for the merged cluster.
note that to avoid merging distinct labels differing by a small string 682table details of the subjects used in our experiments sub total total avg period total max jects tickets words words mths assignees entropy .
.
.
.
.
.
difference not due to an error e.g.
cost error vs post error we filter the words through an english language dictionary list.
additionally clusters with labels where the order of words is flipped e.g.
job failed failed job are collapsed into one.
therefore at the end of this cluster merging technique we obtain a unified set of document clusters along with their corresponding labels.
we now proceed to provide detailed experimental evaluation of the entire approach in the next section.
.
experiments this section is organized as follows.
we first describe the different datasets used for the experiments.
we then describe in detail the experimental methodology employed here to evaluate and compare the proposed approach with existing techniques and finally discuss the results obtained.
.
datasets for our evaluation we used two different datasets.
the first is a small hand labeled dataset used to perform various experiments and compare performance against ground truth.
the second dataset is a large collection of problem tickets from different customer projects currently being maintained by ibm.
ds1 hand labeled data we built a dataset where tickets were collected from different problem types classes manually marked by a subject matter expert sme across multiple ibm clients.
in order to avoid any biases due to imbalanced clustering we collected approximately the same number of tickets from each problem type tickets each giving us a total dataset size of tickets.
this task was performed by one sme in approximately hours.
the objective was to measure if the composition of clusters inferred from our approach matched the manually marked classes.
this dataset was used for experiments and .
to measure the performance for each of the experiments we compared the clustering results with the manually marked classes and calculated the precision and recall of the groupings as follows.
for each discovered cluster we assign the class to which maximum tickets in it belong as its primary class.
for precision we calculated the percentage of tickets in that cluster that belong to its primary class and average across this for all classes.
for recall we calculated the percentage of all tickets from the cluster s primary class that had been detected in that cluster and similarly average across all classes.
ds2 ibm customer data we collected problem tickets from live projects within ibm.
to protect their identities they have been referred to as subjects .
every ticket had a title field containing a short description of the problem.
all subjects had tickets created both automatically by systems and by humans.
however the proportion of auto generated versus human created tickets varied across these subjects.
in table we show the details of these subjects.
we used this dataset to conduct experiments and .
the user studies for experiments and were also conducted by contacting practitioners working on these projects.
.
methodology and results through the experiments performed we seek to answer the following three key research questions.
the first question rq1 seeks to address whether our approach produces good clusters as measured by various metrics described below and analyzes its performance compared to existing popular clustering methods.
the second question rq2 focuses on the efficacy of the specific individual steps of our approach i.e.
noise removal cluster merging good label generation .
finally the third question rq3 addresses whether the application of our approach led to identification of coherent patterns that project teams acted upon to reduce their software maintenance cost.
.
.
rq1 quality of clustering with ptc to address this research question we performed experiments.
in experiment we used the hand labeled dataset where tickets are associated to different classes based on the type of problem described.
we ran our approach on this dataset and then calculated the precision andrecall scores by comparing the clustering assignments obtained against the manually marked ground truth.
this is also the evaluation approach followed in existing literature and possibly the most intuitive one.
however this experiment while necessary would not be sufficient to confirm the goodness of the clustering because the size of hand labeled datasets are arguably small.
winbladh et al also report on the challenge of creating such a truth set.
in their work it took .
hours to manually label the dataset even for a small set of comments.
this made us further explore additional metrics to evaluate the cluster goodness.
intuitively tickets that indicate the same problem should be described using same or similar words and hence should show a high textual similarity when grouped together.
based on this we developed our first new metric which we call textual coherence .
we calculate each cluster s textual coherence by first calculating the textual similarity between each pair of tickets in that cluster and then compute the average across all such pairs of tickets.
the text similarity metric employed here is the well known cosine similarity and is applied on the cleansed tickets.
this is described in detail in experiment .
next we introduce another metric called assignee entropy .
in every problem ticket there exists a ticket assignee field which identifies the individual to whom that ticket was assigned for resolution.
typically these tickets are not assigned randomly to individuals.
rather there is a top level process called triaging where an experienced person observes the nature of the problem and assigns the ticket to an individual based on their expertise.
for example password resets are addressed by a select few amongst the entire pool of individuals.
hence a small subset of people become specialised or trained in resolving a particular type of issue.
as a result of this practice during steady state maintenance each type of issue is generally assigned to those limited set of individuals who would have developed proficiency in solving that issue type.
we therefore exploit this knowledge from software maintenance ecosystem that coherent problems problems of the same type are solved by a limited set of individuals from amongst the entire pool .
an information theoretic view of this would be that more coherent clusters have lower uncertainty about the assignment of tickets in the cluster to personnel thus the information content orentropy of coherent clusters should be lower.
based on this view we conjecture that good clusters i.e.
clusters with high coherence exhibit low assignee entropy values whereas incoherent or dissimilar clusters exhibit high entropy.
to mathematically capture this knowledge if we look at a histogram of counts for the entire pool of assignees for tickets from the same problem type .
.
.
.
.
assignee only lda random ptc assignee only lda random ptc assignee only lda random ptc assignee only lda random ptc assignee only lda random ptc assignee only lda random ptcsimilarity score figure textual coherence score of clusters for different techniques assignee only lda random and our ptc approach a cluster we should observe a distribution that exhibits low entropy i.e.
there is a fairly high certainty regarding which individuals these tickets are being assigned to .
this entropy value is calculated as shown below in eqn.
h m summationdisplay i 1ni nlog2 ni n wheremis the number of total number of assignees in a project nis the total number of tickets in a problem type cluster whose entropy you want to calculate and niis the number of tickets from that cluster assigned to assignee number i. in experiment we use this to demonstrate using historical data that clusters obtained by our ptc approach do exhibit low assignee entropy values and therefore are reflective of coherent problem types.
our choice of the two new metrics introduced above are also driven by their complementary nature.
while the textual coherence measures the clustering goodness based on the textual patterns which is an intrinsic property of the tickets themselves the assignment entropy measures the quality of clustering based on the steady state practise observed in software engineering ecosystems and is therefore essentially an extrinsic measure.
therefore employing both these metrics helps us comprehensively measure the goodness of the clusters obtained.
in both experiments and we compared the performance of our ptc approach against the following methods lda latent dirichlet allocation which is the clustering technique of choice for most of the recent work in literature employing text based clustering.
we used the lda implementation from a tool called mallet3.
we configured mallet according to the approach used in where we keep the number of topics per subject as a constant for our experiments and associate a document j to a topic i only if document to topic proportion is .
.
random a random clustering approach where we create clusters per subject by randomly assigning tickets to each cluster.
there are four cluster sizes per subject selected as median of the quartiles of cluster sizes in ptc.
assignee only clustering based on ticket assignee field completely ignoring the problem text.
here we ignore individuals who have resolved less than tickets.
finally in order to ensure that end users actually find the clusters useful we did a user study where practitioners were asked to rate the lda random ptc lda random ptc lda random ptc lda random ptc lda random ptc lda random ptcassignee entropy figure assignee entropy of clusters for different techniques lda random and our ptc approach.
clusters as meaningful i.e.
all tickets in the cluster indicate the same problem that is repeatedly actioned.
this evaluation is discussed in experiment .
experiment does ptc give high precision and recall when evaluated against a truth set?
experiment does ptc give clusters with higher textual coherence when compared to other approaches?
experiment does ptc give clusters with higher assignment entropy when compared to other approaches?
experiment do end users find the clusters identified by ptc as coherent?
experiment precision and recall we observe that we obtain both high average precision .
and recall .
values.
we also observe that the standard deviation values are quite low for both metrics .09and0.24respectively showing that the clustering is consistent across the various classes in the dataset.
these values demonstrate that our approach produces good quality clusters on the small hand labeled dataset.
experiment textual coherence in this experiment we calculated the textual coherence scores obtained with our approach on each of the ibm datasets and compared these scores against the ones obtained using lda random and assignee only.
the results of this experiment are shown in the fig.
.
the box plots show the mean and the variance of textual coherence scores for various clusters obtained using the four approaches across each of the datasets.
on comparing the distributions we clearly observe that our approach yields clusters with higher textual coherence as compared to lda and random clustering.
the textual coherence of our approach is also better than assignee only clusters which validates the obvious that even though coherent problems are solved by a limited set of individuals the same individual can solve multiple sets of coherent problems and hence grouping tickets by assignee might not necessarily provide problem clusters of high textual coherence .
this experiment reflects the better quality of clusters obtained from our approach as measured by their intrinsic similarity.
experiment assignee entropy similar to the previous experiment upon running the lda and random algorithm on each of the datasets we calculated the corresponding assignee entropy values and compared them against the ones obtained using ptc.
we remind the reader that good clusters must exhibit low assignee entropy values whereas incoherent or dissimilar clusters exhibit high entropy.
the results of this experiment are shown by the box plots in fig.
.
as we can see for subject and we observe similar assignee entropy distributions between ptc lda and random.
however for each of the remaining subjects we clearly observe 684table distribution of responses for experiment total coherent not coherent not sure that ptc yields clusters with lower better assignee entropy as compared to lda.
for these subjects the difference in assignee entropy between our and lda approach was statistically significant withp value .
.
upon further investigation we found in subjects and the problem ticket assignment to people were more dictated by their availability rather than their specific skill.
some of the reasons for this are most of the tickets required a generic skills to fix or the project had well documented resolution faqs for reference.
based on experiments and we observe that our approach yields clusters with better textual similarity and equivalent or better assignee entropy when compared to lda which is a popular topic identification technique.
our understanding is that this performance is primarily because we employ multiple clustering algorithms in ptc to handle the variance in the nature of text in problem tickets and merge their results to obtain representative clusters therefore making it better suited to this domain.
experiment user study for this experiment we first randomly picked problem clusters obtained from ptc across the range of assignee entropy and textual similarity scores to conduct a userstudy on.
we then contacted the technical experts from each of ibm project teams and asked them the following question are these problem clusters coherent enough to take unified responsive actions in your project?
we asked the experts to rate each problem cluster as either coherent not coherent or not sure.
overall we had problem clusters data points for the experts to verify across subjects.
per subject for each problem cluster a pool of experts to people provided their consolidated response.
from the response distribution seen in table we find that out of responses corresponding to problem clusters clusters were marked as coherent by experts and as not.
we also observed that experts were able to identify the coherent categories without any deliberation however for marking them as not coherent they took longer.
this demonstrates that for a randomly selected subset of clusters the experts found coherent i.e.
the problem cluster indicates a problem reported in multiple tickets in project and it could be removed by taking a single action.
these set of experiments indicate that our technique yields good clusters as measured using metrics of precision recall textual coherence and assignment entropy.
in the user study project experts further confirmed that a large percentage of identified clusters were useful.
.
.
rq2 efficacy of individual steps of ptc our approach had three steps noise removal clustering and merging.
our clustering approach specifically focused on generating readable labels for clusters.
in these sets of experiments we evaluate the efficacy of individual steps.
the first step in our approach is to pre process the tickets so as to remove noise.
in experiment we evaluate the impact of this step in the overall approach and measure it in terms of loss in precision and recall on the hand labeled dataset when this step is removed from our ptc approach.
the next step in our approach is to employ two different clustering algorithms and then combine the results using a merge algorithm.
in experiment we evaluate the effectiveness of using two algorithms and merging their results when compared to using only either of them.
again we measure the effectiveness using the standard metrics of precision and recall on the hand labeled dataset.table quantitative evaluation of improvements in the cleansing and merging phases on hand labeled dataset.
before cleansing after cleaning after merging p r p r p r avg .
.
.
.
.
.
stddev .
.
.
.
.
.
finally we measure the effectiveness of our cluster labels with a user study in experiment where practitioners from various projects were given the same cluster of tickets with two sets of labels.
the first one were labels generated using our approach ptc and the other using the state of the art algorithm lda.
they were asked to qualitatively judge which label was more helpful from the point of view of comprehending the problem described by reading the label.
experiment what is the effectiveness of the noise removal step in ptc?
experiment do we require multiple clustering methods and merging in ptc?
experiment do end users find the labelling of clusters in ptc meaningful?
experiment noise removal through this experiment we evaluated the importance of the noise removal step of our approach by measuring the change in precision and recall values on the handlabeled dataset as shown in table.
.
we observed that before cleansing the average precision was quite poor .
implying that on average clusters had high contamination of tickets from classes other than the primary class.
this was because clusters were being formed due to noisy uninformative text across tickets.
for e.g a cluster was formed due to a common signature block of the same person across multiple classes.
but after cleansing the average precision increased to .92indicating that the clusters obtained contained tickets mostly from one class along with a low value of standard deviation .
indicating this to be the case across all classes.
these improvements confirm the importance of the noise removal step of our approach.
we used only the lingo clustering algorithm for this experiment.
experiment merge clusters in this experiment we compared the change in precision p and recall r values on the handlabeled dataset when we just used one clustering algorithm i.e.
lingo versus running both algorithms and merging.
as shown in table.
a low value of average recall even after cleansing .
indicated that not all of the classes were being discovered by the clusters.
but after running both our clustering approaches and merging we observed a rise in the recall to .80showing that combining the results of multiple techniques helps increase the recovery of classes discovered by the clustering due to the ability to handle the variance in the nature of the text across these classes.
this experiment demonstrates the importance of utilizing multiple clustering techniques and merging their results.
experiment user study we conducted another user study to evaluate the effectiveness of labelling of the problem clusters and compared them against the ones obtained via lda.
to do so we first identified overlapping clusters between ptc and lda clusters were marked as overlapping if they had or more tickets in common .
we then randomly picked such overlapping clusters from each subject and presented them to the experts asking them to answer the following question is the label meaningful enough to describe the problem suggested by the cluster?
they could answer either meaningful not meaningful or not sure.
the experts were notmade aware that these labels refer to overlapping problem clusters in order to allow them to make their judgements 685table distribution of responses for experiment method total meaningful not meaningful not sure lda ptc about each label in isolation.
overall we had data points for the experts to verify across subjects.
we reached out to experts for participation but received responses from only of them.
table presents the response distribution received.
as we can see out of the overlapping problem categories between lda and ptc of them were marked as not meaningful for labels generated through lda while only of them were marked not meaningful in ptc.
of the problem categories labeled by ptc were marked meaningful while on the other hand only of lda were marked as meaningful.
this validates our claim that labelling the problem categories for easy comprehension is important towards reporting cohesive problem categories and ptc provides more meaningful labels to experts as compared to lda.
these set of experiments indicate that noise removal use of multiple clustering algorithms and readable cluster labels are indeed essential steps in the approach and lead to significant improvements in the quality of output clusters.
.
.
rq3 usefulness of clusters in ibm projects the quantitative and qualitative experiments performed above help us establish the goodness of our approach and necessity of individual steps.
however the actual benefit of the approach is realized only when project teams choose specific clusters identified using this approach and decide to implement a follow up action which either identifies new requirements or reduces their maintenance cost.
more than sixty different software maintenance projects within ibm have deployed our tool.
we have several discussions with these practitioners to understand the kinds of actions project teams have been implementing using the clusters identified with our approach.
in this section we present two such case studies.
since these are existing ibm customers in the interest of confidentiality we omit certain specific details but nonetheless report key observations and actions taken thereof.
case study a particular project was observing a continuous increase in backlog of tickets.
both ticket volumes and average resolution time are key process metrics of concern to maintenance projects.
service level agreement sla adherence could be adversely impacted if ticket volumes or ticket resolution times were to keep surging.
to address these concerns the project team primarily looked at those problem clusters which had a high volume of tickets in them.
amongst such clusters of particular interest were those that took a longer time to fix.
below we discuss a few examples of unified actions this project implemented in response to these identified problem clusters.
upon inspecting large clusters such as cheque payment on liv exception not working and po output zneu triggering for any field changed the teams realized that these problems point to design issues and can be avoided by implementing enhancements in the payment andpurchase order processes respectively.
additionally to address the problem of high resource turnover and to ensure that productivity is not lost when a new resources come in to solve tickets the team started authoring faqs for large problem clusters such as changing dsn table entries incorrect instance in new user forms gsap file missing for business users .
this project also found problem clusters that indicated a requirements drift.
there was an existing functionality to convert a document to an ocr image.once the image was successfully generated it was transmitted to another system.
the project kept receivingtickets thrice a week that required manual intervention to transmit the document.
the identified problem clusters through ptc such as manually transmit document andocr unsuccessful indicated that the ocr program was only able to handle documents that strictly adhered to a template.
based on this the new requirement identified was to reject non compliant documents upfront even before they are passed to the ocr system.
case study in another project the software maintenance had been transferred over from another service provider to ibm.
the new team did not have any prior knowledge of the repetitive problem patterns and therefore used our approach to quickly come up to speed.
the team observed that a large number of tickets formed clusters such as password reset .
this was despite the fact that a self help password reset functionality was already available.
a discussion with the client indicated that there was a user awareness issue since the feature had only been recently implemented.
further the team found clusters such as process chain failure job abended .
these clusters were indicative of step failures in an otherwise large process chain.
in most cases the resolver would just re initiate the process chain.
once the problem clusters were identified it became easy for the ibm team to train there resolvers also to do the same.
but this time in response to the identified clusters the team performed a deeper and more thorough investigation and found that many of these failures were reported on fridays and resolved by monday.
it turned out that the system was shut down for routine maintenance for a couple of hours every friday evening.
quite a few reports were also auto configured to run at the same time.
now as the system was not available during these times the corresponding jobs as well as the process chains failed.
the root cause of the problem was that over the years the knowledge and ownership of scheduled jobs and system maintenance had changed multiple hands and was now resident in multiple teams.
this issue led to a new requirement for the system maintenance team to change system downtime to a more opportune time.
also a new application was developed to handle job failures during scheduled downtimes.
system administrators advertised the system downtime and all problem tickets raised for job failures during that time period were auto queued for job restart once the system came up.
in general we have found that ptc found clusters helped project management teams better manage the volume of ticket streams and identify new requirements latent in these problem tickets.
they are able to follow up with the technical leads to get a deeper understanding of the issues identified and decide on some unified actions.
building on this work we are developing techniques to correlate these problem categories to operational process metrics such as volume resolution time sla adherence to automatically identify those problem categories that cause majority of the operational grief and therefore present them in a prioritized manner.
.
threats to validity threats to external validity arise when the observed results cannot be generalized to other experimental setups.
in our experiments we tried to limit this threat by evaluating on tickets across different ibm client projects.
the average number of words per ticket also varied across the subjects from to .
also our approach has been deployed within ibm and has been used by different projects till date.
we have received anecdotal evidence of its usefulness some of which has been presented in rq3.
finally our approach has also proven to be useful in solutioning bidding for new projects where the problem tickets are from client accounts which are not serviced by ibm.
threats to internal validity arise when factors affect the dependent variables the datasets described in section .
.
in our study such 686factors are errors in implementation of cleansing and identifying candidate labels.
moreover we used certain heuristics in our cleansing phase based on manual inspection of the tickets and guidance from project leads.
the clustering approach required some configuration parameters such as minimum cluster size minimum cluster merge threshold minimum and maximum n gram phrase length etc.
the choice of these parameters was heuristic based.
also factors like ticket assignment process maturity of the project geographical and cultural aspects of the people participating in the project could impact the assignee entropy metric.
the choice of metrics can be considered as threats to construct validity.
precision and recall are appropriate measures in search retrieval domain.
however for measuring clustering efficiency of our approach in comparison with the golden set we leverage these metrics.
to address this we used other metrics for evaluation and also performed user studies to validate our approach.
gold standard was produced by a single subject matter expert sme .
while its true that in open domain such as news articles etc there can be significant inter annotator disagreement in the domain of it tickets where the notion of problem categories in operations are fairly wellunderstood and agreed upon we did not see the need to calculate inter annotator agreements.
further since we use multiple clustering techniques which employ different measures we wanted to employ a general textual coherence method for evaluation that is agnostic to all clustering methods compared.
also we did not conduct a formal validation of the conjecture behind asignee entropy metric.
.
related work using text based analysis to identify groupings from software artifacts is not new.
most recently the work of extracted topics from user queries on software forums.
having extracted the topics they then try to create an faq frequently asked question from answers put in all the user queries grouped under a topic.
in authors have tried to extract automated labelled topics from commitlog comments recovered from source control systems to provide insights into evolving software development activities.
both these and other approaches such as use lda .
as we have seen in this paper one of the primary challenges with using lda is the labeling aspect of the extracted topics.
because of unreadable labels the topics are often discarded as not useful in our use case.
prior work exist very similar to ours towards the specific goal of managing feature requests .
in frictionary they proposed an approach to extract problem topics from firefox support requests.
they extract a problem topic from each request by transforming it into a predefined grammatical format subject verb object such as bookmark disappeared browser and by then aggregating the topics using certain heuristics.
while such linguistic assumptions for transformation may be reasonable in their domain they are highly restrictive for problem tickets which as shown before can be machine generated logs no grammar or email threads contain too much noise .
in they focused on discussion threads on public forums and applied a known clustering algorithm called spherical k means.
our approach builds upon these works by customizing known text clustering approaches to work well with noisy and diverse textual formats found in problem tickets seen in an industrial setting.
text analysis approaches can be of two types unsupervised such as ir based techniques clustering topic modeling or supervised such as classification.
software engineering community has used for of these types to solve various use cases.
multiple works have applied unsupervised text analysis to detect similar or duplicate bug reports.
bug reports have more structure such as platform and components used stack traces and free text.
in authors apply adifferent similarity detection approach for each of the constituents for example stack traces are similar if the method sequences being reported in trace are same which titles are similar if they contain same bag of words.
identify if a bug is duplicate by building using a combination of ir based features and topic based features.
in authors use natural language features along with execution information to find duplicate bug reports.
supervised text analysis has been applied to propose solution to problems such as bug assignment for classifying whether a given bug report is a feature or bug or for determining whether a bug will be fixed or not .
in authors learn a model per developer based on text similarity of maintenance requests and use it to assign these requests to an appropriate developer.
in authors train a naive bayes and regression based model to distinguish between bugs and enhancement requests.
in a regression model is trained on structured features available in bug reports such as who opened the bug how many times it was re assigned who it was re assigned to etc to determine if a bug will get fixed or not.
a supervised approach would not work well in our scenario where problem categories are not known upfront and need to be discovered from problem tickets.
.
conclusion in this research we addressed the problem of managing highvolume streams of trouble ticket requests in an outsourced software maintenance ecosystem.
using unstructured information processing methods we developed our ptc approach to cluster the stream into unified groups and assigning suggestive salient labels to the groups.
we quantitatvely validated the clustering approach by showing that a the groups were textually coherent b the groups were coherent enough to facilitate assignment to smaller teams drawn from the larger collection of project personnel and c our clustering approach was significantly better than commonly used lda based approaches.
a qualitative case study with domain experts suggested that our approach provided useful results which improved upon lda based approaches.
our approach is quite general and has proven to be useful in live customer engagements on a wide range of software platforms oracle java sap etc and application domains this experience suggests that this approach will be useful in a broad range of settings in the growing outsourced maintenance industry.
in future work we intend to explore beyond cluster size what other metrics we can use to rank prioritize clusters for action by the account team.
we also wish to explore how change in trends for high ticket volume problem categories are explainable through code changes being performed on application.
.