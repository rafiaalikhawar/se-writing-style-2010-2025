reliability analysis in symbolic pathfinder antonio filieri1 institute of software technology university of stuttgart stuttgart germany antonio.filieri informatik.uni stuttgart.decorina s. p as areanu carnegie mellon silicon valley nasa ames moffet field ca usa corina.s.pasareanu nasa.govwillem visser stellenbosch university stellenbosch south africa wvisser cs.sun.ac.za abstract software reliability analysis tackles the problem of predicting the failure probability of software.
most of the current approaches base reliability analysis on architectural abstractions useful at early stages of design but not directly applicable to source code.
in this paper we propose a general methodology that exploit symbolic execution of source code for extracting failure and success paths to be used for probabilistic reliability assessment against relevant usage scenarios.
under the assumption of finite and countable input domains we provide an efficient implementation based on symbolic pathfinder that supports the analysis of sequential and parallel programs even with structured data types at the desired level of confidence.
the tool has been validated on both nasa prototypes and other test cases showing a promising applicability scope.
i. i ntroduction pervasiveness of software systems in critical applications is stressing the need for methodologies and tools to do reliability assessment.
with the term reliability we generically refer to the probability of the software to successfully accomplish its assigned task when requested by the user .
in reality most of the software we use daily is defective in some way though it can most of the time do its job.
indeed the presence of a defect in the code may never be realized if the input does not activate the fault .
for this reason even incorrect software may be quite reliable under specific usage profiles.
most of the available approaches for software reliability analysis are based on formal models derived by architectural abstractions .
to deal with code black box or some ad hoc reverse engineering approaches have been proposed e.g.
.
model driven techniques have also been used to keep design models synchronized with the implementation though their application scope is usually limited to specific domains e.g.
embedded systems.
in this paper we propose the systematic and fully automated use of symbolic execution to extract logical models of failure and successful execution paths directly from source code.
in the past decade the use of symbolic execution as a means to analyze programs have steadily increased and it is now routinely used to find errors in code and to generate tests see for example .
in this work we show how to take the path conditions generated by these tools and use them to estimate the reliability of the implemented software.
1part of this work has been done while the author was with the deepse group dei politecnico di milano milan italy.a path condition is a set of constraints on the inputs that if satisfied by the input values will allow the execution to follow the specific path through the code.
we label each terminating execution path with either success orfailure .
since the set of path conditions produced by symbolic execution is a complete partition of the input domain given a probability distribution on the inputs we can compute the reliability of the software as the probability of satisfying any of the successful path conditions.
the probability distribution on the input formalizes the expected usage profile that accounts for all the external interactions of the software both with the user and with external resources such as remote components.
to account for non termination in presence of loops we use bounded symbolic execution.
in this case interrupted paths will be modeled by path conditions labeled as grey.
for an input satisfying a grey path condition we cannot predict success nor failure.
the resulting uncertainty will be used to define a precise confidence measure to assess the impact of the execution bounds and the consequent reliability prediction.
although our approach can be customized for any symbolic execution system we focus on symbolic pathfinder spf that works at the java byte code level.
as failures we can consider the typical errors reported by spf e.g.
assert violations null pointer exceptions race violations or deadlocks but also more general properties of interest not necessarily related to low level bugs present in the code which presumably could be corrected before code release .
for example in section viii a we describe the reliability analysis for a nasa software component that monitors for flight rule violations during flight.
the analyzed code has no software bugs but we characterize as failure the paths that lead to mission abort due to flight rule violations.
we focus on programs whose inputs range over finite discrete domains.
this restriction allows us to make use of model counting procedures for general and efficient algorithms to compute probabilities.
our implementation supports linear integer arithmetic which is the most common theory handled by symbolic execution tools complex data structures loops and also concurrency.
for multi threaded programs the actual reliability depends both on the usage scenario and on the scheduling policy.
in this case we identify the best and worst schedule for a given usage profile that lead respectively to the highest and lowest reliability achievable for that usage.
.
c ieee icse san francisco ca usa622safety checkactuator aactuator bgoalwind effectpositionfig.
.
running example structure.
the main contributions of the paper are calculating reliability for source code a confidence measure for results computed from a bounded analysis support for multithreading support for complex data structures an efficient implementation and an evaluation of the approach on nasa s onboard abort executive.
in the rest of the paper we first define a running example that we will use throughout the paper and in section ii we give some background on symbolic execution and probability theory.
in the following three sections we will show how to calculate reliability for non looping iii looping iv and multi threaded programs v each time elaborating on the running example to illustrate the techniques.
in section vi we then show how reliability can be computed when symbolic data structures are given as input followed by details of the implementation of our general approach vii .
the paper finishes with the validation of the reliability calculations on nasa s onboard abort executive viii a and the example of a broken binary tree container viii b followed by related work ix and conclusions x .
a. example analysis we illustrate our reliability analysis on an example modeling a simplified flap controller of an aircraft.
the controller is composed of two actuators and a safety check to avoid overrun of the flap.
the user sets the goal position for the flap and the actuators move toward it from the current position.
the actuation power of each controller is hindered by the effect of the wind which could push on both head or tail.
the high level structure of the flap controller is shown in figure .
the position of the flap is identified by an integer value between and .
goal is an integer value that states the next flap position and is provided by the user we assume that any goal position can be requested with the same probability .
the wind effect represents the action of the wind and could be any integer between and depending on wind direction.
an actuation step of actuator a moves the position of the flap toward the goal position by units actuator b by unit.
we will refer to and as the actuator strengths of a and b respectively.
the actual movement depends also on the wind effect which is added to the effect of the actuator.
safety check represents an invariant to be verified after each actuation.
the invariant asserts that the value of the output position is actually between and .
we will describe parts of this system along with the presentation of reliability analysis for different control structures and for multi threading.
in the latter case we will show how concurrency can be managed with schedules leading to better or worse reliability.
weakstrongprobability wind effect fig.
.
wind effect profiles.
the effect of wind is uncertain and will be profiled through a random variable resembling specific operation scenarios.
figure shows the two profiles we will use.
the ranges of values for the wind effect are reported on the x axis and the corresponding vertical bars represent the probability of a value in the range to be inserted as input.
in this example we assume symmetric distributions for the two profiles.
weak wind is more concentrated around zero while strong wind is more likely to produce extreme values.
the analysis will estimate the mission success probability of our toy system in the two different wind conditions.
ii.
b ackground in this section we recall some basic notions concerning symbolic execution and probability theory .
further extensions will be provided in subsequent sections as they are needed.
for simplicity in this section we will assume all the variables in our program to be integers from a finite domain and all the conditions appearing in a branch or a loop statement to be expressed as linear constraints.
we will later show how to relax these assumptions to make our framework more general.
we will focus here on symbolic execution for java as performed by symbolic pathfinder spf .
a. symbolic execution symbolic execution is an extension of normal execution in which the semantics of the basic operators of a language is extended to accept symbolic inputs and to produce symbolic formulas as output .
the behavior of a program pis determined by the values of its inputs.
such a behavior can be defined through a state transition systems where the execution of an instruction identifies the next reachable states.
definition state of a program .the state s of a sequential program is defined by the tuple ip v pc where ip represents the next instruction to be executed.
v is a mapping from each variable v iof the program to its symbolic value i.e.
a symbolic expression.
pc is the path condition .
pc is a boolean expression over the symbolic inputs si.
a path condition is a conjunct of terms plus the constants true and false true as short form for a tautology and an unsatisfiable condition.
the current state sand the next instruction ipdefine the set of transitions from s. without going into the details of every java instruction we informally define these transitions depending on the type of instruction pointed to by ip.623assignment .
at bytecode level we consider an assignment as setting the value of the area of memory corresponding to the left hand side variable vi2v while at source code an assignment requires also to evaluate the right hand side statement.
the execution of an assignment leads to a new state where ipis incremented to point to the next instruction and vis updated to map vito its new symbolic value.
pcdoes not change.
branch .
the evaluation of an if then else instruction on condition c introduces two new transitions.
the first leads to the state s1where ip1points to the first instruction of the then block and pc1 pc c. the second leads to a state s2where ip2points to the first instruction of the else block andpc2 pc c. if the pc associated with a branch is not satisfiable symbolic execution will not follow the branch.
loop .
awhile loop is unfolded by spf until its condition evaluates to false or the exploration depth limit is reached1.
analogous transformations are applied to the other loop constructs.
method invocation .
method invocations are managed by means of macro expansion i.e.
the execution jumps to the invoked subprogram and executes it making the necessary assignments to represent method s arguments.
the state s1will have the same pc ip1will point to the first instruction of the method and v1will contain now the new assignments to the formal parameters of the method.
the initial state of a program is s0 ip0 v0 pc0 where ip0points to the first instruction of the main method v0 contains the arguments of main if any and pc0 true.
a program may have also one or more terminal states.
they represent terminal condition of the program such as the exit of the program or an uncaught exception making the program terminate abruptly.
in section v we will extend the definition of symbolic execution to the case of multi threading.
b. probability theory in this section we recall some fundamental notions of probability theory for finite spaces.
for an extensive exposition the interested reader could refer for example to .
the possible outcomes of an experiment are called elementary events .
for example the rolling of a sided dice may produce the elementary events and .
elementary events have to be atomic i.e.
the occurrence of one of them excludes the occurrence of any other.
the set of all elementary events is called a sample space .
a set of elementary events is called an event .
in this paper we consider only finite and countable sample spaces meaning that the underlying set of elementary events is countable and finite.
definition probability distribution .let s be the sample space of an experiment.
a probability distribution on s is any function pr p s !
r 1the choice of an exploration depth limit and its refinement is further investigated in section ivthat satisfies the following conditions probability axioms pr fxg 0for every elementary event x pr s pr a b pr a pr b for all events a b s with a b the pair s pr constitutes a probability space .
definition conditional probability .let s pr be a probability space.
let a and b be events a b s and let pr b .
the conditional probability of the event a given that the event b occurs is pr ajb pr a b pr b pr ajb is also referred to as probability of agiven b. given the definition of conditional probability the following law holds definition law of total probability .let s pr be a probability space and fbn n gbe a finite partition of s. then for any event a pr a npr ajbi pr bi the law of total probability can also be stated for conditional probabilities pr ajx npr ajx bi pr bijx where biare defined as in definition and xdoes not invalidate the assumptions of definition .
iii.
c omputing reliability in this paper we consider reliability rel as the probability that the program accomplishes its execution without hitting any failure under specific usage assumptions.
a failure could be any observable error for spf such as a failed assertion or an uncaught exception.
since spf operates at java bytecode level the definition of failure events can be very detailed and flexible.
the main flow of our analysis chain is shown in fig.
.
the input of the process is the java source code.
this is symbolically executed by spf whose output is a set of path conditions pcs a path condition is a set of constraints on the program inputs whose satisfaction leads either to the occurrence of a failure event or to success i.e.
termination without failures .
javaspfprobabilistic analysispcsreliability analysisrel fig.
.
reliability analysis chain.
let us assume for now that the symbolic execution of the program always terminates we will relax this assumption in the next section .
we classify the pcs produced by624spf in the two sets pcs fpcs pcs pcs mgandpcf fpcf pcf pcf pgaccording to the fact that they lead to success or failure respectively.
note that all path conditions identified by spf define disjoint input sets and because of the termination assumption they cover the whole input domain.
therefore the path conditions define a complete partition of the input domain .
we further assume that all the input variables range over finite discrete domains whose joining is generically indicated asd.
we profile the expected usage for the program through ausage profile up .upis a set of pairshci piiwhere ciis a usage scenario defined as a constraint representing a subset ofdandpi pi is the probability that a user input belongs toci.
we further require for simplicity fcigto be a complete partition of d and thus ipi checked automatically by our implementation .
intuitively upis the distribution over the input space.
notice that cicould contain even a single element of d allowing for the finest grained specifications of up.
throughout the paper we will use constraints and the sets characterized by them interchangeably.
for example if the input domain of variable xisf0 3g we may use the constraints characterizing this set x x .
given the output of spf reliability can be redefined as the probability of executing the program p with an input satisfying any of the successful path conditions given the usage profile up.
this definition can be formalized as rel prs p ipr pcs ijup an analogous definition can be provided for failure probabilityprf p and it is straightforward to prove that prs p prf p .
a. computing reliability using model counting let us now look more closely at how to compute the actual value of rel or conversely the failure probability of p .
since updefines a partition of the input domain from the law of total probability pr pcjup ipr pcjci pi furthermore from the definition of conditional probability pr pcjci pr pc ci pr ci .
in order to use modelcounting techniques e.g.
for the computation of the conditional probabilities let us define for a constraint cthe function c that returns the number of elements of dsatisfying c. is always a finite non negative integer because we assumed dfinite and countable.
under this same assumption pr c is by definition c d where d is the size of the domain that we implicitly assumed not null .
applying the same argument to the combination of equations and we obtain rel prs p ipr pcs ijup i jpr pcs ijcj pj i j pcs i cj cj pj the implementation of depends on d that is on the types of the input variables and will be described in section vii with some insight on its computational complexity.
b. example from our example of section i a let us consider the safety check.
after every step of an actuator the position of the flap is updated from its previous value adding the effects of the actuator and the wind.
after each update the current position is checked against the safety invariant as in listing .
listing .
safety invariant.
flapposition flapposition actuatoreffect windeffect if flapposition max position flapposition min position throw new overrunexception the probability of raising an overrunexception depends on both the actuator strength and the wind profile.
let us assume that only actuator b strength operates a single action according to listing .
in case of weak wind cf.
fig.
the reliability of the system is in case of strong wind it drops down to .
if instead of actuator b we use actuator a strength the reliability for weak and strong wind is and respectively.
notice that none of our actuators can converge to the goal in a single step for all the values of wind effect but the stronger actuator is more effective in keeping the flap position in a safe range even for stronger wind according to the invariant .
in sections iv a and v a we will first allow a single actuator to operate as many actions as needed and then we will study the complete system involving the concurrent use of the two actuators.
iv.
l ooping constructs in the previous section we assumed the termination of symbolic execution.
in general the presence of loop constructs may lead to infinite computation and requires convenient analysis strategies.
the solution provided by spf is based onbounded symbolic execution a bound is set for the exploration depth i.e.
the number of transitions executed when the bound is reached the exploration backtracks.
in this setting the symbolic execution is no longer complete and besides success andfailure paths a new set of paths is collected for executions interrupted before reaching an error or completing the run.
we call this set of paths grey and label the corresponding path conditions as pcg pcg pcg q. for the set of grey path conditions it is possible to define prg p analogously to the other sets prg p ipr pcg ijup 625the three setsfpcsg fpcfg andfpcggare disjoint and constitute a complete partition of the entire domain d .
hence it is straightforward to prove that prs p prf p prg p .
the intuitive meaning of prg p is to quantify the ratio of elements of dfor which neither success nor failure have been revealed at the current exploration depth.
this information is a measure of the confidence we can put on our reliability estimation confidence prg p confidence means that the symbolic execution is complete i.e.
for each element of dwe can state if it leads to a success or a failure.
smaller values of confidence may reveal a too small bound for the exploration depth and thus suggest the need for a deeper analysis.
confidence can thus be used for iterative refinement of symbolic execution.
indeed this measure is by construction a non decreasing function of the exploration depth hence the exploration depth can be increased until the desired confidence goal has been reached.
notice that if for very large values of the exploration depth the confidence keeps a steady value it could be the bad smell of an infinite loop that should be further investigated by the designers.
a. example let us now allow an actuator to run as many actions as possible that is until the goal comes closer that its actuation strength.
the body of the loop is constituted by an actuator action defined as in sect.
iii b listing .
actuator loop.
while abs goalposition flapposition actuator strength actuatoreffect sgn goalposition flapposition actuator strength flapposition flapposition actuatoreffect windeffect if flapposition max position flapposition min position throw new overrunexception the loop in listing may not terminate because of the wind effect i.e.
a wind effect greater than the actuator strength inhibits the ability to converge to the goal .
the results of the analysis using actuator a only strength with different exploration depths is shown in table i for the profiles of the wind effect table i analysis confidence .
exploration depth weak wind strong wind 10conf conf prs prs 30conf conf prs prs 50conf conf prs prs 70conf conf prs prs increasing the exploration depth the confidence grows revealing more accurate reliability predictions.
notice that due to the possibility of infinite loops the confidence cannot reach because in some cases the symbolic execution does not terminate.
indeed analyzing for example the weak profile with exploration depth still leads to conf with prs .
v. m ulti threading multi threading introduces non determinism in the choice of the next thread that could access the cpu.
different choices may affect the occurrence of failures and thus the reliability of the program.
without making any assumption on the way the next thread is chosen i.e.
without assuming a nextchoice distribution nor specific scheduling polices we want to identify the best possible sequence of choices that is the one leading to the highest reliability and the worst one .
the benefit of our approach is twofold first any possible thread scheduler will provide a reliability between the worst and the best case second inspection of the best case could possibly provide insights for the design of a scheduler able to improve software reliability for specific usage profiles.
the latter research path is still under investigation and in this paper we will focus on the identification of the best and worst cases.
first we need to extend the definition of symbolic execution provided in sect.
ii a by replacing ipwith a set of pairs hti ipii where each tiidentifies an active thread and ipi represents the next instruction to be executed by thread ti.
aschedule sis a sequence ti tj tkdefining the order of access to the cpu for all the active threads.
notice that for a given schedule the multi threaded program is reduced to a sequential one whose next instruction corresponds to the next instruction of the next thread to be executed according to the schedule.
spf allows one to symbolically execute multithreaded programs and to produce a set of pairs hsi pcii where pciare path conditions classifiable as success failure orgrey as in the previous sections and siis the schedule associated with the specific execution.
although for each schedule siin the spf output it is possible to analyze the corresponding pcs to compute its prs prf and prgit is not always true that their sum is equal to .
indeed it could be the case that for some inputs the execution terminates earlier than for others.
for example a specific value of the wind effect may lead to the failure of actuator a and the abort of the system while another may allow a further move to be performed for example by actuator b assuming the actuators run concurrently .
though the two schedules would be reported by spf as different it is the case that the first one is a prefix of the second.
generalizing certain input values may lead to early termination of the program either success of failure while others may let the execution continue.
based on this observation we can record the schedules produced by bounded symbolic execution into a prefix tree and define the maximal schedules definition maximal schedule .amaximal schedule s is a thread schedule that is not a prefix of any other schedule in the paths reported by a bounded symbolic execution.626for a maximal schedule siwe can define the set of path conditions pc sj2pre si pcj where pre si is the set of all the prefixes of siincluding the maximal schedule.
the intuition behind the construction of pc is the assumption of sias the prescribed schedule and then the accounting for possible early terminations of the execution captured in the pcs of its prefixes.
for a maximal schedule it is immediate to notice that pc covers the entire domain d. indeed either the execution has been terminated because of the exploration bound or no more instructions can be executed by spf.
for each maximal schedule siwe can now define prs p si jpr pc s jjup where pc sis the subset of pc leading to success.
analogous definitions can be stated for pc fandpc g. since pc covers the entire input domain we have that prs p si prf p si prg p si for every si.
maximal schedules can now be ordered according to their reliability obtaining the best and worst schedules and the corresponding reliability values.
in the ordering a special role is played by the grey area.
indeed it can be seen either in an optimistic or a pessimistic way in the former we add the value of grey to success while in the latter to failure.
notice that grey area keeps its role of measuring the confidence we can entrust in the analysis hence the same considerations of sect.
iv hold.
finally it could be the case that there is no single best or worst schedule but more than one reach the maximum possible reliability.
in this case the designer might pick the most suitable one according to the application domain.
the main issue in dealing with multi threading is the computational complexity of both spf and the probability analysis because of the number of schedules to analyze is exponential in the number of the active threads.
this problem can be alleviated by means of partial order reduction por .
the idea behind por it to exploit the commutativity of concurrently executed instructions which result in the same state when executed in different orders.
in particular two symbolic paths that are in the same partial order have logically equivalent path conditions and thus we conjecture they lead to the same probability results.
a. example concluding our flap controller example from sect.
iv a let us now allow the two actuators to run in two concurrent threads.
the run method of both of them is the same as listing with strength for actuator a and for actuator b. we bound the analysis at exploration depth and in order to rank the different schedules we conservatively consider the grey area as failure.
the reason for this small value for exploration depth lies in the complexity of symbolic execution.
indeed the number of paths to be stored in memory grows exponentially with the number of threads saturating our 4gb availability for depth .
the pcs collected are with spf time of minutes and probabilistic analysis of seconds thanks to the high reuse of previous computations as will beexplained in sect.
vii .
for the sake of space we report the results for the weak wind profile only the case of strong wind leads to analogous observations.
the best schedule is m m m a a b where mstands for the main thread with reliability and confidence .
we reported only the shortest schedule all the equivalent ones show the same pattern m as many actions of actuator a as possible and then as many actions of b as needed.
notice that this pattern is known in control allocation theory for aeronautics systems as daisy chain and is actually applied for flap control .
hence in this example our analysis did not only provide reliability estimation but suggested also a scheduling policy for the two actuators that corresponds to the one designed by control engineers.
the worst schedule is instead for example m a m a b a a b a b a b a a m a with reliability and confidence this is the shortest worst case schedule .
our conservative assumption played a significant role in the ranking because of the large grey area assumed as failure.
on the other hand the worst scheduling policy in this case keeps bouncing between actuators a and b reducing the effectiveness of control.
indeed beside the role of grey area within exploration steps the flap never reached the goal.
vi.
i nput data structures we now describe how the reliability analysis is extended to also handle programs that take as input structured data types e.g.
lists or trees .
a. usage profiles for data structures usage profiles for data structures are defined with the help of java predicates i.e.
boolean methods that define data structure properties that partition the input state space.
for example for a program with an input list the up may specify that the input list is acyclic of the time and cyclic .
as before we restrict ourselves to finite input domains which for data structures will also limit the number of possible heap nodes in the input.
spf can analyze programs with unbounded data structures as inputs using lazy initialization .
the result of symbolic execution is a set of paths each characterized by a path condition that encodes both numeric and heap constraints.
listing .
swapnode example class node int elem node next node swapnode if elem elem next.elem node t next next t.next t.next this return t return this 627as an example consider the java code in figure that declares a class node implementing linked lists.
the fields elem andnext represent respectively the node s integer value and a reference to the next node in the list.
the method swapnodedestructively updates its input list referenced by implicit parameter this according to a non linear condition on the first two nodes.
symbolic execution results in eight symbolic paths due to the condition and the different aliasing possibilities in the input e.g.
pc1 in elem in elem in elem in next in in6 null pc2 in elem in elem in elem in next in in6 null etc.
.
seven paths are successful and one leads to failure null dereference for pc3 in next null in6 null .
there are no grey paths since there are no loops .
b. model counting for data structures though the procedure described so far can be applied on any finite and countable input domain the case of data structures deserves special attention in the definition of the counting procedure .
indeed in the worst case a complete and expensive enumeration of all the possible input instances might be performed.
to deal with this issue we propose to use korat a tool that performs constraint based generation of structurally complex test inputs for java programs.
korat provides efficient generation and therefore also counting of input data structures that satisfy a complex predicate within pre defined bounds.
the predicate is written as a boolean method called repok whose body can embed any arbitrary complex computation.
the finitization of the input domain is instead accomplished by specific korat methods to specify bounds on the size of input data structures as well as on the domain of primitive fields.
thus we can encode the constraints provided by symbolic execution together with the constraints from the usage profile as a repok predicate and run korat to count the data structures that satisfy the constraints for the given finitization.
reliability for the list example as an example of the use of korat let us compute the reliability of our list program.
assume a usage profile that specifies that the input list is acyclic with probability .
and it is cyclic with remaining probability .
.
as there is only one failure symbolic path revealed by a null pointer exception in the evaluation of the if condition it is simpler to compute the failure probability and thus the corresponding reliability.
the path condition for the failure path as revealed by spf is input6 null input next null.
since the path condition for the failure constraint is only satisfiable for acyclic lists we get the probability of failure prf p as input6 null input next null acyclic input acyclic input korat computes input6 null input next null acyclic input and acyclic input for lists having up to nodes and elem between and giving probability of failure .
.
thus since the execution always terminates due to finitization the probability of success is .
.
.c.
non linear and floating point constraints korat can handle arbitrary numeric constraints on the fields in the input data structures including non linear integer constraints and constraints involving floating point numbers as in the example above for floating points the finitization should include discretization criteria.
thus korat could be used as an universal model counting procedure that can handle arbitrary constraints expressed as java predicates.
on the other hand for special input domains more efficient procedures can be substituted to korat as will be discussed in the next section.
we also note that it is the responsibility of the user to write the complex java predicates in the usage profile and to ensure that these predicates are disjoint.
to ease this burden we have defined patterns for some commonly used predicates such as acyclic andsizefor linked lists that can be used and modified easily.
in the future we would like to explore established logics to simplify the specification task.
vii.
i mplementation reliability analysis is performed in two phases cf.
sect iii .
first spf collects path conditions leading to success failure and grey conditions second the probabilistic analysis is performed.
in this section we will describe the implementation of probabilistic analysis for spf implementation see .
the purpose of the probabilistic analysis phase is to compute pr a where ais a set of constraints on the input variables of the program.
the complexity is in terms of the number of variables and the number of constraints composing a. for large problems in either of the two dimensions approaching the problem as a whole could be time consuming.
in this section we will show a divide and conquer strategy to improve time efficiency.
the central idea is that constraints in aidentify a dependency relation dep among the constrained variables that can be formalized as follows let x y and zbe variables in a 8x dep x x 8x yifxand yappear in the same constraint then dep x y 8x y z dep x y dep y z dep x z the intuitive meaning of the deprelation is that if dep x y then the values assumed by xaffect the values that can be assumed by y. for example from fx y x 5gwe deduce that the value of yis affected by the values of x and vice versa.
the relation depis an equivalence relation thus it induces a partition on the set of variables appearing in a. for this reason we can rewrite aas the conjunction of the subsets a each of whom collects all the constraints involving a variable in the equivalence class of deprepresented by v. such conjuncts are logically separated hence it can be proved that pr a vpr a .
for example let abefx y 5g its probability can be computed as pr a pr x pr y .
ifa is a set of linear integer constraints we add a further normalization step to remove inequalities and redundant constraints.
the normal form obtained is composed by the disjunction of non overlapping constraint sets describing628exactly the same event space of a .
for example a fx x x x6 7gwould be transformed into the equivalent disjunction fx x 6g f x x 9g.
this step allows to both simplify the set of constraints by removing redundancies and to possibly split a into a number of smaller subsets let us call them ai .
the sets ai are non overlapping by construction hence we have that pr a ipr ai .
this simplification is performed through the external tool omega .
model counting is now used to compute the probability value of each a ai respectively in the assumed domain as explained in section iii.
this operation is performed through the external tool latte for integer linear constraints because of its efficiency on this special case.
for more general constraints or data structures an analogous simplification can be defined and the counting procedure can be performed through korat as shown in section vi.
the reduction of ainto a set of sub problems ai has three valuable benefits.
first the time complexity of latte is polynomial in the number of variables and for korat it is up to exponential in the same measure each a usually involves a subset of the variables appearing in a speeding up the execution time of the external tools.
second both the split from atoa and from each of them to ai allow a natural parallelization in a map reduce fashion.
third the reduction of the constraints into sub problems enhance the reuse of previous computations through caching of the results indeed the same subset of constraints may appear in many path conditions because for example they share the same prefix.
reuse can lead to a significant improvement in the probabilistic analysis time see sect.
viii .
the effect of reuse is even more evident in case of multi threading where different schedules may lead to similar path conditions and in case of large usage profiles where several usage scenarios may share a large set of constraints as will be shown in the next section.
in the worst case simplification does not improve execution time and the elements of ahave to be counted all at once.
but in our experience real software is far from the worst case and simplification leads to a significant speed up cf.
sect.
viii .
as a final remark notice that the two phases are completely independent.
by replacing spf with a symbolic execution engine for another language it is possible to reuse our implementation for the reliability analysis of programs written in any other language.
our tool implementation can be donwloaded from as well as the source code and usage profiles of our example.
viii.
v alidation a. on board abort executive oae we applied the reliability analysis to a java model of a nasa software component that was originally written as a prototype for the crew exploration vehicle s ascent abort handling the onboard abort executive oae .
the oae monitors the status of the vehicle during the ascent phase offlight and it checks a set of flight rules that are supposed to be invariant during the ascent.
whenever a flight rule is violated the oae decides that an abort is required and it selects the abort mode which is safest for the astronauts.
the oae receives its inputs e.g.
current altitude launch vehicle internal pressures etc.
from sensors and other software components.
the analyzed code is approximately lines of code it has a large input space input variables and complicated logic.
we note that the component did not have any errors so instead of computing the probability of reaching an error or conversely the probability of the component behaving correctly we compute the probability of an abort or conversely the probability of mission success .
domain some of the range restrictions on the inputs were directly provided by the domain experts while others were determined from the simulation data used for testing the flight software.
for each input we determined the minimum min and maximum max values and we used these values to encode the ranges as the domain for the analysis.
we used the extra quantity dto increase the chances that symbolic execution would analyze failure cases.
usage profile table ii shows the results of analyzing three different usage profiles for the oae.
the first profile was to simply look at a uniform distribution of values for each variable within its domain.
to see how the analysis scales we then considered a profile where for one variable thrust a gaussian normal distribution of values was used and lastly a case where a gaussian distribution for two variables thrust andtank pressure was used.
the gaussians were produced by discretizing the ranges into segments for thrust and for tank pressure which led to and usage profile constraints for respectively the one and two variable cases.
results the analysis was run on a red hat linux 64bit machine with .7gb of memory and a .8ghz intel i7 cpu.
from the results in table ii one should first notice that since we run the reliability analysis after symbolic execution the total paths paths and the paths that lead to an abort aborts doesn t change for different profiles.
similarly the time for symbolic execution spf is essentially the same for all runs the small variance is natural in java .
since the number of profile constraints increase between the three analyses and respectively the number of times we need to get a model counting result also increase counts which in turn results in an increase in the time spent calculating the probabilities probs .
however notice that the time spent simplifying constraints with omega and the model counting with latte is pretty much invariant for the different profiles.
this is due to the caching described in section vii.
the number of times a reliability result would need to be calculated will always be the number of paths times the number of profile constraints however from section vii we know that we first split the total path constraint into independent parts then apply omega for simplification and only then call latte.
caching is applied at all steps following the dependency step thus both omega and latte results can be reused.
because of the nature of symbolic execution where629table ii onboard abort executive profiles profile paths aborts spf ms counts probs ms omega ms latte ms reliability uniform .
gaussian .
gaussian .
prefixes to a path are shared there is very high reuse of previously calculated results although not shown it is more than for both the omega and latte caches .
the actual chance of an abort happening is clearly extremely small and the code seems to be very robust with regards to the different profiles.
it is interesting to note that the result of our analysis could be further used to help estimating the reliability not only of the oae component but also of the vehicle monitored by the oae since we essentially compute the probability of a flight rule being violated and the rules should be invariant for the whole flight.
b. binary tree for the next set of experiments we use the binary tree implementation previously studied in and .
this code has an error in the delete operation that is triggered when non leaf nodes are deleted.
what we want to study here is the reliability of the code i.e.
the probability of avoiding the error under different usage profiles.
we limit the values in the container to the range .
the actions we can perform areadd anddelete of an element.
lastly we only look at sequences of actions after which we check an assertion to determine if the error was triggered.
since our sequences are of finite length there cannot be any grey paths here all paths will be either correct or they will trigger the error.
table iii binary tree reliability profiles actions values reliability uniform uniform delete uniform delete uniform no delete last uniform no delete last uniform uniform ordered uniform ordered uniform ordered uniform ordered uniform ordered uniform ordered the usage profile can vary in the probability of the actions being performed and the values being used.
table iii shows the reliability results obtained with various different usage profiles for actions and values.
note that the examples are small and as such we do not report timing seconds in all cases .
the first case considered is if both the actions and the values are chosen uniformly from their respective domains.
the reliability result is the same as reported by the tool2 from where only uniform probabilities are considered.
next inspired by the fact that we know the error is within the delete action we consider a case where we delete of 2available from time and therefore add of the time and then a case where we delete only of the time.
our intuition might suggest that the former case should be more unreliable but in fact the more one delete the more reliable the code is.
the reason for this seeming anomaly is that the error is not just due to delete it is also when and how much you delete as shown by the case where we specify a profile that will never have a delete action in the last entry or the last two entries in the sequence.
in the latter case the reliability is .
next we consider adjusting the profile of the values we calladd anddelete with but we keep the choice between these two actions uniform i.e.
each with probability .
specifically we focus on how ordered the values are that we call the actions with.
we consider sequences of length hence if the value viis the parameter to call iin the sequence i we can express the profile as using the ordered case usageprofile v1 v2 v2 v3 v3 v4 v4 v5 !
v1 v2 v2 v3 v3 v4 v4 v5 the results show that the code becomes more reliable when the ordering of the values increases.
in fact it will be completely reliable if the values are ordered.
the reason for this is that in the ordered case one will only delete the last value added and the actual error in this code is only triggered when deleting a non leaf node in the tree and the last value added will always be at a leave node .
ix.
r elated work reliability analysis is considered a key element in the design of software systems.
the high availability of monitoring data allows nowadays to compute valuable and accurate characterization of both the actual software behavior and the usage profile.
the original approaches tended to consider the software system as a black box and to observe its reliability from the outside .
most recent trends are focused on modular or component based systems and assume an architectural level perspective .
with our work we further open the view on the internal structure of the software down to source code.
previous approaches deal with code artifacts passed through an intermediate abstract representation e.g.
as sketched in or applied in .
a number of architectural styles can be mapped to analytical models e.g.
there have been defined parametric contracts for software architectures e.g.
the issues of error propagation and transformation in complex architectures has been analyzed .
the idea to extrapolate models from code is effective for a number of purposes.
nonetheless the coherency between models and code and among different models e.g.
concerning different measures of the same related phenomena 630could be hard to maintain.
models could also add a semantic gap between the developer and the artifacts she is used to reason on.
direct syntax driven approaches have not been so popular.
in a syntax driven approach has been proposed to deal with workflows specified by structured languages.
this methodology is based on attribute grammars lr parsing and exploits attribute synthesis mechanisms to compose reliability estimates provided for single tasks up through the entire workflow.
on a technical level a closely related work is that of where model counting was used within an extension of spf to calculate path probabilities.
however they did not consider usage profiles and therefore cannot calculate reliability as described here they also didn t consider multi threading nor structures.
although our approach is cast in terms of an extension to spf it should be noted that it can be applied to any symbolic execution approach e.g.
pex and klee where we have access to a path condition thread schedule and whether the path led to a failure success or unknown.
probabilistic model checking is also related to what we do here with the main difference being that we calculate probabilities for the system based on the usage profiles whereas for probabilistic model checking the input is a system with the probabilities for each transition already provided.
another difference is that we take as input java code whereas probabilistic model checking tends to analyze models of real systems.
an exception is and that takes as input java code annotated with probabilities but their goal is to determine the progress of the model checking and not reliability analysis.
x. c onclusions and future work we presented an approach to calculate software reliability under a specific usage profile directly from the source code.
the current implementation supports linear integer arithmetic operations using the latte model counter and an extension using korat to count structures is under development.
many areas for future work exist a section vi c describes extensions to the data structure approach b more failure properties e.g.
temporal properties c usage profiles for sequences where probability distributions depend on previous events d partial order reduction for multi threading and e runtime analysis to derive profiles directly from running systems.
acknowledgment this research has been partially funded by the european commission programme ideas erc project 227977smscom.