why don t software developers use static analysis tools to find bugs?
brittany johnson yoonki song and emerson murphy hill north carolina state university raleigh nc u.s.a. bijohnso ysong2 ncsu.edu emerson csc.ncsu.edurobert bowdidge google mountain view ca u.s.a. bowdidge google.com abstract using static analysis tools for automating code inspections can be beneficial for software engineers.
such tools can make finding bugs or software defects faster and cheaper than manual inspections.
despite the benefits of using static analysis tools to find bugs research suggests that these tools are underused.
in this paper we investigate why developers are not widely using static analysis tools and how current tools could potentially be improved.
we conducted interviews with developers and found that although all of our participants felt that use is beneficial false positives and the way in which the warnings are presented among other things are barriers to use.
we discuss several implications of these results such as the need for an interactive mechanism to help developers fix defects.
i. i ntroduction software quality is becoming more important with the increasing reliance on software systems.
there are different ways to ensure quality in software including code reviews and rigorous testing.
software defects or bugs can cost companies significant amounts of money especially when they lead to software failure .
static analysis tools provide a means for analyzing code without having to run the code helping ensure higher quality software throughout the development process.
there are a variety of ways to perform automatic static analyses including at the developers request continuously while creating the software in a development environment and just before the software is committed to a version control system.
the tool may allow the developer to configure what kinds of bugs it finds and sometimes even define new bug patterns.
some automated static analysis software such as the software integrated into intellij idea provide quick fixes .
a quick fix is a suggested solution for a defect that is automatically applied to a developer s code.
to help explain the state of the art of static analysis tools let us look at findbugs as a concrete example of how these tools work .
findbugs runs as a plug in for the eclipse and netbeans integrated development environments ides .
it can also be run from the command line or as a separate tool on its own.
when run in the ide findbugs has its own perspective where the defects are listed and organized.
each defect is assigned a severity signifying how important the defect is either high medium or low each represented by red yellow andgreen bug markers respectively.
findbugs offers a select few quick fixes.there are many situations where a developer may consider using a static analysis tool to find defects in their code.
let us consider a developer susie.
susie is a software developer at a small company.
she wants to make sure that she is following the company s standards while maintaining quality code.
she needs a way of checking her code in her ide before submitting it to the general code repository without worrying about any outside dependencies that she has no control over.
susie decides that her best bet is to install a static analysis tool.
she decides to install findbugs because she likes the quality of the results and the fact that bugs can be found as she types at first she is very happy with her decision and feels productive when using it.
the above scenario is an interpretation of an experience one of our participants recalled during their interview.
static analysis tools use well defined programming rules to find defects early in the development process when they are cheap to fix .
for example there are static analysis tools that can alert developers to synchronization issues which can lead to unsafe thread interactions.
developers have been able to eliminate many defects that were previously overlooked at large companies using the warnings produced by static analysis tools.
despite the benefits of using static analysis tools to find bugs consistent usage of these tools is not very frequent .
remember susie who adopted a static analysis tool to improve the quality of her code?
after using the tool for a while dealing with the interface became a burden finding the warnings was not easy and when she did she had a hard time interpreting the feedback.
inspecting her code without using the tool involved more work but she prefered to do it this way to avoid the time and confusion involved with using the tool.
there have been studies to investigate ways of improving static analysis tools.
however none look at what the tools do or can do for a developer what features developers use what could be improved andwhy .
our research aims to understand why software developers are not using static analysis tools and how current tools could be improved to increase usage based on developer feedback.
for our study we intend to focus on static analysis tools used to finds bugs.
this includes tools like findbugs lint intellij which includes built in static analyzers and pmd .
findbugs will be referenced the most as it is the tool we chose to use during our interviews.
.
c ieee icse san francisco ca usa672 in the following sections of this paper we will first discuss some related work section ii and the methods used in our study section iii .
section iv presents the results and threats to the validity of our study.
in section v we discuss implications for static analysis tools and finish with a discussion of future work section vi and take away points section vii .
ii.
r elated work there have been many studies on static analysis tools many of which focus on their correctness and functionality .
unlike existing work our work focuses on developers perception on using static analysis tools including interacting with the interface of the tool and what may have caused their perceptions.
perception plays an important role in when considering human and computer interactions and can be influenced by a number of things such as the subjective preferences of the user.
ayewah and pugh conducted a study where they claimed that static analysis tools should help engineers find bugs as early as possible in the development cycle when they are cheap to fix .
they interviewed findbugs users by phone and conducted a controlled study with students to see how they use findbugs and handle defects that are labeled not a bug .
their work is similar to ours in that they are interested in how developers use static analysis tools.
our work builds on this work by recruiting various tool users for interactive participatory interviews.
khoo et al.
examined and focused on the interface of static analysis tools and how the interface could be improved .
they developed a user interface toolkit called path projection that uses program visualizations to help developers walk through the error reports produced by static analysis tools.
path projection was designed to improve and simplify the process of triaging bug reports or labeling bugs as a false or true positives by utilizing checklists to systematically label bugs.
this study is similar to our work in that they look at improving the static analysis tool user experience.
our study builds on this study by investigating not only improving the user experience but also finding out why these improvements need to be made from the developers who use them.
heckman and williams conducted research in an attempt to develop a benchmark faultbench that would help developers compare and evaluate static analysis alert prioritization and classification techniques .
the overall goal of their research was to make using static analysis tools easier and more useful to developers.
our work is related in that we are also looking for ways to improve current static analysis tools for developers.
layman et al.
recruited participants to investigate factors that developers may consider when deciding whether to address a defect when notified of it .
this study is related to our work in that a similar methodology is used and they are also interested in learning more about how developers use these tools and how it can be made easier.
our work builds on these works by focusing on various aspects of using static analysis tools including how users interact with the tools.iii.
m ethodology for this study we conducted interviews with software developers.
each semi structured interview lasted approximately minutes and with the participant s consent was recorded.
by conducting semi structured interviews we aimed to achieve the flexibility needed to get as much detailed information as possible .
we prepared a script of questions for the interview but would add or omit questions on the fly depending on how detailed a participant was in their responses.
we created and modified the script as we conducted trial interviews any changes made to the script was based on the responses we got from our trial participants .
upon completion we manually transcribed each session.
we performed qualitative analysis1on the transcripts by coding the transcriptions.
this process is discussed in detail in section iii f. a. participants we conducted this study with a group of participants.
although this seems like a small sample we followed a similar methodology to that of layman et.
al.
s study that only had participants .
participants were recruited using an electronic recruitment flyer that was sent out to our industry contacts to then be sent to developers within their company.
sixteen of our participants are professional developers at a large company and are graduate students at north carolina state university with previous industry experience.
participants years of development experience ranged from to years.
we did not explicitly ask participants about their experience building static analysis tools however based on conversations approximately participants had tool building experience.
we interviewed two participants remotely one by phone and one by video chat due to location differences.
each participant filled out a short questionnaire used to collect demographic information.
table i shows the statistics and background information gathered from the questionnaire and interviews.
the first column lists the participants pseudonyms given for confidentiality purposes.
the second and third columns show the open source tools and closed source tools that they have used to find bugs.
if a space has a it indicates no response from the participant.
b. research questions for this research we want to learn rq1 what reasons do developers have for using or not using static analysis tools to find bugs?
rq2 how well do current static analysis tools fit into the workflows of developers?
we define a workflow as the steps a developer takes when writing inspecting and modifying their code.
rq3 what improvements do developers want to see being made to static analysis tools?
1all study materials including interview scripts and coding categories are available at bijohnso ffsat.html673table i descriptive statistics reported by participants .
participant open source tools closed source tools local abby findbugs intellij yes adam checkstyle findbugs pmd intellij yes andy findbugs lint jtest yes chris checkstyle findbugs lint coverity yes cody dehydra yes frank yes gordon lint checkstyle findbugs yes jake findbugs lint flexlint klocwork insight visual studio yes james lint checkstyle findbugs visual studio yes jason lint findbugs yes john checkstyle copy paste detector cpd findbugs lint pmd codepro yes jordan checkstyle findbugs pmd jtest yes josh findbugs lint coverity no lee checkstyle findbugs lint visual studio yes matt lint flexlint pycharm yes mike cpplint lint yes phil yes ray checkstyle findbugs yes ryan findbugs lint coverity yes steve checkstyle cpd findbugs lint intellij yes tony cpd findbugs lint splint cpplint pmd checkstyle coverity no we ask these questions because answers to these questions will give toolsmiths and researchers areas for future work and improvement in the area of static analysis tools.
research has shown that the way a tool interrupts a developer s workflow is important therefore we wanted to specifically investigate this aspect of tool usage .
the interviews focused on developers experiences with finding defects using static analysis tools.
learning developers relevant experiences and observing how they use static analysis tools to find bugs may shed some light on why these tools may be underused.
the interviews were organized into into three main parts questions and short responses section iii c interactive interview section iii d and participatory design section iii e .
c. part i questions and short responses during part question and short response we asked developers questions related to their general usage understanding and opinion of static analysis tools in order to answer rq1.
some of the questions asked include can you tell us about your first experience with a static analysis tool?
can you remember anything that stood out about this experience as easy or difficult?
have you ever used a static analysis tool in a team setting?
was it beneficial and why?
have you ever consciously avoided using a static analysis tool?
why or why not?
what in your opinion are the critical characteristics of a good static analysis tool?
d. part ii interactive interview the second part is what we call the interactive interview.
the goal behind the interactive interview is to be able to observe developers actually using a static analysis tool.
thisallowed us to get more detailed information as to how developers are using their tools.
we aim to use the information obtained during this portion to address rq2.
we asked our participants to explain what they are out loud so we could get a better understanding of their workflow and thought process.
practice interviews before this study revealed that using the interactive interview portion produced more detailed information regarding when and how developers use their static analysis tools .
some of the questions asked during this portion include now that you have run your tool and gotten your feedback what is your next move s ?
do you configure the settings of your tool from default?
if so how?
does this static analysis tool aid in assessing what to do about a warning?
do you feel that quick fixes or code suggestions would be helpful if they were available?
for confidentiality reasons not all of our participants could use their own workstation for this part of the interview.
for those who could not we provided open source projects in java such as log4j and ant and asked each participant to run findbugs on one of them.
we chose findbugs because it is one of the most popular and mature static analysis tools for eclipse.
due to technical difficulties our remote interviews were not able to fully experience the interactive portion.
each was given a scenario of static analysis tool usage and asked to first explain their thought process in walking through that particular scenario.
we then asked the same questions as we would have asked if they had been local.
2participants were only asked about quick fixes and code suggestions being useful when they mentioned either during the question and answer or interactive interview that they either a find quick fixes useful b felt that the tool should be more helpful or c did not understand how to fix the defect we presented them with.674e.
part iii participatory design we intended the last part of the interview to get the participants to make design suggestions for improving static analysis tools.
we utilized a concept called participatory design which involves getting stakeholders in this case our participants involved in the design process by allowing them to show what they want instead of saying it.
in order to promote creativity each participant was given a blank sheet of paper and asked to show us what they wanted their tool to look like and how it should work .
participants were not required to draw something but of them did.
the rest of our participants gave verbal descriptions of tool features they desired.
f .
coding interview responses after completing the interviews we manually transcribed each interview.
then the transcriptions were coded .
coding is a process that is meant to make referencing transcriptions quicker and easier .
we used gordon s basic steps to code our interviews and use the codings to help organize the results section iv .
before coding an interview coding categories need to be defined.
these should be general enough for relevant information to be grouped together but detailed enough that a concrete example only falls under one category.
because of this it is possible to have emergent categories that may need to be defined after reading the transcriptions.
we developed and used the following coding categories tool output which includes anything related to the output produced by the tool for example false positives supporting teamwork which includes anything about using static analysis tools in a team or collaborative setting user input and customizability which highlights points made about the customizability of the static analysis tools for example modifying rule sets result understandability which includes anything said about the ability or inability to understand or interpret the results produced by a static analysis tool workflows which is defined as anything related to the steps a developer takes when writing inspecting and modifying their software for example tool integration and tool design which includes the proposed tool design ideas from our participants.
examples of each of these categories from the transcriptions are as follows tool output jason .
.
.
like i mentioned with flexlint it gives you so many warnings and sifting through them is so arduous that whenever i just look at it i m like ehhh forget this.
user input customizability andy .
.
.
it s like is this list prioritized by you know what s important to me?
no.
you know?
and there may be a default listing that should be prioritized because like this one s inefficient.
supporting teamwork john the only reason i like the batch results is to communicate broadcast to the team a sense of progress or lack of progress.
result understandability matt so now i wanna know why raising a string exception is bad.
like what should i be instead?
since it thinks it s a problem.
and so none of these really help me.
workflows mike clang is my favorite.
its built into the compiler.
you don t have to invoke anything special.
tool design chris i dont mind the idea of the actual source code itself having some plasticity .
.
.
lets say the fourth line there was some error here.
.
.
having the 5th line drop down and having the content expand with maybe all sorts of annotations about my code.
the next step in gordon s methodology is to assign category symbols to each category for easier indexing and processing of information.
gordon then suggests finding and classifying the relevant information in the transcriptions using the category symbols.
in our codings each coding category had its own color as a symbol if a portion of a participant s transcription fell into one of the categories the text would be highlighted the same color as its respective category.
a participant s coded interview could contain multiple categories or even multiple data items for one category.
to ensure consistency one person was responsible for coming up with the coding categories and symbols and going through the transcriptions to apply them.
the last step is to check the reliability of the codings.
for our study once the codings were complete it was passed off to the other contributors to look over.
if there were any discrepancies they were discussed and resolved as a group.
this includes items that could fall into more than one category in this situation either a new more specific category or a sub category was created for the item.
the purpose of the categories are to organize the data in a relevant and useful manner they are not meant to directly correlate with the research questions.
iv.
r esults in this section we will discuss the results we obtained.
we answered our research questions by linking the questions to coding categories and interview parts.
after analyzing the results we believe the following to be true our first research question rq1 can be answered by observing the results that have been categorized under tool output supporting teamwork user input and customizability result understandability and developer workflows the information collected in these categories could be reasons why developers are or are not using static analysis tools.
our second research question rq2 can be answered by observing the results that have been categorized under developer workflows.
our third research question rq3 can be answered by observing the results that have been coded under tool design most of these results are from the participatory design portion.675in each category we expected there to be negative and postive remarks about current tools both of which are equally important in answering our research questions anything positive could be a reason for use while anything negative could be a reason to discontinue use.
for each coding category we separated the relevant statements into positive statements and negative statements if something good is said about a static analysis tool it s considered a positive comment and vice versa for a negative comment.
in figure we can see that the majority of our participants have had problems with tool output customizability and workflow integration and all but one of our participants have had problems with understanding results.
tool design is not included because this category was defined to capture the developers ideas for improving static analysis tools.
their reasons for wanting the features are captured in the other categories.
a. rq1 reasons for use and underuse our interviews revealed that there are a variety of reasons developers may have for choosing to use a static analysis tool to find bugs in their code.
one of the obvious reasons is because too much time and effort is involved in manually searching for bugs.
five out of our participants feel that because static analysis tools can automatically find bugs they are worth using.
during his interactive interview jason told us anything that will automate a mundane task is great.
in other words one reason for using static analysis tools is that they automates the process of finding bugs.
another reason developers might use a static analysis tool is if it is already available in the development environment and ready to be used.
for of our participants this was the case.
development environments such as intellij and pycharm come with built in static analyzers which requires little extra effort on the developer s part.
two of our participants matt and adam use pycharm and intellij regularly and like the fact that static analysis is already integrated.
for of our participants a good reason to use static analysis tools is to support team development efforts.
according to josh and andy static analysis tools do this by raising awareness of the potential problems or dumb mistakes in the code earlier in the development process.
for cody and ray static analysis tools are useful for communicating and enforcing coding standards and styles on development teams.
some developers enjoy using the static analysis tools they use to find bugs because of the level of customizability.
three of our participants fit into this category.
according to james the customizability of a tool can play a large part in the volume and quality of output developers get.
although some of our participants could find reasons to use static analysis tools to find bugs most of our participants brought up conflicting concerns that could make the decision to adopt and use a static analysis tool less obvious.
tool ouput.
tool output was a popular dicussion topic.
out of the people we interviewed people expressed the negative impacts of poorly presented output.
static analysis tools are known to produce false positives and these false positives can outweigh the true positives in volume .another known fact is that especially with larger projects the number of warnings produced by a tool can be high sometimes in the thousands .
some of our participants felt however that false positives and large volumes of warnings would be less burdensome if the way the output is presented was more user friendly and intuitive.
cody who likes using dehydra finds himself frustrated at times because the results are dumped onto his screen with no distinct structure causing him to spend a lot of time trying to figure out what needs to be done.
jason wishes that his tool s output would be a slice that shows what the problem is and what else could be affected in order to more quickly assess what is or is not important.
this slice should be taken from the entire project using call hierarchies to show which parts are affected by each defect.
during his interactive interview he commented on a previous experience with findbugs.
he had a large list of warnings to scroll through but without there being any context to the problems it just seemed like a bunch of junk to sift through which made him not want to bother using it.
it may be worth investigating how valuable an output like this would be.
collaboration.
in industry software development is often a team effort.
for of our participants lack of or weak support for teamwork or collaboration is one reason that teams as well as individual developers may not adopt or regularly use static analysis tools.
according to john although static analysis tools are useful for trying to enforce coding standards there is no easy way to share the settings with other people on the team so it ends up being a cumbersome manual process and causing confusion when the standards need to be changed.
many of our participants mentioned the desire for a way to easily communicate and collaborate when using their static analysis tool especially in a team setting.
although static analysis tools can be beneficial in team settings current tools are not collaborative enough for some developers.
newer versions of findbugs offer a cloud storage feature that can be used store share and discuss warning evaluations .
although a feature like this does make it easier to communicate and share warning evaluations between developers to add a comment to a bug or current evaluation a web browser is needed.
this takes the devloper out of context and out of the development environment which could demotivate some individuals from checking them when they should.
customizability.
for of our participants customizability is important however many tools are not trivial to configure and do not accomodate the customizations that developers want.
false positives and large volumes of warnings are wellknown downsides to use static analysis tool to find bugs however frank told us he believes that the way you configure your tool plays a large part in the output you get.
john stated during his interview that many tools are so hard to configure they prevent you from anything.
sometimes it is difficult just to get to the menu where the options for configuring a particular feature are which participants matt and josh agree with.
one of our participants jake found himself in an interesting situation during his interactive interview where he could not figure out how to customize his676714793171019715 positives negatives positives negatives positives negatives positives negatives positives negativestool output supporting teamwork user input andcustomizabilityresult understandability developer workflowsnumber of participantsfig.
.
the number of participants in each category expressing the good and the bad about static analysis tools they have used.
tool and wound up having to search the web to find out where the tool s preferences were.
a common problem expressed by most of the participants is the inability to temporarily ignore or suppress certain warnings.
although some static analysis tools allow developers to turn off certain filters not all developers are comfortable with turning warnings completely off.
matt for example is afraid that he may not remember to turn it back on.
the notion of dismissing or ignoring static analysis warnings may be too coarse as jordan noted he would prefer that static analysis tools offered a way of recording his judgement about that warning.
more sophisticated judgements may include things like this warning isn t a problem now but may be in the future if the following conditions are met.
.
.
.
result understandability.
the main objective when using a tool like findbugs is to learn what defects are in the code so that problems can be removed.
a developer not being able to understand what the tool is telling her according to our participants is a definite barrier to use.
nineteen of our participants felt that many static analysis tools do not present their results in a way that gives enough information for them to assess what the problem is why it is a problem and what they should be differently.
james told us during his interview that it s one thing to give an error message it s another thing to give a useful error message.
when talking about the eclipse python plug ins he also stated i find that the information they provide is not very useful so i tend to ignore them.
a few participants felt that it would be helpful to have links to more details or examples in the error reports.
in some situations more information is needed to understand exactly what the problem is and why it is a problem understanding why a defect is a problem can help the developer better assess whether the error is a false positive and try to avoid repeating the same problem.
ryan told us during his interactive interview that a start would be using real words or a more natural language to explain the problem.
the most frequently mentioned difficulty when using static analysis tools is lack of or ineffectively implemented quick fixes.
most of our participants expressed interest in having their tool provide code suggestions or quick fixes that assist them when attempting to fix a bug abby proclaimed if youcan tell me it s an error you should be able to tell me how to fix it.
jordan strongly agrees he loves tools that have quick fixes and hates tools that do not.
according to our interviews these fixes do not have to be automatic some prefer that code suggestion previews be used or possibly using examples to get a better understanding of how to fix the problem.
some participants expressed interest in but skepticism toward integrating quick fixes into static analysis tools.
for example during jordan s interactive interview he noted that sometimes when using multiple tools they may have conflicting quick fixes or solutions.
in frank s past experiences with automated code changes he has had to do manual refactorings because something was done wrong because of this he prefers to use find and replace to make his own changes.
another participant adam was concerned with knowing whether the semantics of his code would be preserved after applying a quick fix.
most static analysis tools if they offer quick fixes leave it to the developer to figure out exactly what has been done after it has been done.
almost all of our participants agree that effectively designed quick fixes can help them to better understand the problems its tool is telling them about leading to a better sense of productivity for the developer.
b. rq2 workflow integration the most common topic during the interviews was tool environment integration.
sometimes a developer s process includes running a static analysis tool but more often it is not part of a developer s workflow to stop and run a tool in the middle of working on some code or a specific task she usually prefers finding a stopping point in her code to run the tool .
analysis of our interviews reveal that while this is true there are many different ways that developers may want their tool to fit into their development workflow.
for example some developers prefer that the tool run in the background it is easier for them to figure out what is wrong if they are in the process of it and do not have to think about invoking the tool.
on the other hand some developers do not use ides so if they are to use a static analysis tool compiler integration is very important.
nineteen of the developers we interviewed expressed the importance of workflow integration to them and how these needs have or should be met.677for some of our participants there are features of static analysis tools they have used that helped the tool better integrate into their workflow leading to increased usage of the tool.
in fact john feels that static analysis tools can be used to help organize your workflow based on the results it produces.
for example if you are running a static analysis tool on some code for the first time it can be a good indicator of the kinds of bugs the tool finds and that may be present this can give an idea as to how detailed of an analysis the tool does possibly giving you a better idea of when it would be best for you to run it.
of all the tools adam has used in the past he much prefers to use intellij and its built in static analysis to find bugs they are tightly integrated making it seem more real time .
for these participants as well as a few others integration with the development environment plays a major role in their decision to use or continue using a static analysis tool.
common standalone static analysis tools like findbugs and pmd have the ability to integrate with ides like eclipse and netbeans which becomes especially important when you are using more than one static analysis tool at a time as we learned from discussing a past experience of steve s where he was using different static analysis tools.
jordan and chris like how findbugs pmd and checkstyle fit into their development processes for jordan it is an integral part of his workflow.
for the majority of our participants however current static analysis tools are not enough to effectively integrate into their development process.
one of the biggest demotivational forces on a developer when it comes to using a static analysis tool to find bugs is when it is what tony calls a disjoint process.
many of our participants especially those who do not use ides do not like when they have to go out of their coding environment to use a tool or view the results produced by the tool.
for example frank lee james and andy commented on how painful it was during their interactive interview to have to switch perspectives in findbugs to explore the complete listing of bugs.
according to lee having to open another perspective to know what is going on is a guarantee that unmotivated people will not do it.
for frank although it is nice that the results are hidden so that you are not overwhelmed having to go back and forth and drill down to see the bugs requires extra effort and is disruptive to his workflow.
other tools our participants had similar complaints about was coverity and lint for c c projects.
for ryan and tony the biggest downside to using coverity is that it is not capable of being integrated into their coding environment leading to a lot of clicking back and forth between their editor and the static analysis tool.
phil does not like using lint because of the fact that he has to go out of his way to do so.
some of our participants made it clear however that even if the tool is integrated with their development environment it is still possible that the tool does not integrate well into their development process .
for example one of our participants mike does not use ides so using a tool that integrates well with an ide does not fit well into his development process he likes using clang because it can be tied into his compiler whichdoes not require a development environment .
according to gordon one of the key problems with static analysis tools is that at times they can prevent him from being productive.
one way this can happen is when the tool slows the developer down by taking a long time to run which was a common complaint amongst our participants.
from jason s experience he believes that if it disrupts your flow you re not gonna use it.
jason s statement rings true among other participants as well like steve who has used various tools in his past but does not like to use findbugs because even though it is ide integrable it runs slow.
intellij which contains built in static analyzers utilizes idle time when reporting bugs in an attempt to prevent the problem of interrupting the developer s workflow but for matt it can still be bothersome.
jason believes that the problem with current static analysis tools is that they are not capable of running well on larger code bases leading to a break in his development flow as he waits for the tool to catch up.
in terms of workflow participants valued using static analysis both to fix bugs once they are introduced into the program but also later in the development process.
from a workflow standpoint it is valuable to fix potential bugs when they are entered into a program because the necessary context to understand the bug is already in the developers working memory.
in contrast fixing bugs later is difficult because a developer must recall the context to analyze the corresponding static analysis warning.
this contrast is similar to the difference between floss refactoring and root canal refactoring where the former involves restructuring code as it is being worked with and the latter involves refactoring by finding the worst code and dealing with that first .
root canal refactoring is a discouraged practice and its analog in static analysis finding the most severe static analysis warnings in a whole codebase and dealing with those first may also be a wasteful practice.
research has shown that many static analysis warnings in working systems do not actually manifest as program failures .
c. rq3 tool design our main goal in this research is to improve static analysis tools for developers.
the best way to do this is to find out how developers want their tool to be designed.
most of the proposed designs are for warning notification and manipulation or quick fix display.
participants made some other interesting proposals which will also be presented.
quick fix design.
ten of our participants made a suggestion related to the way in which a quick fix should be displayed.
most of our participants wanted to be able to preview the fix and how it is going to change their code before they apply it.
abby and tony recommended splitting the code editor to show a diff of the code using highlighting to show what code has changed or been added to their code.
on one side there would be the code now and on the other the code once the fix is applied.
some felt that you should be able to see the fix before applying it but then also manually apply it so that you know the fix is being applied without introducing any678new problems.
one participant mike prefers not to have quick fixes at all because he feels the error messages are enough to assess what to do about an error.
one interesting quick fix design idea which came from ryan during his interactive interview was to have what he called a three option dialog box available when applying a quick fix.
this dialog box would pop up upon a click to fix the bug and there would be three choices apply the entire fix default option do not apply the fix or step by step apply the solution allowing the developer to decide which parts of the solution they would like to keep.
static analysis tools like findbugs and intellij offer some quick fixes.
however they do not give a full context preview of the changes that will be made leaving it to the developer to manually ensure that the fix was applied correctly and to their liking.
warning notification and manipulation design.
all of our participants told us when and how they want to be notified of errors in their code.
the theme in this category is fast.
developers want tools that provide faster feedback in an efficient way that does not disrupt their workflows.
for some of our participants this meant running the tool in the background of the ide so that feedback occurs as soon as a problem is detected.
for other participants this meant running the tool at build time or compile time.
in this way the results are presented when the developer is at a stopping point.
.
overall our participants find that current static analysis tools are not fast enough when providing them with feedback this quickness should be accompanied with discretion as the developer does not want the tool to break their thought process.
our participants also thought it would be beneficial to have the ability to easily make judgements about defects such as setting it aside to view later save these judgements and share them with other developers.
many of our participants suggested that static analysis tools should allow developers to ignore specific defects and move them to their own list for later viewing a form of temporary suppression .
most tools if they allow the developer to ignore specific warnings only allow the developer to turn off or suppress a bug category for particular line of code using a comment like annotation which gordon told us makes the code smell .
developers would like to have the option to ignore each individual defect in case they either do not want to fix it and do not want to be bothered by it again or do not want to be bothered with it at that particular time but would like to come back to it later.
other design ideas.
our participants also came up with creative design ideas.
one participant chris suggested giving the editor plasticity .
when he is given a warning and would like to get more information the tool should move the code surrounding the warning to embed this information into the editor.
a couple of our participants thought it would be useful to have visual output possibly a pie style diagram of the project and the bugs in it instead of standard list and tree outputs to make it easier to go back and forth between warnings and code.
during frank s participatory design session he suggested a potential solution a parts to a whole corpus view of the project as a heat map .
the heat map would fig.
.
one of our participant matt s participatory design drawing a shows where matt wants the gradient colors and b shows the way his current tool represents severity.
use colors to show where the errors are and how severe the problems are.
it would start with an overall view of the project and as you drill down you can see the condition at each level to see where the most attention is needed.
this is similar to the concept behind khoo s toolkit path projection in that the toolkit is meant to visualize output that is usually if not always textual and difficult to understand .
an interesting suggestion made by a couple of our participants is to represent the severity of the defects using gradients of one color instead of multiple different colors the darker the color the more important or urgent the bug is.
figure depicts a drawing one of our participants matt drew during his participatory design he labeled the side of the editor gradient a where he would like to see his severity representation.
in the top right corner matt also lists the colors that his current tool uses b for example r means red.
the idea behind this is not new other studies have focused their attention on using colors for error representation .
d. threats to validity there are several threats to the validity of our study here we categorize each threat as a threat to external internal or construct validity.
external.
one limitation to the generalizability of our study is the sample size.
although we obtained valuable information from the interviews due to time constraints and busy developers they may not be representative of the larger population that use static analysis tools.
although we would have liked more participants having a large number of interviews to transcribe and code could lead to less accurate analysis.
the study conducted by layman et al.
which we discussed earlier as utilizing a similar methodology had a participant pool of similar size .
another possible threat679is that we only interviewed developers who have used static analysis tools.
in some cases it may be that static analysis tools are not being used for other reasons such as lack of awareness.
it should also be noted that some of our participants had experience building static analysis tools giving them somewhat of a biased opinion of the usage of these tools.
internal.
another threat to the validity of this study is the way in which we conducted remote interviews.
we did not thoroughly prepare for what we would do if the technology we wanted to use did not work or was not available.
therefore the interactive interview and participatory design in remote interviews had to be conducted differently than local interviews.
despite this there was still value in the results obtained from our remote participants they could still give useful insights from their previous experiences.
only of the interviews fell into this category so this helps limit the impact of this threat.
construct.
the objective for using the interactive interview was to get more accurate information on how developers use their tools.
one limitation here is that some developers were not as familiar with the code or environment they had to use in our interviews as they would be with their own code in their own development environment.
this could have caused some developers to take different actions than they would if they were in their own environment.
ideally it would have been better to have been able to observe our participants working in their own environment however for confidentiality reasons we were not allowed to view participants own proprietary code.
in an effort to compensate for this threat the open source projects and tool we chose are well known open source projects.
another threat to the validity our work is that we did not originally consider is that we may have said things in our consent form or session script that would give unintended hints to our participants concerning our research expectations.
one example of this is us outlining our research goals in the introductions we gave prior to beginning each session.
this could have led to what is called hypothesis guessing where participants respond to questions based on what they think the researcher wants to hear .
in retrospect we helped alleviate this threat in our interviews by asking our participants experience questions.
v. d iscussion a. implications our interviews have several implications for current and future static analysis tools.
current static analysis tools may not give enough information for developers to assess what to do about the warnings produced and very seldom offer a fix to what it claims is an issue.
if static analysis tools offered quick fixes giving a potential solution and applying it to the problem may help developers assess warnings more quickly and ultimately save time and effort.
our results indicate that findbugs for example would be more useful if it had more informative messages and offered quick fixes.
at the same time quick fixes do not appear to be a universally applicable mechanism to help developers resolve static analysis warnings because many static analysis warnings do not have a smallset of solutions.
for example findbugs warns developers when two method names in the same class differ only by capitalization no quick fix for this problem is likely to satisfy a developer.
instead interactive quick fixes that enable easy access to refactoring and code modification tools may be able to semi automatically help developers resolve static analysis warnings.
on the negative side quick fixes could also cause developers to be hasty in fixing their code which could potentially lead to more problems such as the introduction of new defects.
there are also challenges related to implementing usable interactive quick fixes.
we have not yet investigated what theses challenges are or how to address them as they are out of scope for this particular study.
developers like tools like intellij and findbugs because they have the ability to run without the developer telling it to however there is still the issue of giving the developer information they find useful.
one way to allow developers to focus on making judgments about defects is to treat each warning like a mylyn task where the program elements that are explored when making a judgement such as the assignments to a variable when judging a null pointer warning automatically populate a warning s task context.
in this way extraneous warnings and program elements not related to a warning under investigation can be automatically elided reducing distractions.
like mylyn task contexts such judgement contexts could also be saved and passed around between developers enabling knowledge about static analysis issues to be more easily shared.
developers may prefer a tool where the usage is tied into their normal workflow.
for example if a developer has to commit their code to a repository so many times a day they may be more likely to use a static analysis tool if it can be run each time they go to commit their code this way they do not have to go out of their way to use the tool.
developers may want also features such as the ability to modify existing warnings or rule sets or choosing how and when their tool runs.
most static analysis tools for finding bugs today offer some type of customization to the bugs it finds for example in findbugs and intellij it is fairly simple to turn off or suppress warnings for any of the categories of bugs that the tool finds.
if a tool is to be customizable it should be customizable in a way that is simple and useful to the developer.
findbugs allows developers to turn off certain bug detectors but only on a project level.
turning off a detector for a specific class or file has to be done manually in the code at each line you want ignored.
configurations that require this much effort may cause the developer to discontinue configuring the tool which could eventually lead to the developer discontinuing use of the tool.
vi.
f uture work the results from our study suggest that there are ways to make static analysis tools more useful to developers.
in the future it may be necessary to perform a follow up study that focuses on the adoption of static analysis tools to give a more holistic view of what factors developers consider680when choosing to use a static analysis tool.
we have begun to implement a static analysis tool prototype based on the results we obtained in this study.
one of the main features we plan to focus on are defect remediations as this seemed to be one of the most frequently mentioned requests made by our interviewees.
more specifically we are interested in implementing interactive quick fixes giving the developer more enhanced control over the automatic fix beyond what would normally be offered in a one shot quick fix.
we also plan to conduct a user study to evaluate our prototype with software developers with a range of experience.
vii.
c onclusion in this paper we investigated why developers do not widely use static analysis and how current tools could be improved to increase usage.
we conducted a user study involving software developers who have an average of about years of experience with using static analysis tools to find bugs.
we also discussed the implications of our results.
our results confirmed that false positives and developer overload play a part in developers dissatisfaction with current static analysis tools.
each of the factors presented in this paper should also be considered when implementing a tool that will lead to higher usage of static analysis tools for improving software code quality and maintaining coding standards.
future static analysis tools could improve adoption by software developers by enhancing support for team development while using static analysis tools improving integration of the tool into developers processes having intuitive defect presentation and detailed explanation of defects with automatic fixes where appropriate and including easy and useful configuration options for the tool.