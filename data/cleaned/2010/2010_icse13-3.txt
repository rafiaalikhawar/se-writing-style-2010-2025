boa a language and infrastructure for analyzing ultra large scale software repositories robert dyer hoan anh nguyen hridesh rajan and tien n. nguyen iowa state university usa rdyer hoan hridesh tien iastate.edu abstract in today s software centric world ultra large scale software repositories e.g.
sourceforge projects github projects and google code projects are the new library of alexandria.
they contain an enormous corpus of software and information about software.
scientists and engineers alike are interested in analyzing this wealth of information both for curiosity as well as for testing important hypotheses.
however systematic extraction of relevant data from these repositories and analysis of such data for testing hypotheses is hard and best left for mining software repository msr experts!
the goal of boa a domain specific language and infrastructure described here is to ease testing msr related hypotheses.
we have implemented boa and provide a web based interface to boa s infrastructure.
our evaluation demonstrates that boa substantially reduces programming efforts thus lowering the barrier to entry.
we also see drastic improvements in scalability.
last but not least reproducing an experiment conducted using boa is just a matter of re running small boa programs provided by previous researchers.
index terms mining software repository reproducible scalable ease of use lower barrier to entry i. i ntroduction ultra large scale software repositories e.g.
sourceforge projects github projects and google code projects contain an enormous collection of software and information about software.
assuming only a meagre 1k lines of code loc per project these big3 repositories amount to at least .
billion loc alone.
scientists and engineers alike are interested in analyzing this wealth of information both for curiosity as well as for testing such important hypotheses as how people perceive and consider the potential impacts of their own and others edits as they write together?
what is the most widely used open source license?
how many projects continue to use des considered insecure encryption standards?
how many open source projects have a restricted export control policy?
how many projects on an average start with an existing code base from another project instead of scratch?
how often do practitioners use dynamic features of javascript e.g.
eval ?
what is the average time to resolve a bug reported as critical?
.
however the current barrier to entry could be prohibitive.
for example to answer the questions above a research team would need to a develop expertise in programmatically accessing version control systems b establish an infrastructure for downloading and storing the data from software repositories since running experiments by directly accessingthis data is often time prohibitive c program an infrastructure in a full fledged programming language like c java c or python to access this local data and answer the hypothesis and d improve the scalability of the analysis infrastructure to be able to process ultra large scale data in a reasonable time.
these four requirements substantially increase the cost of scientific research.
there are four additional problems.
first experiments are often unreproducible because replicating an experimental setup requires a mammoth effort.
second reusability of experimental infrastructure is typically low because analysis infrastructure is not designed in a reusable manner.
after all the focus of the original researcher is on the result of the analysis and not on reusability of the analysis infrastructure.
thus researchers commonly have to replicate each other s efforts.
third data associated and produced by such experiments is often lost and becomes inaccessible and obsolete because there is no systematic curation.
last but not least building analysis infrastructure to process ultra largescale data efficiently can be very hard .
to solve these problems we have designed a domainspecific programming language for analyzing ultra large scale software repositories which we call boa.
in a nutshell boa aims to be for open source related research what mathematica is to numerical computing r is for statistical computing and verilog and vhdl is for hardware description.
we have implemented boa and provide a web based interface to boa s infrastructure .
to evaluate boa s design and effectiveness of its infrastructure we have written programs to answer different research questions in four different categories questions related to the use of programming languages project management legal and those that relate to platform environment.
our results show that boasubstantially decreases the efforts of researchers analyzing human and technical aspects of open source software development allowing them to focus on their essential tasks.
we also see ease of use substantial improvements in scalability and lower complexity and size of analysis programs see figure .
last but not least reproducing an experiment conducted using boais just a matter of re running often small boa programs provided by previous researchers.
we now describe boa and explore its advantages.
first we present the language section ii and describe its infrastructure sections iii iv .
section v presents studies of applicability scalability and reproducibility.
section vi positions our work in the broader research area and section vii concludes.
c ieee icse san francisco ca usa accepted for publication by ieee.
c ieee.
personal use of this material is permitted.
permission from ieee must be obtained for all other uses in any current or future media including reprinting republishing this material for advertising or promotional purposes creating new collective works for resale or redistribution to servers or lists or reuse of any copyrighted component of this work in other works.422java ... imports 9public class getchurnrates public static void main string args new getchurnrates .getrates args public void getrates string cachepath for file file file fileio.readobjectfromfile cachepath string url getsvnurl file if url !
null !url .isempty system.out.println url getchurnrateforproject url private double getchurnrateforproject string url double rate svnurl svnurl ... connect to svn and compute churn rate return rate private string getsvnurl file file string jsontxt ... read the file contents into jsontxt jsonobject json null jsonproj null ... parse the text get the project data if !
jsonproj.has programming languages return if !
jsonproj.has svnrepository return boolean hasjava false ... is the project a java project?
if !
hasjava return jsonobject svnrep jsonproj.getjsonobject svnrepository if !
svnrep.has location return return svnrep.getstring location boa 1rates output mean ofint 2p project input 3foreach i int p.code repositories .
kind repositorykind.svn len p. code repositories .revisions 4exists j int match java lowercase p.programming languages foreach k int len p.code repositories .
revisions .
files rates len p.code repositories .
revisions .
files boa sadvantages easy to use simple programs as small as lines better abstractions hides specifics of mining software repositories efficient and scalable results in as little as one minute enhances reproducibility researchers can publish their small boa programs and the dataset used so that others may reproduce the results performance results 304262javaboa input size number of projects time sec 168x speedup fig.
.
programs for answering what are the churn rates for all java projects that use svn?
and performance results on three input sizes.
ii.
m otivation creating experimental infrastructure to analyze the wealth of information available in open source repositories is difficult .
creating an infrastructure that scales well is even harder .
to illustrate consider a question such as what are the average numbers of changed files per revision churn rates for all java projects that use svn?
answering this question would require knowledge of at a minimum reading project metadata and mining code repository locations how to access those code repositories additional filtering code controller logic etc.
writing such a program in java for example would take upwards of lines of code and require knowledge of at least complex libraries.
a heavily elided example of such a program is shown in figure left column.
this program assumes that the user has manually downloaded all project metadata available as json files and svn repositories from sourceforge.
it then processes the data using a json library and collects a list of subversion urls.
a svn library is then used to connect to each cached repository in that list and calculate the churn rate for the project.
notice that this code required use of complex external libraries in addition to standard java classes and resulted in almost lines of code.
it is also sequential so it will not scale as the data size grows.
one could write a concurrent version but this would add additional complexity.a.
boa enabling data intensive open source research we designed and implemented a domain specific programming language that we call boa to solve these problems.
boa aims to lower the barrier to entry and thus enable a larger more ambitious line of data intensive scientific discovery in open source software development related research.
the main features of boa are inspired from existing languages for dataintensive computing .
to these we add builtin types that are specifically designed to ease analysis tasks common in open source software mining research.
to illustrate the features of boa consider the same question what are the churn rates for all java projects that use svn?
.
aboa program to answer this question is shown in figure right column.
on line this program declares an output called rates which collects integer values and produces a final result by aggregating the input values for each project indexed by a string using the function mean .
on line it declares that the input to this program will be a project e.g.
apache openoffice.
boa s infrastructure manages the details of downloading projects and their associated information.
for each project the code on lines runs.
if a repository contains 600k projects the code on lines runs for each.
on line this program says to run code on lines for each of the input project s code repositories that are subversion and contain more than revisions to filter out423boa s data infrastructure svn local cache protocolbuffers boa language provided by sawzall mapreduce compile execute on hadoop cluster sf.net deploy replicator caching translator query program query plan query result boa s compiler provided by sizzle mapreduce domain specific types protocolbuffer input reader quantifiers runtime user functions quantifiers user functions domain specific types fig.
.
an overview of boa s infrastructure.
new components are marked with green boxes and bold text.
new or abandoned projects .
on line this program says to run code on lines if and only if for the input project at least one of the programming languages used is java.
line selects only revisions from such repositories that have less than files changed to filter out extremely large commits such as the first commit of a project .
finally on line this program says to send the length of the array that contains the changed files in the revision to the aggregator rates indexed by the project s unique identifier string.
this aggregator produces the final answer to our question.
these lines of code not only answer the question of interest but run on a distributed cluster potentially saving hours of execution time.
note that writing this small program required no intimate knowledge of how to find access the project metadata how to access the repository information or any mention of parallelization.
all of these concepts are abstracted from the user providing instead simple primitives such as the project type which contains attributes related to software projects such as the name programming languages used repository locations etc.
these abstractions substantially ease common analysis tasks.
since this program runs on a cluster it also scales extremely well compared to the sequential version written in java.
the time taken to run this program on varying input sizes is shown in the lower right of figure .
note that the y axis is in logarithmic scale.
the time to execute the java program increases roughly linearly with the size of the input while the boa program sees minimal increase in execution time.
we have built an infrastructure for the boa programming language.
an overview of this infrastructure is presented in figure .
components are shown inside dotted boxes on the left the flow of a boa program is shown in the middle and the input data sources are shown on the right.
the three main components are the boalanguage compiler and runtime and supporting data infrastructure.
first an analysis task is phrased as a boa program e.g.
that in figure see section iii .
this program is fed to our compiler seesection iv a via our web based interface see section iv c .
the boa compiler produces a query plan.
our infrastructure then deploys this query plan onto a hadoop cluster where it executes.
the cluster makes use of a locally cached copy of the source code repositories see section iv b and based on the query plan creates tasks to produce the final query result.
this is the answer to the user s analysis task.
we now describe these components in detail.
iii.
d esign of the boa language the top left portion of figure shows the main features of the boa language.
we have four main kinds of features at the moment domain specific types to ease analysis of open source software repository mining mapreduce support for scalable analysis of ultra large scale repositories quantifiers for easily expressing loops and the ability to define functions.
a. domain specific types in boa the boa language provides several domain specific types for mining software repositories.
figure gives an overview of these types.
each type provides several attributes that can be thought of as read only fields.
type attributes project id name created date code repositories .
.
.
repository url kind revisions revision id log committer commit date files person username real name email file name kind fig.
.
some of the domain specific types provided in boa.
theproject type provides metadata about an open source project in the repository including its name url some descriptions who maintains and develops it and any code repository.
this type is used as input to programs in the boa language.
therepository type provides all of the revision s committed into that repository.
a revision represents a group of artifact changes and provides relevant information such as the revision id commit log and time the person who committed the revision and the file s committed.
b. mapreduce support in boa in mapreduce frameworks computations are specified via two user defined functions a mapper that takes key value pairs as input and produces key value pairs as output and a reducer that consumes those key value pairs and aggregates data based on individual keys.
syntactically boa is reminiscent of sawzall a language designed for analyzing log files.
in boa like sawzall users write the mapper functions directly and use built in aggregators as the reduce function.
users declare output tables process the input and then send values to the tables.
output declarations specify aggregation functions and the language provides several built in aggregators such as summing min max mean etc.
for example we could write an output declaration for the table rates as shown in figure line .
for this table we want to index it by string s and give it values of type int.
we would also like to use the aggregation function424mean which produces the mean of each integer emitted to the aggregator.
thus the final result of our output table is a list of string keys each of which has the mean of all integers indexed by that key.
the plan generated from this code creates one logical process for each project in the corpus.
each process then analyzes a single project s revisions emitting to the project s table the number of changed files for each revision.
the aggregation process then reduces the values sent to it and computes the means.
c. quantifiers in boa boa defines he quantifiers exists foreach and ifall .
their semantics is the similar to when statements with quantifiers as in sawzall.
quantifiers represent an extremely useful sugar that appears frequently in mining tasks.
the sugared form makes programs much easier to write and comprehend.
for example the foreach quantifier on line of figure is a syntactic sugar for a loop.
the statement says each time when the boolean condition after the semicolon evaluates to true execute the code on lines .
the exists quantifier on line is similar however the code on lines should execute exactly once if there exists some non deterministically selected value of jwhere the boolean condition holds.
not shown is the ifall quantifier.
this quantifier states the boolean condition must hold for all values.
if this is the case then the associated code executes exactly once.
d. user defined functions in boa the boa language provides the ability for users to write their own functions directly in the language.
to ease certain common mining tasks we added built in functions.
since we can t anticipate all needs of the users or since our choice of a particular algorithm may not match what the user needs having the ability to add user defined functions was important.
the syntax as inspired by sawzall requires declaring the parameters for the function and return type and assigning it to a variable.
functions can be passed as a parameter to other functions or assigned to different variables if the function types are identical .
a concrete example of a user defined function hasjavafile is shown later in figure .
iv.boa ssupporting infrastructure the bottom left portion of figure shows the various parts of the boa compiler and runtime.
a. compiler and runtime for our initial implementation we started with code for the sizzle compiler and framework.
sizzle is an opensource java implementation of the sawzall language.
unlike the original sawzall compiler sizzle provides support for generating programs that run on the hadoop open source mapreduce framework.
our main implementation efforts were in adding userdefined functions in the boa compiler adding support forquantifiers and supporting the protocol buffer format as input.
these efforts were in addition to adding support for our domain specific types and custom runtime model.
user defined functions the initial code generation strategy for user functions uses a pattern similar to the java runnable interface.
a generic interface is provided by the runtime which requires specifying the return type of the function as a type argument.
each user defined function then has an anonymous class generated which implements this interface and provides the body of the function as the body of the interface s invoke method.
this strategy allows easily modeling the semantics of user defined functions including being able to pass them as arguments to other functions and assigning them to similarly typed variables.
quantifiers we modified the compiler to desugar quantifiers into for loops.
this process requires the compiler to analyze the boolean conditions to automatically infer valid ranges for the loop.
the range is determined based on the boolean condition s use of the declared quantifier variable.
currently quantifiers must be used as indexers to array attributes in our custom types and the range of the loop is the length of the array.
we plan to extend support to any array variable in the future.
protocol buffers protocol buffers are a data description format developed by google that are stored as binary messages.
this format was designed to be compact and relatively fast to parse compared to other formats such as xml.
messages are defined using a struct like syntax and a compiler is provided which generates java classes to read and write messages in that format.
the boa compiler was modified to use these generated classes when generating code by mapping them to the domain specific types provided.
the boa compiler accepts hadoop sequencefile s as input which is a special file format similar to a map.
it stores key value pairs where the key is the project and the value is the binary representation of the protocol buffer message containing that project s data.
this format was chosen due to its ease in splitting the input across map tasks.
b. data infrastructure while the semantic model we provide with the boa language and infrastructure states that queries are performed against the source repository in its current state actually performing such queries over the internet on the live dataset would be prohibitive.
instead we locally cache the repository information on our cluster and provide monthly snapshots of the data.
the right portion of figure shows the components and steps required for this caching.
the first step is to locally replicate the data.
for sourceforge there are public apis we make use of.
the first is a json api that provides information about projects including various metadata on the project and information about which repositories the project contains.
we simply download and cache the json objects for each project.
the second api is the public subversion svn urls for code repositories.
we make use of a java svn library to locally clone these repositories.425loc rtime sec task java boa diff java boa speedup a. programming languages .
what are the ten most used programming languages?
15x 10x .
how many projects use more than one programming language?
8x 11x .
in which year was java added to svn projects the most?
9x 171x b. project management .
how many projects are created each year?
14x 16x .
how many projects self classify into each topic provided by sourceforge?
11x 12x .
how many java projects using svn were active in ?
11x 90x .
in which year was svn added to java projects the most?
18x 13x .
how many revisions are there in all java projects using svn?
12x 79x .
how many revisions fix bugs in all java projects using svn?
13x 239x .
how many committers are there for each java project using svn?
12x 216x .
how many java projects using svn does each committer work on?
18x 180x .
what are the churn rates for all java projects that use svn?
14x 168x .
how did the no.
of commits for java projects using svn change over years?
13x 244x .
for all java projects using svn what is the distribution of commit log length?
14x 239x c. legal .
what are the five most used licenses?
16x 11x .
how many projects use more than one license?
8x 9x d. platform environment .
what are the five most supported operating systems?
15x 8x .
what are the projects that support multiple operating systems?
8x 15x .
what are the five most popular databases?
15x 11x .
what are the projects that support multiple databases?
8x 9x .
how often is each database used in each programming language?
14x 12x fig.
.
several example mining tasks with lines of code and execution times in seconds for both java and boa programs solving the tasks.
once the information is stored locally on our cluster we run our caching translator to convert the data into the format required by our framework.
the input to the translator is the json files and svn repositories and the output is a hadoop sequencefile containing protocol buffer messages which store all the relevant data.
c. web based interface we provide a web based interface for submitting boa programs compiling and running those programs on our cluster and obtaining the output from those programs.
users submit programs to the interface using our syntax highlighting text editor.
each submission creates a job in the system so the user can see the status of the compilation and execution request the results if available and resubmit or delete the job.
a daemon running on the cluster identifies jobs needing compiled and submits the code to the compiler framework.
if the source compiles successfully then the resulting jar file is deployed on our hadoop cluster and the program executes.
if the program finishes without error the resulting output is made available to the user to download as a text file .
v. e valuation this section presents our empirical evaluation on the scalability and the usefulness of our language and infrastructure.
the dataset used in this section contains all metadata about all sourceforge projects 620k and subversion repository metadata for only the java projects that use subversion 23k .
programs were executed on a standard hadoop .
.
install with name node job tracker node and compute 1this includes user projects which aren t listed.nodes.
the cluster was not tuned for performance except for setting the maximum number of map tasks for each compute node equal to the number of cores on that node and increasing the vm heap size to use the available memory on each node.
a. applicability our main claim is that boa is applicable for researchers wishing to analyze ultra large scale software repositories.
in this section we investigate this claim.
research question does boa help researchers analyze ultra large scale software repositories?
to answer this question we examined a set of tasks see figure that cover a range of different categories.
for each task we implemented a boaprogram to solve the task.
we also implemented small java programs to solve the same tasks.
the java programs were written by an expert in mining software repositories and then reviewed by a second person who is an expert in programming languages.
the second person performed a code review and also simplified and condensed the programs to decrease the total lines of code as much as reasonably possible without impacting performance.
this process substantially reduced almost by half the lines of code for the java versions.
the java programs were not written as hadoop programs.
writing the programs in hadoop would have added substantial additional complexity and lines of code to these programs.
we were interested in investigating how boa helps researchers along three directions are programs easier to write do those programs take substantially less time to collect the data and is the language expressive enough to solve such tasks.
for each task we collected two metrics lines of code loc the amount of code written running time rtime the time to collect the data all results are shown in figure .
the lines of code give an indication of how much effort was required to solve the tasks using each approach.
for java the tasks required writing lines of code and on average required lines of code.
performing the same tasks in boa required at most lines of code and on average less than lines of code.
thus there were times fewer lines of code when using boa.
not shown in the table was the fact the java programs also required using several libraries for accessing svn parsing json data etc .
the boaprograms abstracted away the details of how to mine the data and thus the user was not required to use these additional complex libraries.
the table also lists the time required to run each program and collect the desired data for the tasks.
note the java programs accessed all json and svn data from a local cache and the times do not include any network access.
for the java programs there are three distinct groups of running times.
the smallest times a. a. b. b. and all of c and d are tasks that only require parsing the project metadata and did not access any svn data.
the medium times a. b. b. and b. accessed the svn repositories but only required mining one or very few revisions.
the largest times b. b. all accessed the svn repositories and mined most of the revisions to answer the task and thus required substantially more time.
note that for the boa programs all tasks finish on average in seconds regardless of the type of task.
we see minimum speedups of times but in the best case the boa program solves the task almost times faster!
task java cached java remote svns boa speedup a.
117x b.
459x b.
390x b.
303x b.
120x b.
705x b.
651x b.
426x b.
346x b.
006x fig.
.
time in seconds if java tasks do not cache svn repositories first.
while the times in figure utilize local caches for all data including svn repositories researchers implementing such tasks might not first cache the svn data.
as such we again present the times for all tasks that access svn in figure with the difference being the java programs now access the svn repositories remotely.
compared to this strategy boa programs run over times faster!
detailed examples figures show four interesting boaprograms used to solve some of the tasks.
these programs highlight several useful features of the language.
figure answers task a. and demonstrates the use of a user defined functions.
the function hasjavafile line takes a single revision as argument and determines if it 2ignores comments and blank lines.
output sum ofint 2p project input 4hasjavafile function rev revision bool 5exists i int match .java rev.
files .
name return true 7return false 10foreach i int def p.code repositories exists j int hasjavafile p.code repositories .
revisions counts .
revisions .
commit date fig.
.
task a. querying years when java files were first added the most.
contains any files with the extension .java .
if the revision contains at least one such file it returns true .
this function is used in the when statement line as the boolean condition.
1counts output sum ofint 2p project input 4exists i int match java lowercase p.programming languages 5foreach j int p.code repositories .
url .kind repositorykind.svn foreach k int isfixingrevision p.code repositories .
revisions .
log counts fig.
.
task b. querying number of bug fixing revisions in java projects using svn.
figure answers task b. and makes use of the built in function isfixingrevision line .
the function uses a list of regular expressions to match against the revision s log.
if there is a match then the function returns true indicating the log most likely was for a revision fixing a bug.
1counts output top ofstring weight int 2p project input 4foreach i int def p.licenses 5counts p.licenses weight fig.
.
task c. querying the five most used licenses.
figure answers task c. and makes use of a top aggregator line .
the emit statement line now takes additional arguments giving a weight for the value being emitted.
the top aggregator then selects the top n results that have the highest total weight and gives those as output.
1counts output sum ofint 2p project input 4foreach i int def p.programming languages 5foreach j int def p.databases counts fig.
.
task d. querying pairs of how often each database is used in each programming language.
figure answers task d. and makes use of a multidimensional aggregator line to output pairs of results.
again the emit statement line is modified.
this time the statement requires providing multiple indexes for the table.427javac phpcpythonc javascriptperlunix shelldelphi kylix010 811number of projectsfig.
.
task a. popularity of programming languages on sourceforge.
results analysis we also show some interesting and potentially useful results from four of the tasks.
for example figure shows the results of task a. and charts the ten most used programming languages on sourceforge.
of the languages appear in the top of the tiobe index .
languages such as visual basic did not appear in our results despite being on the tiobe index.
this demonstrates that while the language is popular in general it is not popular in open source.
similarly objective c did not appear in our results as most programs written in objective c are for ios and are most likely commercial closed source programs or not typically hosted on sourceforge.
projects commiters number of committersnumber of projects fig.
.
task b. number of committers in each java project using svn.
note y axis is in logarithmic scale.
the results of task b. are shown in figure .
note that the y axis is in logarithmic scale.
these results show that a large number of open source projects have only a single committer.
generally open source projects are small and have very few committers and thus problems affecting large development teams may not show when analyzing open source software.
task b. looks at this data from the other angle.
figure shows the number of projects each unique committer works on.
again the vast majority of open source developers only work on a single project.
only about of committers work on more than three projects!
number of projectsnumber of committersfig.
.
task b. no.
of java projects each svn committer works on.
note y axis is in logarithmic scale.
or more fig.
.
task b. no.
of words in svn commit logs for java projects.
another interesting result came from task b. and is shown in figure .
this task examines how many words appear in log messages.
first around of all log messages were completely empty.
we do not investigate the reason for this phenomenom but simply point out how prevalent it is.
second over two thirds of the messages contained words which is less than the average length of a sentence in english.
a normal length sentence in english is words according to various results in google and thus we see that very few logs contained descriptive messages.
b. scalability one of our claims is that our approach is scalable.
we investigate this claim in terms of scaling the size of the cluster and scaling the size of the input.
research question does our approach scale to the size of the cluster?
to answer this question we run each of the sample programs listed in figures using our sourceforge.net dataset.
we fix the size of the input to 620k projects and vary the number of available map slots in the system from .
figure shows the results of this analysis where each group represents one of the sample programs the y axis is the total time taken in seconds to run the program and the x axis is the number of available map slots in the cluster.
each value is the average of executions.428a.1a.2a.3b.1b.2b.3b.4b.5b.6b.7b.8b.9b.10b.11c.1c.2d.1d.2d.3d.4d.
java boa number of projects 6k 60k 620k total time seconds fig.
.
scalability of input.
y axis is total time taken.
x axis is the size of the input in number of projects.
note y axis is in logarithmic scale.
task a. task b. task c. task d. map2 maps4 maps8 maps16 maps32 mapsexecution time seconds fig.
.
scalability of sample programs.
y axis is total time taken.
x axis is the number of available map slots in the cluster.
as one might expect the hadoop framework works well with this large dataset.
as the maximum number of map slots increases we see substantial decreases in execution time as more parallel map slots are being utilized.
note that with our current input size of 620k projects the maximum number of map slots needed is .
thus we don t generally see any benefit when increasing the maximum map slots past that.
as we increase the size of our input however we would expect to see differences in these data points indicating scaling past map slots.
research question does our approach scale with the size of the input?
to answer this question we fix the number of compute nodes to with a total of map slots available and then vary the size of the input 6k 60k and 620k projects .
the results for all tasks in figure are shown in figure .
wecompare against the programs written in java to answer the same questions.
all programs access only locally cached data.
note that the y axis is in logarithmic scale.
for the smallest input size 6k on certain tasks the java program runs in around seconds while the boa program runs in seconds.
at this size boa only uses one map task and thus the overhead of hadoop dominates the execution time.
for the larger input sizes boa always runs in substantially less time than the java version.
the results also show that the hand written java programs do not scale based on input size.
as the input size increases the running time for the java programs also increases roughly linearly .
the boa programs however demonstrate scalability.
for the two smallest input sizes the boa programs take roughly the same amount of time.
for the largest input size the boa programs despite having to process an input times larger than the smallest input size only take around twice as long.
this shows that the boa infrastructure scales well as the input size increases.
note that with our current maximum input size boa only utilizes out of map slots and thus we expect more scalability should the input size increase further.
c. reproducibility one important claim we make is that if researchers publish results obtained from our infrastructure other researchers can easily reproduce the same results.
research question using our infrastructure can researchers easily reproduce previously published results?
to answer this question we performed a small controlled experiment.
we selected a group of researchers graduate student and post doc who are experts in software mining and graduate and undergraduate students who are not experts.
each student was given a short tutorial on how to use our infrastructure as well as the location of boa source code for42918 tasks.3this source code represents what a researcher would publish in their paper along with the dataset they used.
for each of the tasks results files were provided.
this represents the data the previous researchers produced.
each student chose tasks they were interested in reproducing and were given a maximum of hour per task.
we measured the length of time required to reproduce each task as well as the number of tries in case they failed to reproduce the results .
intro task task task expert education time task time task time task time yes post doc b. b. b. yes phd a. b. b. no phd b. b. b. no phd a. b. d. no ms a. b. d. no ms b. c. d. no ms a. b. b. no bs a. d. d. fig.
.
study results.
all times given in minutes.
the results are given in figure and clearly show that all students were able to reproduce the previously published results in substantially less than one hour.
note that all students were also able to reproduce the results on their first try.
thus we assert that using only previously published source code and which dataset was used other researchers are able to easily reproduce the results.
vi.
r elated work despite the popularity of mining software repositories msr only a few research groups have attempted to address the problem of mining large scale software repositories.
in this section we discuss some of these efforts and programming languages similar to boa.
a. mining software repositories bevan et al.
proposed a centralized approach in which they define database schemas for metadata and source code in software repositories and such data is downloaded into a centralized database called kenyon .
the data can be accessed from kenyon via sql commands with their predefined data schemas.
unlike our infrastructure which is aimed to support ultra large data in software repositories kenyon was not designed for ultra large data with hundred thousands of projects and billions lines of code.
additionally our language and infrastructure can easily support new metadata from repositories as a newly defined type in the language.
in boetticher menzies and ostrand introduced the promise repository an online data repository for empirical software engineering data mainly for defect prediction research.
they make the repository publicly available and encourage the authors of research papers on defect prediction to upload data.
the data in promise are the post processed data i.e.
the data that were already processed to be suited with each individual research problem in each research paper.
3at the start of the study we only had tasks a. b. and b. missing .
for consistency all participants used the same set of tasks.for example the authors of a new bug prediction model using weka as their machine learning tool would upload the data files in weka format.
this hinders the applicability and usability of the data if other researchers would like to use the original data for a different tool set a different approach or even a different problem.
promise data is also limited to defect prediction.
additionally since the data is uploaded for individual research promise potentially contains duplicate data and inconsistencies.
sourcerer provides an sql database of metadata and source code on over 18k projects.
queries are performed using standard sql statements.
thus their approach easily supports joins on the data where ours does not.
however being built on mapreduce allows easier scalability for our approach.
their approach also does not contain history information revisions .
supporting for the reproducibility of research papers published in the msr area gregorio robles and his team advocated for the construction of open access data repositories for msr research.
their goal was to build a web page with the additional information most desirably a sourceforge like site that acts as a repository for this type of data and tools and that frees researchers from maintaining infrastructure and links .
their vision is similar to promise but with more general types of data.
we focus more on the raw data of opensource projects that can be utilized in any msr research.
aiming to improve the scalability and speed of msr tasks hassan et al.
and gabel et al.
use parallel algorithms and infrastructures.
they have shown that using map reduce and other parallel computing infrastructure could achieve that goal.
in comparison they focus only on specific mining tasks e.g.
finding uniqueness and cloned code while our infrastructure supports a wide range of mining tasks.
additionally the details of using map reduce are not exposed to the programmers when using boa.
b. programming languages martin et al.
define a program query language pql to allow easily analyzing source code.
their language models programs as certain events such as the call or return of a method or reading writing a field and allow users to write query patterns to match sub sequences of these events.
to match pql performs a static analysis that is flow sensitive and performs a pointer analysis to determine all possible matches to the query.
it also provides an online checker that instruments the program and dynamically matches.
each instance of pql however is limited to matching against a single program and has a limited set of events provided by the language.
our approach is designed to perform queries efficiently against a large corpus of data instead of single programs.
dean and ghemawat proposed a computing paradigm called mapreduce in which users easily process large amounts of data in a highly parallel fashion by providing functions for filtering and grouping data called mappers and additional functions for aggregating the output called reducers .
programs that are heavily data parallel and written in mapreduce can be executed in parallel on large clusters without the user430worrying about explicitly writing parallel code.
over the years a large number of languages that directly or indirectly support mapreduce or mapreduce like paradigms were proposed.
here we discuss some of these languages.
sawzall is a language developed at google to ease processing of large datasets particularly logfiles.
the language is intended to run on top of google s distributed filesystem and map reduce framework allowing users to write queries against or process large amounts of log data.
our framework while syntactically similar to sawzall provides several key benefits.
first we provide domain specific types to ease the writing of software mining tasks.
these types represent a lot of cached data and provide convenient ways to access this data without having to know specifics about how to access code repositories or parse the data contained in them.
second our framework runs on hadoop clusters whereas sawzall only runs on a single machine or on google s proprietary map reduce framework.
apache pig latin aims to provide both a procedural style map reduce framework as well as a more higher level declarative style language somewhat similar to standard sql.
unlike pure map reduce frameworks or implementations such as sawzall pig latin provides the ability to easily perform joins on large datasets.
the language was also designed to ease the framework s ability to optimize queries.
since our approach is based on sawzall we do not directly provide support for joins.
unlike boa however pig latin does not directly provide support for software mining tasks.
dryad is a framework to allow parallel processing of large scale data.
dryad programs are expressed as directed acyclic graphs and thus are more general than standard mapreduce.
a high level procedural language dryadlinq is provided that compiles down to dryad.
this language is based on .net s language integrated query linq and provides a syntax somewhat similar to a procedural version of sql and thus is relatively similar to pig latin.
also similar to pig latin dryad does not directly aim to support easing software mining tasks.
microsoft no longer supports dryad dryadlinq.
vii.
f uture work and conclusion ultra large scale software repositories contain an enormous corpus of software and information about that software.
scientists and engineers alike are interested in analyzing this wealth of information however systematic extraction of relevant data from these repositories and analysis of such data for testing hypotheses is difficult.
in this work we present boa a domainspecific language and infrastructure to ease testing msrrelated hypotheses.
we implemented boa and provide a webbased interface to boa s infrastructure.
our evaluation demonstrated that boa substantially reduces programming efforts thus lowering the barrier to entry.
boa also shows drastic improvements in scalability without requiring programmers to explicitly parallelize code.
we also demonstrate that experiments conducted using boa are easily reproduced simply by re running boaprograms provided by the previous researchers.
in the future we plan to support additional version control systems and source repositories.
a key challenge in thisprocess will be to reconcile terminological differences between these systems to be able to provide a unified interface.
acknowledgment this work was supported in part by nsf grants ccf ccf ccf and cns .