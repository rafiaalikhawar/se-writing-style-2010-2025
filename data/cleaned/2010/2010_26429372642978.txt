mil testing of highly configurable continuous controllers scalable search using surrogate models reza matinnejad shiva nejati lionel c. briand snt centre university of luxembourg luxembourg reza.matinnejad shiva nejati lionel.briand uni.luthomas bruckmann delphi automotive systems luxembourg thomas.bruckmann delphi.com abstract continuous controllers have been widely used in automotive domain to monitor and control physical components.
these controllers are subject to three rounds of testing model in the loop mil software in the loop and hardware in the loop.
in our earlier work we used meta heuristic search to automate mil testing of fixed configurations of continuous controllers.
in this paper we extend our work to support mil testing of all feasible configurations of continuous controllers.
specifically we use a combination of dimensionality reduction and surrogate modeling techniques to scale our earlier mil testing approach to large multi dimensional input spaces formed by configuration parameters.
we evaluated our approach by applying it to a complex industrial continuous controller.
our experiment shows that our approach identifies test cases indicating requirements violations.
further we demonstrate that dimensionally reduction helps generate surrogate models with higher prediction accuracy.
finally we show that combining our search algorithm with surrogate modelling improves its efficiency for two out of three requirements.
categories and subject descriptors software program verification keywords search based testing continuous controllers automotive software dimensionality reduction supervised learning.
.
introduction embedded software systems are pervasive in the electronics system industry e.g.
automotive.
many embedded software systems are partly generated from mathematical models that describe how devices are monitored controlled or regulated .
a well known category of such models is closed loop continuous controllers which are widely used in industrial control systems and are designed using differential equations over continuous time.
these controller models are typically specified in mathematical modeling environments most notably simulink that support automatic transformation of models to source code.
the generated code is then integrated with other necessary software components and deployed on embedded devices.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september vasteras sweden.
copyright acm ... .
.
identify early design errors of continuous controllers engineers create a model of the environment capturing the behavior of the device that interacts with a controller and perform testing and simulations of the controller and the environment models.
this stage of testing is known as model in the loop mil testing and is performed in various embedded system sectors such as the automotive domain.
the subsequent stages of controller development are referred to as software in the loop sil where the integrated software system is developed and tested and hardwarein the loop hil where the software deployed on the embedded device is being tested using a real time simulator.
compared to sil and hil the development and testing at mil level are considerably faster as the engineers can quickly modify the controller model and immediately test the system.
in addition mil testing is much less expensive than sil or hil testing.
in this paper we focus on mil testing of continuous controllers specified as mathematical models in simulink.
it is important to note that more than half of the controllers used in industry are continuous controllers .
continuous controllers are used to capture controllers of low level devices e.g.
controlling the velocity of a dc motor or the position of a flap.
modeling such controllers using more conventional software engineering notations such as state machines results in trivial discrete models where most important details are abstracted away and hence cannot be tested.
in control theory continuous controllers are specified using differential equations known as proportional integral derivative pid .
these equations include time continuous variables as well as parameters that are not time dependent and are fixed for every controller configuration.
these parameters are referred to as calibration orconfiguration parameters and optimize the behavior of a particular controller configuration for specific hardware.
many existing approaches to testing embedded software systems focus on analyzing discrete or mixed discrete continuous systems .
these techniques however are not amenable to analyzing controllers of low level devices with a trivial discrete behavior.
some recent work has concentrated on using search based algorithms to develop automated and systematic testing techniques for embedded software systems .
among these our earlier work particularly focuses on testing fixed configurations of continuous controllers where only time dependent variables of controllers are included in the test input data.
in this work we present an approach for testing continuous controllers while accounting for their feasible configurations.
specifically we develop a search based technique to generate test cases i.e.
worst case scenarios attempting to violate controller requirements.
our search strategy traverses individual points in the input search space of the controller and is guided by an objective function defined based on the output of the controller simulation for each point in the input space.
the input space of the controller in our previous work was made of time dependent variables only.
in this paper we extend the search space to include configuration parameters as well as the time dependent variables allowing us to test controllers for different hardware configurations at the mil level.
however the expanded search space becomes so large that our previous approach can no longer scale to identify worst case scenarios.
that is to handle multiple controller configurations we cannot merely expand the input search space used in our earlier work.
instead we have to build strategies to scale the search to large multi dimensional spaces and to reduce the cost of computing objective functions for individual points in the search space.
in this paper we extend our previous work to support mil testing of all feasible configurations of continuous controllers.
specifically we use a combination of dimensionality reduction and surrogate modeling techniques based on supervised learning to scale our search to large multidimensional spaces.
given an objective function we first use dimensionality reduction techniques to identify the input variables that do not have a significant impact on the output of the objective functions i.e.
varying the values of those variables does not cause a significant change in the objective function output.
we then apply an explorative random search and focus the explorative search only on significant variables.
using the exploration results we select some partitions of the input space that are more likely to include worst case input scenarios.
we then apply a single state search to the selected partitions to identify worst case scenarios in each partition.
our objective functions require us to simulate simulink models and are computationally expensive.
therefore for each objective function and for each partition we use the exploration results to build a surrogate model based on supervised learning techniques .
the surrogate model is faster to compute than the objective function and is able to predict its output within some confidence interval.
our single state search uses the surrogate model to predict the output of the objective function when the decision as to which point the search should move to can be made based on the surrogate model.
we evaluated our approach by applying it to a complex industrial controller consisting of simulink blocks from the automotive domain.
our experiment showed that applying dimensionality reduction prior to exploration helps generate more accurate and predictive surrogate models for two out of three requirements.
in addition combining single state search with surrogate modeling remarkably improves our approach for the same two requirements.
specifically for one requirement the search combined with surrogate modeling is eight times faster than the search without surrogate modeling and for the other requirement the search with surrogate modelling computes higher output values that could not be computed by the search without surrogate modeling.
finally our approach identified critical violations of the controller requirements that had been found neither by our earlier work nor by manual testing based on domain expertise.
.
background and motiv ation in this section we discuss mil testing of continuous controllers and motivate our work based on the needs of the automotive domain.
figure a shows an overview of a controller and a plant environment model in a feedback loop.
the system input and output are respectively shown as desired andactual in figure a .
the variable desired represents the location we want a robot to move to the speed we require an engine to reach or the position we need a valve to arrive at.
the variable actual represents the actual state speed position of the plant.
the actual value is expected to plant model e t actual t desired t kpe t kdde t dtkire t dtpidplant model controller model sut a b desired actualerroroutputoutput t figure continuous controllers a a controller model and its environment plant model and b a controller pid formulation .
initial desired id desired valuei input actual value output final desired fd fsmfrfsttimet tt0 figure continuous controller input and output.
objective functionsfst fsmandfrare illustrated on the output signal.
reach the desired value over a certain time limit making the error i.e.
the difference between the actual anddesired values eventually zero or practically negligible.
the task of the controller is to eliminate the error by manipulating the plant to obtain the desired effect on the actual status of the plant.
the variables desired and actual are time dependent and are specified using signals over time.
continuous controllers are designed via mathematical models known as proportional integral derivative pid equations .
figure b shows the generic most basic formulation of a pid equation.
lete t be the difference between desired t andactual t .
a pid equation is a summation of three terms a proportional termkpe t an integral term kirt 0e t dt and a derivative termkdde t dt.
the coefficients kp kiandkdare configuration parameters and are fixed for every instance of a controller.
these parameters depend on the physical properties of the plant hardware that the controller eventually interacts with.
the number of configuration parameters in real world pid controllers is often more than three.
this is because these pid controllers have more than three terms or each term may have a more complex formulation consisting of more coefficients.
for example our industrial system includes six configuration parameters.
pid formulations are typically specified in simulink allowing engineers to simulate controllers analyze their output and eventually generate code from the controller design.
figure shows simulations representing the input and output variables of a controller.
the input i.e.
variable desired is shown by a dashed line and is given as a step signal.
specifically this input signal first sets the controller at an initial desired id value until time t0 and then requires the controller to move to a final desired fd value by time t. the output i.e.
variable actual shown by a solid line starts at zero and gradually moves to reach and stabilize at the initial desired id and then it moves towards the final desired fd and stabilizes there.
to test a controller engineers simulate the controller using different input step signals by varying idandfd.
for each simulation they generate the output signal i.e.
the actual signal in figure 164and check if the output conforms to the following three main requirements that we identified in our previous work stability the controller shall guarantee that the output will reach and stabilize at the input after a time limit.
smoothness the actual value shall not change abruptly when it is close to the input.
responsiveness the controller shall respond within a time limit.
we define three objective functions fst fsmandfrover the output signal to estimate quantitative values for stability smoothness and responsiveness requirements respectively.
we provided formal definitions of these functions in our earlier work .
briefly to evaluate stability engineers check whether after time the difference between input and output converges to zero and in addition the output remains stable afterwards.
function fstmeasures the maximum difference between input and output over the time periods shown by thin dashed arrows in figure .
to evaluate smoothness engineers check whether the undershoot or overshoot of the output signal is not too large.
as shown in figure fsm measures the maximum undershoot and overshoot of the output signal.
the response time is the time it takes for the controller output to reach or to become close to its input.
as shown in figure fr measures the response time intervals of the output.
to compute fst fsmandfr we provide an input to the controller simulink model and use the generated output signal to compute these functions.
the input to the controller includes values for the controller configuration parameters and values for idand fd which characterize the input step signal.
therefore functions fst fsmandfrdepend on the configuration parameters and id andfdvariables.
having computed these three functions over an output signal engineers can then decide based on their domain knowledge and thresholds provided in the requirements whether the controller under analysis satisfies each of the above requirements or not.
in general the higher the objective function value the more likely it is that the controller violates the requirement corresponding to that objective function.
currently in most companies mil testing of controllers is limited to running the controller for a small number of input signals that are often selected based on the engineers domain knowledge and experience.
existing mil testing often fails to find erroneous scenarios that the engineers are not aware of a priori.
identifying such scenarios later during sil hil is much more difficult and expensive than during mil testing.
in our earlier work we proposed a search based approach for testing an individual controller configuration .
that is we fixed the values for configuration parameters based on those used during hil testing and developed search algorithms maximizing fst fsmandfrwithin an input search space with two dimensions idandfd.
for example consider a controller with float variables idandfdranging from to .
the input search space of this controller with dimensions idandfdis shown in figure a .
our approach has two steps exploration and search.
for each objective function our approach first computes that function for several points randomly selected from the input space exploration step .
we divide the input space into a number of regions e.g.
equal regions in figure a and shade each region based on the objective function output.
the resulting diagram is called a heatmap diagram.
for example figure a is a heatmap diagram generated based on the stability objective function values for points.
specifically in a heatmap diagram the points in darker regions yield a higher average objective function output than the points in lighter regions.
hence darker regions are more likely to include input values violating the controller requirements.
in the search id .
fd .08time s .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
idfddesired value input actual value output a b figure an example representing our mil testing approach for a single controller configuration a a heatmap diagram generated by our approach for the stability requirement and b the identified worst case scenario violating the stability requirement.
fst fsm frrid rfd rcal1 rcal2 rcal3 rcal4 rcal5 rcal6 d objectivefunctions input spacecontroller figure controller objective functions and the ranges of the input variables and configuration parameters for our industrial controller.
step of our approach we apply a single state search algorithm to dark regions to find points that maximize our objective functions and hence are more likely to violate the requirements.
for example the space shading in figure a is produced based on the output of the stability objective function fstapplied to a faulty controller.
the controller satisfies the stability requirement for the input signals generated by the points in the clear shaded regions.
however applying our single state search to the dark region of id andfd results in finding the simulation in figure b which clearly violates the stability requirement.
note that since the controller behavior for most of the input space more than of the input space conforms to the stability requirement it is very unlikely that one can discover the faulty behavior by manually selecting and running a few simulations.
in this paper we extend our mil testing approach to include not only idandfdvariables but also the controller configuration parameters.
for example the industrial controller used as a case study in this paper has six configuration parameters referred to as cal1 tocal6 respectively.
the type of variables id fd and cal1 to cal6 is float.
we denote the range of variables idandfdbyrid andrfdrespectively and the range of each cali byrcali.
figure provides value ranges for each of the configuration variables andidandfdvariables in our industrial controller.
to compute highest values of fst fsmandfrfor any controller configuration our search algorithm has to handle the search space size of jri d rfd rcal1 rcal6j.
due to sheer size of the search space we cannot effectively solve our problem by simply applying existing search algorithms.
instead in this paper we combine dimensionality reduction andsurrogate modeling techniques based on supervised learning to perform search in large and multidimensional input spaces and to reduce the cost of computing our objective functions for individual points in the search space.
.
mil testing using search figure shows an overview of our search based approach to mil testing of continuous controllers.
similar to our previous work our approach is composed of an exploration and a search step.
the input to our approach includes a set of objective functions and a controller model simulink worst case scenarioslist of critical partitionsregressiontree .search withsurrogate modelingfst fsm frregressiontree t1.2exploration domainexpertf r1 ... rd!r1.1dimensionalityreduction f r ... r d!r2.
build surrogate model f r ... r d!rset of pairsp2.
search builduse p f p w v1 ... vd f f r ... r d !rpoint with the highest output of surrogate mmodel a exploration b search1.exploration with dimensionality reductionfigure an overview of our automated approach to mil testing of different configurations of continuous controllers a exploration step b search step.
controller simulink model required to compute the objective functions.
specifically in our work objective functions are fst fsm andfras described in section .
the input spaces of these functions are the same and equal to the cross product of the ranges for idandfdvariables and the configuration variables.
for example the input space objective function domains in our case study isrid rfd rcal1 rcal6 see figure .
the range of the objective functions is the set of real numbers r. in the exploration step figure a we apply a random unguided search to the entire input space of the objective functions and then based on the results we build a regression tree partitioning the input space such that the variance of the objective function values within each partition is minimized.
before performing exploration for each objective function we use a dimensionality reduction strategy to identify dimensions that have the most impact on that objective function.
this allow us to focus the exploration step only on dimensions with most impact on the objective functions and hence increase the scalability of our approach.
the regression tree built based on the exploration results enables us to divide the space into partitions such that for each partition the value of the objective function is predictable within a certain confidence interval.
in addition regression trees allow the engineers to visualize partitions from a multidimensional space.
we then use regression trees to identify higher risk partitions i.e.
those partitions that contain input values that are likely to violate controller requirements.
regression trees replace the heatmap diagrams e.g.
figure a we used in our earlier work which can no longer be used for a space that has more than three dimensions.
from the regression trees we select the partitions with the highest mean for the objective function which are considered to be higher risk as they are more likely to contain critical errors.
in the search step figure b we focus our search on the selected partitions and employ single state search algorithms to identify within those partitions the worst case scenarios to test the controller.
in this step we build surrogate models to minimize the need for running simulations of simulink controller models so as to make the search more scalable.
based on our previous experience the main cause of computation time of our search is such simulations.
recall that to compute objective functions we have to simulate the input controller model.
in this work for each objective function and for each input space partition we create a surrogate model that predicts the objective function values within that partition.
we use the exploration results related to each partition to build surrogate models.
a surrogate model built for an objec tive function is able to predict the output of that function within a confidence interval.
using this model our single state search can determine whether the decision as to which point the search has to move to can be made without resorting to running simulations or not.
in sections .
exploration and .
search we describe the first and second steps of our approach respectively.
.
exploration figure a shows the exploration step of our approach.
the input of this step is an objective function f r1 rd!r.
functionfcan be any of the objective functions in figure .
the goal of this step is to efficiently select a set of points in the space of r1 rdand compute the output of ffor each point in this set.
the output of this step is used to identify critical parts of the r1 rdspace i.e.
those partitions for which fproduces the most critical highest values and further to create a surrogate model for individual critical partitions that can estimate as precisely as possible the output of ffor any arbitrary point in that partition.
given an objective function f we first use dimensionality reduction techniques to identify search input dimensions that have the least impact on the output of f. a dimension i i d has a low impact on the output of fif varying the input vector v1 vi vd offby varyingviwithin the range ridoes not yield a significant change in the output of f. in addition this step uses adaptive random search to explore the input space of fby focusing on the dimensions with most impact onf.
here we briefly discuss adaptive random search applied to the entire input space without considering dimensionality reduction.
in section .
.
we show how this algorithm is modified to focus on significant search dimensions only.
adaptive random search is an extension of the naive random search that attempts to maximize the euclidean distance between the selected points.
adaptive random search explores the space by iteratively selecting points in areas of the space where fewer points have already been selected.
let r1 rdbe the input space and let pibe the set of points selected by adaptive random search at iteration i. at iterationi adaptive random search randomly generates a set p of candidate points in the input space.
the search computes distances between each candidate point pand points already selected inpi.
formally for each point p v vd inp the search computes a function dist p pi as follows dist p pi min v0 v0 d 2piqpd j vj v0 j the search algorithm then adds to pia pointpinpsuch that dist p pi is the largest.
the algorithm terminates after generating a specific number of points.
adaptive random search is similar toquasi random number generators that are available in some languages e.g.
matlab .
similar to our adaptive random search algorithm these number generators attempt to generate points that are evenly distributed across the entire space.
below we describe how we use dimensionality reduction and adaptive random search to efficiently select a set of points in the input space of fthat can be utilized in the search step figure b for building an effective surrogate model.
.
.
dimensionality reduction using dimensionality reduction we identify the dimensions of the domain of fthat have the most impact on the output of f. to do so we rely on sensitivity analysis which is the study of how the variations in the outputs of a function are related to the variations in its inputs .
an application of sensitivity analysis is identifying input variables with the most and the least significant impact on a 166cal5idcal3fd cal4cal6cal1 cal20.
.
.
.0sample standard deviation .
.
.
.
.2sample mean 2s i ifigure the elementary effect analysis results for the stability objective function f st with an eight dimension input space.
given function.
among different sensitivity analysis techniques we use the elementary effects method .
this method is intuitive and compared to other techniques such as variance based methods requires fewer number of function evaluations and hence is well suited for functions that are expensive to compute.
the elementary effects method works as follows using adaptive random search we generate rpoints in the input space of r1 rd.
for each dimension i i d and for each pointj we varyviin the input vector v1 vd offby a parameter and measure the resulting variation ijin the output of f. we then compute the sample mean i and the sample standard deviations ifor each input space dimension ito assess the impact of that dimension on f. figure shows an example output of the elementary effects method for the stability objective function fst with an eight dimension input space.
provided with this diagram engineers choose the dimensions with significant impact on fst.
for example they may decide that cal1 cal2 cal4 and cal6 which have sample means and standard deviations close to zero are not significant.
.
.
exploration in the reduced dimensional space to explore the input space we use the adaptive random search algorithm as described at the beginning of section .
.
the difference is that we modify the dist function to ensure that the search maximizes diversity along the dimensions with significant effect on f. otherwise note that exploration with and without dimensionality reduction take about the same time and operate in the same space.
letdrbe the set of dimensions with significant impact on f e.g.
drin figure isfid fd cal3 cal5g.
for each candidate pointp we compute distdr p pi as follows distdr p pi min v0 v0 d 2piqp j2dr vj v0 j in contrast to the dist function presented at the beginning of section .
in distdr we consider only the values related to the dimensions in dr. that is we focus on maximizing the diversity of the selected points along the dimensions in dr and the values of the variables along the dimensions in f1 dgndrcan either be fixed or set arbitrarily.
this allows us to intensively explore parts of the space that results in the most variations in the output of f. having selected npoints in the input space of fand having computedffor each point we build a regression tree based on these points e.g.
see figure .
the regression tree represents a stepwise partition of the input space aimed at getting increasingly homogeneous partitions with respect to f. such a representation is a convenient and intuitive way to visualize the impact of input space dimensions on f .
figure shows an example of the regression tree generated from the exploration results for fst n .
each node in the tree corresponds to a space partition and is labeled by the number of the points in that partition as well as the mean and standard deviation of the values of fstfor those points.
for example all pointsfd .43306count meanstd devcount meanstd devfd .43306count meanstd devid .64679count meanstd devcount meanstd devcal5 .020847cal5 .020847count meanstd devcount meanstd devcal5 .014827cal5 .014827count meanstd devcount meanstd dev1000 .
.
id .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0023515figure an example of a regression tree generated for fst.
the highlighted node in figure corresponds to a partition where min fd fd and0 cal5 max cal5 and it includes points selected during exploration.
the mean and standard deviation of fstfor these points are and respectively.
we select a partition with the highest mean value for the objective function from the regression tree.
note that such a partition has to be a leaf node because any non leaf node has exactly one child node whose mean value is higher than the mean value of its parent.
the partition with the highest mean is more likely to include errors and critical scenarios.
we denote input space partitions by r and define it asr r r dsuch thatr i ri for1 i d. for example in figure we select the highlighted partition that has the highest mean value and denote it by r r i d r fd r cal1 r cal2 r cal3 r cal4 r cal5 r cal6 such thatr f d min fd r c al5 max cal5 and r v rv for every other variable v. after selecting a partitionr this partition together with the points generated during exploration inside this partition are passed to step .
.
search figure b shows the search step of our approach.
the search step takes as input a function f a partitionr and a setpof p f p pairs computed during the exploration step.
specifically the spacer is a critical input space partition identified in the previous step and pincludes pairs of p f p wherepis a point in r that was selected by the adaptive random search in step .
and f p is the output of fapplied top.
we refer to pas an observation set.
in step .1in figure b we first build a surrogate model musing supervised learning techniques .
the surrogate model mconsists of two functions a predictive function f r !r that estimates the output of ffor any point inr and an error function f r !r.
for each point p2r and a given confidence level cl f p cl is the prediction error indicating that with a confidence level of cl the actual value of f p is within f p f p cl .
in step .
we search the points in r to find a point that maximizesf.
since computing fis expensive we expedite the search by checking whether we can conclusively decide the next point that the search should move to using f. specifically the output of f is conclusive if the prediction error f is less than the difference between the output of fand the existing highest value found by the search.
otherwise we have to compute the actual output of f by simulating the simulink controller related to f. .
.
surrogate modeling we use supervised learning techniques to build a surrogate model of the function f. given a point p supervised learning predicts 167f p using a setpof observations with known output values .
we divide the observation set pinto a training set and a testset.
the training set is used to infer a predictive function f. this is done by estimating the parameters of fsuch that ffits the training data as well as possible i.e.
for the observations in the training set the differences between the output of fand that of fare minimized.
the test set is then used to evaluate the accuracy of the predictions produced by fwhen applied to observations outside the training set.
supervised learning techniques are categorized into regression andclassification techniques where the goal is to predict real valued and categorical outputs respectively.
we use regression techniques becausefis a real valued function.
specifically we use the following regression techniques linear regression lr .
linear regression assumes that fis linear.
given an observation point p v1 vd linear regression infers fas a linear function f v1 vd 1v1 2v2 dvd exponential regression er .
exponential regression assumes that fis non linear but monotonic and infers fin the following form f v1 vd 0v 1v v d d p olynomial regression pr .
polynomial regression assumes that fis neither linear nor monotonic.
the inferred function ftakes the form of an nth degree polynomial f v1 vd dp i i1vi i2v2 i invn i in this paper we consider pr n because based on our experiments the predictability of our surrogate models decreases forn .
in the above three regression methods parameters i i d and ij i dand1 j n have to be estimated using the training data.
the goal is to estimate these parameters such that the sum squared error of the predicted outputs for the training points is minimized.
that is pm i f pi f pi wheremis the number of observations in the training set is minimized.
in addition we use stepwise regression to build fin the above three regression methods .
instead of including all variablesv1tovdat once stepwise regression aims at selecting a minimal subset of variables that are statistically significant at explaining the variation in f. the variables may be selected in a forward or backward way.
in the forward selection variables are iteratively selected and added as long as the sum squared error over the training data decreases i.e.
the predictive power of fimproves.
at each iteration a statistical test f test is used to determine which variable best improves the predictive power of f .
dually in the backward elimination variables are iteratively removed as long as the predictive power of fover training data does not decrease.
most implementations of stepwise regression e.g.
stepwiselm in matlab combine the backward and forward methods by iteratively switching between them.
that is they add variables using forward selection for some iterations and then switch to backward elimination after a while to remove unnecessary variables.
we note that there are a number of other supervised learning methods such assupport vector regression and neural networks that we do not discuss in this paper due to lack of space and leave them for future work.
in addition to function f all the above regression methods provide a function f r !r.
function f p cl estimates the prediction error for a point pbased on a given confidence level cl which is a percentage value between 0and100 usually above .
for example the input cl implies that the actual value of f p lies in the interval of f p f p cl with a confidence level of .
.
.
single state search using surrogate model as discussed in our earlier work to compute highest values of our objective functions among the existing meta heuristic search techniques we opt for single state algorithms as opposed topopulation based ones.
this is because population based search algorithms compute fitness functions for a set of points a population at each iteration .
hence they are less likely to scale when objective functions are computationally expensive.
we propose a new hill climbing hc single state search algorithm extended to use surrogate models.
our algorithm speeds up the search by avoiding simulations when it is possible to decide the next move for search based on the surrogate model predictions.
the algorithm is shown in figure .
it takes as input the function f the setpof observations in the partition r r d the surrogate model m f f and a confidence level cl.
the output of the algorithm is a point wwith the highest output of f found intsseconds.
the algorithm first identifies the observation p f p 2psuch thatf p is the largest in p and sets the variable highest tof p lines .
at the beginning of each iteration of this algorithm highest is the highest output of fcomputed so far.
the algorithm then iteratively generates a new point newp by tweaking the current pointp line .
the tweak operator is similar to the one used in our earlier work .
that is we tweak a point p v1 vd by shifting each viwith a value xrandomly selected from a normal distribution with mean 0and variance jr ij.
in our previous work we showed that this tweak operator yields effective results for two dimension input space functions for each new point newp the algorithm computes surrogate model functions f newp and f newp cl lines .
at line the algorithm determines whether it needs to compute the actual value of f newp or it can decide the next move only using f newp .
figure depicts the conditions under which we have to compute f newp .
specifically if highest is less than or equal to f newp f newp cl or more than or equal to f newp f newp cl with a confidence level of cl highest is less or greater than f newp respectively.
in the case of highest greater than f newp f newp cl the search does not move tonewp and hence no need to compute f newp .
in the case of highest less than f newp f newp cl the search may move tonewp depending on the value of f newp .
thus we compute f newp .
if highest is between f newp f newp cl and f newp f newp cl we cannot confidently compare highest with the actual value of f newp using f newp and hence have to compute f newp .
line in figure summarizes the condition for determining whether f newp has to be computed or not.
if yes the algorithm computes f newp and refines the surrogate model musing the new observation newp f newp lines .
otherwise at line the algorithm decides the next point that the search should move to.
when it decides to move lines it updates the current pointpwithnewp and highest with the highest value of fcomputed so far and stored in y. in addition it keeps a copy of newp .
finally once the loop at line terminates the algorithm reports the pointwwith the highest output of ffound intsseconds.
168algorithm.
si ngle state search input f r r d!r.
the setpof p f p pairs.
the surrogate model m f f .
a confidence level cl.
output point w v1 vd with the highest output of f. .
let p f 2ps.t.
for all p0 f0 2p we havef f0 .highest f .fortsseconds do .newp tweak p .y f newp .
f newp cl .
ifhighest y ifhighest is less than f newp f newp cl we .
y f newp simulate and compute f newp as shown in figure .
.p p newp y otherwise we bypass simulation.
.
f f buildsurrogatemodel p .
ify highest .
highest y .p w newp .return w figur e single state hill climbing hc search algorithm with surrogate modeling.
... f newp f newp cl f newp f newp cl ... f newp f newp cl f newp f newp cl ... perform simulationconfident that confident that not confident aboutthe relation between highest f newp highest f newp highest f newp andbypass simulation figure depicting the conditions used by the algorithm in figure to perform or to bypass simulations.
note that the higher the value of cl the algorithm in figure is more likely to compute the actual value of fby running simulations.
forcl the interval of f newp f newp cl is equal to and hence the algorithm behaves like a conventional hill climbing algorithm and runs a simulation at each iteration.
for cl we have f newp cl and hence the algorithm runs fewer simulation i.e.
only when highest f newp see figure .
.
experiment setup in this section we present the research questions some information about our industrial subject the metrics used to evaluate surrogate models and information about our experiment design.
.
research questions rq1 how do the different surrogate modeling techniques perform compared to one another?
rq2 does dimensionality reduction improve prediction accuracy of the best surrogate modeling technique identified in rq1?
rq3 how do our single state search algorithms with and without surrogate modeling perform compared to each other?
rq4 does our approach help identify testing results that are useful in practice?
inrq1 for each objective function we identify among the four regression methods discussed in section .
.
the method that yields a surrogate model with highest prediction accuracy.
for each objective function we then use this best surrogate model for the single state search.
in rq2 we determine whether focusing exploration on significant dimensions of each fitness function improves the prediction accuracy of the surrogate models.
recall that exploration with and without dimensionality reduction take about the same time and operate in the same space.
however the exploration results with dimensionality reduction are more diversely distributed along the dimensions with significant impact on the objective functions.
the question is whether this gives rise to moreaccurate and predictive surrogate models?
in rq3 we compare the performance and results of our single state search algorithms with and without surrogate modeling in order to determine whether our new approach scales better in large search spaces.
finally in rq4 we compare our best results i.e.
test cases with highest objective function values with those obtained in our previous work as well as the existing test cases used in practice.
.
industrial subject supercharger is an air compressor blowing into a turbo compressor to increase the air pressure supplied to the engine.
the air pressure is controlled by a mechanical bypass flap when the flap is completely open resp.
closed the air pressure is minimum resp.
maximum .
our industrial subject is the supercharger bypass flap position controller sbpc which determines the position of the bypass flap to achieve a desired air pressure.
sbpc is one of the most complex controllers among those dedicated to engine management.
in sbpc the desired andactual variables see figure represent the desired and actual positions of the flap respectively.
the flap position is bounded within 0for open and 0for closed i.e.
rid rfd in our experiments.
the sbpc controller and plant models are both implemented in simulink and include 443blocks in total.
sbpc has six configuration parameters cal1 tocal6 impacting the pid controller terms and hence the controlling behavior of sbpc.
figure shows the ranges for the sbpc configuration parameters.
.
surrogate modeling evaluation metrics to assess the goodness of fit and predictive power of the generated surrogate models we use two well known evaluation metrics for supervised learning methods coefficient of determination or r2 and mean of relative prediction error or mrpe .
specifically r2measures the proportion of the total variance of f explained by ffor the observations in the training set.
in other words r2is a measure of goodness of fit on the training set.
the value ofr2is always less than .
the higher the value of r2 the higher the goodness of fit of fpredictions.
the mean of relative prediction error mrpe measures the predictive power of fusing the observations from the test set.
letkbe the number of observations in the test set.
we compute the mrpe as follows k pk i 1jf pi f pi f pi j that is mrpe measures the average of the relative prediction errors of ffor the observations in the test set.
the lower the value of mrpe the higher the predictive power of f. note that a surrogate model fwith highr2may not yield low mrpe values due tooverfitting i.e.
when fis excessively tailored to the training observations but does not generalize well to the test observations.
therefore to compare different surrogate models we have to take into account both r2and mrpe metrics.
in theory both r2and mrpe can be computed on either the training set or the test set.
when applied to the training set they measure the goodness of fit and when applied to the test set they assess the prediction accuracy.
however it is more common to use r2to measure goodness of fit to get an idea of how much variance remains unexplained in the data used to fit the model and mrpe to address prediction accuracy as it is more readily interpretable to assess the applicability of a prediction model.
therefore we chose to computer2on the training set and mrpe on the testset.
.
experiment design and analysis strategy we implemented the elementary effect analysis adaptive random search surrogate modeling techniques including lr er and pr n and our single state search algorithm in matlab.
both adaptive random search and single state search are required to call simulations of the sbpc simulink model.
the latter in addition may resort to surrogate modelling by calling and refining surrogate models.
we ran each simulink simulation for 2sec t 2s in figure to give sbpc enough time to stabilize.
we ran all the experiments on amazon micro instance machines which is equal to two amazon ec2 compute units.
each unit has a cpu capacity of a .
.
ghz xeon processor.
each sec simulink simulation of sbpc takes about 31sec on the amazon machine.
while calling the surrogate models is negligible less than 5ms and rebuilding them takes about 2sec on average.
to investigate rq1 rq4 we designed and performed the following experiments.
below we use abbreviations dr and sm for dimensionality reduction and surrogate models respectively.
exp i. to answer rq1 andrq2 we computed the output of exploration once with and once without dr. to compute exploration results with dr we applied the elementary effect analysis to our three objective functions with parameters r and .
recall from section .
that ris the number of points and is the size of the modification applied to each dimension of each point.
these values are selected based on the guidelines in .
to account for randomness we repeated the elementary effect analysis 10times.
the results across different repetitions were consistent and out of the eight input space dimensions four were significant forfst and three were significant for frandfsm.
we then applied our adaptive random search to each of these functions by focusing the exploration on their significant dimensions only.
we let n and executed our adaptive random search to generate points forfst and points forfrandfsm.
note that sincefrandfsmhave the same significant dimensions we generated the same points for both functions but kept two output values for each point.
to compute exploration results without dr we let n and generated points across the eight dimensions of the input space but computed fst fr andfsmseparately for each point.
note that the total number of points generated during exploration with and without dr was the same and equal to .
for each objective function we built two regression trees one from the exploration results with dr and one from the exploration results without dr. each regression tree node corresponds to an input space partition.
suppose that and respectively denote the standard deviation and mean values related to each partition.
we expand each regression tree until for every leaf node falls below .
expanding the trees further often results in leaf nodes containing very few observations and corresponding to very small space partitions.
by expanding the tree the variance of the objective function values in leaf node partitions decrease.
in each tree among all the leaf nodes with we select the one that has the highest for further search of worst case scenarios.
for the partitions with the highest based on their observation points we created four surrogate models sms using lr er and pr n techniques.
to effectively apply surrogate modeling techniques we want to have at least 200points in the selected partitions.
if lower we generate additional points before building a sm.
we then use these 200points as training data to build sms.
for the test sets we generate an additional 50points using a naive random selection technique.
that is the test points are chosen the same way irrespective of the use of dr. this allows us to use the sametest sets to facilitate the comparison of the sms generated with and without dr. in total to obtain the exploration results with and without dr we performed experiments objective functions surrogate modeling techniques with without dr .
to account for randomness we repeated each of the experiments times.
exp ii.
to answer rq3 andrq4 we performed single state search for a given input space partition once with and once without using sms.
to compute the search results with sm we first built a sm using the best technique identified in rq1.
then for each objective function we applied our sm based single state search algorithm in figure to a given input space partition.
we ran this algorihtm for three confidence levels cl and .
for each objective function and each confidence level we ran the search for sec.
to compute the search results without sm for each objective function we applied a hill climbing hc algorithm similar to that used in our previous work to a given input space partition.we let the search run for 3000s i.e.
the total search budget time for the single state search with and without sm was the same and equal to 3000s.
we refer to our sm based single state search algorithm in figure as hc sm and to our single state search algorithm without sm as hc nosm.
in total to obtain the search results with and without sm we performed experiments with sm objective functions confidence levels and experiment corresponding to the objective functions without sm.
to account for randomness we repeated each of the experiments times.
.
experiment results this section provides responses based on our experiments for research questions rq1 torq4 described in section .
.
rq1.
to answer rq1 we use the sms generated by the exp i experiments section .
.
since exp i includes experiments and each experiment was repeated 10times we obtain different groups of sms where each group consists of different sms.
for all the sms we compute r2and mrpe values.
figure a shows the averager2and average mrpe values for sm groups generated forfsm fst andfrbased on the exploration results with dr. as shown in figure a the sms built using pr n have the best goodness of fit highest r2 and best predictive accuracy lowest mrpe .
the results for the other sm groups generated from the exploration results without dr are consistent with those shown in figure a .
specifically our results confirm that compared to lr er and pr n pr n generates the most accurate sms for our objective functions.
in addition the results in figure a show that while the surrogate models generated by pr n forfsmandfrhave high goodness of fit and predictive power this is not the case for fst.
additionally though due to space constraints this cannot be shown here we observed that applying pr with n 3results in less accurate sms i.e.
lower r2and higher mrpe.
rq2.
to answer rq2 we focus on the best sms in the exp i experiments i.e.
the sms generated by pr n .
figure b shows the mrpe distributions related to the best sms generated from the exploration results both with and without dr and for each offsm frandfst.
to statistically compare the mrpe values we performed the non parametric pairwise wilcoxon pairs signed ranks test and calculated the effect size using cohen s d .
the level of significance was set to and following standard practice dwas labeled small for d medium for0 d and high for d .
testing differences in mrpe distributions shows that for fsmandfr the sms built b distribution of mrpe for three objective functions with and without dr .
.
.
.
.
.
.
.
.
.
.
.
.
drno drdrno drdrno dr a mean of mrpe values for different surrogate modeling techniques r2 smoothness fsmresponsiveness frstability fstfstfsmfrpr n r2 mrpe0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.2519lrr2 mrpeerr2 mrpepr n r2 mrpefigure experiment results for rq1 and rq2 a comparing different surrogate modelling techniques and b comparing exploration results with and without dimensionality reduction dr .
with dr have significantly better predictive power than those built without dr. in addition the effect size is high for both fsmand fr.
forfst however there is no statistically significant difference between the mrpe values of the sms generated with and without dr. specifically focusing exploration on a reduced dimension space significantly improves with a high effect size the predictive power of the sms for fsmandfr but does not have a significant impact on the predictive power of the sms related to fst.
this may be due to the sms for fstbeing less accurate than those for fsmandfr.
since based on our results dr never decreases the predictive power of the resulting sms our results suggest to focus exploration on the significant dimensions identified by dr. rq3.
to answer rq3 we use the exp ii experiments section .
.
recall that in exp ii each run of each algorithm was executed for sec.
in each run we recorded the value of the variable highest i.e.
the highest found output of the objective function see figure at every sec time interval.
figure compares the value distributions of highest obtained from hc sm with cl and and from hc nosm at some selected representative time points for each of the objective functions fsm fst andfr.
specifically figure a shows the value distributions of highest forfsmat 800s 1500s 2500s and 3000s.
figure b shows the value distributions of highest forfrat 200s 300s and 3000s and figure c shows the value distributions of highest forfst at 3000s.
time points for each fitness function were selected to make the overall trends visible for hc sm and hc nosm.
figures a and b respectively represent the differences between the mean values found for fsmandfrby each of the hcsm algorithms and by hc nosm over 3000s of time.
for fsm as shown in both figures a and a within 3000s all the hcsm algorithms are able to find higher values compared to those found by hc nosm.
among the hc sm algorithms cl finds highest values for fsmat 2500s with a mean of and a median of .
hc sm with cl andcl reach slightly lower values for fsmat around 3000s.
hc nosm however is not able to go higher than both mean and median in the 3000s allotted time.
that is in 3000s and across different runs half of the values computed by hc sm indicate an over undershoot of or higher while the highest smoothness violation found by hc nosm is about .
the one percent improvement of hc sm over hc nosm is important in practice.
this is because depending on the hardware configuration engineers specify a maximum over undershoot see figure that can be tolerated for the smoothness requirement.
ex0.215cl 80after secondsafter secondsafter secondsafter seconds cl 90cl 95nosm0.
.
.
.
after seconds .
.
.
after secondsafter secondsafter seconds .
.
.
a smoothness fsm b responsiveness fr c stability fstcl 80cl 90cl 95nosmcl 80cl 90cl 95nosmcl 80cl 90cl 95nosm cl 80cl 90cl 95nosmcl 80cl 90cl 95nosmcl 80cl 90cl 95nosmcl 80cl 90cl 95nosmfigure boxplots for single state search output values with and without surrogate modeling at some selected time points and applied to a smoothness f sm b responsiveness f r and c liveness f st .
.
.
.
.
.000050010001500200025003000time s .
.
.
.
.
.001050010001500200025003000time s a for fsm frmean output differencesmean output differencescl 80cl 90cl nosm cl 80cl 90cl nosm b for figure differences of mean output values of search with surrogate modeling hc sm with cl and search without surrogate modelling hc nosm .
ceeding this value even slightly is not in general acceptable.
in the particular case of our case study sbpc slight deviation for the smoothness causes the flap to hit other hardware parts generating noise and damaging hardware over time.
we note that even after running hc nosm for 5000s its average and median output remained at around .
in general the overall increase in the output of hc nosm over 5000s was very small.
forfr as shown in both figures b and b the hc sm algorithms find their highest values within the first 300s of time with cl being the fastest again.
specifically at 300s on average hc sm identifies a worst response time of 167s while the average output of hc nosm indicates a response time of 163s.
in contrast to the results of fsm forfr hc nosm is able to match the hc sm algorithms in around 2500s of time see the area shown by a dashed circle in figure b .
that is the hc sm algorithms are about times faster than hc nosm.
finally for fst figure c we did not observe any noticeable difference between the values found by the hc sm algorithms and those found by hc nosm within 3000s of time.
that is within this time all the algorithms behaved the same.
this is due to the sm for fst which is clearly less accurate than those for fsmandfr.
hence 171mil testing different configurationsstabilitysmoothnessresponsivenessmil testing fixed configurationsmanual mil testing .
deviation over undershoot over undershoot over undershoot ms response time80 ms response time50 ms response timefigure comparing our mil testing results with the results of mil testing fixed controller configurations and the results of manual mil testing.
forfst the hc sm algorithms almost run simulink simulations at every search iteration producing the same results as hc nosm.
we conclude that for accurate sms with high predictive power i.e.
those built for fsmandfr hc sm outperforms hc nosm.
specifically for fsm hc sm computes higher output values that could not be computed by hc nosm and for fr hc sm is about eight times faster than hc nosm in finding the same output.
this is because compared to hc nosm hc sm runs fewer simulink simulations relying on the sm output in many search iterations.
in addition hc sm with cl is slightly faster than hc sm with cl and because it runs fewer simulations.
rq4.
to demonstrate practical usefulness of our approach we argue that mil testing for different configurations of the sbpc controller finds requirements violations that have neither been identified via mil testing of fixed configurations nor by manual testing based on domain expertise.
figure compares our results with the results of our previous work on mil testing with fixed configuration parameters and the results of manual expertise based mil testing.
as shown in the figure by extending our approach to test different configurations within some given ranges we were able to identify a critical violation of the stability requirement with a deviation of .
note that our previous results as well as the results from manual testing never indicated any stability error in sbpc.
in addition for smoothness and responsiveness our current work found more critical violations specifically the maximum observed over undershoot was compared to the found by our previous work and found by manual testing.
finally we computed a worst response time of 170ms compared to 80ms found by our previous work and 50ms identified via manual testing.
we conclude our results by noting that due to limitations of manual testing mil testing in practice mostly focuses on a single controller configuration which is typically the one specified based on hil configuration parameters.
this obviously falls short when the controller is configured and deployed on a hardware with parameters that differ from those used on hil.
since existing mil testing does not consider different configurations the errors that could have been found at mil level go unnoticed until the very late development stages.
our work attempts to alleviate this shortcoming by enabling mil testing for various configurations obtained by varying configuration parameters within their given ranges.
.
related work several approaches to testing and analysis of simulink models rely on formal methods e.g.
by analyzing hierarchical finite state machines verifying logic control systems and applying formal model checking .
these approaches are more amenable to verification of logical and state based behaviors.
in our work we focused on testing pure continuous controllers which have not been previously captured by any discrete event or mixed discrete continuous notation .
further our approach does not require any additional modelling since we apply our technique directly to simulink models already developed as part of the controller development process in industry.search based techniques have been applied to simulink models to generate test input data with the goal of maximizing some coverage criteria .
these criteria however are inadequate for testing continuous behaviours.
further coverage criteria satisfaction alone is a poor indication of test suite effectiveness .
our work enables generation of test cases specifically for controllers and based on their high level requirements.
continuous controllers have been widely studied in control engineering where the focus has been to optimize controller behaviors for a specific hardware .
such optimization techniques are largely performed at late stages of development on real hardware and cannot replace our testing approach which is particularly useful for early design and development of controllers.
surrogate modeling has been used to scale up computation in various application domains such as avionics chemical systems and medical domain .
similar to our work they use surrogate models in the context of evolutionary algorithms and their goal is to approximate complex mathematical models with faster to compute models i.e.
surrogate models to speed up highly time consuming and costly simulations and experiments.
our work is the first to apply surrogate models for testing continuous controllers.
furthermore our hill climbing hc single state search algorithm represents a new combination of surrogate modelling and evolutionary algorithms by precisely showing when the search has to run real simulations and when the surrogate model is sufficient.
finally our algorithm continuously refines surrogate models using the real simulations performed during the search.
in this work we applied surrogate modeling in conjunction with dimensionality reduction to test controllers.
dimensionality reductionin our work differs from input domain reduction used in software code testing where the goal is to remove irrelevant variables prior to test case generation via static analysis .
by dimensionality reduction we mean identifying significant dimensions of each fitness function to focus exploration on those dimensions.
.
conclusions testing and verification of the software embedded into cars is a major challenge in the automotive industry.
the problem becomes more pressing when engineers have to consider different hardware configurations on which the software will be eventually deployed.
in this paper we proposed an approach to mil testing of continuous controllers in large configuration spaces based on meta heuristic search with respect to smoothness responsiveness and stability requirements.
the main challenge is to scale search to large multi dimensional input spaces made up of possibly many configuration parameters.
to scale search we combined techniques for dimensionality reduction and supervised learning to build surrogate models that accurately predict simulation results without resorting to simulation in many cases.
our evaluation shows that our approach is able to identify critical violations of the controller requirements that had neither been found by our earlier work nor by manual testing.
further we showed that combining search with surrogate modeling remarkably improves our approach for two out of three requirements.
specifically for one requirement the search combined with surrogate modeling is eight times faster than the search without surrogate modeling and for the other requirement the search with surrogate modelling computes more critical requirements violations than what could be detected by the search without surrogate modeling.