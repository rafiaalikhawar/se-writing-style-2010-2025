interrogative guided re ranking for question oriented software text retrieval ting y e bing xie y anzhen zou xiuzhao chen software institute school of electronics engineering and computer science peking university beijing china key laboratory of high confidence software t echnologies ministry of education beijing china yeting12 xiebing zouyz chenxz12 sei.pku.edu.cn abstract in many software engineering tasks question oriented text retrieval is often used to help developers search for soft ware artifacts.
in this paper we propose an interrogativeguided re ranking approach for question oriented software text retrieval.
since different interrogatives usually indicateusers different search focuses we firstly label kinds ofquestion answer pairs according to the common interrogatives.
then we train document classifiers by using questions along with answers from stackoverflow ap ply the classifiers to our document repository and present are ranking approach to improve the retrieval precision.
insoftware document classification our classifiers achieve the average precision recall and f measure of .
.
and .
respectively.
our re ranking approach presents .
improvement in ndcg upon the baseline and we also ob tain .
improvement in ndcg when more candidates are included.
categories and subject descriptors h. .
informa tion search and retrieval d. .
miscellaneous keywords interrogative guided ranking software text retrieval .
introduction text retrieval tr is widely used by software developers.
the performance of retrieval depends greatly on the textualquery and its relationship to the software text .
consider the following two questions about lucene s standardanalyz corresponding author permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or toredistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september v asteras sweden.
copyright acm ... .
.
class names of lucene1 .
where is standardanalyzer?
.
how do i use standardanalyzer with termquery?
ifwesimplyapplythesetwoquestiontokeywordsbasedtr their queries are similar.
yet they are much different in whatthey are asking for.
question wants to know the positionwherestandardanalyzer locates.
while question wants to know how to use the standardanalyzer withtermquery .i f we use keyword based search method the corresponding an swer list is coarse.
in our experiments we found question 1has search results common with question in top tr results.
and when retrieving the question by lucene the answer of question ranked higher than the answer of ques tion .
thus some new approaches are needed to improvethe performance of question oriented software retrieval.
to achieve this objective we need to answer a question first is it possible to find any differences between the an swers of the above two questions?
it is obvious that thesetwo questions have different interrogatives i.e.
how where etc.
.
it indicates that we have an expectation of what the answer be like when we ask a question.
as a result their answers are different not only in words but also in codes andlinks.
thus we decide to build a software document classifi er which can automatically distinguish whether a documentis suitable to answer a question.
based on the classifier we can re rank the search results so as to improve the precision of software text retrieval.
training a document classifier need a lot data samples.
fortunately somesocialsoftwarequestion answering q a forums such as stackoverflow provide ideal vehicle to investigate developers questions and answers.
we can down load plenty of question answer pairs from these websites.every question is well described and its answers are votedand ranked.
therefore we can learn from the stackoverflow about what the answer would be like when given a question.
comparing with the various existing works our approach provides the following benefits .
we build six interrogative guided software document classifiers firstly in software engineering based on q a pairs from stackoverflow.
we make use of features in 1lucene is open source software project which offers a free open source information retrieval technology.
fouraspects i.e.
text code linkanddocumentlength to train a better software text classification model.
.
we firstly propose an interrogative guided re ranking approach for software documents retrieval.
when aquestionissubmitted itsinterrogativeisidentifiedandthe corresponding classifier is adopted to re score thetop k search results.
if a software text is classified as the same type with the question it would gain higher score and vice verse.
as a result we could improvethe performance of software text retrieval significantly.
.
compared with filter policy our re ranking approach demonstrates higher accuracy in the experiments.
weachieve .
improvement in ndcg and .
im provement in ndcg upon the baseline.
.
overview in this paper we proposed a re ranking approach in software text retrieval.
as shown in figure this approach iscomposed of two parts classifier building andre ranking .
theclassifier building part aims to build classifier for each type and the re ranking part takes advantage of these classifiers to re rank the retrieved documents.
classifier building this part includes question labeling document representation feature selection andmodel training.t h e question labeling is to label all the q a pairs.
we find that developers focus on different information whenthey use different interrogatives in their questions.
thus we label the questions with different tags based on inter rogatives e.g.
how why etc.
.
naturally the answers canbe labelled the same tags as their corresponding question s. we perform some checks on the tag of every question to ensure the accuracy of our dataset.
the document representation component deals with these labeled answers.
it represents each answer document as a feature vector.
each document contains a textual description along with code snippers links and document length.
the feature selection componentlearnsthemostimportantfeaturesfromdifferent answer vectors using machine learning technique.
we usethese features to reduce the dimension of the feature vector.next the selected features are provided to a model training algorithm to build classifiers individually.
re ranking this part includes text retrieval andreranking.t h e text retrieval engine ranks the relevant documents for a given question.
then we use the classifier to re rankthe documents retrieved by the tr engine.
we consider the final re rankscore as a linear combination of the learned score i.e.
the score generated by the classifier and thetext retrieval score.
when user submitted a question search engine could identify its corresponding tag and rescore every search result.
for those answers belonging to user s question type they possibly match some learned fea tures so that gain a higher score meanwhile those softwaretexts which do not contain those learned features would be scored lower.
a primary challenge in our work is to get enough amount of training data including correctly labeled q a pairs with high quality.
fortunately social software q a forum s such as stackoverflow provide a large amount of q a pairs and have a voting system to point out the quality of q a pairs .
we chose the questions and answers taggedwith lucene from stackoverflow from apr.
to sept. and got a sample consisted of questions with answers.
and considering there are some invalid and7 kx 6goxy kyzout 2ghkrotm ui sktz 8kvxkyktzgzout kgz xk 9krkizout 3ujkr xgototm rgyyolokx2ghkrkj ty kx 9kz4gz xgr 2gtm gmk kyzout 8kzxok gr uur 8gtqkj ui sktz 8ky rzy 8k xgtqotm 8k xgtqkj ui sktz 8ky rzy data process figure approach overview low quality questions and answers we select these questions with positive votes.
by this way we select questionsfor labeling.
these questions contain positively votedanswers.
.
classifier building in this section we describe the process of classifier building.
first the question labeling module labels the questionswith nine tags.
second the document representation mod ule transforms every labeled answer document into vector.then the feature selection module learns the discriminativefeatures for the answers of six main question types.
finally the model training module builds classifiers for each type individually.
.
question labeling as shown in table we labeled the questions as howto what why etc.
and these label names e.g.
what how where why etc are chosen because interrogatives frequently occur in questions of the corresponding category.
we con sidered the question meaning as well.
table question tags tag description sum howto asks for instructions which usually begins with word how .
e.g.
how to update a lucene.net index?
what asks what a variable is.
e.g.
what is the segment in lucene?
which asks for an advice when meeting multiple choice.e.g.
which is the best choice to indexing a boolean value in lucene?
where asks for location path url directory etc .
e.g.where is standardanalyzer?
why asks for reasons.
e.g.
why lucene merge indexes?
when asks about the time at which things happen.
e.g.when should one stop using lucene and graduateto solr?
who asks about the name or identity of a person orgroup of people.
e.g.
who created the c programming language?
whether asks for yes or no.
e.g.
should an index be optimized after incremental indexes in lucene?
others the rest question which are hard to be labeled.
phd students mater students and senior undergraduate students are asked to do cross validation on the labeling i.e.
if two persons have divergences on labeling one question a third person would discuss together untilthey have the same opinion on labeling this question.
thus the mistakes of question labeling are minimized.
fromtable1 themostfrequenttypeis howto f o l l o w e db y whether what why which where.r a r e whenandwhotype questions are labeled.
actually the taxonomy is not orthog onal i.e.
one question would be labeled as several tags .
thus we want to build classifiers for each type individually.
as the amount of whenandwhois much less than the amount of other types and the questions in othersdo not contain any interrogatives at all.
so we do not build classifiers for these types and focus on the main types i.e.
howto whether what why which where.
based on the l a b e l e dq u e s t i o ns e t w ec a ng e tal a b e l e da n s w e rs e t .
.
document representation we need to represent every answer document as a feature vector.
and two kinds of features i.e.
text feature andnon text feature are used in our approach.
a textual features the text of an answer gives an explanation to the question thus some words may closely relateto the question type.
to automatically extract informationfromthetext werepresenteachdocumentasabagofwords.
for these words we perform stop word removal and stemming .
in our work stop words include these four parts .
common stop words such as is am would etc.
.
java reserved words such as public static etc.
.
labels defined by stackoverflow such as code img a etc.
.
words related to specific projects such as lucene index search etc.
stemmingreducesawordtoitsroot forexample choose chooses choosing could all be reduced to choos .
this process can potentially increase the discriminative power of root words that are good at differentiating answers of a spe cific question type from answers of other types.
we choose snowball stemming algorithm 5to reduce the word to its root.
note that the root of noun and verb are different e.g.
explains explained are reduced to the stem ex plain but explanation and explanations are reduced to the stem explan .
b non textual features theprogramming answerusually contains code snippets and links.
empirically we think that the number of code snippets links and the length are relatedwith the question types e.g.
a how totype answer is more likely to contain code snippets in order to explain a how to type question step by step than other type answers.
thereare tags presented by stackoverflow e.g.
tag pair code code indicates the code snippers and a a indicates the links.
benefit from these tags we can extractthe code snippers and links conveniently.
these features are shown in table .
features w 1town are textual features features code are code features featureslink are link features features length are length features.
some features we used are similar with .
4a question post which contains several question sentences would be labeled with several tags e.g.
http stackoverflow.com questions .
we added more features in table such as codemalc code clc.
we add to indicate the different features.
based on the textualandnon textual analysis as described above the answer document can be represented as dj x1j x2j ... x nj wherexijrefers to one of these features in table .
table features for document representation id feature description w1towneach feature represents a stemmed word in the answer set and has a value corresponding to the number of times the word appears in the answer.
code num n. of code snippets code avc a v e r a g ec o d el e n g t h code mac maximum code length code mic minimum code length code sdc code length standard deviation code malc maximum code lines code milc minimum code lines code sdlc code lines standard deviation code clc n. of code lines link xlc n. of links to external sources link ilc n. of links to other query answer in the forum link lc n. of links length wc n. of words length sc n. of sentences length cc n. of characters .
feature selection we do the feature selection for two reasons one is that we expect there are some features with similar values contained in the same type answers but differ between differ ent type answers.
so we are interested in finding the bestfeatures for discriminating the different answer types.
theother reason is that the dimensional of our combined featurevector is more than one thousand we only have answers available for training and testing.
this makes discriminative learning extremely hard since most methods tend toover fit in training set.
we address both issues by seeking acompact low dimensional and readable representation thatfacilitates learning from a few samples.
among so many feature selection algorithms the information gain algorithm infogain suits our feature selectiontask most.
infogain is a statistical measure of how much agiven feature contributes to discriminate the class to which any given answer belongs.
all the features are ranked ac cording to the information gain metric.
with the help of infogain algorithm we can find the discriminative featuresfrom thousands of features.
in our experiment we selecttop features for each type.
.
model training the objective of model training is to build classifiers to automatically classify the answers into their correspondingtypes.
to achieve this objective we consider both classification and regression method.
the assumption of the latter is that the degree of different documents belonging to a spe cific type is not the same.
for example assuming there aretwo documents belonging to howtotype.
if document acontains more howtotype features than document b document acan get more score than b. we adopt two algorithms 117bayesian logistic regression blr for classification and linear regression lr for regression.
bayesian logistic regression it is a good classifier for text classification.
genkin et al.
first propose this approach for text categorization.
this approach uses a laplaceprior to avoid overfitting and produces sparse predictivemodels for text data.
among all the classification approaches blr performs best in our situation.
we choose it to build the classification model.
linear regression lr on the other hand performs regression.
lr is an approach to modeling the relationship between a scalar dependent variable yand one or more explanatory variables denoted x.g i v e n y i xi1 ... x ip n i 1of n statistical units a lr model assumes that the relationship between the dependent variable yiand the p vector of regressors xjis linear.
the reason we adopt the lr model is that assuming there are two documents if document a is more likely to answer a positive type question than docu mentb acan get a higher score.
.
re ranking in text retriev al as we mentioned above the type of retrieved document may differ from the input question type.
if the answerdocument can be identified correctly and automatically wecan pick out the right answer documents which have thesame type as the input question.
this is the institution behind the re ranking approaches.
we would introduce the tr tool first then introduce our re ranking approach re scoring based on the text retrieval.
.
text retrieval we use lucene as our baseline search tool for re ranking.
when given a natural language question we take two stepsto search the relevant documents pre processing we represent the input question as a bag of words and perform stop word removal and stemming on these words.
the delimiters that we used to split the question into words are all the characters excluding digit s e.g.
etc.
and letters e.g.
a b etc.
.
then weremove the first three types of stop words mentioned in sec tion .
.
and stem the remain words by snowball algorithm.
retrieval we use lucene engine to search the set of words and get the documents it returns.
.
re scoring after getting the ranked documents we can utilize the classifiers to do re scoring.
some former works use the clas sifiers to filter out the irrelevant documents e.g.
gottipati et al.
use the classifier to filter out the junks .
we namedthis re ranking approach as filter .
we adopt another way i.e.
re scoring to re rank the documents.
we combine estimated score with the original rank ing score to obtain the re ranking score.
in particular wemodel the re ranking score s ras the linear combination sr x y xi so x where xidenotes feature vector of each document using the discriminative features yis the blr classification function or lr regression function for estimating the possibility ofdocument belonging to the type of the input question ands ois the original ranking score.
the re ranking is obtained by sorting srin descending order.the score of blr is binary either .
or .
depending the answer was classified as the same type with the inputquestion or not .
to make the lr score comparable with blr score we apply the equation normalization to bring the lr regression score into range .
y i xi yi xi minm j 1yj xi maxmj 1yj xi minmj 1yj xi wheremis the number of sample yi xi i st h eithoutput value by lr regression and maxm j 1yj xi is the maximum value minm j 1yj xi is the minimum value.
.
ev aluation our experiments address two research questions rq1 what s the behaviour of the classifiers and what affects the behaviour of the classifiers?
rq2 does our re ranking approach really work?
does our approach performs better than other re ranking approach?
for the classifier evaluation we use answers with positive votes.
for each type a the positive samples are thosea labeled positive voted answers.
and we randomly selected the negative samples from the rest type answers as the same numbers as the positive samples.
then we use a ten fold cross validation method to evaluate the classi fiers.
thus all samples are randomly split into ten parts in each run one part is used as test set the remaining nineparts are used as training set.
the split on training and test sets is the same in all experiments.
the final results of each experiment represent the average of ten runs.
thepositive answer numbers of six main types are howto whether what why which where respectively.
for the re ranking evaluation we use questions with positive votes.
and we collect all the lucene taggedanswers as the repository.
we also use a ten fold cross validation method to evaluate the re ranking performance.
since the ranking evaluation metric needs a rating assessment i.e.
degree of relevance for each document of the giv en question we calculate the degree of relevance with thesetwostepsautomatically foreachquestion weregardthe corresponding positive vote answers as the relevant answers while other answers are irrelevant.
the degree of rele vance is measured by the score of the answer.
we score theminimum positive vote answer and the second minimumanswer etc.
while others are scored irrelevant .
.
rq1 classifier evaluation the feature sets and the threshold kare the two major factors which influence the classifiers effectiveness.
wewould introduce the evaluation criteria for the classifiersfirst.
then we conduct a series of experiments on differ ent feature sets and also on the varying threshold k. precision recall and f measure.
in order to evaluate the classifiers effectiveness we use precision recall and thef1 measure .
these measures have been commonly usedto evaluate the accuracy of various retrieval classification and mining algorithms .
performance on different feature sets we extract textual and non textual features for each question.
then we conduct a feature selection on the all features i.e.
a com bination of textual and non textual features vector.
thus we get four feature sets selected feature set all feature set 118textual feature set and non textual feature set.
we compare the best f1 score of these feature sets at the appropriate threshold k. the per type result is shown in table .
sfs afs tfs ntfsrepresent selected feature set all feature set textual feature set non textual feature set respectively table precision recall and f measure results for different question types with different featuresets ques.
p r f1 sfs afs tfs ntfs howtoprec.
.
.
.
.
recall .
.
.
.
f1 .
.
.
.
whetherprec.
.
.
.
.
recall .
.
.
.
f1 .
.
.
.
whatprec.
.
.
.
.
recall .
.
.
.
f1 .
.
.
.
whyprec.
.
.
.
.
recall .
.
.
.
f1 .
.
.
.
whichprec.
.
.
.
.
recall .
.
.
.
f1 .
.
.
.
whereprec.
.
.
.
.
recall .
.
.
.
f1 .
.
.
.
from table it could be noticed that the selected features perform best while text features and non text features i.e.
using a single set features performs worst in this situation.
as the selected feature set performs best for theblr and lr we will use selected features in the followingre ranking step.
performance on varying threshold k blr has a varying parameter kranging in .
.
.
when we decrease the parameter k the number of tp true positive and fn false negative increases.
table reports the result of wheretype answers with varying values kfor selected feature set and all feature set.
as we increase the parameter k the recall values of both features decrease and the precision values increase first then decrease when kis larger than .
.
in table whenksets .
the f1 measure on both selected feature set and all feature set come to maximum.
table performance on varying threshold kfor selected feature set and all feature set of where classifier kselected feature set all feature set prec.recall f1prec.recall f1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
summary as an answer to rq1 we show that the classifiers could achieve an f1 score of up to .
.
and the selected feature set is the best feature set for blr method.forthevaryingthreshold k wecanfindthattherecallwould decrease and the precision may increase first then decrease when the kincreases.
.
rq2 re ranking evaluation to evaluate the re ranking performance we compare our approach with other re ranking approaches including the pure tr approach .
we would introduce the evaluation criteria i.e.
ndcg k for the re ranking approach first.
then we illustrate these comparisons in detail.normalized discount cumulative gain ndcg .
the normalizeddiscountedcumulativegainattopk ndcg k for short is a ranking evaluation metric first proposed in .it allows us to measure how close the predicted answer rank ing is to the ground truth ranking.
more formally ndcg kis defined as ndcg k idcg kk summationdisplay i 12reli log2 i whereriis the true rating assessment for the answer at position i in the ranking and idcg kis a normalization factor.
idcg k means the discounted cumulative gain thesumpartinequation2 oftheidealranking i.e.
therankingwhere given a pair of answers a i aj aiis better ranked thanajifr prime iis greater than r prime j. since our work addresses the problem of ranking answers for software text retrieval we set the lucene search as thebaseline.
andwecomparewiththefilterapproach gottipatiet al.
and the lr re scoring approach.
figure shows the comparison of different methods.
we observe that almost all the re ranking methods outper formed the baseline lucene search in all types except thefilter in wheretype .
this result indicates that all the reranking methods show a better performance than baseline.
and our blr re scoring approach obtains an improvement of .
in ndcg and .
in ndcg upon thebaseline.
what we also observe is that the blr re scoringoutperformed the other methods for all values of k the ndcg k in the overall improvement in our dataset.
figure ndcg k of overall improvement in the overall improvement the filter performs worse than blr re scoring.
and the filter behaves unstably.
it obtainsthe best performance on the whatandwhytype while it also obtains the worst performance on the where e v e nw o r s e than the lucene search.
as for the lr re scoring we are a little disappointed at its performance.
after taking a look at the score of every document using lr method we find 119figure ndcg k of howto what where why which whether improvement for methods of blr re scoring blr filter lr re scoring and lucene search that most scores are close.
thus the result of lr re scoring is comprehensible.
summary forrq2 weshowthatourre rankingapproach performs significantly better than the pure tr approach which gains .
in ndcg over the tr approach.
andour approach also has an advantage over the filter approach and the re scoring approach using lr.
.
conclusion and future work inthispaper wepresentaninterrogative guidedre ranking approach for question oriented software text retrieval.
we build several software document classifiers which learn from a large number of question answer pairs on stack overflow.
then we apply the classifiers to our document repositoryand adopt a re scoring approach which combines the clas sification score and the text retrieval score.
the performance of our classifier still needs further improvement.
in our approach documents are classified auto matically by its corresponding question types.
in practice a document may answer several types of questions and maybe not limited to the type of its original question.
in the fu ture we will try some fuzzy learning model in our classifierso as to gain higher classification precision.
.