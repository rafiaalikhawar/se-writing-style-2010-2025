semantics based obfuscation resilient binary code similarity comparison with applications to software plagiarism detection lannan luo jiang ming dinghao wu peng liu sencun zhu the pennsylvania state university university park pa usa lzl144 jum310 dwu pliu ist szhu cse .psu.edu abstract existing code similarity comparison methods whether source or binary code based are mostly not resilient to obfuscations.
in the case of software plagiarism emerging obfuscation techniques have made automated detection increasingly di cult.
in this paper we propose a binary oriented obfuscationresilient method based on a new concept longest common subsequence of semantically equivalent basic blocks which combines rigorous program semantics with longest common subsequence based fuzzy matching.
we model the semantics of a basic block by a set of symbolic formulas representing the input output relations of the block.
this way the semantics equivalence and similarity of two blocks can be checked by a theorem prover.
we then model the semantics similarity of two paths using the longest common subsequence with basic blocks as elements.
this novel combination has resulted in strong resiliency to code obfuscation.
we have developed a prototype and our experimental results show that our method is e ective and practical when applied to real world software.
categories and subject descriptors d. .m software protection general terms security veri cation keywords software plagiarism detection binary code similarity comparison obfuscation symbolic execution theorem proving .
introduction with the rapid growth of open source projects software plagiarism has become a serious threat to maintaining a healthy and trustworthy environment in the software industry.
in there was an intellectual property lawsuit led by compuware against ibm .
as a result ibm paid permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
fse november hong kong china copyright acm ... .
.million in nes to license compuware s software and an additional million to purchase compuware s services.
examples such as this point to a critical need for computeraided automated software plagiarism detection techniques that are capable of measuring code similarity.
the basic research problem for code similarity measurement techniques is to detect whether a component in one program is similar to a component in another program and quantitatively measure their similarity.
a component can be a set of functions or a whole program.
detecting code similarity is faced with an increasing challenge caused by emerging readily available code obfuscation techniques by which a software plagiarist transforms the stolen code in various ways to hide its appearance and logic not to mention that often the plainti is not allowed to access the source code of the suspicious program.
existing code similarity measurement methods include clone detection binary similarity detection and software plagiarism detection.
while these approaches have been proven to be very useful each of them has its shortcomings.
clone detection e.g.
moss assumes the availability of source code and minimal code obfuscation.
binary similarity detection e.g.
bdi is binary code based but it does not consider obfuscation in general and hence is not obfuscation resilient.
software plagiarism detection approaches based on dynamic system call birthmarks have also been proposed but in practice they incur false negatives when system calls are insu cient in number or when system call replacement obfuscation is applied .
another approach based on core value analysis requires the plainti and suspicious programs be fed with the same inputs which is often infeasible.
consequently most of the existing methods are not e ective in the presence of obfuscation techniques.
in this paper we propose a binary oriented obfuscationresilient method named cop.
cop is based on a new concept longest common subsequence of semantically equivalent basic blocks which combines rigorous program semantics with longest common subsequence based fuzzy matching.
speci cally we model program semantics at three di erent levels basic block path and whole program.
to model the semantics of a basic block we adopt the symbolic execution technique to obtain a set of symbolic formulas that represent the input output relations of the basic block in consideration.
to compare the similarity or equivalence of two basic blocks we check via a theorem prover the pair wise equivalence of the symbolic formulas representing the output variables or registers and memory cells.
we then calculate the percentage of the output variables of the plainti block that have a semantically equivalent counterpart in the suspicious block.
we set a threshold for this percentage to allow some noises to be injected into the suspicious block.
at the path level weutilize the longest common subsequence lcs algorithm to compare the semantic similarity of two paths one from the plainti and the other from the suspicious constructed based on the lcs dynamic programming algorithm with basic blocks as the sequence elements.
by trying more than one path we use the path similarity scores from lcs collectively to model program semantics similarity.
note that lcs is di erent from the longest common substring.
because lcs allows skipping non matching nodes it naturally tolerates noises inserted by obfuscation techniques.
this novel combination of rigorous program semantics with longest common subsequence based fuzzy matching results in strong resiliency to obfuscation.
we have developed a prototype of cop using the above method.
we evaluated cop with several di erent experiments to measure its obfuscation resiliency precision and scalability.
benchmark programs ranging from small to large real world production software were applied with different code obfuscation techniques and semantics preserving transformations including di erent compilers and compiler optimization levels.
we also compared our results with four state of the art detection systems moss jplag bdi and darungrim2 where moss and jplag are source code based and bdi and darungrim2 are binary code based.
our experimental results show that cop has stronger resiliency to the latest code obfuscation techniques as well as other semantics preserving transformations and can be applied to real world software to detect code reuse or software plagiarism.
in summary we make the following contributions.
we propose cop a binary oriented obfuscation resilient method for software plagiarism or code reuse detection with strong obfuscation resiliency.
we propose a novel combination of rigorous program semantics with the exible longest common subsequence resulting in strong resiliency to code obfuscation.
we call this new concept the longest common subsequence of semantically equivalent basic blocks .
our basic block semantic similarity comparison is new in the sense that it can tolerate certain noise injection or obfuscation which is in sharp contrast to the rigorous veri cation condition or weakest precondition equivalence that does not permit any errors.
the rest of the paper is organized as follows.
section presents an overview of our method and the system architecture.
section introduces our basic block semantic similarity and equivalence comparison method.
section presents how we explore multiple paths in the plainti and suspicious programs and calculate the lcs scores between corresponding paths.
the implementation and experimental results are presented in section .
we analyze the obfuscation resiliency and discuss the limitations in section .
the related work is discussed in section and the conclusion follows in section .
.
overview .
methodology given a plainti program or component and a suspicious program we are interested in detecting components in the suspicious program that are similar to the plainti with respect to program behavior.
program behavior can be modeled at di erent levels using di erent methods.
for example one can model program behavior as program syntax.
obviously if two programs have identical or similar syntax they behave similarly but not vice versa.
as program syntax plaintiff binary front endlongest common subsequence of semanticallyequivalent basic block computation symbolic executionscore theorem proversuspicious binary basic block semantics similarity computationfigure architecture can be easily made di erent with semantics preserved this syntax based modeling is not robust in the presence of code obfuscation.
another example to model program behavior uses system call dependency graphs and then measure program behavior similarity with subgraph isomorphism.
this method is also not very robust against obfuscations as an adversary can replace system calls.
instead we propose to use formal program semantics to capture program behavior.
if two programs have the same semantics they behave similarly.
however formal semantics is rigorous represented as formulas or judgments in formal logics with little room to accommodate similarity instead of the equivalence relation.
if two programs or components have the same formal semantics their logical representations are equivalent.
if they are similar in terms of behavior their formal semantics in a logical representation may be nonequivalent.
that is it is hard to judge similarity of two logical representations of program semantics.
to address this problem we combine two techniques.
the rst is to model semantics formally at the binary code basic block level.
we not only model basic block semantics equivalence but also model similarity semantically.
the second is the longest common subsequence.
based on the basic block semantics equivalence or similarity we compute the longest common subsequence of semantically equivalent basic blocks between two paths one from the plainti component and the other from the suspicious.1the length of this common subsequence is then compared to the length of the plainti path.
the ratio calculated indicates the semantics similarity of the plainti path as embedded in the suspicious path.
note that the common subsequence is not compared to the suspicious path since noise could be easily injected by an adversary.
by trying more than one path we can collectively calculate a similarity score that indicates the semantics from the plainti component embedded in the suspicious potentially with code obfuscation or other semantics preserving program transformations applied.
in other words the program semantics is modeled collectively as path semantics based on basic block semantics and we compute a ratio of path semantics similarity between the plainti component and the suspicious.
the semantics similarity is modeled at two levels one at the binary code basic block level and one at the path level.
note that we are actually not intended to discover what the semantics are of the plainti and suspicious programs but rather to use the semantics to measure the basic block and path similarity and thus report a similarity score indicating the likelihood that the plainti component is reused with obfuscation or not legally or illegally.
.
architecture the architecture of cop is shown in figure .
the inputs are the binary code of the plainti component and suspi1here we refer semantically equivalent basic blocks to the blocks that are semantically similar with a score above a threshold which will be presented in the next section.cious program.
the front end then disassembles the binary code builds an intermediate representation and constructs controlow graphs and call graphs.
we then compute the longest common subsequence lcs of semantically equivalent basic blocks sebb .
we explore multiple path pairs to collectively calculate a similarity score indicating the likelihood of the plainti code being reused in the suspicious program.
to compute the lcs of sebb of two given paths we must be able to compute the semantic equivalence of two basic blocks.
the basic block similarity computation component in figure is for this purpose.
we rely on symbolic execution to get symbolic formulas representing the inputoutput relation of a basic block.
speci cally we compute a symbolic function for each output variable a register or memory cell based on the input variables registers and memory cells .
as a result a basic block is represented as a set of symbolic formulas.
the semantic equivalence of two basic blocks are then checked by a theorem prover on their corresponding sets of symbolic formulas.
since obfuscations or noise injection can cause small deviations on semantics leading to nonequivalent formulas we accommodate small deviations by collectively checking whether an output formula in one basic block has an equivalent one in the other with possible permutations of input variables.
we then set a threshold to indicate how semantically similar two basic blocks are.
when the score is above the threshold we regard them as equivalent during the lcs calculation.
the details of basic block semantics similarity and equivalence computation are presented in the next section.
.
block similarity comparison given two basic blocks we want to nd how semantically similar they are.
we do not compute their strict semantic equivalence since noise can be injected by an adversary.
.
strictly semantic equivalence here we describe how to compute the strictly semantic equivalence of two basic blocks.
take the following code snippet as an example p a b s x y q a b t x y for the sake of presentation we do not use assembly code.
at the binary code level these variables are represented as registers and memory cells.
these two code segments are semantically equivalent.
the only di erence is the variable names.
at binary code level di erent registers can be used.
via symbolic execution we get two symbolic formulas representing the input output relations of the left basic block.
p f1 a b a b q f2 a b a b similarly for the right basic block we have s f3 x y x y t f4 x y x y we then check their equivalence by pair wise comparison via a theorem prover and nd that a x b y p s and similarly for qandt.
strictly semantic equivalence checking asserts that there are equal number of input and output variables of two code segments and that there is a permutation of input and output variables that makes all the output formulas equivalent pair wise between two segments.
that is when one of the following formulas is true the two code segments are regarded as semantically equivalent.a x b y p s q t a x b y p t q s a y b x p s q t a y b x p t q s a similar method is used in binhunt to nd code similarity between two revisions of the same program.
this can handle some semantics preserving transformations.
for example the following code segment can be detected as semantically equivalent to the above ones.
s x t y s s t t x y however this method is not e ective in general when code obfuscation can be applied for example noise can be easily injected to make two code segments not strictly semantic equivalent.
.
semantic similarity instead of the above black or white method we try to accommodate noise but still detect semantic equivalence of basic blocks.
consider the following block of code.
u x v y s u v t x y r x two temporary variables uand v and a noise output variable rare injected.
strictly checking semantic equivalence will fail to detect its equivalence to the other block.
instead we check each output variable of the plainti block independently to nd whether it has an equivalent counterpart in the suspicious block.
in this way we get a x b y p s a x b y q t which are valid and conclude a similarity score of since there are only two output variables in the plainti block and both of them are asserted to be equivalent to some output variables in the suspicious block.
.
formalization since we do not know in general which input variable in one block corresponds to which input variable in the other block we need to try di erent combinations of input variables.
we de ne a pair wise equivalence formula for the input variable combinations as follows.
definition .
pair wise equivalence formulas of input variables given two lists of variables x and y n m. let y be a permutation of the variables in y. a pair wise equivalence formula on xandy is de ned as p x y n i xi i y wherexiand i y are theith variables in xand the permutation y respectively.
for each output variable in the plainti block we check whether there exists an equivalent output variable in the suspicious block with some combination of input variables pair wise equivalence.
push ebp r esp 1 r esp 1 0x4 mem r ebp 1 mov ebp esp r ebp 1 r esp 1 sub esp 0x40 r esp 1 r esp 1 0x40 mov ebx eax r ebx 1 r eax 1 and eax r eax 1 r eax 1 0x0 lea ecx r ecx 1 r ebx 1 sub esp 0x4 r esp 2 r esp 2 0x4 mov ebp mem r esp 2 mov ebp esp r ebp 2 r esp 2 sub esp 0x40 r esp 2 r esp 2 0x40 xchg eax ebx r temp r ebx 2 r ebx 2 r eax 2 r eax 2 r temp xor eax eax r eax 2 0x0 mov ecx ebx r ecx 2 r ebx 2symbolic inputs block r ebp 1 i0 r esp 1 i1 r eax 1 i2 symbolic inputs block r ebp 2 j0 r esp 2 j1 r eax 2 j2 outputs r ecx 1 i2 r eax 1 0x0 r ebx 1 i2 r esp 1 i1 0x44 r ebp 1 i1 0x4 mem i0 outputs r ecx 2 j2 r eax 2 r ebx 2 j2 r esp 2 j1 0x44 r ebp 2 j1 0x4 mem j0 figure basic block symbolic execution definition .
output equivalence given two basic blocks letx1andx2be the lists of inputs and y1andy2be the lists of output variables respectively if jx1j jx2j.
assume the rst block is the plainti block and the second the suspicious block.
formally we check 8y12y1 9y22y2 p x1 x2 p x1 x2 f1 x1 f2 x2 wheref1 x1 andf2 x2 are the symbolic formulas of y1 andy2obtained by symbolic execution respectively.
each output equivalence formula is checked by a theorem prover.
based on whether there is an equivalent output variable in the suspicious block for each plainti output variable we compute a semantic similarity score that indicates how much semantics of the plainti block has been manifested in the suspicious block.
assume there are noutput variables in the plainti block and we nd pof them have semantically equivalent counterparts in the suspicious block.
the semantics embedding of the plainti into the suspicious is calculated asp n. another method is to consider all the input variable pairwise permutations and check output variable equivalence simultaneously.
this method leads to more than exponential number of theorem prover invocations instead of quadraticon the number of output variables for each input variable combination which is another drawback of the aforementioned strictly equivalence checking method.
.
example our tool works on binary code.
figure shows the binary code and its symbolic execution of two semantically equivalent basic blocks the left one is the original basic block the right one is the corresponding obfuscated basic block.
the assembly instructions are in bold font.
due to the noise from syntax di erences caused by code obfuscation most state of the art binary di ng tools such as darungrim2 and bdi are unable to identify whether or not the two basic blocks are semantically equivalent.
based on our basic block comparison method cop is able to detect that the semantics of the original block has been mostlyembedded in the obfuscated block.
in addition it identi es di erent instructions that have the same semantics.
for example and eax is semantically the same as xor eax eax and lea ecx is semantically the same as mov ecx ebx .
.
path similarity comparison based on the basic block semantics equivalence checking i.e.
similarity score above a threshold we calculate a path embedding score for each linearly independent path of the plainti against the suspicious program using the lcs of semantically equivalent basic blocks.
.
starting blocks the lcs of semantically equivalent basic blocks computation is based on the modi ed longest path algorithm to explore paths from starting points.
we present how to identify the starting points from the plainti and suspicious programs and with two starting points how to explore linearly independent paths to compute the highest lcs score i.e.
the lcs score of the longest path .
it is important to choose the starting point so that the path exploration is not misled to irrelevant code parts in the plainti and suspicious programs.
we rst look for the starting block inside a function of the plainti program.
this function can be randomly chosen or picked by an investigator with pre knowledge of the plainti code.
to avoid routine code such as calling convention code inserted by compilers we pick the rst branching basic block a block ends with a conditional jump instruction as the starting block.
we then check whether we can nd a semantically equivalent basic block from the suspicious program.
this process can take as long as the size of the suspicious in terms of the number of basic blocks.
if we nd one or several semantically equivalent basic blocks we proceed with the longest common subsequence of semantically equivalent basic blocks calculation.
otherwise we choose another block as the starting block from the plainti program and the process is repeated.
.
linearly independent paths at the path level we select a set of linearly independent paths from the plainti program.
for each selected path we compare it against the suspicious program to calculate a path embedding score by computing the lcs of semantically equivalent basic blocks.
once the starting block of the plainti program and several candidate starting blocks of the suspicious program are identi ed the next is to explore paths to calculate a path embedding score.
for the plainti program we select a set of linearly independent paths a concept developed for path testing from the starting block.
a linearly independent path is a path that introduces at least one new node that is not included in any other linearly independent paths.
we rst unroll each loop in the plainti program once and then adopt the depth first search algorithm to nd a set of linearly independent paths from the plainti program.
.
longest common subsequence of semantically equivalent basic blocks the longest common subsequence between two sequences can be computed with dynamic programming algorithms .
however we need to nd the highest lcs score between a plainti path and many paths from the suspicious.
our longest common subsequence of semantically equivalent basic blocks computation is essentially the longest path problem with the incremental lcs scores as the edge weights.
the longest path problem is np complete.
as we have removed all back edges in order to only consider linearly independent1 na b c d 63a 4plaintiff linearly independent path psuspicious cfg 6m 5sps p3b2figure an example for path similarity calculation.
the black blocks are inserted bogus blocks.
there is an opaque predicate inserted in mthat always evaluates to true at runtime which makes the direct ow to the node infeasible.
figure the and tables store the intermediate lcs scores and the directions of the computed lcs respectively.
the three arrows on the left indicate the parent child relationship between two nodes in the suspicious program during the lcs computation.
for example in the computed lcs the parent node of node is node instead of node .
paths the plainti and suspicious programs are represented as directed acyclic graphs dags .
in such case the longest path problem of a dag gcan be converted to the shortest path problem of g derived from gby changing every weight to its negation.
since the resulted weight graphs contain negative weights the dijkstra s shortest path algorithm is not applicable but still the problem is tractable as other shortest path algorithms such as bellman ford can be applied.
instead we adopt breadth rst search with interactive deepening combined with the lcs dynamic programming to compute the highest score of longest common subsequence of semantically equivalent basic blocks.
for each step in the breath rst dynamic programming algorithm the lcs is kept as the longest path computed so far for a basic block in the plainti program.
algorithm shows the pseudo code for the longest common subsequence of semantically equivalent basic blocks computation.
the inputs are a linearly independent path p of the plainti program the suspicious program g and a starting point sof the suspicious program.
our algorithm uses the breadth rst dynamic programming lcs to explore the suspicious program.
the intermediate lcs scores are stored in a memoization table .
an index rpoints to the current row of the memoization table.
the table is di erent from the conventional dynamic programming memoization table in that is a dynamic table.
each time we encounter a new node or a node with higher lcs scores a new row is created in the table.
the table is used to store the directions of the computed lcs .
thealgorithm longest common subsequence of semantically equivalent basic blocks the lcs dynamic programming memoization table r the current row of the table the direction table for the lcs search the array stores the intermediate lcs scores n the length of the plainti path p function pathsimilaritycomparison p g s enq s q insert s into queue q initialize the lcs table initialize the array to all zero r set the current row of table while q is not empty do currnode deq q foreach neighbor u of currnode do lcs u p end for end while maxr i i n get the the highest score if then higher than the threshold re nelcs maxr i i n end if return end function function lcs u p u foreach node v of p do ifsebb u v then semantically eq.
blocks u v parent u parent v u v if u r v then r end if else u v max parent u v u parent v u v or end if if u r v then u r v enq u q end if end for end function vector is used to store the intermediate highest lcs scores for each node.
the inputs of the function lcs are a node uof the suspicious program and the linearly independent path p. function lcs calculates the lcs of two paths where the rst path is denoted by a node in the suspicious program.
the lcs path computed so far at its parent node current node in the algorithm is augmented with this node to form the suspicious path.
the function sebb tells whether two basic blocks are semantically equivalent or not.
the re nelcs function re nes the computed lcs so far by merging potential split or obfuscated basic blocks see the next subsection for the lcs re nement .
the detailed process works as follows using figure as a running example.
the intermediate lcs scores stored in and the directions of the computed lcs stored in are showed in figure .
we rst set sas the current node and insert it into the working queue q line then we initialize with one row in which each element equals to the rstrow in figure and initialize the scores stored in of all nodes in the suspicious to line .
for its neighbor node we found a node of psemantically equivalent to it and its new score calculated by the function lcs line and line is higher than its original one thus a new row is created in line the second row in figure .
next we update the score of node and insert it into the working queue line and line .
during the second iteration line for node we cannot nd a node in pthat is semantically equivalent to either of its neighbors node mor nodea no new row is added to .
both their new scores are higher than their original ones hence their scores are updated and both them are inserted into the working queue line and line .
the third iteration have two current nodes node m and nodea.
for nodem its neighbor node ndoes not have a semantically equivalent node in p hence no new row is added to and nodenis inserted into the working queue after its score is updated.
another neighbor node 5has a semantically equivalent node in p hence a new row is added to line see the third row in figure and it is inserted into the working queue after updating its score.
for node a we cannot nd a node in pthat is semantically equivalent to either of its neighbors node bor nodec hence no new row is added to and the scores of both node band nodecare updated and both nodes are inserted into the working queue.
during the forth iteration node nhas a neighbor node which has a semantically equivalent node in pand it gets a new score higher than its original one thus a new row is added into see the forth row in figure .
to calculate its new sore the function lcs needs rst to nd its parent node which is node and uses the cell value of the row with respect to nodes to calculate each cell value of a new row shown in figure .
then the right most cell value of this new row is the new score for node .
the process repeats until the working queue is empty.
when the working queue is empty we obtain the highest score from the right most column of line and compare it with a threshold line .
if it is higher than the threshold the re nelcs will update the table see the next subsection and a new highest score will be obtained line otherwise the lcs computation is completed.
here we use the example in figure to illustrate a few interesting points.
the rst is how to deal with opaque predicate insertion.
the node mis such an example.
since our path exploration considers both branches we do not need to solve the opaque predicate statically.
our approach does not need to consider path feasibility but focuses on shared semantically equivalent basic blocks.
the second interesting scenario is when some basic blocks are obfuscated.
for example node in pis split into two blocks and embedded intogas node aand node b. in this case the basic block similarity comparison method determines neither node 3a nor node 3bis semantically equivalent to node in p. to address this the lcs re nement which tentatively merges unmatched blocks has been developed.
.
refinement here we discuss some optimization techniques we developed to improve the obfuscation resiliency which are implemented in the lcs re nement.
conditional obfuscation.
conditionals are speci ed by the ags state in the flags registers i.e.
cf pf af zf sf and of .
these ags are part of the output state of a basic block.
however they can be obfuscated.
we handle this by merging blocks during our lcs computation.
no matter what obfuscation is applied eventually a semantically equivalent path condition must be followed to execute asemantically equivalent path.
thus when obfuscated blocks are combined we will be able to detect the similarity.
basic block splitting and merging.
the lcs and basic block similarity comparison algorithms we presented so far cannot handle basic block splitting and merging in general.
we solve this problem by the lcs re nement.
first cop nds the consecutive basic block sequences which do not have semantically equivalent counterparts in the suspicious through backtracking.
then for each such sequence or list cop merges all basic blocks as well as two basic blocks which are right before and after the sequence into one code trunk.
finally it adopts a method similar to the basic block comparison method to determine whether or not two corresponding merged code trunks one from the plainti and the other from the suspicious are semantically equivalent or similar.
if the two merged code segments are semantically equivalent the current longest common subsequence of semantically equivalent basic blocks is extended with the code segment in consideration.
this method can handle basic block reordering and noise injection as well.
one may wonder when blocks are merged the intermediate e ects stored on the stack may be missed.
however since we consider both memory cells and registers as input and output variables of basic blocks the intermediate e ects are not missed.
.
function and program similarity comparison once we have computed the path similarity scores the lengths of the resulted lcs we calculate the similarity score between the two functions.
we assign a weight to each calculated lcs according to the plainti path length and the function similarity score is the weighted average score.
for each selected function in the plainti program we compare it to a set of function in the suspicious program identi ed by the potential starting blocks see section .
and the similarity score of this function is the highest one among those.
after we calculate the similarity scores of the selected plainti functions we output their weighted average score as the similarity score of the plainti and suspicious programs.
the weights are assigned according to the corresponding plainti function size.
.
implementation and ev aluation .
implementation our prototype implementation consists of lines of c code measured with cloc .
the front end of cop disassembles the plainti and suspicious binary code based on ida pro.
the assembly code is then passed to bap to build an intermediate representation the same as that used in binhunt and to construct cfgs and call graphs.
the symbolic execution of each basic block and the lcs algorithm with path exploration are implemented in the bap framework.
we use the constraint solver stp for the equivalence checking of symbolic formulas representing the basic block semantics.
.
experimental settings we evaluated our tool on a set of benchmark programs to measure its obfuscation resiliency and scalability.
we conducted experiments on small programs as well as large real world production software.
we compared the detection e ectiveness and resiliency between our tools and four existing detection systems moss jplag bdi and darungrim2 where moss and jplag are source code based while bdi and darungrim2 are binary codebased.
moss is a system for determining the similarity of programs based on the winnowing algorithm for document ngerprinting.
jplag is a system that nds similarities among multiple sets of source code les.
these two systems are mainly syntax based and the main purpose has been to detect plagiarism in programming classes.
bdi is a binary di ng tool similar to di for text.
darungrim2 is a state of the art patch analysis and binary di ng tool.
our experiments were performed on a linux machine with a core2 duo cpu and 4gb ram.
in our experiments we set the basic block similarity threshold to 7and require the selected linearly independent paths cover at least of the plainti program.
.
thttpd the purpose of the experiments of thttpd openssl see section .
and gzip see section .
is to measure the obfuscation resiliency of our tool.
in our rst experiment we evaluated thttpd .25b and sthttpd .
.
where sthttpd is forked from thttpd for maintenance.
thus their codebases are similar with many patches and new building systems added to sthttpd .
to measure false positives we tested our tool on several independent program some of which have similar functionalities.
these programs include thttpd .25b atphttpd .4b boa .
.
and lighttpd .
.
.
in all of our experiments we select of the functions in the plainti program or component randomly and test each of them to nd similar code in the suspicious program.
for each function selected we identify the starting blocks both in the plainti function and the suspicious program see section .
.
.
resilience to transformation via different compiler optimization levels di erent compiler optimizations may result in di erent binary code from the same source code but preserve the program semantics.
we generated di erent executables of thttpd .25b and sthttpd .
.
by compiling the source code using gcc g with di erent optimization options o0 o1 o2 o3 and os .
this has produced executables.
we cross checked each pair of the executables on code reuse detection and compared the results with darungrim2 a state of the art patch analysis and binary di ing tool.
figure shows the results with respect to four of the ten executables compiled by optimization levels o0 and o2.
results with other optimization levels are similar and we do not include them here due to the space limitation.
from figure we can see our tool cop is quite e ective compared to darungrim2 .
both cop and darungrim2 have good results when the same level of optimizations is applied see the left half of figure .
however when di erent optimization levels o0 and o2 are applied the average similarity score from darungrim2 is only about while cop is able to achieve an average score of .
to understand the factors that caused the di erences we examined the assembly codes of the executables and found these di erences were mainly caused by di erent register allocation instruction replacement basic block splitting and combination and function inline and outline.
due to the syntax di erences caused by di erent register allocation and instruction replacement darungrim2 is unable to determine the semantic equivalence of these basic blocks while cop is able to identify these blocks as very similar or identical.
two interesting cases are worth mentioning.
the rst case is the basic block splitting and combination.
one example is the use of conditional move instruction e.g.
cmovs .
we found that when thttpd .25b was compiled with o2 there was only one basic block using the cmovs instruction when t2 vs s0 s2 vs t0 t0 vs s2 s0 vs t2 t2 vs t0 s2 vs s0 s0 vs s2 t0 vs t2 s0 vs t0 t0 vs s0 s2 vs t2 t2 vs s2 s2 vs s2 s0 vs s0 t2 vs t2 t0 vs t0100 0similarity score cop darungrim2figure code similarity scores resulting from different compiler optimization levels.
higher is better since these two programs share codebase.
legend tiand s istand for thttpd and sthttpd compiled with oi respectively.
it was compiled with o0 there were two basic blocks.
cop addresses this by merging neighboring blocks through the lcs re nement.
as a result cop found the two basic blocks compiled by o0 when merged were semantically equivalent to the one block compiled by o2.
another interesting case is function inline and outline.
there are two basic scenarios.
one is that the inlined outlined function is a user de ned or statically linked library function another is that the inlined outlined function is from a dynamically linked library function.
let us take dedotdot a user de ned function in thttpd .25b as an example.
the function is inlined in httpd parse request when it is compiled with o2 but not inlined with o0.
it is similar for sthttpd .
.
.
cop handles this by inlining the callee function since its code is available during the lcs computation.
in the second scenario where the inlined outlined function is a dynamically linked library function e.g.
strspn cop is not able to inline the callee function which may result in a lower similarity score.
however inlining some function here and there does not signi cantly a ect the overall detection result of cop since we can test all the functions.
one may wonder whether an adversary can hide stolen code in a dynamically linked library.
our assumption is that the source or binary code of the plainti program and at least binary code of the suspicious program is available for analysis.
although cop relying on static analysis has some di culty to resolve dynamically linked library calls it is able to analyze the dynamically linked library as long as it is available and identify the stolen code if exists.
.
.
resilience to transformation via different compilers we also tested cop on code compiled with di erent compilers and compared with darungrim2 .
we generated di erent executables of thttpd .25b and sthttpd .
.
using di erent compilers gcc and icc with the same optimization option o2 .
figure shows the detection results.
with di erent compilers the di erences between the resulted code are not only caused by di erent compilation and optimization algorithms but also by using di erent c libraries.
gcc uses glibc while icc uses its own implementation.
the evaluation results show that cop still reports good similarity scores although a little bit lower than those of using the same compiler but darungrim2 failed to recognize the similarity.
.
.
resilience to code obfuscations to evaluate the obfuscation resiliency we used two commercial products semantic designs inc. s c obfuscator table detection results resilience to single code obfuscation obfuscation similarity score category transformationsource code based binary code based moss jplag darungrim2 bdi cop layoutremove comments space and tabs replace symbol names number and strings controlinsert opaque predicates inline method outline method interleave method convert multiple returns to one return controlow attening swap if else bodies change switch case toif else replace logical operators ?
etc.
with if else datasplit structure object insert bogus variables table detection results resilience to multiple code obfuscation obfuscationsimilarity score source code based binary code based moss jplag darungrim2 bdi cop insert opaque predicates convert multiple returns to one return inline method outline method interleave method insert bogus variables swap if else bodies split structure object ti vs sg si vs sg si vs tg sg vs si ti vs si sg vs ti tg vs si ti vs tg tg vs ti si vs ti sg vs tg tg vs sg si vs si sg vs sg ti vs ti tg vs tg100 0similarity score cop darungrim2 figure code similarity scores resulting from different compilers.
higher is better since these two programs share codebase.
legend p cstands for programpcompiled with compiler c wherepis eithertforthttpd orsforsthttpd andcis eitherg for gcc or ifor icc respectively.
and stunnix s cxx obfuscator as the source code obfuscation tools and two open source products diablo and loco as the binary code obfuscation tools.
we also utilized cil as another source code obfuscation tool.
cil possesses many useful source code transformation techniques such as converting multiple returns to one return changing switch case to if else and replacing logical operators ?
etc.
with if else.
component vs. suspicious.
in the previous tests we evaluated the similarity between two programs.
in this test we evaluated whether a component from the plainti program is reused by a suspicious program.
the experiments we conducted with di erent compilers and compiler optimizations between thttpd andsthttpd can be viewed as a special case of the component vs. suspicious scheme.
the motivation is that for software plagiarism or code reuse scenarios the original software developers often have insights on the plainti program and can point to the critical component.
therefore we can test critical components to see whether they are reusedin the suspicious program.
in this experiment we test on a small component function httpd parse request vs.thttpd and md5 vs. openssl .
in our subsequent experiment we test in large program components the gecko rendering engine vs. the firefox browser.
the obfuscation techniques can be divided into three categories layout controlow and dataow obfuscation .
each category contains di erent obfuscation transformations.
we chose typical obfuscation transformations from all the three categories to obfuscate thttpd and then compiled the obfuscated code to generate the executables.
we compared the detection results of cop with those of four stateof the art plagiarism detection systems including moss jplag darungrim2 and bdi .
we evaluated on code with a single and multiple obfuscations applied.
the single and multiple obfuscation results are shown in table and table respectively.
we analyzed how cop addresses these obfuscation techniques.
the layout obfuscations do not a ect binary code but impair the source code based detection systems.
the data obfuscations also do not a ect cop because its basic block comparison method is capable of addressing noise input and output and is insensitive to data layout changes.
controlow obfuscations reduce quite a bit the scores reported by moss jplag darungrim2 and bdi but have little impact on cop.
we analyzed the obfuscation that changes the switch case statements to if else statements.
this obfuscation is done by cil as source to source transformation in our experiment.
gcc applied an optimization on the switch case statements.
it generated either a balanced binary search tree or a jump table depending on the number of the case branches.
we then conducted further experiments on this case.
when gcc generated a balanced binary search tree code the similarity scores between two code segments one contains switch case statements and the other contains the corresponding if else statements reported by moss jplag darungrim2 bdi and cop are and respectively.
when gcc generated a jump table the similarity scores are and respectively.
the result shows our method is quite resilient to advanced code obfuscations.we especially note that existing tools are not resilient to the control ow attening obfuscation which transforms the original control ow with a dispatch code that jumps to other code blocks.
control ow attening has been used in real world for software software protection.
for example apple s fairplay code has been obfuscated with control ow attening.
clearly this defeats syntax based methods.
cop is able to get good score against control ow attening because of our symbolic execution based path exploration and basic block semantics lcs similarity calculation method.
.
.
independent programs to measure false positives we also tested cop against four independently developed programs thttpd .25b atphttpd0.4b boa .
.
and lighttpd .
.
.
very low similarity scores below were reported.
.
openssl this experiment also aims to measure the obfuscation resiliency.
we rst evaluated openssl .
.1f openssh6.5p1 cyrus sasl .
.
and libgcrypt .
.
where openssh .5p1 cyrus sasl .
.
and libgcrypt .
.
use the library libcrypto.a from openssl .
.1f .
we tested our tool on completely irrelevant programs to measure false positives.
these programs include attr .
.
and acl2.
.
.
in this experiment we test whether the suspicious programs contain an md5 component from the plainti program openssl .
.1f .
the md5 plainti component was compiled by gcc with the o2 optimization level.
to measure obfuscation resiliency we conducted the similar experiments as onthttpd .
the detection results showed that openssh .5p1 which is based on openssl .
.1f contains the md5 component of openssl .
.1f .
their similarity scores were between and with obfuscations applied.
we especially noted that the scores for openssl .
.1f vs.libgcrypt .
.
andcyrus sasl .
.
with obfuscations applied are between and .
with further investigation we con rmed that although both libgcrypt .
.
andcyrus sasl .
.
are based onopenssl .
.1f their md5 components are re implemented independently.
for the completely irrelevant programs acl2.
.
and attr .
.
very low similarity scores below were reported.
.
gzip in our third experiment we rst evaluated our tool on gzip .
against its di erent versions including gzip .
gzip .
gzip .
.
and gzip .
.
.
we also tested gzip1.6against two independent programs with some similar functionalities bzip2 .
.
and advancecomp .
to measure false positives.
the plainti program gzip .
was compiled by gcc with the o2 optimization level.
to measure obfuscation resiliency we conducted the similar experiments as on thttpd andopenssl .
the results showed that the similarity scores between gzip .
and gzip .
gzip .
and gzip .
gzip .
and gzip .
.
and gzip .
and gzip .
.
are and respectively.
moreover when various obfuscation were applied the average similarity score only reduced around indicating that our tool is resilient to obfuscation.
from the results we can see that the closer the versions the higher the similarity scores.
for the independent programs bzip2 .
.
and advancecomp .
very low similarity scores below were reported.
because we have presented a detailed analysis of how our tool addresses various obfuscation techniques for the experiment of thttpd due to the space limit we do not include the similar analysis here.
firefox firefox firefox firefox firefox firefox .
firefox .
firefox .
firefox .
40similarity scoregeck o .
.
geck o .
.
geck o .
.
geck o .
.
geck o geck o geck o geck o geck o 8figure gecko vs. firefox .
gecko to measure the scalability of detecting large real world production software we chose the gecko layout engine as the plainti component and evaluated it against the firefox web browser.
we select versions of firefox each of which includes a di erent version of gecko.
thus we have plainti components gecko versions and suspicious programs.
we cross checked each pair of them and the results are shown in figure .
the line graph contains lines.
the results showed that the closer two versions are the more similar their code is .
especially.
the highest score of each line is the case where the gecko version is included in that firefox version.
to measure false positives we also checked cop on gecko vs. versions of opera .
.
.
and .
and versions of google chrome .
.
and .
which do not use gecko as layout engine.
cop reported scores below for all cases.
.
discussion .
obfuscation resiliency analysis the combination of the rigorous program semantics and the exibility in terms of noise tolerance of lcs is powerful.
here we brie y analyze its obfuscation resiliency.
obfuscation can be classi ed into three categories layout controlow and dataow obfuscations .
layout obfuscation.
because cop is a semantic based plagiarism detection approach layout obfuscation e.g.
comments space and tabs removal identi er replacement etc.
does not have any e ect.
controlow obfuscation.
cop deals with basic block splitting and combination by merging blocks.
basic block reordering can also be handled by merging blocks.
after merging the order does not matter to the symbolic execution used in the basic block similarity comparison if there is no dependency between two blocks or instructions.
instruction reordering is also taken care by the symbolic execution.
function inline and outline is handled by inter procedural path exploration in the lcs computation.
it is virtually impossible to solve opaque predicates however cop can tolerate unsolvable conditions since it explores multiple possible paths in the suspicious program for the lcs computation.
cop also has no di culty on controlow attening branch swapping and switch case to if else conversion obfuscation since it is based on the path semantics modeling which naturally takes controlow into consideration these are also analyzed and illustrated in our evaluation section .
it is similar for the obfuscation that converts multiple returns to one return .
the obfuscation that replaces logical operators can be handled by symbolic execution and path semantics modeling with lcs.
rewriting a loop including reversing a loop and unrolling a loop is another possible counterattack.however automatic loop rewriting is very di cult because they could result in semantically di erent programs.
so far we are not aware of such tools to our best knowledge.
one might manually reverse or unroll a loop but its impact could be very limited in a large program moreover it requires a plagiarist understands the loop and involves a lot of manual work which compromises the initial purpose of plagiarism.
dataow obfuscation.
there are two scenarios.
the rst scenario is that the obfuscation is applied inside a basic block e.g.
bogus variables insertion in a basic blocks.
since our basic block semantics similarity comparison can tolerate variations in the suspicious block it has no e ect no matter how many bogus variables are inserted in the suspicious block except for increased computation cost.
note the block from the plainti is not obfuscated and the base of block comparison is the plainti block.
in the other scenario obfuscation is applied in a inter block manner.
an example is splitting structure objects and dispersing each part into several basic blocks.
since we compare semantics at the machine code level and merge multiple block when it is necessary this attack can be dealt with unless an object is split into two basic blocks far away enough that they are not merged in cop.
.
limitations cop is static with symbolic execution and theorem proving.
it bears the same limitations as static analysis in general.
for example static analysis has di culty in handling indirect branches also known as computed jumps indirect jumps and register indirect jumps .
this problem can be addressed to some extent by value set analysis vsa .
in addition a plagiarist can pack or encrypt the original code e.g.
vmprotect and code virtualizer our current tool does not handle such cases.
symbolic execution combined with automated theorem proving is powerful but has its own limitations.
for example for theorem proving it cannot solve the opaque predicates or unsolved conjectures e.g.
the collatz conjecture but the impact could be very limited in large programs.
also its computational overhead is high.
in our experiment with thttpd and sthttpd it took as long as an hour to complete and in our gecko vs. firefox experiment it took half a day.
currently we perform brute force search to nd pairs of semantically equivalent basic blocks with which to start the path exploration and lcs computation.
we plan to develop heuristics and optimizations to minimize the calls to the symbolic execution engine and theorem prover in the future.
.
related work there is a substantial amount of work on the problem of nding similarity and di erences of two les whether text or binary.
the classic unix di and di and its windows derivation windi compare text les.
we discuss the work focusing on nding software semantic di erence or similarity.
.
code similarity detection symdi is a language agnostic semantic di tool for imperative programs.
it presents di erential symbolic execution that analyzes behavioral di erences between di erent versions of a program.
to facilitate regression analysis ho man et al.
compared execution traces using lcs.
our work is mainly motivated with obfuscation in the context of plagiarism detection while these works do not consider obfuscation.
our work is also di erent from binary di ng tools based mainly on syntax e.g.
bsdi bspatch xdelta jdi etc.
.
purely syntax based methods are not e ective in the presence of obfuscation.
some latest binary di ngtechniques locate semantic di erences by comparing intraprocedural control ow structure .
although such tools have the advantage of being more resistant to instruction obfuscation techniques they rely heavily on function boundary information from the binary.
as a result binary di ng tools based on control ow structure can be attacked by simple function obfuscation.
ibinhunt overcomes this problem by nding semantic di erences in inter procedural control ows.
cop adopts similar basic block similarity comparison method but goes a step further in this direction by combining block comparison with lcs.
.
software plagiarism detection mainly motivated by obfuscation our work compares better with software plagiarism detection work.
we discuss here the more relevant research along several axes.
static plagiarism detection or clone detection.
this type work includes string based ast based tokenbased and pdg based .
the methods based on source code are less applicable in practice since the source code of the suspicious program is often not available for analysis.
in general this category is not e ective when obfuscation can be applied.
dynamic birthmark based plagiarism detection.
several dynamic birthmarks can be used for plagiarism detection including api birthmark system call birthmark function call birthmark and core value birthmark.
tamada et al.
proposed an api birthmark for windows application.
schuler et al.
proposed a dynamic birthmark for java.
wang et al.
introduced two system call based birthmarks which are suitable for programs invoking many di erent system calls.
jhi et al.
proposed a core value based birthmark for detecting plagiarism.
core values of a program are constructed from runtime values that are pivotal for the program to transform its input to desired output.
zhang et al.
further proposed the methods of n version programming and annotation to extract the core value birthmarks for detecting algorithm plagiarism.
a limitation of core value based birthmark is that it requires both the plainti program and suspicious program to be fed with the same inputs which sometimes is di cult to meet especially when only a component of a program is stolen.
thus core value approach is not applicable when only partial code is reused.
.
others symbolic path exploration combined with test generation and execution is powerful to nd software bugs.
in our lcs computation the path exploration has a similar fashion but we only discover linearly independent paths to cover more blocks with few paths.
.
conclusion in this paper we introduce a binary oriented obfuscationresilient software plagiarism detection approach named cop based on a new concept longest common subsequence of semantically equivalent basic blocks which combines the rigorous program semantics with the exible longest common subsequence.
this novel combination has resulted in more resiliency to code obfuscation.
we have developed a prototype.
our experimental results show that cop is e ective and practical when applied to real world software.
.