automatic detection of nocuous coordination ambiguities in natural language requirements hui yang1 alistair willis1 anne de roeck1 bashar nuseibeh1 1department of computing the open university milton keynes mk7 6aa uk h.yang a.g.willis a.deroeck b.nuseibeh open.ac.uk2lero university of limerick limerick ireland basher.nuseibeh iero.ie abstract natural language is prevalent in requirements documents.
however ambiguity is an intrinsi c phenomenon of natural language and is therefore present in all such documents.
ambiguity occurs when a sentence can be interpreted differently by different read ers.
in this paper we describe an automated approach for charac terizing and detecting so called nocuous ambiguities which carry a high risk of misunderstanding am ong different readers.
given a natural language requirements docum ent sentences that contain specific types of ambiguity are first extracted automatically from the text.
a machine learning algorith m is then used to determine whether an ambiguous sentence is nocuous or innocuous based on a set of heuristics that draw on human judgments which we collected as training data.
we im plemented a prototype tool for nocuous ambiguity identification nai in order to illustrate and evaluate our approach.
the tool focuses on coordination ambiguity.
we report on the results of a se t of experiments to assess the performance and usefulness of the approach.
categories and subject descriptors d. .
elicitation methods language methodologies tools general terms management measurement pe rformance experimentation keywords natural language requirements nocuous ambiguity coordination ambiguity machine lear ning human judgments .
introduction natural language nl is still prev alent in the vast majority of requirements documents .
one im portant reason for this is that nl can help various stakehol ders articulate and communicate requirements during the entire life cycle of the software development.
however nl requirement s also suffer from typical nl problems such as ambiguity.
am biguity occurs when a single linguistic expression can be interp reted differently by different readers.
ambiguous expressions in requirements can be potentially dangerous when they result in poor requirements quality .
our research is motivated by the need to reduce the costs of mis understandings that can occur dur ing requirements engineering when these misunderstandings are due to ambiguities in the nl requirements.
our practical goal is to provide a tool to assist writers of requirements documents by alerting them to potentially harmful ambiguities called nocuous ambiguities .
unlike innocuous ambiguities which tend to be interpreted in the same way by all readers nocuous ambi guities give rise to different interpretations by different reader s thus contributing to misunderstandings between stakeholders.
su ch a tool needs to highlight those linguistic expressions in re quirements that are recognized as nocuous ambiguities and allow the wr iters to return to elicitation or rephrase for the purpose of improving requirements quality and facilitating effective communication of these requirements among different stakeholders.
in earlier work we proposed a general methodology for automatic identification of nocuous ambiguity which we use to guide our research on two types of ambiguity coordination ambi guity and anaphora ambiguity .
in contrast to other work which is intended to resolv e ambiguity our research concerns identification of those am biguities that are likely to lead to misunderstandings between diffe rent readers while discounting those which tend to be interprete d in the same way by different readers despite their surface features.
as such we consider ambi guity as a property of the relationship between a text and a group of interpreters rather than a property of a text or expression per se.
we also add the categorization of nocuous and innocuous ambiguity depending on the likely distribution of interpretations held by a group of readers of that text.
we have observed that not all cases of the ambiguity are actually dangerous in fact most remain unnoticed and are resolved to the same interpretation by permission to make digital or hard copi es of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
ase september antwerp belgium.
copyright acm ... .
.
all stakeholders.
only nocuous ambiguity cases that have a high risk of misunderstanding between different readers are truly disruptive and deserving of further attention.
our previous work focused on coordination ambiguity a particularly common kind of structural ambiguity highly preva lent in requirements documents.
we investigated a methodology that used a number of heuristics based on corpus based statistical information together with human judgments to predict whether a coordination ambiguity may be misunderstood for a given ambiguity threshold.
however our fi rst system was semi automatic it relied on manual selection of c oordination ambiguity instances from a number of requirements docum ents.
in this paper we extend our previous work on coordination ambiguity by introducing new functional process modules su ch as the extraction of ambiguous coordination instances and the recognition of coordination constituents contained in c oordination constructions.
these are necessary for automatic identification of nocuous coordination ambiguity.
moreover we implem ented a prototype tool called nocuous ambiguity identification nai to illustrate and evalu ate our approach.
given an nl requirements document our tool can generate a workflow that integrates with several required functional modules to automati cally detect nocuous ambiguity.
the work in this paper differs from our previous work in the following four ways first we bring in information extraction techniques to tackle some probl ems in automatic detection of coordination ambiguity which incl ude pattern based matching to detect ambiguous instances contai ned in the sentences and named entity recognition ner techniques for the extraction of coordination constituents such as coordinating conjuncts and the at tached modifiers.
second we in troduce two more heuristics collocation frequency in the local document and semantic similarity.
these enrich previous heuristics and explore further aspects of coordination ambiguity that may lead a reader to prefer a particu lar reading.
third we employ the logitboost algorithm for the building of a so called nocuity cl assifier .
we show that this machine learning algorithm performs better than the logistic regression algorithm that was used in our previous work .
fourth we implement an automate d tool to detect and highlight on screen nocuous ambiguity in te xt.
we are not aware of any comparable automated tools.
the rest of the paper is structur ed as follows.
in section we introduce the ambiguity problem in requirements documents.
section provides the detailed de scription of individual functional modules in the framework of the nai tool used for coordination ambiguity.
the building of a coordination ambiguity data set and the construction of the nocu ity classifier are described in section .
experimental setup a nd results are reported in section .
section discusses related wo rk and conclusions and future work are presented in section .
.
the ambiguity problem ambiguity is a phenomenon inherent in natural language.
it occurs everywhere in natural langua ge requirements.
for example in a total of requirements engi neering documents we collected for the study there were sentences containing coordination ambiguity instances which make up about .
of all the sen tences in total in this dataset.
computationally structural ambiguity can be recognised when a parser assigns more than one po ssible parse to a sentence.
coordination ambiguity is one of th e particularly common kinds of structural ambiguity.
a coordination structure connects two words phrases or clauses togeth er via a coordination conjunction e.g.
and or etc .
consider the following real examples e1.
they support a typing system for architectural components and connectors .
e2.
it is described the size of vector based input and output .
in the example e1 the coordination construction architectural components and connectors can be bracketed as architectural that is interpreted as architectural components and architectural connectors o r a s and in which case it is only component which is modified by architectural .
however in the example e2 although the coordination construction vectorbased input and output can also be interpreted as vector based or and it seems that the former reading is most likely to be preferred perhaps because of semantic similarity between input and output.
when presented to human judges for example e1 out of judges committed to the reading architectural components and connectors whereas of the remaining judges chose the reading and information about human judgment collection is given in the section .
.
we treat e1 as an example of nocuous ambiguity because the risk of diverging interpretation is high.
in contrast for the example e2 the majority of judges out of agreed with the reading .
so we say e2 exhibits innocuous ambiguity because it is interpreted in the same way by different readers and has a lo w risk of being misunderstood.
table .
construction patterns used in coordination ambigu ity n and n2 v1 and v2 are coordinated compounds i.e.
noun or verb compound the underline part o f speech tag is the headword of the attached modifier c is a coordination i.e.
and or or p is the preposition type pattern example adj n1 c n2 manual input and output vbn n1 c n2 associated doors and windows nn n1 c n2 project manager and designer nn p n1 c n2 the set of plans and tables n1 c n2 nn software and hardware product noun n c n2 p nn book and paper on the table adv v1 c v2 be manually rejected and flagged nn p v1 c v2 some functions for receiving and transmitting v1 c v2 nn generate and print reports v1 c v2 adv be inspected and recorded automatically verb v c v2 p nn be implemented and executed on the platform our examples are modified extractions from requirements documents available on the web site h ttp research.it.uts.edu.au.
figure .
the framework for the nai tool used in coordination ambiguity our automated approach focuses on two main types of coordination ambiguity noun compound coordination and verb compound coordination respectively.
the c onstruction patterns for these two types of coordination ambiguity are depicted in table .
the table shows that each ambiguity construction pattern generally consists of three basic coordination constituents two coordinating conjuncts near conjunct nc and far conjunct fc and the attached modifier f .
the conjunct is allowed to be a noun or verb com pound and the modifier to be an adjective adverb or noun.
for example the coordination construction functional model and description is the exemplification of the ambiguity pattern adj n c n2 where model and description are nc and fc and the adjective functional is the m individually.
syntactically this pattern can be interpreted by two distinct bracketing adj n c n2 and c and .
we say that displays high attachment of the modifier where m applies to both fc and nc and displays low attachment where m applies only to nc.
.
a nocuous ambiguity identification tool we have implemented a nocuous ambiguity identification nai tool to automatically detect nocuous ambiguities in text.
nai identifies potentially ambiguous patterns in a textual input and a nocuity classifier then classifies the instances into nocuous and innocuous cases.
the conceptual architecture of the nai tool is shown in figure .
it consists of four main func tional process modules text preprocessing module ambiguity in stance detection module coordination constituent extraction module and nocuous ambiguity classification module respectively.
we describe the behavior of each of the modules in turn and gi ve details of how the system is trained in section .
.
text pre processing given a text document our tool first executes several text preprocessing steps including sentence splitting part of speech pos tagging and phrase based shallo w parsing.
at first the text is split into a collection of sentence using a sentence boundary detector2.
then for each sentence the stanford parser3 is used to obtain word lemma and pos tags e.g.
noun verb adjective adverb etc.
of individual words and associated phrase information.
furthermore some statisti cal information e.g.
word cooccurrence and word distribution is calculated based on the position of words in the sentence and saved into the back end mysql database.
.
ambiguous instance detection this process consists of two main steps step i is to detect the sentences that contain coordinati on constructions step ii is to extract relevant ambiguity instances from the detected sentences.
given a sentence step i is to search the pos tag sequence of the sentence and determine whether the sentence contains one of the coordination construction patterns de scribed in table .
a searching window is moved from left to right to examine the pos tag sequence of the sentence.
if an ambiguity construction pattern falls into the window then th e corresponding substring is extracted from the sentence as an ambiguous coordination instance.
scottpiao sent detector ftware lex parser.shtml documents data generated pre processed text machine learning heuristics machine judgements human judgements ambiguity instances human constituents nocuous ambiguities instances text sentence splitting pos tagging shallow parsing word co occurrence word distribution text pre processing processes ambiguous instance detection coordination consti tuent extraction nocuous ambiguity classification syntactic rules syntactic patterns nlp tools techniques 55if the sentence does contain one or more of the patterns then step ii extracts relevant coordination constructions from the sentences.
this step is fundamental to th e understanding of a sentence especially for a sentence which is longer and far more complex .
for example e3.
use daylight to achieve the desired light setting of room and hallway section whenever possible.
in the sentence e3 the substring the desired light setting of room and hallway section in practice can be split into two distinct ambiguity instances the desired light settings of room and hallway and room and hallway section .
to simplify the ambiguity analysis it is necessary to treat these two ambiguity instances separately and determ ine whether either is nocuous.
therefore our ambiguity analysis is based on a smaller unit level the instance level other than on the sentence level with respect to long and complicated sentences.
.
coordination constituent extraction the recognition of coordination constitu ents is in fact the task of named entity recognition ner .
ner plays an important role in analyzing the syntactic and sema ntic relations among coordination constituents discussed later.
in order to extract appropriate coordination constituents from an ambiguous construction pattern a set of syntactic rules are created based on the position of indi vidual constituents in the pattern and the property of their corre sponding pos tags.
for each ambiguity instance two coordinating conjuncts near conjunct nc and far conjunct fc and the attached modifier m are separately extracted from the instance by relevant syntactic rule and sa ved into the database together with the corresponding instance id.
.
heuristics to predict nocuity having identified the sentences which contain instances of coordination ambiguity and recognized the constituents which give rise to the ambiguity the final stage is to identify whether the ambiguity is nocuous or not.
to identify cases of nocuous ambiguity we implemented a number of heuristics that identify the properties of an ambiguity instance which in turn may lead an interpreter to favor high attachment or low attachment interpreta tions.
our tool then classifies the input as nocuous or innocuous using the values of the set of heuristics input to a trained classifier using the logitboost algorithm.
th e remainder of this subsection deals with the particular heuristic s details of training and using the classifier are given in section .
we explored a number of heuristics to apply to ambiguity instances.
each of the individual heuristics attempts to identify aspects of the ambiguity instance that may lead an interpreter to favor high attachment or low att achment interpretation.
heuristics run over the instance and their scores are saved as features in a feature vector that will be used later by a machine learning algo rithm to classify coordination ambiguity as nocuous or innocuous.
one of the major approaches we use here is a corpus based approach.
some of the heuristics e.g.
coordination matching distribution similarity and collocation frequency are based on sta tistical information e.g.
word distribution and collocation that is obtained from a large english corpus british national corpus bnc via the sketch engine5 .
we here give a brief description of each of the heuristics below.
besides the four heuristics used in the previous work we introduce two more heuristics collocation frequency in local document and wordnet based semantic similarity.
we present below a brief description of each of the heuristics.
coordination matching.
this heuristics hypothesize that if the headwords of the two conjuncts are frequently coordinated in the text then that coordination form s a single syntactic unit and the particular instance should therefore prefer to high attachment interpretation.
the word sketch facility of the sketch engine provides statistical information about lists of words that are conjoined with and or or .
the higher the ranked score is the more frequently the two conjuncts occu r in the bnc corpus.
consider the example below e4.
security and privacy requirement the sketch engine returned a highly ranking score of .
for the coordinated words security and privacy .
in the human judgments we collected out of judges considered that it should be interpreted as high attachment interpretation thought it ambiguous and only judge chose lo w attachment interpretation.
therefore this ambiguity tends to be interpreted as a high attachment reading i.e.
requirement .
distributional similarity.
this heuristic is based on the assumption suggested by kilgarriff which is that strong distributional similarity between the headwords of the two conjuncts indicates that the conjuncts form a syntactic unit thus resulting in the preference of the high attachment interpretation.
the distributional similarity of two words is a measure of how often these two words can be found in the same contexts.
for example e5.
function for receiving and transmitting the words receiving and trans mitting have strong distributional similarity despite thei r opposite meanings.
of the judges judged this ambiguity to be high attachment interpretation judged it to be an ambiguity but no one considered it a low attachment reading.
collocation frequency.
this heuristic assumes that if the modifier is collocated much more freque ntly with the headword of the near conjunct than it is collocated with the headword of the far conjunct the particular instance should display low attachment of the modifier.
the score returned by the heuristics is the ration of the collocation frequency with the near headword over the collo cation frequency with the further headword.
collocation fre quency had been proven very usef ul in the disambiguation task .
for instance e6.
project manager and designer project has a collocation score of .
with manager in the bnc but it has no collocation with designer .
in the collected a headword is the main word of a phrase and the other words in that phrase modify it.
56judgments judges favored low a ttachment reading while people preferred high attachment readi ng.
it seems this ambiguity is more likely to be interpreted as low a attachment reading.
here this heuristic was employed on two different resources and obtained separate heuristics scores a local document the local based collocation frequency score is calculated based on the collocation frequency information in the local document.
this score looks mo re useful especially when the headwords for coordination constituents are domain specific words that are too scarce to be found in the bnc.
moreover the contexts in the local document pr obably provide stronger cues for the semantic relationships between the conjuncts and the attached modifier.
b bnc corpus the corpus based collocation frequency score that is estimated based on the collocation frequency information on the bnc corpus.
morphology.
this heuristic is based on syntactic parallelism suggested by okumura and muraki in disambiguating coordination.
it hypothesizes that if the headwords of the two conjuncts share a similar morphology th en they form a syntactic unit hence the instance favoring the high attachment interpretation.
the inflectional morphology of a language is the analysis of the changing of words to signify thei r term number gender etc in english it consists largely of suffi xes such as ed ing and s. the derivational morphology of e nglish is more complex but suffixes such as ation and able are also very common.
the score returned by this heuristic is the number of common trailing characters of the headwords of the potential conjuncts.
semantic similarity.
this heuristic presen ts a measure of semantic similarity between the headwords of the two conjuncts based on the taxonomic structure in wordnet .
resnik has pointed out that similarity of meanings of conjoined heads is an important cue to coordination ambiguity resolution.
for instance e7.
vector based input and output e8.
manual input and selection clearly input and output in e7 are more similar in semantics than input and selection in e8 .
therefore e7 could be more likely to be interpreted as high attachment of the modifier.
this assumption conforms to the distribution of human judgment.
in e7 out of judges had a favor of high attachment read ing while in e8 only of judges chose high attachment interpretation.
the similarity of meaning could be captured well by semantic similarity in the wordnet ta xonomy by measuring the distance between the nodes corresponding to the headwords of the two conjuncts.
if the headwords of coordinating conjuncts exhibit strong semantic similarity then the ambiguity instances favour a high attachment interpretation.
this heuristic is implemented by the nlp tool java wordnet similarity library .
.
training and using the nocuity classifier as discussed in section .
each of the heuristics individually indicates a preference for high or low attachment.
however they do not predict whether a given ambiguity instance is nocuous or not.
in this section we describe how the individual heuristics are combined to classify a particular ambiguity as nocuous or innocuous at a given ambiguity threshold.
.
building a dataset in order to build a working classifier we require a collection of human judgments of ambiguous senten ces.
this allows us to identify which sentences display noc uous ambiguity and use this information to identify how the particular heuristics described in section .
are combined to replicate the human judgments.
to train nai to recognise instances of nocuous ambiguity we used the dataset collected by chantree et al.
.
this dataset describes coordination instances from the sentences of a set of requirement documents.
each of th e instances contains one of the ambiguity construction patterns de scribed in table .
among the instances noun compound conjunctions account for a significant number with instances .
.
in noun compound conjunc tions nearly half of the cases ar ose as a result of noun modifiers while there are cases with adjective and with preposition modifiers.
human judgment collection.
the coordination instances that contain potential ambiguity were split into surveys and presented to a group of computing professionals including academic staff or research students.
each instance was judged by people.
for each instance the judges were asked to select one of the three options high attachment ha of the modifier low attachment la of the modifier or ambi guous a .
the latter we call acknowledged ambiguity i.e.
the read er realizes an ambiguity is present in the text but does not feel able to judge which interpre tation was intended by the writer.
table shows the judgment count for two sample instances.
in stance a was judged mainly to have high attachment of the modifier while instance b was judged mainly to be ambiguous.
table .
judgment count for the sample instances ha high attachment la low attachment and a ambiguous judgments ha la a a security and privacy requirements b electrical characteristics and interface .
training the classifier a key concept in training the classifier is the ambiguity threshold .
we require a decision point to determine whether a particular instance exhibits nocuous ambiguity or not.
we use the concept of ambiguity threshold that represents how much agreement is required from the judges over a partic ular interpretation.
use of a threshold also allows us to adju st tolerance levels some application domains e.g.
safety critical systems may wish to use to a low threshold before consid ering an ambiguity nocuous.
figure .
sample output of the nai t ool for nocuous coordination ambiguity given an instance with multiple possible interpretations and a set of judgments the certainty of an interpretation is calculated as the percentage of the judgments for that interpretation against the total number of the judgments fo r the whole instance.
for instance in table the certainty of ha for instance a is .
.
ambiguity thresholds ambiguities inno nocu figure .
proportions of interpretations at different ambiguity thresholds nocu nocuous inno innocuous given a coordination instance if either of the interpretations ha or la has a certainty greater than the ambiguity threshold we say this coordination instance displays innocuous ambiguity.
the relationship between ambiguity threshold and the classification of nocuous ambiguity in the dataset is illustrated in figure .
it shows that at low thresholds very few instances exhibit nocu ous ambiguity because the certainty constraint is so low that it is easier to be satisfied with a small number of agreements.
however almost all instances are cla ssified as nocuous since it is very hard for judges to reach a consensus on the same interpretation.
building the nocuity classifier.
our nocuity classifier was trained based on a set of heuristic scores together with the human judgments collected in the training data.
more specifically each ambiguity instance is described as a training test instance which is represented as an attribute value vector where the value of each attribute is the score of a particular heuristic described ear lier.
the class label of a traini ng instance nocuous y or innocuous n at a given ambiguity thre shold is determined by the distribution of multiple human judgments discussed earlier.
to select an appropriate machine learning ml algorithm to build our nocuity classifiers we tested our dataset on a number of 58ml algorithms available in the weka package9 including the logistic regression algorithm that was used in our previous work .
finally we selected the logitboost algorithm for building the nocuity classifier because it performed better than other candidates including decision trees j48 naive bayes svm and logistic regression.
.
applying the classifier to determine whether a test ambiguity instance displays nocuity or not we presented the feature v ector of the instance to the classifier and obtained the predicted class label returned by the classifier.
once the process of nocuous ambiguity identification was completed we highlighted the ambi guous sentences in the original text each of which contained at least one nocuous ambiguity.
in addition we presented the user with the extracted nocuous ambiguity instances with the identif ied coordination constituents highlighted by different colors.
the sample output of our nai tool for nocuous coordination ambiguity is shown in figure .
.
experiments and results to evaluate the performance of our tool we used a standard 10fold cross validation technique in which for each iteration we trained on data and tested on of the remaining data.
the performance of the system is measured in terms of precision p recall r f measure f and accuracy fn tptpr fp tptpp rpprmeasuref totaltn tpaccuracy where tf true positives is the number of correctly identified nocuous ambiguities tn true negative is the number of correctly identified innocuous ambiguities fn false negatives is the number of nocuous ambiguity not identified by the system and fp false positives is the number of innocuous ambiguities which the system incorrectly classified as nocuous.
the weight is set with .
in order to favor the precision.
all results are averaged across ten iterations .
.
performance of the classifier we report in this section the performance i.e.
precision recall and f measure of the ml based classifier using the logitboost algorithm at different ambiguity thresholds.
figure summarizes the precision and recall results of the classifier.
we compared the precision of the classifier with a baseline precision p bl that assumes that all of the instances are potentially nocuous ambigui ties.
compared with baseline precision p bl the logitboost classifier performed well with precision of up to on average at different threshold levels.
it suggests that the heuristics we developed contain distinguished features which provide strong discriminating power in determin ing the nocuity property of an ambiguous instance.
however at some low threshold levels especially when the thresholds are below the classifier did not work well with respect to recall.
the recall dramatically dropped down to .
a possible reason for this is the lack of positive cases i.e.
nocuous ambiguities at the low threshold level see figure which results in deterioration of performance.
ambiguity threshold p r p bl figure .
the performance of the classifier at different ambiguity thresholds.
p bl baseline precision .
impact of the heuristics as described earlier we introdu ced two more heuristics in our nai tool one is local based collocation frequency which exploited the co occurrence frequency between coordinating conjuncts and the attached modifier in the local context the other is semantic similarity that investigated the semantic relationship between the far conjunct and near conjunct using the wordnet taxonomy.
to estimate whether th e two newly added heuristics can improve system performance we conducted another set of experiments with the heuristics only used in our previous work.
we compared the f measure performance of two sets of experi ments one is for all of the heuris tics f all that we described in this paper and the other is fo r the heuristics except for the two newly added heuristics f part .
moreover to compare the performance of our proposed heuristics based approach for nocuous ambi guity identification we used a random model as a baseline.
in the random model we assume that each recognized ambiguity instance has the potential to be a nocuous ambiguity and is counted as a positive match for the baseline model.
the random model achieves an ideal recall r bl of and the precision and f measure are calculated as s ambiguitieof totaljudgmentsby identified nocuouspbl bl blbl bl blr prpmeasuref as figure shows compared with the baseline f measure f bl the heuristics based appro ach is an effective method for the identification of nocuous ambiguity due to relatively high precision see figure .
figure s hows that with those selected thresholds .
.
the heuris tics based approach exhibits a marginal improvement in f measure compared to the baseline model.
nevertheless the improve ment gradually decreases with the increase of the threshold value.
after the threshold is set to 59above .
the heuristics based a pproach performs slightly better than the baseline model due to th e quite high f measure value.
our results also show that the f all performs consistently better than the f part throughout all of the threshold levels.
it suggests that the performance of our tool did benefit from the newly added two heuristics which indicates th at local context information and semantic relationships between the coordination constituents provide the useful clues for the id entification of nocuous ambiguity.
ambiguity threshold f measure f bl f part f all figure .
the impact of the heuristics on system performance.
.
system performance comparison in the implemented nai tool we chose the logitboost lb algorithm to build our ml based nocuity classifier which replaced the original regression lr algorithm used in our previous work .
to evaluate the accuracy of our proposed ml based model we used two baseline bl mode ls bl and bl models.
the bl model assumes that all the instances are innocuous ambiguities whereas the bl model suppos es that all the instances are nocuous ambiguities.
as discussed previously at low ambiguity thresholds most of the instan ces are judged as innocuous ambiguities while at high ambiguity thresholds the majority of the instances are nocuous ambiguity see figure .
therefore to compare fairly with the performance of the ml based model a good strategy is to compare it with the bl model at low thresh olds and to compare it with the bl model at high thresholds.
the performance comparison of the logitboost lb model against the two baseline models at different ambiguity thresholds is shown in figure .
the lb model performed well with an accuracy of above on average at different ambiguity threshold levels.
as expected at very high and very low thresholds the lb model did not outperform the ba seline models due to the high accuracy.
however the lb model displayed its advantage when the ambiguity threshold fell in the range between .
and .
.
the lb model generally performed better than the baseline models and the maximum improveme nt was achieved around the crossover point where the two base line models intersect.
our tool accomplished an approximate increase in accuracy.
it sug gested that the combined heuristics do have some capability of distinguishing nocuous from innocuous ambiguity at the weakest region of the baseline models.
figure also shows that with th e lb model the tool improved the overall accuracy with an increase of approximate .
on average compared with the previous model the logistic regression lr model.
the possible explanation is that the lb algorithm is more suitable for the dataset with the numeric attribute feature vectors.
ambiguity threshold accuracy bl bl lb lr figure .
the performance comparison of the ml based model to the two baselines bl and bl .
bl baseline all innocuous bl baseline all nocuous lb logitboost lr logistic regression .
related work nlp tools applied to nl requirements.
a number of natural language processing nlp systems or tools applied to nl requirements had been developed in recent years.
ambriola and gervasi developed a web base d nlp tool called circe which was designed to facilitate the gather ing elicitation selection and validation of nl requireme nts.
ibm rational doors10 a requirements management tool provides relevant functional modules for the generation of nl requirements and the traceability among nl requirements.
goldin and berry implemented a tool called abstfinder to identify the abstrac tions from natural language text used for requirements elicitation.
lee and bryant developed an automated system to assist the engineers to build a formal representation from informal requi rements like nl requirements.
ambiguity in nl requirements.
several studies dealing with ambiguity identification have aimed to help improve the quality of nl requirements documents.
so me tools have been developed specifically to detect measure or reduce possible structural am biguities in text.
kamsties et al.
describe pattern driven inspection technique to detect am biguities in nl requirements.
fuchs and swhwitter present a restricted nl called attempt controlled english ace to translate specifications into sen tences in first order logic in or der to reduce the ambiguity in requirement specifications.
mich and garigliano investigate the use of a set of ambiguity indices for the measurement in syntactic and semantic ambiguity which is implemented using an nlp system called lolita.
kiyavitska ya et al.
proposed a twostep approach in which a set of lexical and syntactic ambiguity measures are firstly applied to am biguity identification and then a 60tool to measure what is potentially ambiguous specific to each sentence.
finally some of the tools have been developed to examine the quality evaluation of requirements.
these include quars arm and the tool by fantechi et al.
for use case re quirements.
these approaches define a quality model composed of a set of quality metrics e.g.
vagueness subjectivity optional ity weakness etc.
and devel op analysis techniques based on a linguistic approach to detect the defects related to the inherent ambiguity in the requirements.
coordination ambiguity.
in last decade a number of approaches have been proposed to address ambiguity resolution in coordination constructions.
previous resear ch efforts have generally focused on either corpus based statistical methods e.g.
cooccurrence frequency between coordinating conjuncts and the modifier over a text corpus such as web resource or wall street journal or linguistic approaches that made use of part of speech pos tagging and shallow e.g.
phrase and deep parsing e.g.
parsing tree inform ation to apply pattern or rulebased matching .
in add ition similar to our work resnik took advantage of semantic similarity of a taxonomy to resolve coordination ambiguity involving nominal compounds.
unlike other related ambiguity work that attempts to resolve am biguity by applying disambiguation techniques to select the most appropriate reading our studies present readers with nocuous ambiguities which may potentially be misunderstood by different readers and allow these readers to determine the preferred inter pretations.
.
conclusions and future work natural language still prevails in a large number of requirements documents.
we need ways to cope with the ambiguity inherent in natural language.
it is important to develop scalable automated techniques to detect potential noc uous ambiguities in natural language requirements in order to mi nimize their side effects at the early stages of the software development lifecycle.
in this paper we described an automated approach to characterize and identify potentially nocuous ambiguities which have a high risk of misunderstanding among diffe rent readers.
given a natural language requirements document ambiguous instances contained in the sentences were first extracted.
then a machine learning approach was employed to cla ssify ambiguous instances as nocuous or innocuous subject to a give n ambiguity threshold.
a prototype tool which focuses on c oordination ambiguity was implemented in order to illustrate and evaluate our approach.
we reported on a set of experimental re sults to evaluate the performance and effectiveness of our automated approach.
the results show that our tool is capable of accurately detecting potentially dangerous ambiguities in the nl requirements.
based on significant technical de velopment and substantive empirical studies we believe that the application of our approach is lightweight and usable in that it allows requirements analysts to experiment iteratively to identify potential nocuous ambiguity in requirements depending on their chosen analysis sensitivity threshold.
however a number of interesting i ssues remain to be investigated in order to improve our tool s performance and validate its use in practice.
first more heuristics n eed to be developed to further explore aspects of ambiguity that enhance the accuracy of our tool.
second our current tool is specific to coordination ambiguity.
it is necessary to extend it to a wider range of ambiguity types for example to other types of structural ambiguity like prepositional attachment ambiguity and semantic ambiguities such as anaphora.
we have begun to explore the identification of nocuous ambiguity in terms of anaphora ambiguity in our ongoing work .
third we need and plan to make our tool more widely accessible to validate its use in practice.
this may include migrat ing the technology to a web based environment providing an automated analysis as some kind of web service.
indeed we en visage that this automated suppor t for ambiguity analysis should fit into a number of requirement s management environments in which requirements authors are able to invoke such analysis tools in the same way as writers invoke spell checkers.
we are cur rently investigating the developm ent of this capability within a well known commercial tool.
.