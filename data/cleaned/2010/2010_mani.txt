see discussions st ats and author pr ofiles f or this public ation at .researchgate.ne t public ation ausum approach for u nsu pervised bug report su mmarization conf erence paper no vember .
.
citations 121reads author s including senthil k umar k umar asamy mani ibm publica tions citations see profile vibha sinha ibm publica tions citations see profile avinav a dube y carne gie mellon univ ersity publica tions citations see profile all c ontent f ollo wing this p age was uplo aded b y senthil k umar k umar asamy mani on may .
the user has r equest ed enhanc ement of the do wnlo aded file.ausum approach for unsupervised bug report summarization senthil mani rose catherine vibha singhal sinha avinava dubey ibm research india sentmani rosecatherinek vibha.sinha in.ibm.com avinava.dubey gmail.com abstract in most software projects resolved bugs are archived for future reference.
these bug reports contain valuable information on the reported problem investigation and resolution.
when bug triaging developers look for how similar problems were resolved in the past.
search over bug repository gives the developer a set of recommended bugs to look into.
however the developer still needs to manually peruse the contents of the recommended bugs which might vary in size from a couple of lines to thousands.
automatic summarization of bug reports is one way to reduce the amount of data a developer might need to go through.
prior work has presented learning based approaches for bug summarization.
these approaches have the disadvantage of requiring large training set and being biased towards the data on which the model was learnt.
in fact maximum efficacy was reported when the model was trained and tested on bug reports from the same project.
in this paper we present the results of applying four unsupervised summarization techniques for bug summarization.
industrial bug reports typically contain a large amount of noise email dump chat transcripts core dump useless sentences from the perspective of summarization.
these derail the unsupervised approaches which are optimized to work on more well formed documents.
we present an approach for noise reduction which helps to improve the precision of summarization over the base technique to across subjects and base techniques .
importantly by applying noise reduction two of the unsupervised techniques became scalable for large sized bug reports.
categories and subject descriptors d. .
testing and debugging debugging aids i. .
natural language processing text analysis keywords unsupervised summarization bug report this work was done when he was a researcher at ibm research india.
his current affiliation is lti carnegie mellon university.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
sigsoft fse november cary north carolina usa.
copyright acm ... .
.
.
introduction bug management systems are an integral part of any software project.
they are used by the project team to record problems raised by end users and to track the investigation and resolution of bugs.
as part of root cause analysis significant amount of communication might happen between the reporter of the bug and the developers through the bug report.
as a result bug reports often resemble recorded conversations messages from multiple people ordered sequentially where each message is composed of multiple paragraphs of unstructured text.
as the bug gets resolved over a period of time the bug report accumulates useful information about the problem reported such as the investigation and resolution details.
these resolved bugs are then archived for future reference.
when a new bug is reported developers first check the archived bugs to see if similar problem was resolved in the past .
finding a similar bug could help them in multiple ways in better understanding of current problem if the resolution has been recorded then reusing it for the new problem.
especially in industrial projects a large number of bugs might not require code changes.
the reported problem could be because of some configuration or usage issue.
in such cases the archived bugs become all the more indispensable .
this is because the support team can quickly check if similar problems reported in past required a code fix or were solved through other means.
in most cases this either reduces the time taken to solve the issue or enables a developer with relatively less experience on the project to accurately respond to the issue.
search can help a developer identify similar bugs from the archived repository.
however the developer still needs to follow the tedious process of going over each of the recommended bug and identify if the bug report contains any information of interest or not.
wading through complete contents for each recommended bug might be too time consuming and frustrating.
for example the bug reports in our two experiment subjects on an average had and sentences respectively.
in the authors suggest that one way to reduce the time a practitioner takes in identifying the correct bug report is to provide a summary of each recommended bug.
the ideal way to do this would be that after a bug is resolved an assigned developer manually writes out an abstract.
figure shows an example of the bug report and it s corresponding summary.
this abstract could further be used by other developers in the system to get a better understanding of the bug.
however this ideal method is unlikely to work in practice since it involves huge manual effort.
hence there is a need for automatic summarization of bug reports.
there are two approaches available for summarizing textual artifacts.
one is a learning based supervised approach various variants of which are described in .
the essence offigure example of bug reports and it s summary these approaches is as follows ask users to manually summarize a set of documents training set extract out a set of text features from these documents and train a statistical model.
the model tries to identify the features that help predict the manual summaries provided in the training set for a new document extract the features of interest from the document and use the trained model to predict it s summary.
the other is an unsupervised approach various variants presented in .
these assign centrality and diversity measures to various sentences in a document and use these measures to select the sentences to be put in the summary.
we describe these measures later in the paper in section .
.
in a prior work rastkar et al.
applied supervised learning approaches to summarize bug reports.
supervised learning approaches have their drawbacks as they involve collecting manually labelled training data and once a model is trained the efficacy of the model depends highly on the similarity of the training and actual data on which it is applied.
the practical application of such a supervised technique in any project could be hampered owing to the initial training cost involved.
the authors reported a pyramid precision of when the model was trained on bug re ports from same subject against precision when the model was trained on a corpus of mail threads.
in our work we applied four unsupervised approaches centroid maximum marginal relevance mmr grasshopper anddiverse rank to the problem of summarization of bug reports on the dataset used in sds and one internal industrial project dataset db2 bind .
across the two subjects divrank andgrasshopper had convergence problems on large bug reports which resulted in no summaries being generated for them.
for the remaining bug reports in sds dataset when compared to supervised techniques all the unsupervised approaches provided better recall an improvement by .
there was however a dip in pyramid precision between the best of unsupervised approach mmr grasshopper and the best of supervised approach i.e when the approach was trained on bug reports from the same subject.
but all unsupervised approaches provided comparable precision to the supervised approaches that were trained on a different dataset other than the subject where they were applied.
the drop in precision and the convergence issues made us believe that bug reports have specific type of data that is causing the generic unsupervised approaches to not perform as well as the supervised approaches.
bug reports resemble conversations very often with email and chat content pasted.
this content contains email headers salutations like hello hi and closing phrases like thank you thanks etc that are not useful from summary perspective.
there are very often long stack traces command outputs pasted that are not that useful from summarization perspective.
to handle this we augmented our summarization approach by adding a pre processing step to filter out such sentences noise from the bug reports.
this noise removal step helped improve the precision of the base unsupervised algorithms.
also the algorithms divrank andgrasshopper which failed to converge and produce any summaries for few large bug reports successfully generated summaries for the same bug reports once the noise was filtered from them using our approach.
the main contributions of the paper are as follows a classification scheme for bug report content and a novel noise reducer to classify bug report sentences into this classification automatically.
this bridges the gap between theory and practice.
experimental evaluation of four well known unsupervised summarization algorithms to the problem of summarizing bug reports.
the results indicate that unsupervised approach generates as good a summary as supervised approach proposed in earlier paper.
the summarization approach was enhanced to filter out specific classes of content from bug report that could be perceived as noise from summarization perspective.
experimental validation of the impact of using a noise filter before applying the unsupervised techniques indicates that better summaries are generated.
rest of the paper is organized as follows.
in section we describe our approach.
in section we give a brief overview of the unsupervised summarization algorithms we used.
section describes the experimental studies we conducted.
we present some observations and threats to validity in section .
the related work is presented in section and conclusions in section .
.
approach 1available at approach for unsupervised bug report summarization figure outlines our summarization approach.
for a bug that needs to be summarized we first pass it through the noise reducer module.
here we broadly classify each sentence into question investigative sentence code fragment and others .
in different variants of our approach we filter out different type of sentences and pass the filtered set to a summarizer which applies the unsupervised techniques to extract the summary from the set of useful sentences.
for example in one variant the other sentences are filtered out and in another both code andother sentences are removed.
the filtered content is passed to the summarizer module.
this modules has different techniques for summarization plugged in centroid based mmr divrank and grasshopper.
in the subsequent subsections we describe the noise reducer and summarizer module in detail.
.
noise reducer we analyzed the nature of content in bug reports and came up with following classification for information in bug reports.
question sentences these sentences describe the problem being reported and are usually contain words such as why how what .
investigative sentences these sentences describe some options that the user can try to further investigate the issue or give more details on understanding the problem.
these sentences would typically contain application specific information often indicated by presence of domain specific keywords.
code sentences these sentences contain code fragments stack traces or command outputs that the user might have provided as part of initial bug description or as part of investigation or solution.
others these are very often greeting sentences for example hello thank you for your support .
figure shows the sentence classification for the sample bug report shown in figure .
the bug report consists of series of communication as comments.
each comment in turn contains a set ofsentences.
in figure sentence s3 is classified as question .
sentence s5 in an example of a code sentence.
sentences s2 s6 s7 contain key words like bit bit sqlint32 sqluint32 preprocessor precompile and are examples of investigative sentences.
sentences s1 s8 and s9 are examples of other category.
as part of our initial investigation we had summaries created manually by different developers.
we further analyzed the sentences used in these summaries and found that all these sentences were either question andinvestigative type.
based on this we concluded that if we were to filter out all sentences from bug reports which were not of these kinds and then run the summarizer we would not lose any useful information but rather improve the efficacy of the summarization.
in order to implement a system that can classify bug report content into the above mentioned classes we had two design choices.
first follow a supervised approach where we manually classify label some percentage of the sentences into the four categories train a statistical model and then execute the model on remaining set to predict the classification.however this approach would have the disadvantage that the model needs to be re trained for new subjects.
second option was to follow an unsupervised approach.
leverage the sentence structure and develop heuristics to come up with a classification.
we chose to follow the latter approach as it can be widely applied for different subjects datasets and the effort involved in fine tuning the heuristics will be way less than manually classifying labelling for new datasets.
the noise reducer works as follows.
first we convert the bug report into a sequence of sentences.
for this we use an off the shelf english text parser stanford nlp parser2.
once we have the sentences we further classify each sentence into one of the four categories using heuristics mentioned below.
finally we filter out the sentences not useful from summarization perspective and pass this filtered set to the summarizer.
.
.
question sentences a sentence can be marked as question if it starts with words such as what why where etc and optionally ends with a question mark.
regular expressions as follows could be used to identify questions.
example sentence classification of sample bug report hat.
?
ow.
?
another way to identify questions is to use the parsed syntactic structure of sentences.
sentences that contain node sbarq or sq3in their parse tree are typically questions.
for example the parsed structure of s3 in figure is as follows root sbarq whadvp wrb how sq md can np prp we vp vb avoid np np nn data .. vp vbn faced pp .. .
.
we used the parsed syntactic structures to classify sentences as question type.
.
.
code sentences to classify sentences into code we analyzed the bug report contents and came up with patterns that indicate presence of stack trace java code commands command output.
some examples of these patterns are starts with db2 proc public contains sql else if.
if.
public static int char ends with if a sentence satisfies any of the above rules it is classified as a code sentence.
while these patterns are fairly generic they might have to be enhanced only in special cases.
.
.
investigative and other sentences the non question non code sentences can be further classified into investigative andother .
investigative sentences give insight into the problem and or solution being proposed or implemented.
these typically contain some application specific information as indicated by presence of bit bit in sentence s2 in figure .
any sentence that does not contain application specific information could potentially be marked as others.
to classify sentences into these two categories we applied the following heuristic if a sentence was of a minimum length of words and contained more than two application specific keywords it could be marked as investigative .
we derived the keyword dictionary from the bug report content itself as explained in the next subsection.
3penn treebank tags treebank .
.
keyword dictionary intuitively what we are trying to achieve here is to identify a ranked order of words in each bug report that indicates the discriminating power of that word in the bug report.
this relative ranking is obtained by leveraging the term frequency inverse document frequency metric typically used in text search.
to create the dictionary we first extract the terms i.e.
single words from all the bug reports available for a subject.
for each term t per bug report d we calculate it s tf idf t d term frequency inverse document frequency .
term frequency tf t d is defined as the number of instances of a term in a document normalized by the document size.
tf t d pt jdj the inverse document frequency idf t for a term t is the measure of general importance of the term across the subject.
it is obtained by dividing the total number of bug reports d by the number of reports containing the term d and then applying the logarithm.
idf t d logjdj jd2d t2dj then the tf idf is calculated as tf idf t d tf t d idf t for each term per bug report we further normalize the tf idf t d w.r.t.
the highest tf idf obtained for that report.
the key words in the document were then picked up by applying a cut off on the normalized tf idf .
.
.
filter for each bug report once the sentences are classified into these four categories then we can selectively choose different classes of sentences or combinations thereof to use as inputs for summarization.
in our experiments we have evaluated the impact of removal of other sentences and removal of other andcode sentences.
.
unsupervised summarization unsupervised summarization refers to the method of summarizing a text or document without using a trained model.
this has the obvious advantage of not requiring any labeled data which is usually difficult and or costly to collect due to the manual effort involved.
it is usually classified into two broad types extractive and abstractive .
an extractive summarization refers to any summarization method in which the sentences in the summary are chosen from the input document and are used as is.
in abstractive summarization the sentences of the summary are usually different fromwhat is present in the original text and could involve paraphrasing.
majority of automatic summarization techniques developed so far are extractive in nature.
though abstractive summaries are closer to what a human would have constructed the complex nature of the methods and lower returns in terms of accuracy has greatly reduced their applicability in various domains .
in extractive summarization techniques an algorithm decides at the specified granularity level whether the piece of text in the input document should be present in the output summary or not.
the granularity can be at the level of a phrase sentence or in the case of bug reports at a comment level.
in this paper we evaluate the summaries at the granularity level of a sentence.
unsupervised summarization methods work by choosing sentences that are central to the input document.
this centrality can be measured in various ways.
a simple method for the same is to compute the centroid of the document.
then the sentences chosen for the summary are based on their distance from the centroid .
in this paper we refer to this method as centroid .
a more systematic method of measuring centrality is based on the amount of similarity to other sentences in the input document.
this is based on the reasoning that sentences that form the essence of the document are repeated in different ways and thus have more overlapping terms with other sentences than those that are not central to the document.
similarity to other sentences can be considered as a recommendation by those sentences .
for example in this paper a sentence that contains unsupervised summarization and bug reports would overlap with a large number of other sentences since these phrases form the theme of this paper thus making that sentence an important entry in the summary.
though a centrality measure would suffice to select the important sentences of the document it can make the sentences in the summary repetitive.
diversity is an important notion in summarization where the new objective is to choose sentences that are central but at the same time have low redundancy among themselves and fully represents or covers the document.
one of the first works to propose diversity in summarization is the maximal marginal relevance mmr algorithm which is discussed in section .
.
mmr algorithm is also one of the most well known works in the area of summarization but it has been generally regarded to be lacking a principled mathematical treatment in its use of heuristics.
the next important work and one of the first ones to propose a mathematical model is the grasshopper algorithm .
this method is discussed in section .
.
a recent work in summarization which was proposed as an improvement to the grasshopper algorithm is the divrank algorithm .
we discuss this in section .
.
in this paper we employed four extractive unsupervised summarization techniques namely centroid mmr grasshopper and divrank to summarize bug reports.
in the next section we give a brief overview of each of these techniques.
.
overview of unsupervised algorithms used in this section we give a brief overview of each of the four unsupervised algorithms we used for our experiments.
.
centroid centroid method is a simple technique for extracting summary sentences from the input document.
in this each unit of text is represented as a weighted vector of tf idf term frequency times inverse document frequency .
the unit of text in our paper is a sentence from the bug report.
the algorithm then proceeds byfinding a centroid sentence which is a pseudo sentence whose vector has a weight equal to the average of all the sentence vectors in the report.
sentences that contain words from the centroid are more indicative of the topic of the document and hence are chosen in the summary.
for each sentence si the algorithm defines a term called centroid value ofsi which is calculated as the sum of the corresponding weights in the centroid sentence of the terms in si.
once the centroid values of sentences have been calculated the summary is constructed by choosing sentences in the decreasing order of their centroid values.
.
mmr the maximal marginal relevance mmr based summarization proposed by carbonell et al.
suggested that a sentence should be included in the summary if it has minimal similarity to the already chosen sentences in addition to its centrality.
this criteria is expressed as a linear combination of the centrality of the sentence and its dissimilarity to the already chosen summary sentences.
here the dissimilarity is computed as the negative of cosine similarity but can be the negative of any general method for computing similarity of documents.
the algorithm proceeds by incrementally adding sentences to the summary where at each step it greedily chooses that sentence which has the maximum value for the above linear combination.
.
grasshopper graph random walk with absorbing statesthathop s among peaks for ranking grasshopper is a graph based method proposed for ranking documents with an emphasis on diversity.
it represents the document to be summarized as a graph where each sentence becomes a node.
the edges between these nodes are assigned a weight equal to the similarity of the sentences.
now the algorithm performs random walks on this graph similar to google s pagerank algorithm to get the stationary distribution of the graph which is the probability of the random walk visiting the node.
nodes with higher probability are more central to the document than others.
when a random walk reaches a node that is already included in the summary the walk is discontinued and a new node is chosen to start the walk.
the next node chosen is the one that has the highest expected number of visits before the walk initiated from that node would end up in an existing summary node.
here the reasoning is that nodes that are closer highly similar to the already chosen nodes will have only few visits before being absorbed.
in contrast nodes that are farther to the chosen nodes dissimilar to the chosen ones still allow the random walk to linger among them and hence will have more number of visits before eventually getting completely absorbed.
.
divrank diverse rank divrank is a recent work that also uses random walks on graph representation of data.
the random walk used in this method is a time variant random walk where the probability of moving from one node to another transition probability varies as time changes.
the particular method used in which belongs to the family of time variant random walks is called a vertexreinforced random walk where the basic idea is that the transition probability to one state from others is reinforced by the number of previous visits to that state which causes the transition probabilities to vary as time passes.
though divrank was shown to perform better than mmr and grasshopper our experiments tend to prove otherwise section .
for the experiments we used home grown implementations of all the four algorithms.
also it should be noted that all these al table details of the subjects used for our experiments average average subjects no.
of comment comments total bugs count size sentences sds .
.
db2 bind gorithms have one or two hyper parameters that can be fine tuned if training data is available.
however we used the default values to keep it completely independent of the data.
the next section presents the evaluation of our summarization approach.
.
experiments we conducted three experiments to evaluate our summarization approach and answer the following research questions rq1 feasibility of unsupervised techniques for summarization are unsupervised techniques applicable for generating summaries?
how do they perform when compared to supervised techniques presented in earlier work ?
rq2 impact of noise identifier on effectiveness of unsupervised techniques how does the efficacy of the unsupervised techniques improve for bug summarization after the noise has been filtered from the bug reports?
rq3 goodness of noise identifier how good is the noise identifier based classification of sentences into question investigative code andothers ?
we first provide the experimental setup followed by the results of the experiments.
.
experimental setup we conducted our experiments on two subjects.
the first subject referred to as sds was obtained from .
this corpus was created and used by the authors to evaluate their supervised summarization technique4.
the second subject was obtained from ibm db2 team.
we got bug reports corresponding to the db2 bind component.
this subject is referred to as db2 bind .
in total our dataset consisted of bug reports.
table lists the total number of bugs average number of comments average comment size and total number of sentences per subject.
the sds corpus contains bug reports from four different open source software projects eclipse platform gnome mozilla and kde from each .
the reports vary in length reports had between five and fourteen comments the remaining eleven bugs had to comments each.
nine of the bug reports were enhancements to the target system the other were defects.
there are a total of sentences in these bug reports.
thedb2 bind corpus contains bug reports.
the reports vary in length reports had between two to ten comments the remaining nine out of reports had to comments.
only one bug report had around comments .
we chose not to remove the outlier so as to evaluate the effectiveness of the unsupervised approaches on very large bug reports.
figure shows the distribution of bug reports based on the number of comments.
there are a total of sentences in these bug reports.
even though the number of bug reports is less it exceeds sds in total sentence count.
.
.
oracle creation to create the oracle or ground truth for our db2 bind subject we asked two members from the db2 bind development team to manually summarize the bug reports.
we asked them to mark extractive summary manually created for db2 bind by two annotators annotator annotator mean stdv mean stdv sentences in the summary .
.
.
words in the summary .
out the sentences they would want to include in the summary.
table lists some basic statistics on the manual summary.
we also performed the kappa test to measure the level of agreement between the two members as typically each individual can summarize the bug report in their own ways.
the value was .
showing a moderate level of agreement.
for sds the summarization was also provided along with the bug corpus.
they had used three annotators per bug to mark out a summary for the bug reports.
across the annotators they had a value of .
again indicating a moderate level of agreement.
.
.
metrics to measure efficacy of summarization we use four metrics namely precision recall f score and pyramid precision to compare and evaluate our techniques.
precision it is a measure of how accurate the predictions are.
intuitively it is the fraction of sentences in the generated summary that are also present in the oracle set and is formally defined as precision tp tp fp wheretpstands for true positive and fpis false positive pyramid precision p this evaluation scheme is used when multiple reference summaries are available and we used the one defined in .
let rs si be the number of reference summaries in which the sentence sihas been included gsum be the generated summary of length nandsum n be any summary of length n. then it is defined as pyramidprecision si2gsumrs si max sj2sum nrs sj denominator in the above equation is the score of the best possible summary of length n. recall it is a measure of the ability of algorithm to select the sentences of the summary.
intuitively it is the fraction of summary sentences in the oracle that we also identify in our generated summary.
it is formally defined as recall tp tp fn wherefn is false negative f score is the harmonic mean of precision and recall.
it is defined as f score precision recall precision recall rest of the section is organized as follows.
in section .
we apply the unsupervised techniquees to both the subjects and compare the unsupervised approaches with the supervised approaches for thesds subject.
in section .
we evaluate the impact of applying the proposed noise reduction technique on unsupervised summaries for both sds anddb2 bind .
finally in section .
we present the results of the goodness of classification of sentences for both sds anddb2 bind .figure distribution of bug reports in five segments of average pyramid precision with respect to unsupervised techniques for subjects sds and db2 bind.
figure distribution of bug reports for db2 bind with respect to number of comments.
.
rq1 feasibility of unsupervised techniques for summarization in this section we first evaluate the feasibility of the unsupervised techniques for summarization without applying any noise filtering.
further we compare the results with supervised techniques forsds subject.
.
.
efficacy of unsupervised summarization all unsupervised algorithms return a ranked list of sentences from the bug reports.
since our granularity was at sentence level we generated the summaries for each bug by picking the same percentage of sentences that the bug had in the reference summaries oracle set created manually.
figure shows the results of applying the four unsupervised summarization techniques on the two datasets.
x axis represents the four unsupervised techniques.
each vertical bar represents the number of bug reports covered by that technique and each segment represent the range of pyramid precision.
forsds mmr worked the best by summarizing out of bugs with pyramid precision .
.centroid technique performed the worst by summarizing almost of the total bugs with precision .
.
similarly for db2 bind grasshopper worked the best out of bugs with precision .
and centroid the worst.
forsds divrank andgrasshopper did not generate summarization for of the bug reports.
similarly for db2 bind divrank missed two bug reports and grasshopper missed one.
as discussed in the method earlier section these two techniques work by applying random walks on a graph.
depending on the nodes in the graph size of bug report and the edges similarity between sen tences the algorithm might not converge in a finite number of iterations or time period.
while experimenting we found these were not converging at all for the bug reports overall.
based on this experiment we conclude that though grasshopper anddivrank are more sophisticated algorithms and at individual bug level might give better efficacy mmr is a safer algorithm to use for bug reports because it guarantees summarization for any bug size in finite time period.
.
.
comparison of unsupervised summarization with supervised approach we now compare the results of our unsupervised techniques with three supervised algorithms used by rastkar et.
al on the sds dataset5 ec this classifier was basically developed for summarizing emails and was trained on enron email corpus .
this was chosen for its similarity to the bug report corpus.
emc this is similar to ec but was trained on a combination of email threads and meetings where the meeting data comes from ami meeting corpus .
brc this is the only classifier that was trained on bug reports from the same projects as the test bug reports.
they used a linear classifier from the liblinear toolkit6 table gives the numbers for pyramid precision precision recall and f score for the three supervised techniques and four unsupervised techniques.
based on this data following observations can be made even though brc achieves the highest pyramid precision and precision both the unsupervised methods mmr andcentroid out performed it on the overall f score numbers.
mmr was almost at par with brc in terms of pyramid precision.
the unsupervised method mmr performed much better than the ec and the emc techniques with improvement in all measured metrics.
centorid was almost at par in precision and provides better recall and f score.
this suggests that probably the email and meeting corpora are not as similar to the bug reports as was perceived before in .
also this shows if the domain of the classifier is not similar to the domain of the data then the returns are lower than using an unsupervised summarizer.
the unsupervised methods grasshopper divrank when they worked provided much better recall and f score than brc and almost similar levels for precision.
5we did not apply the learning approach to db2 bind dataset as the implementation of the supervised approach was not available.
comparison of the unsupervised approaches against the supervised technique on sds dataset technique algorithm pyramid precision recall f score brc .
.
.
.
supervised ec .
.
.
.
emc .
.
.
.
centroid .
.
.
.
unsupervised mmr .
.
.
.
grasshopper .
.
.
.
divrank .
.
.
.
based on the above we conclude that unsupervised approaches are viable options to apply to summarize bug reports.
they outperform learning based approaches that are not trained on bug reports from similar domain.
.
rq2 impact of noise identifier on effectiveness of unsupervised techniques figure shows the observed change in average pyramid precision when applying noise reduction.
the black bar shows the precision without applying noise reduction.
the gray bar show precision when only the others sentences were filtered.
white bar shows the summarization precision when we also filtered out the code sentences.
the x axis lists the different summarization algorithms applied.
as is evident noise reduction was effective in both the techniques.
however in the sds dataset filtering away code and others sentences was more effective.
while on the db2 bind subject just filtering the others sentences worked better in three of the four techniques.
among the different summarization techniques noise reduction impacted centroid the least.
this is because centroid method inherently uses tf idf t d scores of terms to choose the summary sentences and was already filtering out those with low tf idf value.
the maximum pyramid precision value was obtained forsds using mmr .
while for db2 bind using grashopper .
.
on average across the four unsupervised techniques applying noise reduction improved the summarization efficacy by forsds and for db2 bind .
a question that arises is why did precision decrease in some case when we applied noise filtering.
there are two reasons for this our classification of sentences is not full proof.
it is likely that we classify summary sentences as others and remove them by filtering.
the goodness of classification is discussed in next section.
code fragments though they never show up in the summary they affect the calculation of centrality measure of other sentences.
by removing code sentences some oracle summary sentences which earlier had a high centrality measure has low centrality score and hence were removed from the calculated summaries.
one important thing to note is that after applying noise reduction both divrank andgrasshopper generated summaries for all the nine bug reports which they earlier missed across the two subjects.
.
rq3 goodness of noise identifier in this section we evaluate the goodness of noise identifier in terms of how well it is able to auto classify sentences.
we first present our approach to choose the cutoff value for identifying whether a word is a keyword or not.
then we present the correctness of the classification technique.
.
.
cut off value for identifying keywords as mentioned in the proposed approach section .
.
in order to apply noise reduction we first need to populate the keyword dictionary.
the keyword algorithm requires us to select a tf idf t d cut off to be used.
to calculate the cut off we followed the following process.
we extracted the list of keywords for both the subjects.
the average and median for tf idf scores for the subject sds was .
and .
and for db2 bind was .
and .
respectively.
for these four iterations of cut off scores .
.
and .
we executed the following process.
for each bug rank each sentence based on the sum of tf idf scores of the keywords present in each sentences.
then pick top of sentences as the summary and compare with the manual summaries provided by the annotators.
we refer to this process as the naive summarization .
the average precision recall and f score of the naive summarization across all annotators for both subjects is presented in table .
for sds there was no change in any of the metrics across the various cut offs indicating that sentences in the summary oracle had very high tf idf scores.
similarly for db2 bind there was hardly any difference in precision recall f score between and .
cutoff.
with .
cut off there was larger dip of .
in recall and .
in f score when compared to .
cut off.
based on this analysis we chose to apply .
as tf idf t d cut off score for both the subjects and populated the keyword dictionary.
.
.
goodness of classification table gives the details on the classification of the sentences across the four classes question code investigative andothers for both the subjects.
across all bug reports almost of sentences forsds and for db2 bind are marked as others .
also .
of sentences for sds and .
for db2 bind are marked as code .
in order to confirm the goodness of our noise reduction approach we calculated the overlap between the identified others sentences and the summary sentences in the oracle.
figure presents the distribution of bug reports across percentage of sentences classified as others which were actually used by the annotators in their summary.
the x axis represent the percentage distribution and the y axis the number of bug reports.
each segment in the vertical represents the number of bug reports per subject.
the total number of bug reports across both the subjects for each percentage overlap is listed on top of each bar.
as it is evident from the plot for db2 bind mo15 of the total bug reports have only or less summary sentences classified as others .
similarly for of sds bug reports out of only or less summary sentences are classified as others .
for of the bug reports across subjects no summary sentences were classified as others .
however for just bug report of sds the percentage overlap was more than .
to summarize the keyword based classification scheme was not full proof.
for db2 bind out of the sentences i.e.
in the oracle summary were incorrectly classified as others .
similarly f for out of oracle summary sentences were incorrectly classified.
however this is acceptable as even with less than correctness we did see an improvement in precision in both sds anddb2 bind after noise filtering and typically most automatic classification techniques are not completely fool proof.
.
discussion in this section we discuss our choice of unsupervised summarization algorithms and the threats to validity of our experiments.
.
choice of unsupervised summarization algorithms the summarization algorithms that we used namely centroid mmr grasshopper anddivrank form a representative set of unsu figure effect of noise reduction in unsupervised summarization.
average pyramid precision of four unsupervised techniques using noise reduction for sds left and db2 bind right .
table average precision recall and f score for different tfidf cut offs applied for naive summarization subject cut off precision recall f score .
.
.
sds .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
db2 bind .
.
.
.
.
.
.
.
.
.
.
.
table classification of sentences subjects total question code investigative others sds db2 bind pervised summarization algorithms in the existing literature starting from simple earlier methods to recent sophisticated methods.
out of these four methods centroid is the least sophisticated and is expected to perform the worst which is evident from the experiments.
however it is still interesting to note that this simple method is on par with the supervised ec and emc methods refer table .
also according to the literature and as discussed in section .
divrank is expected to perform better than grasshopper which is in turn expected to be better than mmr .
however our experiments show that mmr performed the best when applied without noise reduction.
this could be because divrank s performance was tuned to the social and citation author and paper network data in and may not be reproducible on all datasets.
divrank and grasshopper also missed generating summaries for bugs where the convergence was not attained owing to the graph size.
increasing the iteration cut off in the algorithm or reducing the size of the bug reports can help the algorithm complete.
the latter which required a research solution is precisely what we achieved through noise reduction.
once the noise reduction was applied the size of the bug reports largely reduced and for both these unsupervised techniques we were able to generate summaries for all the bugs in the datasets.
.
threats to validity threats to external validity arise when the observed results cannot be generalized to other experimental setups.
in our experiments we tried to limit this threat by evaluating a bug corpus of bug reports that had equal number of bug reports from different opensource eclipse projects and one internal industry project.
the number of sentences required to be summarized per bug report also varied from as low as to a maximum of .
however we cannot conclude how our observations might generalize to other projects.
figure distribution of bug reports with respect to percentage of sentences classified as others which were actually used by annotators in their summary.
more empirical evaluation with other projects is required to evaluate how our results may generalize.
these we intend to do in future research.
threats to internal validity arise when factors affect the dependent variables the data described in section without the researchers knowledge.
in our study such factors are errors in implementation of noise reduction.
moreover we used certain heuristics in classification of sentences into classes.
these include patterns for identifying code and question sentences cut off of three keywords chosen to classify a sentence into investigative sentence cut off of .
chosen on tf idf score to identify keywords.
these heuristics were based on manual inspection of bug reports in the test subjects and might not be generalizable to an unseen population of bugs.
they need re configuration depending on the subject to which noise reduction is applied.
.
related work summarization of documents is a well researched area starting from the first work published in by luhn for creating literature abstracts.
this was followed by a slew of papers that improved the summarization method from the simple tf idf based techniques to more complex natural language processing and machine learning based methods.
summarization has been applied successfully to many domains like news articles social media medical documents videos and audio in addition to technical documents.
it has also been incorporated into various products like ibm s intelligent miner for text and microsoft s office suite summarization techniques have been broadly classified into two types extractive and abstractive as discussed in section .
.
oneof the earliest methods for abstractive summarization is the summons system which extends a template driven document understanding method.
abstractive methods have also used language generation models in the context of summarizing arguments and controversial statements .
another broad classification of summarization techniques is as supervised and unsupervised methods.
supervised methods like those proposed in train a decision tree svm etc to learn sentences that belong to the summary.
however obtaining training data is usually costly which has led to unsupervised methods being more popular.
summarization methods proposed initially like considered only centrality of the chosen sentences.
however with the introduction of the concept of diversity in there has been considerable work in this area like .
more recent works have proposed using different types of random walks on a graph constructed out of the document for summarization like .
summarizing bug reports have been previously studied only in where they used supervised methods.
however text analytics and machine learning methods have been previously applied to software repositories and bug reports like for assigning reports to the correct person for estimating the time taken to solve a bug for detecting duplicate reports etc.
summarizing bug reports using unsupervised techniques and dealing with noise in the reports have not been studied before.
a different but related problem is extracting problem resolution information from various datasources like online discussion forums emails problem tickets etc.
this is similar in the sense that a summary can be constructed out of the extracted solutions.
however these tasks are more tuned to extracting solutions and are not constrained by any summary size nor do they give emphasis to diversity.
a related research problem is the issue of summarizing source code.
sridhara et al.
developed novel heuristics to automatically summarize a java method in the form of natural language comments .
they also developed heuristics to automatically detect and describe high level actions within a method .
.
conclusion in this paper we evaluated four unsupervised techniques for bug report summarization.
we compared the efficacy of the techniques with supervised approaches.
mmr divrank andgrasshopper algorithms worked at par with the best of the supervised approach.
for both the subjects the efficacy of the unsupervised techniques improved by applying noise identifier and filtering out sentences classified as useless andcode .
importantly two of the algorithms divrank andgrasshopper which did not converge for bug reports converged successfully for them and generated summaries once noise identifier based filtering was applied.
one area of future work is to see if we can improve the precision of summarization approaches so as to be able to auto extract frequently asked questions from a bug repository.
another direction for future work is to evaluate if the text summarization approaches mentioned in this paper can be used for code summarization.
the heuristics discussed in can be used to summarize methods to natural language.
given this natural language text augmented with comment text we can consider generating class and package level summaries.
acknowledgment we would like to thank ananthkumar peddi and randeep ghosh of ibm db2 team for manually summarizing the db2 bug reports.thanks are also due to sarah rastkar and gail c. murphy for making available the bug report corpus and the annotated summaries.
finally we would also like to thank giriprasad sridhara rema ananthanarayanan and karthik visweswariah of ibm research india for their valuable comments and suggestions.