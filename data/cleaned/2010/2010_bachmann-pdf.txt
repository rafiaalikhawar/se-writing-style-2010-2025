zurich open repository and archive university of zurich university library strickhofstrasse ch zurich year the missing links bugs and bug fix commits bachmann adrian bird christian rahman foyzur devanbu premkumar bernstein abraham posted at the zurich open repository and archive university of zurich zora url conference or workshop item originally published at bachmann adrian bird christian rahman foyzur devanbu premkumar bernstein abraham .
the missing links bugs and bug fix commits.
in acm sigsoft fse eighteenth international symposium on the foundations of software engineering check santa fe usa .
missing links bugs and bug fix commits adrian bachmann1 christian bird2 foyzur rahman2 premkumar devanbu2and abraham bernstein1 1department of informatics university of zurich switzerland 2computer science department university of california davis usa bachmann bernstein ifi.uzh.ch cabird mfrahman ptdevanbu ucdavis.edu abstract empirical studies of software defects rely on links between bug databases and program code repositories.
this linkage istypicallybasedonbug fixesidentifiedindeveloper entered commitlogs.
unfortunately developersdonotalwaysreport which commits perform bug fixes.
prior work suggests that such links can be a biased sample of the entire population of fixed bugs.
the validity of statistical hypotheses testi ng based on linked data could well be affected by bias.
given the wide use of linked defect data it is vital to gauge the nature and extent of the bias and try to develop testable theoriesandmodelsofthebias.
todothis we must establish ground truth manually analyze a complete version history corpus and nail down those commits that fix defects and those that do not.
this is a difficult task requiring an expert to compare versions analyze changes find related bugs in the bug database reverse engineer missing links and finally record their work for use later.
this effort must be repeated for hundreds of commits to obtain a useful sample of reported and unreported bug fix commits.
we make several contributions.
first we present linkster a tool to facilitate link reverse engineering.
second we evaluate this tool engaging a core developer of the apache http web serverproject to exhaustively annotate commits that occurred during a six week period.
finally we analyze this comprehensive data set showing that there are serious and consequential problems in the data.
categories and subject descriptors d. .
metrics product metrics process metrics general terms experimentation measurement verification keywords case study apache bias tool manual annotation permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
fse november santa fe new mexico usa.
copyright acm ... .
.
.
introduction software process data especially bug reports andcommit logs are widely used in software engineering research.
the integration of these two provides valuable information on the history and evolution of a software project.
it is used e.g.
to predict the number and locale of bugs in future software releases e.g.
.
the two data sources are normally integrated by scanning through the version control log messages for potential bug report numbers conscientious developers enter this information when they check in bug fixes e.g.
see .
we used similar techniques in our previous work and in fact improved current practice by adding heuristics to check the results .
even so the links between program code commits and bug reports thus extracted cannot be guaranteed to be correct as they are reliant on voluntary developer annotations in commit logs.
in prior work we have shown that such data sets are plagued by quality issues furthermore these issues e.g.
incompleteness bias etc.
adversely affect applications and algorithms which rely on such data .
we defined two types of bias bug feature bias where only the fixes of certain types of defects are linked and commit feature bias where only the certain kinds of fixes or fixes to certain kinds of files are linked.
in addition to these data quality issues many researchers make questionable process assumptions for instance they assume that all the relevant bugs of a software product are actually reported in the bug tracking database of the project.
to truly understand defectreportingbiasandverifysuchassumptions wemustuncover theground truth we must analyze completely at least a time window of the commit version history of a project and precisely identify allthe commits that are defect fixes and those that are not.
to get at ground truth requires skill knowledge and effort one must compare successive versions understand the changes identify any relevant reported bugs in the repo and establish a link when possible.
this process must be repeated until we have a large enough sample for statistical analysis.
this is costly difficult and time consuming.
linkster is a convenient interactive tool integrating multiple queryable browseable time series views of version control history and bug report history.
linkster enables an expert to quickly find and examine relevant changes and annotate them as desired specifically linkster makes it easy to find defect fix commits.
we engaged an expert apache core developer dr. justin erenkrantz to use linkster to manually annotate full weeks including commit messages of the apache history.
this case study helped us toimprove the tool and yielded a trove of data to examine three research questions.
traditionally researchers have made several assumptions about the bug fixing reporting and linking phenomena.
the first two research questions reflect general internal validity concerns that arise when using linked bug data for software engineering research.
rq do the bug reporting and fixing practices of developers correspond to the assumptions commonly made by researchers?
second researchers have tended to gloss over the issue of whether automated tools that find links between commits and bug reports have false positives or false negatives.
rq how well does the automated approach of finding links between commits and bug reports work?
finally the linked set of bug fixing commits are a sample of the full set of bug fix commits.
we can check and see if this sample is biased in any detectible way.
rq is there any evidence of systematic bias in the linking of bug fix commits to bug reports ?
to our knowledge the only published study on this question is by aranda and venolia they analyzed the completeness and degree of truth in software engineering datasets and provided a partial answer to rq see sub section .
.
most studies do not even address data quality issues .
in addition we were able to qualitatively explore how theapache project actually uses software engineering tools such as bug tracker and version control systems yielding some rather surprising observations.
we begin with a discussion of related work section followed by an overview of the tools and processes section used in apache http web server project.
we then present section a description of linkster and details of the case study procedure evolving an apache core developer section .
in sections and we present our findings which we summarize briefly below finding a so called bug is not always a bug neither is a commit always a commit.
in other words in apache the most important bugs are not handled in the bug tracker but mentioned in the mailing list system and only a fraction of commits actually pertain to program changes rq .
finding wecomparedthemanualannotationswithdata produced by automated linking viz.
for false positives or false negatives the automated approach finds virtually all the commit log messages which contain a link to the bug tracking database rq .
sadly however many defect fix commits are un identified in the commit logs and thus are invisible to automated approaches.
finding in the manually annotated sample we find strong statistical evidence that different bug fixers vary in their linking behavior.
investigating further we find anecdotal evidence suggesting that factors such as experience ownership and the size number of files of the commit affect linking behaviour.
we also find that reporting bias affects theperformanceofabugpredictionalgorithm bugcache .
given the small size of the manually annotated sample the evidence here is mostly suggestive rather than statistically significant however it points out the strong need for further studies for if this type of reporting bias is confirmed as a widespread problem this is of serious fundamental concernto all empirical research that uses this type of linked bug fix data.
.
related work areas closely related to this research include data extraction and integration data quality in software engineering data verification in software repositories and our own previous work on data quality effects on empirical software engineering.
.
data extraction and integration software engineering process data such as bug reports and version control log files are widely used in empirical software engineering.
therefore the extraction and integration of this data is critical.
fischeret al.
presented a release history database rhdb which contains the version control log and the bug report information.
to link the change log and the bug tracking database fischer et al.searched for change log messages which match to a given regular expression.
later they improved the linking algorithm and built in a filemodule verification .
a similar approach to link the change log with the bug tracking database was chosen by other researchers.
all of them used regular expressions to findbugreportlinkcandidatesinthechangelogfile e.g.
.
in we presented a step by step approach to retrieve parse convert and link the data sources.
we improved the well established prior art enhancing both the quality and quantity of links extracted.
.
data quality in software engineering as discussed in empirical software engineering researchers have considered data quality issues.
space limitations inhibit a full survey we present a few representative papers.
koruandtian surveyedmembersof52differentmedium to large size open source projects with regards to defect handling practices.
they found that defect handling processes varied among projects.
some projects are disciplined and require recording of all bugs found others are more lax.
some projects explicitly mark whether a bug is pre release or post release.
some record defects only in source code others also record defects in documents.
this variation in bug datasets requires a cautious approach to their use in empirical work.
liebchen et al.
examined noise a distinct equally important issue.
liebchen and shepperd surveyed hundreds of empiricalsoftwareengineeringpaperstoassesshowstudiesmanage data quality issues.
they found only that explicitly referenced data quality.
four of the suggested that data quality might impact analysis but made no suggestion of how to deal with it.
they conclude that there is very little work to assess the quality of data sets and point to the extreme challenge of knowing the true values and populations.
they suggest that simulation based approaches might help.
bettenburg et al.
provided first analysis of bug report quality.
they investigated the attributes of a good bug report surveying developers and used it to develop a computational model of a bug report quality.
the resulting model allowed to display the current quality of a defect report whilst typing.
hooimeijer et al.
also analyzed thequality of defect reports and tried to predict whether the defect report will be closed within a given amount of time.
chenet al.
studied the change logs of three open source projects and analyzed the quality of these log files.
in we surveyed five open source and one closed source project in order to provide a deeper insight into the quality and characteristics of these often used process data.
specifically we defined quality and characteristics measures computed them and discussed the issues arose from these observation.
we showed that there are vast differences between the projects particularly with respect to the quality of the link rate between bugs and commits.
aranda and venolia provided a field study of coordination activities around bug fixing based on a survey of software professionals at microsoft.
specifically they studied bugs in detail and showed that i electronic repositories often hold incomplete or incorrect data and ii the histories of even simple bugs are strongly dependent on social organizational and technical knowledge that cannot be solely extracted through the automated analysis of software repositories.
they report that software repositories show an incompletepictureofthesocialprocessesinaproject.
while they studied bugs in detail we focus on commit history we employed an expert supported by a specially designed tool to fully annotate a sample of commits.
this data helped us uncover a some of the weaknesses of software repositories as well as b anecdotal evidence of systematic bias in bug fix reporting.
.
studying bias papersinempiricalsoftwareengineeringrarelytackledata quality issues directly see discussion earlier in this sectio n our earlier work is an exception.
in and we investigated historical data from several software projects and found strong evidence of systematic bias.
we then investigated potential effects of unfair imbalanced datasets on the performance of prediction techniques.
ideally all bug fixing commits are linked to bug reports then empirical research would consider all type of fixed bug reports.
however only some of the fixed bugs have links to the bug fixing commits.
this raises the possibility of two types of bias bug feature bias where only certain types of bugs are linked or commit feature bias whereby only certain types bug fixing repairs are linked.
either type of bias is highly undesirable.
with access to all the fixed bugs and the linked bugs we could check for bug feature bias.
our study suggested that bug feature bias does exist and also that it affects the performance of the award winning bugcache defect prediction algorithm .
in this work we have a fully annotated list of commits for the first time thus achieving ground truth for a subset of the apache dataset andthuswecananalyzethedataforcommitfeature bias.
in summary a few studies explicitly consider the quality of systematic bias in the data.
this study in contrast explores the implications of this behavior by attempting to unearth the ground truth by enlisting a core developer to annotate all commits and thus seek out quality and bias issues.
.
case study apache theapache http web server is an open source software system developed under the auspices of the apache software foundation.
apache is the most popular webserver on the internet serving over of all websites .
apache isalsooneofthemostpopularopensourceprojects among researchers.
it is widely used in current empirical software engineering research e.g.
and thus a good subject for an in depth examination of data quality.
.
project tools like many other open source projects apache uses the bugzilla1bug tracker and the svn2version control system.
in addition the apache software foundation provides officially maintained git3mirrors for all projects.
the apache project allows free access to the contents of all these tools.
apache also maintains a public mailing list for developers andapache users to discuss issues of concern.
.
data gathering and integration we retrieved processed and linked the apache http web server process data as presented in .
basically we downloaded all bugzilla bug reports and svn version control log files.
then we scanned each commit log message for indications of fixing a bug using a set of heuristics typically we look for bug report numbers in log messages.
this leads to a set of automatically extracted links between program code commits and bug reports.
this set of links is validated using another set of heuristics op cit .
.
apache dataset with our own rather modest resources we could only completely evaluate and manually verify a subset of the originalapache dataset.
therefore we had to sample the original dataset.
there were two choices random sampling or temporal sampling.
random sampling requires some rationale for selecting a sample e.g.
prior knowledge of the distribution of the relevant co variates to the study so that a sample representative of the population could be chosen.
it is difficult to decide a prioriwhat such co variates might be let alone their distribution.
so we chose to perform temporal sampling .
table apache datasets details dataset original evaluation dataset sample considered time period bug reports fixed bug reports4559 .
.
linked bug reports .
.
duplicate bug reports .
.
invalid bug reports .
.
different bug reporters commit messages transactions empty commit messages .
.
linked commit messages .
.
different developers 1see 2see 3see 4we define fixed bug reports as bug reports that have at least o ne associated fixing activity which means a status change to fi xed within the considered time period.a b c drag drop double click to view diff figure linkster screenshot with this approach we chose to verify all the commits in a given period.
with complete results for that period we can then revisit our earlier results and judge the quality against this limited but complete and accurate temporal sample.
to find a typical period for our evaluation dataset we analyzed the whole original apache dataset based on week long epochs.
then we chose a period of consecutive weeks that was as representative as possible to the overall original apache dataset in terms of its descriptive process statistics e.g.
similar proportions of bugs and commits .
table lists some basic software process statistics for both the original and the evaluation apache datasets including the finally defined time frames.
.
linkster the use of linkster simplified our domain expert s task greatly accelerating an otherwise tedious repetitive and inconvenient sequence of invocations of multiple tools.
figure shows a screenshot of linkster showing windows containing three kinds of information commit transactions including all the changed files a bug reports b and diff blame information for all of the lines in a file before and after a particular commit c .
linkster requires access to a version control system for file content and a database local or remote containing the raw mined repository and bug tracking information.
we use git as our backend repository format given its increasing popularity and ready availability of tools supporting conversion from competitors such as cvs svn etc.however for convenience linkster displays the revision ids from the original repository.
all notes links and annotations explained below made by the user are also recordedin the database to facilitate use and analysis thereof after annotation.
linkster efficiently displays integrates and allows inspection and annotation of information from all data sources.
linkster is written in python using the pyqt widget toolset and has been written with portability in mind.
we have successfully run it on linux os x and windows.
to our knowledge no other tool provides integrated project information in combination with functionality to annotate link commits.
hipikat which was developed at ubc is similar in that it creates links between different types of software artifacts.
however these links are based purely on heuristics and hipikat functions as a recommender system rather than a browsing and annotation system.
other tools such as evolens softchange or shrimp provide only part of the functionality but all existing tools hav e goals other than expert commit annotation.
softchange is a tool to aid software engineering researchbyvisualizingdata.
similarto linkster softchange integrates data from multiple sources such as version control systems releases and bug databases.
however softchange uses visualizations usually plots to answer questions e.g.
how many bugs are closed in each time period?
and does not allow annotation of data as linkster does.
evolens helps developers to understand the evolution of a piece of software by visualizing the software as well as metrics of the software over time.
the visual nature across time facilitates identifying design erosion and hot spots of activity.
linkster does not leverage advanced visualization techniques and integrates multiple types of data rather than just source code information.
shrimp integrates and visualizes source code docu mentation javadoc and architectural information to aid source code exploration.
linkster is more concerned with processrelated artifacts e.g.
changes discussions bug reports and fixes than understanding the source code itself.
.
commit information figure a shows the commit information window of linkster .
the top contains a list of commits that satisfy some query e.g.
commits within a time window or changes made by a particular author.
each line shows the revision identifier as used in the original repository commit time author and the first line of the commit message.
the entire commit message is shown in a tooltip when the mouse hovers over an entry.
when a commit entry in the list is selected the metadata is updated in the bottom half .
the list of files modifiedinthecommit isalsodisplayed.
doubleclicking a file brings up the blame diff information for the file allowing the user to examine the exact changes that were made.
for annotation purposes the user may select the reason s for the commit by checking boxes or drag and drop or remove a bug record from the bug information window into the list of bug ids which is populated with the set of automatically identified links between the commit and bug records.
finally the user may enter free form notes for the commit .
.
bug information figure b contains the bug information window .
the top portion is a scrollable list of bugs from the bug database.
each entry contains the bug id the date of creation and a one line summary of the bug.
hovering over an entry shows the bug severity in a tooltip.
any of these entries may be dragged to the bug idslist in the commit information window to indicate a commit that is associated with the bug.
selecting a bug entry populates the bottom half of the window with detailed information.
the left side contains short attributes of the bug while the right side displays the full bug description followed by all of the comments in chronological order with author and date.
clicking on thebug activity tab displays a list not shown of all changes to the bug record such as assigning the bug to a developer or marking a bug as closed.
each entry indicates when the change was made and who made it along with old and new values for the changed field as appropriate.
finally clicking on the fixing files tab presents a list not shown of all of the commits to files that are associated with the fix of the bug.
this list is comprised of files automatically or manually linked to the bug.
double clicking on any file in this list will bring up a blame diff window for the commit.
.
blame diff information figure c shows the blame diff information window for the changes to a file in a particular commit.
the left view shows the content of the file prior to the change and the right view shows the content after the change.
removed lines are prefixed with and are highlighted red and added lines are in green with a prefix.
each line is also prefixed with revision identifier of the commit that introduced the line.
selecting a line highlights all other li nes introduced in the same commit and also updates the metadata area with information about that commit.
thiscan help the user learn why when and by whom the line was originally added.
if additional information is desired double clicking a line will bring up a new blame diff window for the commit which introduced the line if for example one desires to see why a line that was removed in one revision was originally added in a prior revision .
an annotator can thus gradually step back through version history.
the views are synchronized such that scrolling up down left or right in one view causes the other to change accordingly.
the thumbnail view graphically shows the differences for the entire file with red indicating removed lines and green added lines.
clicking on a location in the thumbnail view will cause the pre and post views to jump to that location making it easier to identify and examine changes in larger files.
.
apache data ev aluation to address our research questions we began our evaluation with the creation of an evaluation dataset as defined in section .
.
armed with linkster to facilitate browsing and annotation we engaged the services of an informant an experienced apache developer dr. justin erenkrantz to manually annotate a temporal sample of commits using linkster .
clearly the quality of this completely annotated evaluationdatasetispredicatedontheexpertiseoftheannotator.
justin is a core developer of the apache http web serverproject since january the president of the apache foundation and serves on the foundation s board of directors.
he also develops for apache portable runtime apache flood and subversion5.
usinglinkster justin annotated each commit to flag it as abug fix an implemented feature request amaintenancetask orother.
with this information we obtain fully annotated commit data providing a complete picture of all the changes during the given period and how why by whom these changes were made.
this data can be used to verify our automated linking approach which includes mainly bug fixes and some feature requests .
indeed annotating program code commits dating back months or years in the past is a challenge even for an experienced core developer like justin.linkster was very helpful providing an integrated view of all the relevant information.
based on the log message the changed files and the file diffs of the changed files justin was able to annotate all commits and in most cases provided additional information about the commits.
justin s familiarity with the apache project gives us confidence that the results of our evaluation can be trusted.
in addition detailed discussions and interviews with him revealed facts about the tools and processes used in the apache http web server project and also ideas for improvinglinkster .
.
results all commits in our selected temporal sample were annotated.
in addition to the annotation into the four categories above bug fix feature request maintenance refactoring andother our informant helped us further sub classify the commits.
table summarizes the annotation results including the sub classification.
note a single commit can have many annotations e.g.
a commit may be annotated as both a bug fix and a feature request .
5see for more details.table linkster commit categorization non exclusive category sub category commits bug fix bug fix bug report bug fix bug report merge bug fix mailing list bug fix backport bug fix other feature request feature request documentation feature request backport feature request other maintenance maintenance documentation maintenance backport maintenance other other other documentation other backport other non functional other release other voting other other based on justin s insights into the apache development process we developed a second orthogonal categorization that was more consistent with the procedures within the project .
in contrast to our categorization this one assigns each commit exclusively to one of its processspecific categories backport forward port security fix bug fix documentation voting release orother.
table process specific commit categorization exclusive category commits backport forward port security fix bug fix documentation voting release other in the following sub sections we present our findings relativetotheresearchquestionspresentedinsection1.
wealso present additional findings based on interviews with justin.
.
bugs incognito contrary to conventional wisdom participants of the apache projectdo not report all the bugs solely through bugzilla .
we found that developers and professional users also make use of the apache mailing list to report bugs and provide bug fixes sometimes at the same time without reporting them in the bug tracker.
finding .
not all fixed bugs are mentioned in the bug tracking database.
some are discussed only on the mailing list.
as shown in table we have bug fix related commits in our evaluation dataset.
of them bug report are directly related to the bug tracking database.
othercommits contain a bug fix but are not the initial bug fix commit rather than a merge of versions which contain bug fixes indirectly bug report merge .
this means that only .
of bug fix related commits are documented in the bug tracking database.
for other commits of total identified by justin as bug fixes there are related discussions in the apache mailing list.
this leads to the discouraging observation that many bugs never appear in the bug tracking database but rather are onlydiscussed on the mailing list.
such a discussion often includes the bug fix provided by a non apache core developer.
according to justin these bugs are often the very important bugs especially because of the high attention by apache developers and the core community on the mailing list.
note also that reporting some types of bugs e.g.
security related ones on the mailing list is a practice explicitly requested by the apache foundation6.
unfortunately even knowing about the mailing list bugs it is hard to i identify and ii automatically mine them or extract information similar to a bug report stored in the bug tracking database such as status changes priority severity etc.
.apache svn revision see figure for instance is related to a bug discussed on the mailing list7.
if one were to inspect the mailing list message one would find almost no evidence that this was a bug fix.
finally justin found other bug fixing commits which have neither an associated bug report or mailing list message.
this phenomenon of under reporting of bugs is a big problem.
if important bugs are excluded from experimental data i.e.
many bugs are left out then the effectiveness of defect prediction models and the validity of statistical studies which rely on them being in the bug tracking database may be threatened.
this leads to the conclusion that not all fixed bugs are reported as bugs in the bug tracking database or in other words bugs go incognito .
.
backport incognito in theapache http web server project only a few developers are allowed to commit to an apache release version thus a bug fix on one release may actually have to be committed by someone else to an older or different release.
typically this process works as follows.
first a developer fixes a given bug and commits the new version to the current version under active development also known as the trunk .
ideally s he also refers to the related bug report in the commit log.
next at least two other developers review the changed code verify the changes and vote either for or against the fix this step is related to the voting commits as shown in table and .
finally if the votes are positive the fix is committed or merged to apache release versions which is called a backport.
as a result of this process we might find several different commits in the version history that fix the same bug.
finding .
to fix a bug in an apache release multiple similar commits by different developers are needed.
unfortunately backport commits are not that easy to identify by existing linking algorithms and heuristics frequently while the log message for original commit to the trunk refers to the bug report the backport commit log does not.
to worsen matters after the bug is actually closed 6see 7see .mbox 3c200509260627.
news.perlig.de 3ethere is a rigorous review verification and voting process before the backport is accepted and committed.
therefore the time difference between the backport commit and the status change to fixed on the bug report may rise to several days which again makes it difficult to link the bug with the commit.
as a result automated linking algorithms will largely ignore backport fixes.
arguably these are fixes are very important often they are involved in post release failures.
they should not be ignored by researchers engaged in hypothesis testing or defect prediction work.
alas finding themmayrequireextensive high expertisecombingthrough commit histories.
figure commit message of apache http web server revision .
impact of defect vs. cause of defect this is a thorny issue a defect in one project s code base might actually manifest as a failure in a different project.
thus some of the reported bugs in apache http web server have their root cause outside of theapache programcode.
apache usesexternallibraries aswellasapache commonsmodules.
therefore failuresinthe apache http web server evenifdulyreportedinthe apache bugtracking database may actually have to be fixed elsewhere.
the reverse is also possible.
the mod python8sub project maintains its own version control system repository and an apache project s main bug tracker independent jira issue tracker9.
mod python issue for instance was reported in the jira issue tracker but fixed in the apache program code.
finding .
developers sometimes fix bugs that are only reported in some other projects bug tracker rather than in their own and vice versa.
ideally we have a complete integrated source of all the bugs in the bug repository and all the fixes in the version control system.
our findings and indeed the widespread prevalence of cross project module reuse we can expect that this type of separation between causes and effects of defects is quite common.
given this it would be helpful if a report of a bug impacting one system would be transferred to the bug repository of the causingsystem and linked to fix in the version control of that system.
however given the poor linking behaviour when the cause and effect are in the samesystem we might expect that this type of cross system linking is pretty unlikely to occur.
.
commits incognito in earlier work we encountered the problem of unexplained commits e.g.
due to empty commit log messages.
sadly even an experienced developer would find it difficult to retrospectively reconstruct the explanation of an unexplained commit.
and .
even if we annotate all commits the cause of a commit still remains unspecified in some cases.
table and show the annotation sub classification and process oriented classification of all the commits in our eval uation dataset.
based on the values in table for commits .
we have a process specific annotation of other.
the reason for these commits therefore is not justified by one of the apache software engineering core tasks.
in addition most of the commits are not justified by a bug fix or feature request rather than for documentation voting .
or releases .
.
only .
of all commits have a functional impact on the software product feature requests and bug fixes including all backport which leads us to the conclusion that not all commits are commits that actually change the software.
for additional information to the quality and characteristics of the version control data we refer to our previous work presented in .
.
performance of the linking algorithm inearlierwork wereportedalinkingalgorithm whose performance was found to be best in class.
the fully annotated data provided the first known oracle to evaluate linking algorithms and so we evaluated ours.
finding .
the algorithm op cit finds most of the commit log messages that the developers linked to bugs reported in the bug tracker subject to the time constraints used by our algorithm.
inthechosentemporalsample ourlinkingalgorithmfound links between the commit messages and the bug tracking database.
justin also identified all these links we thus foun d no false positive links in our evaluation dataset.
in additio n to these justin found additional links.
seven did not satisfy our heuristic for valid links time constraint of days between commit and status change on the bug report and so our algorithm rejected them as invalid links.
hence we found three false negative links in our evaluation dataset.
the seven invalid links resulted from backport commits as explained earlier sub section .
.
these backports corresponded to bug fix links in the original trunk which in fact were successfully discovered by our algorithm.
unfortunately as we elaborated before even with a high linkingratebetweenthecommitmessagesandthebugtracker only a subset of the fixed bugs are considered.
hence bugs discussed on the mail discussion system are often left out by automated linking approaches.
.
performance of linkster linkster performed mostly as expected and justin was able to annotate all the commits of our evaluation sample dataset in one working day.
in the discussions with justin we found some minor issues which were promptly remedied.
in addition we found that the most important bugs are discussed in the mailing list system only.
therefore linkster has been extended to support browsing of messages from development mailing lists and also enables linking them to both bug reports and repository commits.
.
threats to validity this sub section discusses external and internal threats to validity that can affect the results reported in this section.
threats to external validity.
can we generalize from theresultsbasedonthe apache http web server dataset to other datasets?
software engineering tools and processes vary in different projects and therefore our findings based onapache may not generalize.
however our findings indicate that developers may use software process support tools for various goals not envisioned by its original developers such as version control systems for voting or mailing list systems for bug reporting .
it seems prudent to assume that theapache project is not a complete exception and that therefore the data used in studies of other projects may also lack important information.
another threat is the use of a single annotator justin .
getting the same data annotated by other developers and checking agreement would have been better we hope to do this in future work.
threats to internal validity.
did we choose our evaluation dataset well and properly analyze it?
we chose our time frame carefully however it may not properly represent the original apache dataset.
the annotation and classification were performed carefully by a very experienced apache core developer.
still there may be errors.
nonetheless according to justin the interesting practices of the apache developers are by no means exceptional to this time period.
.
commit feature bias revisited the manual annotation effort indicates that many bug fixes are not identified in the commit logs and thus are completely invisible to the automated linking tools used to extract bug fix data.
thus the linked bug fix commits are asampleof the entire group of commits.
however samples thus extracted have been central to many research efforts.
the natural question is is this sample representative or biased?
we seek to test for the two kinds of bias bug feature bias whereby only fixes to certain kinds of bugs are linked andcommit feature bias whereby only certain types of commits are linked .
earlier with access to the entire set of fixed bugs and the subset of linked bugs we could check for and did find bug feature bias lacking access to a fully annotated set of commits that tells us which commits are bug fixes we were previously unable to check for commit feature bias.
now with a fully annotated temporal sample of commits we can indeed check for commit feature bias.
commit featuresarepropertiesofthefileanditsrevisionhistory suchas size complexity authorship etc.. these are critical properties that have been studied in dozens of papers that test theories of bug introductions they are also the features used for bug prediction.
so it is important to test for commit feature bias and evaluate its impact.
in this section we describe some findings related to commit feature bias and its effect on a well known bug prediction algorithm bugcache .
we remind the reader that our sample size despite the time and effort required to gather even that much is not big enough to realistically expect to find statistically sign ificant support for answers to the questions discussed in this section.
however there are some takeaways we do find statistical support for the answer to one question and we do find some anecdotal answers for the other questions.
furthermore actual bias along any of the lines discussed here would have a highly deleterious effect on the external validity of theories tested using only the linked data.
mostimportantly we hope to convince the reader that such studies are important and need to be repeated and conducted at larger scales .
.
sources and extent of commit feature bias the first question arises naturally from the fact that there are different individual developers who may have different attitudes towards linking.
the simplest and most obvious question is as follows do different developers show significantly different linking behaviour?
the anonymized table of developers linking behavior indicates that this is the case p .
.
name linked not linked name linked not linked a b c d e f g h i j k l m n o p q total we now hypothesize several different specific possible motivationaltheoriesoflinkingbehavior.
inseveralcases there was a visually apparent signal in boxplots albeit none that were statistically significant.
the results are shown in figure .
we list them below but we caution the reader to interpret all these findings as at best anecdotal .
however it is important to bear in mind that actual bias influenced by any of the processes hypothesize below would be very damaging to the external validity of theories tested solely on t he linked data .
does the experience of the author s whose code is being fixed influence linking behaviour?
we hypothesized that the quest for greater reputation might incentivize people to link fixes when the code under repair belonged to an experienced and thus more reputable person.
we measuredthefixedcode s authorreputation asthegeometricof the prior commit experience of everyone who contributed to the fixed code.
the left most boxplot in figure is weakly suggestive that fixes made to code with more experienced authorship are more likely to be linked.
does the number of files involved in the bug fix matter?if more files are repaired in a bug fix perhaps the fix is more impactful this might motivate the fixer to more carefully document the change.
in fact the boxplot second from left in figure is suggestive that this might be the case with all the unlinked fixes being single file fixes.
are more experienced bug fixers more likely to link?
we might expect that more experienced developers behave more responsibly.
we measure experience as the number of prior commits.
the boxplot second from right suggests support for this theory with a noticeably higher median for the linked case.
are developers who own a file more likely to link bug fixes in that file?
one might expect that people fixing bugs in their own files are more likely to behave responsibly and link on the other hand there is a anti social reputation preserving instinct that suggests that they may belesslikely to link.
we measure ownership as the proportion of lines in the file authored by the bug fixer.
indeed the boxplot visually supports the anti social theory.
we created plots to evaluate two other theories are bug linked not linked0 1500apache weighted experience of revisionweighted experience commit count linked not linked1 6apache number of files committednumber of files committed linked not linked0 2000apache experience of the fixing authorexperience commit count linked not linked0.
.
.
.
.
.0apache file ownership of fixing authorfile ownership figure commit feature bias reading left to right weighted experi ence of the original authors of the fix inducing code number of files changed in the bug fix experience of the a uthor committing the bug fix proportion of fixed file owned by bug fix author at the time of the bug fix.
fixes to bigger files more likely to be linked?
anddoes the prior experience of the file owner influence linking behaviour?
and found no informal visual evidence supportive of these theories.
.
practical effects bugcache revisited the above analysis shows that the extent of bias in the data is significant and that the effort of finding the ground truth e.g.
through manual annotation with linkster leads to important insights.
but do those insights translate to practical impact?
in this sub section we investigate the impact of approaching ground truth in terms of changes in the accuracy of the award winning bugcache algorithm .
to that end we repeated our experiment showing the impact of bias using apache data .
specifically we departed from two different datasets the first dataset calledabelow contained all bugs introduced in the apache .
branch.
the second one contained the additional bugs found by justin called j .
table shows the resulting accuracies for training and predicting on each combination of these two datasets.
consider training on the extracted data aand predicting onthesamedata.
thisprovidesabaselineaccuracyof0 .
.
if the prediction is however performed on the dataset representing ground truth for the period of manual annotation a jthentheaccuracyfallsto0 .
.
weaccedethatdueto the limited manually annotated period the difference like all the differences in the table is not significant.
but as the following shows we can recognize a tendency.
alternatively consider adding the manually annotated bugs to the training set i.e.
training on a j .
in each possible prediction target i.e.
a j anda j we find that the availability of the additional information actually leads to an improvement in prediction accuracy.
this is especially impressive where the prediction target is aas it shows that the manually annotated bugs actually contain information relevant to the automatically extracted ones helping bugcache to find four additional bugs.
table bugcache prediction quality learning set test set accuracy confidence interval a a .
.
.
a a j .
.
.
a j .
.
.
a j a .
.
.
a j a j .
.
.
a j j .
.
.
.
discussion and conclusions in this paper we analyzed three main research questions andtriedtofind groundtruth inthecommitannotationsof a very popular software engineering dataset.
we used temporal sampling to define an evaluation subset of the original apache dataset and manually annotated all commits with the assistance of an apache core developer and the use of linkster .
as presented in our previous work bias in empirical software engineering datasets may affect results of applications which rely on such data .
unfortunately based on our data verification we found that things are even worse our findings cast doubt on some of the core assumptions made in empirical research.
specifically .
bugsoftengoincognitoastheyarenotalwaysreported as a bug in the bug tracker but e.g.
in mailing lists and .
commits not always clearly change the functionality of the program.
specifically weshowedthatnotallfixedbugsarereportedin the bug tracking database and most of the commits .
are not related to a bug fix or feature request which would introduce a program change rather than for documentation voting .
or releases .
.
in addition we presented the curious case of backport commits and the challenging impact of defect vs. cause of defect problem.
bothissueshaveanimpactonsoftwareengineeringdatasets.
consequently even though automated linkage tools are able to connect a remarkable number of commits to bugs reports many bugs sometimes the most critical ones never show up in the bug tracker and are therefore not linked.
this raises new issues concerning the validity of studies that rely on version control and bug report data only beyond what we reported earlier .
we presented a detailed examination of the bias in automatically linked set when compared to the manually linked set.
especially notable is the significant variation in linking behavior among developers and the anecdotal evidence suggestingthat bug fixing experienc e and code ownership play a role in linking behaviour.
we also showed that bugcache has a strong tendency to miss predictions if it is not trained on ground truth.
another implication of the work presented here is that empirical software engineering studies will need to take the whole software development social eco system revision control system bug tracking database mailing list systems email discussions discussion boards chats etc.
as well as thesedatafromother relatedprojects intoaccountinorderto elicit a more complete picture of the underlying development process.
this would allow to capture the nature of some of the bugs and commits that our informant tediously collected manually.
nonetheless this study is only a first step towards qualityapproved datasets and we acknowledge that we were only able to verify a small subset of the overall apache dataset.
therefore we hope to influence the community to seek more ground truth for more software engineering datasets.
granted such work would entail a significant manual labor but undoubtedly the resulting valuable improvements in data fidelity will serve the community well in years to come.
we seek mechanisms for fostering this community effort and welcome suggestions from readers to this end.
acknowledgment many thanks to dr. justin erenkrantz for the time he spent in zurich annotating commits and providing feedback to theapache dataset and linkster .
this work was supported by zurich cantonal bank bachmann u.s. nsf sod team and an ibm faculty fellowship bird rahman and devanbu and swiss national science foundation award number bernstein .
.