automati c construct ion of an effective training set for prioritizing static analysis warnings guangtai liang1 ling wu1 qian wu1 qianxiang wang1 tao xie3 hong mei1 institute of software school of electronics e ngineering and computer scien ce key laboratory of high confidence software technologies peking university ministry of education peking university beijing china lianggt08 wul ing07 wuqian08 wqx meih sei.pku.edu.cn department of computer science north carolina state university raleigh nc usa xie csc.ncsu.edu abstract in order to improve ineffective warning prioritizatio n of static analysis tools various approaches have been proposed to comput e a ranking score for each warning .
in these approaches an effe ctive t raining set is vital in exploring which factors impact the ranking score and how.
while manual approaches to build a training set can achieve high effectiveness but suf fer from low eff iciency i.e.
high cost exist ing automatic app roaches suffer from low effectiveness .
in this paper we propose an automatic a pproach for constructing an effective training set.
in our approach we select three categories of im pact factors as input attributes of the training set and propose a new heuristic for identi fying actionable warnings to automatically label the training set .
our empirical evaluations show that the precision of the top warnings for lucene for ant and for spring can achieve with the help of our constructed training set.
categories and subject descriptors d. .
reliability statistical methods f3.
program anal ysis g. correlation and regression analysis general terms algorithms experimentatio n measurement keywords static analysis tools warning prioritization training set constru ction generic bug related lines .
introduction lightweight s tatic analysis tools such as findb ugs pmd jlint and lint4j aim at detecting generic bugs by analyzing source code or bytecode against pre defined bug pat terns without executing the program .
compared with formal verification techniques such as model checking and theorem proving these bug pattern based tools use lightweight analy sis tec hniques and they are effective in detect ing generic bugs in large software .
however there are two main challeng es for these exis ting tools their reported warnings often have a high false positive rate and even if some warnings reveal t rue bugs they are not always acted on by developers .
researc hers pointed out that more effort s should be spent on r efining these tools warning reports .
to address these challenges faced by existing tools various prioritization approaches have been proposed to reorder warnings by assign ing each warning with a ranking score .
to calculate the ranking score for each specific warning different approach es use different impact factors e.g.
warning category warning pri ority warning accuracy code fe atures and code local ity .
however in order to generate an accurate ranking score for each warning assign ing reasonable weights for differ ent impact factors of the ranking score is needed but cha llenging.
a training set also called a sample set plays a key role in learn ing weights for different impact factors .
a training set consists of a vector of input attributes multiple impact factor s for a specific warning and an output attribute the warning being actionable or not .
a training set is used to train a predictor e.g.
a neural network or a na ve bayes classifier which learns and records a weight for each impact factor .
a training set can be constructed manually or automatically.
a manual approach usually achieves high effectiveness but suffers from low efficiency i.e.
high cost while an automatic approach achieves high efficiency but suffer s from low effectiv eness .
given that software related data is growing rapidly an automatic approach with higher effectiveness is highly desirable .
for example open source projects usually have thousands of revisions stored in their source code repositories .
among these revisions bug fix rev isions are those revisions aiming at fixing bugs and they can be further divided into two groups generic ones and project specific ones.
generic bug fix revisions are responsible for fixing generic bugs while project specific bug fix revisions are respons ible for fixing project specific bugs.
generic bugs are those bugs that appear across projects such as dead lock null pointer dereference and resource leak .
most static analysis tools can report only generic bugs unless developers permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific pe rmission and or a fee.
ase september antwerp belgium.
copyright acm ... .
.
write project specific bug patterns .
the lines modified in generic bug fix revi sions are called generic bug related lines and the lines modified in project specific bug fix revisions are called project specific bug related lines .
a warning reported by static analysis t ools may have multiple manifestations or multiple poss ible fixes but a good static analysis tool should be able to indicate at least one of th em to a deve loper and ideally should indicate lines that the developer can choose to fix.
therefore these gene ric bug related lines can be used to evaluate static analysis war nings and then construct a training set for warning prioritization.
however a previous approach by kim and ernst used all bug related lines directly to evalu ate these warnings.
actuall y we found that more than of bug related lines co mputed by their approach were related to project specific bugs which static anal ysis tools cannot detect or report .
those project specific bugrelated lines need to be eliminated when being used to evaluate static analysis warnings otherwise effectiveness would be greatly compromised as shown in our evalu ation section .
in this paper we propose an automatic approach to construct an effective training set for warning prioritization which is based on generic bug related lines .
to the best of our knowledge we are the first to point out the importance of generic bug related lines in evaluating static analysis warnings.
in addition we also propose an automatic technique for identifying generic bugrelated lines .
in order to automatically construct a training set for warning prioritization using generic bug related lines we need to a ddress the following challenges how to automatically identify generic bug fix revisions from thousands of revisions of open source projects?
how to accurately identify generic bug related lines for a specific revision?
how to construct a training set based on generic bugrelated lines i.e.
how to extract input attributes and label output attribute s for the training set ?
to address the first challenge we propose a technique based on natural language processing nlp to assist in identifying gene ric bug fix revisions effective ly.
to address the second challenge we propose a new algorithm to effectivel y identify accurate generic bug related lines .
to address the third challenge we select three categories of impact factors as a warning s input attributes descriptors of the warning statistics of warnings from different tools and feature s of the bugg y source code related to the warning we propose a new heuristic to label actionable warnings a war ning is labeled as actionable if it disappears in later revision s and is revised during a generic bug fix revision .
based on our approach we develop and r elease an online code defect analysis servic e codas odas.seforge.org which integrates multiple static analysis tools including fin dbugs p md jlint and lint4j and prioritize s warnings with the help of our predictor trained with our effective training set.
this paper makes the following main contributions the first to identify the importance of generic bug related lines and use them to automatically construct an effective training set for warning p rioritization .
new technique s to identify generic bug fix revisions and generic bug related lines .
empirical evaluations using open source projects which show the effectiveness of our training set construction .
a publicly available online defect analysis servic e which integrates four static analysis tools and prioritizes warnings with a predictor trained with our effective training set.
in the rest of th is paper section presents background on software repositories static analysis tools and ma chine learning workbench .
section presents an overview of our automatic approach of build ing a training set .
section describes the details of our training set construction .
section describes our evaluation result s. section discusses related work and section conclude s. .
background .
software repositories in recent years many software projects publish their software repositories over the internet.
these open source projects provide sufficient historical data for building a training set.
our approach compute s generic bug fix information for subject projects and uses the information to label the training set by determining whether the warnings are actionable or not .
we retrieve bug fix inform ation with the help of software repositories such as sourc ecode repos itories e.g.
cvs or svn and issue tracking system s. a source code repository keep s track of changes performed on source cod e who changed what when why and how.
a change transforms a n old revision r1 to a new revision r2 by inserting deleting or modify ing lines.
source code repositories handle revisions of textual files by storing the difference between revisions.
when comparing two different files they use diff tools to find groups of differing lines .
.
static analysis tools static analysis too ls for java such as findbugs fortify pmd jlint and lint4j are widely used in recent years .
these t ools use bug pattern based matching technique s to detect potential bugs and report warnings .
findbugs generates warnings for bug patterns and assigns each warning with a priority e.g.
high medium or low according to its severity and accuracy.
pmd disco vers suspicious or abnormal coding practice s which may imply serious bugs by sea rching syntactic errors and stylistic conve ntions violation s from source code .
jlint finds bugs inconsi stencies and synchronization problems by data flow analysis and building lock graph s. lint4j detects issues about locking and threading problems about performance and scalability and violations against complex contracts such as java serialization by performing different analys es e.g.
type data flow and lock graph analys es against bytecode or source code.
.
machine learning workbench in our approach we use machine learning based predictors to provide ranking score s for warning s. there are two phases for machine learning based approach es in the training phase predictor s learn and record weights for impact factors with the help o f training set s and in the prediction phase predictor s predict ranking scores for warning s according to the given values of impact factors .
an effective training set which comprises a set of input attributes and an output attribute for each warning is vital in exploring how each input attribute affects the output attribute for a warning .
in this paper we use a machine learning workbench named waikato environment for knowledge analysis weka to help carry out our training process .
weka is a popular suite of machine le arning software written in java .
it suppor ts standard data mining tasks such as data preprocessing clustering classif ication regression visualization and feature selection .
it also pr ovides implementations for various machi ne learning algorithms e.g.
bayesian network logistic regression bootstrap aggregating random tree k nearest neighbor s and decision table .
.
approach overview we extract a training set with warning s reported by static ana lysis tools label it with the help of generic bug related lines and use it to train a predictor that can be further used to prioritize static analysis warnings.
figure shows the overview of our a pproach .
source code repository identifying generic bug fix revi sions identifying generic bug related lines training set generating static analysis warnings gene ric bug fix revi sions extracting a training set issue tracking system training a predictor predictor prioritizing warnings original warnings reordered warnings build ing a training set warnings generic bug related lines subject program figure .
approach overview table .
project s under analysis project name number of revisions development period community source code repository issue track ing system lucene .
.
apache svn jira spring .
.
apache svn jira ant .
.
sable.mcgill svn bugzilla log4j .
.
apache svn bugzilla jpf .
.
objectweb svn ow2 gforge .
preparing historical data most open source projects publish their source code reposit ories and issue tracking systems over the internet.
the source code reposit ory of a project is responsible for recording all change hi stories of its source code and its issue tracking system maintains all detailed information of its issues e.g.
bug issues .
we first obtain source code repositories and issue tracking dat abases of five open source projects under analysis before building our training set.
table describes these projects.
.
buildi ng a training set our approach includes four main steps in building a training set identifying generic bug fix revision s. generic bug fix revisions are those revisions submitted to source code rep ositories with the purpose of fixing generic bugs only which most static analysis tools can detect.
these revisions provide hints to locate generic bug related lines.
section .
shows more d etails.
identify ing generic bug related lines .
generic bugrelated lines are lines modified or removed by generic bugfix revisions.
these lines are helpful in labeling our training set.
section .
shows more details .
generating static analysis warnings.
we generate war nings by running static analysis tools against different rev isions of different projects under analysis.
extrac ting a training set .
we select three categories of impact factors as input attributes of our training set i.e.
warning descript ors statistic s for warnings of different tools and features of the warning related source code and extract them from warnings and the warning related source code.
we label the output attribute of each warning in the training set with the help of generi c bug related lines .
section .
shows more details.
.
traini ng a predictor to prioritize war nings weka provides implementations of various machine learning algorithms.
we select the implementations of s ix well know algorithms in weka to train predictors bayesian network logistic regression k nearest neighbor s bootstrap aggregating ra ndom tree and decision table .
we use default setting values of each algorithm during the training process and use folds cross validation during the validation proces s. to explore the most suitable machine learning algorithm for our problem we use the six select ed algorithms to train different predictors against the same training set extracted from the five project s under analysis .
the validation result s show that the k nearest neighbor s based predictor achieves the best precision .
recall .
and f measure .
.
therefore we select the k nearest neighbors as the best machine learning algorithm for our problem and use it as the default algorithm in our evaluation section .
after training the predictor with our training set we use the predictor to prioritize static analysis warnings we first use t he predictor to predict the actionability v alue for each warning the probability value that the warning is actionable and t hen priori tize all warning s according to their actionability values .
.
training set construction in this paper we build a training set for warning prioritization by automatically determining whether static analysis warnings are actionable i.e.
accurate and wor th fixing for developers .
a warning reported by static analysis tools may have multiple m anifestati ons or multiple possible fixes .
however a good static analysis tool should be able to indicate at least one of th em to a developer and ideally should indicate lines that the developer can choose to fix.
because most warnings provided by static analysis tools are related to generic bugs we use generic bug related lines which are modified or deleted by developers in generic bug fix revisions to evaluate static analysis warnings.
in order to identify generic bug related lines we identify generic bug fix revisions first since the changes made in these generic bug fix revisions are related to generic bugs .
.
identifying generic bug fix revisions open source projects usually include thousands of revision s in their source code repositories.
we classify these revisions into three c oarse grained categories bug fix revisions non fix rev isions and multi purpose revisions.
bug fix revisions are those revisions aiming at fixing bugs.
according to the type of their fixed bugs fix revisions can be further divided into two groups generic bug fix revisions and project specific bug fix revisions.
non fix revisions do not involve fix activities but involve other activities such as new feature addition and code refactoring .
multi purpose revisions are those revi sions that involve not only fix activities but also non fix activities.
open source projects are common ly co developed by develo pers distributed all over the world.
to facilitate their cooperation strong guidelines for writing the log message of each revision are undertaken .
chen et al.
studied the quality of open source change log and found that almost all log message s are consistent with their corresponding submitted change s. therefore it is reasonable to identify generic bug fix revisions with the help of log message s. in previous work two techniques were proposed for identify ing bug fix revisions based on analyzing log message s identification based on bug issue key reference s and ident ification based on searching bug fix related keywords .
these two techniques achieve d acceptable precision and r ecall in identifying bug fix revisions .
however through our evaluation we find that about of bug fix revisions identified by the first technique for lucene are project specific bug fix ones and about by the second technique are project specific bugfix revisions .
therefore we cannot directl y use these two tec hniques to identify generic bug fix revisions .
in this paper we propose an automatic technique for identify ing generic bug fix revisions.
figure presents the pseudo code of our algorithm for identifying generic bug fix revisions.
the algorithm performs one time identification process for each revision r. first it computes the number n of java files mo dified by revision r. by bound ing the value of n line most multi purpose revisions and non fix revisions are filtered out and most bug fix revisions are kept.
the rationale of this step lies in that through our investigation we find that usually generic bugfix revisions modify only a few files whereas multi purpose rev isions and most non fix revisions usually modify a lot of files.
by bounding the number of modified java files we first remove most multi purpose revisions and non fix revisions keeping most generic bug fix revisions .
then line determines whether the log message l of revision r contains a bug issue key.
a comm on phenomenon exists for ope nsource projects when committing bug fix revisions developers tend to include only the bug issue key and avoid re describing the bug issue in the log message since there is already a copy of detailed description for the bug in the issue tracking system see figure .
therefore if the log message contains a bug issue key the corresponding issue description issuedes is first retrieved from the project s issue tracking system line and then used to conduct the following id entification process because the issue description is more detailed than the log message in this case.
lines are responsible for computing the maximal similar ity maxsimilarity between issuedes the description of the bug issue and each bugdes the d escription of each generic bug that static analysis tools can detect .
in this process based on the theory of vector space model vsm the similarity between the issuedes and each bugdes is measured by computing the cosine value of the angle between their corresponding vectors .
when maxsimilarity is large enough l ine the revision r is identified as a g eneric bug fix revision.
if the log message does not contain a bug issue key the log message is used directly to compute the maximal similarity maxsimilarity with each bugdes lines .
when maxsimilarity is large enough line the revision r is also identified as a generic bug fix revision.
figure .
the algorithm for identifying generic bug fix rev isions issuetracking system sourcecode repositoryissue key type new featureissue key type bug fixed bug 00666linking a bug issue with a bug fix revision ... revision ...issue key type improvement figure .
associating a bug fix revision with a bug issue by unique issue key figure .
the pie chart of generic bug fix revisions divided by the number of their modified java files in our algorithm three threshold values represented as a b and c should be determined empirically .
before determining the va lues we first iden tify generic bug fix revisions manually and algorithm identify genericbugfixrevisions begin .
foreach revision r do .
compute n the number of java files modified i n r .
if n a then .
maxsimilarity .
if the log message l of r contains a bug issue key then .
extract issuedes description of the bug issue .
foreach bugdes description of each generic bug do .
compute similarity s between issuedes and bugdes .
if s maxsimilarity then .
maxsimilarity s .
if maxsimilarity b then .
identify r as a generic bug fix revision .
else .
foreach bugdes do .
compute similarity s between l and bugdes .
if s maxsimilarity then .
maxsimilarity s .
if maxsimilarity c then .
identify r as a generic bug fix revision end 96randomly from all revisions of the five subject projects including lucene ant spring log4j and jpf .
as a result revisions are identified as generic bug fix ones.
determination of a. in order to identify the value of a the threshold of the number of the modified java files for most gene ric bug fix revisions we conduct a statistical analysis on the number of their modified java files for the generic bug fix revisions .
figure shows the pie chart of the se generic bugfix rev isions divided by the number of their modified java files.
we can find that and of them modify only and java files respectively.
in total most generic bug fix revisions about modify no more than java files.
in add ition we also find that most multi purpose revisions about modify more than files.
according to this statistical analysis result we set a as .
determination of b. in order to determine the value of b the threshold of the maximal similarity between the description of a bug issue and the description of each generic bug that static anal ysis tools can detect we execute the algorithm against only those revisions whose log message s contain bug issue key s a subset of the generic bug fix revisions because the identification process for only those revisions is affected by b. we fix a as and c as c can be set as an arbitrary value since it does not affect the identification process for those revisions .
we set b as diffe rent values from to .
increased with .
and for each value evaluate the algorithm s precision recall and f measure value respectively .
figure shows the resu lt. from the result we determine that the f1 value of the algorithm is the best when b is set as .
.
figure .
the precision recall and f1 of the algorhim again st revisions whose log message s contain bug issue keys figure .
the precision recall and f1 of the algorhim a gainst revisions whose log message s contain no bug issue key determination of c. in order to determine the value of c the threshold of the maximal similarity between a log message and the description of each generic bug that static analysis tools can detect we execute the algorithm against only those revisions whose log message s contain no bug issue key because the ident ification process for only those revisions are affected by c. we fix a as and b as .
.
we set c as different values from to .
increased with .
and for each value evaluate the algorithm s precision recall and f1 value respectively .
figure shows the result .
from the result we determine that the f1 value of the alg orithm is the best when c is set as .
.
our automatic identification algorithm can reduce large manual efforts for developers in ident ifying generic bug fix revisions but its precision about is not high enough so far.
after the a utomatic identification process we conduct manual verification on its result to remove false positives e.g.
spending about .
man hours on automatically identified generic bug fix revisions .
.
identify ing generic bug related lines after generic bug fix revisions are identified the next step is to ident ify which lines of a specific revision have been deleted or modified during generic bug fix changes .
a code line l is a gene ric bug related line if and only if l is modified or removed during any generic bug fix change.
figure .
the identification algorithm of generic bug related lines for a revision x figure presents our algorithm for identifying generic bugrelated lines.
it takes as input x the number of the revision whose generic bug related lines need to be computed .
the algorithm returns brls x the generic bug related lines of the revision x as output.
the algorithm uses each generic bug fix revision whose rev ision number is larger than x to compute generic bug related lines for revision x. when revision n is a gene ric bug fix revision and its revision number is larger than x lines the diff inform ation diff n n between revision n and revision n is obtained first line .
then the generic bug related lines brls n n of revision n modified to result i n revision n are identified by parsing diff n1 n line .
for each generic bug related line of brls n n its introducing revision number is retrieved from the project s source code repository line and then compared with x line .
if the introdu cing revision number is smaller than or equal with x it means that this line was introduced no later than revision x and thus an identical generi c bug related line must exist in revision x. then the identical line in revision x is located an d mark ed as a generic bug related line of revision x lines .
example .
figure illustrates an example on identifying g eneric bug related lines for revision .
all generic bug fix revisions whose numbers are larger than are listed in figure function identifygeneric bugrelatedlines foraspecific revision input x the number of the revision whose generic bug related lines are to be compute d output brls x the generic bug related lines of the revision x begin .
foreach generic bug fix revision n do .
if n x then .
get the dif f info diff n n between revision n and n .
parse diff n n to identify brls n n generic bug related lines of revision n modified to resul t in revision n .
foreach generic bug related line brl in brls n n do .
retrieve the introducing reversion number y of brl .
if y x then .
identify the corresponding line brlx of brl in revision x .
add brlx to brls x end 97revision and rev ision .
revision modified the lines of the top two gray blocks of revision .
therefore these lines are identified as generic bug related lines of revision .
the introducing revision number of the lines in the uppermost gray block of revision is which is smaller than indica ting that these lines must also exist in revision .
therefore their corresponding lines in revision are located and marked as generic bug related lines of revision .
however the introducing revision number of the line in the middle gray block of r evision is larger than indicating that these lines do not exist in revision and the analysis for these lines terminates.
after that the next generic bug fix revision r evision is analyzed with the same process.
aft er analyzing all the generic bug fix revisions we finish ident ifying the generic bug relate d lines of revision as three lines .
revision revision generic bug fix revision revision generic bug fix revision revision under analysis if x !
y z !
y tom tom if x !
null ken if y null kim if y null kimif x y z y ken ken marking markingfix fix fixif x !
y z !
null tom tomif x !
y z !
y tom tom if x null kit if y !
null ben figure .
identifying generic bug related lines for revision .
generating static analysis warnings since different tools ar e complementary with each other to some degree we extract a training set with warnings from multiple tools.
we generate w arnings of four tools findbugs pmd jlint and lint4j by running codas which integrates the four tools tightly against the selected revisions of the projects under analysis .
.
extrac ting a training set a training set which is used for training a predictor is a set of effective warning examples actionable warnings and non actionable warnings .
an effective training set sho uld be accurate representative fair and abundant .
our training set is extracted based on warnings analyzed source code and generic bug related lines .
table lists input attributes impact factors of our training set.
these factors can be divided into three categories warning descriptors.
a given warning s descriptors e.g.
the pattern name and tool name are taken directly from the warning report .
statistics for warnings from different tools.
these factors include the number of warnings that are reported for the same warning location the same file and the same project by each tool findbugs pmd jlint and lint4j.
the rationale behind these factors is that if there are more tools reporting more war nings for the same piece of code the code is more fault prone and the war nings generated from the code are more accurate and a ctionable.
source code features.
we consider eight factors that may provide insight into the warning related code s features.
the first one is the depth of the warning related code in the file indicating that how far down this warning is in the file in terms of the percentage of the lines of the file.
some other factors are code length comment length and comment code ratio where the length denotes the number of lines .
the rationale behind these factors is th at more comments may reflect higher maintainability and quality of the code.
the fifth factor considers how far down this warning is in the method of the warning related code named warning related method in terms of the percentage of the lines of the warning related method.
the length of the warning related method is considered as the sixth factor.
the last two fa ctors are the number of callers and callees of the warning related m ethod.
the rationale behind these two factors is that a method with more callers is more likely to be the kernel part of the project which is often tested more sufficiently and is less fault prone a method with more callees tends to be of higher complexity a nd more fault prone.
table .
input attributes of our training set input attributes description warning descriptors pattern nam e bug pattern name of the warning tool name name of the tool reporting the warning statistics for warnings of each tool attribute s location warnings number of warnings reported for the same location by each tool file warnings number of warnings reported for the same file by each tool project warnings number of warnings reported for the same project by each tool source code features file depth how far down in the file this warning is code length number of lines of code of the warning related file comment length number of lines of comments of the warning related file comment code ratio the ratio of comment leng th and code length of the warning related file method depth how far down in the method this warning is method length number of lines of the warning related method method callers number of callers of the warning related method method callee s number of callee s of the warning related method we extract t he impact factors for warning s by collecting statistics for the warnings reported by each tool and conducting simple static analysis for the warning related source code.
the output attribute of a warni ng in the training set is isactionable which indicate s whether the warning is accurate and actionable.
if the location of a warning contains some generic bug related line s we can assume that this warning is very likely accurate and actionable for dev eloper s. however i t is often the case that some code fragment is related to several different war nings.
when t he fragment is rev ised to fix some certain warning i.e.
lines of the fragment are marked as generic bug related lines a problem arises other warnings could be wrongly ident ified as actionable ones .
to address the problem we use a new heuristic if a warning disappears in a later revision and also r eports at least one generic bug related line the warning is treated as actionable and its outpu t attribute is labeled as isactionable otherwise it is non actionable .
the rationale of this he uristic lies in that i f a warning disappears in a later revision without reporting any generic bug related line we can not regard it actiona ble because it ma y disappear due to some code changes introduced by code refactoring or new features addition if a warning reports some generic bug related line but still exists at later rev isions we cannot label it as actionable because the generic bug related line 98may be revised by developer s aiming at fixing its other associated warnings.
after constructing the training set the first three authors of this paper conduct manual verification on it with the principle of majority voting when there is no consensus durin g verification the opinion of majority people is adopted.
manual verification shows that about of actionable warnings in the training set are confirmed to be actionable.
before training a predictor we apply seven factor selection a lgorithms against our generated training set .
the final statistical analysis shows that there are four factor s input attributes ranked last with their effectiveness way behind the other factors by all of the seven algorithms method callers tool name location warni ngs of jlint and location warnings of lint4j .
such analysis result indicate s that these four impact factor s have small correlation coefficients with the output attribute .
therefore we ignore these four impact factor s when training a predictor .
.
evalua tion in this section we present the evaluation for the effectiveness of our proposed approach of constructing a training set with different strategies.
because a training set is used to train a predictor which is finally used to prioritize warnings we c an explore the best strategies of constructing a training set by evaluating the prioritized warnings.
given a certain training set we train a machine learning based predictor collect original warnings from multiple tools findbu gs pmd jlint and lint4j apply the pr edictor to predict actionability information ranking score for each warning and then reorder these warnings based on their action ability .
to evaluate the quality of reordered warnings we use the prec ision of top warnings as the criterion .
since developers are unlik ely to scrutinize all reported warnings due to their limited time the top warnings quality is of considerable significance .
we use precision to evaluate top warnings top n warnings precision p n n actionable n nactionab le represents the number of actionable warnings among the top n warnings.
an actionable warning should be accurate and also acted on during some generic bug fix procedure .
in our evaluation we address the following research questions rq rq1 how effec tive is our training set for warning prioritiz ation compared with previous work?
rq2 should we construct a training set from multiple tools warnings instead of a single tool s warnings for a given project under analysis?
rq3 should we construct a training set from multiple projects history instead of a single project s history for a given project under analysis?
.
rq1 effectiveness evaluation we evaluated the performance of our prioritization compared with kim and ernst s .
in their evaluation they pr ioritized warning s of three tools findbugs pmd and jlint for the revision submitted in august of lucene.
we also applied our approach to prioritize warnings of the three tools for the same revision.
in our approach we first constructed a training set from war nings reported by three tools findbugs pmd and jlint for five projects lucene spring jpf log4j and ant from revision s to m m is the maximum revision number for each project revisions for lucene for spring for ant for log4j and for jpf .
with the training set we trained a knearest neighbors based predictor and then used the predictor to prioritize warnings for that revision submitted in august which was among revisions m to m .
figure shows the precision of our prioritization kim s prior itization and the built in prioritization of each tool for lucene .
in figure and the remaining figures in this paper the x axis represents the top n warnings in the war ning list and the y axis represents the precision of the top n warnings co rrespondingly.
figure .
precision of top n up to warnings prioritized by our prioritization and kim s prioritization from figure we can observe that for kim s prioritization the best precision for a subset of the warnings is only top for lucene for our prioritization the precision of top warnings is always .
moreover our prioritization can keep the prec ision of top warnings always higher than tools default prior itization.
in summary the result shows that our priorit ization has a remarkable improvement over previous work and our training set is effective for warning prioritization.
.
rq2 training set construction using single tool vs. multiple tools figure .
prioritizing warnings based on training set co nstructed from warnings of the single tool findbugs figure .
prioritizing warnings based on training set s constructed using warnings of multiple tool s vs. single to ol in this section we made a comparison between a training set constructed using one single tool and that using multiple tools.
99for both strategies we attained warnings for revisions from rev isions to m of all the five projects under analysis the next section shows that using multiple projects can achieve better effectiveness than using one specific project in most cases .
based on the constructed training set we trained predictors to reorder warnings for revision m of the project s under anal ysis.
since the results of all projects are similar we show only the r esult of lucene here due to space limit.
figure shows the effectiveness of our prioritization when using one single tool fin dbugs here .
in figure rw findbugs denotes the reorde red war nings by our priorit ization bas ed on the training set built using findbugs and ow findbugs denotes the original warnings of findbugs.
the figure shows that our approach improved top warnings precision compared with findbugs default prioritiz ation.
figure shows results of our warning prioritization based on a training set constructed using warnings of f our tools findbugs pmd jlint and lint4j where rw all rw findbugs rw pmd rw jlint and rw lint4j denote the reordered war nings by our app roach based on the training set s extracted from all tools findbugs pmd jlint and lint4j respe ctively.
the result shows that constructing a training set using warnings of more tools can help warning prioritization achieve better effectiveness .
.
rq3 training set construction using single project vs. multiple projects given a program a under analysis which belongs to a project a under analysis we have three strategies to construct a training set for it using only histor ical data of project a using not only histor ical data of project a but also histor ical data from other project s b c ... using only histor ical data of other projects.
this strategy can be used in two situations i the project a is in an initial stage and thus lacks historical data ii the program a is sent to a third party analysis service such as codas where it is usually difficult to attain a s historical data .
we denote the three strategies as a a abc a and abc d with d denoting different projects respectively .
we show th e comparison results for these three strat egies in the subsequent subsections .
note that we construct ed the training sets for our prioritization using all the four tools and conducted the compar ison of warning prioritization for revision m of each progra m a under analysis .
.
.
a a vs. abc a for each project under analysis we constructed different training set s using two strategies a a using its own history and abc a using five projects histor ical data lucene spring jpf log4j and ant .
based on different training set s we trained different predictors to prioritize war nings.
the results of all projects are similar but due to space limit we show only part of them .
figure s show the results for lucene ant and spring respectively .
in figur es ours abc a and ours a a denote our prio ritization based on the strateg ies abc a and a a respectively.
from figure s we have the following observ ations both the abc a and a a bas ed prioritization outperform each tool s built in prioritization effectively.
for e xample for lucene the precision of top warnings of our a pproach is always more than .
in contrast the precision of each tool is always less than .
only one warning among top warnings of findbugs is actio nable whereas are actionable among top warnings of our approach and of them are pr ioritized on the top of the warning list by our abc a based pr ioritization .
for ant only of top warnings of findbugs are actionable while and warni ngs are a ctionable among top warnings of ours abc a and a a based prioritization respe ctively .
the abc a based prioritization is superior to the aa based prioritization for the three projects .
through analysis we find that the result s shown in figure s are reasonable.
our projects under analysis are all open source and their develo pers generally share the same guidelines or customs to fix bugs.
only one pro ject s history is limited and cannot supply enough kno wledge for a predicto r to learn.
a larger training set from more projects can help a predictor learn more.
therefore it is reason able that the abc a based prioritization performs better than the a a based prioritization due to the sufficiency of trai ning data.
figure .
a a vs. abc a based pr ioritization for lucene figure .
a a vs. abc a based pr ioritization for ant figure .
a a vs. abc a based pr ioritization for spring .
.
abc a vs. abc d we used the same training set constructed using history from lucene spring jpf log4j and ant to prioritize warnings for lucene and tapestry.
ther efore for lucene the prioritization is 100based on the abc a strategy whereas for tapestry the priorit ization is based on the abc d strategy.
figure s show the results.
from figure s we observe that both the abc a based prioritization and the abc d based prioritization ou tperform each tool s built in prioritiza tion effectively the abc a based priorit ization performs better than the abc d based pr ioritization.
in summary the abc a strategy can help construct a more effective training set than the abc d strategy.
figure .
abc a based prioritization for lucene figure .
abc d based prioritization for tapestry .
threats to v alidity the main threat to external validity include s the representativ eness of the subject projects that we selected in the evaluation .
therefore the results of our evaluation may be specific only to these projects .
to reduce this threat we chose projec ts from different open source communities and their types are different from each another.
the threat could also be reduced by more evalu ations on more subjects in future work.
the main threat to internal validity is human factors when manually verifying t he generic bug fix revisions automatically identified by our approach and the actionable war nings in our constructed training set .
to reduce the threat the first three authors of this paper conducted the verific ations with the principle of majority votin g and great carefulness .
.
related work to the best of our knowledge we are the first to propose an automatic approach to construct an effective training set for warning prioritization and point out the importance of generic bug related lines for the problem .
there are several threads of related work however th ey use different techniques to prioritize warnings.
kremenek and engler developed a ranking algorithm z ranking to prioritize warnings for a single warning t ype i.e.
a single checker .
they observ ed that clusters of warnings were usually either all false positives or all true bugs .
a generalization of this work produced an adaptive rankin g scheme called fee dback rank .
this work used code locality to identify clusters of false posit ives and true bugs among warnings.
their algorithm updated warning priorities as nearby warnings were classified .
in their approach based on their observation they selected code locality an impact factor to directly prioritize warnings.
ho wever their obse rvation is not generic for all warnings.
boogerd and moonen use d execution likelihood analysis to prioritize warnin gs.
for each warning location they compute d the execution likelihood an impact factor .
if the location was very likely to be ex ecuted the warning was assigned a high priority.
this technique m ight help developers to focus on warnings at location s that had high execution likelihood.
however warnings at location s with low execution likelihood could be also important.
in fact seve re bugs in lines with low execution likelihood are more difficult to detect.
heckman proposed an adaptive mo del that used feedback from developers to rank warnings .
the d evelopers feedback r esulted in updating weights for warnings.
her approach was effective in favorably ra nking and identifying false positive warnings.
h er approach tried to dynamically collect bug fix inform ation a training set from developers feedback to improve warning prioritization.
however developers are usually too busy to provide feedback when using static analysis tools.
williams and hollingsworth used software change hist ories to improve existing static analysis tools.
when a function return s a value using the value without checking it may be a p otential bug.
the problem is that there are too many false positives if a static analysis tool warns all locations that use unchecked return values.
to reduce the false positives they used the software histor y to find w hich kinds of function return values must be checked.
however they focus ed on only a small set of bug pa tterns while our approach is general to all war nings.
ruthruff et al.
use d logistic regression models to predict the categories of warnings from information in the warnings and the related code.
their work was aimed to classify warnings rather than ranking all warnings.
the training data for their model was manually labeled by google s developers.
kim and ernst estimate d the importance of warning categories by mining bug fix information .
they used all bugrelated lines to identify whether a warning is a true positive or not.
in our evaluation we found that more than of bug related lines computed by their approach were about project specific bugs which static analysis tools cannot detec t or report .
those project specific bug related lines should be eliminated when ev aluating static analysis warnings otherwise high inacc uracy could be introduced.
in our work we point ed out the importance of generic bug related lines for evaluating st atic analysis warnings proposed an automatic approach to compute them and used them to construct an effective training set for war ning prioritization .
.
conclusion in this paper we highlighted the importance of an effective training set for warning prio ritization and proposed an automatic approach to construct an effective training set by mining generic bug fix history .
in our training set we summarized a set of impact factors for warning prioritization as its input attributes and l abeled it with the he lp of generic bug related lines .
we pointed out the impo rtance of generic bug related lines and proposed a new technique based on natural language processing nlp to assist in identifying generic bug fix revisions effectively .
we also proposed a new algorithm to effectively identify accurate generic bug related lines and conducted evaluations for our approach .
althoug h our training set is effective there exist some limit ations.
first it is possible that some important bugs never got n oticed or f ixed even when a tool would report them.
second the 101warning location s and fix loc ations for a bug may have no overlap with each other.
sometimes a dding new code may fix an existing warning.
for example if a warning is about unused import stat ements in j ava it could be fixed by addi ng code that uses the i mports .
third our current identification technique for generic bugfix revisions has not achieve d sufficiently high accuracy.
some human intervention is still needed during this phase.
forth w e do not conduct code movement detection when computing bugrelated lines.
if some lines are moved from the original location to a new location the source code repository record s only that the moved lines are deleted at the original loc ation and some new lines a re added at the new location.
without code movement detection there could be some lines falsely d etermined as bug related lines.
in our future work we plan to explore better heuri stics for identifying actionable warnings and more automatic and accurate techniques to identify generic bug fix inform ation.
.
acknowledgment the authors from peking university are sponsored by the n ational basic research program of china grant 2009cb320703 and the science fund for creative research groups of china grant and the national science foundation of china grant .
tao xie s work is supported in part by nsf grants ccf ccf cns aro grant w911nf and aro grant w911nf managed by ncsu sosi.
.