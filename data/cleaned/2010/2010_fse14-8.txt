an empirical analysis of flaky tests qingzhou luo farah hariri lamyaa eloussi darko marinov department of computer science university of illinois at u rbana champaign urbana il usa qluo2 hariri2 eloussi2 marinov illinois.edu abstract regression testing is a crucial part of software developmen t. it checks that software changes do not break existing functionality.
an important assumption of regression testing i s that test outcomes are deterministic an unmodified test is expected to either always pass or always fail for the same code under test.
unfortunately in practice some tests often called flaky tests have non deterministic outcomes.
such tests undermine the regression testing as they make it difficult to rely on test results.
we present the first extensive study of flaky tests.
we study in detail a total of commits that likely fix flaky tests in open source projects.
we classify the most commonrootcausesofflakytests identifyapproachesthatcoul d manifest flaky behavior and describe common strategies that developers use to fix flaky tests.
we believe that our insights and implications can help guide future research on the important topic of avoiding flaky tests.
categories and subject descriptors d. .
software engineering testing and debugging general terms measurement reliability keywords empirical study flaky tests non determinism .
introduction regression testing is a crucial part of software development.
developers use regression test suites to check that software changes do not break existing functionality.
the result of running a regression test suite is a set of test outcomes for the tests in the suite.
the outcomes are important for developers to take actions.
if all the tests pass developers typically do not inspect the test runs further.
if any test fails developers reason about the cause of failure to understand whether the recent changes introduced a fault in the code under test cut or whether the test code itself needs to be changed .
the key assumption behind this process is that a test failure indicates that the recent changes introduced a problem in the cut or the test code.
permission to make digital or hard copies of all or part of thi s work for personal or classroom use is granted without fee provided th at copies are not made or distributed for profit or commercial advantage an d that copies bear this notice and the full citation on the first page.
to cop y otherwise to republish to post on servers or to redistribute to lists re quires prior specific permission and or a fee.
fse november hong kong china copyright acm ... .
.unfortunately test outcomes are not reliable for tests tha t canintermittentlypassorfailevenforthesame codeversio n. following practitioners we call such te sts flaky1 their outcome is non deterministic with respect to a given software version.
flaky tests create several problem s during regression testing.
first test failures caused by fl aky testscanbehardtoreproduceduetotheirnon determinism.
second flakytests waste timewhen theyfail evenunaffected by the recent changes the developer can spend substantial time debugging only to find out that the failure is not due to the recent changes but due to a flaky test .
third flaky tests may also hidereal bugs if a flaky test fails frequently developers tend to ignore its failures and thus could miss real bugs.
flaky tests are not only problematic but also relatively common in large codebases.
many practitioners and researchers have pointed out that flaky tests can be a big and frequent problem in general but the only specific numbers we could obtain2are that the tap system at google had .6m test failures on average each day in the past months and 73k out of .6m .
test failures were caused by flaky tests.
the current approaches to combat flaky tests are rather unsatisfactory.
the most common approach is to run a flaky test multiple times and if it passes in any run declare it passing even if it fails in several other runs.
for example at google a failing test is rerun times against the same code version on which it previously failed and if it passes in any ofthose 10reruns itis labeled as aflakytest .
sever al open source testing frameworks also have annotations e.g .
android has flakytest jenkins has randomfail and springhas repeat to label flakytests that require a few reruns upon failure.
another approach would be to remove flaky tests from the test suite or to mentally ignore their results most of th e time in the limit ignoring the failure every time is equiva lent toremovingthetest .
injunit the ignore annotation is used to exclude a test from the test suite to be run.
however developers are reluctant to use this approach becaus e flaky tests may still provide some coverage and could help find regression bugs.
although the current approaches used to deal with flaky tests may alleviate their impact they are more workarounds rather than solutions.
they do not address the root causes of flaky tests and can potentially waste a lot of machine resources with test reruns or reduce the effectiveness of the test suite with flaky test exclusion .
flaky sometimes spelled flakey means unreliable .
2personal communication with john micco.findings about flaky test causes implications f.1the top three categories of flaky tests are async wait concurrency andtest order dependency .i.1techniques for detecting and fixing flaky tests should focus o n these three categories.
f.2most flaky tests are flaky the first time they are written.
i.2techniquesthat extensivelycheck tests when they are first a dded can detect most flaky tests.
findings about flaky test manifestation implications f.3almost all flaky tests are independent of the platform i.e.
could fail on different operating systems or hardware even if they depend on the environment e.g.
the content of the file s ystem .i.3techniques for manifesting flaky tests can check platform de pendence lower in priority than checking environment dependen ce e.g.
event ordering or time especially when resources are limi ted.
f.4about thirdof async wait flaky tests use a simple method call with time delays to enforce orderings.i.4manyasync wait flaky tests can be simply manifestedby changing time delays of order enforcing methods.
f.5mostasync wait flaky tests do not wait for external resources and involve only one ordering.i.5mostasync wait flaky tests can be detected by adding one time delay in a certain part of the code without the need of control ling the external environment.
f.6almost all concurrency flaky tests contain only two threads or their failures can be simplified to only two threads and o f their failures are due to concurrent accesses only on memory objec ts.i.6existing techniques of increasing context switch probabil ity such as could in principle manifest most concurrency flaky tests.
f.7manytest order dependency flaky tests are caused by dependency on external resources.i.7not all test order dependency flaky tests can be detected by recording and comparing internal memory object states.
man y tests require modeling external environment or explicit reruns w ith different orders .
findings about flaky test fixes implications f.8manyasync wait flaky tests are fixed using waitfor .
which often completely removes the flakiness rather than jus t reducing its chance.i.
for developers explicitly express the dependencies between chunks of code by inserting waitfor to synchronize the code.
for researchers comparing the order of events between correct runs and failing runs techniques could automatically inse rt orderenforcing methods such as waitfor to fix the code.
f.9various concurrency flaky tests are fixed in different ways are fixedby addinglocks are fixedby makingcode determin istic and are fixed by changing conditions.
our results are consi stent with a study on concurrency bugs .i.9there is no one common strategy that can be used to fix all concurrency flaky tests.
developers need to carefully investigate the root causes of flakiness to fix such tests.
f.10mosttest order dependency flaky tests are fixed by cleaning the shared state between test runs.i.
for developers identify the shared state and maintain it clean before and after test runs.
for researchers automated techniques can help by recording the program state before the test starts execution and comparin g it with the state after the test finishes.
automatically generating code in setup teardown methods to restore shared program state such as static fields could fix many test order dependency flaky tests.
f.11fixing flaky tests in other categories varies from case to cas e.i.11there is no silver bullet for fixing arbitrary types of flaky te sts.
the general principle is to carefully use api methods with no ndeterministic output or external dependency e.g.
time or network .
f.12some fixes to flaky tests modify the cut and most of these cases fix a bug in the cut.i.12flaky tests should not simply be removed or disabled because they can help uncover bugs in the cut.
table summary of findings and implications despite the pervasiveness of flaky tests in practice they have not drawn much attention from the research community.
the few recent efforts focus on only one category of flaky tests due to test order dependency .
in this paper we present the first extensive study of flaky tests.
we analyze in detail commits that likely fix flaky tests from open source projects from the apache software foundation.
for each flakytest we inspect thecommit log message thecorrespondingbugreport ifany andthecorrespondin g patch to determine the root cause of the non deterministic outcome and the way it was fixed.
we also examine how the flakiness was introduced in the test suite and how it could be manifested.
we focus our study on the following questions that we believe could provide insights for practitioners and researc hers what are the common causes of flakiness?
by studying the root causes of flaky tests we reveal the most prominent categories of flaky tests for developers and researchers to focus on.
our study of how flakiness is introduced also suggests the best stage to identify flaky tests.
how to manifest flaky test failures?
by studying the possible ways to manifest flaky test failures we suggest howautomatedtechniquescoulddetectunknownflakytests.
what are the common fixing strategies for flaky tests?
by studying how developers fix flaky tests in practice we provide insights for both developers about som e principled ways for avoiding certain kinds of flaky tests andfor researchers about the potential techniques that could a utomatically fix flaky tests.
table shows the summary of our findings and implications.
the remaining sections discuss these in more detail.
.
methodology our goal is to provideactionable information about avoiding detecting and fixing flaky tests.
to that end we focus on identifying and analyzing version control commits that likely fix flaky tests .
one can view each flaky test as a bug in the test code whereas it can produce a non deterministic rather than deterministic outcome.
most empirical studies of bugs start from bug reports .
however we start from commit logs because we are mostly interested in flaky tests that are fixed.
starting from commits gives us a larger dataset than we would get starting from bug report databases .
first some fixes are made without ever being reported in bug report databases.
second some reports from bug report databases are open and not fixed.
in brief every fixed flaky test is reflected in the version control system but may not be reflected in its bug report database.
to identify commits that likely fix flaky tests we choose to search through the central svn repository of the apache software foundation .
this repository hosts a diverse se t of over top level projects written in various programming languages having varying sizes and being actively de intermit flak totaltotal w. bug reportshbase activemq hadoop derbyother projects all commits commits about flaky tests ldfft commits inspected commits async wait concurrency test order dependency resource leak network time io randomness floating point operations unordered collections hard to classify table summary of commit info and flaky test categories veloped for varying amount of time from months to years by a large open source community.
wefirstextractthecompletecommithistoryofallprojects from the apache software foundation.
to identify commit messages that may indicate a fix of a flaky test we search for the keywords intermit and flak .
one could search for more keywords but these two already find enough commits for several months of inspection.
more precisely the searc h yielded commit messages.
table shows the distribution of these between the two keywords.
after collecting these commit messages our study proceeds in two phases.
filtering phase.
the first phase in studyingthese commits is to identify those that are likely about fixing flaky tests.
we manually inspect each commit and if needed the bug report s associated with this commit.
to increase confidenceinourinspection twoofthepaperauthorsseparatel y inspect each commit and then merge their results.
the upper partoftable 2shows thesummaryofthis initial labeling phase.
we discuss the lower part of table in section .
ourgoal is todetermine for each commit whether itis likely about reporting or fixing a flaky test labeled commits about flaky tests in table and it attempts to fix a distinct flaky test labeled ldfft commits in table .
we find commits that are likely about flaky tests the other commits match intermit or flak but are either about the cut not about the test code or just incidental matches e.g.
ausername that contains flak .
ofthese commits are likely distinct fixed flaky tests ldfft commits the other commits either report that certain tests are flaky but do not provide fixes for those tests or are duplicates e.g.
multiple attempts to fix the same flaky test or multiple copies of commits from one development branch to another .
for duplicates we keep only the latestcommit about the distinct flaky test because we want to study the most recent change related to the flaky test e.g.
the most effective fix is usually the latest fix .
comparing the results across the two keywords we find that intermit is used more often than flak to indicate a flaky test.
the numbers of commits for intermit are larger than the corresponding numbers for flak both in absolute terms vs. and in relative terms vs. .
comparing the column for all commits and the column for the commits that have a bug report we can see that a large fraction of fixes for flaky tests have no bug report.
for example ldfft commits have no bug reports while such commits have bug reports.
hence we could have missed a large number of commits related to flaky tests if our methodology relied solely on bug reports.the key result of this phase is a set of ldfft commits.
table shows the number of these ldfft commits across various projects.
at least projects out of the projects in apache likely have at least one flaky test.
for each project we tabulate the programming language s that it uses the number of ldfft commits the number of commits that have at least one associated bug report and the total number of lines of code in the project computed by the cloc script .
we can see that flaky tests occur in a diverse set of projects using various languages ranging ove r various sizes and implementing code for various domains.
analysis phase.
we study in more depth a subset of the ldfft commits selected as follows.
first we sort the projects according to their number of ldfft commits and split them into two groups small less than ldfft commits and large greater than or equal to ldfft commits .
table shows a line separating these two groups.
then we select to inspect all the commits from the small groupandsampleonethirdofthecommitsfromeachproject from the large group.
we are thus covering all the projects from the apache software foundation where we identified some ldfft commits and our results are not overly biased toward the projects with the largest number of ldfft commits.
this sampling gives us a total of out of ldfft commits to inspect.
for each of these ldfft commits we first have one of the authors examine the commit in detail.
our goal is to answer the following set of questions is the commit indeed fixing a flaky test?
what is the root cause of the flakiness for the test?
how can the flakiness be manifested?
how is the test fixed?
the answers are then inspected and confirmed by another author.
the following sections discuss these answers.
.
causes of flakiness we first analyze the root causes of test flakiness and classify them into categories.
we then study when flakiness is introduced in tests.
.
categories of flakiness root causes we analyze in detail ldfft commits to classify the root causes of the flakiness likely fixed by these commits.
we precisely classify the root causes for commits while the remaining commits are hard to classify for various reasons as described later.
we split the root causes of flakiness into categories.
some of these categories have been previously described byproject language ldfft commits bug reports loc hbase java activemq c java scala hadoop java derby java harmony c c java lucene java tomcat java servicemix java zookeeper java qpid c java cxf java web services c java tuscany java flume java maven java openjpa java oozie java aries java continuum java subversion c python tapestry java mesos c flex java httpcomponents java accumulo java python kafka scala hive java ambari java jena java apr c jackrabbit java sling java openejb java mahout java avro java npanday c cassandra java uima java roller java portals java ode java buildr ruby pig java camel scala archiva java xmlbeans java spamassassin c shindig java mina java karaf java commons java total multiple table many projects contain flaky tests practitioners and others we have identified by study inganumberof similar flakytests.
the lower part oftable shows the summary of our analysis.
the first column lists the categories and the last row shows the commits that are hard to classify.
the remaining columns tabulate thedistributionofthesecategories overvarioustypesofc ommits and various projects.
we highlight the number of flaky tests for the top four projects and the last column sums up the numbers for the remaining projects.
we next discuss in more detail the top three categories that represent of the studied commits.
we finally briefly summarize the other seven categories.
.
.1async wait out of commits are from the async wait category.
we classify a commit into the async wait category when the test execution makes an asynchronous call and does not properly wait for the result of the call to be come available before using it.
for example a test or the cut can spawn a separate service e.g.
a remote server or another thread and there is no proper synchronization to wait for that service to be available before proceeding with the execution.
based on whether the result becomes availablebefore orafter itisused thetestcannon determini stically pass or fail .
while our focus is on understanding flaky tests we point out that lack of synchronization can also lead to bugs in the cut even if it does not manifest in flaky tests.
in fact bugs caused by asynchronous wait fall in a subcategory of concurrency bugs that lu et al.
call order violation because the desired order of actions between multiple threads is not enforced.
we classify async wait as a separate category of root causes from concurrency because it represents a large percentage of flaky tests with some common characteristics that are not shared among all tests that are flaky due toconcurrency .
we next describe an example async wait flaky test.
this snippet is from the hbase project test 2public void testrsreportswrongservername throwsexception 3minihbasecluster cluster test util.gethbasecluster 4minihbaseclusterregionserver firstserver minihbaseclusterregionserver cluster.getregionserv er 6hserverinfo hsi firstserver.getserverinfo 7firstserver.sethserverinfo ... sleep while the region server pings back 10thread.sleep 11asserttrue firstserver.isonline 12assertequals cluster.getliveregionserverthreads .size ... similarly for secondserver the test uses a cluster to start a server firstserver and then uses thread.sleep to wait for it to ping back.
if theserver doesnot respondinatimelymanner e.g.
because of threadschedulingor network delay thetest will fail.
t he test has similar code for secondserver .
basically the test intermittently fails based on how fast the server responds.
so far we have described the root cause of this flaky test but as a preview of our overall analysis we describe how we found this commit and how the developers fixed the flakiness.
this commit matches the keyword flak as its commit message reads hbase testmasterwrongrs flaky in trunk which refers to the bug report id hbase2684 .
the test was added in revision3948632 and was flaky ever since.
the developers fixed this test in revision by making two changes.
first they replaced each thread.sleep statement for firstserver andsecondserver with a call to cluster.waitonregionserver that waits until the server responds back and then removes its corresponding thread.
second to ensure that the test does not run indefinitely in case that the server cannot start for some reason the developers added timeout to the testannotation which fails the test after seconds.
these changes completely removed the flakiness of this test the developers removed the assumption that the server will respondwithin twoseconds andexplicitly expressedthecon dition to wait for before resuming the execution.
table furthercategorizes howmanyfixescompletelyremovedflakiness and howmany just decrease the probability of flakiness.
3all the revision numbers refer to the apache software foundation svn repository .
.
.2concurrency out of commits are from the concurrency category.
we classify a commit in this category when the test non determinism is due to different threads interactin g inanon desirablemanner butnotduetoasynchronouscalls from the async wait category e.g.
due to data races atomicity violations or deadlocks.
the source of non determinism can be either in the cut or in the test code itself.
out of cases are due to non determinism in the cut and manifest by the intermittent failure of a corresponding test.
note that nondeterminism in the test or code execution may or may not be a bug the code could indeed have several correct different behaviors.
however if the test incorrectly accepts onl y a subset of these as passing behaviors then the test has non deterministic outcome and is definitely flaky.
we next describe an example concurrency flaky test where the cause is non determinism in the cut.
this snippet is from the hive project 1if conf !
newconf 2for map.entry string string entry conf 3if entry.getkey .matches hcat.
newconf.get entry.getkey null 5newconf.set entry.getkey entry.getvalue 8conf newconf the code iterates over a map shared by multiple threads if the threads modify the map concurrently a concurrentmodificationexception is thrown .
this code led to flaky failures in several tests.
the developers fixed this by enclo sing the lines to in a synchronized block making them execute atomically.
we classify this case in the atomicity violation subcategory of the concurrency category.
our study finds the main subcategories of concurrency flaky tests to match the common bugs from concurrent programming data races out of atomicity violation s out of and deadlocks out of .
but we also identify a new prominent subcategory that we call bug in condition out of .
we classify a commit in this subcategory whensomemultithreadedcodehasaconditionthat inaccurately guards what threads can execute the guarded code.
the problem is when this condition is too tight or too permissive.
an example is in the project lucene with bug report lucene .
the test is flaky because a code portion should be executed only by the thread named main but the condition does not guard for that so when another thread executes the code the test fails.
the fix was to strengthen the condition to check if the thread name is main.
.
.3test order dependency out of commits are from the test order dependency category.
we classify a commit into this category when the test outcome depends on the order in which the tests are run.
in principle all tests in a test suite shou ld be properly isolated and independent of one another then the order in which the tests are run should not affect their outcomes.
in practice however it is not the case.
this problem arises when the tests depend on a shared state that is not properly setup or cleaned.
the shared state can be either in the main memory e.g.
the static fields in out of cases are hard to classify in any subcategory.java or some external resource e.g.
files or databases .
e ither a test expects to find the state as it was initialized but meanwhile another test changed that state i.e.
running on e polluter test before another test fails the latter test or a test expects the state to be set by the execution of another test that was not run i.e.
notrunning one setup test before another test fails the latter test .
hence dependenci es among tests result in unpredictable behavior when the test order changes.
for example the update from java to java changed the order in which junit finds the tests in a test class due to the change in the reflection library .
we next describe an example test order dependency flaky test.
this snippet is from the hadoop project beforeclass 2public static void beforeclass throwsexception 3bench newtestdfsio ... 5cluster newminidfscluster.builder ... .build 6filesystem fs cluster.getfilesystem 7bench.createcontrolfile fs ... check write here as it is required for other tests 10testwrite the snippet is from a test class where one test testwrite writes to a file via fspreparing data to be read by several other tests.
the developers incorrectly assumed that testwritewould always run first but junit does not guarantee any particular test ordering.
if junit runs some read test beforetestwrite the test fails.
the developers fixed this by removing the original testwrite test and adding a call to testwrite in the beforeclass as shown in line .
.
.
other root causes webrieflydiscuss the otherseven categories of root causes of flakiness.
due to space limitation we do not give a detailed example for each category.
the relative ratio of flaky tests in these categories can be computed from table .
resource leak.
a resource leak occurs whenever the application does not properly manage acquire or release one ormore ofitsresources e.g.
memoryallocations ordataba se connections leading to intermittent test failures.
network.
tests whose execution depends on network can be flaky because the network is a resource that is hard to control.
in such cases the test failure does not necessaril y mean that the cut itself is buggy but rather the developer does not account for network uncertainties.
from the result s of our inspection we distinguish two subcategories of flaky tests whose root cause is the network.
the first subcategory is due to remoteconnection failures and the second subcategory is due to localbad socket management .
time.
relying on the system time introduces non deterministic failures e.g.
a test may fail when the midnight changes in the utc time zone.
some tests also fail due to the precision by which time is reported as it can vary from one platform to another.
we discuss platform dependent tests later sec.
and while theyare not frequent it is ea sy for developers to overlook the differences among platforms.
io.i o operations in addition to those for networks may also cause flakiness.
one example we encountered was in the project archiva where the code would open a file and read from it but not close it until the filereader gets garbage collected.
so a test that would try to open the same file would either pass or fail depending on whether the filereader was already garbage collected or not.randomness.
the use of random numbers can also make some tests flaky.
in the cases that we analyzed tests are flaky because they use a random number generator without accounting for all the possible values that may be generated .
for example one test fails only when a one byte random number that is generated is exactly .
floating point operations.
dealing with floating point operationsisknowntoleadtotrickynon deterministiccas es especiallyinthehigh performancecomputingcommunity .
even simple operations like calculating the average of an ar ray require thorough coding to avoid overflows underflows problems with non associative addition etc.
such problem s can also be the root cause of flaky tests.
unordered collections.
in general when iterating over unorderedcollections e.g.
sets thecodeshouldnotass ume that the elements are returned in a particular order.
if it doesassume thetestoutcomecanbecomenon deterministic as different executions may have a different order.
other cases.
we did not classify the root causes for commits of those do not fix a non deterministic flaky test but rather fix some deterministic bug and do not actually fix a flaky test.
the other are hard to understand.
we inspected each commit for several hours reading the commit message and associated bug reports if any reasoning about the patched code and occasionally even trying to compile and run the tests.
however certain commits turned out to be hard to understand even after several hours.
if the test or code change proven to be too complex and there is not much information in the commit message or bug report to help us understand the root cause of flakiness we mark it as unknown.
we exclude all cases from our further study.
see finding f. and implication i. in table .
.
flaky test introduction we also study when the tests became flaky.
from the tests we categorized are flaky from the first time they were written became flaky at a later revision and others are hard to determine mostly because tests were removed or relocated to other files .
we analyze in more detail the cases in which the tests became flaky later and identify two main causes.
the first reason is the addition of new tests that violate the isolation between tests.
for example consider a test t1 that requires a shared variable to be zero but relies on the initialization phase to set that value rather than explicit ly setting the value before beginning the execution.
a newly added test t2sets the value of that shared variable to without cleaning the state afterwards.
if t2is run before t1 the latter would fail.
the second reason for introducing flakiness after a test is first written is due to test code changes such patching a bug changing the test functionality refactoring a test or incompletelypatchingsomeflakinessitself.
forthe152flak y tests for which we have evolution information we calculate the average numberof days it takes tofix a test tobe .
.
in sum tools should extensively check tests when they are first written but some changes can also introduce flakiness.
see finding f. and implication i. in table .
.
manifestation manifesting the flakiness of flaky tests is the first step in fixingthem.
inpractice givenatest failure thatissuspect ed to be from a flaky test the most common approach is torerun the failing test multiple times on the same code to find whether it will pass and thus is definitely flaky or will not pass and thus may be a real deterministic failure or might be still a flaky test that did not manifest in a pass in those multiple runs .
while such rerunning can be useful in some cases it has disadvantages.
when the probability for a flaky test to change its outcome is low rerunning it a few times may not be enough to manifest that it can change the outcome.
also rerunning the tests multiple times is time consuming especially if there are multiple test failures.
while inspecting each flaky test in our study we have considered possible ways to automatically trigger the fail ure oftheflakytest eithertodefinitelyshowthatitcanbothfai l and pass or at least to increase the probability of its failur e under the assumption that it passes majority of the time .
our analysis leads to findings that could help in developing automatic techniques for manifesting flaky tests.
.
platform in dependency to understand how flaky tests manifest in failures the first question we want to answer is how many of those flaky failures only manifest on a particular platform.
by a platform we refer to the underlying system the test is running on including the hardware operating system jvm jre etc.
it differs from the environment mentioned later as it is not provided or controlled by the test or the application.
an example of platform dependence is a test failing due to a different order of files on a specific file system.
from the flaky tests we categorized we find that out of161 haveoutcomethatdoesnotdependontheplatform.
namely the test failures only depend on the events in the application code and not on any system call to the underlying platform.
of the cases where the flaky failures can only be reproduced on a certain platform tests require a particular operating system to manifest the failure require a particular browser to manifest the failure and only requires a specific buggy jre to manifest the failure.
see finding f. and implication i. in table .
.
flakiness manifestation strategies for each of the top three categories of root causes of flaky tests we next discuss how one could modify the tests from that category to manifest the flaky failures.
.
.1async wait how many async wait flaky tests can be manifested bychanging an existing time delay?
to enforce a certain ordering in test execution in many cases developers use asleeporwaitformethod with a time delay.
a sleep pauses the current thread for a fixed amount of time and then resumes its execution.
with waitfor we refer to a set ofmethodsusedtoeitherletthecurrentthreadbusywait for some condition to become true or block the current thread until being explicitly notified.
we find out of async wait flaky tests use sleeporwaitforwith a time delay to enforce ordering.
their flaky failures can be simply manifested by changing the time delay of such method calls e.g.
decreasing the sleeping time in the test.
for the other async wait flaky tests developers do not use a common method call with a time delay to enforce certain ordering but either do not enforce any ordering at all or use some application specific apis to enforce the desired ordering.
see finding f. and implication i. in table .how many async wait flaky tests can be manifested by adding onenew time delay?
we find two factors that determine the difficulty of manifesting async wait flaky tests.
the first is whether a test depends on external resources or not because it is harder to control externa l resources.
the second is whether the flaky failure involves only one ordering where one thread process is supposed to wait for an action from another thread process to happen or more orderings.
manifesting failures for multiple order ings would require carefully orchestrating several action s which is much harder than just delaying one action.
our study finds that the majority of async wait flaky tests out of do not wait for external resources.
the few that do wait include a test that waits for a specific process to be started by the underlying os and a test that waits for a response from the network.
our study also finds that most async wait flaky tests involve only one ordering.
in fact we find only out of cases with multiple orderings e.g.
a client waits for multiple servers to be started in a certain order.
overall out of async wait flaky tests do not depend on external resources andinvolve only one ordering.
their flaky failures can be manifested by adding only one time delay in the code without the need of controlling the external environment.
note that several test s can be manifested as flaky byeither addinga newtime delay or changing an existing time delay.
while finding the appropriate place to add the delay could be quite challenging researchers can attempt to build on the heuristic or randomizationapproachesthatweresuccessfullyusedinmanifest ing concurrency bugs .
see finding f. and implication i. in table .
.
.2concurrency how many threads are involved?
for all the flaky tests caused by concurrency we also study how many threads are involved in the test failure.
we find out that cases involve more than two threads.
seemingly contradictory lu et al.
found out that most concurrency bugsrequireonly two threads to manifest.
however our result does not contradict their finding because we study real tests that already contain multiple threads whereas they study bugs in the code and reason about tests that could have been written to expose those bugs.
to reproduce concurrency flaky failures all the existing tests with multiple threads that we studied could be simplified into at most two threads.
interestingly enough we even find one flaky test for which only one thread suffices to trigger a deadlock bug.
how many concurrency flaky tests do not depend on external resources?
we also find out that out of concurrency flaky tests do not depend on external resources.
in other words their failures are only caused by concurrent accesses to the objects in the main memory.
see finding f. and implication i. in table .
.
.3test order dependency we further study the source of dependency for each test order dependency flaky test.
we identify three sources of dependency.
we call the first one static field in test out of which means that several tests access the same static field declared in the test code without restoring the state of that field in a setuporteardown method.
we call the second one static field in cut out of which meanscategory fix type totalremovedecrease async waitadd modify waitfor add modify sleep reorder execution other concurrencylock atomic operation make deterministic change condition change assertion other test order dependencysetup cleanup state remove dependency merge tests table flaky test fixes per category that the shared static field is declared in the cut rather than in the test code itself.
test order dependencies of the first two kinds can be exposed by recording and comparing object states.
we call the third one external dependency out of which means that the dependency is caused bysome external environment suchas shared file or network port and not by a static field.
of the test order dependency flaky tests we find that more than half are caused by an external dependency.
those tests cannot be easily manifested by recording and comparinginternalmemoryobject states butinsteadrequir e more sophisticated techniques to model the state of externa l environment or to rerun tests with different order .
see finding f. and implication i. in table .
.
fixing strategies developers fix various root causes of flakiness in different ways.
we identify the main strategies that they use and extract insights for practitioners to use in manually fixing flaky tests and for researchers to use as starting points to develop tools to automate this process.
for the top three categories of flaky tests we give a detailed description of t he common fixes and discuss their effectiveness in removing the flakiness.
for the other categories we only briefly discuss the fixes.
some of the fixes for flaky tests change the cut so we also study those cases.
.
common fixes and effectiveness we describe the main strategies that developers use to fix flaky tests of different categories.
table summarizes the fixes for the top three categories.
an interesting property of these fixes is that they do not always completely eliminate the root cause namely they do not turn a nondeterministic flakytestintoafullydeterministictest.
r ather some fixes change the code such that the test is less likely to fail although it could still fail.
the column remove shows the number of fixes that completely remove the flakiness while the column decrease shows the number of fixes that only decrease the chance of failures in the flaky tests.
.
.1async wait common fixes.
the key to fixing async wait flaky tests is to address the order violation between different threads o r processes.
we describe the strategies used in practice and evaluate their effectiveness.
fixes using waitfor calls out of async waitflaky tests are fixed via some call to waitfor.
recall that those calls block the current thread until a certain con dition is satisfied or a timeout is reached.
the fixes add anew call modify an existing call either the condition or th e timeout or replace an already existing sleep of these fixes replace a sleepwith awaitforcall .
the latter shows thatwaitforisapreferredmechanismthatdevelopersshould use whenever possible.
also outof all the waitforfixes have a time bound while the others are unbounded.
fixes using sleepcalls 20outof74 async wait flaky tests are fixed by stalling some part of the code for a pre specified time delay using sleep.
of these cases increase the waiting time of an already existing sleep whil e the other add a sleep that was missing conceptually increasing the time delay from up to the specified bound in the added sleep .
this shows the effect that machine speed variation can have on flaky tests especially for those of the cases where the sleepwas already there but the waiting time was not long enough on some slower or faster machines leading to intermittent failures.
fixes by reordering code out of async waitflaky tests are fixed by reordering code.
instead of simply using sleeptowait for some time the developer finds a piece of code that can be executed such that the execution of that code achieves the delay and hopefully achieves the particular event ordering.
a benefit of executing some code rather than simply using sleepis that useful computation gets done but the problem remains that the developer cannot precisely control the time taken for that computation.
other out of async wait flaky tests are very specific to the code and hard to generalize.
for example some of these fixes use an application specific api method to trigger an event so the ordering is not violated.
effectiveness of fixes in alleviating flakiness.
using a sleepis rarely a good idea when writing tests.
as analyzed in our earlier work that focused on writing multithreaded tests the use of sleepmakes the test unintuitive unreliable and inefficient.
it is hard to reason from the sleep calls what ordering among what events they try to enforce.
moreover sleepcalls cannot provide the guarantee that the ordering that the developer wants to enforce among events will indeedhappenwithin the amountof time given in sleep.
for that reason developers tend to over estimate the time needed in sleepcalls which makes the tests rather inefficient because most of the time the event can finish way before the time bound and yet occasionally it does not finish before the time bound thus intermittently failing the test .
for all these reasons we find that the fixes where sleepcalls are used to fix async wait flaky tests are only decreasing the chance of a flaky failure running tests on different machines may make the sleepcalls time out and trigger the flaky failures again.
using a waitforis the most efficient and effective way to fixasync wait flaky tests.
because waitformakes explicit the condition that has to be satisfied before the execution can proceed it becomes much easier to understand what ordering the test expects.
moreover the execution is more efficient because it can proceed as soon as the condition is satisfied rather than waiting for some time bound when the condition may or may not be satisfied.
in fact table shows that out of cases with waitforcompletely remove the flakiness.
the remaining cases only decrease the flakiness because those waitforcalls are bounded with a timeout.
even when there is no explicit timeout on some waitfor the test itself can have a timeout.
such a timeout is practically useful for preventing a single test from hang ing the entire test suite forever.
we find that developers set much higher timeouts when using waitforrather than sleep in particular the average waiting time for waitforcalls in our cases is .
seconds while the average waiting time forsleepcalls is .
seconds.
the higher upper bound on waitformakes them more robust against flakiness when the condition that is being waited for gets delayed at the same time waitforis more efficient than sleepwhen the condition gets available earlier than expected.
while the tests withwaitformay still fail when run on an extremely slow machine using waitforis much more efficient and reliable than using a sleep.
reordering code is ineffective as using sleepcalls for the similar reasons.
the only advantage of reordering code over sleeps is that the waiting time is not purely idle time but rather some useful computation happens.
however the developer does not have a precise control over the amount of time taken for that computation.
hence the flaky tests can still fail after the code is reordered.
see finding f. and implication i. in table .
.
.2concurrency common fixes.
concurrency bugs are caused by undesired interleavings among different threads.
the flaky tests from the concurrency category in our study are similar to common concurrency bugs in the cut .
we find four main strategies of fixes for concurrency flaky tests.
fixes by adding locks out of concurrencyflaky tests are fixed by adding a lock to ensure mutual exclusion for code that is supposed to be accessed by one thread at a time.
of these fixes address atomicity violation and each addresses deadlock and race condition .
fixes by making code deterministic out of concurrency flaky tests in the study are fixed by making the execution deterministic.
the specific changes include modifying code to eliminate concurrency enforcin g certain deterministic orders between thread executions e tc.
fixes by changing concurrency guard conditions out of concurrency flaky tests in our study are fixed by changing the guard conditions in the test code or the cut.
for example developers use a condition check to onlyallow certain threads toentercertain partof thecodea t the same time.
if that condition does not take into account all the possible scenarios the test may become flaky.
fixes by changing assertions out of concurrency flaky tests in our study are fixed by changing assertions in the test code.
although non determinism is permitted bythe concurrentprogram thetest assertion fai ls to accept all valid behaviors.
the fix is to account for all valid behaviors in the assertion.
others the remaining fixes for concurrency flaky tests vary from case to case and they are usually specific to theapplication.
forinstance developers mayfixa concurrencyflaky test due to race condition by making a specific shared variable to be thread local.
effectiveness of fixes in alleviating flakiness.
in our study we findthat all the fixes for concurrency flakytests completely remove flakiness in the test.
as long as the root cause of the concurrency flaky test is correctly identified and understood by developers the committed fix always resolves the problem completely.
our finding seemingly contradicts a previous study by lu et al.
who found a number of concurrency bugs to behard to fix and typically have at least one incomplete patch attempting to fix the bug but not completely fixing it.
the difference is likely due to the different methodologies we use .
in particular we analyze one committed fix per a flaky test in the repository and if the same flaky test has multiple commits we pick the last commit to fully understand the flakiness and how it was resolved.
it is quite possible that our study missed some incomplete fixes.
also it is possible that some incomplete fixes were proposed with the bug reports but developers rejected these incomplete fixes wit hout committing them to the repository at all.
in contrast lu et al.
study concurrency bugs from bug reports and not from commits.
also one reason that concurrency flaky tests can be easier to fix than general concurrency bugs is that general bugs may have no tests making it harder to debug than when a specific test is present.
see finding f. and implication i. in table .
.
.3test order dependency common fixes.
test order dependency flaky tests are not easy to debug because it may be difficult to find out which other test is or tests are interdependent with the intermittently failing test.
however once developers figu re out the dependency the fix is usually simple and obvious.
we classify the common fixes into three main strategies.
fixes by setting up cleaning up states out of test order dependency flaky tests are fixed by setting up or cleaning up the state shared among the tests.
basically the test needs to set up the state before it executes clean up the state after it finishes or both.
fixes by removing dependency out of test order dependency flaky tests are fixed by making local copies of the shared variable and removing the dependency on it.
fixesby merging tests 2outof19 test order dependency flaky tests are fixed by merging dependent tests.
for example developers copy the code in one test into another one and remove the first test.
effectiveness of fixes in alleviating flakiness.
all the fixes we find in our study for test order dependency flaky tests completely remove the flakiness.
however the first twostrategies of fixes setup cleanupstate andremove dependency are better than the last strategy.
setting up or cleaning up state ensures that even if tests do depend on the shared state that state is always found as expected independent of the order in which the tests run.
the second strategy remedies the case where two or more tests access and modify a common field that is expected to be local this strategy is somewhat related to concurrency even when tests are executed on one thread.
merging dependenttests makes tests larger and thushurts their readability and maintainability although it does re move the flakiness.
moreover merging smaller tests into large tests limits the opportunities for parallelizing the test suite or applyingtest selection andprioritization techni ques.
see finding f. and implication i. in table .
.
.
others we next discuss briefly the common ways of fixing the other categories of flaky tests.
resource leak.
this category is one of the hardest for generalizing the specific fixes.
fowler suggests to manage the relevant resources through resource pools .
when aclient needs a resource the pool provides it.
the pool can be configured to either throw an exception if all resources are in use or to grow.
when the client is done with the resource it should return it to the pool.
a resource leak occurs if a client does not return resources.
these leaky clients can be detected by reducing the pool size so that requesting a resource triggers an exception and fixed by properly returning the resources.
network.
whether for internal communication between processes on the same machine through sockets or getting data and services from remote servers software relies extensively on the network.
this dependency results in nondeterminism in test and code execution.
one of the best fixes for this category is to use mocks.
whenever the use of mocks is non practical the flakiness can be remedied by usingwaitfor.
time.time precision differs from one system to another.
in general tests should avoid using platform dependent val ues like time.
io.because they deal with external resources i o operations can cause intermittent test failures.
therefore the developer should make sure to close any opened resource file database etc.
and to use proper synchronization between different threads sharing the same resource.
randomness.
this category is associated with random number generation.
to avoid fix flakiness due to randomness the developers should control the seed of the random generator such that each individual run can be reproduced yet the seed can be varied across runs.
the developer should also handle the boundary values that the random number can return e.g.
zero can be a problem in some scenarios.
floating point operations.
floating point operations are non deterministic bynature and can cause a lot of problems if not handled correctly.
in general one has to be careful when dealing with floating point operations.
imprecisio n is not avoidable but one should aim for determinism and it is good practice to have test assertions as independent as possible from floating point results.
unordered collections.
flakiness due to unordered collections arises whenever the developer assumes that the api guarantees a certain order that it does not.
in general a good programming practice is to write tests that do not assume any specific ordering on collections unless an explicit convention is enforced on the used data structure.
see finding f. and implication i. in table .
.
changes to the code under test out of of the analyzed commits fix the flakiness by changing both the tests and the cut.
we classify the changes to the cut as follows.
deterministic bug.
in some cases the source code is deterministic and contains a bug.
the flakiness is in the test code but the test by failing even intermittently hel ps in uncovering the real bug in the cut.
non deterministic bug.
thesourcecodeisnon deterministic and contains a bug e.g.
a race condition that cause s flakiness.
the flaky failures again help to uncover the bug.
non deterministic no bug.
the code is non deterministic but contains no bug.
however the developers decide to make it more deterministic to avoid some cases.
such change can help in writing tests because the flaky test need not consider all the possible correct results.
see finding f. and implication i. in table .
.
threats to v alidity our study is empirical and has the common threats of internal external and construct validity as any empirica l study.
we focus on more specific issues.
choice of projects.
we study only a subset of all software projects so our results may not generalize.
to address this threat we consider allthe projects from the apache software foundation.
we examine a diverse set of projects with more than million loc just in the projects with flaky tests and .
million commits.
the projects use different languages and various types of applications web server databases cloud graphics mail build management etc.
.
however apachedoes notcontain manymobile applications such as android apps or gui tests so some of our findings and implications such as platform independence may not apply to those cases.
selection criteria.
in selecting the cases to study we rely only on the commit log use specific keywords to search it and study only the fixed tests.
section explains in detail the benefits of choices and in partic ular finding fixed flaky tests that other approaches such as relying on bug reports could miss.
however we could still miss many flaky tests that are only reported in bug reports and never got fixed.
concerning choice we search the commit messages using only two keywords intermit and flak so there is no guarantee on the recall of our search.
in fact we believe oursearch could miss manyflakytests whose fixescouldusewords like concurrency race stall fail etc.
in our future work we intend to expand our search for likely fixes of flaky tests.
the large number of commits we already find with just two keywords shows that flaky tests are an important problem to study.
manual inspection.
labeling and characterizing commits manually can lead to incorrect labeling e.g.
our analysis phase found some false positives from our filtering phase.
however the number of false positives is relatively low.
fu rther to minimize the probability of error two authors inde pendently inspect every commit and then merge the results.
.
related work severalresearchersandpractitionershavepointedoutpro blems with non deterministic tests .
forexample fowler describednon determinis tic test outcomes as a recurring problem in regression testing and outlined some ways for avoiding and manually fixing flaky tests.
memon and cohen pointed out a few possible reasons that make gui tests flaky.
lacoste also described some of the unfortunate side effects of flaky tests in automated regression testing e.g.
some features may miss a release deadline because of intermittent test failur es.
more recently marinescu et al.
revealed a number of nondeterministic test suites in their study that analyzed evol ution of test suite coverage .
non deterministic bugs and tests.
most existing work on non deterministic bugs either in the cut or in the test code focuses on onespecific category of non determinism causes.
for example several researchers focus on test order dependency .
zhang et al.
formalized the test dependency problem studied real world test suites with test dependency problems and implemented several techniques to identify these tests by reordering test runs .
mu slu et al.
found that isolating unit tests can be helpful in detecti ngfaults butenforcingisolation canbecomputationallyexp ensive .
bell and kaiser proposed an automatic approach for isolating unit tests in java applications by tracking al l side effects on shared memory objects and un these effects between tests .
concurrency was also well studied.
for example farchi et al.
summarized common bug patterns in concurrent code and employed static analysis to detect some of them .
lu et al.
published a comprehensive characteristic study examining bug patterns manifestation and fixes of concurrency bugs.
compared to prior work our study focuses on characterizing flaky tests across allcategories and we start from commit logs rather than just bug reports.
our results showthatsomecategories offlakytests suchas async wait andconcurrency are more prevalent than test order dependency and should likely get more attention.
while we focus on flaky tests some causes that we find are general non determinism bugs in the cut.
however we ignore the cases of bugs in the cut that do not result in test flakiness but we do include the cases of test flakiness that may have no bug in the cut e.g.
test order dependency .
we believe that the causes and fixes for flaky tests differ enough from general bugs in the cut to warrant more special focus.
bug fixes study.
researchers have also studied different characteristics of bug fixes.
for example murphy hill et al .
conducted a large study to find factors that influence how bugs get fixed .
bachman et al.
found that many bug fix commits do not have corresponding bug reports their results motivate us to start from commits instead of bug reports.
automated techniques have also been proposed to fix concurrency bugs by jin et al.
.
our study revealed a number of different strategies for fixing flaky tests in different categories and we believe that our findings can help in developing more automated techniques for fixing bugs.
fixing tests.
daniel et al.
proposed an automated technique for fixing broken tests .
yang et al.
proposed a differenttechniqueusingalloyspecificationstorepairtests .
however existing test repair only focuses on broken tests that fail deterministically while we study flaky tests that fail non deterministically.
.
conclusions regression testing is important but can be greatly undermined by flaky tests.
we have studied a number of fixes to flaky tests to understand the common root causes identify approaches that could manifest flaky behavior and describe common strategies thatdevelopers usetofixflakytests.
our analysis provides some hope for combating flaky tests while there is no silver bullet solution that can address all categories of flaky tests there are broad enough categories for which it should be feasible to develop automated solutions to manifest debug and fix flaky tests.
.