oasis prioritizing static analysis warnings for android apps based on app user reviews lili wei yepang liu shing chi cheung department of computer science and engineering the hong kong university of science and technology hong kong china lweiae andrewust scc cse.ust.hk abstract lint is a widely used static analyzer for detecting bugs issues in android apps.
however it can generate many false warnings.
one existing solution to this problem is to leverage project history data e.g.
bug fixing statistics for warning prioritization.
unfortunately such techniques are biased toward a project s archived warnings and can easily miss new issues.
another weakness is that developers cannot readily relate the warnings to the impacts perceivable by users.
to overcome these weaknesses in this paper we propose a semantics aware approach oasis to prioritizing lint warnings by leveraging app user reviews.
oasis combines program analysis and nlp techniques to recover the intrinsic links between the lint warnings for a given app and the user complaints on the app problems caused by the issues of concern.
oasis leverages the strength of such links to prioritize warnings.
we evaluated oasis on six popular and large scale open source android apps.
the results show that oasis can effectively prioritize lint warnings and help identify new issues that are previously unknown to app developers.
ccs concepts software and its engineering software maintenance tools software testing and debugging human centered computing smartphones information systems retrieval models and ranking keywords static analysis warning prioritization android lint app user reviews natural language processing concept graph acm reference format lili wei yepang liu shing chi cheung.
.
oasis prioritizing static analysis warnings for android apps based on app user reviews.
in proceedings of esec fse paderborn germany september pages.
introduction android app development has confronted great challenges posed by the features of mobile platforms such as resource limitations and fast evolving ecosystems .
static code analyzers such as android lint enable developers to identify potential bugs issues in their apps before releasing them into the market.
esec fse september paderborn germany association for computing machinery.
this is the author s version of the work.
it is posted here for your personal use.
not for redistribution.
the definitive version of record was published in proceedings of esec fse september static analyzers can help quickly detect various types of issues without the need to execute the app under analysis they usually suffer from a high rate of false positives which reduces their usability .
existing studies have explored the possibilities of prioritizing static analysis warnings using historical issue fixing data.
kim et al.
prioritized warning categories based on the warnings eliminated by code changes.
ruthruff et al.
leveraged logistic regression models to prioritize warnings using historical bug triage and fixing statistics.
one major limitation is that they strongly rely on historical activities e.g.
warning fixing choices and developers prior practices.
as such these techniques are intrinsically biased to warnings that are similar to the fixed.
they can miss many new issues that arise from upgraded system libraries or have not been encountered by developers before.
to address these problems in this paper we propose a new approach to effectively prioritizing the warnings generated by static analysis tools.
we base our discussion on the android apps and the most popular android static analysis tool android lint lint for short .
lint supports the detection of various types of issues that are of multiple severity levels in android apps .
figure illustrates a lint warning that describes a functional issue caused by a deprecated android system event.
typically for an app with a moderate szie of code base lint would report hundreds to thousands of warnings see section for examples .
one way to prioritize such a large volume of warnings is to rank them by severity level.
unfortunately even after such ranking the number of warnings with the highest level of severity can still be unmanageable see section for an example .
an alternative is to selectively disable some checkers in lint and thereby reduce the number of warnings.
however the choice of disabled checkers is generally undecidable.
a bad choice would rule out many valid warnings while inducing invalid ones.
to overcome the limitations of the techniques discussed above we propose a new criterion for prioritizing lint warnings for android apps.
our insight is that a warning should be ranked at a higher place if its described issue can cause user perceivable problems i.e.
affecting visible app execution behavior .
to effectively learn user perceivable problems for android apps we leverage the large corpus of user reviews available on the app markets like google play store.
existing studies have confirmed that app user reviews can provide important information to facilitate app development and maintenance.
these studies propose to extract useful information e.g.
requests for new features by filtering or categorizing user reviews.
however to the best of our knowledge none of them have ever leveraged user reviews to improve the results of static code analyses.esec fse paderborn germany lili wei yepang liu shing chi cheung to effectively prioritize lint warnings our approach oasis prioritize w arnings based on u ser rev iews combines program analysis and natural language processing nlp techniques to recover the intrinsic links between lint warnings and app user reviews.
specifically oasis performs program analysis to augment lint warnings with contextual information on the functionalities that can be affected by the issues of concern and leverages nlp techniques to retrieve user perceived problems from the app reviews.
in this way oasis is able to estimate the severity of lint warnings by analyzing frequency of user complaints on problems caused by the issues of concern for effective prioritization.
one prominent challenge is that app user reviews are by nature unstructured and noisy.
users may describe a problem caused by an issue in their own way.
without understanding the semantics meaning of user reviews the links between lint warnings and user reviews cannot be accurately established.
to address this challenge oasis leverages microsoft concept graph for text understanding and semantic similarity measurement.
as we show later this can significantly improve the effectiveness of warning prioritization.
we evaluated oasis by conducting experiments on six popular and large scale open source android apps which have a sufficient number of recent user reviews.
in the experiments we observed thatoasis is able to effectively prioritize useful warnings on real issues that can cause user perceivable problems.
on average it achieves one order of magnitude improvement in precision over lint s default warning prioritization strategy i.e.
by issue severity level .
such an improvement significantly increases lint s usefulness.
as we show later some developers quickly confirmed and fixed the warnings that are top ranked by oasis .
in addition our semantics aware similarity measurement method also significantly outperforms the traditional token based textual similarity measurement methods in linking user reviews with lint warnings to facilitate warning prioritization.
in summary we make the following major contributions in this paper.
we propose a novel approach oasis to prioritizing static analysis warnings for android apps by leveraging app user reviews.
to the best of our knowledge we are the first to study the intrinsic links between static analysis warnings and app user reviews leveraging such links for effective warning prioritization.
the links generated by oasis between a warning and the associated user reviews facilitate developers to understand the warning s impacts.
we find no such support in existing android tools.
we propose a semantics aware similarity measurement method concept similarity to calculate the similarity between structured warning documents and natural language user reviews.
we evaluate oasis on six large scale and representative opensource android apps.
our evaluation results confirm that oasis can effectively identify useful warnings on issues that cause user perceivable problems from a large number of lint warnings and our concept similarity can outperform traditional token based similarity measurement methods.
paper orgnization.
section presents the basics of android lint.
section motivates our approach using a real example.
section describes our approach oasis in detail.
section evaluates our approach.
sections and discuss threats to validity and related work.
finally section concludes the paper.file file androidmanifest.xml line severity warning description use of android.hardware.action.new picture is deprecated for all apps starting with the n release independent of the target sdk.
apps should not rely on these broadcasts and instead use jobscheduler figure a lint warning for owncloud receiver android name .broadcastreceivers.instantuploadbroadcastreceiver intent filter action android name android.hardware.action.new picture data android mimetype image intent filter intent filter action android name android.hardware.action.new video data android mimetype video intent filter receiver .
.
.
.
.
.
.
.
.
.
figure lines of owncloud s androidmanifest.xml title automatic uploads always fail author keith slagerman rating date comment instant upload not working after updating to v7.
.
saw in the description today that this issue is supposed to be fixed.
still not working though.
title instant upload doesn t work in nougat.
author william goodwin rating date comment only reason i bought this app.
title might be good author joseph noyes rating date comment so far the most important part of the app... for my use case.... is instant download and it doesn t work on android n. that being said n is in beta so of course i expect to run into these issues... but i also expect that this will be fixed on the first stable n release and i will upgrade my rating at that time.
figure example user reviews of owncloud preliminaries of android lint android lint is a static code analysis tool for android apps .
it scans the source files e.g.
java code files resource and configuration files of android apps to detect potential bugs issues for improving the apps correctness security performance usability accessibility and internationalization.
since lint is directly available runs by clicking menu items in the android studio the official ide for android app development it has become a widely used quality assurance tool for android app developers.
issues and checkers.
lint supports the detection of hundreds of types of issues .
each type of issues is detected by a checker and annotated with a pre defined severity level error warning weak warning info ortypo .
lint users can customize the severity levels.
they can also suppress the detection of certain types of issues when they consider the issues to be spurious.
warning instances.
upon detecting a potential issue lint reports to developers the issue s location severity level and problem description.
we call such a report a warning instance orwarning for short .
note that a warning in our work can describe an issue of any severity level error warning weak warning oasis prioritizing static analysis warnings for android apps based on app user reviews esec fse paderborn germany info ortypo .
figure shows an example warning generated by lint when analyzing owncloud a popular open source file syncing and sharing app for android devices.
the described issue has a severity level of warning .
it is detected at line in theandroidmanifest.xml file issue location of owncloud.
this warning indicates that the app registers broadcast receivers to monitor and handle a deprecated type of system broadcast message namely android.hardware.action.new picture which will not be sent starting from the android n release i.e.
android .
.
it also suggests that the app should use the jobscheduler an api for scheduling jobs to be run in an app s own process instead.
motivation the high false warning rate has long been considered a major limitation of static analyzers like lint .
unlike existing techniques that prioritize warnings based on project histories our work prioritizes warnings based on a criterion of whether they prescribe an issue causing user perceivable problems that affect app security functionality or performance.
our intuition is two fold.
first most of such problems observed and reported by users are real.
therefore warnings are likely true if they correspond to these problems.
second existing studies have reported that such security functionality and performance issues are among the top types of code issues that developers would like program analyzers like lint to detect .
as such our ranking policy prioritizes warnings that are likely true and of concern to developers.
however identifying user perceivable problems caused by a particular issue is generally difficult and expensive.
for example it may require extensive user studies to understand the problems.
fortunately for android apps there are a lot of publicly available user reviews in app stores.
the interactive feedback systems of app markets enable users to rate an app and make comments easily.
such reviews can provide useful knowledge about the user perceivable problems and hence can be leveraged to prioritize lint warnings.
key ideas.
we observe that app user reviews and lint warnings are intrinsically related.
both of them can be viewed as comments on the problems of android apps.
user reviews contain prolific information on user feedback such as the complaints about problems caused by certain issues while lint warnings provide hints at the app source level on issues that potentially affect the quality of android apps.
our idea is to recover the links between a warning that reports an issue detected by lint and its related user reviews.
via such recovered links we can estimate the significance of the lint warning and thus the severity of the issue by looking at the frequency of user complaints about the corresponding issue inducing problems.
to ease understanding we use a real example to illustrate how user reviews can be linked to a lint warning and demonstrate the major challenges that we need to address.
recovering links between lint warnings and user reviews.
let us consider the warning shown in figure .
as mentioned earlier it reports that owncloud registers a broadcast receiver for a deprecated type of broadcast message which is no longer sent since the advent of android .
i.e.
android n or nougat .
this breaks the functionality of owncloud on those devices running android .
or above.
figure lists several user reviews that complain about the broken instant upload on android .
devices.
in this example we can observe the links between the user reviews and the warning description.
for instance review mentioned android n which is also included in the warning description as n release .
this indicates the possibility of applying natural language processing techniques e.g.
textual similarity based ones to recover the intrinsic links between lint warnings and user reviews.
augmenting warning descriptions.
however the first challenge is that the links built by simply considering warning descriptions and user reviews can be weak and imprecise.
this is because the warning descriptions are pre defined templates.
they provide only brief information and do not capture any program context specific to the apps under analysis.
therefore warnings should be augmented with program contextual information e.g.
surrounding lines and call hierarchies to enhance the linking accuracy.
for example figure shows the lines surrounding the location of the owncloud issue.
note that owncloud registers the instantuploadbroadcastreceiver to handle the deprecated type of system broadcast message.
in the comments of review and review the functionality of instant upload was reported to be broken.
we also note that the keywords instant upload are included in the class name of the registered broadcast receiver which can be found at the lines near the issue location.
hence by augmenting the warning descriptions with such contextual information we can recover the links between a lint warning and its related user reviews.
handling unstructured and noisy user reviews.
the user reviews in figure suggest that app users tend to comment in their own style by using their preferred wording expressing subjective feeling or providing imprecise information.
such unstructured and noisy natural language data pose the second challenge to the accurate recovery of the links between lint warnings and user reviews.
take review as an example.
its title mentions the instant upload problem but the user comment provides no useful information relevant to the problem.
without properly handling such noise we may mistakenly build the links between the review and those warnings whose description contains keywords such as buy or purchase .
this motivates us to leverage existing user review filtering techniques to eliminate noise and non informative reviews from the user review corpus.
another example is that instant download mentioned in the comments of review is not a functionality provided by owncloud.
in fact the user intended to complain about instant upload rather than instant download .
such cases can be common as non technical end users may confuse upload with download .
in this example upload and download are two semantically related concepts with different tokens.
therefore applying simple textual similarity calculation based on tokens will miss such semantic links between lint warnings and user reviews.
this motivates us to consider word semantics in the link discovery process see section .
.
finally the warning in this example was buried under more than other warnings reported by lint for owncloud.
if app developers simply rank the warnings based on the issue severity levels assigned by lint this warning will be ranked after other errors and likely ignored by the developers.
on the other hand since there are many user complaints about this problem the corresponding issue is certainly a prominent one that needs to be attended in due course.
our approach is designed to help developersesec fse paderborn germany lili wei yepang liu shing chi cheung warningtransformation augmentationuserreviewfiltering prioritization semantics aware warningprioritizerrankedwarningslint warningsuserreviewsspellingcorrectionstanfordtokenizercamelcasetokenizerstanfordtokenizerwarning document user review preprocessingstopwordremoverstanfordwordstemmertf idf based tokenweighter figure overview of oasis components with our major contributions are highlighted with bold text successfully identify such warnings among a huge number of other warnings.
in the next section we present our approach in detail.
4oasis approach the goal of oasis is to prioritize the warnings reported by lint for android apps using the information retrieved from user reviews.
figure gives an overview of oasis .
it takes the lint warnings for a given app and its user reviews as input and then outputs a ranked list of the warnings.
oasis first transforms the warnings and augments them with contextual information to generate warning documents .
in parallel oasis leverages a state of the art tool surf to filter out non informative user reviews.
thereafter oasis prioritizes the warnings by leveraging a widely used concept graph and a semantics aware similarity calculation technique.
.
document preparation and preprocessing in the first step oasis processes its two inputs extracts contextual information for warnings and eliminates noise.
specifically it performs the three following tasks warning transformation and augmentation.
user review filtering and prioritization.
warning document and user review preprocessing.
.
.
warning transformation and augmentation.
as discussed in section a key challenge for linking user reviews with lint warnings is that the information contained in the warning descriptions is inadequate for accurately retrieving related user reviews.
oasis addresses this challenge by augmenting warnings with contextual and user perceivable code information.
specifically for each lint warning oasis first generates a textual document named warning document .
each warning document is initialized with the following data which can be extracted from the analysis results of lint warning category warning summary description of the potential problems and information on the environment e.g.
android .
in figure under which the problems can occur such environmental information is not always provided by lint .
these initialized data aim to describe the concerned issue of the warning in a succinct and pre defined way.
next oasis searches for three kinds of contextual and code related information aboutthe functionalities that can be affected by the issue in the source files of concern to further enrich each warning document.
statements surrounding the issue location oasis first extracts the statements neighboring the issue location from the source file of concern.
these statements provide important contextual information for each warning.
as shown in figure the neighboring statements of the issue location contain the name of the registered broadcast receiver that encodes the broken functionality instant upload which was complained about by users.
the number of statements preceding and following the issue location is a configurable parameter in oasis .
following an existing study s practice to capture code contextual features the number is set to in our current implementation and evaluation.
identifiers in call hierarchy only for java code warnings if a warning is reported at a source code line within a java method m oasis enriches the corresponding warning document with m s name.
it then traverses the call hierarchy to further include the names of all methods that transitively invoke m. the enclosing method mis the code artifact that is directly affected by the issue described in the warning.
by traversing the call hierarchy all the methods that have transitive calling relationships with mwill be included.
in this way the warning document is enriched with the identifiers of functionalities that can be affected.
android components we include identifiers of the android app components such as activities or broadcast receivers that can transitively access the issue location e.g.
invoking the concerned code as another piece of contextual information.
these top level app components provide interfaces and functionalities that are directly exposed to the users and hence are more likely to be described in user reviews.
we extract these app components using the following two criteria.
for warnings on java source code lines the app components can be extracted from the methods in the call hierarchy because these methods are the component classes members.
for warnings on xml elements that register app components in configuration files we match the names of the registered app components against the java class names.
since warnings on app component registrations can affect any of the functionalities provided by the app components we include the identifiers of the app component classes as well as their member methods and fields.
oasis generates a warning document for each lint warning and augments it with the extracted data as described above to help retrieve related user reviews.
.
.
user review filtering and prioritization.
while user reviews provide prolific information of the apps that can facilitate app development and maintenance they often contain a lot of noise that needs to be eliminated before use .
to eliminate the noise in user reviews and retrieve the relevant information of user perceivable app problems oasis filters raw user reviews using a state of the art review analysis tool surf which is designed to exclude non informative reviews and summarize useful user reviews by categorizing them into different topics and intentions.
specifically surf categorizes user reviews into five intention categories information giving information seeking 1it is a good practice to name a method according to its functionality.
please also note thatoasis works on app source files and therefore it does not need to handle code obfuscations which often transform identifiers to meaningless ones.oasis prioritizing static analysis warnings for android apps based on app user reviews esec fse paderborn germany feature request problem discovery and other .
since we aim to learn user perceived app problems we only keep those user reviews that are categorized as problem discovery for further processing.
after filtering we prioritize the remaining ones by assigning weights to them based on their user ratings and submission dates.
intuitively user reviews with lower ratings tend to comment on issues bugs of an android app and thus should be associated with a higher weight.
given a review r oasis uses the following formula to calculate its rating weight i.e.
the weight related to user ratings fratin r ratin r ratin min ratin max ratin max and ratin min denote the maximum and minimum rating supported by the corresponding user feedback system.
the function ratin r returns the user rating of the review r. the rating weight fratin r is normalized as a negative linear function of the review ratings.
for example on google play store user ratings are integers from to .
the rating weights for user reviews with a rating .
.
.
are mapped to .
.
.
.
.
by oasis .
when assigning a weight to a review we also consider its submission date in addition to its user rating.
our idea is that newer reviews should have a higher weight as they tend to reveal recent problems that still exist in the app while old reviews may comment on legacy problems that may have already been fixed by developers.
hence we define the date weight i.e.
the weight related to date of a review ras fdate r index r the function index r returns the index of the review rin the list of all reviews ranked in chronological order and the date weight fdate r takes this value.
then the overall weight of a user review r denoted f r can be calculated as the production of its rating weight and date weight f r fratin r fdate r the overall weight of each review is leveraged later in our warning prioritization procedures.
.
.
warning document and user review preprocessing.
after augmenting warning documents and extracting informative user reviews oasis further preprocesses both of them to eliminate noise and improve the accuracy of later nlp procedures.
following existing studies on information retrieval and text mining oasis applies three standard preprocessing steps tokenization stop word removal and word stemming.
for such preprocessing we use the stanford nlp library which is widely adopted by researchers and practitioners.
as shown in figure the preprocessing steps for warning documents and user reviews are slightly different as they have their own unique features.
warning documents are structured and contain both texts e.g.
problem descriptions and code snippets e.g.
statements surrounding the issue locations .
since code snippets and texts are in different formats we apply different tokenization methods.
in java the camel case is a widely used naming convention for identifiers in code .
with this observation oasis splits code snippets into tokens by underscores and capital letters.
such a tokenization method for code snippets is also commonly used in code repository mining studies .
after tokenizing codesnippets the remaining texts in the warning documents will be tokenized by a standard tokenizer.
on the other hand user reviews are mostly natural language texts and hence a standard tokenizer is sufficient for their tokenization.
however unlike structured warning documents user reviews are often written in an informal way with many typos and much noise .
therefore before tokenization oasis applies an extra spelling correction step to fix typos and errors in user reviews.
for this purpose oasis integrates the english vocabulary module of the jlanguagetool .
after tokenization standard english stop word removal and word stemming are applied on the tokenized warning documents and user reviews.
finally oasis applies the term frequency inverse document frequency tf idf method to filter out generic words e.g.
phone app that appear frequently across documents in the corpus for later steps.
.
semantics aware warning prioritization in this step oasis takes the processed warning documents and user reviews to prioritize the input lint warnings.
the intuition of our warning prioritization method is a warning should be ranked at a higher place if its described issue can cause problems complained in a lot of app user reviews .
based on this intuition we design the ranking score of a warningwas the summation of the similarities between the warning document of wand the user reviews s w r x ri rf ri similarit y w ri in the formula rirepresents the i th user review in a given app s user review set r.f ri is the weight for review rias defined in section .
.
.
the function similarit y w ri calculates the similarity between the warning document of wand the user review ri.
as discussed in section one critical challenge in linking lint warnings to user reviews is that the user reviews are written in users preferred wording and may provide imprecise information.
as such traditional textual similarity calculation techniques that model documents as tokens or entities may fail to precisely recover the intrinsic links between lint warnings and their related user reviews.
to address this problem oasis leverages microsoft concept graph for text understanding and integrates semantic meanings of words into document representation to calculate concept similarity for linking lint warnings and user reviews.
to show the effectiveness of our concept similarity in uncovering the links between structured warning documents and free style user reviews we compare the discovery results of the concept similarity with two baselines jaccard similarity and cosine similarity which are based on the traditional token based document representation models.
these two similarity measurement methods are widely used in existing studies that leverage information retrieval and textual mining techniques in the following we first briefly introduce the baseline methods they are also implemented in our tool and then discuss the details of our proposed concept similarity measurement method.
.
.
baselines.
jaccard similarity calculation models documents as sets of tokens or words and measures the similarity between two documents by looking at the number of common andesec fse paderborn germany lili wei yepang liu shing chi cheung universal tokens contained in the two documents.
formally jaccard similarity between a warning wand a user review ris defined as jaccard w r tw tr tw tr twandtrare the sets of tokens in the warning document of wand the user review rrespectively.
as we can see jaccard similarity simply leverages the set intersection and union operations to compare the documents and associates all tokens with equal weights.
another popular document representation model is the bag ofwords model weighted by tf idf values.
the bag of words model represents documents as vectors of words or tokens and the value for each word is its corresponding tf idf weight.
cosine similarity is commonly used to calculate the similarity between documents represented by the bag of words model.
formally cosine similarity of a warning wand a review rcan be defined as cosine w r cosine vw vr vw vr vw vr vwandvrare the bag of words vectors of the warning document ofwand the user review r. the bag of words model and cosine similarity are widely used in text mining studies .
.
.
microsoft concept graph and concept similarity.
to overcome the limitation that traditional token based models only compare lexical tokens in the document in our work we represent the documents with a new model based on the large corpus of concept relationships provided by the microsoft concept graph .
microsoft concept graph is a large publicly available knowledge base that captures the semantics of words by mapping words to their concept categories.
by querying the graph a word can be mapped to its semantic concept categories with probabilities.
thus a word can be represented as a concept vector that encapsulates semantic information contained in the word.
with the concept vectors a document can then be mapped into the space by cd h in the formula is the vector of the tf idf values of the tokens words in the document and his the concept matrix.
a concept matrix is constructed by concatenating the concept vectors of all tokens in the document i.e.
the concept vector of each token will be one row in the matrix .
via matrix multiplication a document is mapped to a vector of concept categories denoted as cd.
intuitively a document is mapped to the concept space by assigning a probability to each concept category to which the document belongs.
this probability is estimated by summing up the corresponding probabilities of all the tokens contained in the document.
the concept similarity of a warning wand a review rcan then be computed as the cosine similarity between the corresponding concept vectors of the warning document of wand the review r denoted cw cr which can be constructed by using the above formula.
formally the concept similarity of wandris defined as concept w r cosine cw cr with the above method we can successfully integrate semantic information of words into the similarity calculation between lint warnings and user reviews.
we note that there are other alternatives to integrate word semantics into the similarity calculation procedures to accomplishsoftware engineering tasks.
for example ye et al.
trained their own word embeddings from api documents tutorials and reference documents to represent code snippets and natural language texts as vectors in a shared space for improving information retrieval in software engineering tasks.
we currently choose to leverage microsoft concept graph and propose our concept similarity due to two major reasons.
first user reviews are unstructured and written in casual language which is difficult to learn from structured api documents or tutorials.
microsoft concept graph is learnt from billions of webpages and search logs.
its coverage of different words texts and concepts is extremely broad due to the big training data and is more general than other knowledge bases .
such a large and general knowledge base can well capture the semantics of diversified expressions in user reviews.
second microsoft concept graph is publicly available and its knowledge base is continually updated.
studies based on microsoft concept graph to understand short texts are proved to be very effective .
while microsoft concept graph is able to model semantics of general words it is still possible that ye et al.
s word embedding technique based on a specifically trained model can help better model program behaviors.
in future we plan to combine both techniques to study whether it can help achieve better results for lint warning prioritization.
evaluation .
research questions and baselines in this section we present our experiments to evaluate oasis s capability of identifying positive warnings among a huge number of warnings generated by lint when analyzing android apps.
positive warnings refer to those warnings of which the described issues can cause user perceivable problems the concrete judging criteria will be explained shortly in section .
.
in contrast warnings of which the described issues cannot cause user perceivable problems are referred to as negative warnings e.g.
false alarms or warnings on code style issues .
specifically our evaluation aims to answer the following two research questions rq1 usefulness of user reviews can the use of app user reviews facilitate the identification of positive warnings and improve the usefulness of lint s static analysis?
rq2 effectiveness of concept similarity can our proposed concept similarity contribute to oasis s performance of prioritizing positive warnings?
to answer rq1 we compared oasis with these baselines baseline i ranking lint warnings by each project s adopted issue severity settings.
this baseline emulates a typical practice of app developers when they inspect lint warnings.
we compare it with oasis to evaluate whether oasis which leverages app user review data can outperform this common practice.
baseline ii ranking lint warnings by the jaccard and cosine similarity section .
.
.
we compare oasis with these two baseline methods that leverage token based similarity measurements to study whether integrating semantic information can help improve the effectiveness of warning prioritization.
the other program analysis and nlp steps of the two baseline follow those of oasis .oasis prioritizing static analysis warnings for android apps based on app user reviews esec fse paderborn germany table experimental subjects app name category kloc downloads rating version no.
warnings reviews issue id ankidroid education .
.
.
.
c geo entertainment .
.
.
.
k mail communication .
.
.
owncloud productivity .
.
.
.
transdroid tools .
.
.
.
wordpress social .
.
.
.
experimental setup .
.
subjects and ground truth.
for our experiments we selected the latest release versions of six popular and large scale open source android apps as the subjects.
table gives the basic information of each app subject including the app name category size in kloc the number of downloads user rating the version number the total number of lint warnings that are not suppressed by developers and the number of user reviews we crawled from google play store.
as shown in the table these apps are popular received millions of downloads large scale containing thousands of lines of code and diverse covering six different categories .
to obtain results for baseline i we ran lint on each app subject and ranked all the warnings that are not suppressed by developers for each app by the severity level of the corresponding issues.
for issues of the same severity we shuffled the order of their warnings five times and used the average evaluation metric values of these five ranked warning lists as the final evaluation results for baseline i. to obtain results for baseline ii and oasis we ran our implemented tool with three similarity measurement methods on the lint warnings and user reviews for each app respectively.
our experiments were conducted on an imac with .
ghz intel core i5 cpu and gb ram.
to establish the ground truth we manually inspected the ranked warning lists produced by baseline i baseline ii and oasis to label each warning as positive or negative.
the labeling was performed by two co authors of this paper.
before the process the two authors discussed and reached a consensus on the criteria to distinguish positive and negative warnings.
specifically the two authors independently checked each warning s program context to validate whether the warning is a true positive i.e.
the described issue really exists at the reported location .
in the case of a true positive they further decided whether the described issue would cause userperceivable problems including those that affect app functionality performance compatibility and security by checking the official android api guides programming qa sites like stack overflow and other online resources obtained by google search.
after independent labeling the authors cross validated their results and discussed the warnings that received different labels from them.
for such warnings the two authors further manually constructed test cases to confirm whether the issues of concern would indeed cause perceivable problems.
the labeling process is labor intensive and requires acquisition of much domain knowledge.
in this work we only labeled the top warnings in each ranked list reported by baseline i baseline ii and oasis .
specifically we manually labeled warnings for each of the six app subjects.
in our experiments baseline i produced five ranked warning lists due to the shuffling baseline ii produced two ranked warning lists and oasis producesone ranked warning list.
in total we manually checked warnings out of which were unique.
among them we identified unique positive warnings in total.
then we treated the set of all these positive warnings the complete set of positive warnings for later evaluation.
to further validate our ground truth we randomly sampled representative positive warnings and reported them to the app developers for feedback.
these issue reports are available online and we provide the issue ids in the last column of table .
by the time of this paper s acceptance we have received positive feedback from the developers of four subjects.
.
.
evaluation metric.
since the precision of the results is a critical quality metric of static analysis tools in our evaluation we applied the precision n metric to evaluate the performance of oasis .precision n reports the percentage of positive warnings among the top n n ... warnings in each ranked list.
a higher value of precision n indicates a better ranking result meaning that more positive warnings are ranked at the top of the list such that when developers inspect the results for issue fixing they can easily identify positive warnings in the top ranked ones.
in the following subsections we discuss our experimental results and answer the two research questions.
.
rq1 usefulness of user reviews to answer rq1 we compared the results of oasis with those of baseline i. table presents the precision n results of oasis and baseline i for the six app subjects.
from the results we observe that oasis achieved precision for two of the apps and precision for transdroid.
overall oasis achieved over precision up to .
and over .
up to .
precision for four out of the six subjects.
in most of the subjects the precision tends to drop as n grows the trend is shown in figure .
this indicates that oasis can effectively identify positive warnings and rank them most at top positions.
such results favor developers to check the prioritized warnings one by one from the top.
in comparison baseline i performed much worse than oasis .
among the top ranked warnings baseline i only achieved an average precision of .
.
this indicates that by leveraging information retrieved from app user reviews oasis can significantly outperform baseline i which emulates the likely scenario of using static analysis tools like lint.
note that our precision n metric calculates the percentage of positive warnings in the top n ranked warnings rather than the percentage of true warnings or true positives .
according to the definition of positive warning in section .
that true warnings of the issues that cannot cause user perceivable app problems e.g.
code style issues or javadoc issues were considered negativeesec fse paderborn germany lili wei yepang liu shing chi cheung table precision n of oasis and baseline i nankidroid k 9mail owncloud transdroid wordpress c geo oasis baseline i oasis baseline i oasis baseline i oasis baseline i oasis baseline i oasis baseline i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
01020304050precision topnwarningsconceptcosinejaccard a ankidroid 01020304050precision top n warningsconceptcosinejaccard b wordpress 01020304050precision top n warningsconceptcosinejaccard c owncloud 01020304050precision top n warnings conceptcosinejaccard d transdroid 01020304050precision top n warningsconceptcosinejaccard e k mail 01020304050precision topnwarningsconceptcosinejaccard f c geo figure precision n of oasis with different similarity measurement methods in our experiments.
therefore the precision n metric values of oasis and also baseline i are relatively low.
we also observe that in one subject c geo oasis failed to identify a considerable number of positive warnings among the top ranked ones while baseline i performed worse.
we reported the few positive warnings detected by oasis to the developers of c geo.
through the communication we understand why the precision of oasis for c geo is low.
this is because the developers of c geo have integrated lint into their app build process for a long time.
everytime when c geo was built lint was run to check for potential issues.
the developers would then quickly fix any issues they considered useful and suppress the lint checkers that often reported false alarms.
in fact most serious issues reported by lint have been fixed by the developers and the remaining unfixed issues are mostly false alarms or those that do not affect app execution behaviors.
it is also worth mentioning that after receiving our feedback the developers of c geo immediately fixed our reported issues and found that they missed these issues because of their inappropriate lint configuration.
apart from c geo we also received positive feedback from developers of other three subjects.
warnings reported for ankidroidwere accepted and assigned to corresponding developers.
owncloud developers have fixed the critical issue discussed in our motivating example and k mail developers have fixed two of the three categories of issues we reported.
such positive feedback further confirms the importance of the issues identified by oasis.
answer to rq1 by leveraging the information retrieved from app user reviews oasis significantly outperforms baseline i in identifying positive warnings.
oasis can improve the usefulness of lint by effectively prioritizing its reported warnings.
.
rq2 effectiveness of concept similarity figure plots the precision n results generated by oasis and baseline ii.
the results show that our proposed concept similarity method significantly outperformed the token based methods in three of the apps namely ankidroid owncloud and wordpress.
among these apps the precision n of oasis consistently outperformed that of the other two baseline methods from n to n .
for transdroid oasis outperformed the baseline methods from n to n and achieves similar precision when n is larger than .
in addition oasis successfully ranked a positive warning at the top one position of the warning list for owncloud and transdroid.oasis prioritizing static analysis warnings for android apps based on app user reviews esec fse paderborn germany the only app on which oasis failed to outperform baseline ii is k mail .2to understand the reason we inspected the linked user reviews and lint warnings of k mail produced by the three different methods.
we found that of and of positive warnings identified by cosine and jaccard similarities are correlated with the following two categories of similar issues the first category of issues is related to the usage of the deprecated apache http client apis.
the android api guides suggest that these apis have not been actively maintained by the android team since api level and have been officially removed from the android sdk since the release of android .
.
however apache http apis are widely used in k mail for network related tasks e.g.
markservermessagesread .
users of k mail constantly complained about the network or syncing problems caused by the use of these apis and thus the keywords like connect and server were frequently mentioned in k mail s user reviews.
in this situation these commonly used tokens in user reviews can perfectly match the keywords in the lint warnings and therefore the token based methods can also effectively identify such positive warnings.
yet such good performance is rare as suggested by the low precision of the baseline ii methods on the other app subjects.
the other category of issues is related to the usage of the deprecated android system attributes window.progress start and window.progress end .
these two attributes in k mail are correlated with the variable pendingwork and the method updatetitle .
in the warning augmentation step the method and variable names are included in the warning document as contextual information.
after tokenization the tokens work and update are included in the warning document.
on the other hand many users of k mail happened to complain that the app does not work since the last update which shares the tokens work and update with the warning document.
thus the warnings on the issues in this category are accidentally ranked highly by the baseline ii methods although the linkage between the warnings and user reviews recovered by them is clearly wrong.
in contrast the positive warnings ranked at the top by oasisfor k mail are more diverse.
for example oasis recovered positive warnings on two unique categories of issues that cosine and jaccard failed to recover.
it also highly ranked some warnings on the above mentioned http api and android system attributes related issues and precisely linked them to the relevant user reviews.
we also investigated why some negative warnings are ranked top by oasis .
take the top one warning in the ranked list reported by oasis for k mail as an example.
it reports that method parsecommandcontinuationrequest always returns true.
while this warning describes a true phenomenon it is negative because the method is supposed to always return true.
the reason oasis ranks it at the top is because this method is transitively invoked by many other methods that implement functionalities such as user authorization and folder synchronization and lots of user complaints on topics like couldn t even get past the sign up page or email doesn t download any more are linked with this warning.
while the warning document and user reviews are indeed semantically related and such semantic links can only be 2here we do not compare the results of the three methods on c geo as the total number of identified positive warnings is too low we explained the reason when discussing the results for rq1 to produce meaningful comparisons.recovered by our concept similarity method e.g.
download and sync will not be correlated by token based methods the issue described in the warning has no perceivable effects on these related functionalities.
currently oasis cannot precisely capture the perceivable impact of every issue and therefore cannot judge whether or not this issue would affect the related functionalities.
to address this problem the warning documents generated by oasis need to precisely capture the perceivable effect of the issues.
unfortunately understanding such semantics is challenging.
in future we plan to augment warnings with more semantic information e.g.
code comments information in api documentations to further improve oasis .
answer to rq2 concept similarity can improve the performance ofoasis by recovering semantic links between structured warning documents and free style user reviews.
such links cannot be well captured by token based methods.
threats to validity mismatch between user reviews and app versions.
we used all user reviews available at the google play store as the input in our experiment.
since the google play store provides no attributes in these reviews indicating which app version the users were commenting on there are chances that some user reviews are related to the historical versions of the examined app inducing the mismatch between app versions and user reviews.
however user reviews on historical versions can only affect our results if the problems of concern have already been fixed and lint raises false warnings on such problems.
to address this threat before linking user reviews and lint warnings oasis applies a filtering and weighting procedure to weaken the impact of old user reviews.
with such a treatment in our experiments we did not observe any negative impacts from the mismatch between app versions and user reviews.
applying oasis to other static analysis tools.
the second potential threat is that our approach may not generalize to other static analysis tools for android apps.
we currently implemented oasis for the android studio built in lint analyzer because lint is easily accessible to developers and is widely used for improving android app quality .
however our oasis design is not specific to lint and in principle oasis can also be applied to other static analysis tools like findbugs or pmd .
for example to work with findbugs we only need to adapt the warning information extraction part of oasis such that the descriptions and severity levels of the warnings generated by findbugs can be extracted to initialize the warning documents.
we plan to evaluate oasis with other well known static analyzers in our future extension of oasis .
limited number of evaluation subjects.
another threat is that we only evaluated oasis with six open source android apps.
such a scale of evaluation may not be able to fully reveal the strengths and limitations of oasis .
however we can see from table that these apps are diversified and representative covering six categories with different numbers of warnings and user reviews.
in addition we can also observe the difference between evaluation results of these apps e.g.
c geo is very well maintained and its developers make heavy use of lint and thus oasis cannot largely improve the results of lint .
such differences are caused by the special characteristics of these apps.
this also shows thatesec fse paderborn germany lili wei yepang liu shing chi cheung our current evaluation results can be a good estimation of oasis s performance on a wide range of android apps in practice.
impacts of app user reviews quantity and quality.
our approach relies on app user reviews.
the quantity and quality of available user reviews can affect the performance of oasis .oasis may not effectively prioritize static analysis warnings for apps with no or few reviews.
in our evaluation we addressed this concern by selecting subjects with different volumes of reviews see table .
oasis achieved good performance.
this indicates that oasis is applicable for apps with different volumes of reviews.
in future we plan to further study how the quantity and quality of app user reviews would affect the performance of oasis .
related work in this section we discuss related work on mining user reviews and static analysis warning prioritization.
.
mining user reviews various studies were made to leverage the prolific feedback information provided by user reviews such as user ratings and user comments to extract useful information to facilitate app development.
harman et al.
were among the first to mine useful information from app stores.
they proposed a typical procedure for app store mining and correlated app ratings with the number of app downloads.
khalid et al.
leveraged user ratings and correlated device information to prioritize android devices to perform testing.
more recent work focused on mining useful information from textual comments in user reviews.
khalid et al.
manually examined over user reviews and highlighted that users frequently complained about app issues that cause crashing or incompatibility.
this motivates us to leverage user reviews as a knowledge base to learn user perceivable problems.
other researchers aimed to automatically extract useful information from a large number of user reviews.
chen at el.
proposed ar miner to extract informative user reviews by text classification.
gu et al.
designed surminer to classify and cluster user reviews of an app to evaluate its different aspects.
panichella et al.
combined nlp text analysis and sentiment analysis techniques to categorize user reviews into predefined categories to aid app development and maintenance.
villarroel et al.
proposed clap to categorize and cluster user reviews.
clap is also able to prioritize the clustered user reviews to help app release planning.
di sorbo et al.
designed urm a two level classification model to categorize user reviews based on user intents and review topics.
based on urm they implemented surf to categorize user reviews accordingly.
another pioneer work correlated user ratings and code changes and found that developers who take user reviews seriously e.g.
by implementing requested features are rewarded in terms of good app ratings.
these techniques laid the foundation of app review analysis and helped oasis filter out noisy user reviews.
however our work studies a different problem and aims at linking user reviews to static analysis warnings for improving the usefulness of static analysis tools.
.
prioritizing static analysis warnings there are warning prioritization techniques designed for conventional programs that can be applied to android apps.
some existingstudies aimed to prioritize actionable warnings by statistical methods based on code revision histories and code characteristics.
for example kim et al.
prioritized static analysis warnings generated by findbugs pmd and jlint in java programs by mining code revision histories.
ruthruff et al.
predicted actionable findbugs warnings by logistic regression with factors integrating warning fixing histories.
as discussed earlier these techniques are by design biased to warnings similar to the fixed ones while oasis does not rely on the project history data of an app but rather utilizes user feedback to prioritize warnings such that warnings on new issues that developers have not encountered before can also be identified.
shen et al.
proposed efindbugs to perform a two phase issue prioritization based on manually labeled warning false positive data and user designations.
hanam et al.
leveraged machine learning techniques to learn actionable alert patterns and used the trained classifier for warning prioritization.
these techniques need either significant manual efforts or a large amount of data to train the classifier.
in contrast oasis leverages accessible user reviews and does not require additional manual efforts.
finally there are also other pieces of work proposing to redesign a better architecture of static analysis tools to aid developers .
these studies improve static analysis tools from a different angle.
conclusion and future work in this paper we presented a novel semantics based approach oasis to prioritizing lint warnings by leveraging app user reviews.
to achieve effective warning prioritization oasis augments lint warnings with contextual information and links them with app user reviews that contain complaints on problems caused by the issues of concern using our semantics aware similarity calculation.
we empirically evaluated oasis with six popular android apps.
the results show that oasis can effectively identify positive warnings and significantly outperform a baseline strategy that emulates a typical practice of app developers.
we also experimentally validated that our proposed semantics aware similarity calculation technique can largely improve the performance of oasis when compared to traditional token based similarity calculation techniques.
our study makes the first attempt to leverage semantics based similarity to link structured static analysis results with unstructured user reviews.
this work is still exploratory and can be improved in multiple ways.
in future we plan to extend oasis to leverage more contextual information e.g.
developers comments in the code api documentations for warning augmentation and further explore other methods e.g.
word embedding to build semantic links between static analysis warnings and user reviews.