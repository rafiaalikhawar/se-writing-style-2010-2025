recovering clear natural identifiers from obfuscated js names bogdan vasilescu school of computer science carnegie mellon university usa vasilescu cmu.educasey casalnuovo computer science department university of california davis usa ccasal ucdavis.edupremkumar devanbu computer science department university of california davis usa ptdevanbu ucdavis.edu abstract well chosen variable names are critical to source code readability reusability and maintainability.
unfortunately in deployed javascript code which is ubiquitous on the web the identi er names are frequently mini ed and overloaded.
this is done both for e ciency and also to protect potentially proprietary intellectual property.
in this paper we describe an approach based on statistical machine translation smt that recovers some of the original names from the javascript programs mini ed by the very popular u js.
this simple tool a performs comparably to the best currently available de obfuscator for javascript js which uses sophisticated static analysis.
in fact a is quite complementary to js performing well when it does not and vice versa.
we also introduce a new tool js which blends a andjs and signi cantly outperforms both at identi er name recovery while remaining just as easy to use as js .js is available online at ccs concepts software and its engineering software maintenance tools keywords deobfuscation javascript statistical machine translation acm reference format bogdan vasilescu casey casalnuovo and premkumar devanbu.
.
recovering clear natural identi ers from obfuscated js names.
in proceedings of 11th joint meeting of the european software engineering conference and the acm sigsoft symposium on the foundations of software engineering paderborn germany september esec fse pages.
introduction while the primary goal of programmers is to write programs that perform required tasks according to speci cations programs are also written to be read .
this was famously noted by don knuth instead of imagining that our main task is to instruct a computer what to do let us concentrate rather on explaining to human beings what we want a computer to do.
.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior speci c permission and or a fee.
request permissions from permissions acm.org.
esec fse september paderborn germany copyright held by the owner author s .
publication rights licensed to association for computing machinery.
acm isbn .
.
.
.
show that programmers spend a large portion of their work time reading code and trying to understand it .
as such when writing new code programmers consciously spend e ort to reduce the cognitive burden on those who would later read the code.
a central case in point here is the choice of variable names in code while names don t a ect program correctness and function a well chosen name well tting its context of use can considerably improve the readers comprehension .
in some sense the chosen name must seem natural unsurprising in the context so that readers nd the code familiar and easy to read.
coding standards that prescribe how to choose variable names also serve this purpose.
by the same token a poorly chosen variable name has the opposite e ect indeed deliberately choosing cryptic or ill suited names is recognized as a convenient way to obfuscate code.
furthermore in settings where it is desirable to make code hard to read there are tools available to corrupt the variable names.
in javascript js in particular there are two synergistic reasons for seeking to corrupt variable names.
first js programs are downloaded on demand often over mobile networks once downloaded they must be quickly loaded and interpreted.
developers often apply a mini cation technique removing whitespace shortening or mangling variable names etc.
to shrink les thus lowering bandwidth usage without a ecting functionality.
second js programs are shipped as source thus their logic and any proprietary clever tricks that are used can easily be viewed within any browser that executes them.
there is hence a naturally adversarial relationship between the original developers of an application who want to preserve competitive advantage and others who wish to simply reuse the code.
for these two reasons there is a substantial practical imperative to shorten js code since platform api calls and keywords must be retained local variable names should be made as short opaque and confusing as possible.
there are tools to automatically process a given js program with clear well named identi ers and return an ugli ed program that replaces all the variable names with single letter names.
for example u js1is an enormously popular such tool and large amounts of js code on the internet have been subjected to its distortions before being placed on websites.
u js is particularly aggressive about reusing cryptic variable names in multiple scopes this improves gzip compressibility often used after mini cation and diminishes readability while leaving program semantics intact.
there is also a natural need for automatic natural name recovery so that the large amounts of ugli ed js could be made more accessible for review learning maintenance reuse security analysis etc.
especially when source maps2are not available.
this is precisely what our system a does.
esec fse september paderborn germany bogdan vasilescu casey casalnuovo and premkumar devanbu in this paper we adopt a simple yet powerful and languageindependent intuition in the design of a software is highly repetitive .
gabel su observed that code in large corpora is rarely unique and tends to reoccur.
hindle et al.
then rst provided evidence for the naturalness hypothesis showing that repetition in code could be e ectively captured in statistical language models and used for software engineering tasks.
an illustrative task shown by hindle et al.
to work well in this regime was code suggestion given a context a statistical model estimated from a large code corpus was used to suggest the next token by maximizing over a probability from a model of the form p .
given the high degree of repetitiveness in large code corpora these models can make highly accurate predictions as shown by tools such as c .
we transfer this intuition concerning repetitiveness and naturalness of software to identi er naming for clarity and readability programmers will tend to choose the same name in the same context!
thus arises our central claim to guess the clear natural name of an obfuscated identi er all we must do is to guess the most likely name given the context in which in the obfuscated identi er appears .
in other words clear name argmax p given a context we choose that name which maximizes the conditional probability as shown.
given that programmers are uncreative in their choice of variable names and if we have a big enough corpus then we can estimate a decent model that gives reliable scores p .
our tool js realizes this intuition via two complementary mechanisms a the existing js tool which conditions natural names on semantic properties of the mini ed code using static analysis and b a which uses an o the shelf statistical machine translation tool and is comprised of two parts b1 a distortion model that captures exactly how the mini er changes names and b2 alanguage model that captures how programmers choose to write code including variable name choices .
we use the standard natural language translation model from m to estimate these probabilities from a large matched corpus of clear and mini ed names as we describe in more detail below in section .
this paper makes the following contributions we present a novel approach to automatically recover identi er names from mini ed js code training an o the shelf statistical translation model m originally designed for natural language translation on a large matched corpus of clear and mini ed code.
we show that even this simple language independent approach successfully recovers a non trivial number of the names used in the original un obfuscated code.
we show that a simple bit of tuning on the training data to account for js syntax and some post processing to manage scoping improves performance substantially.
with these two simple steps the performance of this m based approach which we call a matches that of js .
finally we observed that js anda are actually quite complementary each often performing well when the other fails this led us to o er to the community the js tool which is an opportunistic blending of the two js performs substantially better than either of its constituents.to our knowledge js is currently the best performing available tool to recover the original names from mini ed js names in addition our work illustrates how with some slight tuning of the training data to account for language speci cs and some simple post processing an o the shelf statistical nlp tool can be quite useful in software engineering applications.
readers are welcome to try js online at background a uses a statistical machine translation smt approach to recover the original names from the contrived ones that u js inserts.
our basic approach is independent of programming language but performance can be improved through scoping analysis the details of which we present below in section .
.
smt smt is a data driven approach to machine translation based on statistical models estimated from large bi lingual text corpora see ochs et al.
for a good introduction.
smt is widely used in services like google translate.
in smt documents are translated according to a probability distribution p e f that a string ein the target language say english is the translation of a string fin the source language say finnish .
as per the bayes theorem the probability distribution p e f can be reformulated as p e f p f e p e p f and the best output string ebestgiven an input string fis ebest argmax ep e f argmax ep f e p e p f argmax ep f e p e for a given f note above that once a finnish sentence fis given the marginal term in the denominator of is xed and we can just maximize the numerator as in .
this formulation of the translation problem is called the noisy channel model intuitively we think of finnish as a noisy distortion of english and attempt to recover the most likely english sentence that would have resulted in the finnish sentence.
as per there are two parts to the smt model a translation model which captures how english sentences can be noisily distorted into finnish ones p f e and a language model p e which captures the likelihood of di erent types of english sentences.
hence the problem of estimating p e f can be decomposed into two subproblems estimating a translation model p f e and a language model p e .
the connection to name de minifying is evident just as smt is used to de noisify and de distort finnish back to english one could use smt to remove the noise of mini ed variables and recover their original form.
the language model often based on n grams is estimated from a text corpus in any single language using fairly straightforward maximum likelihood methods .
the translation model can be estimated from parallel data using the expectation maximization em algorithm .
such parallel data matches text in source and 684recovering clear natural identifiers from obfuscated js names esec fse september paderborn germany target languages 3typically aligning matched text fragments at sentence level .
consequently each sentence in one language is matched to its corresponding translation in the other language.
then a phrase based translation model is typically estimated.
in principle one could estimate a word to word translation model which captures the probability of each word in the source language being related by translation to a speci c word in the target language.
however by including phrases sequences of words in the translation table one can both use local context for disambiguation and also capture local reordering which together substantially improves translation quality .
there s an accuracy performance tradeo here models trained with more data and longer phrase lengths in the extreme entire sentences or in principle even entire documents are more accurate but also more resource intensive slower and more prone to over t. for example m the smt toolkit we use by default uses phrases no longer than words in its translation model unless speci ed otherwise.
in natural language translation matching phrases needn t have the same length e.g.
the french la maison translates to the english home and comprising words needn t be in the same order e.g.
the english i want to go home translates to the dutch ik wil naar huis gaan with the verb now at the end .
therefore in practice the translation model p f e is extended to include a relative distortion probability distribution that penalizes too much reordering and a factor to calibrate the output length usually biasing longer output.
finally the translation model used by popular smt systems such as m the basis for a also includes a lexical weighting component which validates the quality of a phrase translation pair by checking how well its words translate to each other.
for example the lexical weight of the word pair maison home from the french english matching phrases above can be computed as the relative frequency of maison among all possible translations of home into french.
.
smt for name recovery statistical machine translation is surprisingly well suited for the problem of recovering original names from mini ed ones.
as discussed above while translating between natural languages smt needs to be very much aware of context.
thus the word bark in the two phrases the dog s bark and the tree bark would have to be translated quite di erently a modern smt tool is very capable of resolving such contextual ambiguities and making the right word choice based on context.
why?
fortunately there is a high degree of repetitiveness both within a single natural language as well as in the translation processes between languages.
given su cient training data smts are very capable of capturing and e ciently exploiting contextual regularities to provide nuanced correct translations.
as it turns out there is a great deal of predictable repetition in software source code in fact much more so than in natural language corpora .
as mentioned earlier programmers write code to be read by other humans code is harder to read than natural language so arguably readability is an even stronger imperative to consider when writing code.
as noted earlier identi er names are 3e.g.
the simultaneously translated multilingual european parliament proceedings are a valued resource.
input program minified output program un minified moses smtoptional pre processor post processorautonym jsnice aligned clear text minified corpus language model translation model clear text corpus model training figure overview of our approach.
chosen to be natural unsurprising and well suited to local context in much the same way that word choices in natural language re ect context.
thus even though tools like u js reduce many di erent identi ers to cryptic single letters like torx they do so in very systematic ways based on context and thus given su cient data smt based tools are very well suited to capture and exploit this through language and translation models.
a recent tool js aims to recover mini ed variable names based on a similar conceptual stance.
however rather than using smt directly on the source code they rely on semantic analysis and posit that a learned distribution of variable name choices conditioned on semantic dependencies of that variable would be su ciently informative to recover suitable names.
they use conditional random elds to model the name distribution.
their approach requires an analysis of the semantic dependencies in the code our approach only requires variable scoping information and recovers about as many variable names using an o the self smt tool m we also nd that their approach is quite complementary to ours in performance leading to a synergistic combination that outperforms both as we describe below.
approach we now motivate and present the technical details of our a approach.
we present the blending of a andjs later in section .
.
to use smt in any setting we rst have to train the models and then use the trained models to perform the translation task.
our approach relies on using a js mini er u js to generate large amounts of matched clear mini ed pairs of training data and then training an o the shelf smt system m on these pairs figure .
once trained our system a accepts js code as input and tries to recover clear natural identi er names as output by translating using m built in decoder the mini ed code into clear code much the same way one would translate say french to english.
.
translation challenges unlike natural language texts which are inherently polysemous and ambiguous and therefore more amenable to approximate translations computer programs have strictly de ned semantics that must be exactly preserved during translation.
so how can a statistical 685esec fse september paderborn germany bogdan vasilescu casey casalnuovo and premkumar devanbu 1var geom2d function var sum numeric .sum function vector2d x y this .x x this .y y mix vector2d dotproduct function dotproduct vector return sum function mix dest src for var kinsrc dest src return dest return vector2d vector2d a original1var geom2d function var n numeric .sum function t t n this .x t this .y n r t dotproduct function i t return n function r t n for var rinn t n return t return vector2d t b after mini cation with u js figure example javascript program.
approach with the inherent randomness like the one a uses do this?
the key insight comes from the structure of the problem we are trying to solve layout and source code comments aside mini ed js programs are alpha renamings of their clear text counterparts therefore the probability of any language construct being a translation of anything other than itself is zero by construction i.e.
the program s structure can be trivially preserved.4even so there are still two challenges with using smt for translating source code mini ed js in particular which we detail next.
our adaptations addressing these challenges are implemented in the pre and post processor components depicted in figure .
inconsistency.
the context captured by multi word phrases as opposed to single words is paramount to the e ectiveness of smt recall the example pairing bark with tree or dog .
however while each occurrence of bark can be independently translated in a natural language document depending on what is most appropriate for its context the same cannot be said about source code identi er names.
consider the example js program in figure .
indeed renaming di erent occurrences of the same identi er di erently e.g.
the two occurrences of non lines and in the mini ed program in figure 2b both corresponding to sumin the clear text version in figure 2a would alter the program s semantics.
there is no inherent mechanism in phrase based smt to ensure consistency of translation for the same identi er name .
this is especially true since phrases captured by the translation model component would likely not span the entire program recall the accuracy performance tradeo discussed in section indeed by default m translates each line independently.
we describe our solution to this challenge in section .
.
ambiguity.
while source code is unambiguous and admits no ambiguity in reserved language keywords foris always for across a large corpus identi ers could still be named di erently despite having similar context.
so far this is not much di erent from natural language where words can have multiple meanings.
however js mini cation carries an additional complication it is possible 4more sophisticated structure altering types of javascript compression such as those introduced by google s closure compiler compressing javascript are beyond the scope of this work.to overload the same mini ed name by reusing it within several di erent scopes.
an extreme version of this occurs when a line like function vector2d x y is mini ed as function t t n line in figure .
this is entirely legal tis simultaneously a function and a parameter to the function.
indeed this type of name overloading ampli es obfuscation and makes the program even harder to read.
there are two consequences.
first this presents a challenge for name recovery.
it s very unlikely that real programmers overload the same name in this confusing manner.
if we were to recover the same name for both occurrences of t a straightforward solution also to the inconsistency challenge above while certainly correct it would not result in natural names and reusable code.
therefore post translation renaming should be a empted only within scope this overloading is a peculiarity of javascript and is the only step that makes our approach language dependent.
second high performance statistical nlp methods smt included all rely on large training corpora.
in our case we train m translation model on a large corpus of aligned clean mini ed js source les figure .
therefore in addition to the inherent ambiguity resulting from say di erent parameter names being mini ed totacross our corpus this peculiarity of minifying js means that di erent function names are also mini ed to twhen such name overloading occurs.
as a result one has to choose the most likely clear text name that would have resulted in the tmini cation among many possible alternatives.
the more a mini ed name is reused the less useful its context becomes in the training corpus .
we describe our optimization addressing this challenge in more detail in section .
.
.
resolving inconsistencies them smt tool see figure reads a mini ed program line by line for each mini ed line lm m uses a conditional distribution over clear lines p lc lm to produce a candidate set of possible translations.
the number of suggested translations per line can be set as a parameter.
furthermore even without con guring m to provide multiple instead of single suggestions per line di erent suggestions for the same identi er occurring on di erent input 686recovering clear natural identifiers from obfuscated js names esec fse september paderborn germany lines may still ensue since lines are translated independently.
for example m might suggest that the line r t r in mini ed code be rendered as seta conv seta and also suggest that the next line e.g s t s be rendered as setb redup setb thus providing two choices to unminify the identi er t either conv orredup .
therefore for each mini ed identi er we must choose precisely one unmini ed translation from the available suggestions to ensure name consistency.
algorithm describes the process by which we recover in the postprocessor component seen in figure clear names from mini ed ones addressing the inconsistency challenge described above.
there are two subroutines.
first the procedure c c n takes a tokenized input le and uses m to generate a translation for each line line .
then it collects all possible renamings suggested by m for any given mini ed name lines .
however as discussed above recall the function t t n example we need to address the name overloading ambiguity challenge renaming both instances of tto the same name may result in unnatural renamings since one is a function name and the other is a parameter.
in order to preserve program semantics we need only ensure that distinct variable names are retained when they exist in the same scope .
therefore in the aforementioned example function tand parameter t line in figure will have di erent candidate sets and thus can be renamed di erently because they are not in the same scope.
second the procedure r c takes a set of candidate renamings for a mini ed name and ranks them based on how well they t the context where they are to be used i.e.
how natural they are.
for each candidate we use the same language model used bym to score translations recall the two smt components translation model and language model from section to compute a naturalness score for each corresponding mini ed line after temporarily renaming the mini ed name to that candidate lines .
we exhaustively try all candidates and rank them by their average naturalness score line computed across the di erent lines implicated.
we also experimented with other ranking schemes detailed below.
the main procedure p n lines iterates over all mini ed names and chooses the highest ranked candidate for each.
since a candidate could have been suggested for di erent mini ed names in the same scope the traversal order matters.
for example assume two same scope mini ed names nandt with candidates src and src dest respectively e.g.
line in figure .
since a candidate cannot be used multiple times in the same scope choosing srcfortbefore considering nwould invalidate srcas a candidate for nin a next step.
but which names should one give priority to?
we base our traversal on two criteria we assume that more frequent names in the input i.e.
those appearing on more lines are more important for program comprehension and should be considered rst we traverse names with smaller candidate sets rst.
consequently we sort all mini ed names by line frequency descending line then traverse all mini ed names with a single candidate translation lines before mini ed names with larger candidate pools lines .
in the example above we would rst rename ntosrc since thas more candidatesalgorithm choose consistent names procedure c c n input translate input using m .
assert translated lines have as many tokens as originals.
for all line 2m output do parse line.
for all mini ed name 2matching input line do record m suggestion as candidate renaming.
record line number.
procedure r c candidates for all candidate 2candidates do select all a ected mini ed lines and temporarily rename mini ed name tocandidate .
use language model to compute log probability for each line after renaming.
sort candidates by average log probability across all a ected lines descending.
procedure p n input c c n input sort mini ed names by how many lines they appear on in input descending.
for all name 2input with single candidate do rename to candidate if not already chosen elsewhere in same scope.
keep mini ed name otherwise.
for all remaining name 2input do r c candidates rename to top candidate if not used elsewhere in same scope.
keep mini ed name otherwise.
to choose from.
if during this process a mini ed name remains without any feasible candidate we leave it unchanged.
alternative ranking schemes .there are other ways to select a renaming from a set of candidates besides the one we presented above language model .
we describe two additional variants of the r c procedure that we experimented with.
frequency based.
a straightforward approach is to choose the renaming option that is suggested for the greatest number of lines on which the mini ed identi er appears ties can be broken e.g.
by name length .
this is best explained through an example.
recall the two potential choices to unminify the identi er tabove either conv orredup collected across di erent mini ed input lines where tappears.
in the frequency based ranking if tis rendered as conv times and redup only twice we would prefer conv .
this technique is simpler and faster than using the language model so it might be preferred in practice if it performs well.
feature based.
a more general approach is to extract features from both the context and the suggestions themselves and learn a classi er to rank candidates using a weighted combination of these features.
many features could potentially be useful how long is the name in characters?
is it only a single character?6does the name use camel case underscores or the dollar sign?
we can also take advantage of the language model to extract additional 5or we rename to an arti cially su xed mini ed name when the mini ed name itself had been the top candidate for something else in the same scope earlier very rarely .
6single character names are uncommon outside of iterators and should likely be avoided in many other contexts.
687esec fse september paderborn germany bogdan vasilescu casey casalnuovo and premkumar devanbu features what s the average log probability of all the implicated lines after replacing the mini ed name with the candidate i.e.
how well does the candidate t the context on average?7what s the maximum gain in probability across all implicated lines relative to the mini ed le i.e.
does the candidate t any mini ed line particularly well?
in addition to these suggestion features there are features that apply to the context where we are making the suggestion how many lines does the mini ed identi er appear on?
what s the maximum number of times the name appears on any single line?
does the name appear on lines with literals or de mini able names i.e.
globals ?
does the name appear on a line with a loop statement?
we will call these context features .
to determine which if any of these features might be useful and what the weighted combination should be we built logistic regression models in r on a set aside tuning dataset of clear text js les that had been mini ed by u js and de mini ed by a .
the criterion for evaluating a regression model was the number of original names correctly recovered.
we tried to answer the question when the desired renaming is among the translation candidates what is a weighted combination of suggestion and context features that maximizes the probability of ranking it as the top candidate?
therefore in our tuning sample we only considered suggestion lists that contained the original de mini ed name.
furthermore we selected exactly one incorrect renaming at random from each such list of potentially many candidates as imbalanced data can lead to poor models.
we also assessed multicollinearity using the variance in ation factor and excluded predictors accordingly.
finally we pruned the tuning data to remove suggestions with outlying behavior in their features which would act as high leverage points in the regression models.
equation lists the features we selected for our logistic model that performed best in the prediction task on the tuning data.
score lo linessu estedfor len th a era elo probdrop a era erawprob usescamelcase useunderscores contains we found that while including the context features as controls resulted in better model t excluding them led to better predictive performance.
similarly experimentation showed that while some features were useful as controls including their coe cients in the ranking score calculation diminished accuracy in recovering the original names in the tuning set.
the coe cients included in our nal ranking function are underlined the log of the frequency of the suggestion measured in lines the average log probability across implicated lines the average change in log probability between the mini ed and renamed implicated lines and whether the candidate name is a single character or not.
.
optimization reducing ambiguity we described in section .
how scoping information can help to resolve ambiguities caused by overloaded mini ed names e.g.
when a line function vector2d x y is mini ed as function 7this is exactly the naturalness score ranking criterion in algorithm above.table examples of context tokens that make up our hash renamings for some mini ed names in figure .
note that function r line does not get renamed because its hash has been used once by function tin the same scope line .
mini ed original location hash renaming n sum line sha1 var numeric.sum t vector2d line sha1 function t x line sha1 function r mix line r r k line sha1 for var in t t n line in figure .
to amplify this association between context and names we arti cially but consistently rename mini ed names in each scope before feeding the code to m in the preprocessor component in figure so that such name overloading is eliminated already prior to decoding .m can then more easily provide distinct translations.
of course this means that m must be trained on data that consistently renames variables according to scope in the same way.
one can readily imagine many such possible context aware renaming strategies.
one strategy we experimented with is to compute a sha1 hash of all the tokens on the de nition line8of a name that remain unchanged during mini cation i.e.
all language constructs and global names.
table illustrates the context tokens that feed the hashes for some of the mini ed names in figure .
note that all local names have been stripped away and the mini ed name itself has been replaced with the placeholder.
we also make sure not to overload hashes in the same scope see the example of function r .
in practice this renaming scheme increases the vocabulary size unique names in our corpus by approximately i.e.
it is quite e ective at disambiguating!
.
blending autonym with jsnice in order to blend a andjs we simply updated the c c n procedure in algorithm to record for each mini ed name in the input the js renaming as an additional suggestion to the ones o ered by m .
this means that the rest of our algorithm for choosing consistent names remains unchanged the renaming candidates which now include also the js renaming are ranked using the di erent schemes we presented above in section .
the top ranked candidate is chosen as the nal renaming.
we also tried building logistic models to explicitly discriminate between when to choose the name selected by js vsa using the suggestion andcontext features described earlier but found the models extremely poor.
thus js uses the simple blending scheme described here.
evaluation to evaluate a we train and tune a m model on a joint corpus of mini ed and clear text javascript code from g h then measure the accuracy of retrieving the original un mini ed names on a set aside testing corpus.
8we use information stored in the mini er s internal representation of the abstract syntax tree to infer which line is the de nition line for a name.
note that the de nition line is not always the line of rst occurrence e.g.
function r originally mix line is used on line before it is de ned.
688recovering clear natural identifiers from obfuscated js names esec fse september paderborn germany .
.
.
.
.
freqfreq hash lmlm hash logisticlogistic hash selection technique local names recovered filesfigure file level accuracy of our three name selection techniques with and without the optimization.
this evaluation strategy also adopted by js was chosen primarily due to its practicality i.e.
it can be fully automated.
note however that failure to recover the original identi er names is not necessarily a failure of the method as the suggested names could actually be better than the originals.
an extensive human evaluation of the quality of the recovered names for di erent attributes such as readability and while controling for the provenance of the training data e.g.
di erent open source projects have di erent naming conventions and style guides which could in uence performance is beyond the scope of this work.
.
experimental setup corpus.
we rst identi ed the oldest non fork javascript repositories using ght and cloned them from g h yielding .
million les with a .jsextension.
next we removed duplicate les i.e.
having the same sha1 hash leaving .
million unique les from these we randomly sampled to generate the following non overlapping sets les for translation model training les for m hyper parameter tuning and les for logistic regression estimation.
we further used heuristics and the mini er u js to determine which les are already mini ed and we excluded them from our samples rst we excluded les that remained unchanged after mini cation second since not all mini ed les remain unchanged e.g.
les mini ed by a di erent mini er could still change we used additional heuristics based on the distribution of identi er name lengths.
few other les that failed to parse during scope analysis we reuse the scope analysis information that u js collects during mini cation rather than writing our own scope analyzer were discarded as well.
in the end the di erent lters shrunk our samples to les in the m training set in the m tuning set and in the regression estimation set.
in addition we randomly sampled les to train the language model no overlap with the tuning and test sets .
finally we randomly sampled parseable and non mini ed les to form a held out testing set.
tuning and test les had lines or less the median le pre sampling had lines i.e.
our sample is representative .
the training and test sample sizes are comparable to those used by js .
.
.
.
.
.
autonym local autonym all jsnice local jsnice all jsnaughty local jsnaughty all renaming technique names recovered filesfigure file level accuracy for our a tool js and our blended tool js which substantially dominates the state of the art js .
training.
recall the two components to a m based translation system that need to be trained the translation model estimated from a parallel sentence aligned corpus in the source and target languages and the language model estimated from a standard corpus in the target language.
building our parallel js corpus was straightforward we passed all les in the training sample through the mini er u js9and scanned the output to ensure the program structure and alignment was preserved.
we then trained two standard translation models using the built in m train model.perl script one for the default case without the ambiguity reduction optimization in section .
and one with the context aware hash renaming implemented.
the only adjustment to the default settings was that we allowed phrases of length up to to be included in the phrase table default is .
for the language model we estimated a kenlm ve gram model with modi ed kneser ney smoothing after pruning singletons above order three.
tuning.
after training m assigns default weights to the different components that make up its translation model recall the discussion in section .
.
three10of these components are relevant to our discussion here the phrase table component which contributes a probability of two phrases sequences of consecutive tokens being translations of each other e.g.
how likely is it that function t t n is a mini ed version of function vector2d x y ?
the language model component which contributes a probability of the output being being natural js and thelexical weighing component which checks how well tokens translate to each other e.g.
how likely is it that tis a mini cation ofvector2d ?
.
we used the built in m mert moses.pl script with default settings on the hyper parameter tuning set to tune the di erent weights given to each component separately for each of the renaming techniques we tested.
evaluation criterion precision.
for each js le in our test set we minify it ourselves then use a after training and tuning to recover the original names.
we additionally run js 9we used version .
.
with the m mangler parameter.
10trivially token reordering does not occur in our setting.
689esec fse september paderborn germany bogdan vasilescu casey casalnuovo and premkumar devanbu on the same mini ed les for comparison.
we used the publicly available implementation of js from the unuglify js npm package 11which is a client of the nice2predict12framework.
our performance metric is the percentage of original pre mini cation local identi er names that each technique recovers per le global names can be trivially recovered since only local names are renamed during mini cation.
our test les contain between and local mini able names mean .
median or between and total names both local and global mean .
median .
.
choosing consistent names the rst set of results we present pertains to the strategy used to choose a consistent renaming from among a set of candidates.
we implemented all three strategies presented in section .
our default language model based approach lm the straightforward frequency based approach freq and the more general featurebased logistic regression approach logistic .
figure presents the distributions of precision scores on the test set for the three approaches in light green.
we observe that all three distributions are right skewed i.e.
all three methods have limited precision.
lm performs best it retrieves a median of the local names in our test les and it statistically signi cantly outperforms freq paired wilcoxon signed rank test p .
with a medium e ect size13 r .
.
lm and logistic are statistically indistinguishable paired wilcoxon signed rank test p .
i.e.
the more general feature based approach does not provide additional gains over our default language model based approach this is not particularly surprising as we have tuned the logistic regression coe cients only on data pre processed with the context aware hash renaming strategy discussed above in section .
.
.
capturing context using hash based pre renamings the second set of results we present visualized in dark green in figure pertains to the e ectiveness of the context aware hashbased renaming we apply as a pre processing step.
we observe that the model trained on a hash based renamed corpus consistently outperforms the standard not preprocessed one with a between percentage points increase in the median number of local names recovered.
all di erences between precision with and without hashing are statistically signi cant paired wilcoxon signed rank test p .
with medium e ect sizes r .4for freq r .38for lm and r .4for logistic .
the best performing model logistic hash uses the more general feature based logistic regression approach to rank candidate names as well as arti cially renaming local names to a hash of their context tokens section .
prior to translation.
naturally the training corpus must be preprocessed similarly.
the interquartile range of precision on local names is with a median of .
we select this model for the subsequent comparison to js .
13we report the rmeasure proposed by field as an alternative to cohen s dfor non normal distributions.
we interpret rusing the rules of thumb suggested by cohen with suggested thresholds of .
.
and .
for small medium and large magnitudes respectively.
.
.
.
.
.
.
.
.
.
.00autonym file accuracyjsnice file accuracy 204060frequency figure hexbin scatterplot comparing le level accuracy fora and js .
excludes les where both techniques fail or succeed completely.
.
comparison to jsnice the results from the comparison between a the best performing logistic hash variant and js are depicted in figure .
as discussed we perform the comparison of precision only on local names which get mini ed by u js and need recovery.
we also present light gray background boxplots the precision results on all names local and global the latter don t need any recovery to be consistent with the original presentation of js .
we observe that the static analysis based js and our simpler smt m based a perform comparably.
the precision values for js are more dispersed the interquartile range is with a median of in contrast a has higher median precision but the distribution is more concentrated iqr .
formal testing con rms our visual observation there is no signi cant di erence in medians between the two distributions of precision values paired wilcoxon signed rank test p .
.
.
blend between autonym and jsnice the previous comparison suggests that a andjs despite having similar performance behave quite di erently.
figure displays a hexbin scatterplot comparing the le level precision of a andjs at recovering originals from mini ed local names.
points below the main diagonal correspond to les where a outperforms js points above the main diagonal correspond to les where js outperforms a .
analyzing the scatterplot enables us to con rm the earlier intuition the two techniques seem quite complementary with one often succeeding when the other fails and vice versa.
this led us to propose js a straightforward blend between a andjs which adds the renaming proposed by js to the candidate pool proposed by m js uses the logistic regression based approach to rank candidates now one extra described above.
to assess how well does the blend performs we turn our attention back to figure where the last pair of boxplots depicts the distribution of le level precision values for js on local and for consistency with the original js presentation all 690recovering clear natural identifiers from obfuscated js names esec fse september paderborn germanyoriginal reqautonymfilestreamautonym hash reqjsniceqjsnaughtyreqoriginal resautonymresautonym hash resjsnicer jsnaughtyresoriginal headersautonymheadersautonym hash headersjsniceheadersjsnaughtyheadersoriginal jsonstreamautonymiautonym hash ijsnicesjsnaughtysoriginal separatorautonymcautonym hash datajsnicesepjsnaughtyseporiginal dataautonymdataautonym hash reqjsnicedatajsnaughtydataoriginal indexautonymresautonym hash resjsniceindexjsnaughtyindexoriginal tupleautonymtupleautonym hash tuplejsnicekeyjsnaughtytupleoriginal countautonymerrautonym hash errjsnicematchesjsnaughtyerroriginal errorautonymerrautonym hash errjsniceerrjsnaughtyerr1module.exports http.createserver function e r 2vart 3vari newstream.stream ...5varn 6csv .fromstream e .on data function e r 7if !t ... 8vara .zip t e .each function e ... 10i.emit data n json.stringify a 11n .on end function e 13i.emit data 14i.emit end .on error function e 16i.emit error e 17console.log csverror e.message figure case studyvery popular for the majority of potential adversaries theoutput ofuglifyjsprovides a su cient deterrence.turning tode obfuscation a key focus in reverse engineer ing and deobfuscation is on static dynamic analysis tech niques .
this has great relevance for malicious codedetection and has received a great deal of attention.given the constraints on obfuscation use for javascript these approaches are ill suited to the most common use case which is to recover full natural identifier names which arecorrupted by minifiers like javascript.
code analysis toolsgenerally focus on thesemanticsof obfuscated programs torecover intent however what most javascript programmersneed is a way to make minified programs easier to read withnatural well suited identifiers that promote human under standing.
thus in this setting a statistical approach whichhelps make minified programs look familiar viz textuallysimilar to most javascript programs that do the same thing is precisely what is needed thus smt techniques trainedover large corpora are specially well suited.
.
the naturalness of softwaregabel su observed that most short code sequencesare not unique following this work hindleet al showedthat statistical language models were just as e ective in factmore so for software as for natural language corpora thussuggesting that software is also natural.
language modelsare central to the great success of nlp techniques in speechrecognition translation and so on thus hindleet al s worksuggests great promise for the use of language models incode.
there have been substantial further applications ofstatistical models for code in areas such as coding stan dards mining and checking code summarization id iom mining and bug localization .the key insight of this work is thatidentifier names arenatural meaning that programmers choose natural sound ing identifier names in regular predictable repetitive waysthat reflect the context of use so as to convey a predictable unsurprising intent to the reader.
thus even though java script minifiers shorten variables to single letters there issu cient information in the context to predict which namesmake the most sense.
furthermore even if minifiers mightcontrive to map many di erent names in di erent or over lapping contexts to the same single letter names there issu cient regularity in joint distribution of the context ofboth clear and minified context that allows us to geta good statistical prediction on what the unminified nameshould be.
.
smt in software engineeringthere have been e orts to apply smt to software engi neering problems along the two directions below.migration.it is straightforward to imagine a potential useof smt in software engineering if programming languagesare natural can we automatically translate between themthe way we translate from english to french?nguyenet al.
were among the first to address thisquestion by experimenting with translation from java toc .
the authors treat source code as a sequence of lexicaltokens each code token is the equivalent of a word a methodis the equivalent of a sentence which enables them to ap ply a standard phrase based smt model out of the box.empirical evaluation on a parallel corpus of around 000java to c method translations automatically mined fromtwo open source projects available in both languages foundthe approach imperfect but promising more than half of alltranslated methods were syntactically incorrect yet userswould not have to edit more than of the total num ber of tokens in the translations in order to correct them.based on their experiments the authors advocate for moreprogram oriented smt models instead of purely lexical ones.in follow up work they propose several such models aimedat migration of api usages .karaivanovet al.
as did nguyenet al.
experi mented with translation from c to java on a parallel cor pus of around c to java method translations minedfrom open source projects available in both languages.
how ever they also trained hybrid phrase and rule based smtmodels that take the grammatical structure of the target lan guage into account.
experimental evaluation on a sample of1 c methods confirmed that the approach is promis ing smt was sometimes able to learn how to translate en tire methods and map one set of api calls to another espe cially with the more program oriented models roughly of the resulting translations compiled .
still the authorsnote that obtaining a parallel corpus of translated programsis challenging.documentation.pseudo code written in natural languagecan be a valuable resource during program comprehensionfor developers unfamiliar with the source code programminglanguages.
can we automatically translate source code intopseudo code using smt?
odaet al.
experimentedwith generating english and japanese pseudo code frompython statements reporting positive outcomes.
the au thors first created python to english statements and python to japanese statements parallel corpora by hiring programmers to add pseudo code to existing sourcecode.
then they trained di erent phrase based smt mod els that vary in their level of program orientation rangingfrom a purely lexical one to one that operates on modifiedabstract syntax trees asts .
experiments showed that allmodels generate grammatically correct and fluent pseudo code for at least of the statements with the moresyntax aware models performing better.figure example mini ed test le with names suggested by a with and without the hash based optimization js andjs shown alongside.
exact matches are highlighted green approximate matches yellow.
names.
we observe that js clearly dominates both of its constituent approaches it recovers a median mean of the local names in a le with an interquartile range of .
considering all names in a le both local and global js recovers a median of names with an interquartile range of .
the performance improvement is non trivial paired wilcoxon signed rank test p .
js performs signi cantly better than both js r .
and a separately r .
with a small e ect size.
although the e ect size is small given the experimental nding that js does a substantially better job at name recovery than the best available alternative it would make little sense for practitioners to not use our approach.
.
illustration we illustrate the complementary nature of a andjs which inspired js with the example in figure from which we draw the following observations some mini ed names are exactly reverted to the original names bya but not js e.g.
parameters eandron line restored to reqandres respectively while other names are exactly reverted by js but not a e.g.
parameter r on line restored to index .
both techniques sometimes suggest approximate matches e.g.
fornon line sepinstead of separator also eon line which although di erent than the originals capture the same spirit.
this reiterates the need for a future carefully controled human study to evaluate the quality of the suggested names beyond our automated approach.
some names are exactly recovered by all methods e.g.
ton line while others are not reverted by any e.g.
ion line .
related work .
obfuscation deobfuscation code obfuscation is a general topic spanning a wide range of goals in addition to intellectual property protection applications include inter alia tamper resistance and water marking .
obfuscation is also used in malice to evade detection by software defensive mechanisms .
there are some negative theoretical results by barak et al concerning the possibility of code obfuscation in general still obfuscation has durable commercial and intellectual charms a constant stream of new techniques papers and patents has continued over the years despite barak et al.
more speci cally for javascript there are some idiosyncratic constraints that impose some natural limits on excessive use of obfuscation.
first with the current browser ecosystem code must be shipped in source code form.
second typical javascript programs use quite a few apis the api calls must use the correct name which appears in the clear in the source code.
third in general run time e ciency is a concern so that obfuscations that require a runtime overhead are undesirable.
likewise obfuscations that increase code size are undesirable due to impact on bandwidth use indeed small is beautiful applies.
for all these reasons code mini ers are a good trade o they are simple easy to use and make the code smaller and di cult to read without a ecting performance.
u jsis thus very popular for the majority of potential adversaries the output of u js provides a su cient deterrence.
turning to de obfuscation a key focus in reverse engineering and deobfuscation is on static dynamic analysis techniques .
this has great relevance for malicious code detection and has received a great deal of attention.
given the constraints on obfuscation use for javascript these approaches are ill suited to the most common use case which is to recover full natural identi er 691esec fse september paderborn germany bogdan vasilescu casey casalnuovo and premkumar devanbu names which are corrupted by mini ers like javascript.
code analysis tools generally focus on the semantics of obfuscated programs to recover intent however what most javascript programmers need is a way to make mini ed programs easier to read with natural wellsuited identi ers that promote human understanding.
thus in this setting a statistical approach which helps make mini ed programs look familiar viz textually similar to most javascript programs that do the same thing is precisely what is needed thus smt techniques trained over large corpora are specially well suited.
.
the naturalness of software gabel su observed that most short code sequences are not unique following this work hindle et al showed that statistical language models were just as e ective in fact more so for software as for natural language corpora thus suggesting that software is also natural.
language models are central to the great success of nlp techniques in speech recognition translation and so on thus hindle et al s work suggests great promise for the use of language models in code.
there have been substantial further applications of statistical models for code in areas such as coding standards mining and checking code summarization idiom mining and bug localization .
the key insight of this work is that identi er names are natural i.e.
programmers choose natural sounding identi er names in regular predictable repetitive ways that re ect the context of use so as to convey a predictable unsurprising intent to the reader.
thus even though javascript mini ers shorten variables to single letters there is su cient information in the context to predict which names make the most sense.
furthermore even if mini ers might contrive to map many di erent names in di erent or overlapping contexts to the same single letter names there is su cient regularity in joint distribution of the context of both clear and mini ed context that allows us to get a good statistical prediction on what the unmini ed name should be.
allamanis et al.
use this idea to suggest more natural method and class names for otherwise unaltered code we use it here to recover any all mini ed identi er names in code subject to mini cation.
.
smt in software engineering there have been e orts to apply smt to software engineering problems along the two directions below.
migration.
it is straightforward to imagine a potential use of smt in software engineering if programming languages are natural can we automatically translate between them the way we translate from english to french?
nguyen et al.
were among the rst to address this question by experimenting with translation from java to c .
the authors treat source code as a sequence of lexical tokens each code token is the equivalent of a word a method is the equivalent of a sentence which enables them to apply a standard phrase based smt model out of the box.
empirical evaluation on a parallel corpus of around java to c method translations automatically mined from two open source projects available in both languages found the approach imperfect but promising more than half of all translated methods were syntactically incorrect yet users would not have to edit more than of the total number of tokens in the translations in order to correct them.
based on their experiments the authors advocate for more program oriented smt models instead of purely lexical ones.
in follow up work they propose several such models aimed at migration of api usages .
karaivanov et al.
as did nguyen et al.
experimented with translation from c to java on a parallel corpus of around c to java method translations mined from open source projects available in both languages.
however they also trained hybrid phrase and rule based smt models that take the grammatical structure of the target language into account.
experimental evaluation on a sample of c methods con rmed that the approach is promising smt was sometimes able to learn how to translate entire methods and map one set of api calls to another especially with the more program oriented models roughly of the resulting translations compiled .
still the authors note that obtaining a parallel corpus of translated programs is challenging.
documentation.
pseudo code written in natural language can be a valuable resource during program comprehension for developers unfamiliar with the source code programming languages.
can we automatically translate source code into pseudo code using smt?
oda et al.
experimented with generating english and japanese pseudo code from python statements reporting positive outcomes.
the authors rst created python to english statements and python to japanese statements parallel corpora by hiring programmers to add pseudo code to existing source code.
then they trained di erent phrase based smt models that vary in their level of program orientation ranging from a purely lexical one to one that operates on modi ed abstract syntax trees asts .
experiments showed that all models generate grammatically correct and uent pseudo code for at least of the statements with the more syntax aware models performing better.
a related problem is generating descriptive summaries in natural language i.e.
docstrings for functions written in programming languages using smt.
cabot reports on successful experiments with generating english docstrings for python functions.
he builds phrase based smt models on a parallel corpus of around docstring documented functions extracted from open source projects the phrases are the linearized asts extracted from these functions input and the english docstrings output alignment is performed using standard models .
conclusions mini ed javascript code is ubiquitous on the web the mini ed overloaded names in these programs impede program understanding.
using statistical machine translation tools with some postprocessing to both handle name scopes and to choose the best available name consistently we rst built a tool that essentially equals the performance of the state of the art tool js .
during evaluation we noticed that js and our tool a perform well in di erent settings so we then built an opportunistic mix of the two js that statistically signi cantly beats both.
our tool is available online based on its best in class performance we expect it should be the preferred tool for developers seeking name recovery for mini ed programs found in the wild .