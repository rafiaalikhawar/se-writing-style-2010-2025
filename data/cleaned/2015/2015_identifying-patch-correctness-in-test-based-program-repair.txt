identifying patch correctness in test based program repair yingfei xiong xinyuan liu muhan zeng lu zhang gang huang key laboratory of high confidence software technologies peking university moe institute of software eecs peking university beijing china xiongyf liuxinyuan mhzeng zhanglucs hg pku.edu.cn abstract test based automatic program repair has attracted a lot of attentioninrecentyears.however thetestsuitesinpracticeareoften tooweaktoguaranteecorrectnessandexistingapproachesoften generate a large number of incorrect patches.
toreducethenumberofincorrectpatchesgenerated wepropose anovelapproachthatheuristicallydeterminesthecorrectnessof the generated patches.
the core idea is to exploit the behavior similarity of test case executions.
the passing tests on originaland patched programs are likely to behave similarly while the failing tests on original and patched programs are likely to behave differently.also iftwotestsexhibitsimilarruntimebehavior the two tests are likely to have the same test results.
based on these observations wegeneratenewtestinputstoenhancethetestsuites and use their behavior similarity to determine patch correctness.
ourapproachisevaluatedonadatasetconsistingof139patches generated from existing program repair systems including jgenprog nopol jkali acsandhdrepair.ourapproach successfully prevented56.
ofthe incorrectpatchestobegenerated without blocking any correct patches.
acm reference format yingfei xiong xinyuan liu muhan zeng lu zhang gang huang.
.
identifying patch correctness in test based program repair.
in icse icse 40th international conference on software engineering may 27june gothenburg sweden.
acm new york ny usa pages.
introduction inthepastdecades alargenumberofautomatedprogramrepairapproaches havebeenproposed and many of them fall into the category of test based program repair.intest basedprogramrepair therepairtooltakesafaulty program and a test suite including at least one failing test thatreveals the fault as input and then generates a patch that makes alltestspass.however testsuitesinrealworldprojectsareoften theauthorsacknowledgetheanonymousreviewersfortheconstructivecomments andrevisionsuggestions.thisworkissupportedbythenationalkeyresearchand developmentprogramundergrantno.2016yfb1000105 andnationalnaturalscience foundation of china under grant no.
.
lu zhang is the corresponding author.
xinyuan liu and muhan zeng are equal contributors to the paper and their names are sorted alphabetically.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may june gothenburg sweden association for computing machinery.
acm isbn ... .
and a patched program passing all the tests may still be faulty.
we call a patch plausible if the patched version passes all tests in the test suite and we consider a patch correctif it fixes and only fixes the bug.
as studied by long et al.
the test suites in realworldsystemsareusuallyweaksuchthatmostoftheplausible patchesareincorrect makingitdifficultforatest basedprogram repair system to ensure the correctness of the patches.
as existing studies show multiple automatic program repair systems produce much more incorrect patches than correct patches on real world defects leading to low precision in their generated patches.
the low precision of existing program repair systems significantlyaffectstheusabilityofthesesystems.sincetestsuitescannotguarantee the correctness of the patches developers have to manually verify patches.
when the precision of a program repair system is low the developer has to verify a lot of incorrect patches and it is not clear whether such a verification process is more costlythan directly repairing the defect by the developers.
an existingstudy also shows that when developers are provided with low quality patches their performance will drop compared to the situationwherenopatchisprovided.asaresult webelieveitis criticaltoimprovetheprecisionofprogramrepairsystems even at the risk of losing some correct patches.
sinceaweaktestsuiteisnotenoughtofilterouttheincorrect patches produced by program repair systems a direct idea is to enhancethetestsuite.indeed existingstudies haveattempted togenerate newtest casesto identifyincorrectpatches.
however while test inputs can be generated test oracles cannot be automatically generated in general known as the oracle problem .
as a result existing approaches either require human to determine test results which is too expensive in many scenarios or rely oninherentoraclessuchascrash free whichcanonlyidentify certain types of incorrect patches that violate such oracles.
ourgoalistoclassifypatchesheuristicallywithoutknowingthe full oracle.
given a set of plausible patches we try to determine whethereachpatchislikelytobecorrectorincorrect andreject thepatchesthatarelikelytobeincorrect.ourapproachisbased on two key observations.
patch sim .
after a correct patch is applied a passing test usually behaves similarly as before while a failing test usually behaves differently.
test sim .
when two tests have similar executions they arelikelytohavethesametestresults i.e.
bothtriggering the same fault or both are normal executions.
patch sim allows us totest patches without oracles i.e.
werun thetestsbeforeandafterpatchingthesystemandcheckthedegreeof behavior change.
as our evaluation will show later patch sim alone already identify a large set of incorrect patches.
however acm ieee 40th international conference on software engineering authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden yingfei xiong xinyuan liu muhan zeng lu zhang gang huang wecanonlyutilizetheoriginaltestsbutnotthenewlygenerated test inputs as we do not know whether they pass or fail.
test sim complements patch sim by determining the test results of newly generated test inputs.
based on these two key observations our approach consists ofthefollowingsteps.first wegenerateasetofnewtestinputs.
second weclassifythenewlygeneratedtestinputsaspassingor failingtestsbycomparingthemwithexistingtestinputs.third we determinethecorrectnessofthepatchbycomparingtheexecutions before and after the patch for each test including both the original and the generated tests.
wehaverealizedourapproachbydesigningconcreteformulasto compareexecutions andevaluatedourapproachonadatasetof139patchesgeneratedfrompreviousprogramrepairsystemsincluding jgenprog nopol jkali hdrepair andacs .
ourapproachsuccessfullyfilteredout56.
oftheincorrectpatches without losing any of the correct patches.
the results indicate that our approach increasesthe precisionof program repairapproaches with limited negative impact on the recall.
in summary the paper makes the following main contributions.
weproposetwoheuristics patch simandtest sim which provide indicators for patch correctness.
we design a concrete approach that automatically classifies patches based on the two heuristics.
we have evaluated the approach on a large set of patches and the results indicate the usefulness of our approach.
the rest of the paper is organized as follows.
section first discusses related work.
section motivates our approach with examples andsection4introducesourapproachindetails.section5 introduces our implementation.
section describes our evaluation on the dataset of patches.
section discusses the threats to validity.
finally section concludes the paper.
related work test based program repair.
test based program repair is often treatedasasearchproblembydefiningasearchspaceofpatches usuallythroughasetofpredefinedrepairtemplates wherethegoal is to locate correct patches in the search space.
typical ways to locate a patch include the follows.
search algorithms.
some approaches use meta heuristic or random search to locate a patch.
statistics.
someapproachesbuildastatisticalmodeltoselect thepatchesthatarelikelytofixthedefectsbasedonvarious information sources such as existing patches and existing source code .
constraint solving.
some approaches convertthesearchproblemtoasatisfiabilityoroptimization problem and use constraint solvers to locate a patch.
whiletheconcretemethodsforgeneratingpatchesaredifferent weaktestsuitesproblemstillremainsasachallengetotest based program repair and may lead to incorrect patches generated.
as ourevaluationhasshown ourapproachcaneffectivelyaugment these existing approaches to raise their precisions.
patch classification.
facing the challenge of weak test suites several researchers also propose approaches for determining the correctness of patches.
some researchers seek for deterministicapproaches.
xin and reiss assume the existence of a perfect oracle usually manual to classify test results and generate new test inputs to identify oracle violations.
yang et al.
generate test inputs and monitor the violation of inherent oracles including crashesandmemory safetyproblems.comparedwiththem our approachdoesnotneedaperfectoracleandcanpotentiallyidentify incorrectpatchesthatdonotviolateinherentoracles buthasthe risk of misclassifying correct patches.
other approaches also use heuristic means toclassify patches.
tan et al.
propose anti patterns to capture typical incorrect patchesthatfallintospecificstaticstructures.ourapproachmainlyreliesondynamicinformation andastheevaluationwillshow thetwoapproachescanpotentiallybecombined.yuetal.
studythe approach that filters patches by minimizing the behavioral impact on the generated tests and find that this approach cannot increase the precision of existing program repair approaches.
compared withtheirapproach ourapproachclassifiesthegeneratedtestsand puts different behavioral requirements on different classes.
finally weimeretal.
highlightpossibledirectionsinidentifyingthe correctness of patches.
patch ranking.
many repair approaches use an internal ranking component that ranks the patches by their probability of being correct.
patch ranking is a very related but different problem from patch classification.
on the one hand we can convert a patch classificationproblemintoapatchrankingproblembysettingaproper thresholdtodistinguishcorrectandincorrectpatches.ontheother hand a perfect patch ranking method does not necessarily leadto a perfect patch classification method as the threshold can be different from defect to defect.
therearethreemaincategoriesofpatchrankingtechniques.the firstrankspatchesbythethenumberofpassingtests.however this category cannot rank plausible patches.
the second category uses syntactic andsemanticdistances fromtheoriginal program to rank patches.
as our evaluation will show later our approach could significantly outperform both types of distances.
the third category learns a probabilistic model from existingrulestorankthepatches.ourapproachcouldcomplement theseapproaches asourevaluationwillshowlater ourapproach isabletoidentify50 oftheincorrectpatchesgeneratedbyacs the newest approach in this category.
approaches to the oracle problem.
the lack of test oracle is a long standingprobleminsoftwaretesting andthesummariesof thestudiesonthisproblemcanbefoundinexistingsurveys .
among them a few studies focus on automatically generating heuristictestoracles.forexample invariantminingcouldpotentially mine invariants from passing test executions to classify newtestinputs.however theeffectofsuchanapplicationonpatch correctnessidentificationisstillunknownasfarasweareaware and remains as future work.
other related work.
marinescu and cadar propose katch forgeneratingteststocoverpatches.ourapproachcouldbepotentiallycombinedwithkatchtoimprovethequalityofthegenerated tests.
this is a future direction to be explored.
mutation basedfaultlocalizationsuchmetallaxis andmuse sharesasimilarobservationtopatch sim whenmutatingafaulty location passing tests would exhibit significantly smaller behavior authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
identifying patch correctness in test based program repair icse may june gothenburg sweden public void draw ... if true return ... a an incorrect patch produced by jkali public void testdrawwithnulldataset ... jfreechart chart chartfactory.
createpiechart3d test null ... try ... chart.draw ... success true catch exception e success false asserttrue success b a failing test checking for a null dataset public void testnullvalueindataset ... dataset.setvalue ... null jfreechart chart createpiechart3d dataset try ... chart.draw ... success true catch exception e success false asserttrue success c a passing test checking for a null value in a dataset figure an incorrect patch for chart changethanfailingtests.thisdualitybetweenapproachesindifferent domains indicates that the observation is general and could potentially be applied in more domains in future.
patch correctness and behavior similarity inthissection weanalyzetherelation betweenpatchcorrectness andbehaviorsimilaritytomotivateourapproach.wedefineapatch asapairofprogramversions theoriginalbuggyversionandthe patched version.
to simplify discussion we assume the program contains only one fault.
as a result a failing test execution must trigger the fault and produce an incorrect output.
weakoraclesandpatch sim.
asmentioned atestsuitemay beweakineitherinputsororacles orboth tomisssuchanincorrect patch.
to see how weak oracles could miss an incorrect patch let us consider the example in figure .
figure a shows an incorrect patch generated by jkali for defect chart in the defect benchmark defects4j .
in this example calling drawwill result in an undesired exception if the receiver object is initialized with a nulldataset.figure1 b showsatestcasethatdetectsthisdefect by creating such an object and checking for exceptions.
figure c shows a passing test checking that drawing with a null value inthe dataset would not result in an exception.
this patch simplyskips the whole method that may throw the exception.
in boththe passing test and the failing test the oracle only checks that no exception is thrown but does not check whether the output of the program is correct.
as a result since the patch prevents the exception both tests pass.
asmentionedintheintroduction werelyonobservationpatchsimtovalidatepatches.givenapatch wewouldexpectthatthe original program and the patched program behave similarly on... if repeat for inti i searchlist.length i intgreater replacementlist .length searchlist .length if greater increase greater ... a an incorrect patch generated by nopol ... for inti i searchlist.length i if searchlist null replacementlist null continue intgreater replacementlist .length searchlist .length if greater increase greater ... b the correct patch generated by human developers figure an incorrect patch for lang a passing test execution while behaving differently on a failing test execution.
in this example the passing test would draw something on the chart in the original program but would skip draw method completelyin thepatched program leading to significant behavioraldifference.basedonthisdifference wecandetermine the patch as incorrect.
weak inputs and test sim.
to see how the weak test inputs could lead to the misses of incorrect patches let us consider the exampleinfigure2.thisexampledemonstratesanincorrectpatch for lang in defects4j produced by nopol .
the original program would throw an undesirable exception when an element inreplacementlist orsearchlist isnull.
to prevent such an exception the correct way is to skip those elements as shown in figure b .
ho wever the generated patch in figure a blocks the whole loop based on the value of repeat which is a parameter of the method.
interesting all existing tests either previously passed or failed happen to produce the correct outputs in the patchedprogram.
this is because the value of repeathappens to be trueinallpassingtestsandbe falseinallfailingtests and the condition greater isnotsatisfiedbyanyelementin searchlist andreplacementlist inthefailingtests.
however enhancingthe testoraclesbypatch simisnotusefulbecausethebehavioron passingtestsremainsalmostthesameastheoriginalprogramwhile the behavior on failing tests changes a lot.
tocapturethoseincorrectpatchesmissedbyweaktestinputs we need new test inputs.
to utilize patch simwith new test inputs weneedtoknowwhethertheoutputsofthetestsarecorrectornot.
to deal with this problem we utilize observation test sim.
we assumethat whentheexecutionofanewtestinputissimilarto that of a failing test the new test input is likely to lead to incorrect output.
similarly when the execution of a new test input is similar tothatofapassingtest thenewtestinputislikelytoleadtocorrect output.basedonthisassumption wecanclassifynewtestinputs by comparing its execution with those of the existing test inputs.
in the example in figure for any new test input triggering this bug there will be an exception thrown in the middle of the loop whichissimilartotheexecutionsoffailingtests.ontheother authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden yingfei xiong xinyuan liu muhan zeng lu zhang gang huang hand for any test input that does not trigger this bug the loop willfinishnormally whichissimilartotheexecutionsofexisting passing tests.
measuring execution similarity.
animportantprobleminrealizingtheapproachishowwemeasurethesimilarityoftwotest executions.inourapproach wemeasurethesimilarityof completepathspectrum betweenthetwoexecutions.acomplete path spectrum orcpsinshort isthesequenceofexecutedstatement ids during a program execution.
several existing studies showthatspectraareusefulindistinguishingcorrectandfailing test executions and harrold et al.
find that cps is among the overall best performed spectra.
for example let us consider the two examples in figure and figure .
in both examples the defect will lead to an exception which further leads to different spectra of passing and failing tests the passing tests will execute until the end of the method while the failing tests will stop in the middle.
furthermore the failingtestexecutionswillbecomedifferentafterthesystemispatched no exception will be thrown and the test executes to the end of the method.
multiple faults.
inthecaseofmultiplefaultsinaprogram the twoheuristicsstillapply.whentherearemultiplefaults thefailing testsmayonlyidentifysomeofthem.wesimplytreattheidentified faults as one fault and the rest of them as correct program and the above discussion still applies.
approach .
overview figure approach overview figure shows the overall process of our approach.
we take the original buggy program a set of test cases and a patch as input andproduceaclassificationresultthattellswhetherthepatchis correct or not.
ourapproachconsistsoffivecomponentsclassifiedintothree categories test generation including test input generator distance measurement including testdistancemeasurer andpatchdistance measurer andresultclassification including testclassifier andpatch classifier .first testinputgenerator generatesasetoftestinputs.
we then run the generated tests on the original buggy program.duringthetestexecution wedynamicallycollectruntimeinformation about the test execution.
based on the runtime information test distance measurer calculates the distance between the executions of each newly generated test input and each original test case.
adistanceis a real number indicating how different two test executions are.
the result is a vector of test distances.
this vector isthenpassedto testclassifier whichclassifiesthetestaspassing orfailingbycomparingitsdistancestopassingtestsandthoseto failing tests based on test sim.
nowwehaveanenhancedsetoftestinputswhichareclassified aspassingorfailingandwecanusethemtodeterminepatchcorrectness.
givena patch patch distancemeasurer runs eachtest on the original program and the patched program and measure the distancebetweenthetwoexecutions.theresultisavectorofpatch distances.finally thisvectoristakeninto patchclassifier whichdetermines patch correctness by the distances based on observation patch sim.
in the rest of this section we introduce components in the three categories respectively.
.
test generation givenaprogram testgenerator generatestestinputsforthisprogram.furthermore sinceourgoalistodeterminethecorrectness of the patch we require the generated tests to cover the patched method.ifapatchmodifiesmultiplemethods thegeneratedtests should cover at least one of them.
intheory anytestinputgenerationtechniquescanbeusedinour approach.wecanutilizesymbolicexecutiontechniquestocoverthe specific method especially those designed for testing patches .
wecanalsoadoptrandomtestingtechniques andfilter out those that do not cover any of the modified methods.
.
distance measurement .
.
measuring distance.
as mentioned previously we measure the distance between two executions by comparing their complete path spectra.
as a result the problem reduces to measuringthedistancesbetweentwosequences.ingeneral therearemany different metrics to measure sequence distances such as longest common subsequence levenshtein distance hamming distance.
asthefirstattemptinclassifyingpatchesbasedonsequencedistances weusethelongestcommonsubsequence lcs asadistancemeasureandleaveotherdistancemetricstofuturework.anlcsoftwosequences aandbisthelongestsequencethatcanbeobtained from both aandbby only deleting elements.
we then normalize the length of lcs into a value between and using the following formula where aandbare two sequences.
distance a b lcs a b max a b .
.
test distance measurer.
component testdistancemeasurer takes a generated test and calculates its distance with each orig inal test.
the result is a vector of distances where each element represents a distance between a generated test and an original test.
to focus on the fault we only consider the executed statements within the calling context of the patched methods.
that is we locatepairsofpositionsontheruntimetrace a enteringapatched method from a method call and b leaving the method from the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
identifying patch correctness in test based program repair icse may june gothenburg sweden same call and keep only the statements between the positions.
thisstepcouldhelpusfilternoises twotestsmaybedifferentin statement executions outside the calling context of the patched methods butsuchadifferenceisoftennotrelatedtothefault.ifan originaltestdoesnotcoveranypatchedmethod wealsoexclude the test from distance measurement.
.
.
patch distance measurer.
component patch distance measurertakes each test either generated or original and calculates the distance between its executions on the original program and on the patched program.
the result is a vector of distances where each element represents the distance of a test.
differentfrom testdistancemeasurer hereweconsiderthefull sequence of executed statements.
this is because the compared executions come from the same test and they are unlikely to be noises outside the patched method.
.
classification based on the distances we can classify the generated tests and the patches.
we describe the two components one by one.
.
.
test classifier.
thetestclassifier classifiesthetestresult ofageneratedtestaspassingorfailing.somegeneratedtestsare difficulttopreciselyclassifyandwediscardthesetests.let result t denotesthetestresultoftheoriginaltest t i.e.
either passing failing ordiscarded.
let distance t t prime denotes the distance between the executionsof tandt primeontheoriginal program.givenagenerated testt prime weusethefollowingformulastodetermineitsclassification result.
the formula assigns the result of the nearest neighbor to the generated test.
classification t prime passing ap af failing ap af discarded ap af where ap min distance t t prime classification t passin af min distance t t prime classification t failin notethattheaboveformulacanonlybeappliedwhenthereis atleastapassingtest.ifthereisnopassingtest wecomparethe distances with all failing tests with a threshold ktand deem the testaspassingifthetestexecutionissignificantlydifferentfrom all failing tests based on the assumption that the original program worksnormallyonmostoftheinputs.pleasenoticethatthereis always at least one failing test which exposes the defect.
classification t braceleftbiggpassing kt af failing kt af where af min distance t t prime classification t prime failin .
.
patch classifier.
thepatchclassifier classifiesapatchas correctorincorrectbasedonthecalculateddistances.let distance p t denotes the distance between the executions of test tbefore and afterapplyingthepatch p.wedeterminethecorrectnessofapatch pusing the following formula.
classification p incorrect ap kp incorrect ap af correct otherwise where ap max distance p t classification t passin af mean distance p t classification t failin this formula checks the two conditions in observation patchsim.first thepassingtestshouldbehavesimilarly.tocheckthis condition we compare the maximum distance on the passing tests with a threshold kpand determine the patch as incorrect if the behaviorchangeistoolarge.second thefailingtestshouldbehave differently.
however since different defects require different ways to fix it is hard to set a fixed threshold.
as a result we check whethertheaveragebehaviorchangeinfailingtestsisstilllarger than all the passing tests.
if not the patch is considered incorrect.
we use the maximum distance for passing tests while using the average distance for failing tests.
an incorrect patch may affect onlyafewpassingtests andweusethemaximumdistancetofocus onthesetests.ontheotherhand afterpatched thebehaviorsof all failing tests should change so we use the average distance.
please note this formula requires that we have at least a passing test either original or generated.
if there is no passing test we simply treat the patch as correct.
implementation wehaveimplementedourapproachasapatchclassificationtool on java.
given a java program with a test suite and a patch on the program our tool classifies the patch as correct or not.
inourimplementation wechoserandoop arandomtestingtool asthetestgenerationtool.sinceourgoalistocoverthe patchedmethods testinggenerationtoolsaimingtocoveraspecific location seem to be more suitable such as the tools based on symbolic executions or search based testing .
however we did not use such tools because they are designed to cover the program with fewer tests.
for example evosuite generates at most three test cases per each buggy program in our evaluation subjects.
such a small number of tests are not enough for statistical analysis.
evaluation the implementation and the evaluation data are available online.
.
research questions rq1 towhatextentaretest simandpatch simreliable?
rq2 how effective is our approach in identifying patch correctness?
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden yingfei xiong xinyuan liu muhan zeng lu zhang gang huang rq3 howisourapproachcomparedwithexistingapproaches namely anti patterns opad syntactic similarity and semantic similarity?
rq4 how does test generation affect the overall performance?
rq5 howdotheparameters ktandkp affecttheoverall performance?
rq6 what are the causes of false positive and false negatives?
rq7 how effective is our tool in classifying developers correct patches?
rq1 examines how much test sim and patch sim hold in general.rq2focusesontheoveralleffectivenessofourapproach.in particular we are concerned about how many incorrect and correct patches we filtered out.
rq3 compares our approach withfour existing approaches for identifying patch correctness.
anti patterns capture incorrect patches by matching them with pre definedpatterns.opad isbasedoninherentoraclesthat patches should not introduce new crashes or memory safety problem.syntacticsimilarity andsemanticsimilarity are patch ranking techniques that rank patches by measuring syntactically or semantically how much their changes the program whichcouldbe adaptedtodeterminepatch correctnessbysetting a proper threshold.
rq4 and rq5 explore how different configurationsofourapproachcould affecttheoverallperformance.rq6 investigates the causes of wrong results in order to guide future research.
finally as will be seen in the next subsection though we have tried out best to collect the generated patches on java the correctpatcheswerestillsmallinnumbercomparedwithincorrectpatches.tooffsetthis rq7furtherinvestigatestheperformanceof our approach on the developers correct patches.
.
dataset we have collected a dataset of generated patches from existing papers.table1showsthestatisticsofthedataset.ourdatasetconsists of patches generated by six program repair tools.
among the tools jgenprog is a reimplementation of genprog on java a repair tool based on genetic algorithm jkali is a reimplementation ofkali onjava arepairtoolthatonlydeletesfunctionalities nopol isatoolthatreliesonconstraintsolvingtofixincorrect conditionsandtwoversions and2017 areusedin our experiment hdrepair uses information from historical bugfixestoguidethesearchprocess acs isatoolbasedon multiple information sources to statistically and heuristically fixincorrect conditions.
the selected tools cover the three types of patchgenerationapproaches searchalgorithms jgenprog jkali constraint solving nopol andstatistical hdrepair acs .more details of the three types can be found in the related work section.
the patches generated by jgenprog jkali and nopol2015 are collected from martinez et al.
s experiments on defects4j .
the patches generated by nopol2017 are collected from a recent report on nopol .
patches generated by hdrepair is obtained from xin and reiss experiment on patch classification .
the patches generated by acs is collected from acs evaluation .
all the patches are generated for defects in defects4j a widely usedbenchmarkofrealdefectsonjava.defects4jconsistsof six projects chart is a library for displaying charts math is alibraryforscientificcomputation timeisalibraryfordate timeprocessing lang is a set of extra methods for manipulating jdk classes closureisoptimizedcompilerforjavascript mockitoisa mocking framework for unit tests.
some of the patches are not supported by our implementation mainly because randoop cannot generate any tests for these patches.
inparticular randoop cannotgenerate anytests forclosure and mockito.
we removed these unsupported patches.
the patches from martinez et al.
s experiments the acs evaluation and qi et al.
s experiments contains labels identifying the correctnessofthepatches whichmarkthepatchesas correct incorrect o r unknown.
the patches of nopol2017 do not contain such labels.wemanuallycheckedwhethertheunlabeledpatchesand some labeled patches are semantically equivalent to the humanwrittenpatches.
sincethepatcheswhose correctnessisunknown cannot be used to evaluate our approach we remove these patches.
intheend wehaveadatasetof139patchesgeneratedbyautomatic program repair tools where are incorrect patches and are correct patches.
toanswerrq6 wealsoaddedalldeveloperpatchesondefects4j intoourdataset.sameasgeneratedpatches weremovedtheunsupportedpatches includingallpatchesonclosureandmockito.
in the end we have developer patches.
please note that developer patches are only used in rq6 since they have different characteristics compared with generated patches.
.
experiment setup test generation.
we kept randoop to run minutes on the originalprogramandcollectedtheteststhatcoveredthepatchedmethods.westopat3minutesbecauseformostdefects randoopproduced enough tests within three minutes and for the remaining defects that do not have enough tests lengthening the time would not lead to more tests.
we then randomly selected tests for each patch.
if there were fewer than tests we selected all of them.
in the end we have .
tests per patch in average with a minimum of 0test.basedontheclassificationoftest sim ofthegenerated tests are passing tests.
rq1.to evaluate patch sim we measured the average distance between test executions of patched and unpatched versions and check whether there is significant differences between passing and failingtestsoncorrectandincorrectpatches.toevaluatetest sim we measured the distances between tests and analyzed whether closer distances indicate similar test results.
rq2.weappliedourapproachtothepatchesinourdatasetand checkedwhetherourclassificationresultsareconsistentwiththe labels about correctness.rq3.
weappliedthefourexistingapproachestothedatasetand compared their results with our result.
anti patterns was originally implemented in c. to apply antipatternsonjavadataset wetookthesevenanti patternsdefined bytanetal.
andmanuallycheckedwhetherthepatchesinour dataset fall into these patterns.
opad uses inherent oracles that patches should not introduce newcrash ormemory safety problems.
opadwas originally designedforcandweneedtoadaptitforjava.ontheonehand authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
identifying patch correctness in test based program repair icse may june gothenburg sweden table dataset projectjgenprog jkali nopol2015 nopol2017 acs hdrepair total generated developer patches pcipcipcipcipcipcipcipci chart lang math time total p patches c correct patches i incorrect patches crashesarerepresentedasruntimeexceptionsinjava.ontheother hand memory safety problems are either prevented by the java infrastructure or detected as runtime exceptions.
therefore we uniformly detect whether a patch introduces any new runtime exception on test runs.
if so the patch is considered incorrect.
regarding syntactic and semantic distances different papers have proposed different metrics to measure the syntactic and semantic distances and a summary can be found in table .
however as analyzed in the table many of the metrics are defined for a specific category of patches and cannot be applied to general patches.
in particular many metrics are designed for expression replacementonlyandtheirdefinitionsonothertypesofchangesare notclear.asaresult wechosethetwometricsmarkedas general in table for comparison one measuring syntactic distance by comparingastandonemeasuringsemanticdistancebycomparing complete path spectrum.
theast basedsyntacticdistanceisdefinedastheminimalnumber of ast nodes that need to be deleted or inserted to change the oneprogramintotheotherprogram.forexample changingexpres sion a b 1toc d 1needstoatleastremovethreeastnodes a b and insert three ast nodes c d giving a syntactic distanceof6.thesemanticdistancebasedoncomplete pathspectrum forprogram pisdefined usingthefollowingformula where tois the set of all original tests that cover at least one modified method anddistance pis defined in section .
.
.
led p mean distance p t t to thesyntactic semanticdistancegavearankedlistofthepatches.
then we checked if we could find an optimal threshold to separate the list into correct and incorrect patches.
rq4.weconsideredtwodifferenttestgenerationstrategiesand compared their results with the result of rq1.
nogeneration.
thisstrategysimplydoesnotgenerateany testinput.thisstrategyservesasabaselineforevaluating how much the newly generated test inputs contribute to the overall performance.
repeated randoop runs.
since randoop is a random test generation tool different invocations to randoop may lead to different test suites.
this strategy simply re invokes randooptogenerateapotentiallydifferentsetoftestsandthe comparison helps us understand the effect of the randomness in test generation on the overall performance of our approach.
since the second strategy involves re generating the tests and isexpensive to perform we evaluated this strategy on a randomlyselectedsubsetof50patches.toseetheeffectofrandomness we repeated the experiments for times.
rq5.duringtheexperimentsforthefirstthreeresearchquestions we setthe parametersof ourapproach asfollows kp .
kt .
.
these parameter values are determined by a few attempts on a small set of patches.
toanswerrq4 wesystematicallysetdifferentvaluesforparameterskpandktandthenanalyzedhowtheseparametersaffectour result on the whole dataset.rq6.
we manually analyzed all false positives and false negatives to understand the causes of false classification and summarize the reasons.
rq7.we applied our approach on human written patches provided by defects4j benchmark and check whether our approach misclassified them as incorrect or not.
hardware platform.
the experiment is performed on a server with intel xeon e3 cpu and 32gb memory.
.
result of rq1 reliability of heuristics table shows the results of evaluating patch sim i.e.
the distancesbetweentestexecutionsonpatchedandunpatchedversions.
as we can see from the table for correct patches the distances of passing tests are very close to zero while failing tests have a muchlargerdistancesthatis9.5timesofpassingtests.theresult indicates that patch sim holds in general.
on the other hand thepassingtestsandthefailingtestsdonotexhibitsuchastrong propertyonincorrectpatches.whilethedistancesoffailingtests are still larger than passing tests the ratio is only .
times rather than9.5times.thisresultsindicatethatpatch simcanbeused to distinguish correct and incorrect patches.
figure4showstheresultsofevaluatingtest sim.thex axis shows intervals of distances while the y axis shows the percentage of tests fall into the intervals.
as we can see from the figure when twotestshaveashortdistance theyaremorelikelytohavethesame testresultsratherthandifferenttestresults.thisresultindicates that test sim holds in general.
on the other hand when the two tests have a long distance they are more likely to have different test results rather than the same test results.
.
result of rq2 overall effectiveness table and table shows the performance of our approach on thedatasetpertoolandperproject respectively.asshowninthe tables our approach successfully filtered out of incorrect plausiblepatchesandfilteredoutnocorrectpatch.furthermore our authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden yingfei xiong xinyuan liu muhan zeng lu zhang gang huang table different syntactic and semantic metrics metric typescope description ast based syngeneral number of ast node changes introduced by the patch.
cosine similarity synreplacement thecosinesimilaritybetweenthevectorsrepresentingastnodeoccurrencesbefore and after the change.
it is not clear how to apply it to insertions and deletions because there will be a zero vector and cosine similarity cannot be calculated.
locality of variables and constants synexpression replacementthe distance is measured by the hamming distance between the vectors representing locations of variables and constants.
it is not clear how to apply it to patches with multiple changes.
expression size distance synexpression replacementthe distance is when two expressions are identical otherwise the size of the affected expressionafterpatching.itisnotclearhowtoapplyittochangesotherthanexpression replacement.
complete path spectrum semgeneral the difference between complete path spectra.
model counting semboolean expression replacementthedistanceismeasuredbythenumberofmodelsthatmaketwoexpressionsevaluate differently.
the definition is bounded to boolean expressions.
output coverage semprograms with simple outputsthedistanceismeasuredbytheproportionofdifferentoutputscoveredbythepatched program.
it is not clear how to define output in general for complex programs.
syn and sem stand for syntactical distance and semantic distance respectively.
table patch sim passing tests failing tests incorrect patches .
.
correct patches .
.
x axis intervals of distance on tests y axis percent of tests figure test sim approachshowssimilarperformanceondifferenttoolsanddifferent projects indicating that our results are potentially generalizable to different types of projects and different types of tools.
please note that although our approach did not filter out any correct patch on our dataset in theory it is still possible to filter out correct patches.
for example a patch may significantly changetable overall effectiveness per tool tool incorrect correct incorrect excludedcorrect excluded jgenprog .
jkali .
nopol2015 .
nopol2017 .
acs .
hdrepair .
total .
in correctexcluded showsthenumberofpatchesthatarefiltered out by our approach and are in correct.
table overall effectiveness per project project incorrect correct incorrect excludedcorrect excluded chart .
lang .
math .
time .
total .
the control flow of a passing test execution e.g.
by using a new algorithmorcallingasetofdifferentapis butthetestexecution couldproducethesameresult.however giventhestatusofcurrentprogramrepairapproaches suchpatchesareprobablyscarce.when applying our approach on human written patches some correct patchesarefilteredout.moredetailsfortheeffectivenessonhumanwritten patch is discussed in rq6.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
identifying patch correctness in test based program repair icse may june gothenburg sweden ourapproachtookabout5to10minutestodeterminethecorrectnessofapatchinmostcases whilesomepatchesmighttake upto30minutes.mostofthetimewasspentongeneratingthetest inputs and recording the runtime trace.
.
result of rq3 comparing with others anti patterns.
amongall139patches anti patternsfilteredout 28patches where27areincorrectand1iscorrect.theresultshows that our approach significantly outperforms anti patterns.
furthermore of the incorrect patched filtered out by anti patterns were also filtered out by our approach while the remaining patches were not filtered out by our approach.
this result suggests thatwemay potentiallycombinethetwo approachestoachievea better performance.
opad.when applied with the same set of test inputs as our approach opadfailedtorecognizeanyoftheincorrectpatches.to further understand whether a stronger test suite could achieve better results we further selected up to tests instead of tests for each patch.
this time opad filtered out incorrect patches.
this resultsuggeststhatinherentoraclesmayhavealimitedeffecton classifying java patches as the java infrastructure has already preventedalotofcrashesandmemorysafetyproblemsanditmaynot be very easy for a patch to break such an oracle.
x axis intervals of syntactic distance y axis numbers of patches figure syntactic distance syntactic and semantic distance.
fig.
shows the distribution of incorrect patches and correct patches on syntactic distance.
the x axisshowstheintervalsofdistanceswhiley axisshowsthenumbers of patches within the intervals.
as we can see from the figure theincorrectpatchesandcorrectpatchesappearinallintervalsand the distribution shows no particular characteristics.
if we would like to exclude .
incorrect patches using syntactic distance we need to at least exclude .
of the correct patches.
this result indicates that syntactic distance cannot be directly adapted to determine patch correctness.
fig.
shows the distribution of incorrect and correct patches on semantic distance.
as we can see from the figure both types of patches tend to appear more frequently when the distance is small.
x axis intervals of semantic distance y axis numbers of patches figure semantic distance when the distance grows larger both types decrease but correct patches decrease faster.
if we would like to exclude .
incorrect patches using semantic distance we need to exclude .
correct patches.
this result indicates that semantic distance could be a bettermeasurementthansyntacticdistanceindeterminingpatch correctness but is still significantly outperformed by our approach.
please note that the above results do not imply that syntactic andsemanticdistancesarenotgoodatrankingpatches.whileit isdifficulttofindathresholdtodistinguishcorrectandincorrect patches for a group of defects it is still possible that the correct patches are ranked higher on most individual defects.
.
result of rq4 effects of test generation table6showstheresultwithoutgeneratingtestinputs.without the generated test inputs our approach filtered out less incorrect patchesandstillfiltered0correctpatch.thisresultsuggeststhat patch sim alone already makes an effective approach but test generationandtest simcanfurtherboosttheperformancenontrivially.
table comparison with no test generation default approach no generation incorrect excluded correct excluded regardingrandomness werepeatedourexperimentontheselected50patches5timesandgotdifferentresultsononly3incorrect patches.
the best case had only one more excluded incorrect patch than the worst case.
the result shows that randomness does affect the result but the impact is limited.
.
result of rq5 parameters two parameters are involved in our approach ktandkp both ranging from to .
the results of our approach with different parameters are shown in table and where each column shows theresultwiththeparametervalueinthetablehead.aswecansee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden yingfei xiong xinyuan liu muhan zeng lu zhang gang huang table parameter kp .
.
.
.
.
.
.
.
.
ie71 ce4 ie incorrect excluded ce correct excluded table parameter kt .
.
.
.
.
.
.
.
.
ie65 ce41000000000 ie incorrect excluded ce correct excluded from thetables settingthe parameterstodifferentvalueshave a limitedimpactontheoverallresultsandalargerangeofparameter value could achieve the best performance.
the result indicates that our approach does not require a precise tuning of parameters.
.
result of rq6 causes of wrong result ourapproachgavewrongresultson47incorrectpatches.wemanually analyzed these patches and identified three main causes of the wrong classification as follows.
too weak test suite.
it is often the case out of that only onefailingtestcoversthepatchedmethod.withoutpassingtest our approach only relies on the threshold ktto classify the tests anditissometimesdifficulttogenerateteststhatpassthethreshold.
asaresult wemighthavenooronlyafewpassingteststoperform the patch classification leading to a low performance.
unsatisfying test generation.
another common case out of overlap with the previous case is that test generation tool failstogeneratesatisfyingtests.randoopmightfailtogivetests thatcoverthepatchedmethodorfailtogenerateteststhatcould expose the incorrect behavior.
the patches in this category have the potential to be correctly identified if we use a stronger test generation tool.
unsatisfyingclassificationformula.
thefinalcase 8outof48 wascausedbylargebehaviorchangesinsomefailingtestexecution.
since we calculated the average distance of all failing test executions in the patch classification if there was a very large value the averagevaluemightbecomelargeevenifalltherestfailingtests had small behavior changes.
as a result the patch may be misclassified.thisproblemmaybefixedbygeneratingmorefailingtest cases to lower down the average value or to find a better formula to classify the patches.
.
result of rq7 developer patches among the correct developer patches our tool classified .
patchesasincorrect.wefurtheranalyzedwhythe16patches aremisclassifiedandfoundthatallthe16patcheshavenon trivially changedthecontrolflowandcausedasignificantdifferenceincps inthepassingtestexecutions.inparticular thebehaviorsofpassing testshavesignificantlychangedin6patches whileintherest10 patches the behaviors remain almost the same but the executedstatementschangedsignificantly e.g.
callingadifferentmethodwith the same functionality .
the results imply that human patches are indeed more complex than those generated by current automatedtechniques whenthecomplexityofpatchesgrows ourapproachisprobablystilleffectiveasonlyasmallportionofcorrectpatchesisexcluded tofurtherenhancetheperformance we need to enhance patch sim and cps to deal with such situations.
threats to validity and limitations the main threat to internal validity is that we discarded some patchesfromourdataset eitherbecausetheircorrectnesscannot bedetermined orbecausetheinfrastructuretoolusedinourimplementationcannotsupportthesepatches.asaresult aselectionbias may beintroduced.
however we believethis threat isnot serious becausetheremovedpatchesaresmallinnumbercomparedwith thewholedatasetandtheresultsonthesepatchesareunlikelyto significantly change the overall results.
the main threat to external validity is whether our approach canbegeneralizedtodifferenttypesofprogramrepairtools.while we haveselectedrepairtoolsfrom allmain categoriesof program repair tools including tools based on search algorithms constraint solving and statistics it is still unknown whether future tools will havecharacteristicssignificantlydifferentfromcurrenttools.tominimize such a threat we have added rq7 to test on developer patches which can be viewed as the ultimate goal of automatically generated patches.
the results indicates that our approach mayhave different performance on developer patches and generated patches but the difference is limited.
themainthreattoconstructvalidityisthatthecorrectnessofthe patchesaremanuallyevaluatedandtheclassificationmaybewrong.
to reduce this threat all difficult patches are discussed through the first two authors to make a mutual decision.
furthermore part oftheclassificationcomesfrommartinezetal.
sexperiment whoseresultshavebeenpublishedonlineforafewyearsandthere isnoreportquestioningtheclassificationqualityasfarasweare aware.
therecanbemanydifferentchoicesindesigningtheformulas.
for example we can use a different sequence distance or even a different spectrum to measure the distance of two executions.
we canusedifferentstatisticalmethodsforclassifyingtestsandpatches.
the current paper does not and cannot explore all possibilities and leave them as future work.
conclusion inthispaper wehaveproposedanapproachtoautomaticallydeterminingthecorrectnessofthepatchesbasedonbehaviorsimilarities between program executions.
as our evaluation shows our approach could effectively filter out .
of the incorrect patches generated without losing any of the correct patches.
the resultsuggests that measuring behavior similarity can be a promising way to tackle the oracle problem and calls for more research on this topic.