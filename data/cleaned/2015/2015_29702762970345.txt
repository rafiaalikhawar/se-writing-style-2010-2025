what makes killing a mutant hard willem visser department of computer science stellenbosch university south africa willem gmail.com abstract mutation operators have been studied at length to determine which ones are the best at some metric for example creates the least equivalent mutants creates hard to kill mutants etc.
.
these studies though have focused on speci c test suites where the test inputs and oracles are xed which leads to results that are strongly in uenced by the test suites and thus makes the conclusions potentially less general.
in this paper we consider all test inputs and we assume we have no prior knowledge about the likelihood of any speci c inputs.
we will also show how varying the strength of the oracle have a big impact on the results.
we only consider a few mutation operators mostly relational only a handful of programs to mutate amenable to probabilistic symbolic execution and only consider how likely it is that a mutant is killed.
a core nding is that the likelihood of reaching the source line where the mutation is applied is an important contributor to the likelihood of killing the mutant and when we control for this we can see which operators create mutations that are too easy versus very hard to kill.
ccs concepts software and its engineering !software testing and debugging keywords mutation testing probabilistic symbolic execution .
introduction in mutation testing see excellent survey of the history of the eld in one makes many small changes to the original program called mutants via the application of mutation operators and then run a test suite to see how many of these mutants are detected referred to as being killed the ratio of killed mutants over the total mutants is called the mutation score for the test suite.
the appeal of this mutation score as a test adequacy measure is in large part due to itbeing related to how e ective a test suite is at nding errors.
there is of course the small matter of whether mutations are proxies for real errors but there seems to be more and more evidence suggesting they are the most comprehensive study on this topic so far is .
this is not the topic of the current paper here we are instead interested in another use of mutations namely as a way to seed faults in a piece of code.
more speci cally when seeding faults via mutations how di cult is it to detect these mutations faults?
although fault seeding was the original motivation for the work the outcomes of this research is also directly applicable to probably the biggest drawback of mutation testing there are too many mutations possible and thus the technique has scalability issues.
we will show not only how to reduce the set of mutation operators by focusing on ones that create mutations that are in general hard to kill but also where to apply these mutations.
we are not the rst to consider how di cult it is to kill mutants the most closely related work to ours is that of yao et al.
in .
however what makes our work unique is that we basically will use an approach that is in general impossible do an exhaustive analysis of all possible test inputs to determine how hard a mutant is to kill.
our approach will produce the percentage of the input space that will exhibit a di erence in the oracle for the mutated program from that of the original.
in other words on how much of the input space is the mutant killed.
what this allows us is to remove the bias an existing test suite might have when determining whether a mutant can be killed.
as a by product our approach will be able to detect equivalent mutants i.e.
ones where zero percent of the inputs show a di erence between the oracle results for the mutation and the original.
obviously there needs to be a tradeo that makes this analysis possible and indeed what we lose is the generality of the results.
we will only be able to handle programs that t with the following requirements must be amenable to symbolic execution as well as model counting that calculates the number of solutions to a constraint .
in fact in this paper we will restrict even more by imposing that only linear integer arithmetic operations are allowed.
we will only look at some common mutation operators but our approach is general for any operator but space and time limitations dictate this limitation.
so our new idea is that we can only look at some programs but for those programs we can do a much more thorough analysis of mutation operators than what has gone before.
we will make three main contributions with regards to what makes a mutant hard to kill permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
reachability of the mutation is important for example some mutation operators creates mutations that are always killed if you can reach them.
mutation operators are not all equally hard to kill once you control for the reachability of the mutation .
we will show which ones tend to create hard to kill mutants and which creates mutants that are easily killed.
oracle sensitivity is another major component of the detection capability.
we will show the delicate balance between equivalent mutants and hard to kill mutants.
.
small example consider the very simple java method below boolean test int i int j return i j and assume we want to apply the classic relational mutation operator to it which will change the to and !
to produce mutations.
assume further that we consider the test oracle to be whether the mutations return the same output as the original.
lastly our analysis requires a bound on the size of the variables iand jwhich we pick to be the range i j they are of course bounded in java anyway but we pick a smaller range just to illustrate the ideas better .
in this very simple case it should be obvious that the to mutation is the hardest to kill and our tool indicates only out of the possible inputs will kill this mutant in other words of the inputs similarly to will be killed by all inputs i.e.
of the inputs.
since our analysis is actually at the bytecode level there is another mutation operator that can be applied and that is to change true tofalse and vice versa of course actually it is changing to and vice versa at the bytecode level .
intuitively it would be obvious that both these mutations should be killed by of the inputs but interestingly it turns out in both cases it hovers around and respectively .
why is this?
it is because to reach these mutations you rst need to evaluate the boolean condition and only about half the values reach each of the locations i j and i j .
so on the face of it it looks like this mutation is not bad but in fact if you control for how many values reach the mutation point it turns out this mutation is terrible since all inputs will kill it.
in other words all the input values that reach the mutation point will kill the mutant.
the output our tool will produce for this program is the following note that during the translation from source code to bytecode some operations are negated for example the becomes a at the bytecode level table analysis output description di erent reaching kill ratio the description shows the mutation applied including the line number di erent shows the number of input values that show a di erence in output between the mutated and original program reaching shows the number of inputs that reach the point where the mutation is applied lastly kill ratio is the ratio of the values showing a di erence over the ones that reached the mutation.
in the following section we will describe the infrastructure required to produce this data automatically.
.
approach the approach taken here is similar to that proposed in and is just specialised for the analysis of mutations.
speci cally our approach is based around symbolic execution of both the original and the mutated program and recording the path conditions under which the oracle di ers between them.
these path conditions combined with constraints on the ranges for each variable is then fed to a model counter to count the number of satisfying assignments for each path condition.
we use symbolic pathfinder spf for the symbolic execution and we use green to interface to the model counting which can then use the model counters latte or barvinok as a back end .
.
symbolic execution symbolic execution has been well studied the last few years and we point the interested reader to the following two surveys on the topic .
the basic idea is just to consider inputs symbolic rather than concrete and to collect constraints called path conditions on these inputs during the symbolic analysis when encountering branching instructions.
non branching instructions such as assignments just change the contents of variables that could later be involved in branching.
we will here use classic symbolic execution and not dynamic symbolic execution that operates a little di erently see .
however please note either will work equally well.
as noted above we use spf as our symbolic execution engine.
for each program we want to analyse we create the following pseudo code that we will symbolically execute analyser params mutation mutatedmethodx params original originalmethodx params if !oracle mutation original count the parameters params above are symbolic and both programs original and mutated version takes the same symbolic parameters.
note that if the two programs are identical then exactly the same paths will be followed in both programs after the rst though the path conditions will be xed and the second one will just collect the same constraints.
an important restriction on the code being analysed is that it may not change the input parameters because that will break the above rule that identical programs will follow the same paths.
this is not a serious restriction.
another more severe restriction is that the code being analysed may not contain loops that are dependent on the symbolic inputs.
if this happens then the mutated method will not terminate during the symbolic analysis.
one possible way around this issue is to limit loop un rollings but 40here we rather not do that since that makes the results partial.
an interesting scenario occurs though when a mutation creates an in nite loop see section .
.
a mutation is almost always di erent than the original though at least syntactically but it might be that the oracle is not precise enough to detect the di erence.
we will revisit this point in section .
.
if we consider our simple example from section and the mutation to recall the oracle is identity then the path condition where the oracle is unequal would be input i input j and input i input j where the rst conjunct comes from the mutated code and the second conjunct from the original.
.
mutator we use an in house mutation tool1that mutates the java bytecode.
it takes as input the class le as well as a list of methods on which mutation is to be applied.
each method that we want to mutate we create a copy of and rename it by prepending the word mutated as in the pseudo code above and this and the class it is from then becomes the input to the mutation tool.
the output is a class le that contains code as above including the mutated method and the original method as in the pseudo code .
right now we do not support high order mutations i.e.
mutations applied on programs that are already mutated.
for this study we only support some common mutation operators relational operators integer constant operators and arithmetic operators to for example .
as stated before there is no limitation to what can be used as mutation operators but for now we just focused on some popular operators.
similarly a source code mutator would allow more exibility for the operators that we can support for example deleting statements and we are working on such an extension.
there is one extra step that the mutator needs to accomplish and that is to indicate where the mutation is being applied since we want to use that position to record the number of inputs that reach the mutation.
our solution to this problem is very simple we just add a nop instruction2right before the mutation in the controlow.
for in case a java compiler might add a nop for some reason we just check that there are not more nops than mutations currently xed at which so far has never been the case.
there are probably other more elegant solutions to this problem but this one ts well with our analysis as will be discussed next.
.
analysis the most important point about the analysis is that we record path conditions and then use a model counter to count the number of solutions for the path conditions.
the actual analysis is done via a jpf listener which allows code to be added on callbacks from jpf s execution.
the main method we are listening on is instructionexecuted which is called for every bytecode instruction being executed.
we listen for these speci c events the entry method to the symbolic analysis analyser from above to nd the domain size for each symbolic variable.
1modi cations to the jumble tool from 2the nop bytecode instruction doesn t seem to be used by any java compiler we tested the rst call to the nop instruction on each path to record the path condition and then to use that to calculate the values owing to the point where a mutation is applied.
note that it is possible to hit the same nop instruction more than once if it is in a loop or if it is in a method that is called more than once.
calls to count to determine the path conditions on which the mutation and the original di er and then count the solutions to these path conditions.
when a property is violated propertyviolated call which here refers to when the mutation caused the program to throw an exception.
we again record the path condition and do the model counting for it as with the count case above since the original doesn t throw exceptions of course it can in theory but we don t consider those cases at this point the nal step at the end of the symbolic execution run is to produce the output as in table .
to make this analysis more e cient we use green to interface to the model counting.
basically green uses partitioning canonisation and caching of results to speed up the analysis.
it was previously shown that when analysing mutations where most of the program is unchanged the speed up by using green can be substantial .
since we are only using a model counter for linear integer arithmetic here latte to be precise we are of course putting one more restriction on the type of programs we can analyse i.e.
programs that only contain linear integer arithmetic nothing non linear for example .
this is also a semi arti cial restriction since we can use model counters for more domains but for some of these the counting becomes approximate.
one tricky scenarios that can occur during the analysis is when a mutation creates an in nite loop.
detecting innite loops is of course non trivial and we just assume it is an in nite loop if a loop is un rolled too often during the analysis knowing that the original program didn t contain an in nite loop .
since this is quite rare we simply label the mutants for which this occurs and try to validate by hand that it is in fact an in nite loop.
.
evaluation we will do three di erent experiments in this section3.
firstly we will show how our analysis compares to where a similar but manually driven approach was taken then we will show that controlling for the location of the mutation is important when considering mutations that are hard to detect and lastly we will show that the power of the oracle is crucial for killing mutants.
we will use the following two de nitions stubborn mutant we will consider a mutant hard to kill or in the terminology of stubborn if less than of the inputs can kill the mutation.
we will refer to mutations that are stubborn even after controlling for the number of values reaching the mutation location as really stubborn .
sometimes when the context is obvious we will refer to really stubborn mutants as just stubborn.
equivalent mutant if zero percent of the inputs can kill a mutant it is considered equivalent to the original.
3artefacts available from 41we are not reporting any runtime for our experiments but no run took more than mins and the vast majority nished in seconds due to reusing results cached by green .
.
hard to kill from it seems relational mutation operators is an interesting class where they found that these operators create about the same amount of equivalent and stubborn mutants.
their de nition of stubborn mutants are that a branchadequate test suite cannot kill the mutant but that it is not equivalent which they determined by hand .
we picked two of the programs they analysed namely triangle and tcas and also analysed them.
we greatly appreciate that they made all their examples available plus all their results see y.jia projects equivalent mutants.
we also added two of our own versions of triangle there are tens of examples of this classic problem available online with and without bugs and thus it is great to study in mutation analysis .
interestingly the example from is incorrect but that makes no di erence to their results nor this comparison.
since we do our analysis at the bytecode level we can also get results for logical connector mutation operators to kkand vice versa by lifting them out of the results for the bytecode mutations we did this by hand for the tcas example.
we used the following ranges for our symbolic variables for triangle and for tcas.
table relational mutation operator examples program muts stubborn equivalent t0 t0 us t1 t2 tcas tcas us tcas lc tcas lc us program is the program being analysed where t0 is the triangle version from and the second entry is our analysis of the same code.
since we are only looking at relational operators here for mutation muts refer to the number of these mutations stubborn is the number of hard to kill mutants but has a di erent meaning for us than for .
equivalent is the number of equivalent mutants.
first thing to note is that gets two more equivalent mutants this shows how hard it is to determine whether something is in fact equivalent by hand we could con rm that the two extra ones are in fact not equivalent.
neither of them are even stubborn by our de nition and likely .
since t0 is in fact not semantically equivalent to t1 and t2 means we cannot compare them any further.
however if we consider t1 and t2 against each other where they both correctly solve the triangle problem one can see what a big in uence even just di erent solutions to the same problem has when looking at mutations t1 is a much more verbose solution and thus have more scope for mutations and even stubborn ones whereas t2 that solves the exact same problem has no stubborn mutants.
note however that the two semantically equivalent programs has the same number of equivalent mutants.the tcas results from seems to have missed two lines where a number of mutations were possible hence only considering mutations versus our it also explains why they missed two equivalent mutants.
note that tcas has the same number of mutations as the larger triangle example but a lot more stubborn and equivalent mutants.
the last two entries in table is where we also looked at the logical connector lc mutations which found to be particularly good for creating stubborn mutants but few equivalent mutants.
our results mirror their s here but we nd less stubborn mutations.
.
controlling for location a well known principle in mutation testing is the so called rip requirements for killing a mutant which dictates you must be able to r each the mutation point it must i nfect the state of the program and this must p ropogate to somewhere it can be observed by an oracle.
test cases that can show infections are said to weakly kill the mutations and ones that can show a di erence in the oracle are strongly killing the mutants.
in this section we will consider only strongly killing the mutations i.e.
with an oracle but we will take a novel approach to the reachability issue by not just looking at whether it is reachable but rather howreachable it is by considering the number of inputs that reach the mutation point .
furthermore we will show that if you consider only the values that reach the mutation then one can have a much better handle on which mutation operators create stubborn mutants and which do not.
table stubborn after controlling for location prog muts stubb really always easy t0 t1 t2 tcas table shows the results when we control for the number of values that reach the mutation after the jj .muts are the total number of mutations applied in these examples there are only relational constant replacements and arithmetic operator replacement .
numbers in parenthesis are for just the relational mutation operators hence comparable to table except for tcas where these are the raw numbers whereas in table we post processed them to lift it to the source level for comparison with .
the stubb column indicates the number of hard to kill mutants i.e.
less than of the inputs kill the mutation and after the jjwe see the ones that are really stubborn really as well as the ones that now become inevitably killed always and those that are now easy to kill since more than of the inputs reaching the mutation can kill it.
what this data is showing should come as somewhat of a surprise to most users of mutation testing when taking how hard it is to reach a mutation point out of the equation it seems the majority of mutations are easy to kill.
first thing to notice is that the constant and arithmetic replacements seem to account for almost half the killed cases in fact if we consider the allways kill ratio i.e.
killed over total number of these mutants we get t0 t1 t2 and tcas .
42the problem is that even for the relational replacement which is probably the most popular class of mutation operator they are simply too easy to kill.
if we consider the easy kill ratio i.e.
killed over the total relational mutations we get t0 t1 t2 and tcas .
the good news is that not all relational replacements are created equally and when we look at the operators individually we see that some are more prone to creating easy always killed mutants and some are better at creating stubborn mutants.
table show the operations in terms of the percentage that fall into each category.
table relational replacement always above and stubborn above operator equiv stubborn always easy !
!
table shows the results per operator operator with the number in the parenthesis indicating how many of these mutations there were equiv for the percentage equivalent mutants stubborn the percentage mutants that are hard to kill less than of the inputs reaching the mutation always the percentage killed with of the inputs reaching and easy the percentage that kills the mutant with more than of the inputs reaching the mutation.
above the dividing line we have all the operators that were always above i.e.
the bad operators since they are too easy to kill and below the line we have all the ones where stubborn is above i.e.
the good operators that are hard to kill even after controlling for reaching the location of the mutation .
with one exception we have the not operations above the line only doesn t t and we have the offby one cases below the line.
at this point pretty much anyone will think dah!
i did not need all this machinery to tell me something this obvious.
of course one could argue that we have too little data which is a fair point but it is very hard to argue against these intuitive results though.
.
oracle strength in the previous two sections we looked at code where the oracle has been rather precise in fact the identity function applied to the outputs of the mutated and original program.
this means that we made sure of reaching the mutation since we consider all inputs and our examples has no dead code since we never have of the inputs reach a mutation and we made sure that if the error infected the state and propagate the oracle will catch it the only real variables were the mutation and the semantics of the code.
the problem is that in the real world the oracle is not always that precise and this could also in uence things.
here we will analyse a binary search tree this speci cexample has been well studied by the testing community with two di erent oracles the representation invariant repok and one where the linearisation of the tree is checked linearise .
this code also introduces loops.
basically we will look at all sequences of operations to the tree of length where each step is either adding or removing something from the tree after the complete sequence we check the oracle i.e.
check that the mutant is strongly killed .
in the mutated case either the add or remove operation is mutated.
all variables that we add or remove are symbolic with a range of .
table bst with di erent oracles.
linearise repok oper equiv alw easy equiv alw easy all !rel rel not table shows the results for the two oracles linearise and repok .
we again show the percentages of equivalent equiv always killed alw and too easily killed easy but no stubborn cases since there were none.
the results are shown for all all the mutations the non relational !rel ones arithmetic and constant replacement all the relational cases rel and lastly the negated cases not which we saw previously to produce lots of easily killed mutations including always killed .
we don t show any data for the o by one mutations since there was only one in the code and that produced an equivalent mutant.
as before these results control for reaching the mutation.
what should be immediately obvious is that the representation invariant repok for binary trees seem not to be very good at nding errors since it has more than double the amount of equivalent mutants than when you compare the linearisation of the trees produced by the mutated versus correct code .
conversely since the linearisation is so much more precise it has almost double the amount of easily killed mutants compared to repok.
the relational operators that negate conditionals again perform badly when it comes to producing easily killed mutants.
for the precise oracle it has easily killed mutants of which the majority is in fact always killed.
even for the imprecise oracle things are not much better easily killed and again the majority is always killed.
.
discussion we know that reaching a mutation is required for killing it but here we show that justreaching it can be enough for many relational mutation operators to be killed.
the one obvious criticism of this work is that the programs we looked at here are rather small triangle largest one around loc tcas around loc and bst around loc.
we know that larger programs have more opportunity for mutations and larger programs will also have more paths branches in all likelihood.
however what we are really analysing here is in some sense the width of the paths i.e.
the number values that ow down that path not the number of them.
if you have more branches paths then by de nition they will on average become less wide every branching point will reduce the width of the paths 43from that point on .
following this line of reasoning to its conclusion would mean that the larger the program the more likely it is on average that reaching the point of mutations will become less likely.
this would lead to more stubborn mutations since the e ect of just reaching the mutation will dominate but should have very little e ect on really stubborn mutants i.e ones that only look at the values that reach the point of mutation.
of course this would be in the case where the oracle is precise however all bets are o if you have a bad oracle.
luckily even in the case of an imprecise oracle we have shown that relational mutation operators that negate a conditional i.e.
completely change the direction of the ow of values leads to mutations that are too easy to kill.
hence they should best be avoided.
however if you have concerns over equivalent mutants then this class is great because being easy to kill implies less equivalent mutants table top part and table last line .
on the other end of the scale we have the o by one relational operators that are by de nition hard to kill since they make small changes to the ow of values.
guess what though making small changes would also mean it would be easier for the oracle to miss those changes and hence we could see more equivalent mutations table bottom part .
.
related work this is a well studied eld hence we will try and focus on what we consider the most closely related work to ours.
we have already discussed the relationship with at length but it is by some distance the most closely related.
on a operational level we use the core ideas from the approach of probabilistic symbolic execution and more speci cally to do our analysis.
discovering equivalent mutants has been a particularly active area of research see for an extensive list .
here we restrict this problem to a domain where we can get precise results and we can show that grun et al.
has taken the correct approach by looking at the impact of a mutation on the execution with higher impact leading to less equivalence when you have a large impact as in the case of the negated conditional you see less equivalent mutations.
there are not many studies looking at how hard it is to kill a mutant and they both take the approach of evaluating against a test suite.
we show here that when the analysis exhaustively gives one interesting new insights that are hidden when the test inputs are limited and one doesn t consider the strength of the oracle.
.
conclusions a simple takeaway from this paper is that if you want to test whether your test suite is good at nding subtle errors or you want to only seed faults that are hard to detect by some testing technique then only use the o by one relational operators a reduction of to .
extending this work to more operators is clearly possible and left for future work.
a few interesting puzzles are left unanswered.
firstly just et al.
in a large study observed a positive correlation between mutation detection and real fault detection independent of coverage .
our results seem to suggest coverage is very important.
secondly what about higher order mutations mutating a program already mutated will reachability play as much of a role there?
lastly can weaddress the link between real faults and mutations in our setting?
on this last point we evaluated all the buggy versions of triangle and no single mutation we tried matched any of them.
of course higher order might work especially if you allow the delete statement mutation.
.