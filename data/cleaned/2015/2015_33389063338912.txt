a statistics basedperformance testing methodologyforcloud applications sen he sen.he utsa.edu universityof texasat sanantonio usaglennamanns gmm6jd virginia.edu universityof virginia usajohnsaunders js8ra virginia.edu universityof virginia usa wei wang wei.wang utsa.edu universityof texasat sanantonio usaloripollock pollock udel.edu universityof delaware usamarylousoffa soffa virginia.edu universityof virginia usa abstract thelowcostofresourceownershipandflexibilityhaveledusers toincreasinglyporttheirapplicationstotheclouds.tofullyrealize the cost benefits of cloud services users usually need to reliably know the execution performance of their applications.
however due to the random performance fluctuations experienced by cloud applications the black box nature of public clouds and the cloud usagecosts testingoncloudstoacquireaccurateperformanceresults is extremelydifficult.
in thispaper we presentanovel cloud performance testingmethodology called pt4cloud .by employing non parametric statistical approaches oflikelihood theory and the bootstrap method pt4cloud provides reliable stop conditions to obtain highly accurate performance distributions with confidence bands.
these statistical approaches also allow users to specify intuitive accuracy goals and easily trade between accuracy and testing cost.
we evaluated pt4cloud with benchmark configurations onamazonwebserviceandchameleonclouds.whencompared with performance data obtained from extensive performance tests pt4cloud provides testing results with .
accuracy on average while reducing the number of test runs by .
we also propose twotestexecutionreductiontechniquesfor pt4cloud whichcan reduce the number of test runs by .
while retaining an average accuracy of .
we compared our technique to three other techniques andfoundthat our results are muchmore accurate.
ccs concepts general and reference performance computer systems organization cloud computing software and its engineering software testinganddebugging .
keywords performancetesting cloudcomputing resourcecontention nonparametric statistics permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
esec fse august 26 30 tallinn estonia associationfor computing machinery.
acm isbn ... .
format senhe glenna manns john saunders weiwang lori pollock and mary lousoffa.
.astatistics basedperformancetestingmethodologyfor cloud applications.
in proceedings of the 27th acm joint european software engineeringconferenceandsymposiumonthefoundationsofsoftwareengineering esec fse august 26 30 tallinn estonia.
acm new york ny usa 12pages.
introduction the low costs of ownership flexibility and resource elasticity have prompted many organizations to shift applications to the cloud in particular infrastructure as a service iaas clouds to host their applications .forclouddeployments itiscriticalthattheusers have accurateknowledge of the performance of their applications sothattheycanselecttheappropriatevirtualmachine vm configurationstosatisfytheirperformanceandcostobjectives .
the effectiveness of cloud elasticity policies also relies on the accurateknowledge ofapplicationperformance .
the most reliable approach to determine the performance of an application on the cloud is performance testing.
to obtain accurate results performance testing usually has two requirements .
first thetest inputs should be accurately generated based onthedesiredusecases.second itrequiresthattheperformance of each test input is independently and accurately determined.
for betteraccuracy acommonpracticeistoruntheapplication undertestwithatestinputmultipletimestoobtaintheaverageorcertain percentiles ofits performance .
however itextremelydifficulttodeterminewhenenoughperformance testruns areexecuted andaccurate resultsare obtained on the cloud.
for example figure 1gives the results of two performancetestsofthesamebenchmark ycsb onthesameamazon web service aws vm .
for each test the same test input is executedrepeatedlyforanextendedperiodof15hours.asfigure shows theperformancedistributionsobtainedfromthetwotests aredrasticallydifferent andtheiraveragethroughputsaredifferent by12 .itisevidentthatthesetwotestresultscannotbebothaccurate.
in fact as shown in section both results are inaccurate and more test runs need to be conducted to get reliable performance.
this difficultytogetreliableperformanceisalsoa majorobstacle facedbysystemresearch .
the difficulty of getting accurate performance testing results on the cloud is caused by cloud performance uncertainty which esec fse august 26 30 tallinn estonia sen he glenna manns john saunders weiwang lori pollock andmary lou soffa throughput .
.
.
.
.
.0005density test test figure1 performancedistributionsofycsbfromtwotests.
constitutestheconsiderableandunpredictableperformancefluctuation of cloud applications .
because of this uncertainty a new performance testing methodology is required for the cloud to obtain accurate performance foreach testinput.
in particular a good methodologyshouldaddressthefollowingchallenges posed bycloud performance uncertainty.
the first challenge arises from the various factors that cause performanceuncertainty includinghardwareresourcecontentions frommulti tenancy multiple vmssharing hardware andtherandomness in vm scheduling .
to obtain accurate results performance testing ona cloud shouldcoverallthe uncertaintyfactors.evenmoreimportantly itshouldcovertheuneven impactsofallthesefactorsproportionalto theirfrequenciesexperiencedonthe clouds.
the second challenge is that cloud services are usually provided to the users as black boxes and do not allow users to control the execution environments.
this black box environment makes it nearly impossible to know what uncertainty factors are covered by a test run.
hence it is also impossible to know exactly when enough runs have been conducted to cover all uncertainty factors.
thethirdchallengecomesfromthecloudusagecostincurred by performance testing.
theoretically executing a test input for months can produce accurate results.
however such long tests can incur a high cost.
to minimize testing costs a good performancetestingmethodologyshouldstop testingimmediatelyafter itdeterminesthat the results are accurate.
inthispaper wepresentanovelperformancetestingmethodology called pt4cloud for iaas clouds.
our primary goal with pt4cloud istoprovidereliablestopconditionstoterminaterepeated testrunsofatestinputtoobtainhighlyaccurateperformanceresult.whileensuringhighaccuracy oursecondgoalistoreducethe numberoftestrunstocutdownonthecostofperformancetesting.
pt4cloud is designed to obtain performance distributions since in manyusecases itisimportanttoknowthebest case worst case and percentiles of the performance in addition to averages .
to determine the number of performance test runs required for accurate result for one test input in black box clouds pt4cloud leveragestheobservationofstatisticalstability whichstatesthat thefrequenciesandaveragesconverge i.e.
becomestable given a large number of samples .
based on this observation if a performance distribution obtained from the test runs is stable this distribution may be deemed accurate.
that is performance test runs can be stopped once the results are statistically stable with the expectation that the results are accurate and all uncertainty factors are properly covered.
moreover astheperformancedistributionsofcloudapplications do not always followknown distributions common parametric statisticalapproaches e.g.
student st test cannotbeusedforstability application to test and a workloadstep execute tests for the app continuously for a time interval i. let the set of perf.
data acquired from these tests be s1.step calculate performance distribution d1 from s1.
step combine s1 and s2 into a new sample set s. calculate performance distribution d2 from s.step execute the app for another time interval i. let the set of perf.
data from these new tests be s2.
yes no report d2 as the perf test resultsstep let s1 s and let d1 d2 step compare d1 and d2 to determine if stable?
figure the workflow of pt4cloud .
determination.therefore pt4cloud employsnon parametricstatisticalapproachesfromlikelihoodtheory .thesestatistical approaches also allow the users to easily specify accuracy objectivesandtradeaccuracyfortestingcost.additionally tohelpusers interpret the performance testing results with more confidence we employ the bootstrap method to generate confidence bands for the resulting performance distributions .
to further reduce the testingcost we alsoexploredtwotest reduction techniques.
weevaluated pt4cloud onthechameleoncloud andaws usingsixbenchmarksrepresentingweb database machinelearning andscientificapplicationsonsixvmconfigurations.theevaluation results show that the performance distributions acquired with pt4cloud always have an accuracy higher than with .
accuracyonaverage whencomparedwiththeperformanceresults obtainedfromextensivebenchmarkexecutionswhilereducingtest runsby62 .moreover ourtestreductiontechniquescanreduce the testrunsby90.
whileretaining an averageaccuracyof .
our results also showed that pt4cloud had significantly higher accuracy than state of the art testing and prediction methodologies from software engineeringandsystemresearch.
the contributionsofthis paper include thecloudperformancetestingmethodology pt4cloud which employsnon parametricstatisticalapproachestoprovidereliable stoppingconditionstoobtainhighlyaccurateperformancedistributions.pt4cloud alsoallowstheusersto specifyintuitiveaccuracy objectives andtrade accuracyfor testingcost.
twotestreductiontechniquesthatcansignificantlyreduce the number oftest runswhileretaining ahigh level of accuracy.
athoroughevaluationof pt4cloud withsixbenchmarkson sixvmconfigurationsonawsandchameleoncloudtoexamine thept4cloud saccuracyandtestreductionefficiency anditsbenefit over the state of the artapproaches.
overview of pt4cloud .
the pt4cloud methodology pt4cloud conductsperformancetestsoncloudapplicationsinmultiple time intervals periods of test runs.
in each time interval time period the application under test is executed with its test inputrepeatedlytoacquire nperformancesamples.then pt4cloud determinesifaddingthesenew nsamplessignificantlychangesthe performancedistributionacquiredfrompreviousintervals.ifthe 189a statistics basedperformance testingmethodologyforcloudapplications esec fse august 26 30 tallinn estonia changeisinsignificant thenthecurrentperformancedistribution isconsideredstableandrepresentativeoftheactualperformanceof theapplicationonthecloud.otherwise moretestrunsforanother time interval are required.
we conduct the test in time intervals insteadofthenumberoftestrunsduetothedifficultytocontrol thecloudexecutionenvironments.morediscussionsonthistime intervalare providedinsection .
.
figure2shows the pt4cloud workflow.
the first step step of pt4cloud istoexecuteanapplicationonthetargetcloudwithauserselectedvmconfiguration i.e.
vmtypesandcount repeatedlyfor atimeinterval i.letthesetofperformancedatacollectedfromthese tests bes1.
based on s1 an initial performance distribution d1can becalculated.particularly d1hererepresentsthenon parametric probabilitydensityfunction pdf of s1andcanbecalculatedusing the kernel densityestimation kde technique .
instep2 theapplicationisagainexecutedforanothertimeintervaliusing the same vm configuration.
let the set of performance datafromthesenewtestrunsbe s2.combining s1ands2 weobtain a new set of performance data s. that is s s1 s2.
using s it is possible to calculate another performance distribution d2.
intuitively d1alwaysrepresentsthecurrentdistribution whereas d2alwaysrepresentsthedistributionwithnewdata.if d1andd2are the same then addingnewdata does not changethe distribution.
in step d1andd2are compared using non parametric statistical approaches to determine if the performance distributions are stable.morespecifically this comparisondeterminestheprobabilitypthatd1andd2are the same distribution.
users can choose anobjectiveprobability pobeforehand.if p po thentheperformance distributions are deemed stable and d2is reported as the performancedistributionofthisapplication.if p po thenmore tests need to be conducted to acquire more stable results.
note that ausercanchooseanyobjectiveprobability.ahigherobjective probability may require more tests but it can also produce moreaccurateperformanceresults.whenreportingperformance distribution d2 pt4cloud alsocomputesconfidencebandswitha user selectedconfidence level cl .
whileaconfidence intervalis for a single point of estimation e.g.
mean a confidence band is for aseries ofestimations e.g.
distribution .
if the test results are deemed unstable more test runs are required.in step thenew s1isset to be sand thenew d1to bed2.
that is s1 sandd1 d2.
thenpt4cloud directs the performancetesttogobacktostep2totestforanothertimeinterval i and repeat the comparison for stability.
this loop repeats until the performance results are stable.
.
coveragecriteriaandtimeintervallength asstatedabove thecoveragecriteriaforcloudperformancetesting shouldincludeallperformanceuncertaintyfactorsandtheirproportional impacts.
however as clouds are provided to the users as black boxes it is impossible to directly determine what factors are coveredinonetestoraseriesoftests.consequently weadopted an indirectapproach to ensure the coveragecriteriaare satisfied.
previousstudiesobservedthatapplicationperformanceonthe cloudroughlyexhibitsperiodic e.g.
dailyorweekly cycles .
theseperiodiccyclesreflectthefactthatthemainfactorsofperformancefluctuation variousresourcecontentionsamongvms caused by multi tenancy have a dependency on time.
motivated execution time sec.
.
.
.
.
.
.05density two weeks d2 one week d1 a perf.
dist.
for canneal execution time sec.
.
.
.
.
.
.
.
.07density two weeks d2 one week d1 b perf.
dist.
for swaptions .
figure testing cannealandswaptions withpt4cloud bythisobservation wechoosetoconducttestsintermsofmultiple time intervals to satisfy the coverage criteria.
in this paper we choose the length ofeach time interval ito be a week.based ona priorstudyandourexperimentalresultsforthispaper aweekis longenoughtoprovidegoodcoverageofuncertaintyfactorsforthe types ofapplicationsandcloud services studiedinthis paper .
notethat aweekisthelongesttimeperiodrequiredforcloud performance testingbased on our observations.
depending on the applicationandthevmconfiguration thesmallestrequiredtime intervallengthmaybelessthanaweek.itisalsounnecessaryto continuously execute tests for each time interval.
we also explored reducing the interval length and test runs per time interval and reportedthe results insection .
.
examples ofapplying pt4cloud to illustrate the complete process of using pt4cloud methodology toconductperformancetesting wepresenttwoexamplesoftesting the performance of cannealandswaptions applications from the parsec benchmarksuite onaws.
figure3agivestheperformancedistribution d1forcannealafter step1 where cannealisexecutedcontinuouslyonavminstance withvmtype t2.mediumforatimeintervallengthofaweek .
figure3aalso gives the performance distribution d2forcanneal after step which is calculated from two time intervals weeks of testruns.
using the stabilitycomparisoninstep3 theprobability thatd1andd2are the same is determined to be .
.
as shown in figure 3a d1is very close to d2.
if the user sets the objective probability poto be then d2is reported as the final testing result.
theshaded areainfigure 3ashows theconfidencebandof d2with99 cl.
figure3bgives the performance distribution d1forswaptions afterstep1 whichincludesexecuting swaptions continuouslyon at2.mediuminstance for a week.
figure 3balso shows the performance distribution d2forswaptions after step which is calculatedfromtwointervals weeksoftestruns.usingthestability comparisoninstep3 theprobabilitythat d1andd2arethesame is determined to be .
.
in figure 3b d1andd2are still close but they are clearly less similar than those of canneal.
if the user s objective probability pois or less then d2is reported as the final testing result.
otherwise the test input has to be executed from more time intervals for more accurateresults.
190esec fse august 26 30 tallinn estonia sen he glenna manns john saunders weiwang lori pollock andmary lou soffa statistics basedstopconditions section2statesthat pt4cloud determineswhethertheperformance test can be stopped bydetermining if two distributions d1andd2 are similar stable .
that is pt4cloud determines if adding another interval of tests significantly changes the performance distribution.
akeyissuehereistoidentifytheproperstatisticalapproachfor thisdistributioncomparison.astatisticalapproachthatisproper for cloud performance testing should satisfy three requirements.
first theapproachshouldbeabletohandlenon typicaldistributions as performance distributions of cloud applications usually donotfollowknowndistributions.second thisapproachshould be able to handle distributionsacquired from experiments asitis practicallyimpossibletomakeahypothesisabouttheexacttheoreticaldistributionforacloudapplication sperformance.third the comparisonresult from thisapproachshouldbe intuitive so that the ordinary user can understand.
the comparison result should also provide a quantitative definition for significant change in distributions to ensure testingresults are accurate.
themostcommonapproachfordistributioncomparisonisgoodness of fit gof statistical tests .
however many gof tests areonlydesignedforspecifictypesofdistribution e.g.
shapiro wilk test for normal distributions or require one distribution to be theoretical instead of experimental e.g.
kolmogorov smirnov test .
the anderson darling test is a generic gof test that can compare two distributions acquired from experiments.
however this test requires critical values that do not yet exist for the nontypicalcloudperformancedistributions.thechi square 2 test is another generic gof test.
to use 2 test one needs to first divide the range of performance data e.g.
execution times acquired fromtestsintobins.unfortunately the 2 testissensitivetothe widths of the bins .
we experimented with 2 test but were unabletofindabinwidththatworkedwellforthediversetypesof distributionsobtainedfrom cloud performance tests.
thedistributioncomparisonapproachthatweeventuallyadopted is kullback leibler kl divergence which can handle any types of distributions .
more specifically kl divergence measures how one distribution diverges from a second distribution.
without loss of generality consider two distributions pandqover random variablex.theequationtocomputehow pdivergesfrom qwith kl divergenceis dkl p q p x logp x q x dx.
the value of kl divergence i.e.
dkl p q ranges from to infinity.
a value of for kl divergence indicates no divergence whereasinfinityindicatestwodistributionsarecompletelydifferent.
however thisinterpretationisnotintuitiveforuserstounderstand the amount of difference divergence .
to help a user interpret kldivergence weemployedmultinomiallikelihood l fromthe likelihoodtheory whichcan be computedas l p q dkl p q intuitively lrepresentstheprobabilitythat pisdifferentfrom q.asourgoalistocomparethesimilaritybetweendistributions d1andd2 it requires considering d1andd2symmetrically.
that is ifd1andd2are similar then d1is not different from d2 andd2is notdifferentfrom d1.therefore wedefinetheprobability pthatd1andd2are similar as p l d1 d2 l d2 d1 as describedin section pis theprobability used in pt4cloud to determine whether the performance distributions obtained from performance tests are stable.
note that to compute the integration in eq we employed numerical integration by partitioning each distributioninto1000strips.forourexperiments using1000strips was sufficient adding more strips changes the probabilities by less than0.
.
notethat kl divergenceiscommonlyusedasanasymmetric metric.weusedthesymmetrickl divergencefollowingitsoriginal definition as we treat d1andd2equally in the comparison.
.
a potential variant of pt4cloud may use asymmetric kl divergence.
while using asymmetric kl divergence does not affect the results ofour experiments its exact impact requires more investigation.
establishingconfidencebands tohelpusersbetterunderstandtheirapplication sperformanceand interprettheperformancetestingresultswithhigherconfidence pt4cloud also presents each final performance distribution with its confidence band.
because the performance distributions of cloud applications do not necessarily follow known distributions we chose to compute point wise confidence bands cb using bootstrap .
point wiseconfidencebandsarecommonlyusedtodescribenonparametricdistributions andbootstrapisastatisticalmethodfor treating non parametricdistributions.
bootstrapisessentiallyaresamplingtechnique.togeneratea cb theoriginalperformancedataset sisresampled.eachresample samples a new data set with s data points from swith replacement and a new probability density function pdf is generated based on the new data set.
repeating the resampling for rtimes allowsustocalculate rpdfs.thenapoint wiseconfidenceband with confidence level cl c can be determined by calculating the probabilitydensityregionthatcontains c oftherbootstrapped pdfs.
figure 3also shows the confidence bands with shaded areas.
bootstrap can produce correct results assuming re sampling on thedataset sbehavessimilarlytowhen sissampledfromthetrue population.thisassumptionisusuallytruewhen siscompleteand raresufficientlylarge .wesetrtobe1000 followingcommon practice .sisdeemedcompleteby pt4cloud .therefore aslong aspt4cloud methodologyisaccurate confidencebandgenerated withsisalsoreliable.
experimentalevaluation this section presents the methodology and findings from evaluatingpt4cloud on two public clouds.
this evaluation seeks to answerthefollowing researchquestions howaccuratearethe performancedistributionsacquiredwith pt4cloud ?
howdoes pt4cloud compare withthe state of the art?
.
experimentalsetup benchmarks we evaluated pt4cloud with a variety of benchmarks thatrepresentwebapplications high performancecomputing hpc applications database db applications and machine 191a statistics basedperformance testingmethodologyforcloudapplications esec fse august 26 30 tallinn estonia table benchmarks andtheir application domains.
benchmark domain origin ft.c hpc npb ep.c hpc npb jpetstore jps web j2ee ycsb db ycsb tpc c db oltpbench in memory analytics ima ml cloudsuite table vm configurationsused forevaluation.
config.
vmcnt x vmtype core cnt memsize chm 4x small sm vm 2gb vm chm 2x medium md vm 4gb vm chm 1x large lg vm 8gb vm aws 4x m5.large m5lg vm 8gb vm aws 2x m5.xlarge m5xlg vm 16gb vm aws 1x m5.2xlarge m52xlg vm 32gb vm learning ml applications to demonstrate that pt4cloud can be appliedtodiversecloudapplications.table 1givesthebenchmarks and their application domains that were used in our evaluation.
more specifically we used jpetstore jps from java platform enterprise edition j2ee ftandepfrom nas parallel benchmarks nbp yahoo!cloudsystembenchmark ycsb withapachecassandradatabase tpc cfromotlpbenchand in memoryanalytics ima fromcloudsuite .forftandep weusedthe npb s c workloads so that the input data can fit in the memory of amediumsizedvm.for ycsb weuseditsworkloadawhichhas reads and50 writes.
publiccloudsandvmconfigurations toshowthat pt4cloud works properly on public clouds we evaluated it on two public clouds chameleon chm and the amazon web services ec2 aws .weusedthreevmconfigurationsoneachcloudthatrepresent use cases with both single and multiple vms.
table 2details thevmconfigurationsusedinourexperiments.notethatawshas alarge selection ofvmtypes inthiswork we chose m5vmsas they are the latest general purpose vms.
except for ft epandima each benchmark was tested on all configurations.
due to insufficient memory ft epandimacould not execute on chameleon s smallvms.intherestofthispaper wecallanexperimentwithone benchmark on one vm configuration as a benchmark configuration .
in total benchmarkconfigurationsare evaluated.
parameters of pt4cloud for this evaluation we chose the objectprobability po fordistributionstabilitycomparisonto be i.e.
expectingtheaccuracyofperformancetestingresultstobe atleast90 .wealsosettheconfidencelevel cl tobe99 forthe confidencebandsgeneration.additionally asstatedinsection we usedatime intervallength ofone weekinthis evaluation.
evaluation methodology and metric for each benchmark configuration weevaluatetheaccuracyofitsperformancedistributionacquiredwith pt4cloud bycomparingthisdistributionwith aground truth performance distribution .we then report the probability i.e.
the multinomial likelihood introduced in section that thept4cloud and ground truth distributions are the same.
for the rest of this paper we simply refer to this probability as accuracy.
toobtainthegroundtruthdistribution weexecutedeachbenchmarkconfigurationforsixweeks inadditiontotheperformancetable the number of intervals weeks w required to obtain stable performance distributions and the accuracy of theperformancedistributionsobtained with pt4cloud .
test run length accuracy chm chm chm aws aws aws ft.c n a 2w 2w 2w 2w 2w ep.c n a 2w 2w 2w 2w 2w jps 2w 2w 3w 2w 3w 2w ycsb 3w 3w 2w 3w 2w 2w tpc c 2w 2w 2w 2w 2w 2w ima n a 2w 2w 2w 2w 2w tests conducted with pt4cloud and calculated the performance distributionsusing the performance data from thesesix weeks.
open data our data andsourcecode are available at .
perf.dist.acquiredwith pt4cloud figure4presents the performance distributions acquired with pt4cloud for eight benchmarks configurations.
due tospace limitation only fourbenchmarks on two vmconfigurations chm andaws 2 areshowninfigure .therestofthebenchmarksand configurations have similar results.
figure 4shows that the performance distributions for these benchmarks gradually became stable over time.
that is the changes in the distributions become smaller aftereachnewinterval week.itcanalsobeseenfromfigure 4that thedistributionsfromthefirstinterval weekcanbedramatically differentfromthefinalstabledistributions suggestingthenecessity of a performance testing methodology like pt4cloud .
the distributionsinfigure 4alsodonotalwaysfollowknowndistributionsand varywithbenchmarkconfigurations provingtheneedforusing non parametricstatisticalmethodsas employedby pt4cloud .
table3provides the numbers of intervals of test runs that were conducted to obtain stable performance distributions with more than90 stabilityprobability i.e.
pois90 forallbenchmarkconfigurations.theperformancedistributionsofthemajorityofthe benchmarkswerestablewithintwoweeks.oncertainvmconfigurations thedistributionsofdbandwebapplicationsrequiredthree weeks to become stable as i o network and disk performance has higher fluctuationsthancpuandmemory performance .
.
accuracyof pt4cloud figure5compares the performance distributions obtained with pt4cloud andtheground truthperformancedistributions.dueto space limitation only four benchmarks on two vm configurations areshowninfigure .asthefigureshows thegroundtruthperformanceisveryclosetotheperformanceobtainedfrom pt4cloud .
table3alsogivestheaccuracyof pt4cloud sperformancedistributionswhencomparedwithgroundtruthdistributionsforall benchmark configurations.
as shown in table the accuracy of pt4cloud sperformancedistributionsisalwayshigherthan90 .
theaverageaccuracyis95.
.theseresultsindicatethat pt4cloud methodology is highly accurate for cloud performance testing.
moreover pt4cloud methodology executed considerably fewer teststhanthegroundtruthtests.figure 8giveshowmanyfewer 192esec fse august 26 30 tallinn estonia sen he glenna manns john saunders weiwang lori pollock andmary lou soffa execution time sec.
.
.
.
.
.
.
.
.035density week1 week1 a ft.con 2xmd chm .
successful requests .
.
.
.
.000008density week1 week1 b jpson 2xmd chm .
overall throughput .
.
.
.
.
.
.
.00035density week1 week1 week1 c ycsbon 2xmd chm .
execution time ms. .
.
.
.
.
.
.0000030density week1 week1 d imaon 2xmd chm .
execution time sec.
.
.
.
.
.
.
.030density week1 week1 e ft.con 2xm5xlg aws .
successful requests .
.
.
.
.000020density week1 week1 week1 f jpson 2xm5xlg aws .
overall throughput .
.
.
.
.
.0005density week1 week1 g ycsbon 2xm5xlg aws .
execution time ms. .
.
.
.
.000020density week1 week1 h imaon 2xm5xlg aws .
figure performancedistributionsacquired with pt4cloud forfour benchmarks on configurationsofchm and aws .
execution time sec.
.
.
.
.
.
.
.
.
.040density perf.
dist.
grd truth a ft.con 2xmd chm .
successful requests .
.
.
.
.000008density perf.
dist.
grd truth b jpson 2xmd chm .
overall throughput .
.
.
.
.
.0005density perf.
dist.
grd truth c ycsbon 2xmd chm .
execution time ms. .
.
.
.
.
.
.0000030density perf.
dist.
grd truth d imaon 2xmd chm .
execution time sec.
.
.
.
.
.
.
.
.035density perf.
dist.
grd truth e ft.con 2xm5xlg aws .
successful requests .
.
.
.
.
.
.
.0000175density perf.
dist.
grd truth f jpson 2xm5xlg aws .
overall throughput .
.
.
.
.
.
.0006density perf.
dist.
grd truth g ycsbon 2xm5xlg aws .
execution time ms. .
.
.
.
.000020density perf.
dist.
grd truth h imaon 2xm5xlg aws .
figure5 performancedistributionsobtainedwith pt4cloud andgroundtruthperformancedistributionsforfourbenchmarks on twovm configurationson chmandaws.
shaded areasare confidencebands.
test runs pt4cloud executed compared to the ground truth tests.
onaverage pt4cloud executed62 fewer test runs.
figure5alsoshowstheconfidencebands cb forthefinaldistributionswith99 cl.with pobeing90 theprobabilitythatthe true distribution falling within the cb is expected to be roughly .notethegroundtruthdistributionsdonotnecessarilyfallwithintheconfidencebandswiththissameprobability as theyarestillexperimental i.e.
nottrue distributions.nonetheless the majority of the ground truth distributions including those not showninfigure fall within the confidence bands.
.
comparisonwith state of the art todemonstratetheimportanceofanewmethodologylike pt4cloud for performance testing on the cloud we compared pt4cloud with threeperformanceteststoppingandperformancepredictionmethodologies from software engineeringandcomputer systemresearch.
193a statistics basedperformance testingmethodologyforcloudapplications esec fse august 26 30 tallinn estonia 100ft.c chm ft.c chm ep.c chm ep.c chm jps chm jps chm jps chm ycsb chm ycsb chm ycsb chm tpc c chm tpc c chm tpc c chm ima chm ima chm ft.c aws ft.c aws ft.c aws ep.c aws ep.c aws ep.c aws jps aws jps aws jps aws ycsb aws ycsb aws ycsb aws tpc c aws tpc c aws tpc c aws ima aws ima aws ima aws averageaccuracy pt4cloud w. week interval sota1 sota2 figure accuracyof pt4cloud whencompared with twoother state of the artperformancetestingmethodologies.
the first methodology sota1 stops the performance test by detecting the repetitiveness of the performance data obtained from test runs .
more specifically it picks a short period of data from the whole performance testing results and compares this period with other periods in the whole data set to determine if this period of data is repeated.
the methodology repeats this process for times to estimate the percentage of the performance results that arerepeated i.e.
repetitiveness .iftherepetitivenessremainsunchanged for a user defined time span then the test can be stopped.
wereproducedthismethodologyusingthe18setsofparameters given in that paper and report the accuracy of the performance distributions obtained with this methodology in figure .
for each benchmarkconfiguration onlythebestaccuracyofthe18setsof parameters is reported.
for all of the benchmark configurations thismethodologystoppedthetestsrelativeearly alwaysinless than minutes on aws and minutes on chameleon.
due to spacelimitation we cannotprovidedetailedstop times.
asfigure 6shows thisfirstmethodologyhasanaverageaccuracy of only .
due to the early stop times.
many benchmark configurationsexperiencedlowerthan50 accuracy.weobserve thatthis low accuracy ismainlybecause this methodologyisonly designed to detect performance changes due to an application s internalfactorsinsteadofexternalfactors suchascloudperformance uncertaintyfactors.forinternalfactors onlymeasuringwhether adatumisrepeatedornotmaybeenough.however forexternal factors how frequent each datum being repeated is also important.
additionally wefoundthattheaccuracyofthismethodologyrelied heavily on its parameters i.e.
the length of the period and the user defined time span.
nonetheless it is unclear how to select theseparametersto maximizeaccuracy.
thesecondmethodologywecompared sota2 isaclassicaltechniqueforperformanceanalysis .inthismethodology the performancetestis stoppediftheconfidence interval ci forcertainstatistics narrowstoadesiredwidth .aswetestedfor performancedistribution weextendedthisideatostoprunningthe tests ifthe cifor eachpercentileofthe performancedropped within of the observed percentile performance.
this is chosen toreflect marginof error similar toour accuracy goal i.e.
po .
with this extension we applied this methodology to our benchmarks.
the cis were generated using the bootstrap approach.
this methodology caused the performance testing to stopatvariabletimes from2hourstoeven7weeks.theextralong tests were caused by performance outliers which made the cis ofcertainfringepercentileshardertoconverge.theaccuracyofthe performancedistributions obtainedwiththismethodology isalso reportedinfigure .asfigure 6shows theaverageaccuracyfor this methodology is only .
with the lowest accuracy being .
ep chm3 .
this low accuracy shows that ci width is a poor stopping criteria for cloud performance testing.
that cis failed to determine requiredsamplesizesisalsoobservedinothersciencefields .
the main issue is that for non parametric distributions cis are reliable only when data is complete .
that is for cloud performance testing results the cis are only reliable when the results coversalluncertaintyfactors.anarrowcisimplymeansthatthere arelargeamountofperformancedataobtainedundertheobserved uncertainty factors not that allfactors are observedbythe tests.
wecomparedourtechniquewith a thirdmethodology which wasaperformancedistributionpredictiontechniqueforcloudusing monte carlo simulation .
this methodology can be applied to applications with multiple steps assuming the min and max performance of each step are known.
two of our benchmarks ycsbandft.c have two steps.
thus we applied this prediction technique on the benchmark configurations involving these two benchmarks using the min and max performance obtained from the ground truth.
for ft.c the distributions predicted by this techniquehaveaccuraciesof10 onthefivevm configurations from chm to aws .
for ycsb the accuracies are42 forchm 1toaws .theaverage accuracy for the configurations is .
.
the low prediction accuracy is primarily because this technique assumes each step hasauniformperformancedistribution whichisnottrueonreal publicclouds.infact ifthestableperformancedistributionforeach stepobtainedwith pt4cloud isusedforthismethod theaverage prediction accuracy of this methodology can be increased to .
with one configuration s accuracy increased to .
this increase inaccuracyshowsthatnotonlydoes pt4cloud benefitperformance testing it can also benefit performance prediction techniques by providingreliabletrainingdataset.moreover pt4cloud canalso benefit performance prediction techniques by providing reliable ground truthto evaluate the goodness of the prediction results.
reducing the numberoftests as performance tests on clouds incur cloud usage cost a good performancetestingmethodologyshouldalsostrivetominimize the testing costs.
in this section we explore the approaches to 194esec fse august 26 30 tallinn estonia sen he glenna manns john saunders weiwang lori pollock andmary lou soffa application to test and a workloadstep execute tests for the app continuously for a short interval i. let the set of perf.
data acquired from these tests be s1.step calculate performance distribution d1 from s1.
step combine s1 and s2 into a new sample set s. calculate performance distribution d2 from s.step execute the app for another short interval i. let the set of perf.
data from these new tests be s2.
no report d2 and validation if any as the perf test results step optional validation conduct additional tests of size s and calculate performance distribution d3.
validation is successful validation failsyes and choose to validate d2 yes and choose not to validatestep compare d1 and d2 to determine if stable?
step compare d2 and d3 to validate d2.step increase the length of interval i. figure the workflow of pt4cloud when using intervals shorter than oneweek.
reduce the length of the intervals and test runs per interval for pt4cloud whichinturncan reduce the test costs.
.
reducinginterval length in figure several benchmarks performance distributions obtained after the first week were already very similar to the final performancedistributions e.g.
ft.candima suggestingthatsome cloud applications can use intervals shorter than one week.
indeed the proper interval length depends on the application and thevmconfigurationtobetested.forinstance acpu intensive application is less likely to be affected by the cloud performance uncertaintyfactorsfromi ocontentions.therefore theinterval forthisapplicationcanpotentiallybesmallerthanaweek asthe performancetestonlyneedstocoverasmallernumberofperformanceuncertaintyfactors.however forapplicationswithheavy usage of disk and network the intervals must be longer to cover the fluctuationsdueto alluncertainty factors.
tohelpusersconduct testswithshorterintervals we modified the workflow of pt4cloud to allow users to search for proper interval lengthswhileconducting tests.
figure 7depicts a variantof pt4cloud designed for short intervals.
in this newvariant the user firstconductstestsfortwoshortintervals.thenourdistribution comparison technique is used to determine if the performance distributionisstableafterthese twoshort intervals.ifthe distribution is stable then it can be reported as the final results.
otherwise theintervallengthisincreased step5 andanewroundoftests startsfromthebeginning step1 withthenewlongerinterval.the testing interval gradually increases if the results do not stabilize.
whentheintervallengthreachesoneweek thestandard pt4cloud methodology isemployed.
in this new variant we choose to increase the interval length instead of conducting tests for more intervals because our primary goal is to provide highly accurate testing results.
therefore weaggressivelyassumethatfailuretostabilizeisduetoashortinterval ratherthanthat insufficientintervals are tested.
a disadvantage of shorter intervals is that the resulting distributions are more likelyto be inaccurate than those acquired from longerintervals asthe impactsofcloud performanceuncertainty factors are more likely to remain unchanged within a short period than a longer period.
for example an application s performance mayappear tobe stable within two hours although itsactual performance over a week may be quite different.
consequently in pt4cloud forshortintervals wealsoincludedanoptionalvalidation step step of figure .
more specifically if the performance distribution is stable with two short intervals of tests then an additionaltwointervals withsameintervallength oftestsareexecuted.
the performance distribution from the additional test runs is compared with the original distribution.
if both distributions are the samewitha probabilityhigherthantheobjectiveprobability po then the testing result is validated and can be reported.
otherwise the intervallength has to be increased.
based on theperformance resultsweobtainedfromtheexperimentalevaluation werecommend the user to take the validation step if the testing result is stablewithin aweek i.e.
the intervallengthisless than3.5days .
.
reducingtestrunswith sampling to further reduce the testing cost especially for the applications requiring1 weekintervals wealsoexploredanhourlysampling technique.sofar testrunshavebeenexecutingconsecutivelyfor eachintervalwiththegoaltocovereverychangeinthebehavior ofthecloud performance uncertainty factors.similartoother statisticalpractices thisconsecutiveexecutionoftestscanbereplaced withsamplingtoreducecostwhileprovidingsimilaraccuracy.here we employed an hour based sampling technique.
more specifically foreach hour within an interval tests are only executed fora portionofthishour.wesampledourperformancetestingdataobtained fortheevaluationinsection 5withvariousportionsizesandfound that our benchmarks can achieve more than accuracy with portion sizes from to .
on average a portion size of i.e.
samplingonehalfofanhour provideshighlyaccurateresultsfor the majority ofour benchmarks.
.
evaluationoftestreductiontechniques we conducted additional evaluations to examine the effectiveness andaccuracyofourtwotestreductiontechniques.byeffectiveness evaluation wemeanevaluatingwhetherourtestreductiontechniquescaneffectivelyreducethenumberoftestruns.weextracted smaller intervals of performance data from the tests conducted for the evaluation in section 5and performed sampling on these data.
for interval reduction we explored intervals of hour hours day days days etc.
up to days.
for hourly sampling we used aportionsize of1 i.e.
sampling one halfof an hour .
effectivenessevaluation table4showsthereducedinterval lengths to achieve at least stable probability for all benchmark configurationsusedinsection .asthetableshows theinterval lengths ranged from hours to one week.
ml and hpc benchmarksthatarecpu and ormemory intensivearemorelikelyto use intervals lessthan a week.
however db andweb applications thatarei o intensiveoftenrequireanintervalofaweekorclose 195a statistics basedperformance testingmethodologyforcloudapplications esec fse august 26 30 tallinn estonia table4 reducedintervallengthsandthenumberofintervalsittakestostabilize forallbenchmarkconfigurationsusingthe twotestreductiontechniques d standsforday .
reduced intervallength ofintervals w o hourlysampling reduced intervallength ofintervals w. hourlysampling config.
chm chm chm aws aws aws chm chm chm aws aws aws ft.c n a 3d 3d 12hrs 12hrs 12hrs n a 3d 3d 12hrs 12hrs 12hrs ep.c n a 4d 4d 4d 1d 12hrs n a 4d 4d 4d 12hrs 12hrs jps 12hrs 12hrs 1w 1d 1w 3d 12hrs 12hrs 1w 1d 1w 4d ycsb 4d 1w 5d 1w 2d 4d 4d 1w 5d 1w 2d 3d tpc c 3d 1d 4d 12hrs 12hrs 12hrs 3d 1d 4d 12hrs 12hrs 12hrs ima n a 3d 4d 12hrs 12hrs 1d n a 3d 4d 2d 2d 4d 100ft.c chm ft.c chm ep.c chm ep.c chm jps chm jps chm jps chm ycsb chm ycsb chm ycsb chm tpcc chm tpcc chm tpcc chm ima chm ima chm ft.c aws ft.c aws ft.c aws ep.c aws ep.c aws ep.c aws jps aws jps aws jps aws ycsb aws ycsb aws ycsb aws tpcc aws tpcc aws tpcc aws ima aws ima aws ima aws reduced t ests standard pt4cloud with week interval reduced interval reduced interval sampling figure numberoftests reduced by pt4cloud andour testreductiontechniques compared to the groundtruth tests.
to a week.
prior studies also observed that the performance of i obound applications depended strongly on contention .
moreover more benchmark configurations can use short intervals on awsthanonchameleon whichreflectsthefactthatawsprovides betterresourcecontentionmanagementandvm placement.
figure8shows the number of tests reduced by our test reduction techniques.
on average the interval reduction can reduce test counts by .
of the tests required for ground truth tests.
applying both test reduction techniques can reduce the test counts by90.
.thisreductionintestcountcanreducebothvmusage costs and network cost proportionally for popular public clouds includingaws microsoftazure andgoogle compute engine.
accuracy evaluation figure9gives the accuracy of the performance distributions acquired with pt4cloud methodology after applying the two test reduction techniques.
as the figure shows our test reduction techniques can reduce test count without significantly scarifying accuracy.
even with the test reduction the performancedistributionsacquiredwith pt4cloud canstillreach upto99 accuracy jpsonaws .onaverage theperformance distributions acquired with pt4cloud after applying interval reduction is .
.
the average accuracy after applying both interval reductionand hourly sampling is91 .
it is also worth notingthat when the interval length is longer than days sampling half an hour has littlenegative impact onaccuracy.
the lowest accuracy is .
which is for imaon aws when using both reduced intervals and sampling.
for this benchmark configuration therewereafewperformanceoutliers.asthetests were only conducted for days with sampling these outliers were observedatadifferentfrequencyduringthe pt4cloud teststhan the ground truth.
as kl divergence is sensitive to outliers this differencereducedtheaccuracy eventhoughthemajorityofthe pt4cloud andground truthdistributionswere similar.
threats to validity execution environment changes .
the performance results obtainedwith pt4cloud areonlyvalidwhentheexecutionenvironment includingtheunderlyinghardwareandmulti tenancybehavior remainsthesame.whentheexecutionenvironmentischanged newperformance tests should be conducted.
furthermore while our results indicate that the performance of cloudapplicationshasweeklyordailycycles events thathappen only a few times a year month may still affect overall performance distributions.
an example of such yearly events may be holiday shopping where all websites running in the clouds exhibit high resourcedemands.ingeneral webelievetheseeventsdonotsignificantlychangeoverallperformancedistributionsastheyhappen infrequently.additionally cloudserviceproviderscanimplement resourcemanagementpoliciestolimittheimpactoftheseevents.
ourexperimentsonawsactuallyoverlappedwithamazon2018 primedaywhenamazon sownwebsiteexperiencederrors .
however weobservednearlynoimpactontheperformanceofour benchmarks.
nonetheless the potential impact of these yearly and monthly eventsshould be acknowledged.
otherapplications workloadsandvmconfigurations .although we strive to evaluate pt4cloud with all types of cloud applications wecanonlyevaluatealimitednumberofbenchmarks andvmconfigurations duetocost.othercloudapplications workloads vmconfigurationsandcloudsmayexhibitdifferentaccuracy withpt4cloud orrequire differentintervallengths.
other cloud uncertainty factors .
here we focused on the cloudperformanceuncertaintyfactorscausedbymulti tenancyand vm scheduling.
otherfactors such asdatacenterlocation bursty vm types and hardware variation may also affect performance.
the vms used for our evaluations are not affected by these factors.
however these factors do exist for other cloud services and vm 196esec fse august 26 30 tallinn estonia sen he glenna manns john saunders weiwang lori pollock andmary lou soffa 100ft.c chm ft.c chm ep.c chm ep.c chm jps chm jps chm jps chm ycsb chm ycsb chm ycsb chm tpc c chm tpc c chm tpc c chm ima chm ima chm ft.c aws ft.c aws ft.c aws ep.c aws ep.c aws ep.c aws jps aws jps aws jps aws ycsb aws ycsb aws ycsb aws tpc c aws tpc c aws tpc c aws ima aws ima aws ima aws averageaccuracy standard pt4cloud with week interval reduced interval reduced interval sampling figure accuracyoftestreductiontechniques.
types.although pt4cloud methodologyisgenericenoughtohandle thesefactors more tests maybe requiredto cover thesefactors.
performanceresultsotherthandistributions .insomecases usersmayonlyneedtoknowthemeanoraparticularpercentile of the performance of their applications.
although an accurate performancedistributioncanprovideaccuratemeanandpercentiles theremaybeacheapertestingmethodologytoobtainthemdirectly.
however asthemeansandpercentilesarestillfromthenon typical performance distributions of cloud applications common parametricstatisticaltoolsstillcannotbeapplied.wearecurrentlyworking onaseparate testingmethodologyformeans andpercentilesthat can further reduce test costs.
related work a large body of work in performance testing focused on the generationandprioritizationoftestinputs .burnimetal.
designed an input generator that can produce worst performing testinputsforanyinputsizes .zhangetal.proposedamethodologytogeneratetestinputsgivenaninputsizeanddiversitybased onsymbolicexecution .chenetal.extendedthisideatoemploy probabilistic symbolic execution to generate test inputs with frequenciessothataperformancedistributioncanbeconstructed .
perflearner can help test input generation by finding important parameters from bug reports .
perfranker is a test input prioritization mechanism for performance regression testing .
there is also work on detecting performance bugs by analyzing source code applicationbehaviorandtraces .
these test input generation prioritization and performance debugging studies are orthogonal to our work as our work focuses on determiningthe accurateperformance ofatest input.
severalstudiesdocumentedtheimportanceofrepeatedlyexecutingatestinputforperformancetesting .however these studiesdidnotprovidemeanstoproperlydeterminethenumberof testrunsrequiredtogetreliableperformancewithlowtestingcost.
they either used extensively long test runs or an arbitrary numberofruns .themostrelatedworkonperformancetest stoppingconditionisprobablythestudydonebyalghmadietal.
and the ci based methodology .
however our comparison experiments in section .4showed that these methodologies could notprovideaccurateperformancetestingresultsonthe cloud.guo et al.
also employed bootstrap in their work for building performance models andcross validation instead oftesting .
thereisalsoresearchonpredictingcloudapplication sperformance.
the mostly related prediction work is proposed by luke etal.topredicttheperformancedistributionsforapplicationswith multiple steps .
our comparison experiments in section .
showedthatthisworkcouldprovideaccuratepredictions.hsuetal.
investigatedpredictingthebestvmconfigurationusingbayesian optimizationwithlow levelmetrics .however thisworkdid not aim at predicting the actual performance of cloud applications.
wang et al.
predicted the performance of cpu and memoryintensive applications .
paris is a model that could predict the performance of a cloud application on a vm configuration .
however parismayhaveupto50 rmse error .gambietal.
employedthekrigingmethodtopredicttheaverageperformance of a cloud application under different workloads and vms .
it is worth noting that the existence of predictive work cannot eliminate the necessity of measurement based performance testing approaches such as pt4cloud .
as shown in section .4and inpriorwork measurement basedperformancetestingisstillrequired to provide complete training set and accurate ground truths for predictive approaches .
conclusion performance testing onclouds to obtain accurate testing results is extremelychallengingduetocloudperformanceuncertainty the inabilitytocontrolcloudexecutionenvironmentsandthetesting cost.inthis paper we present acloud performancetestingmethodology pt4cloud .byemployingnon parametricstatisticaltoolsof likelihoodtheoryandbootstrapping pt4cloud canprovidereliable stopping conditions to obtain highly accurate performance results asperformancedistributionswithconfidencebands.theevaluation resultsshow that withtwotestreductiontechniques pt4cloud s performanceresultsareonaverage91 similarwithtestingresults from extensive test runswith90.
fewertests comparedtothese extensive runs.
this is considerably better than the state of the art as evidencedinour empirical comparisons.
acknowledgement thisworkwassupportedbythenationalsciencefoundationunder grants ccf and ccf .
the views and conclusions containedhereinarethoseoftheauthorsandshouldnotbeinterpretedasnecessarilyrepresentingtheofficialpoliciesorendorsements either expressed or implied of nsf.
the authors would like tothanktheanonymousreviewersfortheirinsightfulcomments.
wewouldalsoliketothankjamesskripchuk tianyiliuandxin niefor theirvaluable inputs.
197a statistics basedperformance testingmethodologyforcloudapplications esec fse august 26 30 tallinn estonia