towards automatically generating descriptive names for unit tests benwen zhang university of delaware newark de usa benwen udel.eduemily hill drew university madison nj usa emhill drew.edujames clause university of delaware newark de usa clause udel.edu abstract during maintenance developers often need to understand the purpose of a test.
one of the most potentially useful sources of information for understanding a test is its name.
ideally test names are descriptive in that they accurately summarize both the scenario and the expected outcome of the test.
despite the bene ts of being descriptive test names often fall short of this goal.
in this paper we present a new approach for automatically generating descriptive names for existing test bodies.
using a combination of natural language program analysis and text generation the technique creates names that summarize the test s scenario and the expected outcome.
the results of our evaluation show that compared to alternative approaches the names generated by our technique are signi cantly more similar to human generated names and are nearly always preferred by developers the names generated by our technique are preferred over or are equivalent to the original test names in of cases and our technique is several orders of magnitude faster than manually writing test names.
ccs concepts software and its engineering !software testing and debugging software maintenance tools keywords unit testing descriptive names maintenance .
introduction one of the most di cult aspects of software maintenance is comprehension understanding the software that is being modi ed.
in fact the amount of time needed by developers to locate and understand code is frequently greater than the amount of time that they spend making modi cations .
in the context of testing one of the most frequent comprehension tasks is understanding the purpose of a test.
for example when a test fails it is necessary to understandthe purpose of the test as a rst step towards identifying the cause of the failure.
in addition knowing the purpose of a test is necessary to decide whether the test should be left alone modi ed or removed in response to changes in the application under test and whether the test should be included in a regression test suite.
one of the most potentially useful sources of information for understanding a test is its name.
ideally test names aredescriptive in that they accurately summarize both the scenario and the expected outcome of the test .
if a test s name is descriptive developers no longer have to read through its body to understand its purpose.
in addition descriptive names make it easier to tell if some functionality is not being tested if a behavior is not mentioned in the name of a test then the behavior is not being tested help prevent tests that are too large or contain unrelated assertions if a test cannot be summarized it likely should be split into multiple tests and serve as documentation for the class under test a class s supported functionality can be identi ed by reading the names of its tests.
despite their numerous bene ts not all tests have descriptive names.
because naming is di cult and there is no immediate downside developers often write poor names.
for example developers may create generic test names e.g.
test1 test2 etc.
or test names that contain little information e.g.
testadd testsubtract etc.
.
in addition a test s name can become erroneous when it is out of sync with the test s body.
for instance a developer may modify a test s body but fail to make the corresponding changes to the test s name.
such erroneous names no longer accurately summarize the test s body.
in practice erroneous names can be more harmful than poor names.
because poor test names are often easily identi able developers are unlikely to consider them useful sources information.
conversely erroneous names often appear plausible and can easily lead developers into making incorrect assumptions.
in this paper we present a new natural language program analysis nlpa based technique that can help simplify the comprehension task of understanding the purposes of tests.
the technique accomplishes this by automatically generating descriptive names for unit tests.
at a high level the technique statically analyzes tests to identify the parts of the body that correspond to the test s scenario and expected outcome.
to identify these parts the technique uses both syntactic knowledge of how unit tests are commonly structured and the semantic knowledge captured in the names of entities used by the test e.g.
variables parameters methods etc.
.
then using text generation the technique creates a permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
descriptive name that summarizes both the scenario and the expected outcome.
to evaluate our technique we implemented it in a prototype tool nameassist that automatically generates descriptive test names for unit tests written using the junit framework.
using the prototype we conducted an empirical evaluation of the technique.
the results of our evaluation are promising and show that the technique is feasible useful and e ective.
speci cally this work makes the following contributions a novel nlpa based technique for automatically generating descriptive test names for unit tests nameassist a prototype implementation of the technique that automatically generates descriptive names for unit tests written using the junit framework an empirical evaluation of the technique that demonstrates that compared to alternative approaches names generated by nameassist are signi cantly more similar to human generated names and are nearly always preferred by developers names generated by nameassist are preferred over or are judged equivalent to the original test names in of cases and the runtime costs of nameassist are several orders of magnitude less than the amount of time needed by developers to manually generate descriptive names.
.
motivation to motivate our technique we performed an exploratory study of the names of 423real world test cases from over 000publicly available java projects hosted on sourceforge.
selecting such a large number of varied projects helps address the potential biases associated with small samples.
because these projects include applications of various sizes ages and coding styles they are likely to contain representatives of various testing styles which helps improve the generalizability of our results.
to identify the test names we initially considered all 516methods de ned in these projects.
then based on unit testing conventions we ltered out methods whose name does not start with test except if they have an test annotation methods that are declared inside of anonymous classes and methods that accept parameters.
to analyze the test names we used a custom parser for tagging test names with their parts of test pots .
the pots identi ed by the parser include the action action object action modi ers predicate and predicate object .
in general the action action object and action modi ers compose the action phrase which should describe the scenario while the predicate and predicate object compose the predicate phrase which should describe the expected outcome.
for example in the test name testsettingheightthatistoosmallleavesheightunchanged the action phrase is comprised of the action setting the action object height and the action modi ers that is too small .
the predicate phrase is comprised of the predicate leaves unchanged and the predicate object height .
using the pots assigned by the parser we consider a test name to contain a full action phrase if the name has an action that is some variation of get or set e.g.
getting gets etc.
and an action object or an action that is not some variation of get or set e.g.
drawing lock send etc.
with or without an action object.
we consider a test name to contain a full predicate phrase if the name has both a predicate and a predicate object.
then basedon whether a test name contains a full action phrase and a full predicate phrase we classify the test name as follows vacuous names are test names that contain neither a full action phrase nor a full predicate phrase partial names are test names that contain either a full action phrase or a full predicate phrase but not both and complete names are test names that contain both a full action phrase and a full predicate phrase.
in our set of test names we found that are vacuous 674out of are partial out of and only are complete 692out of213 .
as this data shows partial names are the most common.
although this may seem promising in the majority of these cases the test name contains only the name of the method under test.
for example we found 724tests named testequals and1 087tests named testserialization .
while including the name of the method under test is better than nothing it is unlikely that such names will be su cient to help developers understand the purposes of the tests.
vacuous names are the second most common.
in general these test names contain the word test optionally followed by a number.
for example we found tests named test 514tests named test1 342tests named test2 etc.
clearly these names are useless for understanding the purposes of the tests.
finally complete names are the least common.
this set includes test names such as testgetselectorthrowsclasscastexception andtestgetadapterdoesnotacceptnullargument that appear to explain both the scenario and the expected outcome.
the results of this study motivate our technique by showing that a majority of existing test names are either vacuous or partial.
because such names are not complete they do not contain the information necessary for supporting common maintenance tasks such as understanding the purposes of tests.
our technique addresses this issue by analyzing test bodies to automatically generate descriptive names that summarize both the test s scenario and its expected outcome.
.
generating descriptive names figure shows a high level overview of our approach.
as the gure shows the technique requires two inputs a test body and the application under test.
it creates corresponding descriptive test names using two phases the analysis phase and the text generation phase.
the rst phase analysis involves statically analyzing the test body and application under test to identify the information that should be summarized in the descriptive name.
based on the results of our motivating study see section we found that there are three primary pieces of information that are included in descriptive names the action and scenario under test which together comprise the test s scenario and the expected outcome.
the action is the focus of the test.
it is usually an invocation of one of the class under test s instance methods but it could also be an object allocation e.g.
creating an object with newor a static factory method .
the expected outcome is the part of the test that checks whether the result of performing the action matches the tester s expectation.
in a unit test this is usually one or more assertions e.g.
assertequals asserttrue etc.
.
finally the scenario under test is the part of a test that constructs the environment in which the action should be performed.
in general the scenario under test is the largest part of the test and it frequently contains too much information to include in 626phase analysisphase text generationdescriptivetest nameexpected outcomescenario under testactiontest bodyapplicationunder testfigure overview of our approach for generating descriptive test names.
public void test .
servlet new barcodeservlet .
params.put height .
params.put width .
params.put resolution .
req.setparameters params .
servlet.doget req res .
barcode barcode servlet.getbarcode .
assertequals barcode.getresolution a testdogetsmallfulltestdogetresolutionis72whenparamsresolutionis72andsettingparametersmediumtestdogetresolutionis72 b figure example junit test case a and automatically generated descriptive names b .
the test name.
to prioritize the expressions included in the scenario under test the technique uses a new data structure called the action dependency graph which takes into account both data dependencies among the elements of the scenario under test but also semantic connections between the elements and the expected outcome.
the second phase text generation takes as input the information identi ed by the rst phase.
it then organizes the action expected outcome and scenario under test and translates them into a descriptive test name.
because there is a wide variety in what information testers want to include in their test s names the text generation phase uses a template based approach that allows testers to customize the level of information included in the descriptive names.
we provide three built in templates small which generates names that contain only the test s action medium which generates names that contain the test s action and expected outcome and full which generates names that contain the test s action expected outcome and scenario under test.
given a test name template the next step is to translate the information required by the template into natural language.
because the action expected outcome and scenario under test are usually a small number of expression types e.g.
method invocations we developed a rule based approach for converting them into english phrases that follow both java identi er restrictions and common test naming conventions.
finally the individual phrases are concatenated together by adding necessary conjunctives and placed inside the template.
.
concrete example as a concrete example of how our technique generates test names consider the test shown in figure 2a and assume that a developer wants to replace the test s existing name with a more descriptive version.
for this example the analysis phase of the technique identi es the call to doget at line as the test s action the call to assertequals at line thatcompares the resolution of barcode to 72as the expected outcome and the creation of a new barcodeservlet at line the calls to putat lines and the call to setparameters at line as the scenario under test with the calls to setparameters at line and putat line being the most important parts of the scenario under test.
depending on the template that is used the text generation phase generates one of the three descriptive names shown in figure 2b.
if the small template is used the action is straightforwardly translated into the phrase doget .
if the medium template is used the expected outcome is translated into resolutionis72 and is appended to the small name and if the full template is used the most important elements of the scenario under test are translated into paramsresolutionis72andsettingparameters and are appended to the medium name using when as a conjunction.
.
phase analysis the goal of the analysis phase is to identify the action expected outcome and scenario under test contained in a test.
the remainder of this section describes how the technique uses various forms of static analysis to identify these pieces of information.
.
.
identifying the action the rst step towards identifying the action is to identify the class under test.
in most cases the class under test can be inferred by considering the name of the class that contains the test.
for example tests inside a class called testfoo orfootest usually test methods of class foo.
if this naming convention is followed the class under test can be easily identi ed by stripping the leading or trailing test from a test class s name.
unfortunately identifying the class under test is not always this simple.
we observed a number of counter examples in real test suites.
to account for cases where the standard test class naming convention is not followed the analysis phase uses a rulebased system for determining the class under test.
the technique considers the following rules in order until it identi es the class under test if a class with the name of the test s containing class minus test exists and has one of its methods invoked by the test it is considered the class under test if the test contains a single constructor invocation the class whose constructor is called is considered the class under test if the test contains a single factory method and a class with the same name as the factory class minus factory exists it is considered the class under test if the test invokes a non getter method before calling an assertion method the non getter method s declaring class is considered the class under test if the test invokes a method before calling an assertion method the method s declaring class is considered the class under test if the test invokes a method as part of calling an assertion method the method s declaring class is considered the class under test and if no other rule applies object is considered 627the class under test.
for example the class containing the unit test shown in figure is called barcodeservlettest that satis es the rst rule and therefore barcodeservlet is chosen as the class under test.
identifying the class under test is a necessary prerequisite for identifying a test s action because the action is an invocation of a method declared by the class under test or the creation of an instance of the class under test via newor a factory method .
because we have already identi ed the class under test in the rst step in the second step we only need to consider expressions that are related to the class under test as potential actions.
speci cally the potential actions include invocations of methods declared by the class under test that occur before or as part of the assertion and instantiations of the class under test that occur before the assertion.
for the example shown in figure the set of potential actions contains the instantiation of barcodeservlet at line the call to doget at line and the call to getbarcode at line .
again we use a rule based approach to choose the action from the set of potential actions.
these rules include if the set of potential actions contains a single element either method invocation or object instantiation it is chosen as the action if the set of potential actions contains only one object instantiation and one getter method invocation the object instantiation is chosen as the action if the set of potential actions contains one or more non getter method invocations the one closest to the assertion is chosen as the action if the set of potential actions contains one or more method invocations the one closest to the assertion is chosen as the action and if the set of potenial actions contains one or more object instantiations the one closest to the assertion is chosen as the action.
for our running example the third rule is the rst one that is satis ed which results in the call to doget at line being selected as the test s action.
both the rules for identifying the class under test and the action were derived based on our examination of existing tests in our motivating study.
while they perform well as seen in our evaluation section it is unlikely that they are complete.
if additional experimentation would reveal shortcomings we will adjust the approach by modifying the rule set appropriately.
.
.
identifying the expected outcome the expected outcome part of a test is used to check whether the result of performing the action matches the tester s expectation.
in a unit test this is done by calling a junit assertion method e.g.
assertequals asserttrue etc.
.
currently we focus on tests with a single assertion.
this choice allows us to explore the feasibility of automatically generating descriptive test names without reducing the scope of the problem too far in our study of real tests see section we found that a signi cant number follow this recommendation.
in future work we plan on investigating approaches for summarizing multiple assertion that are similar to the action dependency graph see section .
.
.
while identifying calls to assertions is straightforward generating descriptive names requires a detailed understanding not only of what method is being called but also which argument represents the tester s expectation i.e.
the expected value and which argument represents the result of performing the action i.e.
the actual value .
for single parameter req.setparameters params params.put resolution paramsparams.put height paramsparams.put width paramsservlet.doget req res reqresolutionservlet new barcodeservlet servletfigure an example action dependency graph.
assertions e.g.
asserttrue assertfalse etc.
clearly the single argument is the actual value and the expected value is encoded in the assertion s name e.g.
true false etc.
.
for multiple parameter assertions e.g.
assertequals the situation is not as straightforward.
although the junit api declares the rst parameter to be the expected value we found that testers often switch the order of the parameters.
while reversing the argument order does not impact the test s ability to detect errors it does mean that we cannot rely on argument order to identify the expected value.
instead we use static analysis to track backwards along each argument s use def chains.
if one argument resolves to a constant and the other to a method invocation we consider the constant to be the expected value and the method invocation to be the actual value.
if both resolve to method invocations the technique checks whether one of the invocations calls a method declared by the class under test.
if so the other method invocation is treated as the expected value.
otherwise the technique assumes the tester is using the api correctly and considers the rst argument as the expected value.
for the example shown in figure 2a although the order of the arguments is reversed the technique can correctly identify 72as the expected value since only one of the arguments is a constant.
again these rules were de ned based on our experience and appear to work well.
if necessary we will improve them in future work.
.
.
identifying the scenario under test the scenario under test is the part of a test that is used to set up the necessary environment for performing the action.
in order to identify it we build an action dependency graph that is rooted at the action.
figure shows the action dependency graph for our running example.
at a high level nodes in the action dependency graph are expressions in the test and edges encode relational information among the expressions.
solid edges indicate relations among the identi ers in the source and target expressions.
for example in figure the solid edge labeled req indicates that the expression req.setparameters params uses the identi er req which is used in the action.
the type of relation is indicated by the weight of the edge with bold edges denoting that the target expression de nes the identi er and normal weight edges denoting that the target expression uses the identi er.
dashed edges indicate relations among the words in the test s expected outcome and the target node.
for example the dashed edges indicate that the word resolution occurs both in the expected outcome assertequals barcode.getresolution and the expression params.put resolution .
capturing relations among words in addition to relations among identi ers allows the technique to identify expressions that are related to the assertion but would not otherwise be identi ed.
it also has the bene t of reducing 628the depth of expressions that are closely related to the expected outcome.
for example in figure the depth of params.put resolution is reduced from two to one.
this helps ensure that such expressions are included when the scenario under test is converted into english phrases see section .
.
algorithm building the action dependency graph input action identi ed action input assertion identi ed expected outcome function buildgraph action assertion g graph worklist factiong whilejworklistj6 0do current extract worklist forid2identifiers current do ifvisited id then continue end if foruse2uses id do adduseedge g current use id worklist worklist fuseg end for fordef2definitions id do adddefedge g current def id worklist worklist fdefg end for end for end while forword2words assertion do fornode2gdo ifcontainsword node word then addwordedge g action node word end if end for end for return g end function algorithm shows pseudo code for building the action dependency graph.
as input the algorithm takes the test s identi ed action and expected outcome assertion .
the rst part of the algorithm lines to is responsible for adding identi er based relations.
at a high level this part of the algorithm is a typical worklist based iterative process.
first the worklist is initialized to contain the provided action.
then while it is not empty an element of the worklist is removed.
next the algorithm iterates over each identi er in the current expression.
for each use and de nition of the identi er an appropriate edge is added to the graph and the expression containing the use or de nition is added to the worklist.
the second part of the algorithm lines to is responsible for adding word based relations.
first this part of the algorithm iterates over all words contained in the provided assertion.
the current implementation of the words function produces a list of words by gathering all identi ers and literal values from its argument splitting them using a camel case based identi er splitter and removing stop words such as get and set .
next the algorithm iterates over each expression in the graph and determines whether the expression contains any of the words from the assertion.
if so an appropriate edge is added between theroot node of the graph action and the expression.
currently containsword performs a simple equality check.
in future work we plan to enhance this part of the technique to detect more types of matches.
for example understanding that the number 72and the strings and seventy two all refer to the same concept.
.
phase text generation the goal of the text generation phase is to generate descriptive test names using the information obtained by the analysis phase.
at a high level this phase has two steps.
the rst step is to select the information that will be included in the resulting test name according to the provided template.
the second step is to translate the selected information into a descriptive test name.
.
.
test name templates because there is a wide variety in both what information testers want to include in their test s names and how they want to present that information the approach is con gurable and allows testers to provide a template for the generated descriptive names.
in this way our technique provides testers with more options based on their preferences about test names.
both concise and very descriptive test names can be generated by modifying the templates.
currently the technique uses three built in templates that were created based on common test naming patterns .
the small template only contains the test s action.
the medium template extends the small template by also including the expected outcome.
finally the fulltemplate includes the test s action the expected outcome and the scenario under test.
in practice names generated by the small template are similar to the partial names we encountered in our study while names generated by the medium and full templates are comparable to names that we consider to be complete.
choosing which parts of the action and expected outcome to include is straightforward since they are both typically single expressions.
however if all of the expressions included in the scenario under test s action dependency graph were used the resulting test names would be unacceptably long.
to address this situation the technique prioritizes the expressions in the action dependency graph based on their depth and incoming edge type.
currently the technique selects expressions with a depth of 1and only selects expressions with incoming de nition edges if expressions with incoming use and name edges do not exist.
in our running example this results in req.setparameters params and params.put resolution being selected but not servlet new barcodeservlet .
after selecting the expressions the technique orders them by their position in the test.
while this ordering scheme is simple we found it to be surprisingly e ective.
because test names and bodies typically follow the same order ordering by appearance results in names that closely resemble real test names.
.
.
translation the last step of our technique is to translate the selected expressions into a descriptive name.
more speci cally our technique needs to translate expressions written in java into natural language phrases and concatenate the english phrases into legal test names.
in this step our technique uses a suite of translators that are customized for various expression types.
when an expression needs to be translated 629the most speci c translator is used.
currently our technique implements translators for the most commonly used expressions that appear in a test body.
speci cally our technique has translators for the following expression types if the expression is a single argument assertion it is translated into a predicate phrase using a hand coded lookup table e.g.
asserttrue is translated into is true assertnotnull is translated into is not null etc.
if the expression is a multiple argument assertion it is translated into a linking verb or phrase e.g.
assertequals is translated into is while assertsame is translated into is identical to etc.
if the expression is an invocation of a getter method it is translated into a noun phrase by stripping the leading get e.g.
getwidth is translated into width if the expression is a non getter method invocation it will be translated into a verb phrase using the method name e.g.
req.setparameters params is translated into set parameters shown in figure 2b if the expression is a class initialization it is translated into a gerund phrase e.g.
new foo is translated into creating foo if the expression is an assignment or a variable declaration it is translated into a short english sentence e.g.
int x y is translated into x equals y if the expression is a mathematical expression all mathematical symbols are translated into english words or phrases e.g.
a b is translated into a is greater than b and otherwise the default translation is to simply eliminate all illegal characters for java identi ers.
after translating the expressions into english phrases the technique performs some nal adjustments depending on where the phrase will be placed in the template.
these adjustments improve the uency of the resulting test name.
for example the expression req.setparameters params is initially translated to setparameters .
however because this phrase is part of the scenario under test which starts with when we modify the verb set to its gerund form setting to improve readability.
finally the english phrases are placed in the template with multiple phrases being joined by the appropriate conjunction.
.
evaluation design to evaluate our technique we developed a prototype implementation and investigated the following questions rq1 similarity.
how similar are the automatically generated test names to human generated test names?
rq2 human preference.
which automatically generated test names are preferred by developers and how do the names generated by our technique compare to the original test names?
rq3 productivity.
how much time could be saved by using our technique rather than manually writing names?
the remainder of this section discusses our prototype implementation and provides a detailed discussion of relevant data and analysis for each research question.
.
prototype tool to experiment with our technique we implemented it as a prototype tool nameassist for automatically generating descriptive names for tests written using the junit testing framework.
the implementation of the analysis phase is based on eclipse s abstract syntax tree ast framework and implemented as an eclipse plugin.
we chose to use thisframework for several reasons.
first it parses source code directly.
this gives the analyzer access to variable names and other sources of semantic information that are lost after the source code is compiled.
second eclipse provides implementations of many types of static analysis e.g.
class hierarchy analysis call graphs etc.
that we can leverage.
third eclipse is a commonly used ide.
this allows developers to more easily access the technique and broadens the pool of subjects that we can consider in our evaluation.
in order to identify relevant parts of a test body the analyzer uses a collection of visitors to walk the corresponding ast and eclipse s code search api to build the action dependency graph.
the implementation of the text generation phase is written in java and translates eclipse ast expressions into natural language using the rules described in section .
.
the template design allows for easily extending and improving the implementation in future work.
.
alternate approaches to conduct a suitable evaluation of nameassist we compared it against two alternative test name generation approaches a random approach and a term frequency inverse document frequency tfidf based approach.
the tfidfbased approach represents the current state of the art in summarizing source code and the random approach serves as a baseline.
.
.
tfidf based approach tfidf is a metric for indicating how important a term word is to a speci c document in a corpus collection of documents .
a term s tfidf value with respect to a document increases as a function of the term s frequency in the document but is o set by the frequency of the term in the corpus.
while tfidf is more commonly applied to natural language it has been successfully applied to source code .
because tfidf has been shown to be an e ective method for summarizing software artifacts code we choose it as the basis for an alternative test name generation approach.
the tfidf based test name generator considers the collection of methods in a software project as a corpus and the words that appear in the project s methods as terms.
to transform a software project into a corpus we use an eclipse plugin.
first the methods in the project are identied using eclipse s java model.
then terms are extracted from each method by using an ast visitor that identi es all literal values and identi ers in the method.
note that if the method is a test the test s name is excluded from the set of identi ers.
this prevents the approach from having access to the original test names.
identi ers are then split using a custom identi er splitter based on conservative camel case splitting .
finally stop words are removed and the resulting document is added to a lucene index.
to generate a name for a test the approach queries the lucene index to nd the nterms with the highest tfidf values with respect to the test s body.
the words are then ordered by the position of their earliest appearance in the test and concatenated together.
we chose this ordering scheme since it matches the ordering scheme used by nameassist for the scenario under test.
.
.
random approach to generate a name for a test the random approach identi es the set of words contained only in the body of 630the test using the same ast visitor as the tfidf based approach and selects nwords from the set orders them by position of earliest appearance in the test and concatenates them together.
.
considered unit tests because our evaluation involves human developers both for generating reference test names see section .
and evaluating generated names see section .
we are limited in the number of tests we can consider.
as a result we chose to consider 60tests in our evaluation which allowed the developers to complete their tasks in a single afternoon.
to select the 60used in our evaluation we rst generated a list of all java projects hosted on github.
second we ltered the list to remove projects that cannot be automatically imported into eclipse.
because all of the automated test name generation tools are eclipse based selecting eclipsecompatible projects allows us to apply the tools without modifying the projects.
third we sorted the projects by the number of junit tests they contain and selected the 20projects with the highest number of tests.
finally we randomly selected from the chosen projects 60tests that contain a single assertion.
.
evaluation measure bleu at a high level we can view test name generation as a translation task a descriptive test name is a translation of a test s body into natural language.
by structuring the task in this way we can use bleu a commonly used measure for assessing the quality of text that has been translated from one language to another to evaluate the automated test name generation techniques.
brie y bleu calculates a score for a candidate translation by comparing the candidate to a set of reference translations using a modi ed n gram precision calculation.
the calculated score ranges from 0to1 and indicates the similarity of the candidate to the