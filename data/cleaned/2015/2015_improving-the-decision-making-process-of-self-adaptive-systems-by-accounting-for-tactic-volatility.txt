improving the decision making process of self adaptive systems by accounting for tactic v olatility jeffrey palmerino qi yu travis desell and daniel e. krutz b. thomas golisano college of computing and information sciences rochester institute of technology rochester ny usa email jrp3143 qyuvks tjdvse dxkvse rit.edu abstract when self adaptive systems encounter changes within their surrounding environments they enact tactics to perform necessary adaptations.
for example a self adaptive cloud based system may have a tactic that initiates additional computing resources when response time thresholds are surpassed or there may be a tactic to activate a specific security measure when an intrusion is detected.
in real world environments these tactics frequently experience tactic volatility which is variable behavior during the execution of the tactic.
unfortunately current self adaptive approaches do not account for tactic volatility in their decision making processes and merely assume that tactics do not experience volatility.
this limitation creates uncertainty in the decision making process and may adversely impact the system s ability to effectively and efficiently adapt.
additionally many processes do not properly account for volatility that may effect the system s service level agreement sla .
this can limit the system s ability to act proactively especially when utilizing tactics that contain latency.
to address the challenge of sufficiently accounting for tactic volatility we propose a t actic v olatility aware tv a solution.
using multiple regression analysis mra tv a enables selfadaptive systems to accurately estimate the cost and time required to execute tactics.
tv a also utilizes autoregressive integrated moving average arima for time series forecasting allowing the system to proactively maintain specifications.
index t erms artificial intelligence self adaptation machine learning i. i ntroduction the world is increasingly relying upon autonomous selfadaptive systems that have the ability to function independently without human interaction.
examples of these selfadaptive systems include self driving cars medical devices and many common internet of things iot devices.
many self adaptive processes utilize a closed loop control mechanism that monitors the system s state and its surrounding environment.
furthermore these mechanisms also determine if the system should be altered to perform any necessary adaptations .
these self adaptive approaches typically rely upon a set of adaptation tactics to make necessary changes .
example tactics include the provisioning of an additional virtual machine in a web farm when the workload reaches a specific threshold or reducing non essential functionality on an autonomous unmanned aerial vehicle ua v when battery levels are low.
tactics frequently experience latency which is the amount of time from when a tactic is invoked until its effect on the system is realized .
examples of tactic latency include a cloud based system requiring more than a minute to update certain firmware nodes or a cyber physical system requiring one minute to re activate gps signals .
tactics also frequently have a cost associated with them which may be in the form of energy monetary or other resource costs necessary for execution .
examples of tactic cost include the required energy for moving a physical component in a ua v or the monetary cost of using a remote third party resource for computational tasks.
both tactic latency and cost are likely to be first class concerns in the decision making process as they can directly impact if and when a tactic is executed .
improperly accounting for tactic latency can lead to situations where tactics are begun too early or too late or are not available when needed .
additionally improperly accounting for tactic cost can result in the selection of a tactic that is more expensive than a tantamount less costly alternative .
therefore it is imperative that self adaptive systems properly account for both the latency and cost of tactics.
real world systems will frequently encounter tactic volatility which is any rapid or unpredictable change that exists within the attributes of a tactic.
for example both tactic latency and cost may be highly volatile depending on the system s surrounding environment.
a tactic of transmitting data could take longer than expected due to network congestion or a tactic of moving a physical component in a ua v could be more expensive due to mechanical problems.
unfortunately state of the art decision making processes in self adaptive systems do not account for tactic volatility.
this limitation can be highly problematic adversely impacting the decision making process in several ways.
for example a system may execute a tactic too late to be effective if it assumes that the tactic will always take two seconds to 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
conduct when in reality it has been observed to consistently take longer.
aservice level agreement sla helps to define system objectives such as keeping a value under a specific threshold along with rewards and penalties for meeting defined objectives .
a challenge for purely reactive processes is that the system may not adhere to objectives defined in the sla if tactics are expected to experience latency.
for example the sla for a cloud based self adaptive system may define a reward for operating under a response time threshold.
however tactics such as adding servers to reduce response times will likely experience varying levels of latency.
to perform optimally the system should act proactively by beginning these tactics before the additional resources are required so that the resources are available when needed .
thus a mechanism that enables the system to accurately forecast tactic latency and cost values is highly beneficial to the self adaptive process.
to address the limitations of current self adaptive processes in properly accounting for tactic volatility we propose a tactic v olatility aware tv a solution.
through the use of a multiple regression analysis mra model tv a enables the selfadaptive system to learn from previously experienced tactic volatility and make accurate estimates of how the tactic will behave in the future.
tv a also includes an autoregressive integrated moving average arima component to enable selfadaptive systems to proactively begin adaptations according to their sla specifications.
to summarize this work makes the following contributions problem demonstration using simulations we demonstrate that accounting for tactic volatility is essential in self adaptive systems especially when the system is known to have unpredictable behavior.
concept to the best of our knowledge our tv a approach is the first process that accounts for tactic volatility.
existing processes merely consider tactics to have static values whereas our tv a approach uses run time predictions of latency and cost to handle tactic volatility.
experiments our experiments demonstrate the positive impact that tv a has on the decision making process.
these experiments are conducted using real world data and thus provide additional confidence in our findings.
tool and dataset our v olatility emulator v alet tool includes data generated from a physical system.
this enables the evaluation of our tv a process and provides other researchers with a foundational dataset that they may use to support their own research.
this tool and dataset are available on the project website the rest of the paper is organized as follows.
section ii motivates our research using examples of tactic volatility.
section iii defines our tv a solution to effectively address tactic volatility.
section iv describes valet our tactic volatility dataset.
section v describes our systematic experimentalevaluation of our proposed process.
section vi describes related works while section vii describes threats and future work.
section viii concludes our work.
ii.
p roblem definition tactic latency volatility and tactic cost volatility are the primary motivators for addressing a system s sla with proactive processes.
if a system is purely reactive and does not properly anticipate future system changes in regards to the specifications defined in the sla it will be limited by its inability to provide support for proactive tactic implementation.
a. tactic cost v olatility examples of tactic cost vary widely and are largely domain specific however tactic cost is frequently a primary concern in the decision making process .
possible examples of tactic cost include the energy necessary to move a mechanical component in a physical device the monetary cost to utilize a resource or the number of computations necessary to perform a tactic.
estimating tactic cost will likely be a first class concern for the system especially if there are defined cost thresholds resource limitations or if the system merely has the goal of attaining maximum utility at the lowest cost .
unfortunately despite the cost volatility that many real world systems are likely to encounter existing self adaptive processes that account for cost consider it to be a static value that will encounter no volatility .
accounting for tactic cost volatility is imperative for several reasons cost may be a primary consideration when selecting between multiple tactic options when the system has multiple tactic options that it may choose from the cost of the tactic may be a determining factor when selecting between multiple otherwise tantamount options.
determine if the cost exceeds reward in selecting which tactic s to execute the system will frequently calculate the expected reward and cost for performing the tactic.
if the cost exceeds the reward then it may not be optimal for the system to execute the tactic.
the estimated cost may impact the system s ability to execute concurrent or subsequent tactics the system may possess a finite amount of a resource with one example of being battery power.
assume that a system has units of battery power remaining.
processes that consider cost to be a static value may define the energy usage of tactic a and for tactic b .
therefore the system will assume that it has the ability to execute both tactics concurrently or sequentially.
however if the actual energy usage of each tactic is a andb in reality the system will not have the ability to fulfill the amount of energy necessary for the execution of both tactics either concurrently or sequentially.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
b. tactic latency v olatility the amount of time required to implement a tactic is known as tactic latency which in real world systems can be highly volatile.
for example the precise amount of time a system needs to transmit a file across a network may fluctuate due to varying amounts of network traffic.
previous works have shown that latency aware self adaptive processes offer several advantages over traditional non aware techniques .
however many state of the art self adaptive processes still consider tactic latency to be a predetermined and static value.
systems lack the capability to learn to adaptively accommodate for variability in tactic latency.
due to its large impact on the decisions that a system should make accounting for tactic latency volatility is imperative for several reasons knowing when to begin the execution of a tactic a priority for many self adaptive systems is to ensure that tactics are ready when needed.
therefore if a tactic has expected latency then the system will need to proactively begin its execution so that it is available when needed.
however implementing a tactic proactively will typically have additional costs involved so proactive adaptation must be done with consideration to available resources.
determine when to augment a slow tactic with a faster tactic accurate tactic latency knowledge is imperative for determining when to augment a slow tactic with a faster tactic.
there may be instances when the system decides to utilize a faster less effective tactic to augment a slower but more effective tactic .
for example in a self adaptive system some given tactic amay have higher latency while producing a higher benefit than that of a faster tactic option b. the system may decide to implement both tactic options knowing that the system could potentially realize the benefits of tactic bwhile it is waiting for tactic a. when deciding to augment a slower tactic with a faster one accounting for tactic latency volatility is essential to determine which tactic s should be used to augment a slower tactic.
ensuring the selection of the most appropriate tactic when selecting between multiple tactic options ignoring tactic latency can be problematic.
consider a scenario where the system is deciding whether to use tactic aor tacticb.
if tactic bis only slightly better than tactic a in terms of instantaneous utility improvement then the decision making process would favor tactic b. however if tacticais faster than tactic b then tactic awould start to accrue utility faster.
this means that tactic amay be the most optimal selection .
c. motivating example as a motivating example we will use a cloud based selfadaptive system based on a similar scenario defined by moreno et al.
.
this example represents a multi tier web application that is compromised a web server and database tier.
the webserver s process a client s request and then accessinformation stored in the database tier.
to efficiently provide content while encountering variable workloads the system can either add remove servers from the pool or reduce optional content using the dimmer feature.
this system has has a goal of maximizing utility while minimizing cost.
the sla defines the target response time t and how utility u is calculated.
the system incurs penalties if the target response time is not met and accrues rewards for meeting the target average response time against the measurement interval.
the average response rate is a the average response time is r the maximum request is kand the length of each interval is defined as .
provided content is reduced as necessary using a dimmer value d .
optional content has reward of ro and produces a higher reward than mandatory content rm .
we will slightly modify the equation to incorporate cost c which could be the monetary or energy cost u braceleftbigg a dro d rm c r t min a k ro c r t this system can account for increases in user traffic using two different tactics i reducing the proportion of responses that include the optional content dimmer and ii adding a new server.
while reducing optional content will have negligible latency adding a new server can take several minutes.
tactic latency volatility if the system anticipates that the response threshold will be surpassed in the immediate future then the system could proactively start the tactic of adding a server to keep the response time under the defined threshold.
overestimating latency could result in scenarios where the system unnecessarily incurs additional cost as servers would be active longer than necessary.
additionally if the system determines that it is likely to surpass the defined response time threshold before a new server can be added then the most appropriate system action may be to use the faster tactic that reduces the amount of optional content while it waits for a new server to be added.
improper tactic latency predictions can lead to situations where the system executes the tactic too soon or too late or even selects the improper tactic for the encountered scenario.
accounting for tactic latency volatility is a paramount concern especially when utilizing a proactive adaptation approach or when utilizing complementary tactics.
tactic cost volatility in the provided scenario it is important to account for cost volatility especially since not accounting for cost volatility could lead to inaccurate utility calculations.
in the event that cost is defined to be higher than what the system is actually encountering in the real world then this may lead to scenarios where optional o content is shown too infrequently.
conversely if the cost is defined lower than what is being encountered in the real world then could lead to scenarios where optional o content is shown too frequently.
a volatility aware solution that enables the system to more accurately predict cost would enable the system to make decisions that lead to more optimal outcomes.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
reactive specifications monitoring if the motivating example was a purely responsive system and did not employ any proactive functionality it will only determine if the defined response threshold t is being surpassed at the current moment.
this can be problematic since the tactic of adding additional resources to reduce response time in our example has latency so the system will need to begin adapting before the tactic is actually needed to be complete.
otherwise the system will incur penalties or not realize rewards while it waits for the tactic to become available.
a process that enables the system to better anticipate how it was going to act in accordance with the sla would help the system to perform more optimally in dynamic environments.
iii.
p roposed tv a p rocess our tv a process consists of the following two phases i time series forecasting with autoregressive integrated moving average arima and ii run time model generation using multiple regression analysis mra where arima predicts if system specifications in the sla may be broken in the future and mra creates run time models for predicting tactic latency and cost.
a. autoregressive integrated moving average the first component of the tv a process is arima which is a commonly used approach for the prediction of time series data.
arima is a generalization of the arma autoregressive moving average model which accounts for non stationary data using differencing.
a full arima treatment requires the following notation arima p d q whereprepresents the number of lag observations included in the model drepresents the degree of differencing and qrepresents the size or order of the moving average window.
this work uses a box jenkins approach to find the best fit of a arima model for predicting future time series values.
this involves applying the following three steps identification the first step is to determine the order of the arima model by utilizing differencing to transform the potentially non stationary data to stationary data.
mathematically differences are shown as y prime t yt yt whereytrepresents the current observation and yt represents the immediate prior observation.
differencing allows the model to remove any changes in the levels of time series data thus eliminating trend and seasonality.
in some cases differencing may need to be applied a second time to obtain stationary data.
this second order differencing includes subtracting another termy prime prime t yt yt 2to the arima model.
our work used a differencing order of d as there was a small amount of non stationary time series data.
following this classic autocorrelation and partial autocorrelation plots were used to determine the order of the autoregressiveand moving average terms which resulted in p 1q respectively.
estimation in order to estimate parameters for the boxjenkins models we must apply a solution that can numerically approximate nonlinear equations.
as the goal was to minimize a loss or error term we used the maximum likelihood estimation mle method over a nonlinear least squares estimation to determine the model s optimal parameters.
model checking the final step in applying a full treatment of the box jenkins approach is to perform model checking.
since we have the ability to modify orders in the arima model pandq it is important that we optimize these parameters.
in general optimization should examine i if the model is overfit and ii residual errors.
the former is crucial because it effects how generalizable our model is to other time series data while the latter deals with how well the model performed in terms of predictions.
after examining multiple arima models by fine tuning the three main parameters p dandq the final arima model for our work was arima .
with this notation our model is known as a differenced firstorder autoregressive model .
b. multiple regression analysis the second component of our tv a approach deals with run time predictions of tactic latency and cost.
as discussed previously current self adaptive processes consider these values to be static attributes.
in real world scenarios this is an unlikely phenomena because certain events have different outcomes depending on the surrounding environment.
for example a ua v may need to determine the time it will take to transmit a critical file to its base station.
this latency could drastically vary depending on the distance of the ua v to the base station weather conditions and even component functionality.
therefore the ua v must have a method of modeling its surrounding environment so it can accurately anticipate the length of time required to transmit the file.
in our work we applied this same ideology to predicting tactic latency and cost.
there are many different types of regression models and even machine learning models that could be used to accomplish this.
initially we examined a machine learning approach called bayesian ridge regression brr which through a bayesian process allows the model to train itself over time as more data becomes available.
however we found that the required feature space for brr was too large and that the data we collected could not support it.
therefore our tv a approach uses multiple regression analysis mra to support run time predictions of tactic latency and cost.
consider a classic regression model y x w w0 m summationdisplay j 1wjxj w primex authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
wherex x1 ... xm primewithx0 andxjbeing the j th observation of the independent variable x.g i v e nas e t ofntraining variables x1 ... x n along with the observed responses t1 ... t n we can solve for the best possible weights of this model through minimizing the error function e w 2n summationdisplay n y xn w tn e w is a quadratic function of w the observation weight which can be conveniently minimized leading to w x primex 1x primet wherexis the design matrix whose rows correspond to the observation vectors of the variables and t t1 ... t n prime denotes the observed response values of nvariables.
after minimizing equation we now have the estimated weights w in equation which can be used by our regression model to predict tactic latency and cost.
it is important to note that regression analysis estimates the conditional expectation of the dependent variable that is the average value of the dependent variable when the predictor variables are fixed.
other related methods such as necessary condition analysis nca could be used to estimate the maximum value of the dependent variable however we decided that this was out of scope for our approach since estimating maximum values would be considered the worst case scenario for the tactic.
tv a workflow tv a combines an arima time series model with an mra model to improve the self adaptive process.
in this the system is able to properly maintain its defined specifications while also accounting for tactic volatility leading to better adaptations and better overall performance.
as shown in the pseudo code in algorithm the workflow of tv a is reasonably straightforward.
the top level definitions under procedure represent the specifications that are being monitored and any calculations that are made.
for instance spec represents the current specification that we are analyzing whileresp represents the response we obtain from performing the actual analysis.
furthermore the loop of the workflow represents the main component of tv a. we will next discuss a concrete example that examines the two primary steps of our tv a process as shown in the pseudo code.
for this we will continue to use the self adaptive hosting service example described in section ii c. step monitoring specifications the first step in our tv a approach is to gather and monitor the specifications defined in the sla.
using the self adaptive hosting service example we will assume that it has been configured to record the response time every six seconds.
we will also assume that the system salgorithm tv a workflow procedure workflow spec the specification to analyze resp quantified response from arima analysis loop resp analyzespecification spec ifresp is potentiallybroken then makelatencyestimate makecostestimate goto loop sla specification defines that a penalty will be incurred if response times do not stay under a .
second threshold.
unfortunately the majority of self adaptive processes act in a reactive manner so the system would not be able to adapt until after the threshold was surpassed.
through the application of the time series model arima discussed in this section we will enable the system to act more proactively by allowing it to anticipate future specification values.
given the specification and parameters required to make decisions historical data is used to determine parameters to the arima models algorithm line which provide anticipated future values.
for example in the hosting service when the response time specification is analyzed with arima the response is the forecast value.
if this forecast value determines that the specification may be broken in the future or is close to being broken we continue with the rest of our tv a approach.
step t actic latency and cost prediction when the system has determined that it needs to adapt whether it be because a specification willormay be broken our approach then uses regression analysis to predict tactic latency and tactic cost.
for example if our arima analysis from step determines that the hosting service needs to adapt our tv a approach then predicts tactic latency and cost for all available adaptation tactics.
this could entail predicting the latency and cost for a tactic that adds extra computing resources so that response time can be reduced.
these predictions are made through a multiple regression analysis mra model.
a benefit of focusing on predicting tactic latency and tactic cost is that this can be easily adopted into many existing decision making processes.
for example one simple option is for the predicted values to merely replace the static predefined values in the system s decision making process.
iv .
v alet v olatilityemula tortool an additional contribution of this work is the development of the v olatility emulator valet tool to generate realworld tactic volatility data to enable the evaluation of our tv a process.
while existing datasets such as the internet traffic archive are commonly used when evaluating self adaptive processes these are limited as they do not contain an adequate amount of tactic volatility necessary to evaluate our authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
work.
to create a sufficient dataset to demonstrate the benefits of tv a v alet provides tactic volatility data in the form of latency and cost.
the tool and generated dataset is available on the project website the first action performed in v alet utilizes two physical raspberry pi devices with one acting as the operator and the other as a monitoring instrument.
the operator gathers latency data by recording the time required to download an identical 75mb file at one minute intervals from three different download mirrors around the world.
to determine energy usage for this operation the monitoring instrument collects and records its own energy usage as well as that of the operator.
then the monitoring pi calculates the difference between the operating and monitoring pi s power before and after each download as the overall energy used for the operation.
this action provides both real world latency and cost information.
realworld volatility is also introduced by variations in network speeds that impact the time required to download the file.
the second action performed by v alet obtains an additional form of tactic latency and cost values by performing a grep activity.
after the file is downloaded a simple grep command searches files contents for a specific string.
the amount of time and energy required to perform this task is recorded.
these operations are performed at one minute intervals over the course of an entire day creating over records of tactic volatility data daily.
figure represents a portion of the time series data gathered from our operations and also visualizes the volatile latency values that we obtained.
as shown the latency times gathered from germany were quite volatile with some spikes in volatility in massachusetts and ontario download times.
figure .
hour snippet of latency data v alet benefits the software engineering community by enabling developers and researchers to perform evaluations using real world time series data containing tactic volatility.
although v alet represents a specific example of a simple self adaptive system the created data is generic enough that it can be used to conduct preliminary evaluations of other machine learning processes and tactic volatility aware selfadaptive processes.v.
e v aluation this evaluation addresses the following research questions rq1.
how does not accounting for tactic volatility affect the decision making process?
using the statistical tool r we demonstrate the negative impact of not accounting for tactic latency volatility and tactic cost volatility in the self adaptive process.
rq2.
how effective is arima in allowing the system to monitor system specifications?
in our experiments using time series analysis we demonstrate that time series forecasting with arima helps the system to become more proactive in maintaining specifications defined in the sla.
rq3.
how effective is mra in predicting tactic latency and cost at run time?
in evaluating our multiple regression model we demonstrate that strong predictive power exists throughout our experiments when estimating tactic latency and cost even when faced with varying amounts of volatility in the gathered data.
rq4.
does using tva provide substantial improvement to the self adaptive process over simply using static values for latency and cost?
in comparing our approach against existing self adaptive approaches such as those that consider tactics to have static latency and cost values we demonstrate the substantial improvement to the decision making process that our tv a process provides.
a. experimental data analysis to analyze the results found from our v alet experiments we used the statistical metrics of root mean square error rmse and mean absolute error mae to evaluate the systematic benefits of our tv a approach.
where yiis the predicted value andtiis the actual value at time i rmse and mae are defined as rmse n summationdisplay i yi ti n mae n summationtext i yi ti n these metrics allowed us to determine tv a s ability to reduce uncertainty and increase the effectiveness of the decisionmaking process by examining the prediction accuracy of our time series and machine learning models.
rmse provides a better search landscape for determining model parameters and was used to determine how well our models performed in terms of predictive ability as larger prediction errors become more pronounced due to the squaring of such errors.
on the other hand mae was used to examine the absolute value of these error differences.
mae was also used for looking at authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
forecasting errors in time series analysis which is one of its most common uses .
b. results rq1 how does not accounting for tactic volatility affect the decision making process?
we first explored rq1 by performing a proof of concept evaluation using the statistical tool r where we emulated the negative effects of poor decisionmaking by a self adaptive system.
we began by defining two tactics a b that had arbitrarily defined costs associated with their latency times where if the tactic took xamount of time to execute it had cunits of cost.
table i characteristics of sample tactics for proof of concept simulation cost distribution average sd tactic a positively skewed .
tactic b normal .
we next generated a normal distribution and positively skewed distribution with the same mean and standard deviation to represent latency times.
we gave the tactic with the higher cost b the normal distribution while the tactic with the lower cost a was given the positively skewed distribution.
by using two divergent values we are able to demonstrate the impact of not accounting for tactic volatility in a near perfect environment the normal distribution and in a volatile environment the positively skewed distribution .
it is possible that both latency and cost do not follow these types of distributions but because they are two extremes it was sufficient for our proof of concept.
table i shows the characteristics of each tactic.
cost of executing a tacticfrequenc y of costtactic a tactic b figure .
overall costs of executing a tactic in the r simulations.
after generating the characteristics of each tactic we then performed simulations of random sampling from each tactic s latency and multiplied the sampled latency value by its associated cost.
this enabled us to build a simulated distribution of tactic executions where the same tactic isexecuted every time but with different latency values.
this also allowed us to gather overall cost values that could be used to compare how varying data distributions can impact the predictability of cost for executing a tactic without accounting for any form of volatility.
in our analysis we found that tactic bmay not have had the lowest cost values in the simulations but it was significantly more consistent figure .
however tactic b s data followed a normal distribution which is an unlikely occurrence for many real world systems.
furthermore even though tactic a had a lower cost of execution with a value of five it resulted in much more sporadic and extremely high overall costs to the system.
outcome our proof of concept simulations in r successfully demonstrate that accounting for tactic volatility is essential in self adaptive systems especially when the system is known to have unpredictable behavior .
rq2 how effective is arima in allowing the system to monitor system specifications?
to evaluate our arima model for time series forecasting we used the energy data collected from our v alet tool.
although we had been using a self adaptive hosting service as an example in this paper specifically one that monitored response time the energy usage data is of the same concept.
for both data is gathered over a period of time at equal intervals thus qualifying it as timeseries data.
however the only difference with this analysis is that we did not consider the energy usage when v alet is downloading the file.
therefore the data used in this research question is strictly the energy usage fluctuations when the device was idle and notperforming a tactic activity.
to specifically address this research question we had to loop through the data multiple times to ensure that arima could provide sustainable time series forecasting predictions.
we did this by creating experiments using a randomly selected portion of our data as training data and the other as test data.
this type of process can also be seen as a form of k cross validation which ensures that each data point is included in the training set at least once.
after performing this validation we then calculated both rmse and mae values to determine the predictive ability of the arima model.
figure shows the mae results from our arima model compared to a previously used hidden markov model hmm .
each simulation was independently performed so no patterns should be inferred from the left to right sequence.
over the course of the experiments we saw fairly stable mae values represented by the small range between the lowest mae value and the highest.
furthermore the arima model was not only successful by itself but also better than that of the hmm model.
we believe this occurred as we did not have enough features in our model to allow for a full treatment of a hmm.
as discussed previously larger prediction errors will become more pronounced and smaller prediction errors will become less pronounced when using rmse.
due to this we expected authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
.
.
.
simulation nmae valuehmm arima figure .
mae values over experiments using arima vs hmm for time series forecasting .
.
.
simulation nrmse valuehmm arima figure .
rmse values over experiments using arima vs hmm for time series forecasting the rmse graph to be slightly more dispersed compared to mae.
in examining the rmse differences between hmm and arima we found exactly this.
looking at figure closely we can see how the rmse values have more variation than the mae values in figure .
however this does not mean there was less predictive power or that the model is less useful.
we report on both to show the differences in possible inferences.
for example in applying our tv a technique the system s engineers may care more about larger prediction errors deeming rmse a more powerful statistic for determining predictive ability of their models.
conversely if the system s engineers want more of a man in the middle statistic they may deem mae more appropriate.
for our experiments using rmse or mae would lead to the same conclusion both statistics clearly favor the arima model over the hmm model for time series forecasting.
outcome through an evaluation process using time series data tva demonstrated it s ability to positively support proactive adaptations.rq3 how effective is mra in predicting tactic latency and cost at run time?
unpredictability is considered to be an undesirable trait of a self adaptive system and is frequently associated with much of the uncertainty that surrounds selfadaptive systems .
to determine how well tv a can improve the predictability of a self adaptive system we examined the prediction errors across our tactic latency data and our tactic cost data.
for space considerations and the use of rmse being more appropriate in this context this research question does not report mae values.
unlike rq where we only examined the energy usage for when the raspberry pi was idle rq only examined the energy usage when the device was performing the file download.
if we had used the energy usage data from when the device was idle our models would have been invalid.
this is because the file download represented a tactic of gathering more information thus any energy data gathered while the device was idle did not represent tactic latency.
.
.
.
simulation nrmse valuebrr mra figure .
rmse values for tactic cost predictions of mra vs brr figure demonstrates why mra is more appropriate for tv a. over the course of the simulations mra reported lower rmse values than brr in almost every case.
in cases where brr did outperform mra the differences were fairly negligible.
however the plots are closer together than what we would have initially expected.
we believe this was caused by only having a few cases of extreme volatility in the cost data therefore not allowing the two models to really differentiate themselves.
as shown in figure we observed very similar results between the rmse values calculated for tactic latency predictions and those calculated for tactic cost.
in most cases prediction errors with mra were much smaller than those of brr and in examining the figures closely the differences in volatility that were experienced can be observed.
as mentioned previously there were not as many extreme cases of volatility in the tactic cost data.
however within the tactic latency data we saw many cases of volatility with some cases being extreme.
this can partially be seen in figure since more volatility will likely lead to larger prediction errors.
thus authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
.
.
.
simulation nrmse valuebrr mra figure .
rmse values for tactic latency predictions of mra vs brr it came at no surprise that the rmse values for tactic latency had a wider spread than those associated with tactic cost.
in order to say the mra model can be generalized to other data sets for predicting tactic latency and cost observing consistent rmse values regardless of the data it was modeling was imperative.
figure and figure demonstrate that not only were rmse values low for both data sets but the values were stable.
therefore mra was able to handle the volatile latency data and cost data collected for these experiments when making its predictions.
outcome the rmse results gathered from addressing this research question demonstrate that mra is able to handle tactic volatility when predicting tactic latency and cost.
throughout our experiments mra consistently provided stable predictive power even in the presence of volatile data.
rq4 does using tv a provide substantial improvement to the self adaptive process over simply using static values for latency and cost?
thus far we have demonstrated the benefits of using an arima time series model and a mra model in a self adaptive process to predict tactic latency and cost while also maintaining specifications defined in the sla.
however to provide further confidence in our tv a approach we also compared it to current self adaptive processes.
since current processes consider tactic latency and cost to be static values there is no one specific work that tv a can be compared to.
rather for this research question we compared the results from our approach to what we consider the baseline model defined below baseline model a model that uses a simple average of previous tactic latency and cost values as its prediction process for estimating future tactic behavior .
while utilizing the average value for latency and cost values may seem like a naive comparison in many cases it can be a be a strong predictor especially when compared a static value asin other current self adaptive processes on a dynamic time series.
similar to rq we will also report the mae and rmse values for model comparison.
.
.
.
.
simulation nrmse valuetv a baseline figure .
baseline value approach vs tv a rmse differences as shown in figure tv a was able to obtain substantially lower rmse values.
in comparison tv a had an average rmse value of .
while the baseline approach had an average rmse value of .
.
also represented in this diagram is the ability of tv a to handle volatility.
while the spread of rmse values remained fairly consistent for tv a the baseline approach saw a much wider spread a direct result of only using a static value for latency and cost.
although there were cases where the baseline approach did outperform tv a in total of experiments the differences in these values were fairly negligible.
.
.
.
.
.
simulation nmae valuetv a baseline figure .
baseline value approach vs tv a mae differences in examining the mae values shown in figure we found that mae did not behave as we expected.
in rq it is fairly evident that rmse made larger prediction errors larger and smaller prediction errors smaller.
however in this research question those kinds of results are not as evident.
in examining the mae values from tv a we can see the plot is a bit more condensed than then rmse plot but the mae authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
values for the baseline model did not follow the same trend as strongly.
we believe this occurred because the baseline model s prediction method was extremely poor justified by both the rmse and mae plots.
regardless tv a was still superior in predictive ability compared to the baseline model.
outcome these findings demonstrate that tva is beneficial for decision making processes in self adaptive systems and offer a significant improvement over assuming tactic latency and tactic cost to be static values.
vi.
r elated works although our work is to the best of our knowledge the first known to make tactic volatility a first class concern in the self adaptive decision making process previous works have examined the impact of tactic latency in self adaptive systems.
c amara et al.
was likely the first to consider tactic latency and examined how considering latency could be used to assist the proactive adaptation process.
however this work differs from tv a in that it does not consider any forms of tactic volatility that are likely to be encountered.
in sb pla the latency of an adaptation is considered in the adaptation decision making process .
a primary benefit of sb pla is that systems that cannot use tactic based adaptations can still include latency awareness in their decisionmaking process .
in addition to supporting pro activeness and concurrent tactic execution pla techniques also account for latency.
pla considers the amount of time necessary for a tactic to execute in order to avoid situations that are not achievable when time dimensions are recognized.
however like with many latency aware approaches latency is still considered to be a static attribute.
in our work we do not consider latency to be static and provide the system with the ability to predict tactic latency at run time.
jamshidi et al.
presented fql4ke a self learning fuzzy cloud controller.
this enables systems to not rely upon designtime knowledge but allows users to simply adjust weights that represent system priorities.
this work found that their proposed process outperformed a previously devised technique that did not have a learning mechanism.
our work is similar in that we both utilize learning to enable the system to make better decisions.
however our work differs in that jamshidi et al.
focused on improving resource planning and not in addressing tactic volatility as is accomplished by tv a. while our work is the first to account for cost volatility existing research has considered cost in the self adaptive decision making process.
several works have included cost in their utility equations however they consider it to be a static value and do not account for real world volatility .
jung et al.
demonstrated that ignoring cost can have a significant impact on the ability to satisfy responsetime based slas.
this work also proposed a cost sensitive self adaption engine using middleware to create adaptation decisions.
this work differs from ours in that it only considerscost in cloud based controllers while we focus on cost during the entire decision making process.
esfahani et al.
utilized learning to improve the selfadaptive process.
a primary contribution of this work is a new process of reasoning and assessing adaptation decisions using online learning.
a preliminary work by elkhodary proposed combining feature orientation learning and dynamic optimization techniques to create a new class of self adaptive systems that would be able to modify their adaptation logic at run time.
tv a differs from this work in that learning is utilized to predict tactic latency and cost at run time while also providing away to estimate future values for requirements.
kinneer et al.
developed a process for reusing prior planning knowledge to help the system to adapt to unexpected situations.
this process considered that tactics may fail and supports reasoning about tactic latency.
while this work is helpful for assisting the overall planning process it does not enable the system to actively learn and predict future values for tactic latency and cost like our tv a approach does.
machine learning has been utilized to help determine the most efficient configurations for self adaptive systems while also performing adaptation planning.
quin et al.
enhanced the traditional mape k feedback loop through the use of a learning model that selects subsets of adaptation options from a larger set of adaptation possibilities.
this process enables the system to make more efficient analysis decisions.
jamshidi et al.
used machine learning to discover pareto optimal configurations to eliminate the need to explore every configuration.
this work also restricted the search space necessary to make planning tractable.
these works differ from ours in that while they use machine learning to create a more efficient selfadaptive process they do not use machine learning to address the issue of tactic volatility.
the popular fifa dataset is a collection of requests made to the world cup website over an approximately four month span.
this dataset is widely used in self adaptive research and can be used to simulate when new servers would have to be activated up to handle additional web traffic or when to disable resource heavy features on a website because the traffic load is impeding the system.
however this dataset does not contain latency.
each request is a log entry that consists of information such as timestamp size status and url hit.
because this dataset does not contain any latency information such as a request received and request fulfillment time it cannot be used in collaboration with researching and evaluating latency aware strategies such as our proposed tv a process.
vii.
t hreats and future work our evaluations have demonstrated tv a s ability to enable systems to better account for tactic volatility.
however there are limitations to this work.
in many systems tactic cost may be an ambiguous and tough to define measurement.
this authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
inability to accurately measure cost could inhibit the adoption of our process by limiting the quality and quantity of observed input values into our prediction process.
furthermore cost can also be a relative term and in our tv a approach we consider it to be a quantifiable value.
for example one could argue that the cost for performing an action could be the wear and tear on a physical component in the device.
such cost is difficult to quantify in most situations.
therefore when using tv a the notion of cost must be restricted to a value that is reasonably easy to quantify.
although tv a provides a method of monitoring specifications defined in the sla not all specifications are necessarily measurable with time series analysis.
for example a system may have the specification that it must be available .
of the time.
this is not something that could be measured or predicted using time series analysis rather it would need to be accounted for using fault detection techniques in the system s architecture.
in systems that do not directly utilize a sla the improved tactic estimations can still be used to benefit the decision making process by providing more informed and accurate tactic attribute values.
future work should be conducted to examine precisely how our tv a process can be incorporated into these systems and determine the benefits that they will have.
to be completely proactive future work must be done to update our arima method to consider tactic latency.
currently this process can alert the system of a specification that is about to be broken however the time between monitoring intervals may not leave enough time to fully execute a tactic.
therefore there may be occurrences where specifications are slightly broken for a period of time.
this future work will likely examine other time series models as well that can be flexible to different systems and datasets.
this in turn would also help us to start addressing the problem of not having enough data to build other kinds of models and would allow this work to develop into an entire decision making framework.
although we have demonstrated the benefits of our tv a approach with real world experimental data we have yet to implement tv a on any physical devices.
future work will consist of including our adaptation process into physical equipment such as iot devices small unmanned aerial vehicles ua v and self adaptive web systems.
arima demonstrated its ability to perform time series forecasting however no mechanisms are in place to quantify uncertainty within these predictions.
future work could be done to include confidence intervals around predictions made by arima.
for example instead of only predicting a single point value for a specification we could predict a range that states something such as the following we are certain response time will be between .
and .
seconds .
this would allow the system to have a buffer zone around its predictions therefore providing the decision making process with more information.additionally the arima models utilized in this work were pre trained on gathered historical data.
it is also possible to have the armia models be updated online as new data is gathered by a system to adapt themselves as more information becomes available.
other methods for time series prediction such as recurrent neural networks rnns which have been successfully used for a variety of time series forecasting problems can be examined as well.
rnns may also potentially be able to incorporate nontime series data into predictions.
our evaluation data was created using two raspberry pi s and does not simulate a complicated system such as a self driving car or a ua v .
however this generated data was intended to help demonstrate the capabilities of tv a and the benefits of accounting for tactic volatility.
in reality the data generated by these tools could represent virtually any form of tactic volatility e.g.
the time required for a ua v to communicate with a base station or the energy cost of a self driving car performing a tactic .
there are a few potential limitations to the use of our v alet tool and dataset in the evaluation of our proposed tv a process.
first v alet generates its data by performing a small number of tasks.
a real world self adaptive system would likely perform a large number of tasks in any given adaptation which depending on the system could impact one another.
v alet is also limited in the forms of variability that it may encounter as opposed to a real world system.
for example v alet is significantly less likely to be actively targeted by human hackers than many real world self adaptive systems which could limit the encountered variability.
however it is somewhat unreasonable to expect that any created evaluation system would have the capabilities to address a majority of real world events and possibilities.
despite these possible limitations we are confident in the ability of v alet in creating satisfactory evaluation data for not only our study but for future research conducted by others.
viii.
c onclusion in this work we propose a tactic v olatility aware tv a process that is able to account for tactic volatility in multiple ways.
tv a first uses time series forecasting with an autoregressive integrated moving average arima model to monitor system specifications defined in the sla supporting the system in acting more proactively while maintaining them.
next tv a uses multiple regression analysis mra to predict tactic latency and cost helping to improve the decision making process.
our contribution also includes a tool that utilizes physical devices to create real world tactic volatility data.