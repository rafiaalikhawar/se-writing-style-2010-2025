detecting and fixing precision specific operations for measuring floating point errors ran wang daming zou xinrui he yingfei xiongy lu zhang gang huang key laboratory of high confidence software technologies peking university moe institute of software eecs peking university beijing china wangrancs zoudm hexinrui xiongyf zhanglucs hg pku.edu.cn abstract the accuracy of the oating point calculation is critical to many applications and di erent methods have been proposed around oating point accuracies such as detecting the errors in the program verifying the accuracy of the program and optimizing the program to produce more accurate results.
these approaches need a speci cation of the program to understand the ideal calculation performed by the program which is usually approached by interpreting the program in a precision unspeci c way.
however many operations programmed in existing code are inherently precision speci c which cannot be easily interpreted in a precision unspeci c way.
in fact the semantics used in existing approaches usually fail to interpret precisionspeci c operations correctly.
in this paper we present a systematic study on precisionspeci c operations.
first we propose a detection approach to detect precision speci c operations.
second we propose a xing approach to enable the tuning of precisions under the presence of precision speci c operations.
third we studied the precision speci c operations in the gnu c standard math library based on our detection and xing approaches.
our results show that a signi cant number of code fragments in the standard c math library are precision speci c operations and some large inaccuracies reported in existing studies are false positives or potential false positives due to precisionspeci c operations our detection approach has high precision and recall our xing approach can lead to overall more accurate result.
ccs concepts theory of computation !program analysis this work is supported by the high tech research and development program of china under grant no.2015aa01a203 and the national natural science foundation of china under grant no.
.
ycorresponding author.keywords floating point accuracy precision speci c operations.
.
introduction a well known problem in software development is the inaccuracies from oating point numbers.
since oatingpoint numbers can represent only limited number of digits many real numbers cannot be accurately represented leading to inaccuracies in computation.
in a critical software system inaccuracies from oating point numbers may cause serious problems.
a well cited example is that a patriot missile fails to intercept an incoming missile in the rst persian gulf war due to oating point inaccuracies.
given the importance of oating point accuracy many research e orts have been devoted to the accuracy problem of oating point programs.
these approaches detect whether a program can produce a signi cantly inaccurate output verify whether all errors may be produced by a program are within an upper bound or optimize the program so that the output is more accurate .
a key problem in implementing such an approach is to get the ideal output for an execution of a program.
in detection and veri cation approaches we need to know the ideal output so that we can get the error of the actual output by comparing it with the ideal output.
in optimization approaches we need to know the ideal output so that we can change the program to make the output close to the ideal output.
typically these approaches take a precision unspeci c semantics to interpret the program to get the ideal output.
that is oating point variables are interpreted as real numbers and oating point operations are interpreted as real arithmetic operations.
the outputs produced by the precision unspeci c semantics are treated as the ideal outputs and methods are used to approximate the ideal outputs in an execution.
one commonly used technique is precision tuning1.
that is the computation interpreted by the precision unspeci c semantics is executed in a high precision oating point format such as double long double or higher and the output is expected to be close enough to the ideal output.
for example several detection approaches use a precision that is double of the original precision to get an accurate output and the accurate output is compared with the original output to get the error on the original output.
1in this paper we use accuracy and precision di erently.
accuracy means how close the output is compared with the ideal output.
precision means how many bits are used in computing the output.
this is the author s version of the work.
it is posted here for your personal use.
not for redistribution.
the definitive version was published in the following publication fse november seattle wa usa c acm.
... artifact evaluated by fse 619however a program may contain precision speci c operations which would be di cult to be interpreted in a precision unspeci c way.
precision speci c operations are operations that are designed to work on a speci c precision.
for example let us consider the following program which is simpli ed from a code piece in expfunction in glibc.
double round double x double n .
return x n n the goal of the program is to round xto an integer and return the result.
the constant nis a magic number working only for double precision oating point numbers.
when a double precision number is added with n the decimal digits of the sum are all rounded o due to the limited precision.
when we subtract nagain we get the result of rounding x to an integer.
since the operation is precision speci c precision unspeci c semantics used in existing approaches usually cannot correctly interpret it.
in the aforementioned semantics the variables xandnwould be recognized as real numbers and the whole procedure would be interpreted as computing x n n which is mathematically equivalent to x. when existing approaches follow such a semantics they would produce undesirable results.
for example previously we mentioned that most detection techniques rely on precision tuning to get the error of the program.
when we compute this program in a higher precision e.g.
all variables are long doubles we would not be able to get the rounding e ect as long doubles have enough digits to accommodate the decimal digits.
as a result the higher precision would return xin most cases which is less accurate than the original program and these detection approaches would report an incorrect error when they compare the outputs from the high precision program and the original program.
to x this problem a direct method is to extend the existing precision unspeci c semantics so that such precisionspeci c operations are correctly interpreted.
for instance the above procedures should be interpreted as rounding x to integer rather than adding ntoxand then subtracting naway .
however it is di cult to make such an extension.
first it is di cult to capture all precision speci c operations.
the example shows only one form of precision speci c operation.
there may be other types of precision speci c operations such as adding a di erent constant subtracting a constant and perform bit operations.
it is di cult to enumerate all such forms.
second even if we can enumerate all such forms it is di cult to interpret all operations in a precision unspeci c way.
we need to understand the intention of the operations and map it to real numbers.
in the example program we need to understand its rounding behavior rather than interpreting it as adding a large constant.
third as can be seen from the example the semantics is not syntax directed.
we need to understand at least the valueow of the program to correctly interpret the program.
this makes the semantics di cult to de ne and to implement.
to deal with this problem in this paper we propose a lightweight approach to precision speci c operations.
particularly we focus on detection approaches based on precision tuning aiming to reduce the false positives caused by precision speci c operations.
first we propose a heuristic to detect precision speci c operations.
this heuristic is based on the observation when a precision speci c operationis incorrectly interpreted in precision unspeci c semantics executing the semantics in a high precision and in a low precision usually produce large relative errors except for the variables storing errors.
second we propose a xing approach to enable precision tuning under the presence of precision speci c operations.
the basic idea of the xing approaches is rather than trying to interpret precision speci c operations in a precision speci c way we always execute the operations in the original precision.
as our evaluation will show later the xing approach leads to overall more accurate output than both the original precision and the high precision.
based on our approach we performed a systematic study of precision speci c operations in the gnu c standard math library v2.
.
we used our approach to detect precisionspeci c operations in the library and to x them for highprecision execution.
we also manually evaluated the reported precision speci c operations and summarized them into patterns.
our study leads to several ndings a signi cant number of code fragments in the standard c math library are precision speci c operations and some large inaccuracies reported in existing studies are false positives or potential false positives due to precision speci c operations there are three main patterns of precision speci c operations namely rounding multiplication and bit operations our detection approach has high precision and recall our xing approach can produce overall more accurate result than both the original programs and programs with raised precision and the automatic x has a close performance to the manual x. the rest of the paper is organized as follows.
section presents related work.
section describes precision speci c operations and discusses their e ect on existing tools.
section demonstrates how to detect precision speci c operations under a certain precision unspeci c semantics.
section describes how to x precision speci c operations.
section shows our experiment on analyzing precision speci c operations and evaluation of our detection and xing approach.
section concludes the paper.
.
related work .
inaccuracy detection many approaches treat large inaccuracies as bugs and try to identify such bugs from programs.
one of the earliest approaches is fpdebug which estimates the error on the result of an execution.
fpdebug dynamically analyzes the program by performing all oating point computations side by side in higher precision and produces two results the result in high precision and that in low precision.
then the result in high precision is used as ground truth to calculate the error.
based on the approach of fpdebug several detection approaches have been proposed.
the goal of a detection approach is to locate a test input that could maximize the error on the output.
the current approaches all rely on search algorithms where typical algorithms include lsga and bgrt while lsga is reported to have a better performance than bgrt .
an approach that works di erently is the dynamic analysis proposed by bao and zhang .
this approach is designed to reduce the high overhead in fpdebug.
instead of executing in high precision it relies on a heuristic to detect large errors 620if an operation whose operands are large on exponents and the result is small on exponents the result probably contains large errors.
in this way only slight overhead is introduced to the original execution.
all these approaches interpret the program using similar precision unspeci c semantics which interprets oating point numbers as reals and oating point operations as real operations.
as a result these approaches could not correctly interpret precision speci c operations and may report large errors that do not exist in the program.
our detection approach could help identify the false large errors reported by these approaches.
our xing approaches could still report an accurate result under the existence of precision speci c computations allowing these approaches to continue to work.
as our evaluation will show later several inaccurate program reported in existing papers are false positive or potentially false positive due to precision speci c operations and our xing approach could help produce accurate results on these programs.
.
program optimization while increasing precision is commonly used for getting more accurate results reducing precision is commonly used in program optimization to get faster code.
lam et al.
rubio gonz alez and schkufza et al.
propose approaches that automatically reduce the precision of oatingpoint computation to enhance performance with an acceptable loss of precision.
these approaches typically try to locate typically by search algorithms variables in the program which have only limited e ect on the accuracy of the result when replaced by variables with lower precisions and replace these variables by lower precision ones.
since these approaches rely on precision unspeci c semantics they may incorrectly interpret precision speci c operations.
when a variable is involved in a precision speci c operation the variable probably cannot be selected by these approaches as the change of its precision would lead to signi cant change in the estimated accuracy of the result.
as a result all such variables would never be selected for optimization potentially reducing the e ectiveness of the optimization.
our xing approach could potentially be combined with these approaches to enable the selection of these variables the variables will be computed as low precision in most of the time and be computed in the original precision only during the precision speci c operations.
in this way we can gain the performance boost without losing much precision.
another optimization direction is to increase the accuracy of the computation.
darulova and kuncak propose an approach that automatically increases precision to guarantee the nal result meets the accuracy requirement.
panchekha et al.
propose an approach that searches the program fragments that cause signi cant errors and rewrite them into more accurate versions.
however these approaches still use precision unspeci c semantics and thus may rewrite precision speci c operations into an undesirable direction.
by combining this approach with our detection approach such cases could potentially be avoided.
.
verification a lot of e orts are put into veri cation techniques trying to verify whether the error produced by a program is always within a threshold.
typical approaches combinedata ow analysis with interval or a ne arithmetic to get the upper bound of the errors.
however these approaches also rely on precision unspeci c semantics thus may erroneously interpret precision speci c operations and report imprecise or unsound upper bound.
our approach could potentially be combined with these approaches to provide more informed result the precision speci c computations could also be reported to the users to help the users determine where the upper bounds may be imprecise or unsound.
besides full automatic veri cation several approaches have been proposed to facilitate manual veri cation for oatingpoint programs.
boldo et al.
build support for oatingpoint c programs in coq allowing one to easily build proofs for oating point c program.
ayad and march e propose the use of multiple provers to try to automate the proofs of oating point properties.
whether these approaches are a ected by precision speci c operations depends on how the veri ed properties are speci ed.
if a property is inferred from the program these approaches may also be a ected because the precision speci c operations may be erroneously interpreted.
.
external errors so far we only discuss internal errors which is about how much inaccuracy may be introduced during the execution of the program.
some approaches concern about external errors which is about the stability of the program how much the errors already carried on the input will be magni ed during the computation of the program.
recent work includes static veri cation of robustness by chaudhuri et al.
dynamic analysis by tang et al.
and dynamic sampling by bao et al.
.
these approaches are not a ected by precision speci c operations.
.
overview in this section we demonstrate how the correctness of fpdebug an existing approach we concerned particularly is a ected by precision speci c operations.
.
semantics of fpdebug here we informally describe the precision unspeci c semantics of fpdebug.
as introduced in section fpdebug is a dynamic analysis approach based on precision tuning for detecting the error and canceled bits produced in one execution of the program.
other detection approaches are either directly built upon fpdebug or are designed to behave similarly to fpdebug .
fpdebug implements precision tuning by instrumenting the binary code.
for each address that can be possibly accessed as a oating point variable in the program either on stack or on heap a shadow variable is created for the address.
the precision of the shadow variable can be speci ed by a parameter which is bit by default.
whenever a oatingpoint operation vo v1orvo v1 v2is performed where denotes an unary operator and denotes a binary operator the system rst checks whether the shadow variables of v1andv2are initialized and if so the same operation is performed in high precision where the result is stored in the shadow variable of vo.
if the shadow variable of v1or v2is not initialized the value of the corresponding variable is converted into the high precision to initialize the shadow variable.
when the program ends the di erence between 621the shadow variable of the output and the original output is taken as the error of the program.
a dedicated point is that the di erence between the shadow variables and the original variables may cause a conditional statement to take a di erent branch.
to enable side by side comparison of each statement fpdebug always takes the branch that is chosen in the original precision.
as we can see the semantics fpdebug uses to interpret oating point operations are precision unspeci c. the operations are interpreted as operations on real numbers and thus can be adapted to a di erent precision.
similarly the oating point numbers are interpreted as reals such that can be adapted to a di erent precision.
.
fpdebug on precision specific operations now we use two examples to show how fpdebug shall report false errors on precision speci c operations.
the rst example is the example program we have shown in section .
as explained in section the purpose of the program is to round xto an integer.
however when executed in fpdebug two shadow variables would be created for xand n respectively and a high precision computation for x n n would be performed.
since the high precision variables can represent a much more precise scope than the original precision the high precision computation would produce a result that is closer to the original xfor most inputs.
on the other hand the low precision would produce the accurate result of roundingxto an integer for most inputs.
as a result false errors would be produced when xis not an integer.
in the second example let us consider the following program which is also a typical example of precision speci c operations and is reduced from logfunction in glibc.
union double2int int i double d double test double2int x x.d calc x.i x.i 0xfffff 0x40000000 return x.d the purpose of the code is to get such that 2n jx dj xis the input of function test and is the output.
the union type double2int allows us to manipulate di erent bits of a double variable directly x.i represents the lower half of x.d x.i represents the higher half of x.d and bit operators can be directly applied to x.i andx.i .
the values of high half andlow half depend on the infrastructure of the machine.
the 0xfffff extracts the signi cand of double while 0x40000000 sets the exponent to and the sign bit to .
this operation is obviously precision speci c and is not easy to map to other precisions we need the knowledge of oating point format to properly map the constants to other precisions.
when executing this program in fpdebug fpdebug would create a shadow variable for x.d.
however the operation at line is not a oating point operation so the high precision computation on shadow variables would not be performed.
as a result when the function returns the original value is changed by line but the shadow value isnot changed leading to a false error reported by fpdebug.
in other words the operation at line is precision speci c and when fpdebug maps the operation to high precision it erroneously maps this operation to an empty operation which does nothing.
note that one may argue that the problem of this example is a bug in fpdebug implementation the implementation does not recognize the alias relation between x.dand x.i.
however although we know the shadow value of x.dshould be changed we do not know how to change it as the change operation is speci c to the original precision.
for example we need to construct constants to replace 0xfffff and 0x40000000 in other precision.
.
detecting precision specific operations .
precision tuning to describe the detection approach let us rst de ne precision tuning.
our de nition is independent of any particular precision unspeci c semantics so that our approach can be integrated into di erent detection tools with di erent semantics.
we assume an instruction set lcontaining all instructions that can be written in a program.
each instructionltakes the form vo f vi1 vin vois known as the output variable denoted by out l the setfvi1 vingis known as the input variables denoted by in l andfis a function computing the output from the input.
we do not consider jump instructions because we follow the side by side execution in fpdebug where the high precision program is always executed on the same path as the low precision program.
a precision tuning function l!l takes an instruction as input and produces an instruction in the high precision as output where for any l2l in l in l andout l out l .
for example in the rst example program in section line becomes two instructions at the binary level tmp x n andret tmp n .
the output variable of tmp x n istmp while the input variables are xand n. the precision tuning function of fpdebug maps these two instructions to the same operations in the high precision.
in the second example program in section .
line also becomes two instructions.
since fpdebug fails to correctly capture the two instructions we consider fpdebug maps the two instruction to void instructions which do nothing.
next we describe how to estimate the error on a variable by comparing values from the original and the high precision executions.
we adopt the same side by side execution model as fpdebug the high precision execution always follows the same path as the original execution.
let lbe an instruction in a program vobe the value of out l after the execution of lin the original execution and vhbe the value of out l after the execution of l in the high precision execution.
the estimated relative error or shortly error of output variable out l atlis vh vo vh .
similarly let x2in l be an input variable of l vobe the value of xbefore the execution of lin the original execution and vhbe the value of lbefore the execution of l in the high precision execution.
the estimated relative error of input variable xatlis vh vo vh .
for example in the aforementioned two instructions the error of output variable tmpattmp x n is calculated by 622comparing the values of tmpin the two executions after executing the instruction while the error of the input variable xis calculated by comparing the values of xin the two executions before executing the instruction.
.
the heuristics .
.
possibly precision specific operations given two thresholds eandp we detect precision speci c operations based on a heuristic.
the basic heuristic is as follows which is based on executing the program on a large number of input values.
de nition .
letlbe an instruction eibe the maximum error among the input variables of latlin an execution andeobe the error of the output variable at lin the same execution.
we say lispossibly precision speci c ifeo ei e on at least p of the executions.
in other words if the error in ates signi cantly after an operation for many inputs we consider the operation possibly precision speci c. we explain the intuition of the heuristic from two aspects.
first precision speci c operations usually meet this criterion.
recall our goal is to reduce the false errors caused by precision speci c operations.
if a detection approach reports a false error due to a precision speci c operation the precision speci c operation must have been interpreted incorrectly by the precision unspeci c semantics.
as shown by our examples when the precision speci c operation is incorrectly interpreted the high precision program is usually largely di erent from the original program and thus will cause a signi cant in ation of the error.
for example in the rst example program in section the operation rounding to integer is interpreted as an identity transformation.
as a result the original program rounds xto an integer while the high precision program directly returns x. unless xis a very large number or xis exactly an integer a signi cant relative error will be reported.
in the second example program in section the rst instruction at line tmp x.i 0xfffff is interpreted as a void operation.
as a result tmpis assigned in the original program but remains a random uninitialized value in the high precision program which leads to large errors in most cases.
second normal operations seldom meet the criterion.
as analyzed in many existing papers there are two possibilities of large errors in normal operations accumulation and cancellation.
accumulation indicated that errors propagate through a sequence of operations and are accumulated on variables in those operations.
though the error from each operation is small the accumulated error can be large.
cancellation indicates that the subtraction of two close numbers where many signi cant digits will be subtracted o may cause large errors.
obviously accumulation cannot occur in one operation and thus the only possibility to cause large error in ation by one operation is cancellation.
however when a cancellation causes large errors the input variables need to be close to each other and at least one of the operands contains rounding errors.
among the whole input space such input pairs only consist of a small portion.
therefore an operation that is not precision speci c would probably not cause cancellation on many inputs.
.
.
probably precision specific operations while the heuristic works for most of the time it would incorrectly identify a normal case as precision speci c the calculation of errors.
many algorithms that aim to produce an accurate result use an extra variable to store the error.
given an operation z f x y the oating point computation resultz0is usually di erent from the ideal result zdue to the rounding errors.
to make the nal result more accurate a typical accurate algorithm uses an additional operation gto get the error ezonz0 whereez g x y z0 .
though we cannot store the accurate value of zas a oating point number we can usually store the error ezaccurately as a oating point number.
so this error is stored as a separate oating point number and will be used to correct the nal result or estimate the error of the nal result later.
for example dekker proposes an algorithm to get the error called correction term in the addition operation.
suppose we are going to calculate z x y. the error can be calculated as ez x x y yin oating point operations.
this kind of correction is widely used in real world programs such as the c standard math library.
however given an operation and its high precision form the errors produced by the two operations are usually largely di erent.
the estimated error on the output is likely to be much larger than the errors on the input.
as a result if we rely on de nition we may incorrectly identify such operations as precision speci c operations.
to overcome this problem we further introduce a negative condition to lter out these false detections.
when a variable is used to store errors the errors are usually very close to zero and we can use this feature to lter out these false detections.
given three thresholds vo vh andq we have the following de nition.
de nition .
a possibly precision speci c instruction lis probably precision speci c if on at least q of the executions the output value of the instruction is smaller than voat the original precision and smaller than vhat the high precision.
.
detection algorithm based on the heuristic it is easy to derive our detection algorithm to detect precision speci c operations.
the algorithm consists of the following steps.
sampling.
we rst sample a large number of input values to execute the program.
we sample the space of each parameter within a xed interval.
the purpose of sampling within a xed interval rather than random sampling is to evenly cover the input space.
execution.
we execute the program with the input values.
the high precision program is executed sideby side with the original program.
at the execution of each oating point operation we record the estimated relative errors of the input and the output variables.
data filtering.
we calculate the error in ation by each instruction execution lin an execution and once we identi ed an in ation larger than e we lter out all data collected from the rest of this execution.
this is becauselmay be a precision speci c operation and thus the later operations may be a ected by the error propagated from that operations.
the relative errors of later operations may be much di erent from a normal execution.
detection.
we analyze the recorded data and determine the probably precision speci c operations based on de nition .
note that since we lter out all instruction executions after a large error in ation we may only detect the precision speci c operations that are executed early on the sampled inputs.
fixing and repeating.
we use the method introduced in the next section to x the precision speci c operations and restart the process to detect more precisionspeci c operation.
.
fixing precision tuning in this section we describe how to x precision tuning for precision speci c operations.
our idea is to reduce the precision of the variables right before the precision speci c operation and raise the precision right after the precisionspeci c operation.
in this way the precision speci c operation is still performed in the original precision while most operations are performed in the high precision so the overall result would be more precise than both the original precision and the high precision without the x. please note that a precision speci c operation may be detected by our approach at one instruction but the operation itself may not contain only this instruction.
in out rst example the operation contains two instructions tmp x n and ret tmp n but our detection approach would report only at the latter instruction.
to mark the boundaries of the operations we introduce two xing statements ps begin v vn and ps end v vn .
the programmers could use the two statements to precisely mark the boundary of a precision speci c operation.
the variables v vnare variables that would be used in the precision speci c operation whose precision will be reduced at ps begin and be resumed at ps end .
it is also desirable to make the insertion of the xing statements automatically.
we note that for many precisionspeci c operations reducing the precision at the correct boundaries is the same as reducing the precision at the last instruction.
for example in the rst example if we perform tmp x n in the high precision and reduce the precision of tmpafter the statement the decimal part of x tmp will still be rounded o .
since our detection approach can correctly detect the last instruction in a precision speci c operation automatically inserting ps begin and ps end around the detected instruction would also produce the desirable result in many cases.
therefore we also implement an approach that automatically inserts the two statements.
as will be shown later in the evaluation with a proper ein the detection approach the automatic x can achieve a close performance to the manually inserted ones.
.
ev aluation our experiments are designed to answer the following research questions.
rq1 how many precision speci c operations are detected and what are the main patterns?
rq2 how precise and complete is our detection approach?
rq3 how di erent are possibly and probably precisionspeci c operations?
rq4 how e ective is our xing approach?
rq5 are the errors reported in existing studies affected by precision speci c operation?
.
implementation we have implemented our approach by modifying the fpdebug code.
basically we reused the same instrumentation mechanism of fpdebug to implement the precision tuning and added additional statements for xing precision tuning which include manipulations of di erent precision values.
we also instrumented code to track the nonoating point operation of a union at the high precision which was not tracked by fpdebug.
to precisely analyze our results we used mpfr a c library for multiple precision oating point computations with correct rounding.
in this way we can ensure the relative errors are accurately calculated without signi cant oating point errors.
note that fpdebug also uses mpfr for precision tuning and relative error calculation.
our implementation and all experimental data are available at the project web site2.
.
experimental setup .
.
subjects we evaluated our approaches on the functions of the scienti c mathematical library of the gnu c library glibc version .
.
most math functions in the gnu c library contain three versions for oat double and long double.
a version for a particular precision takes input values at that precision performs the calculation at that precision and produces the result at that precision in most cases.
since many implementations for di erent versions are similar we take only the double version which includes functions.
the subjects cover calculation such as trigonometric functions inverse trigonometric functions hyperbolic functions logarithmic functions exponentiation functions etc.
each function takes double precision oating point or integer input parameters and produces a double precision oatingpoint or integer output3.
a more detailed description of the subjects is available on the project web site.
.
.
oracles to understand the performance of our xing approach we need to know the ideal output values of the functions at a given input.
to get such ideal output values we re implemented the subject functions using a calculator with controlled error .
with a speci ed accuracy requirement e.g.
the digits should be accurate at least for two decimal places the calculator guarantees that the result meets the accuracy by dynamically raising the precision when necessary.
since di erent operations require di erent calculation methods and some are yet unknown the calculator supports only a limited set of operations.
using these operations we implemented in total glibc mathematical functions which are listed in table .
the second column shows how we encode the function using the operations provided by the calculator.
as we can see from the table the functions are the most commonly used math functions of glibc.
oatfeather pso 3function s fmaf takes oat as input and output but the computation is done with double so we still include it in our subjects 624table formulas for scienti c functions in glibc.
function formula acos arccos a acosh ln a a2 asin arcsin a asinh ln a a2 atan arctan a atan2 arctan a c atanh1 ln a a cos cos a cosh ea e a 2exp ea exp2 2a exp10 10a log ln a log2ln a ln log10 log a log1p ln a pow ac sin sin a sinh ea e a 2tan tan a tanh ea e a ea e a .
.
parameters we need to set the thresholds in our detection approach e p vo vhandq .
to set the thresholds we perform a preliminary study on function exp.
the result suggests that changingehas some e ects on the result while changing the other thresholds have little e ect.
as a result we set ve values to e 108and .
for the other thresholds we set them with xed values p with vo with vhwith andq with .
we also need to set the accuracy requirement of the calculator with controlled error.
we require the result to be accurate for at least decimal places which are guaranteed to cover the dynamic range of double precision oating point numbers.
we also need to set the interval for sampling the input space.
since each function has a di erent input space we determine the interval based on the size of the input space where each function is sampled to times.
.
.
procedures we ran our tool to detect precision speci c operations on all subjects.
note that in the detection approach we need to x all detected operation to discover more operations.
we used the automatic x in the detection process.
so in the end we already have all operations discovered and xed.
we repeated the process ve times setting the threshold ewith ve di erent values.
next we reviewed all precision speci c operations discovered in all experiments and determined the false positives.
then we manually inserted xing statements for all true positives and ran the xed functions again for all inputs to get the result of the manual x. finally we review the inaccuracies reported by two existing papers and check whether the result may be a ected by the precision speci c operations we detected.
our detection approach used 9819s to analyze all functions in average 205s for one function.table precision speci c operations per function function number function number function number acos acosh asin asinh atan atan2 cos cosh erf erfc exp10 exp2 exp gamma j0 j1 lgamma log pow sin sincos sinh tan y0 y1 table precision speci c operations per les function number function number easin.c eatan2.c eexp2.c eexp.c epow.c elog.c satan.c ssin.c stan.c dosincos.c .
results .
.
rq1 how many precision specific operations are detected and what are the main patterns?
in the tested functions there are functions detected to contain precision speci c operations.
the functions are listed in table where the number column shows the number of precision speci c operations detected when testing the function.
in total precision speci c operations are detected.
note that di erent functions may call the same internal functions so there are duplicated operations among di erent functions.
after excluding duplicated operations we have unique probably precision speci c operations which spread over les as shown in table .
this result indicates that precision speci c operations are widely spread in the c standard math library.
note that functions in the c standard math library are basic building blocks of other applications performing oating point computation so any applications calling these functions are a ected by precision speci c operations.
we also take a look at c math library functions with the two di erent precisions oat and long double.
we found that when the implementation algorithms for di erent versions are similar similar precision speci c operations can also be found in the other versions.
by manually reviewing all probably precision speci c operations we identify three main patterns namely rounding multiplication and bit operation.
rounding.
the rounding pattern takes the form of x x n n wherexis a variable and nis a constant.
we have already seen an example of the rounding pattern in the example in section .
when applying the pattern di erent ncan be used to round xto di erent levels.
generally the larger the nis the more bits are shifted o .
a list of constants we found in the c math library is listed in table .
multiplication.
the multiplication pattern takes the following form where nis a constant and ais a variable.
t n a a1 t t a a2 a a1 625table values of constants.
constant name value type big 245rounding big 236rounding bigu 10rounding bigv 219rounding threep42 241rounding three33 234rounding three51 252rounding toint 252rounding two52 252rounding t22 222rounding t24 224rounding cn multiplication table detecting probably precision speci c operations without duplicate .
threshold e rounding multiplication bit operation false positive total precision recall this block of code splits ainto two components a1anda2 using the constant n. later the two components can be used to calculate an accurate multiplication of ausing an existing algorithm .
this operation is precision speci c because nneeds to be determined based on the precision of a. the value of nfor double precision is also listed in table which is identi ed from the detected precision speci c operations in our experiments.
bit operation.
the bit operation pattern treats a oatingpoint number as an integer and apply bit operators to the integer.
we have seen an example of bit operation in the example program in section .
.
since the manipulation of bits is related to the format of the speci c precision the operations are naturally precision speci c. the identi cation of the three patterns from the operations show that there may be a limited number of patterns for precision speci c operations showing the hope of improving the precision unspeci c semantics to handle these cases.
however the three patterns are identi ed only from the c math library and we do not know how complete the pattern set is.
furthermore we also do not see easy ways to interpret these patterns precision unspeci cally.
.
.
rq2 how precise and complete is our detection approach?
the result of the detected probably precision speci c operations is shown in table .
we show the numbers of each pattern false positives the total numbers and the precisions for each threshold ewithout duplicates.
we make the following observations from the table.
our approach has an overall high precision.
at the threshold our approach has a precision of .
.
a possible reason for the false positives as we suspect is the insu cient sampling at some instructions.
there are instructions at which only a few inputs can reach table detecting possibly precision speci c operations without duplicate .
threshold e rounding multiplication bit operation false positive total precision recall and we have to determine whether they are precisionspeci c based on the few executions which are not statistically signi cant.
our approach has an overall high recall.
at the thresholds 104and106 our approach achieves a recall of .
the threshold ehas a signi cant e ect on the precision and recall.
the higher the threshold the higher the precision but lower the recall.
threshold 106is superior to 104in both recall and precision and no other threshold can be completely superior to another threshold on both precision and recall.
note that when calculating the recall we treat the detected precision speci c operations on all threshold as the complete set.
theoretically there can be precision speci c operations that are not detected at any threshold in our approach.
however as will be shown later in table our approach achieved very small relative errors compared to the oracles after we xed all identi ed precision speci c operations so it is likely that we have identi ed all precision speci c operations that cause large false errors.
.
.
rq3 how different are possibly and probably precision specific operations?
this question concerns about the usefulness of the negative condition we used to determine probably precision speci c operations.
table shows the result of detecting possibly precision speci c operations which does not use the negative condition to lter the operations calculating errors.
by comparing table and table we can see that the negative condition is very e ective in ltering out false positives where the precision has an increase up to the negative condition did not lter out any true positives as the recalls between the two tables are the same for all thresholds.
the result indicates the negative condition is e ective in improving the performance of our approach.
.
.
rq4 how effective is our fixing approach?
table shows the average relative errors of each function after xing precision tuning for the detected precision speci c operations.
here we show the result of both automatic x and manual x where the automatic x is performed based on the detection at four ethresholds .
we do not include 104because it is inferior to 106on both precision and recall.
note that there are three factors that a ect the precision of the automatic x. precision the lower the precision the more false positives are identi ed leading to more instructions unnecessarily executed in low precision.
recall when we missed a precision speci c operation failing to x 626table average relative error to standard value.
functions with have precision speci c operations.
op original precision.
hp high precision.
fod x only the detected instruction on true positives.
subject automatic fixing manual fod op hp e 106e 107e 108e acos 6106e 6106e 8999e 7265e 7265e 7265e 1048e 7265e acosh 0333e 0333e 0333e 0333e 0329e 0333e 3585e 5122e asin 0898e 0898e 0898e 0898e 0898e 0898e 9899e 0898e asinh 3004e 3004e 3004e 3004e 3000e 3004e 3561e 5351e atan2 3947e 3947e 3947e 3947e 3947e 3947e 0843e 3947e atan 0231e 0231e 0231e 0231e 0231e 0231e 5873e 0231e atanh 3868e 3868e 3868e 3868e 3868e 3868e 9363e 3868e cos 0255e 3145e 3145e 3145e 3145e 3145e 7273e 5843e cosh 2901e 2901e 2901e 2901e 2901e 2901e 4157e 5417e exp10 4842e 4842e 4842e 4842e 4842e 4842e 9779e 2630e exp2 0300e 0300e 0300e 0300e 0300e 0300e 1163e 2491e exp 9083e 9083e 9083e 9083e 9083e 9083e 1266e 5402e log10 1172e 1172e 1172e 1172e 1172e 1172e 2657e 1172e log1p 2697e 2697e 2697e 2697e 2697e 2697e 2729e 2697e log2 3510e 3510e 3510e 3510e 3510e 3510e 3646e 3510e log 6195e 6195e 6195e 6195e 6195e 6195e 0822e 0916e pow 5147e 5147e 6663e 6663e 5188e 5188e 7539e 8444e sin 7354e 7354e 7354e 7354e 7354e 7354e 5656e 2733e sinh 7909e 7909e 7672e 7672e 7672e 7672e 6532e 3316e tan 7135e 7135e 7135e 7135e 7135e 7135e 8932e 4335e tanh 5412e 5412e 6890e 6890e 6890e 6890e 9692e 6890e table sign test for e 107and manual xing.
functions with have precision speci c operations.
n how many times the function is executed.
o original precision.
h high precision.
o h the number of executions that are more accurate than the original precision high precision .
e 107manual function n o o p value h h p value o o p value h h p value acos 87e 30e 87e acosh 63e 78e 63e 78e asin 54e 37e 54e 37e asinh 19e 05e 19e 05e atan2 00e 00e atan 54e 54e atanh 46e 46e cos 00e 00e 00e cosh 67e 67e 67e 67e exp10 33e 33e 33e 33e exp2 87e 87e 87e 87e exp 33e 33e 33e 33e log10 06e 06e log1p 91e 91e log2 74e 74e log 00e 00e 00e pow 25e 00e 00e sin 00e 00e 00e sinh 12e 83e 18e 65e tan 07e 38e 07e 69e tanh 52e 25e 21e this operation may lead to signi cant errors.
boundaries the automatic x cannot correctly detect the boundary but only one instruction which may lead to instructions that should be executed in the original precision still executed in the high precision.
the e ects of the rst two factors can be seen by comparing the results from di erent thresholds but it is not easy to distinguish the last factor.
to understand the e ect of the last factor we further perform a x which xes only the detected instruction in all true positives fod column .
furthermore as the baselines for comparison we also list the errors produced by the original precision op column and those by the high precision hp column .
from the table we can make the following observations the results from the manual x are much more accurate than both the original precision and the high precision.
on all functions the average errors from the manual x are the smallest which are usually smaller than the original precision several orders of magnitude and are greatly smaller than the high precision on many functions with precision speci c operations.
the results from automatic x are also more accurate than the original precision and the high precision in the vast majority of cases.
at a proper threshold the automatic x can have a close performance to the manual x. for example for threshold on all of the functions except pow the error is close to the 627table inaccuracies reported in previous work.
function reported error actual error paper exprel 85e 25e synchrotron 35e 24e synchrotron 67e 97e equake around 00e manual x. the error of powis mainly because of relatively low recall under the high threshold .
when the detection precision decreases the error of automatic x may increase because of excessive xes.
for example the error of tanh increases when the threshold moves from 108to .
when the detection recall decreases the errors of automatic x may increase noticeably because of missing xes.
for pow a precision speci c multiplication is ignored when thresholds are 108and which shows a signi cant increase in error.
fixing only detected instruction and xing all instructions have almost identical performance.
only slight di erence is observed from the function asinh and acosh.
precision speci c operations may not always have a signi cant e ect on the result.
for example acos contains precision speci c operations but the error from the high precision is the same as the manual x. this is because although large errors may be produced in the precision speci c operations these errors may not have a large e ect on the nal result.
for example the result of precision speci c operation is converted to an integer.
in the semantics the shadow value of the integer is not tracked.
the error of precision speci c operation is lost which dramatically produces a right result.
the average errors cannot show on how many executions one approach is superior to another.
to further get this information we perform a sign test between the errors from the xed executions and the errors from the original executions and high precision executions.
we chose the threshold at for automatic x because 107is a balance between precision and recall.
the result is shown in table .
we can see that the manual x outperforms both the original execution and the high precision execution and all results are signi cant because pis much smaller than .
note somep value cannot be calculated because there are too few positive and negative instances .
we can also see that the automatic x outperforms the high precision execution in all functions and outperforms the original execution in all but the powfunction.
a further investigation reveals that the detection approach detects a false positive where the detected instruction corrects the result with a stored estimated error.
when we reduce the precision on this instruction we will use a high precision error to correct a low precision result leading to the inaccurate result.
.
.
rq5 are the errors reported in existing studies affected by precision specific operations?
we reviewed the inaccuracies reported in two existing publications and checked whether they are possibly falsepositives caused by precision speci c operations.
for each program reported to be inaccurate in the publications we investigate the source code of the program and collect the c math functions called by the source code.
if a function contains precision speci c operations detected in our experiments we determine the program as a potential false positive.
then we try to run the program using the input speci ed in the publication but with the precision speci c operations xed and check whether the new error is signi cantly smaller than the reported error.
the result is shown in table .
in total we con rmed three inaccuracies reported by zou et al.
are false positives.
we also identify a potentially false positive in the errors reported by bao et al.
.
the equake function is reported to be inaccurate but this function calls sinand cos both containing precision speci c operations so the inaccuracy might be caused by precision speci c operations.
however we cannot con rm this because bao et al.
did not report what input values are used to trigger the large errors in their experiments.
though a range of a single number is reported but the input to the equake function is a matrix and it is not clear for which number in the matrix the range is speci ed nor the size of the matrix.
this result implies that precision speci c operations may have a non trivial impact on existing detection approaches and need to be treated properly to get more precise results.
.
threats to validity the main threat to internal validity lies within our identi cation of the true and false positives in the detected operations.
we have to manually read the code to identify whether an operation is precision speci c or not.
to reduce this threat we performed an additional process to validate the true positives.
for each identi ed true positive we removed the xing statements for this operation and ran the functions again for the inputs to see whether there is a signi cant increase in errors for most inputs.
the result indicates our manual identi cation is correct.
the main threat to external validity is the representativeness of our subjects.
the subjects are all from glibc and it is not clear how much the results can be generalized to other subjects.
nevertheless glibc is one of most widely used open source oating point library and has been contributed by a lot of developers.
thus the precision speci c operations in glibc should cover a large number of patterns in writing precision speci c operations.
.
conclusion in this paper we have presented a study around precisionspeci c operations.
the study shows that precision speci c operations are widely spread and if not handled properly may cause existing approaches to act incorrectly.
as we analyzed a lot of existing approaches may be a ected by precision speci c operations and some inaccuracies reported in existing papers are false positives or potentially false positives due to precision speci c operations.
nevertheless we show that precision speci c operations can be detected based on a simple heuristic.
also though in general it is di cult to interpret precision speci c operation correctly we can have a lightweight solution that always execute the precisionspeci c operations in the original precision.
as our results show this lightweight solution successfully enables precision tuning under the presence of precision speci c operations.
.
artifact description .
materials the artifact is a replication package for our experiment consisting of the following materials.
.a tutorial on subject selection experiment con guration and how to run experimental scripts.
.a virtual machine image with all tools and subjects installed.
.open source tools the experiment depends on such as fpdebug mpfr.
.experimental scripts that instrument subjects detect and x precision speci c operations in subjects and analyze the results.
.experimental data like instrumented subjects discovered precision speci c operations.
the artifact is published under mit license the open source tools and subjects are published under their original licenses and can be downloaded4.
.
tutorial the tutorial provides step by step guidance for reproducing our experiments.
the tutorial covers two scenario the virtual machine and manual installation on a new machine.
the former is much simpler and is recommended.
the later involves the installation and con guration of several tools and the change of scripts and is only recommended if you plan to modify our code for new tools and experiments.
.
virtual machine image the virtual machine has tools installed fpdebug gmp mpfr and con gured.
the two versions of instrumented glibc are also installed.
the scripts and data are also placed in the virtual machine so that rerunning the experiment only needs a few commands which are described in the tutorial.
.
tools our experiments depend on several open source tools.
we use fpdebug to tune precision.
fpdebug further relies on a modi ed mpfr a c library for multiple precision oatingpoint computations with correct rounding for more accurate computations.
llvm and clang are used to instrument the subjects.
all these tools are installed on the virtual machine except llvm and clang.
.
scripts we provide scripts to run the experiment including instrumentation the detection and xing approach and analysis for the results.
here we brie y introduce how to replicate our experiment in the virtual machine.
the experiments are implemented as two steps.
the rst is to detect and x precision speci c operations.
the second is to analyze the results.
invoking the following command would perform the rst step.
oatfeather pso tree master artifactcd home artifact work exe art .
run 2.sh the second step is implemented as four di erent scripts.
the rst script analyzes the precision and recall of the detection approach and can be invoked using the following commands.
cd home artifact work exe art g errorprocess.cpp o errorprocess .
errorprocess the rest three scripts analyze respectively for the three types of xing in our experiment and can be invoked by the following commands.
cd home artifact work base art .
run auto.sh .
run manual.sh .
run fixlast.sh the results will be stored in result auto e.csv result manual.csv and result fixlast.csv .estands for the parameter eused in our detection approach.
.
data the data include subject functions and other data needed for the experiments such as accurate outputs for each subject function.
the data also include a list of precision speci c operations found in the double version of the gnu c math library.
the tutotial gives a more detailed description of the data.
.