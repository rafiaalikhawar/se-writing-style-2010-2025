singapor e management univ ersity singapor e management univ ersity institutional k nowledge at singapor e management univ ersity institutional k nowledge at singapor e management univ ersity resear ch collection school of computing and information systems school of computing and information systems cerebro context awar e adaptiv e fuzzing for eff ectiv e vulner ability cerebro context awar e adaptiv e fuzzing for eff ectiv e vulner ability detection detection yuekang li yinxing xue hongxu chen xiuheng wu cen zh ang see next page for additional authors follow this and additional works at https ink.libr ary.smu.edu.sg sis r esear ch part of the os and networks commons and the softwar e engineering commons citation citation li yuekang xue yinxing chen hongxu wu xiuheng zh ang cen xie xiaof ei w ang haijun and liu yang.
cer ebro context awar e adaptiv e fuzzing for eff ectiv e vulner ability detection.
.
proceedings of the 27th a cm joint meeting on e uropean softwar e engineering conf erence and symposium on the f oundations of softwar e engineering t allinn est onia a ugust .
.
available at available at https ink.libr ary.smu.edu.sg sis r esear ch this conf erence pr oceeding ar ticle is br ought t o you for fr ee and open access b y the school of computing and information systems at institutional k nowledge at singapor e management univ ersity .
it has been accepted for inclusion in resear ch collection school of computing and information systems b y an authoriz ed administr ator of institutional k nowledge at singapor e management univ ersity .
for mor e information please email cher ylds smu.edu.sg .
author author yuekang li yinxing xue hongxu chen xiuheng wu cen zh ang xiaof ei xie haijun w ang and y ang liu this conf erence pr oceeding ar ticle is a vailable at institutional k nowledge at singapor e management univ ersity https ink.libr ary.smu.edu.sg sis r esear ch cerebro context aware adaptive fuzzing for effective vulnerability detection yuekang li university of science and technology of china china nanyang technological university singaporeyinxing xue university of science and technology of china chinahongxu chen nanyang technological university singapore xiuheng wu nanyang technological university singaporecen zhang nanyang technological university singaporexiaofei xie nanyang technological university singapore haijun wang nanyang technological university singaporeyang liu nanyang technological university singapore zhejiang sci tech university china abstract existing greybox fuzzers mainly utilize program coverage as the goal to guide the fuzzing process.
to maximize their outputs coveragebased greybox fuzzers need to evaluate the quality of seeds properly which involves making two decisions which is the most promising seed to fuzz next seed prioritization and how many efforts should be made to the current seed power scheduling .
in this paper we present our fuzzer cerebro to address the above challenges.
for the seed prioritization problem we propose an online multi objective based algorithm to balance various metrics such as code complexity coverage execution time etc.
to address the power scheduling problem we introduce the concept of input potential to measure the complexity of uncovered code and propose a cost effective algorithm to update it dynamically.
unlike previous approaches where the fuzzer evaluates an input solely based on the execution traces that it has covered cerebro is able to foresee the benefits of fuzzing the input by adaptively evaluating its input potential.
we perform a thorough evaluation for cerebro on different real world programs.
the experiments show that cerebro can find more vulnerabilities and achieve better coverage than state of the art fuzzers such as afl and aflfast.
ccs concepts security and privacy vulnerability scanners .
corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august tallinn estonia association for computing machinery.
acm isbn .
.
.
.
fuzz testing software vulnerability acm reference format yuekang li yinxing xue hongxu chen xiuheng wu cen zhang xiaofei xie haijun wang and yang liu.
.
cerebro context aware adaptive fuzzing for effective vulnerability detection.
in proceedings of the 27th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august tallinn estonia.
acm new york ny usa pages.
.
introduction fuzzing or fuzz testing is progressively gaining popularity in both industry and academia since proposed decades before .
various fuzzing tools fuzzers have been springing up to fulfill different testing scenarios in recent years .
these fuzzers can be classified as blackbox whitebox and greybox based on the awareness of the structural information about the program under test put .
blackbox fuzzers have no knowledge about the internals of put.
so they can scale up but may not be effective.
on the contrary whitebox fuzzers utilize heavy weight program analysis techniques e.g.
symbolic execution tree to improve effectiveness at the cost of scalability.
to have the best of both worlds greybox fuzzers gbfs such as afl are advocated to achieve scalability yet effectiveness.
fig.
depicts the workflow of greybox fuzzing.
a recent trend in academia is to make greybox fuzzing whiter with various light weight program analysis.
for example vuzzer steelix and angora mainly help gbfs to penetrate path constraints via modifications on the seed mutator andfeedback collector modules in fig.
.
however based on the nature that fuzzing s results are strong related with the seeds1 the effects of all the works on these modules can be further maximized by enhancing the seeds 1in this paper we denote allthe files fed to the put by fuzzers as inputs and only those inputs kept by fuzzers for subsequent mutations as seeds .
533esec fse august tallinn estonia y. li y. xue h. chen x. wu c. zhang x. xie h. wang and y. liu quality.
to be more specific as shown in the fig.
gbfs need to deal with two fundamental problems how to select the next seed to fuzz seed prioritization and how many new inputs need to be generated with the selected seed power scheduling .
in most gbfs new inputs are generated by mutating the seeds.
if an input exercises a new path it is retained by the gbf as a new seed otherwise it is discarded.
in such a manner the gbf maintains an increasing set of seeds.
to maximize the number of bugs detected within a limited time budget the gbf needs to wisely put the seeds in order by prioritizing the seeds with better quality.
we call this seed prioritization problem.
after seed prioritization the gbf needs to decide for each seed the number of new inputs a.k.a.
energy in aflfast to be generated from that seed.
ideally the gbf should allocate more energy to a seed that brings more benefits via mutations.
we call this power scheduling problem.
some research endeavors have been made on the two problems .
among these studies shudrak et al.
propose to use software complexity to facilitate the seed prioritization for fuzzers based on the assumption that complex code is more error prone.
in aflfast b hme et al.
address both problems by prioritizing the seeds that exercise rarely executed paths in hope that fuzzing such seeds can achieve more coverage rapidly.
despite these efforts two challenges remain to be addressed.
the first challenge is whatever information paths rarity or code complexity about the execution traces are utilized existing gbfs are not aware of the uncovered code close to the execution trace i.e.
context .
lacking such knowledge is because that context awareness normally requires heavy weight program analysis techniques which can hinder the performance of gbfs.
however context awareness can be very helpful in the fuzzing process e.g.
if the neighboring code around an execution trace gets covered then mutating the seed holding this trace becomes less beneficial as the potential of leading to new coverage has dropped.
the second challenge is existing fuzzers either utilize a single objective for seed prioritization or mix several objectives via linear scalarization into one single objective a.k.a.
weighted sum to perform seed prioritization.
on one hand using one single objective may cause bias and starve certain seeds.
on the other hand the weights used in linear scalarization are empirically decided without statistical justifications.
in this paper we propose cerebro to make proper decisions for seed prioritization and power scheduling by addressing the above challenges.
cerebro focuses on the seed evaluator andseed queue in fig.
.
to bring context awareness to gbfs we propose a new definition named input potential the complexity of not yet covered code near the execution trace of the input together with an online algorithm to efficiently calculate the potentials of inputs with the ever changing context.
to balance multiple important but conflicting objectives we propose a multi objective optimization moo model together with a nondominated sorting based algorithm to quickly calculate the pareto frontier for a growing set of seeds.
in cerebro the moo model is applied to perform seed prioritization and input potential is applied to facilitate power scheduling.
we implement cerebro and evaluate it with widely used real world programs from different projects.
cerebro outperforms aflfast and afl with significantly improved bug detection capability while maintaining a good coverage.
moreover we find 14previously unknown bugs in mjsandxed and cve in radare2 .
besides all the new bugs have been confirmed and fixed.
the contributions of the paper are as following we formulate a new concept input potential which represents the complexity of uncovered code close to an execution trace.
we also propose a cost effective algorithm to quickly calculate the input potential to facilitate power scheduling.
we propose a multi objective based model together with an efficient sorting algorithm for seed prioritization.
we implement cerebro and evaluate its effectiveness with experiments from several aspects.
the results are promising as we discover previously unknown bugs and cve in widely used open source projects and all these bugs are confirmed and fixed.
background problem statement .
background of greybox fuzzing the workflow of greybox fuzzing is shown in fig.
.
the seed evaluator in the gbf first tries to select a seed from the queue and the selected seed is called prioritized seed .
then the seed evaluator calculates a power schedule for the prioritized seed to determine the number of new test inputs to be generated from that seed energy .
the prioritized seed together with its power schedule are then passed to the seed mutator to mutate and generate new test inputs.
then the executor executes the put with the mutated input.
after execution the feedback collector collects the runtime information such as edge coverage and helps to decide whether the test input should be kept as a new seed or not.
alternatively if the test input causes the put to crash then it is kept as a proof of crash poc .
.
problem statement definition seed prioritization .
given a set of seeds s seed prioritization is to select a set of prioritized seeds ass swhich is a solution of the optimization problem minf s s.t.f s s so m1 mk where o m1 mk is an objective function in practice this usually denotes a cost each mi i k denotes a metric that measures the quality of a seed.
existing fuzzing tools such as afl typically apply a scalarized objective function in the form of o m1 mk k i 1mi s and mimay measure the execution time file size number of covered edges etc.
definition power scheduling .
given a set of seeds sto be fuzzed and a duration of nepochs a power schedule forsis to assign an epoch number xsfor each input s s maxb s s s b s xs s.t.
s s xs n whereb s xs denotes the number of bugs found by exercising seed sforxsepochs and usually two types of epochs are used fixed run and fixed time.
afl s power scheduling is based on the performance score of the seed which is calculated mainly based on the edge coverage and execution time of that seed.
while aflfast allocates more energy to seeds that can cover rare edges low frequency paths in .
534cerebro context aware adaptive fuzzing for effective vulnerability detection esec fse august tallinn estonia cerebroseed queueseed evaluatorprioritized seedpower scheduleseed mutatortest inputexecutorfeedback collectornew seedpocputsave assave aswait for figure the workflow of greybox fuzzing motivation example system overview .
motivation example i n t m j s f f i c a l l .
.
.
func a .
.
.
m j s p a r s e f f i s i g n a t u r e .
.
.
func b .
.
.
i n t m j s p a r s e f f i s i g n a t u r e .
.
.
.
.
.
i f tmp e !
i f mjs dlsym null m j s p r e p e n d e r r o r f func c goto c l e a n mjs dlsym .
.
.
func d .
.
.
e l s e goto c l e a n .
.
.
c l e a n .
.
.
listing code snippets from mjs listing is a code segment taken from mjs .
we omit the exact details and assign a short notation for each of the functions for convenience.
inside function b it may call cordbased on two branch conditions.
suppose a seed saexecutes the false branch at line and covers functions a b d with the function level execution trace a b d. fig.
shows two different coverage states corresponding to listing .
at state function cis not covered by any exercised seeds while at state it is covered by one or more seeds.
since at state sahas the potential to be mutated to execute the true branch at line it is reasonable to generate more new test inputs with sa.
when it is at state where the true branch at line has already been covered the possibility of gaining more coverage via mutating sadecreases.
therefore the fuzzer should assign less energy forsaat state .
existing gbfs are not aware of the change from state to state .
in fact their evaluation of the quality of sa remains unchanged between these states.
hence when evaluating a seed the gbf should be aware of the context uncovered code of its trace to evaluate potential benefits brought by mutating that seed.
.
approach overview fig.
depicts the basic work flow and main components of cerebro .
the input of the overall system is a set of initial seeds and the outputs are the decisions for seed prioritization and the power scheduling.
cerebro consists of four main components the green rounded rectangles in fig.
static analyzer dynamic scorer multiobjective sorter andpower scheduler .
abcd a state abcd b state figure seed execution traces under different coverage states.
legend yellow circles denote the functions covered by seed sa at state c white is not covered previously at state c grey has been covered by other seed s .
seed inputsexecution tracesdynamic scorerstatic analyzer multi objective scorerpower schedulercomplexity scorepotential scorenew edge?file sizecoverageexec timeseed prioritizationpower schedulingstatic analysis related metricsinherited properties of inputs figure overview of cerebro legend green rounded rectangles denote the major components of the system squashed rectangles in the grey boxes denote scores or metrics associated with a seed.
incerebro static analyzer scans throughout the source code and calculates a complexity score for each function.
as every seed is associated with an execution trace cerebro calculates the complexity score for the trace by accumulating the complexity scores of functions on it.
details of static analyzer are in .
.
the dynamic scorer updates the potential scores for the seeds on the fly.
the initial values of the potential scores are derived from the complexity scores which is elaborated in .
.
the outputs of static analyzer anddynamic scorer are then supplied to multi objective sorter andpower scheduler to help with the decisions of seed prioritization and power scheduling.
for the seed prioritization problem defined in def.
the multi objective sorter utilizes a cost effective nondominated sorting algorithm to dynamically select the prioritized set of seeds.
a detailed explanation of the chosen objectives the moo model and the sorting algorithm is illustrated in .
.
for the power scheduling problem defined in def.
the power scheduler determines the energy for each seed with the outputs from both static analyzer anddynamic scorer .
the power scheduler is elaborated in .
.
535esec fse august tallinn estonia y. li y. xue h. chen x. wu c. zhang x. xie h. wang and y. liu methodology .
static analyzer incerebro static analysis is used to evaluate the complexity of each function of the put and lay the foundation for dynamic scorer .
we use a mixture of two complexity metrics to evaluate the function level code complexity namely mccabe s cyclomatic complexity cc and halstead complexity measures h.b .ccrepresents the structural complexity of the function and h.brepresents the complexity based on the number of operations and operators.
the reasons of choosing them are as follows ccis a widely used complexity metric based on the number of edges and nodes in a graph h.bis proven to be the second best complexity metric to indicate code vulnerability in ccandh.bcomplement each other since ccevaluates structural complexity while h.b evaluates operational complexity.
after calculating ccandh.bfor each function2 feature scaling is applied to normalize each metric score separately.
the normalized scores are then combined together to form the static complexity score with the following equation.
this complexity score remains unchanged throughout the fuzzing process.
c score norm cc norm h.b based on the complexity scores we initialize the potential scores for each function.
the key rationale behind the potential score is that each function brings bonus scores to the potential scores of its predecessors and once it is covered by the fuzzer the bonus scores are removed.
therefore the initial potential score for a function is calculated by accumulating the bonuses brought by its successors.
the formula to calculate the bonus that a function abrings to its predecessor bis bonus a b c score a 2distance b a where denotes the floor function c score ais the complexity score of aanddistance b a is the shortest distance from btoa in call graph.
for example in fig.
4a a b c d e fandgdenote functions in the program.
assume fhas a complexity score of .
according to equation the bonuses it brings to its predecessors are for d for b and for a .
similarly the bonus g brings to dis .
thus because dhasfandgas its successors its dynamic score is initialized to be .
.
dynamic scorer the purpose of the dynamic scorer is to evaluate the complexity of the code that is not covered by the fuzzer presently but could possibly get covered through mutations.
although a thorough examination of coverage with external tools like gcov can achieve this it will greatly decrease the execution speed of the put.
furthermore calculating accurate coverage on basic block level can introduce significant overhead to gbfs.
due to these reasons we 2due to page limit we omit the formula to calculate ccandh.b.
interested readers can refer to for details.propose the following approach for function level dynamic potential evaluation dpe .
given a function a the calculation of potential score is p score a f sabonus f a where bonus f a is the bonus score that f sabrings to a and sais the set of uncovered successors of a. after the dynamic potential score is calculated it is combined with the static complexity score.
the combined score will be supplied to the power scheduler as a parameter .
.
given a function a the combined score is combined score a c score a p score a where c score a is the static complexity score for aandp score a is the dynamic potential score for a. here we use a step by step example shown in fig.
to illustrate how cerebro performs dpe.
each subfigure shows a coverage state held by the fuzzer.
each node in fig.
is a function yellow nodes are covered functions and white nodes are uncovered functions.
fig.
4a shows the state before fuzzing.
after given an initial input covering function a bande named as abe for convenience the fuzzer holds the state shown in fig.
4b.
since bis covered the bonus it brings to ais removed the function value pair a 2for node b is removed from fig.
4b.
so the potential score of ais updated to .
the potential score of bis updated to 5since its successor e is covered.
thus the potential score of input abe is calculated by accumulating the potential scores of each function on its execution trace the potential score of input abe is8 .
similarly the complexity score of input abe is calculated by accumulating the complexity scores of each function on its execution trace the complexity score for abe is9 and it never changes through out the fuzzing process.
thus the final combined score of abe is17 in fig.
4b.
after generating two more inputs ac andabdf the fuzzer holds the state shown in fig.
4c.
then the static complexity score for input abe remains while its potential score drops to .
the combined score of abe now is .
we can clearly see that as more functions under the trace ofabe are covered the potential score and the total score of abe decrease.
note that fuzzers without awareness of input potentials do not distinguish between state and state and thus will not adjust power scheduling accordingly.
finally assuming the fuzzer covers the last function g now it holds the state shown in fig.
4d.
since all functions are covered all input potentials are used up drop to and the combined score of a function is now only determined by its complexity score.
.
multi objective sorter the purpose of the multi objective sorter mo sorter is to prioritize the seeds via various metrics.
in this section we first introduce the objectives for prioritizing seeds then describe the moo model addressing the problem defined in def.
and finally present the nondominated sorting algorithm which solves the problem based on the moo model.
.
.
metrics.
the metrics for seed prioritization are file size m1 execution time m2 number of covered edges m3 whether the seed 536cerebro context aware adaptive fuzzing for effective vulnerability detection esec fse august tallinn estonia a 5b 6c 0d 6e 0f 0g 0no inputd 4b 2a 1a 1b 1d 2b 1b 2a 1a a step a 3b 5c 0d 6e 0f 0g 0abe 17d 4b 2a 1a d 2b 1b 2a b step a 0b 1c 0d 2e 0f 0g 0abe 10ac 4abdf 23d 2b c step a 0b 0c 0d 0e 0f 0g 0abe 9ac 4abdf 20abdg d step figure a step by step demonstration of the dynamic scoring algorithm legend each node is a function.
for example in f the letter f is the function name the first number in the node is the complexity score the second number is the potential score.
the function value pairs e.g.
d 4for node fin fig.4a associated with the curly brackets are the bonus scores that the function brings to its predecessors.
brings new edge coverage m4 and its static complexity score of the execution trace m5 .
the rationale of using these metrics is file size smaller seeds are more compact mutations on them are more likely to hit interesting bytes.
execution time seeds with shorter execution time can increase the average speed of the put and consequently the fuzzing efficiency.
number of covered edges seeds with higher coverage are preferred like in most gbfs intuitively fuzzing a seed with good coverage might generate more test inputs with good coverage.
whether the seed brings new edge coverage seeds covering new edges are preferred because fuzzing such seeds are more likely to bring new coverage.
static complexity score of the execution trace seeds covering more complex code are preferred because intuitively complex code tends to be more error prone .
our model to handling the metrics takes into account two facts.
.
some of these metrics are conflicting each other e.g.
a seed covering more complex code may execute slower.
.
the scalarization model adopted by existing gbfs needs a proper weighting schema which may vary from project to project.
thus we need a moo model to balance between these metrics.
.
.
moo model for seed prioritization.
with the above multiple metrics a new moo model can be inferred as follows definition multi objective seed prioritization .
given a set of seeds s multi objective seed prioritization is to select a set of seeds s min f s min o1 s o2 s ...ok s s s where f s is an objective vector that denotes kobjective functions ranging from o1took.
in our case k 5as we have metrics.
instead of solving the problem in def.
by scalarizing different metrics into a single objective function we set a separate objective for each metric in 3potential score is not used in seed prioritization see fig.
.
in implementation only when the fuzzer chooses to fuzz a seed its potential score will be lazily updated the actual potential score is calculated on the fly after a seed is selected to be fuzzed since it is too costly to update the potential score for every seed every time whenever a new function is covered.our moo model.
here is the mapping between metrics and objectives let mk s denote the k th metric for seed s s we have min o1 s min m1 s min o2 s min m2 s min o3 s max m3 s min o4 s max m4 s min o5 s max m5 s .
the reason behind the mapping is that we want to minimize m1 m2but maximize m3 m4 m5.
.
.
pareto frontier.
pareto frontier calculation is one of the most commonly used posteriori preference techniques for moo problems.
in our proposed moo model the prioritized seeds are inside the pareto frontier of the entire set of seeds.
given a set of the seeds sand an objective vector f we say s dominates s iff fi s fi s i k where s s s the pareto frontier p is defined as p s s s s s s s s s theoretically the calculation of pareto frontier requires to compare each seed against all the other seeds to check their domination relation.
however as shown in fig.
skeeps expanding as new seeds come in.
it is costly to recalculate the pareto frontier every time a new seed is added.
to tackle this problem we propose algo.
based on the nondominated sorting in .
.
.
nondominated sorting.
in greybox fuzzing the seeds are stored in a queue as shown in fig.
.
to efficiently calculate the pareto frontier the entire queue is splitted into four sub queues as shown in fig.
.
they are frontier p dominion d recycled r andnewly added n .
the basic idea is to keep popping seeds from pfor fuzzing which requires pto be maintained as the pareto frontier of the seeds not yet fuzzed in the current cycle.
to efficiently calculate the pareto frontier we adopt the concept of rank from nondominated sorting.
each seed is associated with a rank representing the domination relation.
intuitively the value of the rank for a seed represents the number of seeds dominating it.
for instance seeds with rank are not dominated by any other seeds so they are on the pareto frontier seeds with rank are only dominated by seeds with rank seeds with rank are only dominated by seeds with rank and and so 537esec fse august tallinn estonia y. li y. xue h. chen x. wu c. zhang x. xie h. wang and y. liu algorithm cycle based nondominated sorting 1p d n r 2loaddwith the user provided test seed s 3queue cycle 4queue rank 5defupdate ranks a seed b seed ifa bthen ifa bthen b.rank ifb athen a.rank 11while not time budget reached orkill signal received do ifpis then n.rank queue rank forn in n d d n forn in n do ford in d do update ranks d n n ifdis then queue cycle queue rank r.rank forr in r forr in r do forr in r do update ranks r r d d r r queue rank ford in d do ifd.rank queue rank then popdfrom d push dintop popsfrom p update power schedule for s fuzzs push sintor on.
so given a set of seeds sand their ranks r pis calculated as p s s s s.rank min r in algo.
the variable queue rank maintains the min r in equation .
ris used to store the seeds fuzzed in the current cycle.
dis to store the seeds that are not fuzzed in the current cycle and are dominated by the seeds in p.nis to temporarily store the interesting seeds kept by the fuzzer through fuzzing the seed popped from p. the logic for updating ranks according to domination relation between seeds corresponds to line line .
the calculation ofpcorresponds to line line .
each time the fuzzer uses up the seeds in p a new pareto frontier will be calculated based on nanddand stored into p line to line in algo.
.
if both n anddare empty during calculation of the new p it indicates that the fuzzer has fuzzed every seed in current cycle and a new cycle starts.
now that every seed is stored in r the fuzzer will move the seeds from rtodand calculate paccordingly line to line .
the benefit of maintaining the rank is to avoid redundant domination relation checks between seeds that are already compared with each other.
assuming there are nseeds in nanddseeds in d d n d n checks are needed without maintaining the ranks.
however only n d n checks are needed with the ranks maintained saving a total number of d2 nd 1checks.
initial seed inputsfrontierdominionrecycledinputnewly addedinputmain fuzzing process mutation trace checking etc.
after one cyclefigure structure of the seed queue in cerebro .
power scheduler the purpose of power scheduler is to assign proper energy to the seeds selected by the mo sorter addressing the problem defined in def.
.
most gbfs calculate the energy for a seed as follows4 ener y allocate ener y qi where qiis the quality of the seed.
incerebro the power scheduler incorporates the combined score of the static complexity and the dynamic potential for seed energy allocation ener y ener y sf is as where ener yis the energy calculated in equation sfis the score factor isis the combined score of the seed and asis the average combined score of all the seeds.
the combined score enables the fuzzer to fully exploit the knowledge about the complexity of both covered and uncovered code.
the static complexity score indicates the inherent complexity of the execution trace of a seed while the dynamic potential score indicates the complexity of uncover code near the execution trace.
by combining both scores cerebro allocates more energy for seeds with better potential of leading to bugs or new edges.
implementation and evaluation we implement a novel fuzzer cerebro .
specifically the static complexity analyzer is written based on clang s python binding andlizard with about lines of python.
the instrumentation module used to track the execution traces is implemented in lines of c on top of llvm framework.
the core dynamic fuzzing logic of cerebro is an extension of our fundamental fuzzing framework fuzzing orchestration toolkit fot which is a rust implementation of afl for better modularity and extensibility.
a more detailed introduction about cerebro together with a demonstration kit is available at site cerebrofuzzer .
.
evaluation setup evaluation dataset.
the evaluation is conducted on widelyused real world open source programs each of which is from a different project in a different domain.
mjsis a light weight javascript engine for embedded system .
pngfix is a tool from libpng .fuzzershell is the official fuzzing 4for clarity we omitted some details in eqt.
and eqt.
.
in actual implementation for both cerebro andafl the fuzzers utilize several different metrics to calculate qi a.k.a.
performance score in or i in .
538cerebro context aware adaptive fuzzing for effective vulnerability detection esec fse august tallinn estonia table details of the evaluated programs stars means the number of github stars.
program size b stars duration project fuzzershell .7m 18y sqlite mjs .0k 2y8m mjs nm .0m 28y binutils cxxfilt .0m 28y binutils pngfix .0k 23y2m libpng radare2 .0m 13y radare2 xed .0m 2y8m intel xed harness of sqlite .xedis the disassembler used in intel pin .
radare2 is a famous open source reverse engineering tool .
cxxfilt is a program in gnu binutils to demangle c function names.
nmis a program from gnu binutils to list symbols in an object file.
specifically we used the same version version .
of cxxfilt andnmas in the aflfast paper for more direct comparison.
furthermore we also ran cerebro on a newer version version .
of nmin hope of finding new bugs.
for clarity we denote nm version .
as nm old andnmversion .
as nm new .
from table we can see that the selected programs are very popular in the community and their sizes vary from a few hundred kbs to tens of mbs.
we can also see that the projects are long time supported.
the diverse dataset helps to demonstrate the generality and scalability of cerebro .
evaluated tools.
we compared cerebro with two other fuzzers namely afl andaflfast .afl is by far the most popular gbf.
aflfast represents the state of the art seed prioritization and power scheduling techniques.
all the mutation operations are the same across the tools and only non deterministic mutation operations are used because power scheduling does not affect the deterministic mutations .
experimental infrastructure.
we conducted all our experiments on a machine of intel r xeon r cpu e5 v3 cores and gb memory running a bit ubuntu .
lts system.
each experiment was repeated for times and each took hours except forradare2 which took hours due to its slow execution speed.
research questions.
we aim to answer these questions rq1.
how is the crash detection capability of cerebro ?
rq2.
how is the vulnerability detection capability of cerebro ?
rq3.
how do seed prioritization and power scheduling in cerebro affect the performance separately?
.
crashes rq1 during the experiments we found crashes in six programs mjs cxxfilt nm old nm new xedand radare2 .
we follow afl s definition of unique crash i.e.
two crashing inputs with the same edge trace are counted as the same crash.
unique crash serves as a good indicator of a fuzzer s capability of exercising error prone paths .
fig.
shows the average number of unique crashes found over time in hours by cerebro aflfast and afl for ten runs.
in general it is obvious that cerebro significantly outperforms afl andaflfast as to unique crashes found over time.
from the trends of the plots we can see that cerebro not only finds more crashes a mjs b cxxfilt c nm new d nm old e xed f radare2 figure unique crashes found over time higher is better after hours but also finds them faster.
this indicates cerebro is both efficient and effective for crash exposure.
in particular we can see that all the fuzzers found less crashes in the newer version of nmandafl even failed to find any crash in hours.
this is as expected because the developers patched the bugs reported in the old version.
on one hand for the old version with much more bugs cerebro can find considerably more crashes at least than both afl andaflfast .
on the other hand for the new version with existing bugs fixed and new bugs introduced cerebro can still find substantially more crashes with a shorter time.
table shows the statistic test results for this experiment.
as suggested by klees et al.
we adopted the mann whitney u test and the vargha delaney a12value here.
the mann whitney utest measures the statistical significance of the results.
the varghadelaney a12value measures if we randomly pick one run out of the ten runs for both cerebro and its competitor the probability thatcerebro performs better.
in general we can see that cerebro finds significantly more crashes than both afl andaflfast .
most of the a12values are also above the conventionally large effect size .
.
the pitfall comes to the case of nm new where all the fuzzers tends to find very few crashes.
despite that cerebro still performs a bit better than the other two tools on nm new .
we can conclude that cerebro 539esec fse august tallinn estonia y. li y. xue h. chen x. wu c. zhang x. xie h. wang and y. liu table details of the unique crashes detected statistically significant results by mann whitney u test are marked as bold c stands for cerebro af stands for aflfast a stands for afl.
projectaverage crash a12 c af a af a mjs .
.
.
.
.
cxxfilt .
.
.
.
.
nm old .
.
.
.
.
nm new .
.
.
.
.
radare2 .
.
.
.
.
xed .
.
.
.
.
performs better than afl andaflfast in terms of crash detection from the statistic point of view.
from the analysis of figure and talbe we can positively answer rq1 thatcerebro significantly outperforms the stateof the art fuzzers in terms of crash detection.
.
vulnerabilities rq2 as suggested by klees et al.
besides unique crashes researchers should also use unique bugs as the ground truth for fuzzer evaluation .
this is because several unique crashes could be related to one unique bug with the same root cause.
after scrutiny we further classified all the unique crashes from these programs into unique previously unknown bugs according to how the developers apply patches5.
notably the bug in nm new is later verified to be a reproducer of cve .
figure is a boxplot showing the time to exposure tte for the bugs that every fuzzer can find for at least four out of ten experiment runs.
the y axis is time.
the dark bars inside the boxes are the median values.
the top border of a box indicates the 75th percentile and the bottom border of a box indicates the 25th percentile.
hence a lower position of the box indicates a shorter tte for a bug which means better performance.
moreover a small box size means the variance between every run is small and the fuzzer s performance is stabler.
in general we can see that the cerebro can detect the bugs with a shorter time comparing to aflfast andafl.
on average cerebro can find the bugs .
times and .
times faster than aflfast andafl respectively.
in most cases the box of cerebro is also smaller than aflfast s and afl s which means that cerebro can stably detect those bugs earlier than them.
however in some cases the box size of aflfast is the smallest.
this is because aflfast finds the bug in fewer runs and the performance appears to be stabler.
for example in xed invalid read aflfast only finds the bug in four experiment runs while cerebro finds the bug in all the ten runs see table and one or two outliers make the box of cerebro larger in size.
table shows the statistic test results for this experiment.
the runs column shows the number of runs that a fuzzer can detect a particular bug in ten runs.
the a12column shows the a12values calculated based on the ttes and the value is marked as bold if the u test shows significance.
5we filtered out assertion fails which also contribute to the crashes found.
the assertion fails have also been reported and fixed.table details of the unique bugs discovered means no bug is found in runs statistically significant results by mann whitney u test are marked as bold c stands for cerebro af stands for aflfast a stands for afl.
project bug cve id runs a12 c af a af a mjsbufferoverflow .
.
useafterfree .
useafterfree .
.
invalidread .
.
negsizeparam .
.
stackoverflow floatpointerror .
.
xedinvalidread .
.
invalidread .
.
invalidread .
.
invalidread .
.
invalidread .
.
invalidread .
.
invalidread .
.
radare2 .
.
nm .
mean .
.
.
.
.
in general we can observe from table that cerebro can find of the bugs significantly faster than afl the bugs that afl cannot detect are not counted .and of the bugs significantly faster than aflfast the bugs that aflfast cannot detect are not counted .
we can also see that the mean a12values are .
against aflfast and .
against afl both of which are above the conventionally large effect size .
.
the results suggest that cerebro can outperform both aflfast andafl from the statistical aspect.
case study.
to demonstrate the reason behind cerebro s superiority we present the case of cve where cerebro has a high confidence of being better a12is .
against aflfast and .
against afl .
this cve is triggered by an invalid memory read caused by a strlen function call where the pointer passed tostrlen points to zero page due to a missing length check for the parent string containing the pointer.
the patched function is r bin dwarf parse comp unit as the function code shown in listing .
the true branch of the ifcondition at line is a relatively rare branch and aflfast will prioritize the seeds exercising this branch.
however this branch could mistakenly guide the fuzzer away from triggering the bug.
this explains the reason why aflfast found the bug only in out of runs while cerebro andafl can detect it in all runs.
comparing to afl cerebro allocates more energy to the seeds reaching r bin dwarf parse comp unit meanwhile not getting into the true branch of the ifcondition on line .
the reason is that function sdb set keeps contributing bonus potential scores to its predecessor r bin dwarf parse comp unit until it is covered and the bug is triggered.
since cerebro manages to generate more new inputs from the seeds close to but not yet reaching the buggy function it can usually detect the bug faster than afl.
from the analysis of figure table and the case study we can positively answer rq2 that cerebro significantly outperforms the state of the art fuzzers in terms of vulnerability detection.
540cerebro context aware adaptive fuzzing for effective vulnerability detection esec fse august tallinn estonia ut8 r b i n d w a r f p a r s e c o m p u n i t .
.
.
.
.
.
i f cu hdr .
l e n g t h d e b u g s t r l e n r e t u r n null while .
.
.
.
.
.
i f .
.
.
m i s s i n g check f o r t h e l e n g t h o f name s d b s e t s dw at comp dir name end o f i f .
.
.
end o f while listing code snippets from radare2 .
evaluation of individual strategies rq3 cerebro uses an moo based seed prioritization strategy together with an input potential aided power scheduling strategy.
to analyze the effects of each individual strategy we configure variants of cerebro cerebro base uses no seed prioritization and allocates a constant energy to every seed.
cerebro moo only uses only the moo based seed strategy and allocates a constant energy to every seed.
cerebro pot only uses no seed prioritization and applies the input potential aided power scheduling.
we evaluated these strategies individually with two programs mjs representing relatively small programs and xed representing relatively large programs see table .
fig shows the number of crashes detected over time for each individual strategy.
in general both strategies can help to detect more crashes within the time budget.
in particular on xed the moo based seed prioritization strategy seems to restrain the crash detection performance as it needs to balance between exploitation and exploration as discussed earlier.
however the seed prioritization strategy does help to boost crash detection as the queue of seeds grows larger and it surpasses the power scheduling strategy after around minutes.
this explains why cerebro pot only has a better performance than cerebro in the first hours but gets surpassed later.
if we compare the results of mjsand xed we can see that the performance improvement over the baseline strategy cerebro base onxedis much more significant than on mjs.
the rationale is that asxedis a much larger program than mjs see table the fuzzers need to keep much more seeds in queue for xedand to evaluate the quality of seeds properly and allocate the computational resource precisely becomes much more important.
as a result both strategies becomes more effective on larger programs.
lastly combining both strategies leads to the best results for finding crashes on both projects which implies the two strategies can mutually benefit each other without conflicts.
thus it makes sense that cerebro needs to combine the seed prioritization strategy with a proper power scheduling strategy by allowing better seeds to produce more test inputs to boost the final coverage after convergence.
from the analysis of figure we can positively answer rq3 that the seed prioritization and the power scheduling can help to improve the fuzzing performance separately combining them can further improve fuzzing efficiency.
figure time to exposure for bugs lower is better the y axis is the time and the unit is hour except for xed invalid read which is second .
.
discussion threats to validity.
the threats of validity come from three aspects.
first although complexity is generally considered as the enemy of software security some researches are skeptical about it .
after analyzing the bugs our empirical finding is that typically the program executes a series of complex functions makes a logical fault somewhere and eventually crashes in a possibly simple function.
so using the execution trace level complexity is reasonable.
second we used only two complexity metrics ccand h.b .
as there exist a vast variety of code evaluation metrics e.g.
jilb metrics cocol metrics leopard etc.
we will 541esec fse august tallinn estonia y. li y. xue h. chen x. wu c. zhang x. xie h. wang and y. liu a mjs b xed figure crashes found over time with different strategies higher is better seek more combinations to facilitate the input potential in future.
third results of tool evaluations could always be taken with a grain of salt.
for instance we set the time limit of most experiments to be hours because the edge coverage of most programs we test tends to converge in hours.
additional experiments.
last but not least we conducted extra experiments to better evaluate cerebro from various aspects.
for example to check how well cerebro complements constraint breaking techniques we combine cerebro with dictionary based mutation operators and conduct experiments to check the coverage gain.
the results of combining with dictionary based mutation show that cerebro can better complement orthogonal techniques.
we also conducted experiments with cerebro by allowing it to export the seed queue information during execution to evaluate the quality of seeds on the pareto frontier.
these extra experiment results and data are available on our website .
related work before cerebro there are several works on using multi objective optimization techniques for seed prioritization selection as well as search based testing .
instead of listing all the related works we focus on fuzzing techniques related to cerebro .
seed prioritization.
several techniques have been proposed for offline seed selection and online seed prioritization.
shudrak et al.
propose an approach to evaluating and improving black box fuzzing effectiveness by prioritizing the seeds which exercise high complexity codes.
as they use software complexity as the sole goal for seed prioritization the results heavily rely on the quality of the complexity analysis.
in cerebro we propose an moo model to balance between different objectives instead of only considering the execution trace complexity.
in vuzzer the error handling paths are deprioritized.
this intuition aligns with the concept of input potential.
however input potential is more general as error handling code can be quite complex and bug prone sometimes .
both vuzzer andaflfast prioritize seeds exercising low frequency paths.
however as shown in .
and the assumption that fuzzing such seeds brings more benefits does not always hold.
in collafl three different strategies are proposed to prioritize seeds with more neighbor branches or descendants.
the intuition of bringing awareness of uncovered code to the fuzzer is similar to cerebro whereas cerebro utilizes the complexity of near neighbors instead of the quantity of direct neighbors in .
seed prioritization is also performed in directedfuzzing to help the fuzzer to reach the targets as early as possible.
power scheduling techniques.
aflfast assigns more energy to seeds exercising the low frequency paths.
aflgo and hawkeye both apply a customized power schedule to improve the directedness of fuzzing towards specific program target locations.
the purposes of directed fuzzing and cerebro are different.
the purpose of directed fuzzing is to reach the given specific targets as fast as possible.
the purpose of cerebro however is to discover as many bugs as possible given a time budget.
in comparison with these seed prioritization and power scheduling techniques cerebro explicitly presents the concept of input potential and proposes an moo model to incorporate different important and conflicting objectives.
constraint breaking fuzzing boosting techniques.
many other techniques in fuzzing are orthogonal to cerebro .
in greybox fuzzing a hot topic is about penetrating through path constraints.
plenty of approaches have been proposed to address this problem.
sage and driller use symbolic concolic execution to solve the path constraints for fuzzers.
vuzzer uses dynamic taint analysis to tackle path constraints.
steelix utilizes comparison progress information to solve magic byte comparisons.
angora adopts byte level taint analysis and a gradient descent algorithm for constraint penetration.
these techniques often involves modifications of the seed mutator executor orfeedback collector in fig.
but not the seed evaluator making them orthogonal to cerebro .
several techniques are proposed to boost the performance of fuzzing from different aspects .
among them fairfuzz utilizes masks to preserve key patterns inside a seed to reach certain edges.
fairfuzz targets rare edges with the help of the seed masks.
however the idea of input masks can be applied to target any edges not just rare edges which makes it orthogonal to cerebro .
another potential enhancement is to replace the coarse loop bucket strategy with static loop analysis .
conclusion in this paper we propose the concept of input potential and a moo model for seed evaluation in greybox fuzzing.
with the complexity of covered code and the potential of uncovered code based on execution traces we address two general problems of greybox fuzzing namely seed prioritization and power scheduling.
we conduct evaluations on real world programs.
results exhibit that cerebro performs significantly better in bug detection and program coverage than state of the art techniques.
in future we plan to complement cerebro with other orthogonal fuzzing techniques.
acknowledgement this research was supported in part by the national research foundation prime ministers office singapore under its national cybersecurity r d program award no.
nrf2018ncr ncr0050001 national satellite of excellence in trustworthy software system award no.
nrf2018ncr nsoe003 administered by the national cybersecurity r d directorate.
the research of dr xue is supported by cas pioneer hundred talents program.
542cerebro context aware adaptive fuzzing for effective vulnerability detection esec fse august tallinn estonia