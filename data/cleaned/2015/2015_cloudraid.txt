cloudraid hunting concurrency bugs in the cloud via log mining jie lu state key laboratory of computer architecture institute of computing technology chinese academy of sciences university of chinese academy of sciences china lujie ict.ac.cnfeng li state key laboratory of computer architecture institute of computing technology chinese academy of sciences institute of information engineering chinese academy of sciences china lifeng2005 ict.ac.cn lian li state key laboratory of computer architecture institute of computing technology chinese academy of sciences university of chinese academy of sciences china lianli ict.ac.cnxiaobing feng state key laboratory of computer architecture institute of computing technology chinese academy of sciences university of chinese academy of sciences china fxb ict.ac.cn abstract cloud systems suffer from distributed concurrency bugs which are notoriously difficult to detect and often lead to data loss and service outage.
this paper presents cloudraid a new effective tool to battle distributed concurrency bugs.
cloudraid automatically detects concurrency bugs in cloud systems by analyzing and testing those message orderings that are likely to expose errors.
we observe that large scale online cloud applications process millions of user requests per second exercising many permutations of message orderings extensively.
those already sufficiently tested message orderings are unlikely to expose errors.
hence cloudraid mines logs from previous executions to uncover those message orderings which are feasible but not sufficiently tested.
specifically cloudraid tries to flip the order of a pair of messages s p if they may happen in parallel but salways arrives before pfrom existing logs i.e.
excercising the order p s. the log based approach makes it suitable to live systems.
we have applied cloudraid to automatically test four representative distributed systems apache hadoop2 yarn hbase hdfs and cassandra.
cloudraid can automatically test different versions of the systems versions per system in hours and can successfully trigger concurrency bugs including new bugs that have never been found before.
the new bugs have all been confirmed by their original developers and of them are considered as critical bugs that have already been fixed.
corresponding author permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa association for computing machinery.
acm isbn .
.
.
.
concepts software and its engineering software testing and debugging cloud computing keywords distributed systems concurrency bugs bug detection cloud computing acm reference format jie lu feng li lian li and xiaobing feng.
.
cloudraid hunting concurrency bugs in the cloud via log mining.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
https introduction cloud computing is now mainstream.
modern online applications from social networking and communication to banking now play an important part of our daily life.
distributed systems such as scale out computing frameworks distributed key value stores scalable file systems and cluster management services are the fundamental building blocks of cloud applications.
however distributed systems are notoriously difficult to get right.
it is too complicated for the programmers to correctly reason and handle concurrent executions on multiple machines.
there are widely existing concurrency bugs in real world distributed systems which often lead to data loss and sometimes service outage .
for example on november 18th the microsoft azure cloud hosting service went down for around hours due to a bug in a server side software update.
distributed concurrency bugs are triggered by untimely interaction among nodes i.e.
unexpected message orderings .
this fact has motivated a large body of research on distributed system model checkers which detect hard to find bugs by systematically exercising all possible messages orderings.
theoretically these model checkers can guarantee reliability when running 3esec fse november lake buena vista fl usa jie lu feng li lian li and xiaobing feng a same workload.
however distributed system model checkers face the state space explosion problem .
despite recent advances it is still difficult to scale these tools to many large real world applications.
for example in our experiments of running the wordcount workload on apache hadoop2 yarn messages are involved and it is impractical to exhaustively test all possible message orderings in a timely manner.
this paper proposes a novel strategy in battling distributed concurrency bugs.
we do not try to exhaustively exercise all possible message orderings.
instead we address a different question which message orderings are likely to trigger an error?
hence our approach analyzes suspicious message orderings and only test those orderings that are likely to expose errors.
the approach is not sound and does not guarantee free of concurrency bugs.
however it is very effective in detecting distributed concurrency bugs as highlighted in our experiments.
it is also simple and can be easily adopted by live systems.
which message ordering is likely to trigger an error?
this question is key to our approach.
we address the question based on the following observations observation .
the errors triggered by different message orderings often share a common root cause pattern their corresponding message handlers access some shared objects inconsistently when given different orders.
observation .
large scale online applications process millions of user requests per second.
many permutations of message orderings have already been extensively tested and exercised in these live systems.
those sufficiently tested message orderings are unlikely to expose errors.
hence we can learn from previous executions to uncover these message orderings that are likely to expose errors.
since we harness the rich execution history from live systems a non intrusive log based approach is desirable.
modern cloud applications often provide a rich set of runtime logs which record important messages and events to help with the diagnosis and monitoring of online systems.
our approach mines logs from previous executions to uncover those message orderings which are feasible but not yet sufficiently tested.
the log based approach makes it suitable to live systems where intrusive instrumentation is often not an option.
we develop cloudraid a new tool to effectively detect distributed concurrency bugs.
cloudraid automatically extracts sequences of important communication events from existing run time logs.
permutations of these event orderings will be further tested if they are feasible but not yet exercised.
a dynamic trigger is employed to exercise and test the selected message orderings at runtime.
previous studies show that more than distributed concurrency bugs can be triggered by a single untimely message delivery.
hence we focus on the order between a pair of messages only.
cloudraid tries to flip the order of a pair of messages s p if they may happen in parallel but salways arrives before pfrom existing logs i.e.
exercising the order p s. we have applied cloudraid to test four representative distributed systems apache hadoop2 yarn hdfs hbase and cassandra .
cloudraid can be easily adopted.
the system under testing can run as is lively without modification.
in a separate testing phase our dynamic trigger performs minimal instrumentationto test a specific message ordering.
in our evaluation we randomly choose different versions of these systems versions per each system and ran different workloads in total on these systems.
cloudraid ran times all together in hours where each run tries to exercise a specific message ordering.
the runs successfully triggered bugs with no false positives including new bugs that have never been found before.
the new bugs have all been confirmed by the original developers and of them are considered as critical bugs and have already been fixed.
contributions.
this paper makes the following contributions we propose a new approach to effectively detect concurrency bugs in distributed systems.
our approach avoids unnecessary repetitive tests by harnessing the rich log information in previous running histories which can drastically improve efficiency.
we develop cloudraid a simple yet effective tool to test distributed systems.
cloudraid targets live systems by automatically analyzing run time logs without instrumentation.
it can be easily adopted and is very effective in detecting distributed concurrency bugs.
we extensively evaluated cloudraid using four representative distributed systems apache hadoop2 yarn hbase hdfs and cassandra.
cloudraid can finish testing different versions of the systems with workloads in total in hours and can successfully detect concurrency bugs.
among them there are new bugs including critical bugs which have already been fixed by their original developers.
the rest of the paper is organized as follows.
section illustrates our approach using a real world example.
we present the design and implementation of cloudraid in section and evaluate its efficiency and effectiveness in section .
section reviews related work and section concludes the paper.
an illustration example the example in figure depicts the common procedure to create a new task in hadoop mapreduce.
there is a concurrency bug mapreduce in this basic procedure.
the bug.
the two messages and that trigger the bug are highlighted in red.
in normal execution figure the remote procedure call rpc to startcontainer finishes execution quickly.
hence the assigned event message is always dispatched and handled before the update event message .
however there is no happen before order between the two messages.
if the update event arrives before the assigned events due to unexpected delays int1 e.g.
insufficient resources in the underlying machine running am an error is triggered and the task cannot be created.
the root cause.
the event handler implements a state machine for each task to update its status according to the incoming events.
the state machine expects to always process the update event after theassigned event.
otherwise an error will be thrown which leads to job fail.
the fix is to introduce smart synchronizations to guarantee that the assigned event always arrives before the update event.
among all these messages the message pair is the root cause of the error.
both messages are handled by a same event 4cloudraid hunting concurrency bugs in the cloud via log mining esec fse november lake buena vista fl usa figure a real world example to start a new task in hadoop2.
am is the application manager node nm is the node manager node and task is the node to run the task.
thread t0in am create a new thread t1 t1invokes the remote procedure startcontainer to start a container on nm thread t2 in the startcontainer method t2creates another thread t3 the rpc remote procedure call to startcontainer returns to t1 after returning from startcontainer t1sends a task assigned event to the event queue the assigned event is dispatched to the event handler t6 the task state is updated to assigned t3starts a new process to run the task on node task using shell script thread t4 t4invokes the remote procedure statusupdate on am thread t5 in t5 the statusupdate method sends a task update event to the event queue the update event is dispatched to the event handler t6 to update the task state to updated method statusupdate returns to t4.
handling method to update the same variable for task status.
how can we automatically pick the message order among all the rest messages?
let us dig into the technical details.
.
source code and runtime logs figure gives the abstracted code snippet of our illustration example.
those lines sending messages are highlighted in red.
we call those source locations sending and handling messages static messages .
hereafter we use the notation mfor a static message andmifor its dynamic instance.
there are common patterns of static messages thread creation message in line rpc remote procedure call message in line and event dispatch message and message in line and line respectively .
for simplicity the code snippets for sending messages and are not given.
the event handling method eventhandler.handle line calls method statemachine.dotransition which invokes different callback functions to handle different types of events.
here we present a simplified version with callbacks inlined.
the boxed lines log static messages.
all messages except for the rpc return message and and the call to shell script message are logged.
a message is often logged at the entry of its corresponding handler.
message line follows immediately after message rpc to method statusupdate in line .
hence a common log line is introduced to serve both messages for better performance.
in this case we group the two static messages together denoted as .
the logs consist of constant strings and values of variables.
the above code snippet will execute multiple times at run time resulting in multiple dynamic instances per static message as well as multiple log instances.
values of variables in the log instances are used to distinguish each dynamic instance.
figure shows the simplified runtime logs where the code snippet is executed twice.
.
methodology ideally we would like to precisely recover runtime message sequences from existing logs as annotated in figure .
each log instance is mapped to one static message or a grouped static message .
log instances from the same run are grouped together in order.
in reality we perform source code analysis and log analysis together to recover such message sequences.
we statically analyze how static messages are handled and logged.
runtime log instances can then be mapped to static messages with static analysis information.
we group logs from the same run together by analyzing the relation between logged variable values based on static dependence analysis and their runtime values.
section gives the technical details.
the recovered message sequences are then mutated for further testing.
in this paper we focus on the order between a pair of static messages p s where pandsmay happen in parallel.
some message pairs follow a strict happen before order e.g.
and .
the order between them can not be mutated.
our observations in section provide the basic guide lines to select a message ordering p s as follows rule .
runtime log instance piandsimust log related runtime values of id variables.
rule .
the order pi sihave not been exercised where piandsiare runtime instances with matching values of id variables.
observation 1leads to rule .
distributed systems frequently use values of id variables as indexes to access shared resources.
thus messages logging completely unrelated values of id variables are unlikely to access a common shared objects and unlikely to expose errors.
rule 2discards these message orderings that have already been exercised according to observation .
in our example the message pairs and may happen in parallel.
all messages record values of related variables in their logs rule .
the message orderings and have already been tested according to the log information.
hence we will select the order for further testing.
the error can then be triggered.
discussion.
our method mutates the order between a pair of messages only.
we do not target bugs that occur due to multiple messages being out of order.
previous studies have shown that most distributed concurrency bugs are triggered by a single untimely message and only of bugs require more than messages.
5esec fse november lake buena vista fl usa jie lu feng li lian li and xiaobing feng thread t0in am public void containerlauncherimpl.servicestart runnable t createeventprocessor new containerlaucherevent this.launcherpool.execute t message public void containerlauncherimpl.createeventprocessor containerlauncherevent event return new eventprocessor event thread t1in am handler of message public void eventprocessor.run log .inf o launchin this .taskattempti d containermanagementprotocolpbclientimpl proxy getcmproxy this.containermgraddress startcontainerresponse response proxy.startcontainer new startconreq ... message this.dispatcher.handle new taskattemptcontainerlaunchedevent this.taskattemptid... message thread t2in nm handler of message public startconres containermanagerimpl.startcontainer startconreq req id containerid req.getconlaucontext .getconid log .inf o start request f or container i d .
.
.
create thread t3via thread pool message thread t6in am handler of message and message dispatched by the event dispatcher public void eventhandler.handle taskevent event tasktattemptevent ev tasktattemptevent event if this.oldstate assigned ev.gettype .equals ta container launched handle message taskattempt attempt ev.gettaskattempt log .inf o t askattempt attempt .attemptid usin container id attempt .container i d .
.
.
update task status else if this.oldstate assigned ev.gettype .equals ta container update handle message .
.
.
other cases thread t3in nm handler of message public void localizerrunner.run nmprivatectokenspath getlocalpathforwrite this.localizerid log .inf o w ritin credentials to the nmpri vate f ile nmpri vatect okenspath .tostrin .
.
.
create task via shell script message thread t5in am handler of message public void taskattemptlistenerimpl.statusupdate taskattemptid taskattemptid log .inf o status update f rom taskattempti d this.dispatcher.handle new taskattemptstatusupdateevent .
.
.
message figure abstracted code snippet of the example in figure .
this study motivates our approach.
our method can effectively detect distributed concurrency bugs in real world cloud systems as demonstrated in our evaluation.
the cloudraid approach we implement cloudraid in wala via a series of sub analyses figure .
communication analysis statically analyzes how static messages are handled and logged.
similar to we represent logging patterns of static messages as regular expressions.
then 6cloudraid hunting concurrency bugs in the cloud via log mining esec fse november lake buena vista fl usa launching attempt 1514878932605 0001 m 000009 0 message start request for container 1514878932605 0001 01 000011 message writing credentials to the nmprivate file hadoop home nm local dir nmprivate container 1514878932605 0001 01 000011.tokens message taskattempt using containerid container 1514878932605 0001 01 000011 message status update from attempt 1514878932605 0001 m 000009 0 message immediately followed by launching attempt 1514878932605 0002 m 000007 0 message start request for container 1514878932605 0002 01 000009 message taskattempt using containerid message writing credentials to the nmprivate file hadoop home nm local dir nmprivate container 1514878932605 0002 01 000009.tokens message status update from attempt 1514878932605 0002 m 000007 0 message immediately followed by figure simplified runtime logs of the example in figure .
table static messages c f l .
messages client site c message handler f logging pattern l eventprocessor.run launching attempt containermanagerimpl.startcontainer start request for container localizerrunner.run writing .
.
.
hadoop... container .tokens eventhandler.handle taskattempt .
.
.
taskattemptlistenerimpl.statusupdate status update from attempt eventhandler.handle status update from attempt figure architecture of cloudraid.
log analysis uses the logging patterns to map each runtime log instance to a static message.
id analysis analyzes relations between logged values according to dependences between logged variables and their run time values from log analysis .
messages from the same run can then be distinguished from other runs and grouped together.
hb analysis statically analyzes the happen before order between static messages.
analysis results from hb analysis and id analysis are used in message pair analysis to select message orderings for further testing.
finally trigger will instrument the source code to exercise the selected message orderings.
.
communication analysis communication analysis is the basis for all subsequent analyses.
it represents each static message as a tuple of elements c f l where cis the client site to send the message fis the corresponding message handler and lis the regular expression expressing its logging pattern.
table gives the static messages for our illustration example client site cis represented using source line number .
wetarget common patterns of static messages thread creation rpc and event dispatch.
.
.
thread creation.
the client site cis the call site to t.start orthreadpoolexecutor.execute t where tis a runnable or thread object.
the message handler fis the run method of the thread object referenced by t. to locate the message handler f we backward slice the program to find the object that tpoints to.
instead of using a standard pointer analysis we simply follow the def use chains since the thread object is often created right before its execution.
in figure the call site this.launcherpool.execute t line is the client to start a thread.
slicing tbackward we can reach the object created at line new eventprocessor .
hence the message handler fiseventprocessor.run line .
static message is thus represented as eventprocessor.run l where is the line number and lis the logging pattern to be analyzed.
.
.
rpc.
rpc allows users to call a remote procedure in the same way as calling a local function.
the client site cis the local call site and the message handler fis the invoked remote procedure.
in common practices e.g.
google protobuf rpc is realized via a client class and a corresponding server class with the same interface.
to identify rpc we require the user to specify the rpc client classes as well as the rpc server classes.
a rpc server class can be matched to its corresponding rpc client class by checking their public apis.
we automatically recognize classes that wrap rpc client classes using the delegation design pattern .
thus given a rpc client site we can easily find its rpc server class and the corresponding remote procedure to handle this message.
in figure containermanagementprotocolpbclientimpl is a rpc client class.
the call to proxy.startcontainer line 7esec fse november lake buena vista fl usa jie lu feng li lian li and xiaobing feng is the client site of rpc.
its corresponding rpc server is identified as containermanagerimpl .
hence the method handler f iscontainermanagerimpl.startcontainer line .
static message is containermanagerimpl.startcontainer l .
.
.
event dispatch.
the client site cis the call site to enqueue an event and the message handler fis the method to handle the dispatched event.
we abstract away the complicated implementation details of the asynchronous event dispatch mechanism.
here we require the user to specify the methods to enqueue an event and the methods to handle an event.
distinct types of events may be handled by different handlers.
in this case we check the type of the enqueued event and the type of the dispatched event formal arguments of the event handler to match the handler with an enqueued event of the same type.
in figure dispatcher.handle is the method to enqueue an event and eventhandler.handle line is the method to handle a dispatched event.
there are two events message and message .
their client sites are the call sites to dispatcher.handle method at line and respectively.
both events are handled by the same handler.
hence the two messages share a common message handler eventhandler.handle line .
.
.
message logging pattern.
we firstly locate a log point e.g.
call to log.info and their wrappers for each static message.
in the simple case a message is logged in the entry block of its message handler when the message is received and handled .
for example the log point for message eventprocessor.run l is line and the log point for message containermanagerimpl.start container l is line .
hence we search the entry block of the message handler f as well as the entry blocks of these methods invoked in the entry block of f for a log point.
if there exists multiple log points for a static message we group them together as one log point.
some systems implement a complicated handler for different types of events.
in hadoop2 the event handler implements a state machine which executes different cases and invokes different callback functions according to the incoming event type and the current state method eventhandler.handle in figure line .
in this case we require the user to specify the transition rules of the state machine e.g.
which callback function handles which type of event.
we can then statically check the type of the enqueued event at the client site to find a matching case in the handler and locate the corresponding log point.
in figure message eventhandler.handle l and message eventhandler.handle l are handled by the same method eventhandler.handle line .
the method executes different cases according to the incoming event type.
hence we check the event type at the client site for a matching case.
message enqueues an event of type taskattemptcontainerlauchedevent which sets its type field to ta container launched in the constructor.
by analyzing this field we can find a matching case lines .
the log point at line is located.
similarly we can deduce that message is handled by lines and .
there is no log point in its handler.
in this case we will search for a log point at the client site c in the basic block containing c. hence line is regarded as the log point of message .
messages share the samelog point are grouped together.
in our example message and message are grouped together.
the message logging patterns can then be extracted at each log point.
following previous work logging patterns are expressed using regular expressions.
we analyze the logging statement at each log point and the tostring method of logged variables to statically extract the constant strings in the log message.
runtime values of logged variable are denoted as .
table summarizes the logging patterns for each message in figure .
.
.
discussion.
we design our communication analysis in such a way that minimal user specification is required.
in our approach for rpc we require the user to specify the rpc client classes and server classes.
for event dispatch we require the user to specify the event enqueue method the event handler and the call back functions if there are any for different types of events.
the communication analysis then automatically analyzes each message client site identifies its corresponding handler and locates the right log point to extract its logging pattern.
the precision of communication analysis can be further improved if the user can provide detailed annotations to specify the client site the message handler and the logging pattern for each message.
alternatively we could get such precise information via instrumentation and profiling.
in the four different distributed systems we studied our communication analysis can correctly extract the logging patterns for the majority of messages without loss of precision.
.
log analysis log analysis tries to match each runtime log instance to a message logging pattern.
a log instance is represented as a tuple m val where mis the static message and valrecords the runtime values of logged variables.
table gives the representation of each runtime log instance in figure .
logs are ordered according to their time stamps the time stamps are calibrated to a centralized time to compensate for the time differences on distinct nodes as in .
we adopt the approach in to match each runtime log instance to a static message logging pattern efficiently.
a reverse index is built as a hash for each logging pattern which can be used to quickly calculate a matching score for each log instance.
the higher the score the more likely it is a match.
for each log instance we select message logging patterns with the highest scores then parse each log instance according to the logging patterns to find a exact match.
for each log instance we also record the runtime values of its logged variables as shown in table .
.
id analysis id analysis organizes related log instances in a hierarchy structure based on id values i.e.
runtime values of id variables .
the hierarchy structure represents tasks and sub tasks.
in distributed systems values of id variables are commonly used to distinguish distinct requests and tasks.
these variables are wrapped in messages and propagated to different nodes and threads.
therefore we regard a variable as an id variable if it is propagated from a message i.e.
formal arguments of message handlers or fields of runnable objects and variables accessible from them via direct or indirect field dereferencing and the variable is printed in logs.
it is very difficult to precisely analyze the propagation of id variables 8cloudraid hunting concurrency bugs in the cloud via log mining esec fse november lake buena vista fl usa table log instances m val .
log instances static message runtime values attempt 1514878932605 0001 m 000009 0 container 1514878932605 0001 01 000011 ...container 1514878932605 0001 01 000011 attempt 1514878932605 0001 m 000009 0 container 1514878932605 0001 01 000011 attempt 1514878932605 0001 m 000009 0 attempt 1514878932605 0002 m 000007 0 container 1514878932605 0002 01 000009 attempt 1514878932605 0002 m 000007 0 container 1514878932605 0002 01 000009 ...container 1514878932605 0002 01 000009 attempt 1514878932605 0002 m 000007 0 statically when complicated pointer and fields dereferencing are involved.
hence we statically analyze an initial set of id variables and use their runtime values to group log instances with same id values together.
these variables which are propagated from formal arguments or runnable object fields to a log point via direct assignments or field dereferences are included in the initial set of id variables.
in our example line we cannot statically determine whether the logged variable nmprivatectokenspath is propagated from formal arguments or not.
it is not included in the initial set of id variables.
however the log can still be grouped according to its runtime value which can be matched to an already existing id value containerid logged at line .
definition .
id value v1and id value v2arerelated if there exists a log instance m val such that both v1 valandv2 valhold.
let lbe the set of log instances of static message m. id value v1is a sub id of v2if for any log instance m val l v1 val leftr g tl ne v2 val but not vice versa.
log instances with a same id value are grouped together to perform a task.
a tasks is indexed with one id value and can be further divided into sub tasks.
two tasks are related if their id values are related.
one task is a sub task of another if its id variable is a sub id of another.
for our example the log instances in table are organized into two groups.
each group consists of two related tasks indexed by the runtime values attempt and container respectively.
.
hb analysis and message pair analysis hb analysis analyzes the happen before relation between static messages.
we consider two simple types of happen before relation.
given static message p cp fp lp and s cs fs ls .
ifcs is in method fp then phappens before s. ifcpdominates csand cpis a rpc client site then phappens before s. we compute the transitive happen before relation for all static messages.
message pair analysis selects the order p sfor further testing.
ifphappens before sorshappens before p then the order is either infeasible or always holds and it will not be selected.
we check whether pandsare related or not and whether the order p s have been exercised or not by comparing their log instances in a pair wise manner.
given log instance pi p and log instance si s .
ifpiandsibelongs to the same or related tasks then pandsare related.
if piandsiare related and piis logged beforesi then p shas already been tested.
the order p swill be selected if pandsare related and the order has not been exercised.
.
trigger and error report figure trigger order p s i.e.
flip the order s p. the trigger tries to exercise the selected message order p sby instrumenting the system in such a way that a dynamic instance sican wait until piis handled.
we introduce an prologue and epilogue for both sandp.
the prologue is instrumented before the message is handled and the epilogue is introduced after the message is handled.
for rpc and thread creation the prologue and epilogue are introduced at the entry and exit of the message handler respectively.
for event dispatches the prologue is introduced at the client site before the event is enqueued and the epilogue is inserted at the exit of the message handler.
the reason is that the prologue will block execution.
if the event handler is blocked no event can be dequeued and the event queue will soon be occupied.
as shown in figure we maintain a state machine at runtime.
initially neither pnorsis executed.
the start state is s0.
when executing s the prologue of ssends a sstart event to our runtime.
the state machine is updated to state s1 and swill sleep for a time interval t waiting for pto be executed.
the interval tis set to be the largest interval between related instances of piandsi in previous executions.
we introduce a to compensate for the delay of our instrumented code and the delay to handle message p. as a 9esec fse november lake buena vista fl usa jie lu feng li lian li and xiaobing feng result pwill have a large chance to be handled while sis waiting.
when pstarts to execute the prologue of psends a pstart event and the state machine is updated to s2.
after pfinishes execution the epilogue of psends a pendevent the state is updated to s3.
after waiting for t the prologue of ssends a timeout event then continues execution.
if phave already finished execution state s3 the message order can be successfully exercised.
the state reaches the final state s4when sfinishes execution.
otherwise if pdoes not arrive in time state s1 or has not finished execution state s2 the runtime state is reset to its initial state suggesting that we have not successfully trigger the order.
finally after execution cloudraid reports an error in the following cases system crash job hang or fail and there exists uncommon error level logs in the log file.
currently we do not report silent errors which lead to unexpected behaviors that are difficult to detect e.g.
silent data corruptions .
how to develop test oracles to automatically detect such unexpected behaviors is an important topic worth separate investigation.
evaluation table systems under testing.
system cloudraid code changes workload hadoop2 yarn wordcount kill hdfs putfile reboot hbase write node crash cassandra write we apply cloudraid to four representative real world distributed systems apache hadoop2 yarn distributed computing framework hdfs distributed file system hbase distributed key value stores and cassandra distributed key value stores .
table presents the systems.
column gives the code changes required in loc to adapt cloudraid to a new system.
on average we need to apply lines of code changes for each system to specify its communication patterns.
in this experiment we use failure triggering workload described in and run the systems using their default configurations including default logging configurations .
these workloads are also common workloads but errors may be triggered by untimely communication among nodes.
each system runs the workload times to generate runtime logs.
cloudraid then performs its analyses using these logs.
we have experimented with larger sets of logs up to runs and no noticeable difference is observed.
all the experiments are performed on a cluster with three identical nodes.
each node has a centos .
system on an intel r xeon r e7 processor with gb of memory.
the evaluation will answer the following research questions rq1.
how accurate can cloudraid extract message sequences from runtime logs?
rq2.
how effective is cloudraid in detecting bugs can it detect new bugs?
rq3.
how much does cloudraid improve testing efficiency?
figure number of static messages sm sm with logs sm lo sm with logs and ids sm lo i d and sm with logs and ids that have been executed.
.
rq1 accuracy figure summarizes the identified static messages in the systems.
there are static messages in hadoop2 yarn and static messages in hbase.
our communication analysis can successfully analyze the logging pattern for more than of static messages in all systems except for hdfs .
.
we manually inspected these static messages without logging patterns.
their logs are often optimized out for performance reasons.
hdfs frequently reads file systems without logging hence a large percentage of static messages in hdfs do not have a logging pattern.
id analysis can successfully find an id value in .
of the static messages for hadoop2 yarn and in .
of the static messages for hbase.
id values can effectively distinguish log instances from different runs.
however only .
static messages in hdfs and .
static messages in cassandra have an id value.
hdfs frequently invokes rpc to get or set the state of name node without id values.
cassandra mainly prints logs during system startup.
since we target live systems we process logs for user requests.
we further analyze these message logs without id values .
of them print variables such as size .
are daemon process printing service start or stop messages and the rest .
are due to bad log quality.
table statistics of runtime logs.
log instances is number of different types of runtime log instances.
static messages is the number of messages covered by runtime logs.
system log instances static messages sm smi d sm nonsm sm sm i d hadoop2 yarn hdfs hbase cassandra table gives the number of runtime log instances columns and the number of static messages covered by runtime logs columns and .
let us compare column with column .
in all benchmarks except for cassandra the majority of runtime log instances record message events .
for hadoop2 yarn .
for hdfs .
for hbase .
in comparing column2 to column we find that most message log instances also record id values .
10cloudraid hunting concurrency bugs in the cloud via log mining esec fse november lake buena vista fl usa for hadoop2 .
for hdfs and for hbase .
these systems provide valuable information for cloudraid to accurately recover the runtime message sequences.
cassandra is an exception with much fewer messages being logged and id values rarely being used in the logs.
the reason is that cloudraid process request logs while cassandra prints most of its logs during the system startup process.
as a result only .
runtime log instances in cassandra record a message event with id values which is also shown in figure .
the runtime log instances covers about of static messages column in table and figure .
the other uncovered part may need a distinct workload e.g.
alter table for hbase or a different configuration e.g.
to execute a distinct resource scheduling model in yarn or it is in an error handling module difficult to reach.
discussion.
the accuracy of message sequences extracted by cloudraid differs in different systems.
overall hadoop2 yarn provides the most accurate information in its logs.
cloudraid can analyze and process .
of the runtime logs and it exercises more static messages than the other benchmarks column in table .
cassandra rarely logs id values.
cloudraid can only analyze .
of runtime log instances and cannot accurately recover its runtime message sequences from logs.
.
rq2 effectiveness we evaluate how effective cloudraid is in testing known bugs as well as its ability in detecting new bugs.
table bug detection results against taxdc .
mr mr mr mr detected mr mr mr mr ca hbase hbase hbase not detectedmr mr mr mr mr hbase hbase hbase .
.
finding existing bugs.
we evaluate cloudraid against the taxdc benchmark suite .
the benchmarks in table are selected because we can manually trigger a failure by changing the order of a message pair in these benchmarks.
we skip those benchmarks in taxdc if we cannot reproduce the bug manually or if it involves timely hardware failure or if it requires to reorder multiple pairs of messages together.
cloudraid can report of the benchmarks automatically.
in of the undetected benchmarks mr mr mr hbase hbase and hbase their messages are not logged.
hence cloudraid fails to report errors in these benchmarks.
we argue that the log quality needs to be further improved in this case to help better diagnose this type of failure.
the two benchmarks mr and mr require instrumentation in the middle of their message handlers which cannot triggered by cloudraid.
a more sophisticated instrumentation strategy can help to trigger such errors.
.
.
detecting new bugs.
we evaluate the ability of cloudraid in detecting new bugs using the four systems in table .
for each system we select different versions the latest version the oldest version and random selected versions .
we apply cloudraid to test each version.
the same bug appearing in different versions is reported as one bug.table bug detection results against the systems in table .
system bugs new all order violation atomicity violation total hadoop2 yarn hdfs hbase cassandra total cloudraid can successfully find bugs including already tracked bugs and new bugs.
most of the bugs detected by cloudraid are message order violation bugs out of which is expected.
cloudraid also detects atomicity violation bugs.
by reordering messages cloudraid can impact other message handlers and make them execute concurrently.
cloudraid detects the most number of bugs out of column in table in hadoop2 yarn and none in cassandra.
cassandra is the benchmark with the least log information figure and table .
the limited log information largely restricts cloudraid s ability in detecting bugs.
table new bugs detected.
all bugs are confirmed by the original developers and of them are already fixed.
bug id type status patched?
symptom yarn order fixed yes attempt fail yarn order unresolved no wrong state yarn atomicity unresolved yes cluster down yarn order unresolved yes resource leak yarn order fixed yes job fail yarn order unresolved yes wrong state yarn order fixed yes null pointer hbase order unresolved no data loss table summarizes the new bugs detected by cloudraid.
these new bugs may lead to serious failure such as cluster down yarn7176 and data loss hbase .
by examining how cloudraid triggers a bug we can easily find its root cause and provide a patch.
we have provided patches to of the bugs and of them have been accepted by their original developers.
figure new atomicity violation bug detected by cloudraid.
messages and their handlers triggering the bug are highlighted in red.
11esec fse november lake buena vista fl usa jie lu feng li lian li and xiaobing feng yarn .
cloudraid detects a new atomicity violation bug in hadoop2 yarn as depicted in figure .
it successfully flips the normal execution order sstartamcontainer pstopamcontainer so that the message pstopamcontainer is handled first.
the message handler fstopamcontainer sends an update app message.
as a result its message handler fupdate app executes concurrently with the handler fstartcontainer .
a race condition is triggered by their concurrent execution and an arrayindexoutboundsexception is thrown crashing the yarn daemon process.
yarn .
in yarn cloudraid triggered an invalidstatetransitionexception error after reordering the start message with the kill message.
hence in our initial patch we simply ignore thestart message if it arrives after the kill message.
the original developer accepted our fix saying ignoring the start event seems to be appropriate here .
however he made another request could you add a unit test of the new start after killed transition logic ?
we then prepared our second patch with a unit test.
interestingly the unit test triggered another two bugs yarn and yarn7703 both are similar invalidstatetransitionexception errors in the state machine implementation of yarn.
although the developers of yarn have tested the state machine implementation with a large set of unit tests there are still numerous subtle cases not handled.
after another different versions of patches months after we reported the bug the developer finally accepted our patch and submitted it to the latest trunk and some previous trunks branch branch .
and branch .
.
discussion.
the effectiveness of cloudraid largely relies on the log quality of the system under testing.
for systems with rich log information hadoop2 yarn and hbase it is very effective.
however if the system only provides limited logs cassandra its ability is largely restricted.
.
rq3 efficiency table analysis and testing times of cloudraid.
system profiling s analysis s trigger s hadoop2 yarn .
.
.
hdfs .
.
.
hbase .
.
.
cassandra .
.
.
table reports the times in testing the latest versions of different systems with cloudraid.
column is the time in profiling each system i.e.
running each workload times.
in practice we can get logs from live systems without profiling.
column is the total analysis time including the time to analyze the source code and the time to parse all runtime logs from the runs.
column is the testing time to trigger all selected orderings.
cloudraid is very efficient.
it finishes its analyses in minutes for all benchmarks column .
in the testing phase column cloudraid finishes testing hadoop2 yarn in .
second .
hours .
cassandra takes less than minute to test.
it is because cloudraid can only extract very limited information from its runtime logs resulting in only message orderings to be tested.table message orderings pruned by each analysis.
total is the number of messages orderings.
hb is the percentage of orderings pruned by hb analysis.
order is the percentage of orderings already exercised.
id is the percentage of orderings where messages do not log related id values.
of pruned system total hb order id all hadoop2 yarn .
.
.
.
hdfs .
.
.
.
hbase .
.
.
.
cassandra .
.
.
table shows how cloudraid achieves efficiency by pruning message orderings using different analyses.
note that here we already filtered out these static messages not logged with id values column .
otherwise the total number of message orderings to be tested is for hadoop2 yarn.
overall cloudraid successfully prunes .
of total message orderings for hadoop2 yarn and .
for hbase column .
hb analysis only prunes very few message orders.
it is very difficult to precisely analyze the happen before order statically due to the complexity in these systems.
cloudraid efficiently prunes most message orderings by skipping those that have already been exercised column and those between unrelated messages column .
we randomly tested pruned message orderings and cannot find any new bugs.
this confirms our observation and assumption messages orderings pruned away by cloudraid are unlikely to expose errors.
when cloudraid tries to exercise all message orderings we find that only .
of them are triggered.
for these message orders not exercised .
of them do have strict happen before relation but our hb analysis fails to analyze the happen before order due to unrecognized control or data dependencies.
hence a more sophisticated may happen in parallel analysis can further improve efficiency.
the other .
of them are due to the fact that our current workload does not cover the specific ordering.
discussion.
cloudraid drastically improves efficiency by pruning away message orderings that are unlikely to expose errors.
we manually tested these message orderings pruned by cloudraid and can verify that they do not expose errors.
nevertheless it trades soundness for efficiency and we cannot guarantee that the pruned message order will not trigger any error.
related work we summarize previous works in detecting distributed concurrency bugs and existing log analysis techniques.
distributed concurrency bug detection.
there is a large body of research on distributed system model checkers .
these systems intercept messages in the system at runtime then permute their orderings exhaustively.
although powerful they face the state space explosion problem.
recent tools have adopted various state reduction techniques to address this problem.
however the more events included the larger the state space to be explored.
it takes up to days to explore some of the state space .
liu et al.
recently extended classic race detection techniques for multi threaded programs to detect race 12cloudraid hunting concurrency bugs in the cloud via log mining esec fse november lake buena vista fl usa conditions in distributed systems.
their technique instruments memory accesses and communication events in the target system to collect runtime traces during execution.
an offline analysis is performed to analyze the happen before relation between memory accesses using a sophisticate happen before model customized to distributed systems.
concurrent memory accesses that may trigger exceptions are regarded as harmful data races.
a trigger is employed to further verify the detected race conditions.
cloudraid differs from their approach in that we mine logs to recover runtime traces without instrumentation.
in addition we target errors triggered by reordering messages as those distributed system model checkers instead of memory accesses only.
fault injection techniques are commonly used to test the resilience of distributed systems.
existing techniques focus on how to inject fault at different system state to expose bugs in the fault recover handler.
cloudraid can be applied together with these techniques to detect fault related concurrency bugs more effectively.
log analysis.
many researchers mine logs to extract various information including temporal invariants user request flow system architecture timing information etc.
the mined information can then be applied to help with better understanding monitoring and analyzing the complicated distributed systems.
xu et al.
mine console logs from a system and apply machine learning techniques to detect anomaly executions.
mined information such as logged values and logging frequencies are visualized to help user diagnose anomaly behavior.
distalyzer compares logs from abnormal execution and normal execution to infer the strongest association between system components and performance.
iprof extracts request id and timing information from logs to profile request latency.
stitch organizes log instances into tasks and sub tasks by analyzing relations between logged id variables to profile different components in the entire distributed software stack.
cloudraid mines logs to uncover insufficiently exercised message orderings to effective detect new concurrency bugs.
conclusion we present cloudraid a simple yet effective tool to detect distributed concurrency bugs.
cloudraid achieves efficiency and effectiveness at the same time by analyzing message orderings that are likely to expose errors from existing logs.
our evaluation shows that cloudraid is easy to be adopted and is very effective in detecting bugs.
it finishes testing versions of different systems in hours and have successfully found bugs including new bugs that have never been reported before.
acknowledgement this work is supported by the national key research and development program of china 2016yfb1000402 the innovation research group of national natural science foundation of china and and the national natural science foundation of china u1736208 .