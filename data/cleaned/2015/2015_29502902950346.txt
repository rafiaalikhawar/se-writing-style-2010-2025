efficient generation of inductive validity cores for safety properties elaheh ghassabani department of computer science and engineering university of minnesota union street minneapolis mn usa ghass013 umn.eduandrew gacek rockwell collins advanced technology center collins rd.
ne cedar rapids ia usa andrew.gacek rockwellcollins.commichael w. whalen department of computer science and engineering university of minnesota union street minneapolis mn usa whalen cs.umn.edu abstract symbolic model checkers can construct proofs of properties over very complex models.
however the results reported by the tool when a proof succeeds do not generally provide much insight to the user.
it is often useful for users to have traceability information related to the proof which portions of the model were necessary to construct it.
this traceability information can be used to diagnose a variety of modeling problems such as overconstrained axioms and underconstrained properties and can also be used to measure completeness of a set of requirements over a model.
in this paper we present a new algorithm to efficiently compute the inductive validity core ivc within a model necessary for inductive proofs of safety properties for sequential systems.
the algorithm is based on the unsat core support built into current smt solvers and a novel encoding of the inductive problem to try to generate a minimal inductive validity core.
we prove our algorithm is correct and describe its implementation in the jkind model checker for lustre models.
we then present an experiment in which we benchmark the algorithm in terms of speed diversity of produced cores and minimality with promising results.
ccs concepts theory of computation !verification by model checking automated reasoning software and its engineering !requirements analysis formal software verification keywords traceability requirements completeness k induction ic3 pdr .
introduction symbolic model checking using induction based techniques such as ic3 pdr and k induction can often determine whether safety properties hold of complex finite or infinite state systems.
model checking tools are attractive both because they are automated requiring little or no interaction with the user and if the permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
fse november seattle wa usa c acm.
isbn .
.
.
.
to a correctness query is negative they provide a counterexample to the satisfaction of the property.
these counterexamples can be used both to illustrate subtle errors in complex hardware and software designs and to support automated test case generation .
in the event that a property is proved however it is not always clear what level of assurance should be invested in the result.
given that these kinds of analyses are performed for safetyand security critical software this can lead to overconfidence in the behavior of the fielded system.
it is well known that issues such as vacuity can cause verification to succeed despite errors in a property specification or in the model.
even for non vacuous specifications it is possible to over constrain the specification of theenvironment in the model such that the implementation will not work in the actual operating environment.
at issue is the level of feedback provided by the tool to the user.
in most tools when the answer to a correctness query is positive no further information is provided.
what we would like to provide is traceability information an inductive validity core ivc that explains the proof in much the same way that a counterexample explains the negative result.
this is not a new idea unsat cores provide the same kind of information for individual sat or smt queries and this approach has been lifted to bounded analysis for alloy in .
what we propose is a generic and efficient mechanism for extracting supporting information similar to an unsat core from the proofs of safety properties using inductive techniques such as pdr and k induction.
because many properties are not themselves inductive these proof techniques introduce lemmas as part of the solving process in order to strengthen the properties and make them inductive.
our technique allows efficient accurate and precise extraction of inductive validity cores even in the presence of such auxiliary lemmas.
once generated the ivc can be used for many purposes in the software verification process including at least the following vacuity detection the idea of syntactic vacuity detection checking whether all subformulae within a property are necessary for its satisfaction has been well studied .
however even if a property is not syntactically vacuous it may not require substantial portions of the model.
this in turn may indicate that either a. the model is incorrectly constructed or b. the property is weaker than expected.
we have seen several examples of this mis specification in our verification work especially when variables computed by the model are used as part of antecedents to implications.
completeness checking closely related to vacuity detection is the idea of completeness checking e.g.
are all atoms in the model necessary for at least one of the properties proven permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
fse november seattle wa usa c acm.
... .
node filter x real returns a b y real let a f x .
pre y b if a .
then a else a y b .
pre y tel figure model with property y before ivc analysis about the model?
several different notions of completeness checking have been proposed but these are very expensive to compute and in some cases provide an overly strict answer e.g.
checking can only be performed on nonvacuous models for .
traceability certification standards for safety critical systems e.g.
usually require traceability matrices that map high level requirements to lower level requirements and eventually leaf level requirements to code or models.
current traceability approaches involve either manual mappings between requirements and code models or a heuristic approach involving natural language processing .
both of these approaches tend to be inaccurate.
for functional properties that can be proven with inductive model checkers inductive validity cores can provide accurate traceability matrices with no user effort.
symbolic simulation test case generation model checkers are now often used for symbolic simulation and structuralcoverage based test case generation .
for either of these purposes the model checker is supposed to produce a witness trace for a given coverage obligation using a trap property which is expected to be falsifiable.
in systems of sufficient size there is often dead code that cannot ever be reached.
in this case a proof of non reachability is produced and the ivc provides the reason why this code is unreachable.
nevertheless to be useful for these tasks the generation process must be efficient and the generated ivc must be accurate and precise that is sound and close to minimal .
the requirement for accuracy is obvious otherwise the minimal set of model elements is no longer sufficient to produce a proof so it no longer meets our ivc definition.
minimality is important because for traceability we do not want unnecessary model elements in the trace matrix and for completeness it may give us a false level of confidence that we have enough requirements.
in addition we are also interested in diversity how many different ivcs can be computed for a given property and model?
requirements engineers often talk about the traceability matrix or the satisfaction argument .
if proofs are regularly diverse then there are potentially many equally valid traceability matrices and this may lead to changes in traceability research.
in the remainder of this paper we present an algorithm for efficient generation of ivcs for induction based model checkers.
our contributions as detailed in the remainder of the paper are as follows node filter x a real returns b y real let b if a .
then a else a y b .
pre y tel figure model with property y after ivc analysis we present a technique for extracting inductive validity cores from an inductive verification of a safety property over a sequential model involving lemmas.
we formalize this technique and present an implementation of it in the jkind model checker .
we present an experiment over our implementation and measure the efficiency minimality and robustness of the ivc generation process.
the rest of this article is organized as follows.
in section we present a motivating example.
in section we present the required background for our approach.
in sections and we present our approach and our implementation in jkind.
sections and present an evaluation of our approach on a set of benchmark examples.
finally section discusses related work and section concludes.
.
motiv ating example consider the model shown both graphically and textually in figure .
this model takes an input combines it with the previous output in some way takes the absolute value and then adds this to an accumulating value.
this model has the property that the output is always non negative i.e.
y .
moreover it happens that this property holds regardless of the way the input is combined with the previous output i.e.
the function fin the model.
formally we say that the minimal inductive validity core ivc does not contain that part of the model.
the model reduced to a minimal ivc is shown in figure .
note that traditional static dependency analysis i.e.
abackward static slice would not be able to remove ffrom the original model.
in our experiments in section we demonstrate that ivcs are much smaller and more precise than static slices.
.
preliminaries given a state space s a transition system i t consists of an initial state predicate i s!bool and a transition step predicatet s s!bool.
we define the notion of reachability for i t as the smallest predicate r s!bool which satisfies the following formulas 8s i s r s 8s s0 r s t s s0 r s0 a safety property p s!bool is a state predicate.
a safety propertypholds on a transition system i t if it holds on all reachable states i.e.
8s r s p s written as r pfor short.
when this is the case we write i t p. 315i s0 p s0 ... i s0 t s0 s1 t sk sk p sk p s0 t s0 s1 p sk t sk sk p sk figure k induction formulas kbase cases and one inductive step for an arbitrary transition system i t computing reachability can be very expensive or even impossible.
thus we need a more effective way of checking if a safety property pis satisfied by the system.
the key idea is to over approximate reachability.
if we can find an over approximation that implies the property then the property must hold.
otherwise the approximation needs to be refined.
a good first approximation for reachability is the property itself.
that is we can check if the following formulas hold 8s i s p s 8s s0 p s t s s0 p s0 if both formulas hold then pisinductive and holds over the system.
if fails to hold then pis violated by an initial state of the system.
if fails to hold then pis too much of an overapproximation and needs to be refined.
one way to refine our over approximation is to add additional lemmas to the property of interest.
for example given another propertyl s!bool we can consider the extended property p0 s p s l s written asp0 p lfor short.
if p0 holds on the system then pmust hold as well.
the hope is that the addition oflmakes formula provable because the antecedent is more constrained.
however the consequent of is also more constrained so the lemma lmay require additional lemmas of its own.
finding and proving these lemmas is the means by which property directed reachability pdr strengthens and proves a safety property.
another way to refine our over approximation is to use use kinduction which unrolls the property over ksteps of the transition system.
for example induction consists of formulas and above whereas induction consists of the following formulas 8s i s p s 8s s0 i s t s s0 p s0 8s s0 s00 p s t s s0 p s0 t s0 s00 p s00 that is there are two base step checks and one inductive step check.
in general for an arbitrary k k induction consists of kbase step checks and one inductive step check as shown in figure the universal quantifiers on sihave been elided for space .
we say that a property isk inductive if it satisfies the k induction constraints for the given value of k. the hope is that the additional formulas in the antecedent of the inductive step make it provable.
in practice inductive model checkers often use a combination of the above techniques.
thus a typical conclusion is of the form p with lemmas l1 l nisk inductive .
.
inductive v alidity cores given a transition system which satisfies a safety property p we want to know which parts of the system are necessary for satisfying the safety property.
one possible way of asking this is what isalgorithm ivc bf brute force algorithm for computing a minimal ivc input i t p output minimal inductive validity core for i t p 1s t forx2sdo if i snfxg pthen 4s snfxg return s the most general version of this transition system that still satisfies the property?
the answer is disappointing.
the most general system isi s p s andt s s0 p s0 i.e.
you start in any state satisfying the property and can transition to any state that still satisfies the property.
this answer gives no insight into the original system because it has no connection to the original system.
in this section we introduce the notion of inductive validity cores ivc which looks at generalizing the original transition system while preserving a safety property.
in order to talk about generalizing a transition system we assume the transition relation of the system has the structure of a toplevel conjunction.
this assumption gives us a structure that we can easily manipulate as we generalize the system.
given t s s0 t1 s s0 tn s s0 we will write t t1 tnfor short.
by further abuse of notation we will identify twith the set of its top level conjuncts.
thus we will write x2tto mean that x is a top level conjunct of t. we will write s tto mean that all top level conjuncts of sare top level conjuncts of t. we will write tnfxgto meantwith the top level conjunct xremoved.
we will use the same notation when working with sets of invariants.
definition .
inductive validity core let i t be a transition system and let pbe a safety property with i t p. we say s tis an inductive validity core for i t piff i s p. wheni t andpcan be inferred from context we will simply say sis an inductive validity core.
definition .
minimal inductive validity core an inductive validity coresfor i t pis minimal iff there does not exist m ssuch thatmis an inductive validity core for i t p. note that minimal inductive validity cores are not necessarily unique.
for example take i a b t a0 b0 andp a b. then bothfa0gandfb0gare minimal inductive validity cores for i t p. however inductive validity cores do have the following monotonicity property.
lemma .
let i t be a transition system and let pbe a safety property with i t p. lets1 s2 t. ifs1is an inductive validity core for i t pthens2is an inductive validity core for i t p. proof .
froms1 s2we haves2 s1.
thus the reachable states of i s are a subset of the reachable states of i s .
this lemma gives us a simple brute force algorithm for computing a minimal inductive validity core algorithm ivc bf .
the resulting set of this algorithm is obviously an inductive validity core for i t p. the following lemma shows that it is also minimal.
lemma .
the result of algorithm is a minimal inductive validity core for i t p. 316algorithm ivc uc efficient algorithm for computing a nearly minimal inductive validity core from unsat cores input pwith invariants qisk inductive for i t output inductive validity core for i t p 1k minimize k t p q 2r reduce invariants k t q p return minimize ivck i t r basequery1 i t p 8s0 i s0 p s0 basequeryk i t p basequeryk i t p 8s0 s k i s0 t s0 s1 t sk sk p sk indqueryk t q p 8s0 s k q s0 t s0 s1 q sk t sk sk p sk fullqueryk i t p basequeryk i t p indqueryk t p p figure k induction queries proof .
let the result be r. suppose towards contradiction that ris not minimal.
then there is an inductive validity core mwith m r. takex2rnm.
sincex2rit must be that during the algorithm i snfxg pis not true for some set swhere r s. we havem r sandx62m thusm snfxg.
sincemis an inductive validity core lemma says that snfxg is an inductive validity core and so i snfxg p. this is a contradiction thus rmust be minimal.
this algorithm has two problems.
first checking if a safety property holds is undecidable in general thus the algorithm may never terminate even when the safety property is easily provable over the original transition system.
second this algorithm is very inefficient since it tries to re prove the property multiple times.
the key to a more efficient algorithm is to make better use of the information that comes out of model checking.
in addition to knowing that pholds on a system i t suppose we also know something stronger pwith the invariant set qisk inductive for i t .
this gives us the broad structure of a proof for pwhich allows us to reconstruct the proof over a modified transition system.
however we must be careful since this proof structure may be more than is actually needed to establish p. in particular qmay contain unneeded invariants which could cause the inductive validity core forp qto be larger than the inductive validity core for p. thus before computing the inductive validity core we first try to reduce the set of invariants to be as small as possible.
this operation is expensive when kis large so as a first step we minimize k. this is the motivation behind algorithm ivc uc .
to describe the details of algorithm we define queries for the base and inductive steps of k induction figure .
note in indquery t q p we separate the assumptions made on each step q from the property we try to show on the last step p. we use this separation when reducing the set of invariants.
we assume that our queries are checked by an smt solver.
that is we assume we have a function c heck sat f which determines iff an existentially quantified formula is satisfiable or not.
in order to efficiently manipulate our queries we assume the ability to create activation literals which are simply distinguished boolean variables.
the call c heck sat a f holds the activation literalsalgorithm minimize k t p 1k0 while check sat indqueryk0 t p p sat do 3k0 k0 returnk0 algorithm reduce invariants k t fq1 q ng p 1r fpg 2create activation literals a fa1 a ng 3c a1 q1 an qn whiletrue do check sat a indqueryk t c r if unsat core then return r forai2unsat core do 9r r fqig 10c cnfai qig inatrue while checking f. whenfis unsatisfiable we assume we have a function u nsat core which returns a minimal subset of the activation literals such that the formula is unsatisfiable with those activation literals held true.
in practice smt solvers often return a non minimal set but we can minimize the set via repeated calls to c heck sat.
we assume both c heck satand unsat core are always terminating.
the function m inimize k t p is defined in algorithm .
this function assumes that pisk inductive for i t .
it returns the smallestk0such thatpisk0 inductive for i t .
we start checking atk0 1since smaller values of k0are much quicker to check than larger ones.
the checking must eventually terminate since piskinductive.
we also only check the inductive query since we know the base query will be true for all k0 k. although we describe each query in algorithm separately in practice they can be done incrementally to improve efficiency.
the function r educe invariants k t fq1 q ng p is defined in algorithm .
this function assumes that p q1 qnisk inductive for i t .
it returns a set r fp q q ng such thatrisk inductive for i t andp2r.
like m inimize k this function only checks the inductive query since each element ofris an invariant and therefore will always pass the base query.
a significant complication for reducing invariants is that some invariants may mutually need each other even though none of them are needed to prove p. thus in algorithm we find a minimal set of invariants needed to prove p then we find a minimal set of invariants to prove those invariants and so on.
we terminate when no more invariants are needed to prove the properties inr.
algorithm is guaranteed to terminate since rgets larger in every iteration of the outer loop and it is bounded above by fp q q ng.
as with algorithm we describe each query in algorithm separately though in practice large parts of the queries can be re used to improve efficiency.
this iterative lemma determination does not guarantee a minimal result.
for example we may find prequires just q1 that q1requires just q2 and thatq2does not require any other invariants.
this gives the result fp q q2g but it may be that q2 alone is enough to prove pthus the original result is not minimal.
also note we do not care about the result of c heck sat only the unsat core that comes out of it.
since p q1 qnis 317algorithm minimize ivck i ft1 t ng p 1create activation literals a fa1 a ng 2t a1 t1 an tn 3check sat a fullqueryk i t p 4r forai2unsat core do 6r r ftig return r k inductive we know the c heck satcall will always return unsat.
the function m inimize ivck i ft1 t ng p is defined in algorithm .
this function assumes that pisk inductive for i t .
it returns a minimal inductive validity core r ft1 t ngsuch thatpisk inductive for i r .
it is trivially terminating.
since algorithms and are terminating algorithm is always terminating.
our full inductive validity core algorithm in algorithm does not guarantee a minimal inductive validity core.
one reason is that reduce invariants does not guarantee a minimal set of invariants.
a larger reason is that we only consider the invariants that the algorithm is given at the outset.
it is possible that there are other invariants which could lead to a smaller inductive validity core but we do not search for them.
in sections and we show that in practice our algorithm is nearly minimal and much more efficient than the naive algorithm.
the following theorem shows that minimality checking is at least as hard as model checking and therefore undecidable in many settings.
theorem .
determining if an ivc is minimal is as hard as model checking.
proof .
consider an arbitrary model checking problem i t ?pwherepis not a tautology.
we will construct an ivc for a related model checking problem which will be minimal if and only if i t 0p.
letxandybe fresh variables.
construct a transition system with initial predicate i xand transition predicate x0 y0 y0 p0 t .
the constructed system clearly satisfies the property x p. thuss fx0 y0 y0 p0 tg is an ivc.sis minimal if and only if neither fx0 y0gnor f y0 p0 tgis an ivc.
since xandyare fresh and pis not a tautology fx0 y0gis not an ivc.
since xandyare fresh f y0 p0 tgis an ivc for the property x pif and only if i t p. therefore sis minimal if and only if i t 0p.
when minimality is a necessity we can combine ivc bf and ivc uc into a single algorithm which aims to efficiently guarantee minimality.
the hybrid algorithm ivc ucbf consists of running ivc uc to generate an initial nearly minimal ivc which is then run through ivc bf to guarantee minimality.
the resulting algorithm is not guaranteed to terminate since ivc bf is not guaranteed to terminate.
.
implementation we have implemented the inductive validity core algorithms in the previous section in two tools jkind which performs the ivc uc algorithm and jsupport which can compute either the ivc bf or the ivc ucbf algorithm using jkind as a subprocess .
moreover our implementation of ivc ucbf uses an additional feature of jkind to store and re use discovered invariants between separate runs.
this reduces some of the cost of attemptingto re prove a property multiple times.
these tools operate over the lustre language which we briefly illustrate below.
.
lustre and ivcs lustre is a synchronous dataflow language used as an input language for various model checkers.
the textual models in figures and are written in lustre.
we will use model in figure as a running example in this section.
for our purposes a lustre program consists of input variables xin the example output variables a b and yin the example and an equation for each output variable.
a lustre program runs over discrete time steps.
on each step the input variables take on some values and are used to compute values for the output variables on the same step.
in addition equations may refer to the previous value of a variable using the pre operator.
this operator is underspecified in the first step so the arrow operator is used to guard the pre operator.
in the first step the expression e1 e2 evaluates to e1 and it evaluates to e2in all other steps.
we interpret a lustre program as a model specification by considering the behavior of the program under all possible input traces.
safety properties over lustre can then be expressed as boolean expressions in lustre.
a safety property holds if the corresponding expression is always true for all input traces.
for example the property for figure is y which is a valid property.
it is straightforward to translate this interpretation of lustre into the traditional initial and transition relations.
we will show this by continuing with the example in figure .
first we introduce a new boolean variable init into the state space to denote when the system is in its initial state the state of the system prior to initialization.
in the initial state all other variables are completely unconstrained which models the underspecification of the pre operator during the first step.
then we define i x a b y init init t x a b y init x0 a0 b0 y0 init0 a0 f x0 ifinitthen 0elsey b0 ifa0 0thena0else a0 y0 b0 ifinitthen 0elsey init0 note thatfis unspecified in figure and so also in t. in a real system fwould be defined in the lustre model and expanded in t. a safety property such as y is translated into init y .
nested uses of arrow and pre operators are handled by introducing new output variables for nested expressions though such details are unimportant for our purposes.
each equation in the lustre program is translated into a single top level conjunct in the transition relation.
this is very convenient as the ivc of a lustre property can be reported in terms of the output variables whose equations are part of the ivc.
equivalently the interpretation of an ivc for a lustre property is that any output variable that is not part of the ivc can be turned into an input variable its equation thrown away while preserving the validity of the property.
thus the granularity of the ivc analysis is determined by the granularity of the lustre equations and can be adjusted by introducing auxiliary variables for subexpressions if desired.
.
jkind jkind is an infinite state model checker for safety properties.
jkind proves safety properties using multiple cooperative engines in parallel including k induction property directed reachabil318ity and template based lemma generation .
jkind accepts lustre programs written over the theory of linear integer and real arithmetic.
in the back end jkind uses an smt solver such as z3 yices mathsat or smtinterpol .
jkind works on multiple properties simultaneously.
when a property is proven and ivc generation is enabled an additional parallel engine executes algorithm to generate a nearly minimal ivc.
jkind accepts an annotation on its input lustre program indicating which outputs variables to consider for ivc generation.
output variables not mentioned in the annotation are implicitly included in all ivcs.
this allows the implementation to focus on the variables important to the user and ignore for example administrative equations.
this is even more important for tools which generate lustre as they often create many such administrative equations which simply wire together more interesting expressions.
.
experiment we would like to investigate both the efficiency and minimality of our three algorithms the naive brute force algorithm ivc bf the unsat core based algorithm ivc uc and the combined unsat core followed by brute force minimization algorithm ivc ucbf .
efficiency is computed in terms of wall clock time how much overhead does the ivc algorithm introduce?
minimality is determined by the size of the ivc cores with a smaller number of variables are preferred to cores with a larger number of variables.
finally we are interested in the diversity of solutions how often do different tools algorithms generate different minimal ivcs?
the use of jkind allows additional dimensions to our investigation it supports two different inductive algorithms k induction and pdr and a fastest mode that runs both algorithms in parallel.
in addition jkind supports multiple back end smt solvers including z3 yices mathsat and smtinterpol .
we would like to determine whether the choice of inductive algorithm affects the size of the ivc whether different solvers are more or less efficient at producing ivcs and whether running different solvers algorithms leads to diversity of ivc solutions.
therefore we investigate the following research questions rq1 how expensive is it to compute inductive validity cores using the ivc bf ivc uc and ivc ucbf algorithms?
rq2 how close to minimal are the ivc sets computed by ivc uc as opposed to the guaranteed minimal ivc ucbf?
how do the sizes of ivcs compare to static slices of the model?
rq3 how much diversity exists in the solutions produced by different solver induction algorithm configurations?
.
experimental setup in this study we started from a suite of lustre models developed as a benchmark suite for .
we augmented this suite with additional models from recent verification projects including avionics and medical devices .
most of the benchmark models from are small 10kb or less with equations and contain a range of hardware benchmarks and software problems involving counters.
the additional models are much larger around 80kb with over equations.
we added the new benchmarks to better check the scalability for the tools especially with respect to the brute force algorithm.
each benchmark model has a single property to analyze.
for our purposes we are only interested inmodels with a valid property though it is perhaps worth noting that there is no additional computation and thus no overhead using the jkind ivc options for invalid properties .
in our benchmark set models yield counterexamples and additional models are neither provable nor yield counterexamples in our test configuration see next paragraph for configuration information .
the benchmark suite therefore contains models with valid properties which we use as our test subjects.
for each test model we computed ivc uc in configurations the twelve configurations were the cross product of all solvers z3 yices mathsat smtinterpol and inductive algorithms k induction pdr fastest and the remaining configuration was an instance of ivc bf run on yices which is the default solver in jkind.
in addition for each of the configurations we ran an instance of jkind without ivc to examine overhead.
the experiments were run on an intel r i5 2430m .40ghz 4gb memory machine with a hour timeout for each analysis on any model.
the data gathered for each configuration of each model included the time required to check the model without ivc with ivc and also the set of elements in the computed ivc.
note that not all analysis problems were solvable with all algorithms for all solvers k induction without ivc was unable to solve of the examples.
when comparing minimality of different solving algorithms we only considered cases where both algorithms provided a solution as will be discussed in more detail in section .
.
.
results in this section we examine our experimental results from three perspectives performance minimality of ivc uc results and diversity.
.
performance in this subsection we examine the performance of our inductive validity core algorithms research question rq1 .
first we examine the performance overhead of the ivc uc algorithm over the time necessary to find a proof using inductive model checking.
to examine this question we use the default fastest option of jkind which terminates when either the k induction or pdr algorithm finds a proof.
to measure the performance overhead of the ivc uc algorithm we execute it over the proof generated by the fastest option.
since the ivc uc algorithm uses the unsat core facilities of the underlying smt solver the performance is dependent on the efficiency of this part of the solver.
looking at tables and it is possible to examine both the computation time for analysis using the four solvers under evaluation and the overhead imposed by the ivc uc algorithm.
figure .
allows a visualization of the runtime for the ivc uc algorithm running different solvers.
the data suggests that yices the default solver in jkind and z3 are the most performant solvers both in terms of computation time and overhead.
the ivc uc algorithm using the z3 and yices smt solvers adds a modest performance penalty to the time required for inductive proofs.
next we consider the overhead of ivc uc vs. ivc bf.
recall from section that ivc bf requires nmodel checking runs wherenis the number of conjuncts in the transition relation.
as expected the performance is approximately a linear multiple of the 1the benchmarks all raw experimental results and computed data are available on .
319table ivc uc runtime with different solvers runtime sec min max mean stdev z3 .
.
.
.
yices .
.
.
.
smtinterpol .
.
.
.
mathsat .
.
.
.
table overhead of ivc uc computations using different solvers solver min max mean stdev z3 .
.
.
.
yices .
.
.
.
smtinterpol .
.
.
.
mathsat .
.
.
.
size of the model so larger models yield substantially lower performance.2we run the brute force algorithm using yices as it is the default solver for jkind and is close to z3 in terms of computation time.
for models ivc bf times out after hour.
figure shows the overhead of ivc bf in comparison to ivc uc with multiple solvers.
the brute force algorithm ivc bf adds a substantial performance penalty to inductive proofs in all cases and is not scalable enough to compute a minimal core for large analysis problems.
finally we consider the combined ivc ucbf algorithm in which we first run the ivc uc to determine a close to minimal ivc then run ivc bf on the remaining set.
the overhead of this algorithm is considered in tables and .
while considerably slower than ivc uc this approach can still be used for reasonably sized models.
.
minimality in this section we examine the minimality of the cores computed by the ivc uc and ivc ucbf algorithms using different inductive proof methods and we compare both algorithms against abackward static slice of the lustre program starting with the property of interest.
there are three interesting aspects to be examined related to this research question.
first rq2.
does the choice of smt solver or algorithm used to produce a proof kinduction or pdr matter in terms of the minimality of the inductive core?
as mentioned in section the ivc uc algorithm is not guaranteed to produce a minimal core due in part to the role of invariants used in producing a proof as k induction and pdr use substantially different invariant generation algorithms it is likely that the set of necessary invariants for proofs are dissimilar and that this would in turn affect the number of model elements required for the proof.
it is possible that one or the other algorithm is 2for lustre models the number of conjuncts is equivalent to the number of equations in the lustre model.
table ivc ucbf runtime runtime sec min max mean stdev yices .
.
.
.
z3 .
.
.
.27table overhead of ivc ucbf algorithm solver min max mean stdev yices .
.
.
.
z3 .
.
.
.
table aggregate ivc sizes produced by ivc uc using different inductive algorithms and solvers solver pdrk induction total z3 yices mathsat smtinterpol total more likely to yield smaller invariant sets.
in addition differences in the choice of the unsat core algorithms in the different solvers could affect the size of the generated core.
however our algorithm already performs a minimization step on unsat cores and thus the only differences would be due to one algorithm leading to a different minimal core than another.
as discussed in section k induction is unable to solve all of the analysis problems therefore we include only models that are solvable using bothk induction and pdr by all solvers models in all.
examining the aggregate data in table we can see the sizes of cores produced by different algorithms and solvers.
neither pdr nor k induction yields a smaller inductive validity core in general.
the choice of underlying smt solver does not substantially affect the size of the inductive validity cores.
the next question rq2.
asks how close to minimal are the cores produced by ivc uc vs. the guaranteed minimal cores produced by the ivc ucbf algorithm?
note that we cannot measure the distance on all models because the combined algorithm times out on of the larger models.
we therefore examine the distance from minimal cores produced by the combined algorithm for models in which it completes within the one hour timeout.
for comparison we run the ivc uc algorithm using z3 and yices with jkind s default fastest algorithm which will use the result of eitherk induction or pdr.
a graph showing the size of the ivcs for each model produced using the yices solver is shown in figure .
in the figure the models are ranked along the x axis by the size of the core produced by ivc ucbf.
the figure demonstrates that while on average there is a modest change in minimality there can be substantial variance on the sizes of the cores produced by the ivc uc algorithm.
summary statistics are shown in table .
the ivc uc algorithm computes cores that are on average larger than those produced by ivc ucbf with substantial variance in some cases.
the final question rq2.
asks how well the approach compares to backwards static slicing since slicing also reduces table increase in ivc size for ivc uc vs. ivc ucbf solver min max mean stdev yices .
.
.
.
z3 .
.
.
.
320figure ivc uc performance on different solvers figure runtime of ivc bf ivc ucbf ivc uc algorithms for yices figure ivc sizes produced by ivc uc ivc ucbf for yices vs. static slices 321table pairwise jaccard distances among all models min max mean stdev .
.
.
.
the set of model elements necessary to construct a proof.
we start the slice from the equation defining the property of interest and use the usual approach that performs an iterative backward traversal from the variables used within an equation to their defining equations.
we expect the ivc mechanism to be more precise because the slice overapproximates of the set of equations necessary for anyproof.
this claim is demonstrated in figure slices are mean larger than the ivcs produced by our ivc uc algorithm and larger than those produced by ivc ucbf algorithm.
both ivc algorithms compute cores that are usually much smaller than backwards static slices.
comparing the sizes of the ivc uc ivcs to the original models the original benchmark models from already had applied slicing so there is no difference between the sliced size and the original model size.
for the remaining benchmarks the number of equations is mean larger than the ivc uc ivcs.
we note however that comparison of ivc size against the original model size can be misleading as the improvement can easily be gamed by adding equations that are irrelevant to the property.
.
diversity recall from section that a minimal ivc set is any set leading to a proof such that if you remove any of its elements it no longer produces a proof.
for certain models and properties it is possible that there are many minimal cores that will lead to a proof.
in this section we examine the issue of diversity do different solvers and algorithms lead to different minimal cores?
this is both a function of the models and the solution algorithms for certain models there is only one possible minimal ivc set whereas other models might have many.
given that there are multiple solutions the interesting question is whether using different solvers and algorithms will lead to different solutions.
the reason diversity is considered is that it has substantial relevance to some of the uses of the tool e.g.
for constructing multiple traceability matrices from proofs see section .
note that our exploration in this experiment is not exhaustive but only exploratory based on the ivcs returned by different algorithms and tools we leave exhaustive exploration of ivcs for future work.
to measure diversity of ivcs we use jaccard distance definition .
jaccard distance dj a b ja bj ja bj dj a b jaccard distance is a standard metric for comparing finite sets assuming that both sets are non empty by comparing the size of the intersection of two sets over its union.
for each model in the benchmark the experiments generated potentially different ivcs.
therefore we obtained combinations of pairwise distances per model.
then minimum maximum average and standard deviation of the distances were calculated figure by which again we calculated these four measures among all models.
as seen in table on average the jaccard distance between different solutions is small but the maximum is close to which indicates that even for our exploratory analysis there are models for which the tools yield substantially diverse solutions.
the diver sity between solutions is represented graphically in figure where for each model we present the min max and mean pairwise jaccard distance of the solutions produced by algorithm ivc uc for each model ranked by the mean distance.
.
discussion in the previous section we presented three algorithms for determining inductive validity cores.
the brute force algorithm is guaranteed minimal but is often very slow.
the other two algorithms the unsat core algorithm ivc uc and the combined algorithm ivc ucbf represent interesting trade offs.
the ivc uc algorithm is much faster but is not guaranteed to be minimal the result of this algorithm can be further and sometimes quickly refined by the combined algorithm.
thus we can choose to trade off speed for guaranteed minimality using these two algorithms the combined algorithm can be viewed as a refinement algorithm that we can terminate either at completion or after a fixed time bound.
although our experiment does not ask statistical questions it is still worth examining threats towards generalizing our results.
first are the models and properties that we chose representative?
we started from an existing benchmark from another research group suite to try to assuage this concern but most of these models were small so we extended the benchmark suite with of our own models.
it is possible that our additions skew the results though these models are immediately derived from previously published work and not modified for our analysis here.
second our models and tools use the lustre language which is equational rather than conjuncted transition systems it is possible though in our opinion unlikely that arbitrary conjuncts rather than equations will yield different performance or minimality characteristics.
our approach is limited by the capabilities of the smt solvers and inductive model checking algorithms that are used.
for example it is difficult given state of the art smt solvers to produce proofs involving complex models involving non linear floatingpoint arithmetic.
however given an inductive proof produced by an unsat core producing smt solver we feel confident that the ivc uc algorithm can produce an ivc.
our approach is theory and invariant generator agnostic so as inductive model checking algorithms evolve and smt solvers add support for new theories the ivc algorithm should be able to work without modification.
.
related work our work builds on top of a substantial foundation building minimally unsatisfiable subformulas muses from unsat cores including .
recent algorithms can handle very large problems but computing muses is still a resourceintensive task.
while some work is aimed at providing a set of minimal unsatisfiable formulae minimality is usually defined such that given a set of clauses m removing any member of mmakes it satisfiable .
the step of producing minimal invariants for proofs has been investigated in depth by ivrii et al.
in .
unsat cores and muses are used for many different activities within formal verification.
gupta et al.
and mcmillan and amla introduced the use of unsatisfiable cores in proof based abstraction engines.
their goal is to shrink the abstraction size by omitting the parts of the design that are irrelevant to the proof of the property under verification.
torlak et al.
in finds muses of alloy specifications and considers semantic vacuity which we consider in section .
alloy models are only analyzed up to certain size bounds however and in general are unable to prove properties for arbitrary models.
also because we are extracting information from proofs it is possible to use ivcs for additional purposes proof explanation and completeness checking .
322figure pairwise jaccard distance between ivcs if we view lustre as a programming language our work can be viewed as a more accurate form of program slicing .
we perform backwards slicing from the formula that defines the property of interest of the model.
the slice produced is smaller and more accurate than a static slice of the formula but guaranteed to be a sound slice for the formula for all program executions unlike dynamic slicing .
predicate based slicing has been used to try to minimize the size of a dynamic slice.
our approach may have utility for some concerns of program slicing such as model understanding by constructing simple requirements of a model and using the tool to find the relevant portions of the model.
another potential use of our work is for semantic vacuity detection.
a standard definition of vacuity is syntactic and defined as follows a system k satisfies a formula vacuously iff k and there is some subformula of such that does not affect in k. vacuity has been extensively studied considering a range of different temporal logics and definitions of affect .
on the other hand our work can be used to consider a broader definition of vacuity.
even if all subformulae are required the property is not syntactically vacuous it may not require substantial portions of the model and so may be provable for vacuous reasons.
the problem is exacerbated when the modeling and property language are the same as in jkind because whether a subformula is considered part of the model or part of the property from the perspective of checking tools can be unclear.
determining completeness of properties has also been extensively studied.
certification standards such as do 178c require that requirements derived tests achieve some level of structural coverage mc dc decision statement depending on the criticality level of the software in order to approximate completeness.
if coverage is not achieved then additional requirements and tests are added until coverage is achieved.
chockler defined the first completeness metrics directly on formal properties based on mutation coverage.
later work by kupferman et al.
defines completeness as an extension of vacuity to elements in the model.
we present an alternative approach that uses the proof directly which we expect to be considerably less expensive to compute.
recent work by murugesan and schuller attempts to combine test coverage metrics with requirements to determine completeness.
.
conclusions future work in this paper we have defined the notion of inductive validity core ivc which appears to be a useful measure in relation to a valid safety property for inductive model checking.
we havepresented a novel algorithm for computing ivcs that are nearly minimal and have shown that full minimality is undecidable in many settings.
our algorithm is applicable to all forms of inductive sat smt based model checking including k induction pdr and interpolation based model checking.
we have implemented our ivc algorithm as part of the open source model checker jkind.
we have shown that the algorithm requires only a moderate overhead and produces nearly minimal ivcs in practice.
moreover the produced ivcs are fairly stable with respect to underlying proof engines k induction and pdr and back end smt solvers yices z3 mathsat smtinterpol .
our work has recently been integrated into the aadl agree tool suite which supports compositional reasoning about system architectures.
first ivcs are used to to automatically compute traceability information between high and low level requirements in compositional proofs.
second ivcs are used by the agree symbolic simulator to explain conflicts when the simulator is not able to compute a next state for a set of chosen constraints.
a pilot project at rockwell collins is using the traceability information produced by the ivc support in the agree tool.
in future work we will compare the traceability matrices generated by ivcs with those produced by human experts and and by automated heuristic approaches.
our expectation is that the traceability information produced by ivcs will be both more accurate and closer to minimal than other approaches.
we also will examine the impact of multiple distinct ivcs on traceability research.
an initial paper on this work which we call complete traceabilityhas been accepted to the re next!
track of the requirements engineering conference .
we are interested in diversity both in terms of regression analysis for testing and proof as well as examining the underlying sources of diversity in our analysis models.
we suspect that in some cases it indicates fault tolerance in the architecture under analysis and in other cases it may indicate redundancy in requirements specifications for subcomponents.
to support a systematic investigation of diversity we plan to investigate algorithms for exploring the space of ivcs e.g.
finding a minimum rather than minimal ivc or finding all ivcs.
finally we are in the process of comparing our approach against other approaches measuring completeness of requirements such as those in .