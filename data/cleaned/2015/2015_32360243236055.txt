appflow using machine learning to synthesize robust reusable ui tests gang hu ganghu cs.columbia.edu columbia university new york ny united stateslinjie zhu linjie cs.columbia.edu columbia university new york ny united statesjunfeng yang junfeng cs.columbia.edu columbia university new york ny united states abstract ui testing is known to be difficult especially as today s development cycles become faster.
manual ui testing is tedious costly and errorprone.
automated ui tests are costly to write and maintain.
this paper presents appflow a system for synthesizing highly robust highly reusable ui tests.
it leverages machine learning to automatically recognize common screens and widgets relieving developers from writing ad hoc fragile logic to use them in tests.
it enables developers to write a library of modular tests for the main functionality of an app category e.g.
an add to cart test for shopping apps .
it can then quickly test a new app in the same category by synthesizing full tests from the modular ones in the library.
by focusing on the main functionality appflow provides smoke testing requiring little manual work.
optionally developers can customize appflow by adding app specific tests for completeness.
we evaluated appflow on popular apps in the shopping and the news category two case studies on the bbc news app and the jackthreads shopping app and a user study of subjects on the wish shopping app.
results show that appflow accurately recognizes screens and widgets synthesizes highly robust and reusable tests covers .
of all automatable tests for jackthreads with the tests it synthesizes and reduces the effort to test a new app by up to .
interestingly it found eight bugs in the evaluated apps including seven functionality bugs despite that they were publicly released and supposedly went through thorough testing.
ccs concepts software and its engineering software testing and debugging empirical software validation software evolution keywords mobile testing test reuse test synthesis ui testing machine learning ui recognition acm reference format gang hu linjie zhu and junfeng yang.
.
appflow using machine learning to synthesize robust reusable ui tests.
in proceedings of the 26th acm joint european software engineering conference and symposium permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa association for computing machinery.
acm isbn .
.
.
.
the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
introduction most applications are designed to interact with humans making it crucial to test the functionality performance and other key aspects of their user interfaces uis .
yet ui testing is known to be exceedingly challenging.
manual ui testing has the advantage of testing faithful human experience but the downside is that it is tedious costly and error prone imagine a poor tester repeating manual tests on different devices.
automated testing is supposed to be the rescue but today s test automation in industry requires a tremendous amount of developer babysitting and few companies have the skills or resources to set it up as illustrated in the following comment at hackernews a top developer forum i have worked in several companies that have had goals of automated ui regression test suites but i ve never worked at a company that pulled it off successfully.
ui test automation often rely on script based testing.
specially to automate ui testing developers must invest a high initial cost to write test scripts diagnose test failures which are often caused by broken tests instead of bugs in the application code and maintain test scripts when the application s ui evolves.
while these tasks seem easy on the surface numerous pitfalls make them daunting because application uis are designed for human intelligence but test scripts are low level click by click scripts.
for instance while we humans can easily recognize without ambiguity the button to add an item to a shopping cart whether or not the button shows add or an icon a test script locates the button typically via a developerprovided hardcoded method e.g.
searching the internal widget id or by text match .
this hardcoded method can easily become incorrect when for example the button evolves from add to an icon or the application has different designs based on device factors such as screen size.
test record and replay reduces the cost of writing tests but recorded tests rarely work out of the box and ui evolution still requires re recording .
these test automation challenges are exacerbated by today s ever faster development cycles.
development trends such as continuous integration and devops require running tests on each code commit or merge to a key branch which may happen a dozen time a day calling for fast fully automated testing.
a standard software engineering practice to writing difficult code is to delegate experts implement the code as a library or service and other developers reuse.
examples include cryptography distributed consensus and image processing .
in ui testing there is ample opportunity for reusing tests because many esec fse november lake buena vista fl usa gang hu linjie zhu and junfeng yang apps are in the same category and implement similar user flows.
for instance almost all shopping apps implement some forms of user sign in search for an item check item details add to shopping cart check out etc.
we studied the top non game mobile apps and found that app categories are enough to cover or .
apps .
demonstrating the huge potential of sharing tests across apps in the same category.
thus it would save much struggling if we could create a robust reusable test library for shopping apps.
unfortunately few of today s automation frameworks are designed for reusing test scripts across apps.
first despite that apps in the same category share much similarity in their flows they may have very different designs texts and names for their screens and widgets.
thus a test script for an app often cannot locate the right screens and widgets for another app.
second apps in the same category may still have subtly different flows.
for instance the sign in flow of an app may contain just the sign in screen but another app may show a welcome screen first.
the add to shopping cart flow of an app may require a user to first visit the item details screen but another app may allow users to add items in search results directly to shopping cart.
these subtle differences prevent directly reusing test scripts on different apps.
this paper presents appflow a system for synthesizing highly robust highly reusable ui tests.
it enables developers e.g.
those in the shopping app community or a testing services company to write a library of modular ui tests for the main functionality of a given category of apps.
this library may be shared open source or stored within a testing cloud service such as google s firebase test lab or amazon s device farm.
then when developers want to test a new app in the same category they can quickly synthesize full tests from the modular ones in the library with a few lines of customization greatly boosting productivity.
by focusing on the main functionality of an app category appflow provides smoke tests or build verification testing for each source code change requiring little or no manual work.
previous work has shown that such tests even incomplete provide quick feedback to developers and help them fix bugs early before the bugs cause greater impact.
optionally developers can customize appflow to add app specific tests or override defaults to perform complete regression testing.
a key idea in appflow is a machine learning approach to recognizing screens and widgets.
instead of relying on developers hardcoded logic appflow learns a classifier from a training dataset of screens and widgets labeled with their intents using a careful selection of features including texts widget sizes image recognition results of graphical icons and optical character recognition ocr results.
the training dataset can come from a developer community for an app category and appflow provides several utilities to simplify this mostly one time data collection.
after the classifier is trained appflow uses it to map variant screens and widgets to canonical ones.
for instance it maps text edit boxes with username your email or example email.com on sign in screens all to signin.username representing the user name widget.
this machine learning approach enables the appflow tests to refer to canonical screens and widgets instead of app specific ones enjoying a variety of benefits.
first apps ui can now evolve without breaking tests as long as the new designs can be recognized by appflow .
second app ui can now respond to device factors suchas screen size without breaking tests.
third canonical screens and widgets abstract app specific variations making it easy to share tests across apps.
fourth appflow s ability to recognize screens enables developers to focus on testing the specific flows of a screen without writing much boilerplate code to first bring the app to the screen or later restore the app to a previous state.
this benefit is crucial for reusability which we elaborate next.
a second key idea in appflow is to automatically discover apps behaviors by applying reusable self contained tests called flows and synthesize full tests from them.
to test a feature such as at the item details page a user can add the item to shopping cart the developer writes a flow that contains three components the precondition of the test such as app must be at item details screen the postcondition of the test such as app must be at shopping cart screen and the actual steps to carry out the test such as click add button.
the precondition and postcondition are in spirit similar to hoare logic and can contain custom conditions on app state such as loggedin true i.e.
the user must have logged in .
this flow is dual purpose it can be used to test if an app implements this feature correctly and it can be used to navigate an app into states which are required to test other features.
specifically given a library of flows appflow dynamically synthesizes full tests as follows it starts the app recognizes its state finds activated flows whose preconditions are met executes each flow and repeats for each new state reached.
appflow s synthesis has two main benefits.
first it greatly simplifies test creation because developers no longer need to write boilerplate code to bring the app to a certain state or clean up the state after.
second modularization enables test reuse.
if tests are specified as a whole a test can hardly be reused due to variations of implementations of not only the scenario under test but also the steps required to reach the scenario.
in contrast modular tests can be properly synthesized to adapt to a specific app s behavior.
for instance we can create a test library that contains two sign in flows with or without the welcome screen and two add to shopping cart flows passing or not passing item details screen.
appflow can then synthesize the right tests for a new shopping app we want to test mixing and matching the modular flows.
in addition it also allows appflow to adapt to apps behavior changes.
appflow can discover an app s new behaviors and automatically synthesize corresponding tests for them.
we implemented appflow for the android platform because of its wide adoption and tough market competitions developers face but the ideas and techniques are readily applicable to general ui testing.
appflow s language to write flows is an extension of gherkin a human readable domain specific language for describing app behaviors.
our evaluation of appflow consists of four sets of experiments.
first we evaluated appflow on popular shopping apps and news app by creating and reusing test libraries for the two app categories.
second we conducted a case study of the bbc news app with two dramatically different versions to see if the tests appflow synthesizes are robust against the changes.
third we conducted a user study of subjects on creating tests for the wish shopping app to compare appflow s approach vs writing tests using an existing test framework.
fourth we analyzed a complete manual test plan from the developers of the jackthreads app and quantified 270appflow using machine learning to synthesize robust ui tests esec fse november lake buena vista fl usa how many tests appflow can automatically synthesize.
results show that appflow accurately recognizes screens and widgets synthesizes highly robust and reusable tests covers .
of all automatable tests for jackthreads and reduces the effort to test a new app by up to .
interestingly it also found eight bugs in the evaluated apps including seven functionality bugs despite that they were already publicly released and supposedly went through thorough testing.
this paper makes three main contributions the appflow system for synthesizing highly robust highly reusable tests our technique that leverages machine learning to recognize screens and widgets for robustness and reusability and our evaluation on real world shopping and news apps that produces tests and found bugs.
appflow s source code and the test libraries evaluated are available at github.com columbia appflow appflow s dataset is available at github.com columbia appflow dataset.
this paper is organized as follows.
an overview of appflow is given in section .
how machine learning is used is shown in section .
method to define flows is presented in section .
the synthesis process is illustrated in section .
implementation details are discussed in section .
we evaluated appflow in section .
we discussed limitations of this approach in section .
related works are reviewed in section .
we conclude in section .
overview this section first presents a succinct example to show how to write appflow tests .
and then describes its workflow .
.
.
example scenario add to shopping cart given screen is detail and cart filled is false when click addtocart and click cart and not see empty cart msg then screen is cart and set cart filled to true figure flow add to shopping cart .
suppose a developer wants to test the flow adding an item to an empty shopping cart clears the shopping cart is empty message for shopping apps.
figure shows an example for this test inappflow .
given... specifies the precondition of the flow.
the screen to activate this flow should be the detail screen the canonical screen that shows an item s details.
this screen exists in almost all shopping apps so using it to specify the condition not only eases the understanding of this flow but also allows this flow to be reusable on other shopping apps.
here screen is a visible property built into appflow .
in contrast the flow specifies in the precondition that cart filled must be false and cart filled is a developer defined abstract property indicating whether the shopping cart is filled.
abstract properties are intended to keep track of the invisible portions of app states which can often be crucial for writing robust tests.
to run this flow appflow ensures that theprecondition of the flow must be met i.e.
all properties specified in the precondition must have the corresponding values.
next the flow does two clicks to the addtocart and cart buttons.
unlike traditional test scripts that refer to the widgets using handwritten fragile logic appflow tests use canonical widgets exported by a test library and appflow leverages machine learning to match real widgets to canonical ones.
then the flow performs a check not see... .
after the two clicks current screen must be the canonical screen cart which represents the shopping cart screen.
thus the flow checks to ensure that the canonical widget empty cart msg which signals that the shopping cart is empty should not be seen on the screen.
finally then specifies in the postcondition that the screen after executing the clicks must be the canonical cart screen which appflow will check after executing this flow.
postconditions are different from checks because postconditions cause appflow to update the app state it maintains.
the flow also sets cart filled to be true after executing this flow which causes appflow to update the abstract properties it tracks to reflect this effect.
after executing this flow appflow will check to see if the new values of these properties satisfy the preconditions of any previously inactive flows and add these flows to the set of flows to execute next.
this simple example shows some key benefits of appflow .
this flow is easy to understand even for non developers e.g.
a product manager .
the canonical screens and widgets used are recognized byappflow automatically using machine learning methods making the test robust against design changes and reusable across different apps.
the system allows developers to describe just the flows to test without writing boilerplate code to bring the app to an item details screen.
.
workflow figure workflow of appflow .
the stick figure here represents developer intervention.
figure shows the workflow of appflow .
it operates in two phases the first phase mostly one time prepares appflow for testing a new category of apps .
.
and the second phase applies appflow to test each new app in the category .
.
.
.
.
prepare for a new app category.
to prepare appflow for a new category of apps developers do two things.
first they create a test library in appflow s language that contains common flows for this category and define canonical screens and widgets 271esec fse november lake buena vista fl usa gang hu linjie zhu and junfeng yang during this process.
second they use simple appflow utilities to capture a dataset of canonical screens and widgets and label them.
sometimes apps in different categories share similar screens e.g.
sign in screens and these samples from other app categories can also be added.
given this dataset appflow extracts key features from each sample and learns classifiers to recognize screens and widgets based on them .
.
.
test a new app.
to test a new app for the first time developers do two things.
first they customize the test library for their app.
machine learning is highly statistical and cannot always recognize every canonical screen and widget.
to correct its occasional errors developers run an interactive gui utility of appflow to discover the machine learning errors and override them.
in addition developers supply values to the variables used in the library such as the test user name and password.
developers may also add custom flows to test app specific behaviors.
the syntax and usage of this customization are described in .
.
second developers run appflow on the app to record the initial test results.
recall that a test library typically contains several variant flows such as signing in from the welcome screen or the menu screen.
appflow runs all flows and reports the result for each letting developers confirm which flows should succeed and which should fail.
under the hood appflow uses the flows in the test library to synthesize full tests through a systematic discovery process.
recall that a flow is active if its precondition is met in a state.
at first only the start app flow is active.
in the discovery process new app states and new paths to reach them are discovered and more flows are activated.
the process terminates when no more flows need to be tested.
the detail of this process is explained in .
.
after the two setup steps developers can now test new versions of the app regularly for regressions.
appflow runs a similar process to synthesize full tests for each new app version comparing the results to those from the previous run.
it reports any unexpected failures and unexpected successes of the flows to developers who should either fix any regressions or confirm intended changes to appflow .
recognizing canonical screens and widgets intuitively screens and widgets for similar purposes should have similar appearance for good user experience and similar names for ease of maintenance.
however simple rules cannot recognize them correctly because of variations across apps and evolution of the same app over time.
for example the login button on the sign in screen may contain login sign in let me in or even an icon showing an arrow.
the underlying ui object usually has a class name of button but sometimes it can be changed to textview or even relativelayout .
instead of using ad hoc manually written rules to recognize widgets appflow leverages machine learning to combine information from many available sources thus it is much more robust.
feature selection is key to accurate recognition and it absorbed much of our effort.
we experimented with a variety of feature combinations settled with the following method.
for each ui object screen or widget the features include its key attributes suchas description text size whether it is clickable the ui layout of the object and the graphics.
all features are converted to values between and in the final feature vector.
numerical features such as size are normalized using the maximum value.
boolean features such as whether a widget is clickable is converted to or directly.
ui layout is converted to text via a pre order tree traversal.
graphical features are handled in two ways.
button icons carry specific meanings so they are converted to feature vectors by calculating their histogram of oriented gradients hog .
other graphical features are converted to text via ocr.
all textual features including those converted from ui layouts and graphics are converted using term frequency inverse document frequency tf idf .
intuitively tf idf gives a higher weight if a term occurs in fewer documents thus more descriminative and more times in a document.
sometimes gram is used to form terms from words in text.
we show the effects of different feature selection schemes on accuracy in .
.
besides feature selection schemes we also experimented with different learning algorithms and found that screen recognition and widget recognition need different algorithms.
the following subsections describes the feature selection scheme and learning algorithm that yield the best accuracy for recognizing screens and widgets.
.
classifying screens appflow uses three types of features to recognize canonical screens.
screen layout the screen layout is a tree containing all the widgets on the screen.
different screens may have different numbers of widgets and feature vectors have to be of fixed length so appflow converts the entire screen s ui layout to one text string.
it traverses the tree in pre order and for each widget visited it selects the text identifier the underlying ui object s class name and other key attributes of the widget.
for size position and other non text attributes appflow generates a set of words to describe them.
for instance consider a search box widget.
it is typically at the top of a search screen with a large width and small height.
its identifier typically contains search and edit to indicate that it is editable and for implementing the search functionality.
given this search widget appflow first generates a set of words describing the geometry of widget top and wide and another set containing the word split of the identifier search and edit using a rule based algorithm.
it then uses the cartesian product of the two sets of words as the description of this widget.
this cartesian product works better than individual words because it captures the correlation between the geometry and identifier for recognizing widgets e.g.
topsearch is very indicative of a search widget it also works better than a concatenation of all words because it is more invariant to minor design differences e.g.
with concatenation topwidesearch and topsearch become different terms .
screen snapshot a user understands a screen mostly based on the screen snapshot.
to utilize this information appflow performs ocr on the snapshot to extract texts inside it.
class information appflow includes the class name of the screen s underlying ui object in the features it selects.
in android 272appflow using machine learning to synthesize robust ui tests esec fse november lake buena vista fl usa the class is always a subclass of activity.
developers tend to name screen classes with human readable names to ease maintenance.
from the training data set we train a neural network classifier that takes the screen feature vectors as inputs and outputs the canonical screen.
it has hidden layer with neurons optimized with a stochastic gradient based optimizer .
.
classifying widgets for each widget in the tree of widgets captured from a screen appflow selects the following features.
widget s text the text attribute of the widget is used.
this usually equals to the text shown on the widget.
the text attribute of the widget is the most evident clue of what the widget represents because usually users understand its usage through text.
however other features are still needed.
in some cases the widget shows an image instead of text.
in other cases text is embedded into the image and the text attribute is empty.
widget s context the widget s description identifier and class name are used.
the description and identifier of a widget are evidences of its functionality especially for widgets which have empty text attributes.
the description is provided for accessibility uses while the identifier is used by developers.
the class name provides some useful information such as whether this is a button or a text box but it can be inaccurate.
widget s metadata the widget s size position and some other attributes are used.
the widget s metadata combined with other information increases the accuracy of the machine learning results.
for example in almost all apps the password widget on the sign in screen has its ispassword attribute set to true which helps the machine learning algorithm distinguish it from the email widget.
neighbour information some widgets can be identified by observing their neighbours.
for example an empty editable text box with no id or description may be hard to recognize but users can understand its usage by observing its neighbour with a label containing text email .
appflow includes the left sibling of the current widget in the feature vector.
ocr result ocr result of the widget s image is used.
some widgets do not have id text or description.
for traditional frameworks these widgets are especially hard to refer to while we found them fairly common among apps.
some other widgets have only generic ids such as toolbar button .
in these cases appflow uses features which humans use to identify them.
a user usually recognizes a widget either through its text or its appearance.
this feature captures the text part while the next feature captures the graphical part.
graphical features the image of the widget is used.
some widgets such as icons use graphical features to hint users its functionality.
for example in almost all apps the search icon looks like a magnifier.
appflow uses the hog descriptor widely used in single symbol recognition to vectorize this feature.
vectorized points from the train set are used to train linear support vector machine svm classifiers.
every linear svm classifier recognizes one canonical widget.
the penalty parameter cis set to .
.
svms are used because it achieves high accuracy while requiring little resources.
because the number of widgets is much larger than the number of screens efficiency must be taken intoaccount.
canonical widgets from different screens are classified using different sets of classifiers.
to classify a widget it is vectorized as above and fed into all the classifiers of its canonical screen.
if the classifier with the highest confidence score is higher than the configurable threshold its corresponding canonical widget is given as the result.
otherwise the result is not a canonical widget .
writing test flows this section first describes the language extensions we made to gherkin to support writing test flows .
then explains some specifics on creating a test library and best practices .
.
.
language to write flows appflow s flow language follows gherkin s syntax.
gherkin is a requirement description language used by behavior driven development tool cucumber which in turn is used by calabash a widely used automated testing framework for mobile apps.
we thus chose to extend gherkin instead of another language because mobile developers should already have some familiarity with it.
inappflow each flow is written as a scenario in gherkin where lines in the precondition are prefixed by given steps of the test are prefixed by when and lines in the postcondition and effect are prefixed by then .
unlike in gherkin which use natural languages for the conditions and step appflow uses visible and abstract properties.
calabash extends gherkin to also include conditions on the visible ui states but it does not support abstract properties.
the actions in a flow are specified using a verb followed by its arguments.
the verbs are common operations and checks such as see click and text .
the arguments can be widgets or values.
for widgets either canonical ones or real ones can be used.
canonical ones are referenced with canonical widget name .
real ones are found using locators similar to how calabash locates widgets.
simple methods such as id arg text arg and desc arg find widgets by comparing their corresponding attributes with the argument arg while method marked arg matches any of those attributes.
here arg may be a constant or a configuration variable indicated using variable name .
below we show four examples of flows.
the first flow tests that a user can log in with correct credentials scenario perform user login given screen is signin and loggedin is false when text username email and text password password and click login then screen is not signin and set loggedin to true the second flow tests that a logged in user can enter shopping cart from the main screen scenario enter shopping cart given screen is main and loggedin is true when click cart then screen is cart 273esec fse november lake buena vista fl usa gang hu linjie zhu and junfeng yang the third flow tests that the shopping cart is empty message is shown on the cart screen when the shopping cart is empty scenario check that cart is empty given screen is cart and cart filled is false then see cart empty msg the last flow which requires the shopping cart to be non empty removes the item from the shopping cart and expects to see the shopping cart is empty message scenario remove from cart given screen is cart and cart filled is true when click item remove and see cart empty msg then set cart filled to false .
creating a test library today developers write similar test cases for different apps in the same category much redundant work.
by contributing to a test library combined with appflow s ability to recognize canonical screens and widgets developers can share their work resulting in greatly improved productivity.
there are two subtleties in writing flows for a library.
first developers need to decide how many flows to include in the test library.
there is a trade off between the cost of creating custom flows and the cost of creating customizations.
with more flows the test library is more likely to include rare app behaviors so less custom flows are needed.
on the other hand more flows in the test library usually means more rare canonical widgets which have fewer samples from apps.
thus these widgets may have lower classification accuracy and having them requires more time to customize.
second the same functionality may be implemented slightly differently across apps.
as aforementioned the add to shopping cart flow of an app may require a user to first visit the item details screen but another app may allow users to add items in search results directly to shopping cart.
although conceptually these flows are the same test of the add to shopping cart functionality they need to be implemented differently.
therefore appflow supports the notion of a test that can have several variant flows and tracks the flow s that works when testing a new app .
best practices.
from our experience creating test libraries for two app categories we learned four best practices.
they help us create simple general and effective test libraries.
we discuss them below.
first flows should be modular for better reusability.
developers should avoid writing a long flow that does many checks and keep pre postconditions as simple as possible.
precondtions and postcondions are simple depictions of the app states.
the concept of app states naturally exists in traditional tests testers and developers sometimes describe them in comments or write checks for them.
when writing preconditions and postconditions it takes no more effort than writing checks for traditional methods.
rich functionalities do not directly translate into complicated design because mobile apps tend to have a minimalism design to focus on providing content to users without unnessary cognitive load .
an appwith rich functionalities usually has properties separated into fairly independent groups and thus have simple preconditions and postconditions.
short flows with well defined pre postconditions are simple to write easy to understand and more likely to be reusable.
for instance most flows should not cross multiple screens.
instead a flow should specify the screen where it can start executing and the screen it expects when its execution finishes and it should not cross other screens during its execution.
second test flows should refer only to canonical screens and widgets.
if a flow wants to check for a specific widget on the current screen this widget should be defined as a canonical widget then the test flow can refer it.
similarly if the flow wants to verify a screen is the expected screen the screen should be defined as a canonical screen.
this practice avoids checks which leads to fragile flows such as searching for specific strings on the screen to verify the screen or comparing widgets text to find a specific widget.
third flows of common functionalities implemented by most apps should be included while rare flows should be excluded from the test library.
from our experience it is crucial for classification results to be accurate.
if there are misclassifications developers would be confused by incorrect test results.
time spent by developers in debugging tests would likely be longer than time required to write a few custom flows.
in addition larger test library increases the exeuction time of appflow .
forth test flows should be kept simple.
complex flows are hard to generalize to other apps.
as we mentioned above it would be helpful in this respect if flows are splitted into smaller pieces and made modular.
also the properties used in flows conditions should also be kept at minimum since having more properties increases the testing time by creating more combinations.
applying a test library to a new app a developer applies a test library to her app in two stages.
first in the setup stage when applying the library to her app for the first time she configures and customizes the test library specifically assigning necessary values to test variables such as test account name and overriding classification errors of machine learning.
the developer may also add custom flows in this stage to test appspecific behaviors.
afterwards she runs appflow to synthesize tests and record the pass and fail results.
note that a failed flow does not necessarily indicate an error.
recall that the same functionality may be implemented differently so a failed flow may simply mean that it does not apply to the tested app.
second in the incremental stage she applies the library to test a new version of the app.
specifically appflow runs all tests synthesized for the previous version on the new version retries all flows failed previously and compares the results with the previous results.
the differences may show that some previously passing flows fail now and other previously failing flows pass now.
the developer can then fix errors or confirm that certain changes are intended.
she may further customize the library if needed.
each incremental run takes much less time than the setup stage because appflow memorizes tests synthesized for the previous version.
both stages are powered by the same appflow s automated test synthesis process to discover applicable flows and synthesize full 274appflow using machine learning to synthesize robust ui tests esec fse november lake buena vista fl usa tests.
appflow starts from the initial state of an app repeatedly executes active flows and extends a state transition graph with new states reached by these flows.
when there are no more active flows the process is finished.
a full test for a flow is synthesized by combining a chain of flows which starts at the initial state and ends at the flow.
in the remaining of this section we describe how a developer customizes a test library .
and how appflow applies the test library with customizations on an app to synthesize full tests .
.
.
configuration and customization developers customize a test library to an app in four steps.
the first three steps are typically required only in the first run.
first developers assign values to test variables.
a new app needs new values for these variables because they contain app specific test data such as the user name and password to be used for login the search keyword etc.
this data is straightforward to provide andappflow also provides reasonable defaults for most of them but developers can override them if they want.
developers may optionally change appflow s options to better suit their needs.
here is an example of this part email user example .
com password verysecurepassword second developers create matchers for screens and widgets to override machine learning errors.
although machine learning greatly reduces the need for developer written screen and widget matchers it inherently misclassifies in rare occasions which developers must override.
to ease the task we build a gui tool that helps developers inspect the machine learning results on their app and generate matchers if needed.
a screenshot of this tool is shown in figure .
operationally the tool guides developers to navigate to their app s canonical screens defined in the test library and overlays the recognition results on the app screen.
when the developers find any classification error they can easily generate a matcher to override the error.
we discuss typical classification errors and how developers can fix them below.
a widget is misclassified in two ways.
first a canonical widget can be misclassified as a non canonical widget.
developers can fix this by creating a widget matcher to help appflow recognize this widget.
they first select the misclassified canonical widget press the space key and click or type the correct label in a pop up dialog.
the tool will generate a boilerplate matcher using the widget s properties.
if its id is unique within the screen the generated matcher finds a widget with this id.
otherwise the tool will examine the widget s class text and description.
if this widget s properties are not unique enough to generate the matcher widgets containing it would also be examined.
second a non canonical widget can be classified as a non existing canonical widget.
developers can fix this in a similar way to the first case.
the only difference is that the label typed in should be empty.
the tool will generate a negative matcher which means that there is no such canonical widget on the current screen.
a screen is also misclassified in two ways.
first a canonical screen can be classified as another canonical screen.
developers can create a screen matcher to fix this.
they press the x key to enter the screen matcher generating mode click unique widgets which figure the gui tool to inspect machine learning results and generate matchers.
the ui of the tool is shown at left.
the recognized canonical widgets have a blue rectangle overlay on them and their labels are shown at center.
a pop up dialog to correct misclassified labels is shown at right.
the possible canonical widgets are provided as buttons.
to bring up the dialog a developer clicks on a widget to select it whose overlay becomes red and presses the space key.
in this example the selected widget is incorrectly classified as signin fb and this dialog asks for the correct label.
only appear on this screen press x again and enter the screen s label in an pop up dialog.
the tool then generates a matcher for this label which requires all these widgets to be present.
second an appspecific screen can be classified as a canonical screen.
developers can fix it in a similar way but put an app specific screen name starting with app in the dialog.
the matchers generated may be further edited to check for widgets which should not exist on a canonical screen.
the tool also checks the generated matchers against other screens which prevents developers from creating a loose matcher matching unintended screens.
alternatively experienced developers can skip the gui tool and directly add custom matchers to their app s configuration file signin.login marked log in bookmark text saved id toolbar here a widget matcher is provided for the login widget on the signin screen.
appflow can use it to locate this widget.
also a screen matcher for the bookmark screen is provided.
third developers may write custom flows to test app specific behaviors.
sometimes none of the library s flows for implementing a feature applies so a custom flow is required for appflow to reach the later flows.
custom flows follow the same syntax as the flows in the test library but they can match app specific screens and widgets in addition to canonical ones.
they can use the same properties defined in the test library or define their own ones.
these custom flows will be executed alongside flows in the test library.
lastly developers run appflow to synthesize tests and generate the pass and fail results.
once developers confirm the results appflow saves them for future incremental testing on each new version of the app.
if developers miss anything in the first three steps they would see unexpected test results in the last step.
since appflow logs 275esec fse november lake buena vista fl usa gang hu linjie zhu and junfeng yang each test s execution including the flows and actions performed and the machine learning results developers can easily figure out what is missing and repeat the above steps to fix.
in our experience we rarely need to repeat more than times to test an app.
these steps are typically easy to do.
the first three steps are manual and often take between half an hour to an hour in our experience applying a test library to two app categories see .
the most time consuming step among them is to create custom screen and widget matchers since developers need to navigate to different screens and carefully examine machine learning results.
the steps of providing values for test variables and writing custom flows are usually straightforward.
the last step takes longer for the apps we evaluated this step takes from one to two hours but it is automated synthesis and requires no developer attention.
after the last step has been completed once rerunning is much faster because appflow saves the test results from the previous run.
in all this setup stage takes .
hours including both manual customization and automated synthesis.
.
synthesizing full tests in both the first run and repeated runs appflow uses the same underlying algorithm to synthesize full tests to run.
it models the app behaviors as a state transition graph in which an app state is a value assignment to all properties including both visible properties and abstract properties.
for instance a state of a shopping app may be screen detail cart f illed true lo edin true.
the transitions of a state are the flows activated i.e.
whose preconditions are satisfied by the state at the state.
starting from the initial state appflow repeatedly selects an active flow to execute and adds the state reached by the flow to the state transition graph.
it stops when it finishes exploring the entire state transition graph.
given the state transition graph synthesizing full tests becomes easy.
to test a flow appflow finds a route that starts from the initial state and reaches a state in which the flow is active and combines the flows along the route and the flow to test into a full test case.
as an optimization appflow stores the execution time of each flow in the state transition graph and selects the fastest route when generating full tests.
one challenge is how to reset the app to the initial state.
when traversing the state transition graph appflow needs to restore a previously visited state to explore another active flow in the state.
appflow does so by uninstalling the app and cleaning up its data and then executes the flows along the route to the state.
this method fails if the app syncs its state to the server side.
for instance a flow may have added an item to the shopping cart already and the shopping cart content is synced to the server side.
when the app is re installed the shopping cart still contains the item.
appflow solves this challenge by synthesizing a state cleanup route that undoes the effects of the flows to reach the state.
for instance to clean the shopping cart state it runs the flow to remove an item from the shopping cart.
implementation appflow is implemented for the android platform using lines of python code.
it uses scikit learn for machine learning and tesseract for extracting text from images.
.
capturing screen layout appflow uses the uiautomator api to capture current screen layout a tree of all widgets with their attributes.
appflow also captures apps embedded webpages by communicating with apps webviews using the webview remote debugging protocol .
this interface provides more details for widgets inside the embedded webpages than the uiautomator api.
.
post processing of the captured layout the layout returned by uiautomator contains redundant or invisible views which would reduce the accuracy of appflow s screen and widget recognition.
appflow thus post processes the layout using several transformations recursively applied on the layout until no more transformations can be done.
for instance one transformation flattens a container with a single child removes empty container and removes invisible widgets according to previously observed screens.
another transformation uses optical text recognition to find and remove hidden views.
it extracts text from the area in a snapshot corresponding to each widget and compares the text with the widget s text property.
if the difference is too large the view is marked as invisible.
if all children of a widget are invisible appflow marks the widget invisible too.
our results show that this transformation safely removes up to .
of the widgets.
evaluation we focus our evaluation on the following six questions.
rq1 how much do real world apps share common screens widgets and flows and can appflow synthesize highly reusable flows?
the amount of sharing bounds the ultimate utility of appflow .
rq2 how accurately can appflow s machine learning model recognize canonical screens and widgets?
rq3 how robust are the tests appflow synthesizes across different versions of the same app?
rq4 how much manual labor does appflow save in terms of the absolute cost of creating the tests that appflow can readily reuse from a library?
rq5 how much manual labor does appflow save in terms of the relative cost to creating a fully automated test suite for an app?
rq6 how effectively can the tests appflow synthesizes find bugs?
while it is out of the scope of this paper to integrate appflow with a production continuous integration system we would like to at least apply appflow to the public apps on app stores and see if it finds bugs.
.
rq1 amount of sharing across apps we first manually inspected the description of all apps with more than million installations on google play android s app store and studied whether they fall into an app category that shares common flows.
of the apps are games which are known to be difficult to test automatically so we excluded them.
in the remaining apps .
of them fall into categories that share many common flows such as shopping and news.
the other .
apps fall into smaller categories which have larger behavior variations such as utilities.
276appflow using machine learning to synthesize robust ui tests esec fse november lake buena vista fl usa we conducted a deeper dive on two representative categories shopping apps1and news apps.
for shopping apps we selected top apps from play store.
for news apps we selected .
we selected them according to the number of downloads.
more than half of these apps have more than million installations and all of them have more than million installations.
we chose more shopping apps because they outnumber news apps in the google play store.
apps which cannot be automatically tested with appflow such as the ones which show errors on emulators and the ones which require sms authentication codes are excluded.
we created test libraries for these two categories and found that we needed canonical screens and canonical widgets for the shopping apps and canonical screens and canonical widgets for the news apps.
these are the canonical widgets and screens required by all the flows we created based on best practices we presented in section .
.
we wrote flows that do unique feature tests the same feature may be implemented slightly differently requiring different flows see .
for the shopping apps and flows that does unique feature tests for news apps.
on average each shopping app can reuse .
.
tests and each news app can reuse .
.
tests.
primarily due to an issue in uiautomator that misses certain widgets when collecting ui layout and other implementation issues appflow was able to synthesize slightly fewer tests .
tests for shopping and .
for news.
figure shows the histogram of the number of apps each flow can test.
the average is .
for shopping apps and .
for news apps.
120shopping 60news figure number of apps each flow can test.
the x axis shows the flows and the y axis show the number of apps.
.
rq2 accuracy recognizing screens and widgets our dataset consists of the tagged sample screens and widgets collected from the shopping and news apps.
for screens with fixed content we collected one sample per app.
for screens with variable content such as product detail screens we collected at most five samples per app.
for the shopping apps we collected screen samples which contain canonical widgets and non canonical widgets.
for news apps we collected screen samples which contain canonical widgets and non canonical widgets.
we used well established method leave one out crossvalidation to evaluate accuracy.
specifically when evaluating appflow on one app we trained the model on data collected from 1coupon and cashback apps such as ebates and flipp just serve as proxies to other businesses.
thus they are not considered shopping apps.all other apps in the same category and used this app s data as the test set.
this method effectively tests how our system works in real usage scenarios where a test library is applied to test a new app which was not used during the creation of the test library.
screen recognition accuracy.
our results show that appflow accurately recognized canonical screen samples for shopping apps achieving .
accuracy and canonical screen samples for news apps achieving .
accuracy.
the accuracy is higher for shopping apps partly due to their larger number of samples.
averaging across all apps the screen recognition accuracy is .
.
we also evaluated the effect of feature selection in classifying screens.
using only screen layouts the accuracy is .
for the shopping apps and .
for the news apps.
with ocr results the accuracy reaches .
and .
.
with activity name the accuracy rises to .
and .
.
the feature of screen layout is essential and other features are also important.
.
.
.
.
2accuracy number of appsshopping apps error figure accuracy vs. number of app.
the x axis shows the number of sample apps and the y axis shows the accuracy in screens.
figures shows how accuracy of machine learning changes with the number of sample shopping apps.
we evaluated the accuracy on randomly picked subsets of sample apps.
it increases with the number of apps and reaches for apps.
the result is similar for news apps.
widget recognition accuracy.
appflow s widget recognition accuracy is .
for shopping apps and .
for news apps and .
averaging over all evaluated apps.
similar to canonical screens we can see that more samples result in higher widget recognition accuracy.
figure evaluates feature selection in classifying widgets.
we order the features to best demonstrate their effectiveness.
using a widget s text alone can only achieve a low accuracy while adding a widget s context and graphical features greatly improves the results.
other features including metadata and neighbour s context also have small contributions to the result.
.
.
.
.
shopping newstext only add graphical add ocr add context add neighbour add metadata figure features used in classifying widgets.
different bars show different combinations of features.
the y axis shows the accuracy of classifying widgets.
277esec fse november lake buena vista fl usa gang hu linjie zhu and junfeng yang .
rq3 robustness to evaluate whether appflow is robust against an app s design changes we conducted a case study with two versions of bbc news whose home screens are shown in figure .
in the old version there is a search topics entry in its menu as shown in the left image.
clicking on it navigates the app to the search screen which overlaps a search input box at the top of the main screen.
in the new version the menu entry is removed.
instead a search icon which looks like a magnifier appears in the toolbar of the main screen as shown in the right image.
clicking on it still navigates the app is to the search screen which has a new design instead of overlapping a search input box a separate screen is shown.
a old version b new version figure old and new versions of bbc news.
using its machine learning model appflow recognized canonical screens main menu and search and canonical widgets including the menu s search entry and the search icon.
both the flow navigate to search screen by clicking search entry on the menu screen and navigate to search screen by clicking the search icon on the main screen are common so they are present in the test library.
when we first run appflow on the old version and later run it on the new version appflow correctly reported that the first flow turns not reusable and the second flow becomes reusable.
all the flows starting from the search screen are not affected.
.
rq4 absolute manual labor savings in creating tests table customizations.
average number of lines of customization required for each app.
number of lines shopping news screen matchers .
.
widget matchers .
.
configuration .
.
custom flows .
.
total .
.
appflow has two major costs writing a test library for an app category and setting up testing for a specific app.
our own experience was that test library and its flows are simple and easy towrite.
the average number of lines of each flow is .
for shopping apps and .
for news apps.
table shows the number of lines of customizations required to test each specific app.
on average an app requires .
lines of customizations.
only .
of canonical screens and .
of canonical widgets require matchers.
comparing with identifying all of them by fragile logic appflow greatly increases tests robustness.
we conducted a user study to quantify the cost saved by appflow .
the cost of using appflow includes both the cost of creating a test library and applying it to a new app so this user study targets both.
the study had participants.
of which are master students and the other two are ph.d. students.
none of them have prior knowledge of appflow .
a state of the art mobile testing framework calabash is chosen for comparison.
calabash is one of the most popular mobile testing frameworks and its language is easy to learn and similar to appflow s. we randomly picked test scenarios of shopping apps from screens.
the task is to write test scripts for these scenarios.
a typical shopping app wish is selected as sample.
subjects are given descriptions of these scenarios and educated with the usage of appflow and calabash then asked to perform following tasks.
for appflow they are asked to write flows capture screen samples from the sample app and tag canonical widgets create customizations for incorrect machine learning results of these samples install the customizations and flows run the flows and add additional customizations if needed.
these tasks are evaluating both the scenario of writing test libraries and the scenario of applying a test library to a new app.
specifically tasks and are evaluating the first scenario while tasks and are evaluating the second.
for calabash they are asked to write test scripts and debug the scripts until they pass.
half of subjects follow this order and the other half write calabash tests first.
this eliminates the effect of familiarity between systems.
we measured time spent in each task.
on average a user spends 78s in writing a flow.
tagging a screen takes 72s.
checking machine learning results and creating customizations requires 22s for one screen.
each reusable flow takes an average of 17s for developers to inspect the test result and customize the widget or screen matchers if needed.
in comparison writing and debugging a case in calabash requires 320s.
based on this data we estimated the cost to create a test library.
when training our model we captured and tagged screen samples.
we also wrote flows for the test library.
combining with numbers above we can calculate the test library for shopping apps takes 72s 78s 35h31mto create.
we also estimated the cost of applying a test library to a new app.
on average .
flows can be reused on an app cf.
.
and .
custom flows are required.
the new app can have at most canonical screens.
thus applying a test library should require 22s 78s .
17s .
30m40s.
these numbers match our own experience.
notice that the last step in the setup stage of applying a test library is not included because it s a mostly automatic process and the developer s time spent is insignificant compared with other steps.
in contrast using calabash requires .
5h29mfor creating these test cases.
these estimations show that writing test cases using appflow only requires .
of the time when comparing with calabash.
even 278appflow using machine learning to synthesize robust ui tests esec fse november lake buena vista fl usa if we include the time to create the test library which should be readily available from the market appflow saves cost as long as a test library is used on more than seven apps.
the cost of creating a test library depends on its complexity the familiarity of developers with appflow and the number of samples captured.
note that this is mostly one time cost.
the cost of applying a test library on a new app mainly depends on the size of it and the accuracy of machine learning models.
this can further be reduced with better machine learning methods.
.
rq5 relative manual labor savings to complete test automation to understand appflow s cost savings relative to the cost of creating a complete test automation suite we obtained and analyzed the manual test plan of android app jackthreads a representative shopping app.
this test plan is obtained directly from the developers who were using this plan for manually testing.
this typical test plan of shopping app contains tests.
among them .
can be checked using test scripts.
the test library of appflow covers .
of those automatable ones.
when a flow covers a test it checks all the automatically verifiable requirements so it is highly effective.
by using appflow .
of the test cases can be automatically created providing large cost savings.
there are two reasons why tests are not covered by the test library.
first test library only covers common tests while some tests are highly specific to this app.
for example a scenario requires the images of categories on the categories screen are shown as a grid with rows containing and images.
this behavior is never seen in other apps so the test script for this scenario is not reusable by nature.
second some scenarios refer to uncommon widgets or screens which are only present in this app.
these widgets and screens are not considered canonical thus the flows corresponding to them cannot enter the test library.
.
rq6 effectiveness in bug finding although appflow is evaluated on apps released on the google play store which should have been tested thoroughly appflow still found multiple bugs in different apps.
we found bugs in shopping apps and bugs in news apps.
these bugs except one are not crash bugs.
the non crash bugs cannot be detected without knowing semantics so they will be missed by tools such as dynodroid or stoat .
we show interesting examples.
one bug appears in the homedepot app a shopping app for home improvements.
after typing a search query into the search input box and clicking search button on soft keyboard the app should show search results.
instead search results appear for a second then quickly retract.
this prevents user from searching using an arbitrary keyword.
on the other hand if user click on one of search suggestions instead of the search button it works.
appflow detected this problem because the postcondition screen is search results screen failed after testing the do a search flow.
another bug appears in the groupon app a shopping app for group deals.
in the search screen if the user typed a query incorrectly and wanted to clear it a natural way is to click the clear search query button which usually looks like an x .
in this version this does not work for the first time but works if you clickagain.
a human tester may miss this bug because she may think that she did not click it and tried again.
appflow detected this bug from the failed last step in the clear query flow which checks for absence of the search keyword.
limitations and future work fundamental limitations of appflow .appflow aims at greatly reducing manual effort implementing automated ui testing.
we did not design appflow to replace manual testing completely it is well known that as of now automated ui testing cannot replace manual ui testing completely because user experience is highly subjective .
however as the advocates of continuous integration and devops articulate early detection of bugs increases developer productivity and software quality thereby indirectly reducing manual testing effort .
along this vein appflow aims at automatically testing common scenarios.
thus the test library should only include common flows not every possible ones.
custom flows may be written to test app specific features.
on the other hand sufficient flows either custom or common must be present for appflow to synthesize executable tests.
for instance if there is no sign in flow applicable appflow cannot reach flows that require a user to be signed in.
our evaluation shows that only a small number of custom flows are needed in .
.
flows in a test library of appflow should only refer to canonical widgets which may limit checks they can perform and reduce their effectiveness.
appflow focuses on testing core functionalities which as we have shown are largely shared across apps and can be tested using only canonical widgets.
as the test library evolves more canonical screens can be added and more canonical widgets can be defined so tests can be more effective.
machine learning misclassification.
appflow leverages machine learning to recognize screens and widgets.
being statistical in nature machine learning occasionally misclassifies requiring developers to provide matchers.
when an app updates these matchers might need update as well.
a flow may pass even if the feature it tests is not correctly implemented.
for example suppose a flow checks for a certain canonical widget and a software update removes that widget the flow may still pass if machine learning incorrectly recognized another widget as the canonical one.
machine learning misclassifications only cause problems for the simplest flows since any flow which depends on interaction with that widget would likely break indicating the problem to developers.
however this problem is not limited to appflow because traditional test scripts typically use fragile rules to match widgets so they have the same problem and these rules may silently fail too.
in contrast since appflow uses machine learning to recognize canonical ui elements as the accuracy of machine learning improves this problem would also be mitigated.
supporting other platforms.
appflow currently only supports the android platform.
it is straightforward to use its ideas to test ios apps.
the ideas also apply to other ui testing environments including web and desktop applications.
unlike mobile apps web and desktop applications tend to have more complex uis so recognizing ui elements might be harder.
we leave these for future work.
279esec fse november lake buena vista fl usa gang hu linjie zhu and junfeng yang related work automated ui testing methods can be classified by whether they need developers input.
random testing tools and systematic tools explore apps state space and detect generic problems without developers help.
unlike appflow these tools can only check for basic problems like crashes so they cannot test if apps can complete scenarios.
other methods need developers to specify expected behaviors.
model based testing requires models or ui patterns which have to be created manually for each app.
these models are usually hard to write and maintain.
pbgt aims to reduce the effort of modeling by reusing models but a model created using it is highly specific to an app and usually not reusable on other apps.
concurrent to our work augusto generates semantic ui tests based on popular functionalities.
it explores an application with gui ripping matches the traversed windows with ui patterns verifies them according to semantic models defined using alloy and generates semantic tests.
we share the same intuition that apps implement common tests called application independent functionalities or aifs in augusto using common ui elements and we both generate semantic tests.
there are also key differences.
at the technical level unlike augusto which uses rules to match widgets and screens appflow uses machine learning methods to recognize them which are more robust.
appflow discovers reusable flows by evaluating flows on an app and progressively constructing a state transition graph while augusto dynamically extracts an application s gui model identifies aifs inside it and generates tests for them.
at the experimental level we conducted studies of real world apps to quantify the amount of sharing across apps in the same category.
in addition two posters discussed the potential of transferring tests written for an app to another.
script based testing frameworks such as calabash espresso and others require developers to write and maintain test scripts.
as we mentioned in section these scripts require considerable efforts to write and maintain.
this prevents companies from adopting such methods.
specifically these scripts use fragile rules to find ui elements which makes them not robust to ui changes and increases maintenance cost.
test record and replay eases test writing.
like other scripts tests generated by it usually refer to ui elements with absolute position or fragile rules .
these scripts produce unstable results and may not adapt to different screen resolutions .
worse these rules may match widgets with properties not intended by developers further reducing robustness.
appflow enables scripts to be robust and reused by using machine learning to locate ui elements and using its synthesis system to automatically discover an app s behavior.
this greatly reduces the cost of adopting automatic testing.
sikuli uses computer vision to help developers and enables them to create visual test scripts.
it allows developers to use images to define widgets and expected feedbacks and then matches these images with screen regions to find widgets and check assertions.
it can also record visual tests and replay them.
similar to sikuli appflow also uses computer vision in recognizing ui elements butappflow also combined non visual features from ui elements which are essential for correct recognition.
appflow s model istrained on samples from multiple apps which enables appflow to adapt to ui changes and recognize same ui element in different apps.
unlike sikuli which may only adapt to spatial changes in ui elements appflow can adapt to behavior changes which may result in addition and removal of ui elements.
ui test repair aims at reducing test maintenance cost by automatically fixing ui tests after applications designs change.
they find alternative ui event sequences for ui tests under repair to keep them runnable.
although these method are efficient they can only fix a portion of all the broken tests while the remaining ones still need manual work.
some previous works create models or tests automatically.
unitplus and other works used available tests to assist developers in creating new tests for the same app but tests still need to be created first.
gk tail and other work create models or tests by mining traces.
polariz uses a crowd with no testing experience to provide test cases and mines common patterns among multiple apps.
appflow can be combined with these works to free developers from writing test libraries.
previous works generate test cases from well defined operations with automatic planning while appflow generates tests by progressively discover an app s behavior which is necessary to handle different app designs and synthesize only tests reusable in this app.
machine learning algorithms has been widely used in software engineering.
previous works learn useful features from codes for code completion clone detection bug finding similar app detection etc.
poster uses off the shelf model only to calculate text similarity between ui elements.
to the best of our knowledge appflow is the first work to apply machine learning in recognizing apps screens and widgets.
conclusion in this paper we presented appflow a system for synthesizing highly robust highly reusable ui tests.
appflow achieves this by realizing that apps in the same category share much commonality.
it leverages machine learning to recognize canonical screens and widgets for robustness and reusability and provides a system for synthesizing complete tests from modular tests of main functionalities of an app category.
we evaluated appflow on popular apps in the shopping and the news category two case studies on the bbc news app and the jackthreads shopping app and a user study of subjects on the wish shopping app.
results show that appflow accurately recognizes screens and widgets synthesizes highly robust and reusable tests covers .
of all automatable tests for jackthreads with the tests it synthesizes and reduces the effort to test a new app by up to .
it also found eight bugs in the evaluated apps which were publicly released and should have been thoroughly tested.
seven of them are functionality bugs.