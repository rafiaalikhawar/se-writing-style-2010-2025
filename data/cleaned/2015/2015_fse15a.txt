a user guided approach to program analysis ravi mangal xin zhang aditya v. noriy mayur naik georgia institute of technology usaymicrosoft research uk ravi.mangal xin.zhang naik gatech.edu adityan microsoft.com abstract program analysis tools often produce undesirable output due to various approximations.
we present an approach and a system eugene that allows user feedback to guide such approximations towards producing the desired output.
we formulate the problem of user guided program analysis in terms of solving a combination of hard rules and soft rules hard rules capture soundness while soft rules capture degrees of approximations and preferences of users.
our technique solves the rules using an o the shelf solver in a manner that is sound satis es all hard rules optimal maximally satis es soft rules and scales to real world analyses and programs.
we evaluate eugene on two di erent analyses with labeled output on a suite of seven java programs of size kloc.
we also report upon a user study involving nine users who employ eugene to guide an informationow analysis on three java micro benchmarks.
in our experiments eugene signi cantly reduces misclassied reports upon providing limited amounts of feedback.
categories and subject descriptors d. .
software program veri cation keywords user feedback program analysis report classi cation .
introduction program analysis tools often make approximations.
these approximations are a necessary evil as the program analysis problem is undecidable in general.
there are also several speci c factors that drive various assumptions and approximations program behaviors that the analysis intends to check may be impossible to de ne precisely e.g.
what constitutes a security vulnerability computing exact answers may be prohibitively costly e.g.
worst case exponential inthe size of the analyzed program parts of the analyzed program may be missing or opaque to the analysis e.g.
if the program is a library and so on.
as a result program analysis tools often produce false positives or false bugs and false negatives or missed bugs which are absolutely undesirable to users.
users today however lack the means to guide such tools towards what they believe to be interesting analysis results and away from uninteresting ones.
this paper presents a new approach to user guided program analysis .
it shifts decisions about the kind and degree of approximations to apply in an analysis from the analysis writer to the analysis user .
the user conveys such decisions in a natural fashion by giving feedback about which analysis results she likes or dislikes and re running the analysis.
our approach is a radical departure from existing approaches allowing users to control both the precision and scalability of the analysis.
it o ers a di erent and potentially more useful notion of precision one from the standpoint of the analysis user instead of the analysis writer.
it also allows the user to control scalability as the user s feedback enables tailoring the analysis to the precision needs of the analysis user instead of catering to the broader precision objectives of the analysis writer.
our approach and tool called eugene satis es three useful goals i expressiveness it is applicable to a variety of analyses ii automation it does not require unreasonable e ort by analysis writers or analysis users and iii precision and scalability it reports interesting analysis results from a user s perspective and it handles real world programs.
eugene achieves each of these goals as described next.
expressiveness.
an analysis in eugene is expressed as logic inference rules with optional weights x3 .
in the absence of weights rules become hard rules and the analysis reduces to a conventional one where a solution that satis es all hard rules is desired.
weighted rules on the other hand constitute soft rules that generalize a conventional analysis in two ways they enable to express di erent degrees of approximation and they enable to incorporate feedback from the analysis user that may be at odds with the assumptions of the analysis writer.
the desired solution of the resulting analysis is one that satis es all hard rules and maximizes the weight of satis ed soft rules.
such a solution amounts to respecting all indisputable conditions of the analysis writer while maximizing precision preferences of the analysis user.
automation.
eugene takes as input analysis rules from the analysis writer and automatically learns their weights using an o ine learning algorithm x4.
.
eugene also requires analysis users to specify which analysis results they like or dislike and automatically generalizes this feedback permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
copyright is held by the owner author s .
publication rights licensed to acm.
esec fse august september bergamo italy acm.
... .
4621package org.
apache .
ftpserver 2public class requesthandler socket m controlsocket ftprequestimpl m request ftpwriter m writer bufferedreader m reader boolean m isconnectionclosed public ftprequest getrequest return m request public void close synchronized this i f m isconnectionclosed return m isconnectionclosed true m request .
clear m request null m writer .
close m writer null m reader .
close m reader null m controlsocket .
close m controlsocket null figure java code snippet of apache ftp server.
using an online inference algorithm x4.
.
the analysis rules hard and soft together with the feedback from the user modeled as soft rules forms a probabilistic constraint system that eugene solves e ciently as described next.
precision andscalability.
eugene maintains precision by ensuring integrity and optimality in solving the rules without sacri cing scalability.
integrity i.e.
satisfying hard rules amounts to respecting indisputable conditions of the analysis.
optimality i.e.
maximally satisfying soft rules amounts to generalizing user feedback e ectively.
together these aspects ensure precision.
satisfying all hard rules and maximizing the weight of satis ed soft rules corresponds to the well known maxsat problem .
eugene leverages o the shelf solvers to solve maxsat instances in a manner that is integral optimal and scalable.
we demonstrate the precision and scalability of eugene on two analyses namely datarace detection and monomorphic call site inference applied to a suite of seven java programs of size kloc.
we also report upon a user study involving nine users who employ eugene to guide an informationow analysis on three java micro benchmarks.
in these experiments eugene signi cantly reduces misclassi ed reports upon providing limited amounts of feedback.
in summary our work makes the following contributions .
we present a new approach to user guided program analysis that shifts decisions about approximations in an analysis from the analysis writer to the analysis users allowing users to tailor its precision and cost to their needs.
.
we formulate our approach in terms of solving a combination of hard rules and soft rules which enables leveraging o the shelf solvers for weight learning and inference that scale without sacri cing integrity or optimality.
.
we show the e ectiveness of our approach on diverse analyses applied to a suite of real world programs.
the approach signi cantly reduces the number of misclassi ed reports by using only a modest amount of user feedback.
.
motiv ating example we illustrate our approach using the example of applying the static race detection tool chord to a real world multi threaded java program apache ftp server .analysis relations next p1 p2 program point p1is immediate successor of program point p2 parallel p1 p2 di erent threads may reach program points p1and p2in parallel mayalias p1 p2 instructions at program points p1and p2 may access the same memory location and constitute a possible datarace guarded p1 p2 at least one common lock guards program points p1and p2 race p1 p2 datarace may occur between di erent threads while executing the instructions at program points p1and p2 analysis rules parallel p1 p2 next p3 p1 parallel p3 p2 parallel p1 p2 parallel p2 p1 parallel p1 p2 mayalias p1 p2 guarded p1 p2 a race p1 p2 figure simpli ed race detection analysis.
figure shows a code fragment from the program.
the requesthandler class is used to handle client connections and an object of this class is created for every incoming connection to the server.
the close method is used to clean up and close an open client connection while the getrequest method is used to access the mrequest eld.
both these methods can be invoked from various components of the program not shown and thus can be simultaneously executed by multiple threads in parallel on the same requesthandler object.
to ensure that this parallel execution does not result in any dataraces the close method uses a boolean ag misconnectionclosed .
if this ag is set all calls to close return without any further updates.
if the ag is not set then it is rst updated to true followed by execution of the clean up code lines .
to avoid dataraces on the ag itself it is read and updated while holding a lock on the requesthandler object lines .
all the subsequent code in close is free from dataraces since only the rst call to close executes this section.
however note that an actual datarace still exists between the two accesses to eld mrequest on line and line .
we motivate our approach by contrasting the goals and capabilities of a writer of an analysis such as the race detection analysis in chord with those of a user of the analysis such as a developer of the apache ftp server.
the role of the analysis writer.
the designer or writer of a static analysis tool say alice strives to develop an analysis that is precise yet scales to real world programs and is widely applicable to a large variety of programs.
in the case of chord this translates into a race detection analysis that is context sensitive but path insensitive.
this is a common design choice for balancing precision and scalability of static analyses.
the analysis in chord is expressed using datalog a declarative logic programming language and figure shows a simpli ed subset of the logical inference rules used by chord.
the actual analysis implementation uses a larger set of more elaborate rules but the rules shown here su ce for the discussion.
these rules are used to produce output relations from input relations where the input relations express known program facts and output relations express the analysis outcome.
these rules express the idioms that the 463e1 race on field org.apache.ftpserver.requesthandler.
m isconnectionclosedorg.apache.ftpserver.requesthandler 13org.apache.ftpserver.requesthandler 15eliminated racesr1 race on field org.apache.ftpserver.requesthandler.m requestorg.apache.ftpserver.requesthandler 9org.apache.ftpserver.requesthandler 18r2 race on field org.apache.ftpserver.requesthandler.m requestorg.apache.ftpserver.requesthandler 17org.apache.ftpserver.requesthandler 18r3 race on field org.apache.ftpserver.requesthandler.m writerorg.apache.ftpserver.requesthandler 19org.apache.ftpserver.requesthandler 20r4 race on field org.apache.ftpserver.requesthandler.m readerorg.apache.ftpserver.requesthandler 21org.apache.ftpserver.requesthandler 22r5 race on field org.apache.ftpserver.requesthandler.m controlsocketorg.apache.ftpserver.requesthandler 23org.apache.ftpserver.requesthandler detected races a before feedback.
r1 race on field org.apache.ftpserver.requesthandler.m requestorg.apache.ftpserver.requesthandler 9org.apache.ftpserver.requesthandler detected races e2 race on field org.apache.ftpserver.requesthandler.m requestorg.apache.ftpserver.requesthandler 17org.apache.ftpserver.requesthandler 18e3 race on field org.apache.ftpserver.requesthandler.m writerorg.apache.ftpserver.requesthandler 19org.apache.ftpserver.requesthandler 20e4 race on field org.apache.ftpserver.requesthandler.m readerorg.apache.ftpserver.requesthandler 21org.apache.ftpserver.requesthandler 22e5 race on field org.apache.ftpserver.requesthandler.m controlsocketorg.apache.ftpserver.requesthandler 23org.apache.ftpserver.requesthandler 24e1 race on field org.apache.ftpserver.requesthandler.
m isconnectionclosedorg.apache.ftpserver.requesthandler 13org.apache.ftpserver.requesthandler 15eliminated races b after feedback.
figure race reports produced for apache ftp server.
each report speci es the eld involved in the race and line numbers of the program points with the racing accesses.
the user feedback is to dislike report r2.
analysis writer alice deems to be the most important for capturing dataraces in java programs.
for example rule in figure conveys that if a pair of program points p1 p2 can execute in parallel and if program point p3is an immediate successor of p1 then p3 p2 are also likely to happen in parallel.
rule conveys that the parallel relation is symmetric.
via rule alice expresses the idiom that only program points not guarded by a common lock can be potentially racing.
in particular if program points p1 p2 can happen in parallel can access the same memory location and are not guarded by any common lock then there is a potential datarace between p1andp2.
the role of the analysis user.
the user of a static analysis tool say bob ideally wants the tool to produce exact i.e.
sound and complete results on his program.
this allows him to spend his time on xing the bugs in the program instead of classifying the reports generated by the tool as spurious or real.
in our example suppose that bob runs chord on the apache ftp server program in figure .
based on the rules in figure chord produces the list of datarace reports shown in figure a .
reports r1 r5 are identi ed as potential dataraces in the program whereas for report e1 chord detects that the accesses to misconnectionclosed on lines and are guarded by a common lock and therefore do not constitute a datarace.
typically the analysis user bob is well acquainted with the program being analyzed but not with the details of underlying analysis itself.
in this case given his familiarity with the program it is relatively easy for bob to conclude that the code from line in the body of the close method can never be executed by multiple threads in parallel and thus reports r2 r5 are spurious.
the mismatch between analysis writers and users.
the design decisions of the analysis writer alice have a direct impact on the precision and scalability of the analysis.
the datarace analysis in chord is imprecise for various theoretical and usability reasons.
first the analysis must scale to large programs.
for this reason it is designed to be path insensitive and over approximates the possible thread interleavings.
to eliminate spurious reports r2 r5 the analysis would need to only consider feasible thread interleavings by accurately tracking controlow dependencies across threads.
however such precise analyses do not scale to programs of the size of apache ftp server which comprises kloc.
scalability concerns aside relations such as mayalias are necessarily inexact as the corresponding property is undecidable for java programs.
chord over approximates this property by using a context sensitive but ow insensitive pointer analysis resulting in spurious pairs p1 p2 in this relation which in turn are reported as spurious dataraces.
third the analysis writer may lack su cient information to design a precise analysis because the program behaviors that the analysis intends to check may be vague or ambiguous.
for example in the case of datarace analysis real dataraces can be benign in that they do not a ect the program s correctness .
classifying such reports typically requires knowledge about the program being analyzed.
fourth the program speci cation can be incomplete.
for instance the race in report r1 above could be harmful but impossible to trigger due to timing reasons extrinsic to apache ftp server such as the hardware environment.
in short while the analysis writer alice can in uence the design of the analysis she cannot foresee every usage scenario or program speci c tweaks that might improve the analysis.
conversely analysis user bob is acquainted with the program under analysis and can classify the analysis reports as spurious or real.
but he lacks the tools or expertise to suppress the spurious bugs by modifying the underlying analysis based on his intuition and program knowledge.
closing the gap between analysis writers and users.
our user guided approach aims to empower the analysis user bob to adjust the underlying analysis as per his demands without involving the analysis writer alice.
our system eugene achieves this by automatically incorporating user feedback into the analysis.
the user provides feedback in a natural fashion by liking or disliking a subset of the analysis reports and re running the analysis.
for example relation r2r argument a2a v c constant c2c fact t2t r a variable v2v ground fact g2g r c valuation 2v!c weight w2r hard rules h fh1 hng h vn i 1ti wm i 1t0 i soft rules s fs1 sng s h w probabilistic analysis c h s analysis input output p q g figure syntax of a probabilistic analysis.
when presented with the datarace reports in figure a bob might start inspecting from the rst report.
this report is valid and bob might choose to either like or ignore this report.
liking a report conveys that bob accepts the reported bug as a real one and would like the analysis to generate more similar reports thereby reinforcing the behavior of the underlying analysis that led to the generation of this report.
however suppose that bob ignores the rst report but indicates that he dislikes the second report by clicking on the corresponding icon.
re running chord after providing this feedback produces the reports shown in figure b .
while the true report r1 is generated in this run as well all the remaining spurious reports are eliminated.
this highlights a key strength of our approach eugene not only incorporates user feedback but it also generalizes the feedback to other similar results of the analysis.
reports r2 r5 are correlated and are spurious for the same root cause the code from line in the body of the close method can never be executed by multiple threads in parallel.
bob s feedback on report r2 conveys to the underlying analysis that lines and cannot execute in parallel.
eugene is able to generalize this feedback automatically and conclude that none of the lines from can execute in parallel.
in the following section we describe the underlying details ofeugene that allow it to incorporate user feedback and generalize it automatically to other reports.
.
analysis specification eugene uses a constraint based approach wherein analyses are written in a declarative constraint language.
constraint languages have been widely adopted to specify a broad range of analyses.
the declarative nature of such languages allows analysis writers to focus on the high level analysis logic while delegating low level implementation details to o the shelf constraint solvers.
in particular datalog a logic programming language is widely used in such approaches.
datalog has been shown to su ce for expressing a variety of analyses including pointer and call graph analyses concurrency analyses security analyses and re ection analysis .
existing constraint based approaches allow specifying only hard rules where an acceptable solution is one that satis es all the rules.
however this is insu cient for incorporating feedback from the analysis user that may be at odds with the assumptions of the analysis writer.
to enable the exibility of having con icting constraints it is necessary to allow soft rules that an acceptable solution can violate.
our userguided approach is based on such a constraint language that extends datalog rules with weights.
we refer to analyses speci ed in this extended language as probabilistic analyses .
probabilistic analysis syntax.
aprobabilistic analysis c de ned in figure consists of a set of hard rules hand a set of soft rules s. a hard rule h2his an inference ground clause wn i gi wm i 1g0 i hard clauses vn i i soft clauses vn i i wi maxsat vn i i wi unsat if q qj qsuch that qj and n i 1fwijqj igis maximized otherwise qj vn i i i 8i qj i qj wn i gi wm i 1g0 ii 9i gi 2qor9i g0 i2q a syntax and semantics of a maxsat formula.
j h s k jhk jsk jfh1 hngk vn i 1jhik jfs1 sngk vn i 1jsik jhk v jhk j h w k v jhk w jvn i 1ti wm i 1t0 ik wn i jtik wm i 1jt0 ik jr a1 an k r ja1k jank jvk v jck c b compiling a probabilistic analysis to maxsat.
figure semantics of a probabilistic analysis.
rulea b whereais a conjunction of facts and bis a disjunction of facts.
an analysis facttcomprises a relation name and a tuple of arguments which include variables and constants a fact is a ground fact gwhen all arguments are constants.
our setting subsumes logical analyses written in datalog where all rules are horn rules i.e.
rules with at most one disjunct on the right hand side jbj .
a soft rules2sis a hard rule along with a positive weight w. a weight has a natural probabilistic interpretation where the con dence associated with a soft rule increases with the weight.
for the precise semantics of weights the reader is referred to .
in the absence of soft rules the analysis reduces to a logical analysis consisting of only hard rules.
the analysis input a program p and the analysis output a resultq are represented as sets of ground facts.
probabilistic analysis semantics.
we de ne the semantics of a probabilistic analysis in terms of an optimization extension of the boolean satis ability sat problem called the weighted partial maximum satis ability maxsat problem shown as procedure maxsat in figure a .
this procedure takes as input a maxsat formula comprising a set of hard clauses and a set of soft clauses .
these are counterparts of hard rules and soft rules in a probabilistic analysis.
the key di erence is that all facts in each hard or soft maxsat clause are grounded whereas analysis rules can contain ungrounded facts.
the maxsat procedure views each unique ground fact in the input maxsat formula as a separate boolean variable.
the procedure returns either unsat if no assignment of truth values to the boolean variables satis es the set of hard clauses or a solution q denoting the assignment g g2q ?true false i.e.
it sets variables corresponding to ground facts contained in q to true and the rest to false.
the solution qnot only satis es all hard clauses in but it also maximizes the sum of the weights of satis ed soft clauses in .
note that qis not necessarily unique two solutions q1andq2areequivalent ifweight q1 weight q2 .
the compilation of a probabilistic analysis to a maxsat formula is shown in figure b .
the compilation proce465input facts next mayalias guarded next mayalias guarded next mayalias guarded maxsat formula w1 parallel next parallel parallel parallel w1 parallel next parallel w1 parallel next parallel parallel parallel w1 parallel next parallel w1 parallel next parallel parallel mayalias guarded race parallel mayalias guarded race w2 race output facts before feedback parallel parallel race race parallel parallel race race output facts after feedback parallel race figure probabilistic analysis example.
dure grounds each analysis rule into a set of corresponding maxsat clauses.
in particular the conversion jhk v jhk grounds analysis rule hby enumerating all possible groundings of variables to constants producing a maxsat clause for each unique valuation to the variables in h. there are a number of solvers that e ciently compile a probabilistic analysis down into a maxsat formula and solve the corresponding formula to produce the desired output that satis es all the hard rules and maximizes the weight of the satis ed soft rules.
eugene treats the underlying solver as a black box and can use any of these solvers to solve the constraints of a probabilistic analysis.
example.
equipped with the concept of probabilistic analysis we can now describe how eugene works on the race detection example from x2.
figure shows a subset of the input and output facts as well as a snippet of the maxsat formula constructed for the example.
the input facts are derived from the analyzed program apache ftp server and comprise the next mayalias and guarded relations.
in all these relations the domain of program points is represented by the corresponding line number in the code.
note that all the analysis rules expressed in figure are hard rules since existing tools like chord do not accept soft rules.
however we assume that when this analysis is fed to eugene rule is speci ed to be soft by analysis writer alice which captures the fact that the parallel relation is imprecise.eugene automatically learns the weight of this rule to bew1from training data see x4.
for details .
given these input facts and rules the maxsat problem to be solved is generated by grounding the analysis rules and a snippet of the constructed maxsat formula is shown in figure .
ignoring the clause enclosed in the box solving this maxsat formula without the boxed clause yields output facts a subset of which is shown under output facts before feedback in figure .
the output includes multiple spurious races like race race and race .
as described inx2 when analysis user bob provides feedback that race is spurious eugene suppresses all spurious races while retaining the real race race .eugene achieves this by incorporating the user feedback itselfas a soft rule represented by the boxed clause race in figure .
the weight for such user feedback is also learned during the training phase.
assuming the weight w2of the feedback clause is higher than the weight w1of rule a reasonable choice that emphasizes bob s preferences over alice s assumptions the maxsat semantics ensures that the solver prefers violating rule over violating the feedback clause.
when the maxsat formula with the boxed clause in figure is then fed to the solver the output solution violates the clause w1 parallel next parallel and does not produce facts parallel and race in the output.
further all the facts that are dependent on parallel are not produced either.
this implies that facts like parallel parallel parallel are not produced and therefore race andrace are also suppressed.
thus eugene is able to generalize based on user feedback.
the degree of generalization depends on the quality of the weights assigned or learned for the soft rules.
.
the eugene system this section describes our system eugene for user guided program analysis.
its work ow shown in figure comprises an online inference stage and an o ine learning stage.
in the online stage eugene takes the probabilistic analysis speci cation together with a program pthat an analysis user bob wishes to analyze.
the inference engine described inx4.
uses these inputs to produce the analysis output q. further the online stage allows bob to provide feedback on the produced output q. in particular bob can indicate the output queries he likes ql or dislikes qd and invoke the inference engine with qlandqdas additional inputs.
the inference engine incorporates bob s feedback as additional soft rules in the probabilistic analysis speci cation used for producing the new result q. this interaction continues until bob is satis ed with the analysis output.
the accuracy of the produced results in the online stage is sensitive to the weights assigned to the soft rules.
manually assigning weights is not only ine cient but in most cases it is also infeasible since weight assignment needs analysis of data.
therefore eugene provides an o ine stage that automatically learns the weights of soft rules in the probabilistic analysis speci cation.
in the o ine stage eugene takes a logical analysis speci cation from analysis writer alice and training data in the form of a set of input programs and desired analysis output on these programs.
these inputs are fed to the learning engine described in x4.
.
the logical analysis speci cation includes hard rules as well as rules marked as soft whose weights need to be learnt.
the learning engine infers these weights to produce the probabilistic analysis speci cation.
the learning engine ensures that the learnt weights maximize the likelihood of the training data with respect to the probabilistic analysis speci cation.
.
online component of eugene inference algorithm describes the online component inference of eugene .inference takes as input a probabilistic analysis h s with learnt weights and the program pto be analyzed.
first in line the algorithm augments the hard and soft rules h s of the analysis with the inputs p ql qdto 1this is due to implicit soft rules that negate each output relation such as w0 parallel p1 p2 wherew0 w in order to obtain the least solution.
466p program to be analyzedprobabilistic analysis specificationinferenceenginelearningenginelogical analysis specification analysiswriteraliceanalysisuserbobofflineonlineql qd parts of output that user likes vs. dislikesq output of analysis on program p p q desired analysisoutput on training programfigure work ow of the eugene system for user guided program analysis.
algorithm inference online component of eugene .
param wl wd weights of liked and disliked queries.
inputc h s probabilistic analysis.
inputp program to analyze.
output q final output of user guided analysis.
ql qd h0 h p repeat s0 s f g wl jg2qlg f g wd jg2qdg q solve h0 s0 ql positiveuserfeedback q qd negativeuserfeedback q untilql qd the analysis to obtain an extended set of rules h0 s0 lines and .
notice that the user feedback ql liked queries andqd disliked queries are incorporated as soft rules in the extended rule set.
each liked query feedback is assigned the xed weight wl while each disliked query feedback is assigned weight wd line .
weights wlandwdare learnt in the o ine stage and fed as parameters to algorithm .
instead of using xed weights for the user feedback two other options are a treating user feedback as hard rules and b allowing a di erent weight for each query feedback.
option a does not account for users being wrong leaving no room for the inference engine to ignore the feedback if necessary.
option b is too ne grained requiring learning separate weights for each query.
we therefore take a middle ground between these two extremes.
next in line the algorithm invokes a weighted constraints solver with the extended set of rules.
note that eugene treats the solver as a black box and any suitable solver su ces.
the solver produces a solution qthat satises all the hard rules in the extended set while maximizing the weight of satis ed soft rules.
the solution qis then presented to bob who can give his feedback by liking or disliking the queries lines .
the sets of liked and disliked queries qlandqd are used to further augment the hard and soft rules h s of the analysis.
this loop lines continues until no further feedback is provided by bob.
.
offline component of eugene learning algorithm describes the o ine component learning of eugene .
it is an adaptation of to our application.
learning takes a probabilistic analysis c h s with arbitrary weights a set of programs pand the desired analysis outputqas input and outputs a probabilistic analysis c0 with learnt weights.
without loss of generality we assume thatpis encoded as a set of hard clauses and is part of h. as a rst step in line learning assigns initial weights to all the soft rules.
the initial weight of a rule h2sisalgorithm learning o ine component of eugene .
param rate of change of weight of soft rules.
inputc h s initial probabilistic analysis.
inputq desired analysis output.
output c0 probabilistic analysis with learnt weights.
s0 f h w0 j9w h w 2sandw0 log n1 n2 g wheren1 jgroundings h q j n2 jviolations h q j. repeat c0 h s0 q0 inference c0 s s0 s0 f h w0 j9w h w 2sand w0 w n1 n2 g wheren1 jviolations h q0 j n2 jviolations h q j. untils0 s violations h q fjhk jq6j jhk g groundings h q fjhk jqj jhk g computed as a log of the ratio of the number of groundings ofhsatis ed by the desired output qto the number of violations of hbyq lines .
in other words the initial weight captures the log odds of a rule being true in the training data.
note that in the case violations h q it is substituted by a suitably small value .
next in line the probabilistic analysis c0 de ned in line with the initialized weights is fed to the inference procedure described in algorithm .
this produces a solution q0 that is integral and optimal for the probabilistic analysis c0.
the solution q0is then used to update the weights of the soft rules.
the weights are updated according to the formulae in lines .
the basic intuition for these update rules is as follows weights learnt by the learning algorithm must be such that the output solution of the inference algorithm for the training program is as close to the desired output q as possible.
towards that end if the current output q0produces more violations for a rule than the desired output it implies that the rule needs to be strengthened and its weight should be increased.
on the other hand if the current outputq0produces fewer violations for a rule then q the rule needs to be weakened and its weight should be reduced.
the formula in the algorithm has exactly the same e ect as described here.
moreover the rate of change of weights can be controlled by an input parameter .
the learning process continues iteratively until the learnt weights do not change.
in practice the learning process can be terminated after a xed number of iterations or when the di erence in weights between successive iterations does not change signi cantly.
.
empirical ev aluation we implemented eugene atop chord an extensible program analysis framework for java bytecode that supports 467writing analyses in datalog.
in our evaluation we investigate the following research questions rq1 does using eugene improve analysis precision for practical analyses applied to real world programs?
how much feedback is needed for the same and how does the amount of provided feedback a ect the precision?
rq2 does eugene scale to large programs?
does the amount of feedback in uence the scalability?
rq3 how feasible is it for users to inspect analysis output and provide useful feedback to eugene ?
.
experimental setup we performed two di erent studies with eugene a control study and a user study.
first to evaluate the precision and scalability of eugene we performed a control study using two realistic analyses expressed in datalog applied to seven java benchmark programs.
the goal of this study is to thoroughly investigate the performance of eugene in realistic scenarios and with varying amounts of feedback.
to practically enable the evaluation of eugene over a large number of a data points in the benchmark analysis feedback space this study uses a more precise analysis instead of a human user as an oracle for generating the feedback to be provided.
this study helps us evaluate rq1 andrq2 .
second to evaluate the practical usability of eugene when human analysis users are in the loop we conducted a user study with nine users who employed eugene to guide an informationow analysis on three benchmark programs.
in contrast to the rst study the human users provide the feedback in this case.
this study helps us evaluate rq3 .
all experiments were run using oracle hotspot jvm .
.
on a linux server with 64gb ram and .0ghz processors.
clients.
our two analyses in the rst study are datarace detection datarace and monomorphic call site inference polysite while we use an informationow info ow analysis for the user study.
each of these analyses is sound and composed of other analyses written in datalog.
for example datarace includes a thread escape analysis and a may happen in parallel analysis while polysite and info ow include a pointer analysis and a call graph analysis.
the pointer analysis used here is a ow context insensitive eldsensitive andersen style analysis using allocation site heap abstraction .
the datarace analysis is from while the polysite analysis has been used in previous works to evaluate pointer analyses.
the info ow analysis only tracks explicit information ow similar to the analysis described in .
for scalability reasons all these analyses are context object and ow insensitive which is the main source of false positives reported by them.
benchmarks.
the benchmarks for the rst study upper seven rows of table are kloc in size and include programs from the dacapo suite antlr avrora luindex lusearch and from past works that address our two analysis problems.
the benchmarks for the user study bottom three rows of table are .
.
kloc in size and are drawn from securibench micro a micro benchmark suite designed to exercise di erent parts of a static informationow analyzer.
methodology.
we describe the methodology for the ofine learning and online inference stages of eugene .
o ine stage.
we rst converted the above three logical analyses into probabilistic analyses using the o ine train table statistics of our probabilistic analyses.
rules input relations output relations datarace polysite info ow ing stage of eugene .
to avoid selection bias we used a set of small benchmarks for training instead of those in table .
speci cally we used elevator and tsp kloc each from .
while the training benchmarks are smaller and fewer than the testing benchmarks they are sizable realistic and disjoint from those in the evaluation demonstrating the practicality of our training component.
besides the sample programs the training component of eugene also requires the expected output of the analyses on these sample programs.
since the main source of false positives in our analyses is the lack of context and object sensitivity we used context and object sensitive versions of these analyses as oracles for generating the expected output.
speci cally we usedk object sensitive versions with cloning depth k .
note that these oracle analyses used for generating the training data comprise their own approximations for example ow insensitivity and thus do not produce the absolute ground truth.
using better training data would only imply that the weights learnt by eugene are more re ective of the ground truth leading to more precise results.
online stage.
we describe the methodology for the online stage separately for the control study and the user study.
control study methodology.
to perform the control study we started by running the inference stage of eugene on our probabilistic analyses datarace andpolysite with no feedback to generate the initial set of reports for each benchmark.
next we simulated the process of providing feedback by i randomly choosing a subset of the initial set of reports ii classifying each of the reports in the chosen subset as spurious or real and iii re running the inference stage ofeugene on the probabilistic analyses with the labeled reports in the chosen subset as feedback.
to classify the reports as spurious or real we used the results of k objectsensitive versions of our client analyses as ground truth.
in other words if a report contained in the chosen subset is also generated by the precise version of the analysis it is classi ed as a real report otherwise it is labeled as a spurious report.
for each benchmark analysis pair we generated random subsets that contain and of the initial reports.
this allows us to study the e ect of varying amounts of feedback on eugene s performance.
additionally eugene can be sensitive to not just the amount of feedback but also to the actual reports chosen for feedback.
to discount this e ect for a given benchmark analysis pair and a given feedback subset size we ran eugene thrice using di erent random subsets of the given size in each run.
randomly choosing feedback ensures that we conservatively estimate the performance of eugene .
finally we evaluated the quality of the inference by comparing the output of eugene with the output generated by the k object sensitive versions of our analyses with k .
user study methodology.
for the user study we engaged nine users all graduate students in computer science to run eugene oninfo ow analysis.
each user was assigned two benchmarks from the set of fsecbench1 secbench2 secbench3g such that each of these benchmarks was assigned to six users in total.
the users interacted with eugene by rst running it without any feedback so as to produce 468table benchmark statistics.
columns total and app are with and without jdk library code.
brief description classes methods bytecode kb source kloc app total app total app total app total antlr parser translator generator avrora microcontroller simulator analyzer ftp apache ftp server hedc web crawler from eth luindex document indexing and search tool lusearch text indexing and search tool weblech website download mirror tool secbench1 securibench micro .
.
.
.
secbench2 securibench micro .
.
.
.
secbench3 securibench micro .
.
.
.
false reports eliminated antlr700 avrora119 ftp153 hedc2597 luindex183 lusearch10 weblech0 true reports retained baseline false reports baseline true reports figure results of eugene ondatarace analysis.
the initial set of reports.
the users then analyzed these produced reports and were asked to provide any eight reports with their corresponding label spurious or real as feedback.
also for each benchmark we recorded the time spent by each user in analyzing the reports and generating the feedback.
next eugene was run with the provided feedback and the produced output was compared with manually generated ground truth for each of the benchmarks.
we next describe the results of evaluating eugene s precision rq1 scalability rq2 and usability rq3 .
.
precision of eugene the analysis results of our control study under varying amounts of feedback are shown in figures and .
in these gures baseline false reports and baseline true reports are the number of false and true reports produced when eugene is run without any feedback.
the light colored bars above and below the x axis indicate the of false reports eliminated and the of true reports retained respectively when the of feedback indicated by the corresponding dark colored bars is provided.
for each benchmark the feedback percentages increase from left to right i.e.
to .
ideally we want all the false reports to be eliminated and all the true reports to be retained which would be indicated by the light color bars extending to on both sides.
even without any feedback our probabilistic analyses are already fairly precise and sophisticated and eliminate all except the non trivial false reports.
despite this eugene helps eliminate a signi cant number of such hard to refute reports.
on average of the false reports are eliminated across all our experiments with feedback.
likewise importantly on average of the true reports are retained when feedback is provided.
also note that with feedback the percentage of false reports eliminated falls to on average while that of true reports retained is .
a ner grained look at the results for individual benchmarks and analyses reveals that in many cases increasing feedback only leads to modest gains.
false reports eliminated antlr119 avrora64 ftp41 hedc71 luindex293 lusearch29 weblech0 true reports retained baseline false reports baseline true reportsfigure results of eugene onpolysite analysis.
false reports eliminated avrora2597 luindex0 true reports retained2 62 baseline false reports baseline true reports figure results of eugene ondatarace analysis with feedback .
.
.
.
we next discuss the precision of eugene for each of our probabilistic analyses.
for datarace with feedback an average of of the false reports are eliminated while an average of of the true reports are retained.
further with feedback the averages are for false reports eliminated and for true reports retained.
although the precision of eugene increases with more feedback in this case the gains are relatively modest.
note that given the large number of initial reports generated for luindex and avrora and respectively it is somewhat impractical to expect analysis users to provide up to feedback.
consequently we re run eugene for these benchmarks with .
.
and .
feedback.
the results are shown in figure .
interestingly we observe that for luindex with only feedback on the false reports and .
feedback on true reports eugene eliminates of false reports and retains of the true reports.
similarly for avrora with only .
feedback on the false reports and .
feedback on true reports eugene eliminates of false reports and retains of the true reports.
these numbers indicate that for the datarace client eugene is able to generalize even with a very limited amount of user feedback.
for polysite with feedback an average of of the false reports are eliminated and of the true reports are retained while with feedback of the false reports are eliminated and of the true reports are re469antlr avrora ftp hedc luindex lusearch weblech05101520running time minutes datarace analysisfeedback antlr avrora ftp hedc luindex lusearch weblech020406080100120140running time minutes polysite analysisfeedback figure running time of eugene .
tained.
there are two important things to notice here.
first the number of eliminated false reports does not always grow monotonically with more feedback.
the reason is that eugene is sensitive to the reports chosen for feedback but in each run we randomly choose the reports to provide feedback on.
though the precision numbers here are averaged over three runs for a given feedback amount the randomness in choosing feedback still seeps into our results.
second eugene tends to do a better job at generalizing the feedback for the larger benchmarks compared to the smaller ones.
we suspect the primary reason for this is the fact that smaller benchmarks tend to have a higher percentage of bugs with unique root causes and thereby a smaller number of bugs are attributable to each unique cause.
consequently the scope for generalization of the user feedback is reduced.
answer to rq1 eugene signi cantly reduces false reports with only modest feedback while retaining the vast majority of true reports.
though increasing feedback leads to more precise results in general for many cases the gain in precision due to additional feedback is modest.
.
scalability of eugene the performance of eugene for our control study in terms of the inference engine running time is shown in figure .
for each benchmark analysis feedback conguration the running time shown is an average over the three runs of the corresponding con guration.
we observe two major trends from this gure.
first as expected the running time is dependent on the size of the benchmark and the complexity of the analysis.
for both the analyses in the control study eugene takes the longest time for avrora our largest benchmark.
also for each of our benchmarks thedatarace analysis with fewer rules needs shorter time.
recollect that eugene uses an o the shelf solver for solving the constraints of probabilistic analysis and thus the performance of the inference engine largely depends on the performance of the underlying solver.
the running time of all such solvers depends on the number of ground clauses that are generated and this number in turn depends on the size of the input program and complexity of the analysis.
second the amount of feedback does not signi cant affect running time.
incorporating the feedback only requires adding the liked disliked queries as soft rules and thus does not signi cantly alter the underlying set of constraints.
finally the fact that eugene spends up to minutes polysite analysis on avrora with feedback might seemdisconcerting.
but note that this represents the time spent by the system rather than the user in computing the new results after incorporating the user feedback.
since eugene uses the underlying solver as a black box any improvement in solver technology directly translates into improved performance of eugene .
given the variety of solvers that already exist and the ongoing active research in this area we expect the running times to improve further.
answer to rq2 eugene e ectively scales to large programs up to a few hundred kloc and its scalability will only improve with advances in underlying solver technology.
additionally the amount of feedback has no signi cant e ect on the scalability of eugene .
.
usability of eugene in this section we evaluate the results of our user study conducted using eugene .
the usage model for eugene assumes that analysis users are familiar with the kind of reports produced by the analysis as well as with the program under analysis.
to ensure familiarity with reports produced by info ow analysis we informed all our users about the expected outcomes of a precise info ow analysis in general.
however familiarity with the program under analysis is harder to achieve and typically requires the user to have spent time developing or xing the program.
to address this issue we choose relatively smaller benchmarks in our study that users can understand without too much e ort or expertise.
the users in this study were not informed about the internal working of either eugene or the info ow analysis.
the two main questions that we evaluate here are i the ease with which users are able to analyze the reported results and provide feedback and ii the quality of the user feedback.
to answer the rst question we record the time spent by each user in analyzing the info ow reports and providing the feedback for each benchmark.
recall that we ask each user to provide eight reports as feedback labeled either spurious or real.
figure shows the time spent by each user on analyzing the reports and providing feedback.
we observe that the average time spent by the users is only minutes on secbench1 .
minutes on secbench2 and .
minutes on secbench3 .
these numbers show that the users are able to inspect the analysis output and provide feedback toeugene with relative ease on these benchmarks.
to evaluate the quality of the user provided feedback we consider the precision of eugene when it is run on the probabilistic version of info ow analysis with the feedback.
figure shows the false bugs eliminated and the true bugs retained by eugene for each user and benchmark.
this gure is similar in format to figures and .
however for each benchmark instead of di erent bars representing a di erent amount of feedback the di erent bars here represent di erent users with feedback amount xed at eight reports.
the varying behavior of eugene on these benchmarks highlights the strengths and limits of our approach.
forsecbench1 an average of of the false reports are eliminated and .
of the true reports are retained.
the important thing to note here is that the number of true reports retained is sensitive to the user feedback.
with the right feedback all the true reports are retained 5thbar .
however in the case where the user only chooses to provide one true feedback report 4thbar eugene fails to retain most of the true reports.
470secbench1 secbench2 secbench30510152025user inspection time minutes figure time spent by each user in inspecting reports of info ow analysis and providing feedback.
false reports eliminated secbench19 secbench216 secbench30 true reports retained baseline false reports baseline true reports figure results of eugene oninfo ow analysis with real user feedback.
each bar maps to a user.
for secbench2 an average of of the false reports are eliminated and of the true reports are retained.
the reason eugene does well here is that secbench2 has multiple large clusters of reports with the same root cause.
user feedback on any report in such clusters generalizes to other reports in the cluster.
this highlights the fact that eugene tends to produce more precise results when there are larger clusters of reports with the same root cause.
forsecbench3 an average of of the false reports are eliminated while of the true reports are retained.
first notice that this benchmark produces only eight false reports.
we traced the relatively poor performance of eugene in generalizing the feedback on false reports to limiting the analysis user s interaction with the system to liking or disliking the results.
this does not su ce for secbench3 because to e ectively suppress the false reports in this case the user must add new analysis rules.
we intend to explore this richer interaction model in future work.
finally we observed that for all the benchmarks in this study the labels provided by the users to the feedback reports matched with the ground truth.
while this is not unexpected it is important to note that eugene is robust even under incorrectly labeled feedback and can produce precise answers if a majority of the feedback is correctly labeled.
answer to rq3 it is feasible for users to inspect analysis output and provide feedback to eugene since they only needed an average of minutes for this activity in our user study.
further in general eugene produce precise results with this user feedback leading to the conclusion that it is not unreasonable to expect useful feedback from users.
.
limitations of eugene eugene requires analyses to be speci ed using the datalogbased language described in x3.
additionally the program to be analyzed itself has to be encoded as a set of ground facts.
this choice is motivated by the fact that a growing number of program analysis tools including bddbddb chord doop llvm soot and z3 support specifying analyses and programs in datalog.the o ine learning component of eugene requires the analysis designer to specify which analysis rules must be soft.
existing analyses employ various approximations such as path ow and context insensitivity in our experience rules encoding such approximations are good candidates for soft rules.
further the learning component requires suitable training data in the form of desired analysis output.
we expect such training data to be either annotated by the user or generated by running a precise but unscalable version of the same analysis on small sample programs.
learning using partial or noisy training data is an interesting future direction that we plan to explore.
.
related work our work is related to existing work on classifying error reports and other applications of probabilistic reasoning.
dillig et al.
propose a user guided approach to classify reports of analyses as errors or non errors.
they use abductive inference to compute small relevant queries to pose to a user that capture exactly the information needed to discharge or validate an error.
their approach does not incorporate user feedback into the analysis speci cation and generalize it to other reports.
blackshear and lahiri propose a post processing framework to prioritize alarms produced by a static veri er based on semantic reasoning of the program.
statistical error ranking techniques employ statistical methods and heuristics to rank errors reported by an underlying static analysis.
non statistical clustering techniques correlate error reports based on a rootcause analysis .
our technique on the other hand makes the underlying analysis itself probabilistic.
recent years have seen many of applications of probabilistic reasoning to analysis problems.
in particular speci cation inference techniques based on probabilistic inference can be formulated as probabilistic analyses as de ned inx3 .
it would be interesting to explore the possibility of solving these speci cation inference formulations using the algorithms proposed in this paper.
another connection between user guided program analysis and speci cation inference is that user feedback can be looked upon as an iterative method by means of which the analysis user communicates a speci cation to the program analysis tool.
finally the inferred speci cations can themselves be employed as soft rules in our system.
.
conclusion we presented a user guided approach to program analysis that shifts decisions about the kind and degree of approximations to apply in analyses from analysis writers to analysis users.
our approach enables users to interact with the analysis by providing feedback on a portion of the results produced by the analysis and automatically uses the feedback to guide the analysis approximations to the user s preferences.
we implemented our approach in a system eugene and evaluated it on real users analyses and programs.
we showed that eugene greatly reduces misclassi ed reports even with limited amounts of user feedback.