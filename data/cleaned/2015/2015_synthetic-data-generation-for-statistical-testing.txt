synthetic data generation for statistical testing ghanem soltana mehrdad sabetzadeh and lionel c. briand snt centre for security reliability and trust university of luxembourg luxembourg email ghanem.soltana mehrdad.sabetzadeh lionel.briand uni.lu abstract usage based statistical testing employs knowledge about the actual or anticipated usage profile of the system under test for estimating system reliability.
for many systems usagebased statistical testing involves generating synthetic test data.
such data must possess the same statistical characteristics as the actual data that the system will process during operation.synthetic test data must further satisfy any logical validity constraints that the actual data is subject to.
targeting data intensive systems we propose an approach for generating synthetic testdata that is both statistically representative and logically valid.
the approach works by first generating a data sample that meets the desired statistical characteristics without taking into accountthe logical constraints.
subsequently the approach tweaks the generated sample to fix any logical constraint violations.
the tweaking process is iterative and continuously guided towardachieving the desired statistical characteristics.
we report on a realistic evaluation of the approach where we generate a synthetic population of citizens records for testing a public administration it system.
results suggest that our approach is scalable and capable of simultaneously fulfilling the statisticalrepresentativeness and logical validity requirements.
index terms test data generation usage based statistical testing model driven engineering uml ocl.
i. i ntroduction usage based statistical testing or statistical testing for short is concerned with detecting faults that cause the most frequent failures thus affecting reliability the most and with estimat ing reliability via statistical models .
in contrast to testingtechniques that focus on system verification fault detection e.g.
testing driven by code coverage statistical testing focuseson system validation from the perspective of users.
statisticaltesting typically requires a usage profile of the system under test.
this profile characterizes often through a probabilisticformalism the population of the system s usage scenarios .
existing work on usage profiles has focused on state and event based systems with the majority of the work beingbased on markov chains .
for manysystems which we refer to as data centric and concentrate on in this paper system behaviors are driven primarily by data rather than being triggered by stimuli.
for example consider apublic administration system that calculates social benefits forcitizens.
how such a system behaves is determined mainly bycomplex and interdependent data such as citizens employmentand household makeup.
the system s scenarios of use are thusintimately related to the data that is processed by the system.consequently the usage profile of such a system is governedby the statistical characteristics of the system s input data or stated otherwise by the system s data profile.
given our focus on data centric systems and the explanation above weequate for the purposes of this paper usage profile and dataprofile and use the latter term hereafter.
when actual data e.g.
real citizens records in the aforementioned example is available one may be able to performstatistical testing without a data profile.
in most cases how ever gaps exist in actual data since new and retrofit systemsmay require data beyond what has been recorded in the past.these gaps need to be filled with synthetic data.
to generate synthetic data that is representative and thus suitable for sta tistical testing a profile of the missing data will be required.
further and perhaps more importantly synthetic data and hence a data profile are indispensable when access to actualdata is restricted.
notably under most privacy regulations e.g.
eu s general data protection regulation repurposing of personal data is prohibited without explicit consent.
thiscomplicates sharing of any actual personal data with third parties who are responsible for software development and test ing.
anonymization offers a partial solution to this problem however so often comes at the cost of reduced dataquality and proneness to deanonymization attacks .
data profiles have received little attention in the literature on software testing.
this is despite the fact that many data centricsystems e.g.
public administration and financial systems aresubject to reliability requirements and thus statistical testing.recently we proposed a statistical data profile and a heuristicalgorithm for generating representative synthetic data .
although motivated by microeconomic simulation rather than software testing our previous approach provides a usefulbasis for generating data that can be used for statistical testing.however the approach suffers from an important limitation while the approach generates synthetic data that is alignedwith a desired set of statistical distributions and has shownto be good enough for running financial simulations the approach cannot guarantee the satisfaction of logical constraints that need to be enforced over the generated data.
to illustrate we note three among several other logical anomalies that we observed when using our previous approach for generating test cases based on a data profile ofcitizens records children who were older than their par ents individuals who were married before being born andindividuals who were classified as widower without everhaving been married.
without the ability to enforce logicalconstraints to avoid such anomalies the generated data isunsuitable for statistical testing and estimating reliability.
thisis because such anomalies may result in exceptions or systembehaviors that are not meaningful.
in either case targetedsystem behaviors will not be exercised.
.
c circlecopyrt2017 ieeease urbana champaign il usa t echnical research872 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the question that we investigate in this paper is as follows can we generate synthetic test data that is both statistically representative and logically valid?
the key challenge we need to address when tackling this question is scalability.
specifically to obtain statistical representativeness we need to construct a large data sample test suite potentially with hundreds orthousands of members.
at the same time this large sample has to satisfy logical constraints meaning that weneed to apply computationally expensive constraint solving.
contributions.
the contributions of this paper are as follows we develop a model based test data generator that can simultaneously satisfy statistical representativeness and logi cal validity requirements over complex interdependent data.the desired statistical characteristics are expressed usingour previously developed probabilistic uml annotations .v alidity constraints are expressed using uml s constraintlanguage ocl .
our data generator incorporates twocollaborating components a a search based ocl constraintsolver which enhances previous work by ali et al.
and b a mechanism that guides the solver toward satisfying thestatistical characteristics that the generated data test suite must exhibit.
we evaluate our data generator through a realistic case study where synthetic data is required for statistical testingof a public administration it system in luxembourg.
ourresults suggest that our data generator can create in practicaltime test data that is sound i.e.
satisfies the necessary validity constraints and at the same time is closely aligned with the desired statistical characteristics.
ii.
b ackground in this section we briefly describe our previous data generation approach .
we leverage this approach for express ing the desired statistical characteristics of data and gener ating initial data samples which we process further to achievenot only representativeness but logical validity as well.
for specifying statistical characteristics we use a set of annotations stereotypes which can be attached to a data schema expressed as a uml class diagram.
fig.
illustratessome of these annotations on a small excerpt of a data schema for a tax administration system.
the probabilistic type stereotypes applied to the specializations of the taxpayer class state that of the taxpayers should be resident and the remainder should be non resident.
the from histogram stereotype attached to the birth year attribute provides via a histogram the birth year distributionfor taxpayers.
the attribute birth year is further annotated with a conditional probability specified via the value dependency stereotype.
the details of this conditional probability are provided by the legal age for pensioners box.
the information in the box reads as follows of pensioners havetheir birth year between and i.e.
are between 57and years old the remaining are older than .
the multiplicity stereotype attached to the association between taxpayer and income classes describes via the income cardinality histogram the distribution of the number multiplicity constraints income cardinality taxpayer incomes .. income income cardinality birth year integer from histogram labels frequencies value dependency queries taxpayer abstract residenttaxpayer probabilistic type frequency .
nonresidenttaxpayer probabilistic type frequency .
from histogram labels frequencies condition self.incomes exists oclistypeof pension dependency age for pensioners ocl query expressions legal age for pensioners from histogram labels frequencies fig.
.
data schema excerpt annotated with statistical characteristics of incomes per taxpayer.
as shown in fig.
of the taxpayers have one income have two and so on.
for generating a data sample we previously proposed a heuristic technique that is aimed exclusively at representativeness .
this technique traverses the elements of thedata schema and instantiates them according to the prescribedprobabilities.
the technique attempts to satisfy multiplicityconstraints but satisfaction is not guaranteed.
more complexlogical constraints are not supported.
in this paper we use as a starting point the data generated by our previous approach and alter this data to make it validwithout compromising representativeness.
indeed as we showin section v our new approach not only results in logicallyvalid data but also outperforms our previous approach in termsof representativeness.
iii.
a pproach overview fig.
presents an overview of our approach for generating representative and valid test data.
steps are manual and step is automatic.
in step define data schema we define using a uml class diagram cd the schema of the data to generate.
this diagram illustrated earlier in fig.
is thebasis for a capturing the desired statistical characteristics ofdata step and b generating synthetic data step .
data schema class diagram .
de fine statistical characteristicsannotated data schema s p p m .
de fine data validity constraints constraints4.
generate synthetic data synthetic data sample test suite ocl1.
de fine data schema data profile fig.
.
approach for generating v alid and representative synthetic data in step define statistical characteristics the cd from step is enriched with probabilistic annotations see sec tion ii to express the representativeness requirements thatshould be met during data generation in step .
in step define data validity constraints users express via the object constraint language ocl the logical constraints thatthe generated data must satisfy.
for example the followingocl constraint states that children must be born at least16 years after their parents self.children forall c c.birth year self.birth year .
here self refers to a person.
steps and of our approach can in principle be done in parallel.
nevertheless it is advantageous to perform step authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
after step .
this is because the probabilistic annotations of step may convey some implicit logical constraints.
for example the annotations of step may specify a uniform dis tribution over the month of birth for physical persons.
it wouldtherefore be redundant to define the following ocl constraint self.birth month and self.birth month .
such redundancies can be avoided by steps and sequentially.
step generate synthetic data generates a data sample test suite based on a data profile.
in our approach a data profile is materialized by the combination of the probabilisticannotations from step and the ocl constraints for step .as stated earlier the synthetic data generated in step mustmeet both the statistical representativeness and logical validityrequirements respectively specified in steps and .
theoutput from step is a collection of instance models i.e.
instantiations of the underlying data schema.
each instancemodel characterizes one test case for statistical testing.
in the next section we elaborate step which is the main technical contribution of this paper.
iv .
g enera ting synthetic data in this section we describe our synthetic data generator.
fig.
shows the strategy employed by the data generator.initially a potentially invalid collection of instance models is created using our previous data generation approach see sec tion ii .
we refer to this initial collection as the seed sample.
our data generator then transforms the seed sample into a collection of valid instance models.
this is achieved using acustomized ocl constraint solver presented in section iv a. the solver attempts to repair the invalid instance models in the seed sample.
to do so the solver considers the constraintspecified in step of our overall approach alongsidethe multiplicity constraints of the underlying data schema andthe constraints implied by the probabilistic annotations fromstep of the approach.
the rationale for feeding the solverwith instance models from the seed sample rather than havingthe solver build instance models from scratch is based on thefollowing intuitions by starting from the seed sample thesolver is more likely to be able to reach valid instance models and the valid sample built by the solver will not end uptoo far away from being representative in turn making it easierto fix deviations from representativeness as we discuss later.
the ocl solver that we use is based on metaheuristic search.
if the solver cannot fix a given instance model within apredefined maximum number of iterations the instance modelis discarded.
to compensate the seed sample is extended witha new instance model byre invoking our previous data genera tor.
this process continues until we obtain the desired numberof valid instance models test cases .
the number of instance models to generate is an input parameter that is set by users.
once we have a valid data sample that has the requested number of instance models in it our data generator attempts to realign the sample back with the desired statistical charac teristics.
this is done through an iterative process delineatedin fig.
with a dashed boundary.
we elaborate the details ofthis iterative process in section iv b.create seed sample create valid sampleseed sample with potential logical anomalies valid data sample all validity constraints user de fined constraints including multiplicity constraints from data schema plus constraints implied by probabilistic annotations generate corrective constraintspropose tweaked instance modelcorrective constraints tweaked instance modelfinal data sample for each instance model in the sample ocl fig.
.
overview of our data generation strategy briefly the process goes in a sequential manner through the instance models within the valid sample and subjects these instance models to additional constraints that are generated on the fly.
these additional constraints which we refer toascorrective constraints provide cues to the solver as to how it should tweak an instance model so that the statistical representativeness of the whole data sample is improved.
for example let us assume that instance models represent households in a tax administration system.
now supposethat the proportion of households with no children is over represented in the sample.
if under such circumstances theiterative process is working on a household with no children a corrective constraint will be generated stating that the number of children should be non zero in that particular household .the solver will then attempt to satisfy this constraint withoutviolating any of the validity constraints discussed earlier.
if the solver fails to come up with a tweaked household that satisfies both the corrective constraint and all the validityconstraints at the same time the original household which isvalid but has no children is retained in the sample.
otherwise that is when a tweaked and valid household is found we needto decide whether it is advantageous to replace the originalhousehold by the tweaked one.
let ibe the original household and i primethe tweaked one.
further let sdenote the current sample containing i but not i prime and lets prime s i i prime .
the decision is made as follows if sis better aligned than s prime with the desired statistical characteristics then i primeis discarded otherwise i primewill replace iin the sample.
the reason why this decision is required is because tweaking may have side effects.therefore i primemay not necessarily improve overall representativeness although it does reduce the proportion of householdswith no children.
for example it could be that the solver addssome children to the household in question but in so italso changes the household allowances.
these allowances toomay be subject to representativeness requirements.
withoutthe comparison above one cannot tell whether the tweakedhousehold is a better fit for representativeness.
in the above scenario we illustrated the iterative process using a single corrective constraint.
in practice the processmay generate multiple such constraints since the data sampleat hand may be deviating from multiple representativenessrequirements.
we treat corrective constraints as being soft.
this means that if after the maximum number of iterations the solver manages to solve some of the corrective constraintsbut not all the process will give the tweaked instance model authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a chance to replace the original one as long as the tweaked instance model still satisfies all the validity constraints.
the rest of this section presents the technical machinery behind the customized ocl solver and our data generator.
a. solving ocl constraints a number of techniques exist for solving ocl constraints notably using alloy constraint programming and metaheuristic search .
alloy often fails to solve constraints that involve large numbers .
we observed via experience that this limitation could be detrimental inour context.
for example our case study in section v hasseveral constrained quantities e.g.
incomes and allowances that are large numbers.
as for constraint programming to ourknowledge the only publicly available tool is uml2csp .
we observed that this tool did not scale for our purposes.
in particular given a time budget of hours uml2csp did notproduce any valid instance model in our case study.
this is notpractical for statistical testing where we need a representativesample with many hundreds or more valid instance models.
search as we demonstrate in section v is more promising in our context.
although search based techniques cannot prove un satisfiability they are efficient at exploring large searchspaces.
in our work we adopt with two customizations thesearch based ocl solver of ali et al.
s hereafter referredto as the baseline solver.
the customizations are a feature for setting a specific instance model as the starting point forsearch and a strategy to avoid premature narrowing of thesearch space.
the former customization which is straightfor ward and not discussed here is necessary for realizing theprocess in fig.
.
the latter customization is discussed next.
the baseline solver has a fixed heuristic for selecting what ocl clause to solve next it favors clauses that are closerto being satisfied based on a fitness function.
for example assume that we want to satisfy constraint c1 defined as follows if x then y 5else if x then y 4else y 0endif endif where xandyare attributes.
for the sake of argument suppose the solver is processing a candidate solution where x satisfying the condition of the second nested if statement andy not satisfying any clause .
this makes the second nested if statement in c1 the closest to being satisfied.
at this point the heuristic employed by the solver narrows the searchspace by locking the value of xand starting to exclusively tweak yin order to satisfy y .
now if we happen to have another constraint c2 stating y the solver will fail since x can no longer be tweaked.
the above heuristic in the baseline solver poses no problem as long as the goal is to find some valid solution.
if search fails from one starting point the solver pseudo randomly picksanother and starts over.
in our context however starting overfrom an arbitrary point is not an option.
for the final datasample to have a chance of being aligned with the desiredstatistical characteristics the solver needs to use as startingpoint instance models from a statistically representative seed sample see fig.
.
if the solver fails at making valid aninstance model from the seed sample that instance model hasto be discarded.
this negatively affects performance since the solver will need to start all over on a replacement instancemodel supplied by the seed data generator as noted earlier.
in a similar vein if the solver fails at enforcing corrective constraints over a valid instance model it cannot helpwith improving representativeness.
to illustrate suppose thatconstraint c2 mentioned earlier is a corrective constraint and that the valid solution instance model for c1 i sx y .
in such a case the baseline solver will deterministically failas long as the starting point is this particular valid solution.in other words c2 will have no effect.
to address the above problem we customize the baseline solver as follows rather than working directly on the originalconstraints the customized solver works on the constraints prime implicants pi .
an implicant is prime minimal ifviolating any of its literals results in the violation of theunderlying constraint.
to derive all the pis for a given oclconstraint we first transform the constraint into a logicalexpression with only ands and ors negation and ocloperations.
we next transform this expression into disjunctivenormal form dnf by applying de morgan s law .
eachclause of the dnf expression is a pi.
for instance constraintc1 yields three pis x and y x and x and y and x and x and y .
note that we use the term pi slightly differently than what is standard in logic.
our literals are notnecessarily independent logically.
for example in the secondpi above x is redundant because x implies x .
such redundancies pose no problem and are ignored.
for each constraint cto be solved the customized solver randomly picks one of c s pis.
for instance if we want to solve constraints c1 and c2 together we would randomly pick one of c1 s three pis alongside c2 whose only pi is y .
this way we give a chance to all pis to be considered thus avoidingthe undesirable situation discussed earlier where the baselinesolver would deterministically lead itself into dead ends.
forexample from the pis of c1 we may pick x and y .n o w if we start the search at x y the solver will have a feasible path toward a valid solution x y which satisfies both c1 and c2.
if a certain combination of randomlyselected pis one pi per constraint fails other combinationsare tried until either a solution is found or the maximumnumber of iterations allowed is reached.
due to space we cannot present all the details of this customization.
we only make two remarks.
first all ocloperations are treated as opaque literals when building pis.for example the operation self.navigation forall x ory is a single literal just like say x .
solving ocl operations is a recursive process and similar to solving operation freeexpressions.
in particular to solve ocl operations we employthe same dnf transformation discussed earlier.
for example to solve self.navigation forall x ory we derive two pis x and y and use them to constrain the objects at the association end that has navigation as role name.
second the dnf transformation can result in exponentially large dnf representations when there are many literals .such exponential explosion is unlikely to arise in our context authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
manually written logical constraints for data models typically include only a handful of literals.
for the corrective constraintsthat are generated automatically through alg.
describedlater the number of literals is at most as many as thenumber of ranges or categories in the bar graphs that capturethe desired statistical distributions.
again these numbers areseldom very large.
in our case study of section v the dnftransformations took negligible time milliseconds .
to summarize using pis instead of the original constraints helps avoid dead ends when solution search has to start from a specific point in the search space.
in section v rq1 weexamine how customizing the baseline solver in the mannerdescribed in this section influences performance.
b. generating v alid and representative data this section presents the technical details of our data generation strategy depicted in fig.
and outlined earlier on.
we already discussed the creation of the seed sample from ourprevious work and how we make this sample valid usinga customized ocl solver section iv a .
below we focus onthe iterative process in fig.
i.e.
the region delineated bydashed lines and present the algorithms behind this process.
we start with some remarks about how we represent statistical distributions.
the instruments we use to this endare barcharts for categorical quantities and histograms for ordinal and interval quantities .
without loss of generality and while we support both notions we talk exclusively abouthistograms in the text.
a histogram is a set of bins.
each bin is defined by a label value or range and a relative frequency denoting the proportional abundance of the bin s label.
we do not directly handle continuous distributions e.g.
the normaldistribution.
continuous distributions are discretized into his tograms.
so is routine and not explained here.
wenote however that the discretization should not be too fine grained e.g.
resulting in more than bins.
this is becausethe corrective constraints in our approach will get complex in turn posing scalability issues for the ocl solver e.g.
with respect to the dnf transformation discussed in section iv a. the pim algorithm.
alg.
process instance model pim presents the procedure for one iteration of the iterative process region within the dashed boundary in fig.
.
pim takes thefollowing as input a valid data sample a specific instance model from the sample to process a set of validityconstraints the desired statistical characteristics defined as histograms a parameter specifying how many attempts thealgorithm should make to generate tweaked instance models and a parameter specifying how sensitive the algorithm isto differences in relative frequencies.
essentially if the difference between two relative frequencies is below the sensitivity parameter the two frequencies are considered equal.
the algorithm works in three stages as we describe next.
generate corrective constraints l. of alg.
in this stage pim calls another algorithm gcc alg.
describedlater .
gcc generates corrective constraints for the instance model being processed l. .
for example assume thatthe instance model is a pensioner and that pensioners arealg.
process instance model pim inputs a set sof valid instance models an instance model inst s to process a set vof validity constraints a set hdesired of desired statistical characteristics expressed as histograms a parameter nb attempts denoting the number of times that the solver will be invoked over inst to create tweaked instance models a parameter freq sensitivity denoting the margin beyond which two relative frequencies are deemed far apart.
freq sensitivity is used only for invoking gcc alg.
.
output either inst or a tweaked instance model whichever leads to a more representative data sample.
fun.
calls gcc generates corrective constraints alg.
solve invokes the customized solver see section iv a .
1cc gcc s inst hdesired freq sensitivity cc is the set of corrective constraints returned by alg.
.
2if cc then return inst 4t twill store potential replacements for inst.
5i iis the number of times the solver has been invoked so far .
6while i nb attempts do inst tweaked solve inst v c c i i if inst tweaked satisfies the constraints in v then 10t t inst tweaked 11inst best inst 12sbest s 13foreach candidate t do 14s prime s inst candidate if s primeis better aligned with hdesired thansbest then inst best candidate 17sbest s prime 18return inst best currently over represented in the data sample.
in response gcc will generate the following corrective constraint namedcc1 self.incomes forall not oclistypeof pension .
if gcc does not yield any corrective constraints then the original instance model will be retained in the sample l. .
build tweaked instance models l. of alg.
in this stage pim attempts to produce a set of tweaked instance models based on the corrective constraints generated previously.these constraints are fed to the solver alongside the validityconstraints l. .
to illustrate consider the example correctiveconstraint cc1 generated at the first stage.
this constraint instructs the solver to tweak the instance model at hand so that the income type will no longer be pension.
pim tries building tweaked instance models multiple times l. .
thisis intended at coming up with multiple candidates ideallymore than one for replacing the original instance model in the sample.
as noted earlier we treat corrective constraints assoft and try to satisfy them on a best effort basis.
therefore any tweaked instance model returned by the solver will be included in the set of candidate replacements as long as thevalidity constraints hold l. .
select best replacement l. of alg.
in this stage pim chooses to either retain the original instance model or replace it with one of the tweaked instance models computedin the second stage.
the criterion applied for the decision iswhich instance model once incorporated into the sample willresult in the most statistically representative sample.
the metric we use for measuring statistical representativeness is euclidean distance .
this metric measures authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
how far two histograms are from one another.
the closer the distance between two histograms is to zero the betteraligned the histograms are.
for example suppose that the datasample is composed of resident versus non residenttaxpayers.
as showed in the data schema excerpt in fig.
the desired distribution is resident versus nonresident.
the euclidean distance between the data sample and the desired distribution is .
indicating that the sample is not representative.
since pim needs to take into account several distributions simultaneously it uses the average of theeuclidean distances computed for all the histograms.
we next describe the gcc algorithm that pim calls on l. of alg.
for generating corrective constraints.
the gcc algorithm.
given an instance model inst within a data sample s alg .
titled generate corrective constraints gcc provides suggestions in the form of constraints as to how inst can be tweaked so that swill become a more representative sample.
the input to gcc was describedpreviously as part of pim s input.
gcc works in three stagesas explained below.
throughout the explanation we will bereferring to table i table ii and fig.
for illustration.
generate ocl literals l. of alg.
in this stage gcc groups histograms that annotate the same data schema element i.e.
class attribute or association as illustrated infig.
l. .
for each group sets oanduwill be built l. .
these two sets contain ocl literals for overrepresented and under represented histogram bins respectively.
these literals will later be assembled into intermediateconstraints see second stage below .
the literals in oanduare derived as follows we compare in a pairwise manner the relative frequencies of theactual characteristics of sagainst the desired characteristics inh desired l. .
to illustrate consider rows and of table i. the relative frequencies to compare are f1 against d1 f2against d2 and so on.
if for an index i fi di freq sensitivity l. the algorithm will generate a literal.
whether an exclusion or inclusion literal is generated depends on whether the underlying bin is over or under represented l. .
for example in table i thedifference between f1andd1is .
.
.
which is larger than the user provided freq sensitivity value on row of table i. since f1is over represented the following literal is added to oin order to exclude l1 taxpayer.allinstances select id forall not birth year and birth year .
note that the generated literal targets the specific instancemodel being processed since ultimately the literal is intendedat tweaking that particular instance model.
the case for under representation is dual and not illustrated.
combine literals l. of alg.
in the second stage the algorithm combines the literals in oanduinto what we call an intermediate constraint.
we use the term intermediate to distinguish the output of this stage from the final corrective constraint built in the next third stage of the algorithm described later.
in particular in the final corrective constraint we have to account for the fact that some histograms applyonly under certain conditions.
for example histogram h2onalg.
generate corrective constraints gcc inputs a set sof valid instance models an instance model inst s for which corrective constraints should be generated a set hdesired of desired statistical characteristics expressed as histograms a parameter freq sensitivity denoting the margin beyond which two relative frequencies are deemed far apart.
output a setcc of corrective constraints for inst.
fun.
calls includebin resp.
excludebin generates an ocl literal prescribing the inclusion resp.
exclusion of a specifichistogram bin.
1cc 2hcurrent statistical characteristics of s 3m h h current h mapsto m maps each histogram in hcurrent onto an intermediate constraint explained in the text .
all histograms are initially mapped onto an empty expression.
4p pwill store histograms from hcurrent which have been already processed.
5foreach h h current do if h p then continue we have already processed hand thus skip the loop.
letebe the data schema element to which hhas been attached letlbe the set of all histograms in hcurrent that annotate e foreach l l do 11p p l histogram lis marked as processed.
letoandube initially empty sets of ocl literals ustores literals generated for under represented bins ostores literals generated for over represented bins.
foreach relative frequency f ldo letdbe the relative frequency in hdesired corresponding to f if f d freq sensitivity then if f d then o o excludebin inst f elseu u includebin inst f if o negationslash oru negationslash then ocl intermediate parenleftbigg j o logicalandtext j 1oj j u logicalortext j 1uj parenrightbigg see fig.
.
21m m l mapsto ocl intermediate 22a a m m a negationslash ais the set of all histograms in m with a non empty intermediate constraint if a negationslash then if a andm single histogram in a is unconditional then ocl final m single histogram in a row of table ii else ocl else true ocl else will store the catch all else rule when all of a s histograms are conditional row of table ii foreach a a do condition a true condition awill store the ocl condition for histogram a s intermediate constraint.
if a is conditional then condition a condition of a ocl else ocl else condition a foreach b a a do now complete a s condition based on other histograms in a. if b is conditional then condition a condition a condition b ocl final ocl final condition a m a if all histograms in aare conditional then ocl final ocl final ocl else 40cc cc ocl final store ocl final incc.
41m h h current h mapsto resetm.
42returncc row of table i applies to pensioners only.
this detail is not captured by the literals in oandu.
the construction of the intermediate constraint is straightforward noting that we take the conjunction of the literals in o which prescribe exclusions and the disjunction of the literals inuwhich prescribe inclusions l. .
in fig.
we provide authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
t able i illustra tive example for alg.
construct value 1excerpt of the instance model to process.
id birth year 1986t1 residenttaxpayer i1 employment 2desired statisticalcharacteristics hdesired for simplicity welimit our illustration to the histogramsattached to thebirth year attribute of taxpayer in fig.
.the first histogram h1 attached to birth year bin labels l l2 l3 l4 relative frequencies d .
d2 .
d3 .
d4 .
condition true none the second histogram h2 attached to birth year bin labels l l6 relative frequencies d .
d6 .
condition self.incomes exists oclistypeof pension 3statistical characteristics ofthe current sample hcurrent computed on l. of alg.
.
hcurrent differs fromhdesired only in the relativefrequencies.histogram h1 primefor the sample differs from h1on row above only in relative frequencies relative frequencies for h1 prime f1 .
f2 .
f3 .
f4 histogram h2 primefor the sample differs from h2on row only in relative frequencies relative frequencies for h2 prime f5 .
f6 .
freq sensitivity.
.
taxpayer.allinstances select id forall not birth year and birth year and taxpayer.allinstances select id forall not birth year and birth year and taxpayer.allinstances select id forall birth year and birth year o ufrom from fig.
.
intermediate ocl constraint for distribution h1 primein table i an example of an intermediate constraint for histogram h1 prime shown on row of table i. generate final constraints l. of alg.
in the third and final stage the algorithm adds to the intermediate constraints conditions that describe under what circumstancesthese constraints apply l. and combines the intermediate constraints now complemented with conditions into corrective constraints l. and .
due to space we do not show the final corrective constraint for the exampleof table i. detailed exemplification of corrective constraints including the corrective constraint generated for the exampleof table i can be found in our supplementary material .
instead in table ii we show all possible scenarios for composing a corrective constraint from the set of histograms that annotate a given data schema element.
in the first scenario row of table ii there is no condition involved.
the correc tive constraint is thus the same as the intermediate constraintbuilt for the unconditional histogram l. of alg.
.
in the second scenario row of table ii the algorithm first complements with conditions the intermediate constraints of the conditional histograms.
the condition of one conditional histogram is naturally exclusive of the conditions of others l. .
this has been illustrated in the second column oftable ii.
the third scenario row of table ii is similar tothe second scenario.
the only difference is that since there is no unconditional histogram we need an extra clause to dealwith the situation where none of the conditional histogramst able ii scenarios for composing corrective constraints possible annotation scenarios for a data schema elementshape of the final corrective constraint 1the element is annotated only by one unconditional histogram u.uintermediate 2the element is annotated by one unconditional histogram u plus one or more conditional histograms ci.
the shape shown is for when there are two conditional histograms.uintermediate or c1condition and not c2condition andc1intermediate or notc1condition andc2condition and c2intermediate 3the element is annotated only by conditional histograms ci.
the shape shown is for when there aretwo conditional histograms.
c1condition and not c2condition and c1intermediate or not c1condition andc2condition andc2intermediate or notc1condition and not c2condition apply l. .
this catch all clause ensures that the final corrective constraint will not impact an instance model towhich none of the conditional histograms should apply.
v. e v alua tion in this section we empirically evaluate our synthetic data generator through a realistic case study.
a. research questions rqs our evaluation aims to answer the following rqs rq1 how does the customized ocl solver fare against the baseline ocl solver?
as discussed in section iv a we customize a baseline ocl solver .
rq1 compares the cus tomized solver against the baseline across two dimensions a execution time and b success rate i.e.
how often each solversucceeds in constructing a logically valid instance model.
rq2 does our synthetic data generator run in practical time?
statistical testing requires representative test data.
achieving representativeness often necessitates a large number of instance models to be built.
rq2 investigates whether our approach can construct a sufficiently large number of instancemodels within practical time.
rq3 can our approach generate data samples that are both valid and statistically representative?
rq3 investigates whether our approach yields data samples suitable for statistical testing.
since the approach enforces the validity constraintsof interest over all instance models data samples generated bythe approach always meet the validity requirement.
answeringrq3 therefore boils down to determining how well our data generator meets the representativeness requirement.
the experimental setup for answering these rqs is elaborated in section v d alongside our results and discussion.
b. implementation our data generator has been implemented in java using the eclipse modeling framework .
excluding comments and third party libraries ourdata generator is approximately 39k lines of code.
c. case study description our case study is motivated by an anticipated difficulty that acceptance testing of a public administration it system in luxembourg will pose once the development of the system iscompleted.
for this system many of the software development authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
t able iii comp arison against the baseline sol ver rq1 baseline solver customized solver execution time per instance model avg .
sec.
std dev .66avg .
sec.
std dev .
success rate calculated based on attempts and testing activities have been commissioned to third parties.
since the actual data that the system will manipulate issensitive and of a personal nature sharing the data with thirdparties poses complications.
further there are gaps in the actual data as well as structural mismatches between the dataschema used by the system under development and the dataschema in which the historical records have been archived.due to these issues our collaborating partners have concluded that the most practical way to ascertain reliability is through testing the system using synthetic test data.
the schema for the core data items manipulated by our case study system was developed with participation from subjectmatter experts at our collaborating partners.
the resultingschema expressed as a uml class diagram has classes enumerations associations generalizations and 344attributes.
the statistical characteristics of the data itemswere captured using histograms e.g.
for age and incometype conditional distributions e.g.
age distribution upon the condition that the individuals are pensioners and distributions of other types e.g.
uniform distribution for theday of the year on which individuals are born .
the validity constraints over the data are expressed using ocl invariants available in our supplementary material .of these target avoiding logical anomalies e.g.
chil dren being older than their parents .
of the remaining 42constraints are implied by the ranges upper and lowerbounds of the probabilistic annotations and the final aremultiplicity constraints from the data schema.
the constraints include nested if then else expressions occurrences of ocl quantifiers variable declarations