improving automatic source code summarization via deep reinforcement learning yao wan zhou zhao college of computer science and technology zhejiang university hangzhou china wanyao zhaozhou zju.edu.cnmin yang shenzhen institutes of advanced technology chinese academy of sciences china min.yang siat.ac.cnguandong xu advanced analytics institute university of technology sydney sydney australia guandong.xu uts.edu.au haochao ying jian wu college of computer science and technology zhejiang university hangzhou china wujian2000 zju.edu.cn haochaoying zju.edu.cnphilip s. yu university of illinois at chicago illinois usa institute for data science tsinghua university beijing china psyu uic.edu abstract code summarization provides a high level natural language descriptionofthefunctionperformedbycode asitcanbenefitthesoftware maintenance code categorization and retrieval.
to the best of our knowledge most state of the art approaches follow an encoder decoder framework which encodes the code into a hidden spaceandthendecodeitintonaturallanguagespace sufferingfrom two major drawbacks a their encoders only consider the sequentialcontentofcode ignoringthetreestructurewhichisalsocritical for the task of code summarization b their decoders are typically trainedtopredictthenextwordbymaximizingthelikelihoodofnext ground truth word with previous ground truth word given.
however itisexpectedtogeneratetheentiresequencefromscratchattesttime.thisdiscrepancycancausean exposure bias issue making the learnt decoder suboptimal.
in this paper we incorporatean abstract syntax tree structure as well as sequential content of codesnippetsintoadeepreinforcementlearningframework i.e.
actor criticnetwork .theactornetworkprovidestheconfidence ofpredictingthenextwordaccordingtocurrentstate.ontheother hand the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations.
we employ an advantage reward composed of bleu metric to train both networks.
comprehensive experiments on a real worlddatasetshowtheeffectivenessofourproposedmodel when compared with some state of the art methods.
ccs concepts softwareanditsengineering documentation computing methodologies natural language generation permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september montpellier france association for computing machinery.
acm isbn ... .
code summarization comment generation deep learning reinforcement learning acm reference format yao wan zhou zhao min yang guandong xu haochao ying jian wu andphilips.yu.
.improvingautomaticsourcecodesummarization viadeepreinforcementlearning.in proceedings of the 33rd acm ieee international conference on automated software engineering ase september montpellier france.
acm newyork ny usa 11pages.
introduction inthelifecycleofsoftwaredevelopment e.g.
implementation testing and maintenance nearly of effort is used for maintenance and much of this effort is spent on understanding the maintenance task and related software source codes .
thus documentation whichprovidesahighleveldescriptionofthetaskperformedby code is always a must for software maintenance.
even though varioustechniques havebeen developedto facilitatethe programmer duringtheimplementationandtestingofsoftware documenting codewith commentsremainsalabour intensive task makingfew real worldsoftwareprojectsadequatelydocumentthecodetoreducefuturemaintenancecosts .it snontrivialforanovice programmer to write good comments for source codes.
a good commentshouldatleasthasthefollowingcharacteristics a correctness.
the comments should correctly clarify the intent of code.
b fluency.
the comments should be fluent natural languages that can beeasily read and understoodby maintainers.
c consistency.
the comments should follow a standard style format for better code reading.
code summarization is a task that tries to comprehendcodeandautomaticallygeneratedescriptionsdirectlyfrom the source code.
the summarization of code can also be viewed as a form of document expansion.
successful code summarization can not only benefit the maintenance of source codes but also beusedtoimprovetheperformanceofcodesearchusingnatural language queries and code categorization .
motivation.
recent research has made inroads towards automatic generationofnaturallanguagedescriptionsofsoftware.asfaras authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france y. wan et al.
a an example of abstractive syntax tree ast .
b thelimitationofmaximumlikelihoodbasedtextgeneration.
figure1 anillustrationofthemotivationofourpaper.traditionalmethodssufferfromthefollowingtwolimitations a on representingthecode thestructureinformationofcodeisalwaysignored.b traditionalmaximumlikelihoodbasedmethodssuffer from the exposure bias issue.
weknow mostofexistingcodesummarizationmethodslearnthesemanticrepresentationofsourcecodesbasedonstatisticallanguage models andthengeneratecommentsbasedontemplates or rules .
with the development of deep learning some neural translation models have also been introduced for code summarization which mainly follow an encoder decoder framework.
they generally employ recurrent neural networks rnn e.g.
lstm toencodethecodesnippetsandutilizeanotherrnn to decode that hidden state to coherent sentences.
these models aretypicallytrainedtomaximizethelikelihoodofthenextword ontheassumptionthatpreviouswordsandground trutharegiven.
these models are limited from two aspects a the code sequential and structural information is not fully utilized on feature repre sentation which is critical for code understanding.
for example given two simple expressions f a b and f c d although they are quite different as two lexical sequences they share the same structure e.g.
abstractive syntax tree .
b these models also termed teacher forcing suffer from the exposure bias since in testing time the ground truth is missing and previously generated wordsfromthetrainedmodeldistributionareusedtopredictthe next word .
figure b presents a simple illustration of the discrepancyamongtrainingandtestingprocessintheseclassical encoder decoder models.
in the testing phase this exposure bias makes error accumulated and makes these models suboptimal not abletogeneratethosewordswhichareappropriatebutwithlow probability to be drawn in the training phase.contribution.
in this paper we aim to address these two mentioned issues.
to effectively capture the structural or syntactic informationofcodesnippets weemployabstractsyntaxtree ast adatastructurewidelyusedincompilers torepresentthestructure of program code.
figure 1ashows an example of python code snippetanditscorrespondingast.therootnodeisacomposite nodeoftype functiondef whiletheleafnodeswhicharetyped asnamearetokensofcodesnippets.it sworthmentioningthatthe tokensfromastparsingmaybedifferentfromthosefromword segmentation.
in our paper we consider both of them.
we parse thecodesnippetsintoasts andthenproposeanast basedlstm model to represent the structure of code.
we also use anotherlstmmodel torepresentthesequentialinformationofcode.
besides we apply a hybrid attention layer to fuse the structure representation and sequential representation of code on predicting theword consideringthe alignmentbetweenpredictedwordand source word.
to overcome the exposure bias we draw on the insights of deep reinforcement learning which integrates exploration and exploitation into a whole framework.
instead of learning a sequential recurrentmodeltogreedilylookforthenextcorrectword weutilizeanactornetworkandacriticnetworktojointlydeterminethenextbestwordateachtimestep.theactornetwork whichprovidesthe confidence of predicting the next word according to current state serves as a local guidance.
the critic network which evaluates the rewardvalueofallpossibleextensionsofthecurrentstate servesasaglobalguidance.ourframeworkisabletoincludethegoodwords that are with low probability to be drawn by using the actor network alone.
to learn these two networks more efficiently we start withpretraininganactornetworkusingstandardsupervisedlearningwithcrossentropyloss andpretrainingacriticnetworkwith meansquareloss.then weupdatetheactorandcriticnetworks accordingtotheadvantagerewardcomposedofbleumetricvia policy gradient.
we summarize our main contributions as follows.
weproposeamorecomprehensiverepresentationmethod for source code with one ast based lstm for the structure of source code and another lstm for the sequential content of source code.
furthermore a hybrid attention layer is applied to fuse these two representations.
to the best of our knowledge it is the first time that we proposeanadvanceddeepreinforcementlearningframework namedactor criticnetwork tocope withtheexposurebias issueexistinginmosttraditionalmaximumlikelihood based code summarization frameworks.
we validateourproposedmodelonareal worlddatasetof python code snippets.
comprehensive experiments show the effectiveness of the proposed model when compared with some state of the art methods.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
improving automatic source code summarization via deep reinforcement learning ase september montpellier france organization.
theremainderofthispaperisorganizedasfollows.
weprovidesomebackgroundknowledgeonneurallanguagemodel rnnencoder decodermodelandreinforcementlearninginsection 2for a better understanding of our proposed model.
we also formally define the problem in section .
section 3gives an overview ofourproposedframework.section 4presentsahybridembedding approach for code representation.
section 5shows our proposed deep reinforcement learning framework i.e.
actor critic network .
section6describes thedataset used inour experimentand shows theexperimentalresultsandanalysis.section 7showssomethreats to validity and limitations existing in our model.
section 8highlightssomeworksrelatedtothispaper.finally weconcludethis paper in section .
background as we declared before the code summarization task can be seen as a text generation task given the source code.
in this section we firstpresentsomebackgroundknowledgeontextgenerationwhich will be used in this paper including language model attentional rnn encoder decoder model and reinforcement learning for better decoding.
to start with we introduce some basic notations and terminologies.let x x1 x2 ... x x denoteasequenceofsource code snippet y y1 y2 ... y y denote a sequence of generated words where denotes the length of sequence.
let tdenote the maximum step of decoding in the encoder decoder framework.
we will often use notation ym...lto refer to subsequences of the form ym ... yl .d x1 y1 x2 y2 ... xn yn is the training dataset where nis the size of training set.
.
language model language model computes the probability of occurrence of a numberofwordsinaparticularsequence.theprobabilityofasequence oftwords y1 ... yt isdenotedas p y1 ... yt .sincethenumber of words coming before a word yi varies depending on its locationintheinputdocument p y1 ... yt isusuallyconditioned on a window of nprevious words rather than all previous words p y1 t i t productdisplay i 1p yi y1 i i t productdisplay i 1p yi yi n i .
thiskind ofn grams approachsuffers apparentlimitations .forexample then grammodelprobabilitiescannotbederived directly from the frequency counts because models derived this way have severe problems when confronted with some n grams that have not been explicitly seen before.
the neural language model is a language model based on neural networks.unlikethen grammodelwhichpredictsawordbased on a fixed number ofpredecessor words a neural language model can predict a word by predecessor words with longer distances.
figure2 a showsthebasicstructureofarnn.theneuralnetwork includes three layers that is an input layer which maps each word toavector arecurrenthiddenlayerwhichrecurrentlycomputes and updates a hidden state after reading each word and an output layerwhichestimatestheprobabilitiesofthefollowingwordgiven the current hidden state.
the rnn reads the words in the sentence one byone andpredicts thepossible following wordat eachtime step.
at step t it estimates the probability of the following wordy1 y2y3y4y5o1o2o3o4o5 o1 o2o3 o4o5 y1 y2y3 y4y5 figure rnn and tree rnn adapted from .
p yt y1 t by the following steps first the current word ytis mapped to a vector by the input layer e. then it generates the hidden state htat timetaccording to the previous hidden state ht 1and the current input yt ht f ht e yt .
here twocommonoptionsfor farelongshort termmemory lstm andthegatedrecurrentunit gru .finally the p yt y1 t is predicted according to the current hidden state ht p yt y1 t ht where isastochasticoutputlayer typicallyasoftmaxfordiscrete outputs that generates output tokens.
.
attentional rnn encoder decoder model rnn encoder decoder has two recurrent neural networks.
the encodertransformsthecodesnippet xintoasequenceofhidden states h1 h2 ... h x witharnn whilethedecoderusesanother rnn to generate one word yt 1at a time in the target space.
.
.
encoder.
asarnn theencoderhasahiddenstate which isafixed lengthvector.atthetimestep t theencodercomputes the hidden state htby ht f ht ct e xt .
here fis the hidden layer which has two main options i.e.
lstmandgru.thelastsymbolof xshouldbeanend of sequence eos symbolwhichnotifiestheencodertostopandoutputthe final hidden state ht which is used as a vector representation of x. .
.
decoder.
theoutputofthedecoderisthetargetsequence y y1 yt .oneinputofthedecoderisa start symbol denoting the beginning of the target sequence.
at the time step t the decoder computes the hidden state htand the conditional distribution of the next symbol yt 1by p yt yt ht ct authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france y. wan et al.
where isastochasticoutputlayerand ctisthedistinctcontext vector for yt computed by ct x summationdisplay j 1 t jhj where t jis the attention weight of ytonhj .
.
.
training goal.
the encoder and decoder networks are jointly trained to maximize the following objective max l max e x y dlogp y x where is the set of the model parameters.
we can see that this classical encoder decoder framework targets on maximizing the likelihood of ground truth word conditioned on previously generatedwords.aswehavementionedabove themaximumlikelihood based encoder decoder framework suffers the exposure bias issue.
motivatedbythis weintroducethereinforcementlearningtechnique for better decoding.
.
reinforcement learning for better decoding thereinforcementlearningisanapproachthatinteractswiththe real environment and learns the optimal policy from the reward signal.ittriestogeneratetextfromscratchwithoutgroundtruthin the testing phase.
under this approach the text generation process canbeviewedasamarkovdecisionprocess mdp s a p r .
inthemdpsetting state stattimestep tconsistsofthesourcecode snippets xand the words actions predicted until t y0 y1 ... yt.
theactionspaceisthedictionary ythatthewordsaredrawnfrom i.e.
yt y. with the definition of the state the state transition function pisst st yt where the action yt 1becomes a partofthenextstate st 1andthereward rt 1isreceived.
isthediscountfactor.theobjectiveofgenerationprocessistofind apolicythatmaximizestheexpectedrewardofgenerationsentence sampled from the model s policy max l max ex d y p x where is the parameter of policy needed to be learnt dis the trainingset yisthepredictedactions words and risthereward function.
our problem can be formulated as follows.
givenacodesnippet x x1 x2 ... x x ourgoalistofinda policythatgeneratesasequenceofwords y y1 y2 ... y y fromdictionaryywiththeobjectiveofmaximizingtheexpected reward.
tolearnthepolicy manyapproacheshavebeenproposed which aremainlycategorizedintotwoclasses .a thepolicy based approaches e.g.
reinforce which optimizes the policy directly via policy gradient.
b the value based approaches e.g.
q learning which learns the q function and in each time the agentselectstheactionwithhighestq value.ithasbeenverified thatthepolicy basedmethodsmaysufferfromavarianceissueand the value based methods may suffer from a bias issue .
thus in our paper we adopt the actor critic learning method which is a more advanced technique that has the advantage of both policy and value based methods.
figure an overall workflow of getting a trained model.
overview of proposed framework in this section we firstly have a simple overview on the workflowofhowtogetatrained modelforcodesummarization.then we present an overview of the network architecture of our proposed deep reinforcement learning based model.
figure 3shows the overall workflow of how to get a trained model.
it includesan offline training stage and an online summarization stage.
inthe training stage we prepare a large scale corpus of annotated code comment pairs.theannotatedpairsarethenfedintoour proposed deep reinforcement learning model for training.
aftertraining we can get a trained actor network.
then given a code snippet correspondingcommentcanbegeneratedbythe trained actor network.
figure4isanoverviewofthenetworkarchitectureofourproposed deep reinforcement learning based model.
the architecture of our model follows the actor critic framework which has beensuccessfullyadoptedinthedecision makingscenariossuch as alphago .
we split the framework into four submodules.
a hybridcoderepresentation cf.sec.
.thismoduleisusedto represent the source code into a hidden space which is also called encoderintheencoder decoderframework.
b hybridattention cf.sec.
.
.
.ondecodingtheencodedhiddenspaceintothecommentspace theattention layeris usedtoassign differentweights tothecodesnippettokensforbettergeneration.
c textgeneration cf.
sec.
.
.
.
this module is a rnn based generative network whichis usedto generatethe nextword basedonprevious generated words.
d critic cf.
sec.
.
.
this module is used to evaluate whether the generated word is good or not.
since the generated tokens on d can also been seen as actions wecanalsocalledtheprocess a b c asactornetwork.comparedwiththearchitectureoftraditionalencoder decoderframework our proposedmodelhasanadditionalcriticmoduleusedtoevaluatethe valueofactiontakenundercurrentstate.theprocess a b c d can alsobe calledas criticnetwork.
we cansee that the actor and critic networks share the modules a b c reducing the number of learning parameters a lot.
we will elaborate each component in this framework in the following sections.
hybrid representation of code different from previous methods that just utilize sequential tokens to represent code we also consider the structure information of source code.
in this section we present a hybrid embedding ap proach for code representation.
we apply an lstm to represent authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
improving automatic source code summarization via deep reinforcement learning ase september montpellier france .
.
.. .
.
code snippet a hybrid code representationhstr hstr2 hstr nhstrroot str i astdstr j tildewidest .
.
.
hybrid.
.
.
baseline reward b hybrid attention htxt 1htxt2htxt n txt idtxt j tildewidestst tildewiderst.
.
.
ytyt 1yt mlp v v tildewidestt... tildewidest tildewidestst tildewider tildewiderst.
.
.
ytyt 1yt c text generation d criticnotes actor network a b c critic network a b c d figure an overview of our proposed deep reinforcement learning framework for code summarization.
thelexicallevelofcode andanast based lstmtorepresentthe syntactic level of code.
.
lexical level thekeyinsightintolexicallevelrepresentationofsourcecodeis that comments are always extracted from the lexical of code such asthefunctionname variablenameandsoon.it sapparentthatwe apply an rnn e.g.
lstm to represent the sequential information of source code.
in our paper the lstm is adopted.
.
syntactic level inexecutingaprogram acompilerdecomposesaprogramintoconstituentsandproducesintermediatecodeaccordingtothesyntaxof the language.
ast is one type of intermediate code that represents the hierarchical syntactic structure of a program .
we represent thesyntacticlevelofsourcecodefromtheaspectofastembed ding.
similar to a traditional lstm unit we propose ast based lstm where the lstm unit also contains an input gate a memory cellandanoutputgate.differentfromastandardlstmunitwhich only has one forget gate for its previous unit an ast based lstm unitcontainsmultipleforgetgates.givenanast foranynode j let the hidden state and memory cell of its l th child be hjlandcjl respectively.
refer to the hidden state is updated as follows ij w i xj n summationdisplay l 1u i lhjl b i fjk w f xj n summationdisplay l 1u f klhjl b f oj w o xj n summationdisplay l 1u o lhjl b o uj tanh w u xj n summationdisplay l 1u u lhjl b u cj ij uj n summationdisplay l 1fjl cjl hj oj tanh cj wherek n.eachof ij fjk ojandujdenotesaninput gate a forget gate an output gate and a state for updating the memorycell respectively.
w andu areweightmatrices b isabiasvector and xjisthewordembeddingofthe j thnode.
isthelogisticfunction andtheoperator denoteselement wise multiplicationbetweenvectors.it sworthmentioningthatwhen thetreeissimplyachain namely n theast basedlstmunit reducestothestandardlstm.figure 2showsthestructureofrnn and tree rnn.
notice that the number of children nvaries for different nodes of different asts which may cause problem in parameter sharing.
forsimplification wetransformthegeneratedaststobinarytrees bythefollowingtwostepswhichhavebeenadoptedin a split nodeswithmorethan2children generateanewrightchildtogether withtheoldleftchildasitschildren andthenputallchildrenexcept theleftmostasthechildrenofthisnewnode.repeatthisoperation in a top down way until only nodes with children left b combine nodes with child with its child.
deep reinforcement learning for code summarization in this section we introduce the advanced deep learning frameworknamedactor criticnetwork whichhasbeensuccessfullyused in the alphago .
we introduce the actor and critic network respectively and then present how to train them simultaneously.
.
actor network afterobtainingtherepresentation ofcodesnippet weneedtodecodeitintocomment.herewedescribehowwegeneratecomment from the hidden space with a hybrid attention layer.
.
.
hybrid attention.
different parts of the code make different contributions to the final output of comment.
we adopt an attention mechanism which has been successfully used in neural machine translation.
in the attention layer we have two attentionscores one str t j forstructuralrepresentationandanother txt t j for sequential representation of code.
at t th step of thedecodingprocess theattentionscores str t j and txt t j are calculated as follows authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france y. wan et al.
str t j exp hstr j st summationtextn k 1exp hstrk st txt t j exp htxt j st summationtextnk 1exp htxt k st wherenisthenumberofcodetokens h j stistheinnerprojectof h jandst which is usedto directly calculate the similarity score between h jandst.
thet th context vector d tis calculated as the summarization vector weighted by t j dstr t n summationdisplay t 1 str t j hstr j dtxt t n summationdisplay t 1 txt t j htxt j. to integrate the structural context vector and the textual vector weconcatenatethem firstlyandthenfeedthem intoanone layer linear network dt wdt dstr t dtxtt bdt where dstr t dtxtt is the concatenation of dstr tanddtxtt.
the contextvectoristhenusedforthe t thwordpredictionbyputting an additional hidden layer tildewidest tildewidest tanh wc bd where is the concatenation of standdt.
.
.
text generation.
the model predicts the t th word by usingasoftmax function.let p denoteapolicy determinedby the actor network p yt st denote the probability distribution of generating t th wordyt we can get the following equation p yt st softmax ws tildewidest bs .
.
critic network unliketraditionalencoder decoderframeworkthatgeneratessequencedirectlyviamaximizinglikelihoodofnextwordgiventhe groundtruthword wedirectlyoptimizetheevaluationmetricssuch asbleu forcodesummarization.weapplyacriticnetworkto approximate the value of generated action at time step t. different fromtheactornetwork thiscriticnetworkoutputsasinglevalue instead of a probabilitydistribution on each decoding step.
before introducing critic network we introduce the value function.
given the policy sampled actions and reward function the value function v is defined as the prediction of total reward from thestate statsteptunderpolicy whichisformulatedasfollows v st est t yt t t t summationdisplay l 0rt l yt yt h wheretisthemaxstepofdecoding histherepresentationofcode snippet.forcodesummarization wecanonlyobtainanevaluation score bleu when the sequence generation process or episode is finished.theepisodeterminateswhenstepexceedsthemax step tor generating the end of sequence eos token.
therefore we define the reward as follows rt braceleftbigg0 t t bleu t to re o s. mathematically the critic network tries to minimize the following loss function where mean square error is used.
l bardblex bardblex bardblex bardblexv st v st bardblex bardblex bardblex bardblex2 wherev st is the target value v st is the value predicted by critic network and is the parameter of critic network.
.
model training we use the policy gradient method to optimize policy directly which is widely used in reinforcement learning.
for actor network the goalof training isto minimizethe negative expectedreward which can be defined as l ey1 ... t summationtextt l trt where is the parameter of actor network.
denote all the parameters as thetotallossofourmodelcanberepresentedas l l l .
forpolicygradient itistypicallybettertotrainanexpression of the following form according to l e t summationdisplay t 0a st yt log yt st wherea st yt is advantage function.
the reason why we chooseadvantagefunctionisthatitachievessmallervariancewhen compared with some other ones such as td residual and reward with baseline .
according tothe definitionofadvantage function we canformulate the advantage function as follows.
one can refer to for more details.
a st yt q st yt v st whereq st yt isthestate actionvaluefunctionwhichisdefined asq st yt est t yt t bracketleftbig summationtextt t l 0rt l bracketrightbig .fromthisformulation wecan findthattheadvantagefunctionmeasureswhetherornottheaction isbetterorworsethanthepolicy sdefaultbehavior.therefore a step in the policy gradient direction can increase the probability of better than average actions and decreasethe probability of worsethan average actions.
ontheotherhand thegradientofcriticnetworkiscalculated as follows l t summationdisplay t v st v st v st .
weemploystochasticgradientdescendwiththediagonalvariant of adagrad to optimize the parameters of our framework.
algorithm summarizes our proposed model described above.
experiments and analysis to evaluate our proposed approach in this section we conduct experiments to answer the following questions rq1.doesourproposedapproachimprovetheperformance of code summarization when compared with some state ofthe art approaches?
rq2.what s the effectiveness of each component for our proposed model?
for example what about the performance authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
improving automatic source code summarization via deep reinforcement learning ase september montpellier france algorithm actor critic training for code summarization.
initialize actor yt stand critic v st with random weights and pre train the actor to predict ground truth ytgiven y1 yt by minimizing eq.
pre train the critic to estimate v st with fixed actor fort tdo receivearandomexample andgeneratesequenceofactions y1 yt according to current policy calculate advantage estimate a according to eq.
update critic weights using the gradient in eq.
update actor weights using the gradient in eq.
.
of hybrid code representation and reinforcement learning respectively?
rq3.what stheperformanceofourproposedmodelonthe datasets with different code or comment length?
we ask rq1 to evaluate our deep reinforcement learning based model compared to some state of the art baselines.
we ask rq2 in order to evaluate each component of our model.
we ask rq3 to evaluateourmodelwhenvaryingthelengthofcodeorcomment.
in the following subsections we first describe the dataset some evaluation metrics and the training details.
then we introduce some baselines for rq1.
finally we report our results and analysis for the research questions.
.
dataset preparation we evaluate the performance of our proposed method using the datasetin whichisobtainedfromapopularopensourceprojects hosting platform github1.
the dataset contains codecommentpairs.thevocabularysizeofcodeandcommentis50 400and31 respectively.forcross validation weshufflethedatasetandusethefirst60 fortraining forvalidationandtheremaining for testing.
to construct the tree structure of code we parse python code into abstract syntax trees via ast2lib.
to convert code intosequentialtext wetokenizethecodeby .
!
space whichhasbeenusedin .wetokenizethecommentby space .
figure5shows the length distribution of code and comment on testingdata.fromfigure 5a wecanfindthatthelengthsofmost code snippets are located between to .
this verifies the quote in functionsshouldhardlyeverbe20lineslong .inpython language the limited length should be shorter.
from figure 5b w e cannoticethatthelengthofnearlyallthecommentsarebetween5to .
thisreveals thecomment sequence that we need to generate will not be too long.
.
evaluation metrics we evaluate the performance of our proposed model based on four widely used evaluation criteria in the area of neural machine translation and image captioning i.e.
bleu meteor rouge l and cider .
bleu measures the average ngramprecisiononasetofreferencesentences withapenaltyfor short sentences.
meteor is recall oriented and measures how code len gth025050075010001250150017502000count a code length distribution.
code len gth010002000300040005000600070008000count b commentlengthdistribution.
figure length distribution of testing data.
iteration0200400600800perplexity iteration010203040reward x50 x50 figure iteration of training perplexity and reward.
well our model captures content from the