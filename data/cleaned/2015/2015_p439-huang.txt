finding schedule sensitive branches jeff huang lawrence rauchwerger parasol laboratory texas a m university usa fje rwergerg cse.tamu.edu abstract this paper presents an automated precise technique tame for identifying schedule sensitive branches ssbs in concurrent programs i.e.
branches whose decision may vary depending on the actual scheduling of concurrent threads.
the technique consists of tracing events at ne grained level deriving the constraints for each branch and invoking an smt solver to nd possible ssb by trying to solve the negated branch condition.
to handle the infeasibly huge number of computations that would be generated by the ne grained tracing tame leverages concolic execution and implements several sound approximations to delimit the number of traces to analyse yet without sacri cing precision.
in addition tame implements a novel distributed trace partition approach distributing the analysis into smaller chunks.
evaluation on both popular benchmarks and real applications shows that tame is e ective in nding ssbs and has good scalability.
tame found a total of ssbs among which are related to concurrency errors and are ad hoc synchronizations.
categories and subject descriptors d. .
testing and debugging debugging aids tracing diagnostics keywords schedule sensitive branches symbolic constraint analysis .
introduction branch statements play a fundamental role in computer programs.
for sequential programs branches are either invariant i.e.
always true or false or input sensitive that their decision depends on the program input.
however branches in concurrent programs can have another dimension of sensitivity schedule sensitivity .
depending on the scheduling of threads the same branch instance in a concurrent program may vary in two runs with the same input.
t2t1if is good useprotocola else useprotocolb if speed is good true else is good false if referencedcolumnmap null ... else if referencedcolumnmap.isset ... ... referencedcolumnmap null t2t1figure an example of schedule sensitive branch t2t1if is good useprotocola else useprotocolb if speed is good true else is good false if referencedcolumnmap null ... else if referencedcolumnmap.isset ... ... referencedcolumnmap null t2t1 figure a real schedule sensitive branch in derby to understand this phenomenon consider an example in figure .
thread t1uses di erent protocols to transfer les over the network depending on the current network speed while thread t2monitors the network speed periodically and updates the shared variable is good which indicates the current network condition.
the branch if is good executed by t1is schedule sensitive since its choice also depends on the schedule.
although the branch schedule sensitivity seems intuitive it is often unintended by the programmer and frequently the result of programming errors.
figure shows a real bug in apache derby .
thread t1 rst checks if the referencedcolumnmap isnull.
if not null t1will proceed to dereference it.
however t2may be concurrently running and set referencedcolumnmap tonull.
the branch statement if referencedcolumnmap!
null is schedulesensitive but it is not expected because if t2executes immediately after t1executes this branch and before it dereferences referencedcolumnmap it will cause nullpointerexception upon the dereference.
in our study of a large collection of popular multithreaded benchmarks and real programs which we will show in section we nd that schedule sensitive branches are a strong indicator of concurrency errors out of of the schedule sensitive branches we nd in our experiments are resulted from concurrency bugs.
we anticipate that e ectively nding schedule sensitive branches ssbs is useful with at least two applications program understanding if ssbs are not intended by the permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august september bergamo italy c acm.
... .
439programmer then they represent good starting points for better understanding the program behavior detecting and localizing potential concurrency errors it is likely that assb is caused by bugs.
in this paper we present an automated technique called tame to precisely identify ssbs in concurrent programs.
tame consists of three steps .
observes a ne grained program execution trace .
constructs symbolic constraints for each branch in the trace and .
uses an smt solver to nd ssbs by solving the negated branch condition conjuncted with the symbolic constraints.
tame is able to identify all the ssbs that can be inferred from the observed execution trace.
moreover for each identi ed ssb it generates a corresponding schedule that can enforce the program to execute a di erent branch choice.
this feature also allows tame to e ectively test concurrent programs as a di erent branch choice may manifest unexpected behaviors such as the runtime exceptions in figure .
a main challenge is how to scale to real programs that produce huge traces.
tame leverages concolic execution to delimit the traces to analyse.
we further propose a distributed trace partition approach that scopes our analysis to a selected range of schedules such that the corresponding constraints can be solved within a reasonable time.
although this approach may result in missing certain ssbs it does not a ect the premise of precision i.e.
every identi ed ssb is truly schedule sensitive.
more importantly it achieves a much better balance between analysis e ciency and e ectiveness compared to a simple window based trace segmentation approach .
our contributions are summarized below we present a technique to precisely identify schedulesensitive branches in concurrent programs based on symbolic constraint analysis and concolic execution.
we present a distributed trace partition approach that scales our technique to real world programs with long running executions.
we evaluate our technique using both popular benchmarks and real programs and show that it is able to analyze real world program executions containing tens of millions of instructions in a minute.
we report for the rst time schedule sensitive branches found in popular benchmarks and real programs.
among them are caused by concurrency bugs and are related to ad hoc synchronizations.
.
overview in this section we start by illustrating our approach using an arti cial example.
we then identify the technical challenges and outline how we address them.
figure top shows an overview of our approach consisting of three components a tracer a constraint builder and an smt solver.
the tracer monitors the execution of a program and produces a trace of ne grained events.
the constraint builder takes the trace as input and constructs a set of constraints for each branch event and invokes thesmt solver to determine if a branch is schedule sensitive.
consider an example in figure a .
the program contains two threads t1and t2 accessing two shared variables x and y and three branches marked as .
branch at line is only input sensitive because thread t2is only started after line .
for branches and at lines and respectively their schedule sensitivity is much harder to see because in addition to the branch conditions we also need to reason about thread schedules.
suppose we run this program once with input x y following a schedule denoted by the line numbers we will observe 13critical events i.e.
shared data reads writes and thread synchronizations in the execution.
note that lines and are not executed in this schedule because their branch conditions are not satis ed and line generates two critical events a read on xand a read on y .
in our constraint model we give each of these events an order variable and each read access a symbolic value variable.
we construct for each branch event a group of constraints over these variables.
letoidenote the order variable of the event at line i x0and y0the input value of xandy andxiandyithe symbolic value ofxandy respectively at line i. to avoid clutter we useo2andy2to denote the order variable and symbolic value variable respectively for the read to yat line .
figure b shows the constructed constraints for the three branches.
there are two types of constraints.
the rst type is simple ordering constraints between critical events to ensure sequential consistency1.
for example o1 o 2means that line must happen before line .
o4 o 9because t2 is forked at line so its rst event should happen after the fork event ando5 o 14 o8 o 9because the two lock regions cannot overlap.
the second type is the constraints corresponding to the branch conditions.
we perform dynamic symbolic execution to compute the path condition for each branch event and negate its corresponding branch condition.
if the negated branch condition can be satis ed with any one valid schedule then we know the branch is schedule sensitive.
for shared data reads that propagate to the branch conditions we use symbolic variables to denote their values because they may read a value written by the same thread or by a remote write from a di erent thread.
therefore the negated constraints for the three branches are written asx1 y2 x1 y2 x3 y6 and x10 y11 respectively.
we then match each read with a valid write following the semantics of sequential consistency a read returns the value written by the most recent write.
for example y11may either match with y0ory7.
if it matches with y0 then either line happens after line or branch or is false.
otherwise if it matches with y7 branches and must be true and line should happen before line .
therefore we write the constraint as y11 y0 o11 o 7 x1 y 2 x3 y6 y11 y7 o7 o x1 y2 x3 y .
the constraints for the other symbolic reads can be constructed in a similar way.
putting all constraints together we invoke an o the shelf smt solver such as z3 or yices to solve them.
for branches and their corresponding constraints cannot be satis ed hence we cannot determine if they are schedulesensitive or not based on the observed trace.
for branch 1for a focused presentation we only discuss sequential consistency in this paper.
nevertheless our technique is generalizable to relax memory models.
440t1t2input x yif x y return a x fork t2 lock l if a y y a unlock l lock l b x if b y y b x unlock l 123o1 1o2 2o3 3o4 4o9 5o10 6o11 x1 0y2 0x3 0x10 y11 tracer constraint builder smt solverprogramschedule sensitive branches 3 x1 y2x1 x0 o1 o13 x1 x13 o13 o1 x3 x0 o3 o13 x3 x13 o13 o3 x10 x0o1 o2 ... o8o9 o10 ... o14o4 o9o5 o14 o8 o9y2 y0y6 y0y11 y0 o11 o7 x3 y6 y11 y7 o7 o11 x3 y6 1x0 y0 0x1 y2 x3 y623x10 y11 a b c figure technical overview of our approach.
branches are not schedule sensitive while branch is.
the solver returns a solution shown in figure c which corresponds to a schedule that can enforce branch to take the else branch upon re execution of the program.
we can then tell that branch is schedule sensitive.
note that the solution returned by a solver may not be unique indicating that there are multiple schedules that can enforce a branch to take a di erent choice.
however regardless of which one is returned as long as there exists any solution the considered branch is schedule sensitive.
.
practical challenges although the overall ow of our approach is easy to follow there are several tough challenges that we must tackle before the approach can be applied to realistic programs.
we next discuss these challenges and describe our solutions.
.
fine grained tracing to obtain the branch conditions we need to trace not only those critical events but also thread local computations on the stack.
though thread local computations do not directly introduce scheduling non determinism they could indirectly transfer the e ect of non deterministic data ow to branches.
however the number of thread local computations is usually much larger than e.g.
several orders of magnitude of that of critical events because real programs often use standard libraries which encapsulate complex thread local functions.
in addition it is not always possible to trace every computation.
many programs contain native code or calls to external libraries of which the source code is not available or hard to instrument.
furthermore the runtime overhead including both memory and time for tracing all computations can be prohibitive which makes any tracing technique di cult to scale to long running programs.
consider a simple but almost full2 java program in figure .
the program starts two threads to compare two different implementations of the sha hash algorithm on an input string.
thread t1uses the hashing class from google 2only the main method and the thread creating statements are ignored to simplify the presentation.
checkhash string hash string hashed map.get input if hashed null map.put input hash else if !hashed.equals hash error string input esec fse2015 thread t1 string hash1 digestutils.sha256hex input checkhash hash1 if t2.isalive system.out.println done thread t2 string hash2 hashing.sha256 .hashstring input charsets.utf 8 .tostring checkhash hash2 hashmap map new hashmap .
.
.
.
.
.
.
.
.
.
.
.
.
.figure example for ne grained tracing guava3 and thread t2uses the digestutils class from apache commons codec4.
both t1and t2call the method checkhash to check the computed hash string.
in checkhash a shared variable mapis used to store and retrieve the hashed value.
there are two ifbranches at lines and respectively.
each thread rst checks at branch line if a hash of the input string computed by the other algorithm exists or not.
if not the input hash will be stored into the map.
otherwise the two hash strings will be compared at branch line and if they are di erent an error will be thrown at line .
regardless of the implementation correctness of the two hash functions as long as they are both 441checkhash string hash string hashed map.get input if hashed null map.put input hash else if !hashed.equals hash error string input esec fse2015 thread t1 string hash1 digestutils.sha256hex input checkhash hash1 if !t2.isalive system.out.println done thread t2 string hash2 hashing.sha256 .hashstring input charsets.utf 8 .tostring checkhash hash2 hashmap map new hashmap .
.
.
.
.
.
.
.
.
.
.
.
.
.
x new x y if x.f y dosomething .
.figure example for missing computations deterministic the branch at line is not schedule sensitive.
however the branch at line is schedule sensitive because the thread which comes rst will nd that hashed isnull and take the true branch and the other thread will nd hashed notnull and take the false branch.
to illustrate the issue brought by native code we also add a branch statement at line which checks if t2is still alive by calling thread.isalive and prints out a message if not.
this branch is schedule sensitive because the start and termination of t2depends on the schedule.
although the program contains only less than lines of code a complete trace of its execution contains more than 390k computations if we trace every bytecode instruction even after excluding the jdk libraries java.lang.
and java.util.
.
the reason is that the library calls digestutils.sha256hex andhashing.sha256 .hashstring involve a myriad of subclasses and other libraries.
moreover the runtime overhead for tracing all these computations is signi cant for such a tiny program.
without tracing this program nishes in 20ms whereas with tracing it takes more than seconds 100x slower to execute and generates a log of more than 100kb.
we can imagine how serious this issue will be for large programs.
furthermore it is di cult to trace the function call thread.isalive because the code it executes is written in a di erent language de ned by java native interface and is not directly observable by the jvm.
to address the above issues we propose to exclude tracing certain classes which can be de ned by the user and matched with regular expressions.
for example the user can specify exclude java.
com.google.
org.apache.
in the command line to instruct the instrumentation tool to not trace classes in these packages.
this is a standard step used also in many other analysis frameworks .
it reduces both the trace size and runtime overhead and also avoids the problem of tracing native code used in those excluded classes.
for instance after excluding the libraries classes in java.
com.google.
org.apache.
the simple program in figure only generates events and the tracing takes only 100ms.
however this approach raises a new problem the computation information in the excluded classes is missed.
because it is unknown what is computed in the missing code it is hard to obtain the branch condition if it is related to the missing computation.
for example in the code in figure suppose the class xis excluded and yis a thread shared variable.
at line because the value of the eld x.fis unknown it is impossible to compute the branch condition and decide its schedule sensitiveness.
to tackle this problem we propose to insert additional value tracking instrumentation to log the return value of method calls and every data read instruction including the load operations on both heap and thread local variables .
the value information will help to recover the e ect of missing computations in a manner of under approximation given the same input value the untracked code always produces the same output value.
for example in figure we may 5in fact we could not trace classes in these libraries due to limitations of the tracing tool we use asm and java agent .log at line that the value of yis and the constructor ofxreturns void and at line the value of x.fis and yis .
when computing the branch condition we can use the logged value to under approximate x.f i.e.
to conservatively assume that x y always takes y and returns x.f .
in this way despite that x y is excluded the obtained branch conditions are sound though it may limit the detection of ssb as additional constraints are introduced .
.
runtime exception handling when a runtime exception either caught or uncaught occurs the program control ow will be transferred to a di erent point speci ed by the programmer.
this causes two problems in constructing the branch conditions.
first the branch condition must consider the exception condition i.e.
the condition for the exception to occur .
if the exception condition is not met the branch may not even be executed.
for example in the code below if xis a divide by zero exception will occur and hence the branch at line will not be executed.
checkhash string hash string hashed map.get input if hashed null map.put input hash else if !hashed.equals hash error string input esec fse2015 thread t1 string hash1 digestutils.sha256hex input checkhash hash1 if !t2.isalive system.out.println done thread t2 string hash2 hashing.sha256 .hashstring input charsets.utf 8 .tostring checkhash hash2 hashmap map new hashmap .
.
.
.
.
.
.
.
.
.
.
.
.
.
x new x y if x.f y dosomething .
.
r x if r dosomething .
.
second exceptions clutter the thread stack frame which if not properly handled can fail symbolic execution.
consider an example in figure .
the method mwhich returns the quotient of divided by an input integer iis called at line with input .
the divide by zero exception is thrown out of mand caught by the try catch block at line .
as we can see from the trace shown in the gure right there is no indication that an exception has occurred in method m and the stack frame of mis exited.
when symbolically executing the instruction astore store a reference at the top of the stack into a local variable at index the current stack is still in method m but there is no reference data in the stack!
and when symbolically executing the instruction iload load an int value from a local variable at index the value which is the input value to m will be loaded from the stack frame of m which is wrong.
the correct value is which is stored to iat line in the stack frame of the caller of m. checkhash string hash string hashed map.get input if hashed null map.put input hash else if !hashed.equals hash error string input esec fse2015 thread t1 string hash1 digestutils.sha256hex input checkhash hash1 if !t2.isalive system.out.println done thread t2 string hash2 hashing.sha256 .hashstring input charsets.utf 8 .tostring checkhash hash2 hashmap map new hashmap .
.
.
.
.
.
.
.
.
.
.
.
.
.
x new x y if x.f y dosomething .
.
r x if r dosomething .
.int i try i m catch exception e int j i static int m int i return i .
.
.
.
.
.
.
.iconst 1istore 0iconst 0invokestatic m i i bipush 100iload 0idivastore 1iload 0istore 1invoke exception figure example for handling exceptions to address these issues we propose to add additional events in the trace to recognize the occurrence of runtime exceptions.
speci cally for every method invoke statement we insert a new statement invoke end after it and enclose them within a try catch block.
if the method invocation returns normally an invoke end event will be logged.
otherwise if an exception is thrown from the invoked method we log in the catch block an invoke exception event and 442re throw the exception.
when performing symbolic execution on the trace upon an invoke end event we pop up the stack frame of the invoked method and push its return value onto the new frame upon invoke exception because there is no return value in the invoked method we pop up its stack frame and push a placeholder object onto the new frame to denote the exception object.
in this way the astore instruction after the exception can be correctly executed.
in addition we propose to capture exception conditions as additional branch constraints.
not all exception conditions can be captured such as jvm internal errors .
we currently handle two kinds of runtime exceptions divide by zero and array out of bounds corresponding to bytecodes idiv ldiv irem lrem dividing an integer or long value and aload where a b c d f i l s representing eight di erent types of array accesses respectively.
for each instruction in idiv ldiv irem lrem we mark the divisor as symbolic say d and create a branch constraintd6 .
for aload we mark the array index value as symbolic say i and keep track of the array size say s and create a branch constraint i s .
.
data induced control flow besides explicit branches such as if while switch etc data ow that involves dereferencing a memory location can also implicitly a ect control ow.
figure illustrates the cases for object dereferencing and array indexing.
at line thread t1calls o.m where depending on the schedule o may reference the object c1 created at line c2 created at line or null set by thread t2.
hence o.m may execute a di erent method mor even throw a null pointer dereference exception.
similarly the array access a byt1at line may write to either a ora which makes the branch choice at line non deterministic.
therefore to construct a sound branch condition the data induced control ow must be considered.
x a x if a dosomething t1t2o new c1 o.m o new c2 o null .
.
.
.
.
.
.
.
figure example for data induced control ow to capture data induced control ow we introduce additional branch events for object dereferencing and array indexing operations.
speci cally for shared non primitive object dereferences o. we introduce a symbolic variable so denoting the symbolic value of o and add a branch conditions addr o whereaddr o is the runtime address of o. for array reads and writes a we introduce a symbolic variablesi denoting the value of the index i and add a branch condition si viwhereviis the runtime value of i. in this way we not only ensure that the path conditions are captured but can also nd schedule sensitive object dereferences and array accesses.
.
precise shared data identification in constructing the constraints for matching reads and writes we can lter out those thread local or immutable data accesses because their mapping is xed.
this also reduces the size of constraints and speeds up the solver.
how ever a caveat is that the address of every data access must be precisely identi ed.
we must not miss any shared data access or match a read with a write on di erent addresses.
otherwise the analysis result might be wrong.
consider the code in figure .
at line the eld o2.x is written to by thread t2.
if o2.x is mis identi ed as o.x then the branch if o.x at line will be miss classi ed as schedule sensitive.
similarly if the write access o.x at line is mis identi ed as a thread local access or on a different address other than o.x the branch at line will be miss classi ed because then the read of o.xat line will be able to match with the write o.x at line .
x a x if a dosomething t1t2o new c1 o.m o new c2 o null .
.
.
.
.
.
.
.t1t2o.x synchronized o if o.x dosomething synchronized o o.x o.x o2.x .
.
.
.
.
.
.
.
.
.
figure example for precise data identi cation to precisely identify shared data accesses we represent the address of heap access as follows.
for array access a we represent its address as addr a .i whereaddr a is the runtime address of the array object a. for eld access o.x we represent as addr o .fid x whereaddr o is the runtime address of oandfid x is the eld id of x a unique integer to the class of o .
for static eld accesses since ois null we setaddr o to .
.
scalable constraint solving there are two challenges associated with constraint solving in this problem complex branch constraints such as non linear real arithmetics long execution traces which generate large number of constraints.
for advancements in theorem provers and decision procedures would be needed to e ciently solve such constraints which is not the focus of this paper.
a practical workaround we employ is to under approximate the behavior of complex computations with the concrete runtime value.
for example for a branch if x2 y because the solver does not support non linear arithmeticx2 we repace x2 y 1by a conjunction of x vx andv2 x y wherevxis the recorded concrete value of x. for it a ects the scalability of our technique.
although high performance smt solvers such as z3 and yices are becoming increasingly powerful in theory they can only solve a limited number of constraints within a limited time.
to improve the scalability of our technique we propose to partition the input trace into smaller chunks such that the constraints generated for each chunk has a reasonable size.
however a strategy proposed in previous race detection work that simply segments the trace into windows of consecutive events each containing nevents will not work well because di erent from the trace for race detection which contains only critical events our trace here contains many more ne grained events.
if nis not large enough most events in a window will be from the same thread which has little space for schedule exploration.
on the other hand if nis too large the corresponding constraints will be too large to solve e ciently.
distributed trace partition .
to achieve a good balance between analysis e ectiveness and e ciency we de443e1e2e3e4e5e6e7e8e9t1t1t1t2t2t3t3t3t3e1e2e3e4e5e6e7e8e9 a b c a trace b simple window based partition c distributed partitione1e4e7e1e4e8e1e4e9e1e4e6e1e5e7e1e5e8e1e5e9e1e5e6e2e4e7e2e4e8e2e4e9e2e4e6e2e5e7e2e5e8e2e5e9e2e5e6e3e4e7e3e4e8e3e4e9e3e4e6e3e5e7e3e5e8e3e5e9e3e5e6figure illustration for trace partition velop a distributed trace partition algorithm that segments the local trace of each individual thread into consecutive windows rather than cutting a global trace and combines windows from di erent threads to form a chunk.
speci cally for a trace let idenote the events by thread ti and i j thejth window in i each containing nevents.
a chunk is a union of i j for alliwith anyj.
there are in totalqk i 1michunks where kis the number of threads andmithe number of windows in the trace of thread ti.
for example as illustrated in figure c suppose there are three threads k and each window contains one event then there are chunks in total.
because each chunk contains an event from each of the three threads the space of possible thread schedules formed by these three events is much larger than that produced by a global window of three consecutive events as shown in figure b .
the limitation of this approach is that some ssbs may still be missed because schedules among events from di erent chunks are not explored.
nevertheless it does not a ect the precision a branch that is determined to be schedule sensitive is truly schedule sensitive.
the trace can be partitioned either o ine or online.
for long running executions because the full trace can be even too large to store online partition is preferable.
we skip the events by the main thread until the rst child thread is created because none of these events is schedule sensitive.
one additional issue is that the initial state for each chunk is unknown which can break symbolic execution.
when a full trace is available we can store the nal state of the current chunk as the initial state of the next one.
however this approach does not work when only one chunk of events is logged online.
fortunately recall section .
that the value of every data read and method return is tracked.
similar to the treatment of untracked computations we are able to use the logged runtime value to recover the initial states.
.
algorithm our algorithm is summarized in algorithm .
for each chunk of events we rst perform a linear scan to nd all the critical events including shared heap accesses and synchronizations and to symbolically compute the branch conditions for each branch event.
then for each branch event we construct a set of constraints and invoke the solver.
if the solver returns a solution we report that the corresponding branch event is schedule sensitive.algorithm findschedulesensitivebranch input a trace of events data structure c containing only critical events m a map from branch events to branch conditions.
c extractcriticalevents m computebranchconditions forb2m keyset do b extractnegatedpathcondition m b c b getrelevantevents c b c b constructconsistencyconstraints c b if satis able b c b then reportbis schedule sensitive .
constraint construction.
recall section that the constraints consist of two parts i b the path condition for the branch event bconjuncted with its negated branch condition.
ii c b the consistency constraints among critical events.
for i the path condition of bis a conjunction of all preceding branch conditions by the same thread.
the only unknown variables in bare the symbolic value variables introduced for each read access to shared heap locations.
for ii the consistency constraints are similar to the thread causality constraints developed in previous work except that the value of reads is not constrained because the control ow consistency is already captured by the branch conditions.
speci cally c bconsists of the conjunction of three types of constraints mhb lock rw where mhb denotes the must happen before constraints lockthe lockmutual exclusion constraints and rwthe read write constraints over read and write events.
for space reasons we refer the readers to gpredict for detailed description of mhband lock.
we next describe rw which is di erent from previous work.
read write constraints rw .consider the read and write events on the same shared data.
for a read it may read the value written by a write by the same or a di erent thread depending on the order relation between the write s. consider a readr and letwdenote the set of write s on the same location as that of r andvrthe value returned by r. rwis written as 8wi2w vr wi owi o r 8wj6 wiowj o wi owj o r the constraint above states that if a read is mapped to awrite for this write its order is smaller than that of the read and there is no other write that is between them.
we group all the read s and write s by the accessed memory address and encode rwfor each read .
constraint complexity .
letnrandnwdenote the number of read s and write s on a certain shared address the size of rwis 4nrn2 w which is cubic in the number of read write events in c b. optimizations .
in practice the size of rwcan be significantly reduced by taking the must happen before relation into consideration.
consider two write eventsw1andw2 andw1 w2 r. we can ignore w1because it is impossible forrto read the value written by w1.
another optimization is that we do not need to repeatedly check for branch events corresponding to the same branch location.
once a branch 444location is determined to be schedule sensitive we can skip all th rest branch events on it.
.
implementation tame is implemented based on asm and z3 and works for java programs.
the architecture is inspired by catg a java concolic unit testing engine.
we extend catg to handle large real concurrent programs.
tame currently traces at the java bytecode level and supports all opcodes in java excepts invokedynamic opcode including array eld accesses loads stores to local variables method invocations stack operations etc.
for di erent types of instructions we log di erent runtime data such as the thread id and the value of loads and stores.
for debugging purpose for all instructions we also maintain a map from a unique instruction id to its location class and line number .
we next describe the instructions related to critical events heap accesses all the eld and array loads and stores.
for eld accesses there are four di erent instructions getstatic putstatic getfield and putfield .
for each eld access instruction we log its intruction id class id eld id read write value and address of the object if not static.
for array access there are di erent instructions astore and aload where denotes eight di erent data types including the refererence type and seven primitive types.
for each array access instruction we log its intruction id address of the array object and index value.
thread synchronizations thread start join wait notify and lock unlock events corresponding to monitorenter monitorexit instructions .
for synchronized methods as there is no corresponding monitorenter monitorexit bytecode instruction we log at the beginning and every return instruction of the method to indicate lock unlock events.
for each synchronization instruction we log the id of the participating threads and address of the lock object.
in addition to events corresponding to the bytecode instructions of the original program execution the trace also contains the inserted new events for handling runtime exceptions and data induced control ow as discussed in section including invoke end and invoke exception to recognize runtime exceptions and two special events branch special and mark symbolic .
we insert branch special after every branch instruction to indicate if the true branch is taken which is used to build the correct branch conditions.
we insert mark symbolic before every load access to shared heap locations.
this is done by pre processing the trace before constructing the constraints.
we mark a heap load access as symbolic if in the logged trace or chunk of events there exists at least one heap store to the same address and by a di erent thread.
when performing symbolic execution for each mark symbolic event we introduce a new symbolic variable to represent the value returned by the load access.
.
ev aluation we have applied tame on a variety of popular multithreaded benchmarks and several real world large complex programs shown in tables and .
all these programs were collected from recent concurrency studies with the total size close to 1mb.
the main goal of our evaluation is to answer two research questions .
how e ective and e cient is our technique for nding schedule sensitive branches?
.
how scalable is our technique when applied on real programs with long running executions?
for the rst question we ran tame on a collection of benchmarks that produce traces with a relatively small size after excluding the events in the jdk libraries.
we set the bound to 100k events and 1k critical events to make sure that tame can nish within a reasonable time.
for the second question we ran tame on a collection of large multithreaded applications including jigsaw .
.
derby10.
.
.
h2 .
.
ftpserver cache4j log4j hedc weblech pool as well as several long running benchmarks.
because the traces of some of these programs contain tens of millions of events it is challenging to even store and load the whole trace.
we hence performed the trace partition strategies online as explained in section .
to log only one chunk of events in each run.
this turned out to work well in practice because there are often many redundant events across chunks that a single chunk can often reveal much information about the whole trace.
all experiments were conducted on an processor core .6ghz intel i7 linux with 8gb memory and jdk .
.
we set the java heap space to 8gb and z3 timeout to one minute.
all data were averaged over three runs.
overall results .tame is e ective for nding ssbs and has good scalability to real world programs and long running executions when our distributed trace partition approach is applied.
for the smaller benchmarks tame was able to analyze all the traces in less than two minutes and found a total of ssbs with of them related to concurrency errors.
for most real programs and larger benchmarks tame could not nish analyzing the full trace in a reasonable time.
with our distributed partition approach however it was able to analyze all these programs in around ten minutes and found a total of ssbs in which are related to concurrency errors.
we present these ssbs in section .
.
.
effectiveness and efficiency table summarizes the results on the smaller benchmarks.
columns report the trace characteristics the numbers of threads events critical events and branch events .
the branch events include both the explicit branching events and those abstracted from the exception conditions and datainduced control ow.
column reports the online tracing time for each benchmark.
column reports the average size of the constructed constraints for the branch events and column the total o ine constraint analysis time including both the constraint construction and solving .
column reports the number of schedule sensitive branches found in each benchmark and the number of those related to concurrency errors .
note that each reported schedule sensitive branch has a unique program location.
redundant schedulesensitive branch events on the same location are ltered out.
tame is highly e ective in nding ssbs in these benchmarks of which the traces have a manageable size.
the number of threads in these traces ranges between and .
the online tracing time ranges from a few milliseconds to 2s.
the average constraint size ranges between .5kb to .2mb and the total o ine trace analysis time ranges from 445table results on smaller benchmarks.
the last column refers to harmful ssb.
program loc thread event critical branch tracing avg cons solving ssb example 7ms .5kb 148ms account 519ms .6kb 2ms airline 85ms 125kb 885ms allocation 193ms 89k 1s critical 243ms .2kb 892ms mtlist 408ms 152kb 17s mtset 377ms 97kb .5s stringbu er 47ms .9kb 287ms bufwriter 181ms 229kb .7s linkedlist 45ms 7kb 443ms deadlock 24ms .5kb 277ms piper 46ms .5kb 398ms loader 2s 293kb .3s shop 93ms .2mb .7s sor 187ms 178kb 15s philo 39ms 52kb 696ms total bench 24k 67k .7s 98s table results on real programs and large benchmarks with distributed trace partition 10k.
for benchmarks marked with tame without distributed partition either ran out of memory or timeout in an hour.
program loc thread event critical branch tracing avg cons solving ssb pool107 178ms .1kb 326ms pool146 287ms .5kb .4s pool149 215ms .2kb 433ms pool162 668ms .8kb 592ms log4j 405ms .1kb .4s weblech 35k .2s 46kb .7s cache4j .3s 85kb 673ms hedc 30k .6s 54kb .5s ftpserver 32k .2s 325kb 36s jigsaw 381k .5s 137kb .5s derby 302k .1s 144kb .1s h2 136k .5s 180kb .8s total real 932k 108m .5s .4s tsp .9s 4kb 282ms garage .1s 475kb 40s elevator 182ms 245kb .6s moldyn 3s 363kb .1s montecarlo 6s 211kb .6s raytracer .9s 397kb 651s total bench 10k 102m .1s .6s 2ms to .7s.
for most benchmarks tame detected ssbs in each of them with the total amount to .
.
scalability results on real programs table summarizes the results on real programs and those benchmarks that produce large traces.
for half of the real programs the trace size is relatively small that tame nished the analysis within a few seconds and found ssbs in them.
for the other half marked with the trace size is much larger ranging between 467k in ftpserver to .5m inh2 and tame was not able to nish analyzing the whole trace in a reasonable time it either ran out of memory or timeout in an hour .
to understand the e ectiveness of our distributed trace partition approach we performed online both the distributed partition with the chunk size of each thread set to 10k and the simple window based approach with the total window size set to 100k and compared between them.
it turned out that our distributed partition algorithm is e cient and much more e ective than the simple window based approach.
columns in table report the corresponding results for tame with our distributed partition approach.
tame was able to analyze all these programs within a minute and detected four ssbs in jigsaw andderby whereas tame with the window based approach did not nd any ssbs though took less time .
for the others although tame did not detect ssbs based on the logged chunk of events tame was able to analyze them within a minute.
for the large benchmarks their trace size ranges from 66k to 60m and similar to many real programs tame could not nish analyzing the whole trace.
however with distributed online trace partition tame was able to analyze them in a 446table a summary of schedule sensitive branches found.
i bug.
ii ad hoc synchronization.
iii unclear.
id program class method line statement type account account checkresult if bank total total balance i 2criticalcritical run if t.turn!
i critical run while t.turn!
ii stringbu erstringbu er getchars if srcend 0jjsrcend count i stringbu er append if newcount value.length i stringbu er delete if end count iii stringbu er delete if len iii bufwriter bufwriter main if res!
i piperpiper llplane if last num ofseats rst i piper llplane passengers name ii piper emptyplane while rst last ii piper emptyplane name passengers ii loaderloader main while !newthread.endd ii loader main if array array i newthread run if array array i shopshop getitem storage iii shop putitem storage iii shop isempty if items iii 19sorcyclicbarrier dobarrier else if index ii cyclicbarrier dobarrier else if r!
resets ii garage garagemanager waitformanager if !status.ismanagerarrived iii 22moldyntournamentbarrier dobarrier while isdone !
donevalue ii tournamentbarrier dobarrier while isdone !
donevalue ii 24pool107corsorablelinkedlist next return next i simplefactory makeobject if activecount maxactive i pool149 genericobjectpool allocate if isclosed iii pool162 genericobjectpool allocate if isclosed iii 28log4jcategory callappenders if c.iia!
null i writerappender checkentryconditions if this.layout null i weblech spider isrunning return running i 31jigsawhttpmessage getheadervalue if d!
null d.o set i headerdescription getholder cls.newinstance i 33derbytabledescriptor getobjectname if referencedcolumn map i tabledescriptor getobjectname if referencedcolumnmap.isset ... i few minutes and found ssbs in garage and moldyn .
the one that tame took the most time is raytracer .
the reason is that the trace of raytracer contains a large number of critical events and a large number of branch events.
the trace produces many large constraint les 400kb on average but none of them is satis able.
.
schedule sensitive branches found in our experiments tame found a total of ssbs of them are indications of concurrency bugs related to ad hoc synchronizations and the rest are either functional requirements or their intended behavior is unclear.
we note that all these programs have been frequently studied before but no previous work reported ssbs.
our work is the rst to report ssbs in these programs.
table summarizes these ssbs.
columns report for each ssbthe class method line number and the signature respectively.
the last column reports the type of the ssb i denotes concurrency bug ii ad hoc synchronization and iii unclear.
we next describe several interesting ssbs.
stringbu er we found four ssbs in this program two of them are lated to concurrency bugs.
the rst one is in method getchars at line if srcend srcend count .
the value of the shared variable count can be changed by concurrent threads which makes the branch choice nondeterministic.
this ssb is a direct manifestation of the concurrency bug in this program the true branch throws stringindexoutofboundsexception .
the second ssbis inmethod append at line if newcount value.length .
the value of the local variable newcount depends on count .
this ssb is also an indication of the concurrency bug.
the third and fouth ssbs are both in method delete at lines if end count and if len respectively.
from the semantics of this method both ssbs are intended.
pool107 we found two ssbs in this program one in class org.apache.commons.pool.impl.corsorablelinkedlist at line return next and the other in method makeobject of class pool107 simplefactory at line if active count maxactive .
in the rst ssb next is a shared variable and we found that it may return a null or non null reference depending on the schedule.
the second ssb is actually a manifestation of the concurrency bug in this program which causes a runtime illegalstateexception .
pool149 andpool162 we found a ssb if isclosed in each of these two programs.
both ssbs are in method allocate of class org.apache.commons.pool.impl.generic objectpool but at di erent lines in pool149 and inpool162 .
the method call isclosed can return either true or false depending on the schedule.
it is not clear if this behavior is buggy or not though.
jigsaw we found two ssbs in jigsaw .
one in method getheadervalue in class org.w3c.
at line if d!
null d.offset .
the variable dreferences a shared headerdescription object and its eld offsetmay be set to by another thread concurrently.
this ssb indicates a vulnerability.
the other ssb is in method 447getholder of class org.w3c.
at line cls.newinstance .
the shared variable cls may be null or non null depending on the schedule which also indicates a concurrency bug.
derby we found two ssbs in derby both in method getobjectname of class org.apache.derby.iapi.sql.dict ionary.tabledescriptor at line if referencedcolumn map and line if referencedcolumnmap.isset ... .
in fact both ssbs reveal the same concurrency bug which is previously known that the shared variable referencedcolumnmap can be set to null concurrently.
.
limitations we note that tame currently has two limitations that we plan to address in our future work.
input sensitivity being a dynamic trace based approach tame may not nd all schedule sensitive branches ssbs in the program but only those that can be inferred from the observed trace information which is sensitive to the test input.
enhancing tame with test input generation will help nd more ssbs that can only be revealed by di erent inputs.
relaxed memory models tame currently only models sequential consistency though the java memory model is not sequentially consistent.
we plan to develop weak memory constraints in tame to nd ssbs in systems exhibiting relaxed memory model behaviors.
.
related work to our best knowledge our work is the rst to focus on nding schedule sensitive branches in concurrent programs.
unlike conventional consistency criteria such as data races and atomicity violation which are based on interleaving patterns branch schedule sensitivity is more e ectoriented and may serve as an alternative criterion for concurrency bug detection.
our technique belongs to the school of predictive trace analysis which has been shown promising for practical concurrency defect analysis.
representative techniques include race detection deadlock detection and nding null pointer dereferences .
our work expands the scope to analyze branch behaviors.
there are two lines of related work ad hoc synchronization nding and concolic testing of concurrent programs.
ad hoc synchronizations are often witnessed together with schedule sensitive branches.
however they are not the same.
as shown in our experiments half of the schedule sensitive branches we found are indications of concurrency errors.
in addition ad hoc synchronizations lack a precise de nition and are hard to nd because of their behavioral semantics.
there exist a few techniques to identify ad hoc synchronizations statically or dynamically .
for instance syncfinder relies on a few heuristics yet it is imprecise and may report false alarms.
to re ne our technique for nding concurrency errors we can integrate with syncfinder or the other techniques to sift out ad hoc synchronizations.
concolic execution rst developed in dart and cute has been the golden approach to test sequential programs facing complex constraints.
our technique leverages concolic execution to construct branch conditions facing the missing computations in native code or excluded libraries.
when a computation is missed its concrete value is used to construct a sound constraint.
the e ectiveness of our tech nique can be further improved by exploring multiple traces driven by concolic execution which is still under active research to improve e ciency and code coverage.
several concolic testing techniques have been proposed for generating both inputs and schedules for concurrent programs.
for generating schedules jcute takes a race direct way that re orders the events involved in data races.
similar to our technique both concrest and the work encode scheduling constraints and use smt solving to generate tests.
while the work combines data ow constraints from multiple traces concrest focuses on one trace and explores the scheduling space by iteratively expanding the interference scenarios formed by shared data reads and writes.
a di erence between our technique and concrest is that we consider the events by chunks instead of interference scenarios.
this reduces the number of invocations to the solver when a branch is not schedule sensitive.
moreover because we do not need to generate inputs our technique achieves much higher scalability than concrest.
at the heart of our technique is symbolic trace analysis combined with concolic execution.
many symbolic trace analyses have been proposed before that extract a model from the execution trace in terms of rst order logical constraints and use smt solving to nd concurrency bugs reproduce concurrency failures and test concurrent programs .
the construction of symbolic constraints in our work is similar to that in except that reads and writes are matched through the use of branch conditions rather than by the recorded concrete value.
.
conclusion we have presented a technique tame to precisely identify schedule sensitive branches in concurrent programs.
tame combines symbolic trace analysis and concolic execution to precisely determine if a branch can make a di erent decision in any feasible schedule based on the observed execution trace.
we have speci cally addressed the various challenges for handling real world programs and proposed a novel distributed trace partition approach to achieve good balance between analysis e ciency and e ectiveness.
our evaluation on both popular benchmarks and large complex real applications demonstrates that tame is e ective and scales well to large programs.
tame found schedule sensitive branches in these programs which were rst reported in this paper with half of them resulting from concurrency errors.
.
acknowledgement we would like to thank the anonymous esec fse reviewers for their constructive comments.
this research is supported by faculty start up funds from texas a m university and a google faculty research award to je huang.
.