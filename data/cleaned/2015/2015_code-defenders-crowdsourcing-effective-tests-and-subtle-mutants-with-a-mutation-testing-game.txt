code defenders crowdsourcing effective tests and subtle mutants with a mutation testing game jos e miguel rojas thomas d. white benjamin s. clegg gordon fraser department of computer science the university of sheffield sheffield united kingdom email j.rojas tdwhite1 bsclegg1 gordon.fraser sheffield.ac.uk abstract writing good software tests is difficult and not every developer s favorite occupation.
mutation testing aims to help by seeding artificial faults mutants that good tests should identify and test generation tools help by providing automatically generated tests.
however mutation tools tend to produce huge numbers of mutants many of which are trivial redundant or semantically equivalent to the original program automated test generation tools tend to produce tests that achieve good code coverage but are otherwise weak and have no clear purpose.
in this paper we present an approach based on gamification and crowdsourcing to produce better software tests and mutants the code defenders web based game lets teams of players compete over a program where attackers try to create subtle mutants which the defenders try to counter by writing strong tests.
experiments in controlled and crowdsourced scenarios reveal that writing tests as part of the game is more enjoyable and that playing code defenders results in stronger test suites and mutants than those produced by automated tools.
i. i ntroduction software needs to be thoroughly tested in order to remove bugs.
to evaluate how thoroughly a program has been tested the idea of mutation testing is to measure the number of seeded artificial bugs mutants a test suite can distinguish from the original program.
testers can then be guided to improve test suites by writing new tests that target previously undetected mutants.
in contrast to more basic code coverage criteria such as statement coverage the ability of a test suite to detect mutants is correlated with detecting real faults .
however writing good tests is difficult and developers are often reluctant to do so .
they are even less likely to write tests for mutants mutation tools tend to produce huge amounts of mutants and many of these mutants are trivial or redundant and sometimes even semantically equivalent to the original program in which case time spent trying to write a test is time wasted.
one possible solution lies in also generating the tests automatically but humans tend to write tests that are stronger have a clear meaning and are typically more readable.
the difficulties of writing good tests and using automated mutation tools are similar in nature to those generally targeted by gamification and crowdsourcing gamification is the approach of converting tasks to components of entertaining gameplay.
the competitive nature of humans is exploited to motivate them to compete and excel at these activities by applying their creativity.
crowdsourcing is a problem solving strategy where a difficult problem is encoded and assigned to an undefined group of workers the crowd who providetheir solutions back to the requester the requester then derives the final solution from the solutions collected from the workers who are usually rewarded e.g.
with cash or prizes.
in this paper we describe an approach to generate good software tests and mutants using gamification and crowdsourcing with the c ode defenders game.
testing activities are gamified by having players compete over a program under test attackers try to create subtle hard to kill mutants while defenders try to create tests that can detect and counter these attacks.
in order to crowdsource sets of good tests and mutants code defenders is played as a multi player game where teams of attackers and defenders compete to defeat the opposing team and to score the most points within their own team.
in detail the contributions of this paper are as follows we introduce the c ode defenders multi player game its players actions and its balanced scoring system aiming to make the gameplay enjoyable for both player roles.
we evaluate the gamification aspects of c ode defenders and present the results of a controlled study comparing it to traditional unit testing in terms of the objective performance and subjective perception of participants.
we evaluate the application of c ode defenders in a crowdsourcing scenario and present the results of multiplayer games played on open source classes comparing the tests and mutants to those generated by automated tools.
all participants of our experiments confirmed that playing the game is fun and that writing tests as part of c ode defenders is more enjoyable than so outside the game.
code coverage and mutation scores are higher compared to tests a written outside the game and b generated by automated tools on average higher mutation score than randoop and higher mutation score than evosuite .
mutants created by attackers are significantly harder to kill than those created by the major mutation tool .
in this paper we target the crowdsourcing aspect of c ode defenders however the game is also naturally suited for educational purposes.
our initial findings for educational applications are documented elsewhere .
to support educational use c ode defenders also provides a single player mode where players compete against an automated attacker the major mutation tool or an automated defender the evosuite test generation tool and a two player mode.
we have made c ode defenders open source and freely available to play online at ieee acm 39th international conference on software engineering ieee acm 39th international conference on software engineering .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ii.
b ackground a. unit test generation developers frequently execute unit tests to guard their programs against software bugs.
as writing a good test suite can be difficult and tedious there is a range of different tools to support this activity by automatically generating tests.
a basic approach to generating tests is to do so randomly.
for example randoop is a mature test generation tool for java that produces random sequences of calls for a given list of classes violations of code contracts are reported as bugs and tests that do not reveal bugs are equipped with regression oracles that capture the current program state for regression testing.
because random test generation tends to result in very large test suites and may struggle to cover corner cases search based testing has been suggested as an alternative.
for example evosuite generates test suites using a genetic algorithm which aims to maximize code coverage.
test suites are minimized with respect to the target criteria thus resulting in far fewer tests than random testing would produce.
approaches based on symbolic execution can be effective for certain types of problems that are particularly amenable to the power of modern constraint solvers.
for example evosuite implements an experimental extension that uses dynamic symbolic execution to generate primitive input values and the pex tool uses dynamic symbolic execution to instantiate parameterized unit tests for c .
the annual unit test generation tool competition compares different unit test generation tools for java and although tools have made substantial progress in recent years there remain several challenges.
xusheng et al.
identify different challenges that hinder test generation tools in reaching code e.g.
object mutation complex constraints etc.
and shamshiri et al.
identified several problems that hinder automatically generated unit tests from finding real faults.
pavlov and fraser demonstrated that some of these can be overcome by including human intelligence by using an interactive genetic algorithm in the evosuite tool.
b. mutation testing in order to evaluate test suites and to guide selection of new tests mutation testing has been proposed as an alternative to traditional code coverage metrics.
mutation testing consists of seeding artificial faults mutants in a program and then measuring how many of them are found killed by the test suite.
the mutation score i.e.
the ratio of mutants killed provides an indication of the test suite quality while mutants that remain alive provide hints on where to add new tests.
there is evidence that test suites that are good at finding mutants are also good at finding real faults.
one of the main advantages of mutation testing over code coverage is that code coverage does not consider the quality of test oracles i.e.
how the correctness of the test execution is checked.
however the practical application of mutation testing is hindered by two significant problems first non trivial code results in large numbers of mutants.
mutants are generatedusing different mutation operators which systematically perform simple modifications e.g.
replace an operator and each application of an operator results in a new mutant.
despite many efforts to reduce the number of mutants produced e.g.
the number remains large which is not only a problem for scalability but also because many mutants are either trivial or subsumed by other mutants .
the second problem is that some mutants are semantically equivalent to the original program such that there exists no killing test.
detecting equivalent mutants is an undecidable problem and effort on trying to derive such a test is likely wasted.
different techniques and systems have been developed to detect equivalent mutants e.g.
but they are generally limited to certain types of mutants.
thus human intervention is still required to discern hard to kill or stubborn mutants from equivalent ones .
one insight underlying this paper is that these two main problems of mutation testing designing good mutants and deciding equivalence both require human intelligence.
this leads us to investigate the use of gamification and crowdsourcing.
c. crowdsourcing and gamification problems that are hard to solve computationally but can be effectively solved by humans can be amenable to crowdsourcing .
the general principle is to identify and extract tasks that require human intelligence and then to present these human intelligence tasks to crowd workers .
additional computational effort is usually necessary to assemble the individual task solutions to solve the overall problem.
in software engineering crowdsourcing platforms such as amazon mechanical turk where crowd workers are paid small fees for completed tasks are often used for empirical studies but there are attempts to crowdsource various parts of the software development process .
gamification uses game design elements competitions with other players game rules point scoring fantasy scenarios etc.
to make unpleasant or dull tasks more entertaining and rewarding .
it is often applied in education settings but has also been useful for improving how people engage with aspects of their work even in software engineering .
a particular form of gamification are games with a purpose where players of the game solve underlying computational problems sometimes without being aware of this .
in other words games with a purpose are a form of crowdsourcing where the incentive for workers is provided in terms of the gameplay.
famous examples include recaptcha or duolingo .
iii.
t hecode defenders game a. gameplay code defenders is a competitive game where two teams compete over a java class under test cut and its test suite one team leads an attack on the cut whereas the other team tries to defend it.
attackers aim to create variants of the cut i.e.
mutants with which they attack the fault detection capability of the associated test suite.
defenders aim to protect the cut by writing unit tests that detect i.e.
killthe mutants.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
the attacker s view.
fig.
the defender s view.
two difficulty levels are available in the game.
in the easy level attackers and defenders see all submitted mutants and tests.
in the default hard level the information presented to players is restricted to balance the gameplay and make it more interesting for both roles.
attackers have a code editor where they create mutants by modifying the cut figure .
they see all mutants in the game including their code diffs and the code editor highlights the line coverage achieved by the tests submitted to the game so far.
the highlighting reflects how often lines are covered the more often a line is covered the darker the highlighting is.
defenders figure see the source code of the cut together with the locations of live and dead mutants.
in their code editor they are given a template to write a unit test for the cut and they also see previous tests as well as their coverage.
unlike the round based gameplay of our preliminary version of c ode defenders attackers and defenders can submit mutants and tests at any time and do not need to wait for other players to act.
fig.
the equivalence duel view.
b. equivalence duels the mutants that attackers create in the game may be equivalent whether on purpose or not.
the gameplay integrates duels that allow players to decide on equivalent mutants.
if a defender suspects a mutant to be equivalent for example because the mutant is still alive after several failed attempts at killing it then he she can challenge the attacker by starting an equivalence duel.
the onus is then on the attacker either to write a test that kills the mutant proving it is not equivalent or to confirm that the mutant is indeed equivalent figure .
c. the multiplayer scoring system the point scoring system is based on assigning each mutant and test a number of points that can change as the game unfolds.
in particular mutant points are calculated as follows if a mutant is killed by an existing test when it is created i.e.
a stillborn mutant then it receives no points.
a mutant gains a point for every test that covers any of the mutated lines but still passes.
thus surviving mutants created on heavily tested lines although risky will result in more points.
if a mutant is created and not killed by any existing tests then it receives one point in addition to points gained from tests that cover it but do not fail .
this is to encourage creation of mutants also for code not yet covered by tests.
once a mutant is killed its score is no longer increased.
test points are calculated as follows for each mutant that a test kills the test gains points equal to the score of the mutant plus one.
this applies to mutants that already existed at the time the test was created as well as mutants added to the game later.
a test gains one point for killing a newly created mutant.
when a mutant is submitted tests are executed in the order of their creation.
thus the oldest test that kills a mutant receives the point and no other tests receive points for the same mutant.
the score of an attacker is the sum of the points of her mutants the score of a defender is the sum of the points of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
her tests.
equivalence duels can update both players scores if a defender claims a mutant as equivalent but the attacker proves non equivalence with a test then the attacker keeps the mutant s points and the mutant is killed.
however if the attacker accepts that the mutant is equivalent or the game ends then she loses all the points she scored with that mutant and the defender who had claimed equivalence gains one point.
while an equivalence duel is active the mutant remains alive and can be killed by other defenders which would cancel the duel the mutant can still gain points for surviving newly submitted tests until the equivalence duel is resolved.
if the attacker submits a test which compiles but fails to kill the mutant they lose the duel and the mutant is assumed equivalent.
an elaborate example of the scoring system can be found on the c ode defenders webpage.
one potential issue with the scoring system is that in the last few minutes of a game a defender could flag all mutants as equivalent leaving no time for attackers to resolve the equivalence this would mean that all mutants are penalized and lose their points.
similarly attackers could submit equivalent mutants in the last few minutes leaving defenders no time to react.
we prevent this from happening by introducing a grace period of configurable duration at the end of each game e.g.
one hour .
in this grace period no new mutants or defender tests can be submitted.
in the first part of the grace period e.g.
min.
defenders can flag mutants as equivalent while attackers wait the remaining time of the grace period can only be used by attackers to resolve pending equivalence duels.
d. code editing restrictions whenever humans engage in competitive games there is the possibility of cheating and unfair behaviour and this also holds in gamified software engineering tasks .
in particular once players understand the scoring system there will likely be some players who try to create mutants or tests in a way that benefits their score without providing a useful improvement in terms of the mutants or tests generated in the game.
for example an attacker could add an if condition of the type if x which could only be killed by a test that happens to use the arbitrary input data which is very unlikely.
this mutant would increase the attacker s score but it may misdirect the effort of the defenders and likely does not resemble a real fault.
to reduce the possibility of such behaviour we implemented a number of restrictions on the modifications that attackers can perform and the tests that defenders can create.
in particular the following restrictions apply when creating tests and mutants conditionals loops boolean operators and method definitions cannot be added.
this prevents too complex tests and mutants which are near impossible for defenders to kill but easy for an attacker to prove non equivalent example above .
calls tojava.util.system.
cannot be added this is to restrict access to system information e.g.
environment variables and to prevent executing unsafe operations e.g.
calls tosystem.exit .
security is enforced by executing all tests in a sandbox using a strict security manager.
calls tojava.util.random cannot be added to avoid flaky tests or impossible to kill mutants.
tests must contain at most two assertions this prevents defenders from writing unit tests with mega assertions which not only is a bad unit testing practice but could also damage the gameplay e.g.
by discouraging other defenders and reducing points of surviving mutants .
iv.
d oes gamification improve testing ?
before evaluating the applicability of c ode defenders as a crowdsourcing solution for test generation we investigated its general feasibility as a gamification approach to software testing.
to this end we used the two player version where one attacker plays against one defender in a round based mode and designed a controlled empirical study to answer the following research questions rq1 do testers produce better tests when playing a game?
rq2 do testers prefer writing tests while playing a game?
a. experiment setup we conducted this controlled study in a computer lab at the university of sheffield.
we invited undergraduate and postgraduate students researchers and professional developers by email.
student candidates were required to have completed at least one java course in their degree and all candidates were asked to complete an online java qualification quiz to demonstrate their java skills.
we selected all candidates who answered at least out of the questions correctly.
of the participants were undergraduate students were master s or phd students and the rest were either professional developers or academics.
all participants were in computer science or software engineering related fields had a diverse degree of experience programming in java but generally little or no industrial work experience .
the majority had used junit or a similar testing framework before and understood well or very well the concept and usage of mutation testing although most admitted to only rarely or occasionally writing unit tests when programming.
prior to the experiment participants attended a training session consisting of a brief tutorial on unit and mutation testing and an introduction to c ode defenders .
they familiarized themselves with the web interface of the game through short guided tasks.
to conclude the training session all participants played an actual c ode defenders game on a simple class.
when asked in the exit survey whether they understood the gameplay only participants partially disagreed and further participants neither agreed nor disagreed.
the actual experiment consisted of two minute tasks per participant.
the three possible tasks were writing unit tests manually playing c ode defenders as an attacker or playing c ode defenders as a defender.
we selected two classes under test sortedintegerlist a standard implementation of a data structure for sorted list of integers and iban a validator for international banking account numbers from the swift wife open source project.
each participant performed one task on each of the two classes.
the manual testing tasks authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
serve as the baseline of regular testing behavior and were also performed using the c ode defenders web interface we asked participants to test the class as well as possible to guard against potential faults but we did not explicitly ask them to optimize for coverage or other metrics.
a pre created assignment determined the two tasks for each participant.
the assignment was designed to balance tasks for the two classes the order in which participants performed each task and the order in which participants played as attackers or defenders for each class.
the assignment further ensured that the attacker and the defender in each game did not sit next to each other.
participants were randomly assigned usernames based on the assignment and did not get to know who they were playing against.
the experiment including training lasted two hours and each participant was paid gbp20 for their involvement.
in total games were played and manual testing tasks were completed.
on average each game lasted .
rounds and a total of valid unit tests were produced by the game players.
manual testers were not bound to the round based setting of the game and produced valid tests a test is valid if it compiles and passes on the original class under test .
to answer rq1 we compare the tests written by participants playing as defenders with tests written by participants manual unguided unit testing.
we measure the standard quality attributes of code coverage and mutation scores using jacoco1to measure coverage and major to calculate mutation scores.
after the experiment all participants were asked to fill out an exit survey which consisted of standard demographic questions questions of agreement on aspects of the gameplay with value likert scale responses questions where we asked users to state their agreement with possible improvements and free text questions to comment on the user interface the point scoring system and the overall game.
to answer rq2 we use the data on five questions that directly asked the participants whether they preferred playing the game to writing tests.
b. threats to validity construct we used mutation scores and branch coverage to compare tests but it may be that other quality attributes e.g.
readability are affected by the gameplay.
we countered this threat by adding restrictions on the tests e.g.
maximum number of assertions .
while evidence supports that real faults are correlated with mutants it is possible that the use of faults created by developers may yield different results.
internal to prepare the study and to process the results we used automation extensively and faults in the automation may have an influence on the results of the study.
to counter this threat we tested all our software and make all data and scripts available.
to avoid bias we assigned tasks to participants randomly based on a pre created balanced assignment.
this assignment ensures that no two neighbouring participants would work on the same class or treatment at the same time.
participants without sufficient knowledge of java and junit may affect the results therefore we only accepted participants accessed august 2016manual code def.
a number of testsmanual code def.
.
.
.
.
.
b branch cov.manual code def.
.
.
.
.
.
.
.
.
c mutation score fig.
boxplots comparing the number of tests created branch coverage and mutation scores achieved when using code defenders vs manual testing means indicated with red dots .
who correctly answered at least three out of five question of a qualification quiz.
we also provided a tutorial on unit and mutation testing before the experiment.
to ensure that experiment objectives are not unclear we tested and revised our material on a pilot study with phd students.
we also interacted with the participants throughout the experiment to ensure they understood their tasks in the exit survey participants confirmed they understood the objectives.
as each participant performed two tasks it is possible that those playing as a defender in the first session could grasp insight on how tests should be written to kill mutants if they are given manual testing as their second task.
to lessen the impact of this learning effect our assignment of objects to participants ensures that each pair of classes treatments occurs in all possible orders.
to counter fatigue effects we restricted the tasks to minutes included short breaks after the training session and between the two main sessions and also provided light refreshments.
in order to minimize participants communication we imposed exam conditions and explicitly asked participants not to exchange information or discuss experiment details during the breaks.
external most participants of our study are students which is a much debated topic in the literature e.g.
.
however we draw no conclusions from absolute performance and see no reason why students experience of playing c ode defenders should be different from other types of players.
the classes used in the experiment are small to allow understanding and testing within the short duration of the experiment.
although object oriented classes are often small it may be that larger classes with more dependencies affect the gameplay.
thus to which extent our findings can be generalized to arbitrary classes remains an open question.
c. results rq1 do testers produce better tests when playing a game?
figure a shows that participants performing the manual testing task wrote more tests than participants playing c ode defenders as defenders this is expected as the two player mode is turn based and after submitting a test defenders have to wait for the attacker to create a new mutant.
figure b authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
i would consider playing code defenders for funcreating mutants is more fun than writing testswriting unit tests as part of the game is more fun than writing unit tests while codingi enjoy writing tests even when it is not part of a gamei enjoyed playing code defenders percentage fully agree partially agree neither partially disagree fully di sagree fig.
exit survey results compares the resulting test suites in terms of branch coverage measured with jacoco.
interestingly the branch coverage achieved by these tests is nevertheless similar mann whitney u test with p .
vargha delaney effect size of a12 .
where a12 .5means there is no difference and a12 .
means higher values for game players on average the tests written by c ode defenders players achieved .
branch coverage while manual testers achieved .
.
in terms of mutation score figure c the tests written by players are clearly stronger with an average mutation score of .
vs. .
for tests written by manual testers.
the difference in mutation score is statistically significant a12 .
p .
.
rq1 in our experiment participants playing code defenders wrote stronger tests than those not playing.
rq2 do testers prefer writing tests while playing a game?
for space reasons we cannot provide the complete survey results.
to answer rq2 we focus on the level of agreement expressed by the participants of the experiment with the following five statements i i enjoyed playing code defenders ii i enjoy writing tests even when it is not part of a game iii writing unit tests as part of the game is more fun than writing unit tests while coding iv creating mutants is more fun than writing tests v i would consider playing code defenders for fun.
data on all other questions most of which are related to the game experience and possible improvements is available at .
figure shows that all participants enjoyed playing the game.
at least partially agree that writing tests in general can be fun but all participants agree that writing tests was more fun as part of c ode defenders .
all but participants also claimed they would consider playing c ode defenders again.
overall these responses indicate that the testing task is more engaging for participants when performed in a gamified scenario.
rq2 participants of our experiments claim they enjoyed writing tests more when playing code defenders .
figure also shows a strong tendency that creating mutants is more enjoyable than creating tests.
this is not surprising however the short duration of the game did not allow many equivalence duels to take place in which case the attacker also has to write tests.
the answers to the survey suggested a range of improvements to game mainly related to the user interface and the point scoring system which we consideredwhen designing the multi player version of c ode defenders .
v. c anwecrowdsource tests and mutants ?
having established that players engage well with c ode defenders and produce useful tests the question now is whether we can make use of the game and apply it in a crowdsourcing scenario where multiple players compete and deliver good test suites and mutants.
to this end we implemented the multi player version of c ode defenders described in this paper and collected data from a number of games in order to answer the following research questions rq3 does crowdsourcing lead to stronger test suites than automated test generation?
rq4 does crowdsourcing lead to stronger mutants than automatically generated mutants?
rq5 do mutation scores on crowdsourced mutants correlate with mutation scores on traditional mutants?
a. experiment setup we followed a systematic procedure to select classes from the sf110 repository which consists of randomly sampled sourceforge projects as well as the top ten most popular ones and the apache commons ac libraries .
first because code defenders currently lacks support for browsing source code trees we selected classes which compile in isolation i.e.
no dependencies .
next we restricted the search by size and identified all classes with non commenting lines of code2.
our experience from previous user studies suggests that classes in this size range tend to be suitable for experimental unit testing tasks .
these two automated filters narrowed the search down to classes from ac and from sf110 which were then manually ranked by complexity e.g.
does the class implement interesting logic?
purpose e.g.
is the class understandable without context?
and testability e.g.
does the class contain public observers?
.
finally twenty of the top ranked classes were selected while preserving some domain diversity.
table i lists the selected classes the projects they belong to their size in ncss and the number of mutants created by major as an indicator of complexity .
twenty games were then scheduled over the course of days one per selected class.
participants of the first study were invited to play the games and the invitation was extended to academic and industrial contacts via direct emails email lists and social media.
in total unique participants signed up and took part in at least one game.
participants were free to chose which games to play and which team to join in each game attackers or defenders .
in order to start at least three attackers and three defenders were required and at most five players could join each team.
games started on their scheduled date and time or were delayed until the minimum number of players was met and lasted for hours from its starting time.
as incentive to play the best attacker and defender in each game were awarded gbp10 in shopping vouchers.
2measured by javancss authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i classes selected for crowdsourcing experiment.
class project ncss major mutants bytearrayhashmap sf vuze bytevector sf jiprof chunkedlongarray sf summa ftpfile ac net fontinfo sf squirrel sql hierarchypropertyparser sf weka hslcolor sf vuze improvedstreamtokenizer sf caloriecount improvedtokenizer sf caloriecount inflection sf schemaspy inthashmap sf vuze parameterparser ac fileupload range ac lang3 rationalnumber ac imaging subjectparser sf newzgrabber timestamp ac net vcardbean sf heal weakhashtable ac logging xmlelement sf inspirento xmlparser sf fim1 hereinafter abbreviated hpropertyparser and istreamtokenizer.
each game resulted in a set of mutants and a test suite containing all tests created in the game.
to answer rq3 we compared these test suites with automatically generated test suites in terms of branch coverage using jacoco and mutation score using major .
we chose evosuite and randoop as representatives of state of the art test generation tools for java and ran them with default configurations and a one minute time budget to generate test suites per class per tool to account for the randomized algorithms they implement .
to answer rq4 we measured how difficult the mutants produced in c ode defenders are compared to mutants generated by using a mutation testing tool.
we calculated the number of random tests that kill each mutant intuitively the fewer random tests kill a mutant the harder it is to kill.
to produce these random tests we run randoop on each game class to generate one single test suite with up to random tests with a minute time budget.
we then executed each of these tests individually on all the mutants generated by major and on all the mutants created in c ode defenders counting the number of tests that killed each mutant.
finally to answer rq5 we calculated the mutation scores of the test suites generated for rq3 and rq4 on all major mutants as well as all mutants generated in the game and investigated the correlation between these scores.
b. threats to validity threats to validity caused by our object selection automation and metrics are similar to what is described in section iv b. the crowdsourcing nature of this second experiment affects the participant selection.
we advertised the experiment among the participants of our first study as well as standard email channels and social media participants of the original study took part and new external participants were recruited.
external participants did not receive the same training participants of the first study received but instead learned about the game purely from the help page on the website and by playing practice games on their own.
it is possible that in practicetable ii details of the multi player games played.
class att.
def.
mut.
tests killed equiv.
score a d bytearrayhashmap bytevector chunkedlongarray fontinfo ftpfile hpropertyparser hslcolor istreamtokenizer improvedtokenizer inflection inthashmap parameterparser range rationalnumber subjectparser timestamp vcardbean weakhashtable xmlelement xmlparser mean .
.
.
.
.
.
participants may have more diverse qualifications and skills.
however the multi player nature of the game means that the results are not dependent on the skills of individual players and remuneration based on contribution would pose no financial risk to including worse players.
nevertheless finding qualified participants is a general concern in crowdsourcing and requires careful planning of incentives.
participants chose the games and their roles without our influence.
all classes originate from open source projects to prevent players searching for existing tests for them we anonymized all classes by removing all project specific details including package declarations.
as games were run in sequence and participants were allowed to join more than one game there may be learning effects between games.
to reduce these effects we only ran one game per class which avoids learning effects on the cuts.
the test suites produced in the game are compared against those produced by randoop and evosuite using default configurations with bounded time.
although the time spent by players in the game is not directly comparable to the running time of the tools it is possible that using larger time budgets or fine tuned parameters would improve their test suites.
however beyond running time there are fundamental limitations in the tools that our approach aims to overcome.
c. results table ii summarizes the games that were played.
on average there were .
attackers submitting a mean of .
mutants.
the average number of defenders was .
submitting a mean of .
tests per game.
out of the games were won by the defending teams and by the attacking teams suggesting that overall the scoring is well balanced.
rq3 does crowdsourcing lead to stronger test suites than automated test generation?
table iii compares the tests written by players of c ode defenders with those generated with randoop and evosuite.
on average the c ode defenders test suites achieved .
branch coverage whereas randoop authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii comparison of test suites generated with c ode defenders with automatically generated test suites bold font represents cases with statistically significant difference at p .
.
class branch coverage mutation score major mutation score code defenders code d. randoop evosuite code d. randoop evosuite code d. randoop evosuite cov.
a12 cov.
a12 score a12 score a12 score a12 score a12 bytearrayhashmap .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bytevector .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
chunkedlongarray .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ftpfile .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fontinfo .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
hslcolor .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
hpropertyparser .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
istreamtokenizer .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
improvedtokenizer .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
inflection .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
inthashmap .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
parameterparser .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
range .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rationalnumber .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
subjectparser .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
timestamp .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
vcardbean .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
weakhashtable .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
xmlparser .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
xmlelement .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mean .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
achieved .
and evosuite .
.
the branch coverage achieved by randoop was lower in of cases and significantly so in cases note that randoop could not generate any tests for class weakhashtable and produced only non compilable tests for class range both cases due to java generics .
randoop achieved a significantly higher branch coverage for class inthashmap.
on closer look at how the game for this class evolved we observed that the in game tests missed branches that the randoop test suites did cover.
a plausible conjecture that also applies for the rest of the games is that the c ode defenders highlighting feature which currently only shows line coverage rather than branch coverage may have misled defenders into thinking some parts of the code were fully tested when in reality they were not.
the average effect size of a12 .94confirms that the c ode defenders tests indeed achieve substantially higher coverage.
for classes coverage is also higher than that of the test suites generated by evosuite with being significant.
however there are also classes significant where evosuite achieved higher coverage and one where the coverage is identical.
the average mutation score calculated by major on the c ode defenders test suites is .
which is again substantially higher than that achieved by randoop .
on average and evosuite .
on average .
there are classes where the mutation score is higher than randoop s significant in cases but there are also cases where the mutation score is lower significant in cases .
compared to evosuite there are no cases with significant differences but the mutation score of the c ode defenders test suites is higher in cases.
finally we also calculated the mutation scores based on the mutants generated during the gameplay.
a similar pattern is revealed here for chunkedlongarray hslcolor and timestamp the randoop test suites have higher mutationscores but for all other comparisons the c ode defenders test suites have higher scores.
on average c ode defenders tests achieve a mutation score of .
whereas randoop and evosuite tests only achieve .
and .
respectively.
rq3 crowdsourcing achieves higher coverage and mutation scores than state of the art test generation tools.
example.
the following test created in the game played on class weakhashtable illustrates how players use stronger assertions than the regression assertions that automated tools are able to generate for example by asserting on chains of calls and using observers that take parameters java.util.hashmap foo newjava.util.hashmap weakhashtable w newweakhashtable foo.put a b w.putall foo asserttrue w.keyset .contains a asserttrue w.containskey a however good for coverage and fault detection tests created in c ode defenders may require post processing some players used profane words in string literals and used esoteric stratagems to bypass our test code restrictions.
rq4 does crowdsourcing lead to stronger mutants than automatically generated mutants?
figure a shows the detection rates for mutants resulting from c ode defenders and those generated by major.
the detection rate is the ratio of randomly generated tests that detects a mutant the lower it is the harder the mutant is to detect.
as we do not know which major mutants are equivalent we calculate hardness onallmutants results are similar if considering only mutants killed by the random tests.
on average the detection rate is .
for c ode defenders mutants and .
for major mutants.
the difference is significant according to a mann whitney u test atp .001with a medium effect size of a12 .
.
consequently c ode defenders mutants are harder to kill.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
code def.
major0.
.
.
.
.
.0detection ratio a detection rate0.
.
.
.
.
.
.
.
.
.
.
.
code defendersmajor b score correlation fig.
comparison of code defenders and major mutants rq4 mutants created by code defenders are harder to kill than mutants created by major.
examples.
the highest scoring mutant in the study is a subtle replacement of a base number in class inthashmap which was ultimately killed in the game but survived all automatically generated tests means original class means mutant intindex hash 0x7 fffffff tab.length intindex hash 0x7 effffff tab.length while this mutant is similar in nature to the common constant replacement mutation operator it does suggest that players can identify subtle mutants.
other mutants are different to standard operators for example by replacing