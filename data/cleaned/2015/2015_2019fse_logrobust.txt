robust log based anomaly detection on unstable log data xu zhang microsoft research beijing china nanjing university nanjing chinayong xu qingwei lin bo qiao microsoft research beijing chinahongyu zhang university of newcastle callaghan australiayingnong dang microsoft azure redmond usa chunyu xie microsoft research beijing chinaxinsheng yang qian cheng ze li microsoft azure redmond usajunjie chen college of intelligence and computing tianjin university tianjin chinaxiaoting he microsoft research beijing china randolph yao microsoft azure redmond usajian guang lou microsoft research beijing chinamurali chintalapati microsoft azure redmond usafurao shen nanjing university nanjing china dongmei zhang microsoft research beijing china abstract logs are widely used by large and complex software intensive systems for troubleshooting.
there have been a lot of studies on log based anomaly detection.
to detect the anomalies the existing methods mainly construct a detection model using log event data extracted from historical logs.
however we find that the existing methods do not work well in practice.
these methods have the close world assumption which assumes that the log data is stable over time and the set of distinct log events is known.
however our empirical study shows that in practice log data often contains previously unseen log events or log sequences.
the instability of log data comes from two sources the evolution of logging statements and the processing noise in log data.
in this paper we propose a new log based anomaly detection approach called lo robust .
logrobust extracts semantic information of log events and represents them as semantic vectors.
it then detects anomalies by utilizing an attention based bi lstm model which has the ability to capture the contextual information in the log sequences and automatically learn the importance of different log events.
in this way logrobust is able to identify and handle unstable log events and sequences.
we have evaluated logrobust using logs collected zhangxu037 smail.nju.edu.cn permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august tallinn estonia association for computing machinery.
acm isbn .
.
.
.
the hadoop system and an actual online service system of microsoft.
the experimental results show that the proposed approach can well address the problem of log instability and achieve accurate and robust results on real world ever changing log data.
ccs concepts software and its engineering maintaining software .
keywords anomaly detection log analysis deep learning log instability data quality acm reference format xu zhang yong xu qingwei lin bo qiao hongyu zhang yingnong dang chunyu xie xinsheng yang qian cheng ze li junjie chen xiaoting he randolph yao jian guang lou murali chintalapati furao shen and dongmei zhang.
.
robust log based anomaly detection on unstable log data.
in proceedings of the 27th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august tallinn estonia.
acm new york ny usa pages.
introduction large and complex software intensive systems such as online service systems and big data systems produce logs for troubleshooting.
the log messages are usually semi structured text strings which are used to record events or states of interest.
engineers can examine recorded logs to understand the status of software systems detect system anomalies and locate the root causes.
because of its simplicity and effectiveness logging has been commonly adopted in practice.
for example an empirical study on two microsoft 807esec fse august tallinn estonia xu zhang et al.
systems and two open source projects shows that logging is commonly used.
statistically there is one line of logging code in every lines of source code.
as the scale and complexity of the system increase it is becoming more difficult to detect system anomalies by manual examination of the logs.
over the years many automated log based approaches have been proposed to detect system anomalies .
these work retrieve useful information from logs and adopt data mining and machine learning techniques to analyze log data and detect the occurrence of system anomalies.
for example xu et al.
formulated the log based anomaly detection problem as an unsupervised learning problem and utilized principal component analysis pca to detect anomalies.
lou et al.
mined the invariants from console logs where a system anomaly is detected if the occurrences of log events break a certain invariant during the lifecycle of the system execution.
although effective the existing log based anomaly detection approaches are not sufficiently robust in practice.
to detect anomalies almost all existing approaches require to construct a detection model using the known log events i.e.
the templates of log messages and log sequences i.e.
series of log events that record specific execution flows extracted from the training data.
they fail to work with previously unseen log events and log sequences.
however our empirical study has found that real world log data is unstable meaning that new but similar log events and log sequences often appear.
we have identified the following two sources of instability in real world log data evolution of logging statements like other software artifacts logs are always evolving.
developers may frequently modify source code including logging statements which in turn leads to changes to log data.
as kabinna et al.
observed around of logging statements in their studied projects changed throughout their lifetime.
many new log events and log sequences are generated by the everchanging logging statements.
it was reported that google s systems have up to thousands of new log printing statements every month .
therefore log based anomaly detection approaches must be able to cater to the evolution of log data.
this is especially important for organizations that adopt the continue delivery deployment approaches .
processing noise in log data during collection retrieval and pre processing of log data it is inevitable that a certain degree of noise is introduced into the original log data.
for example the noise could come from the data collection process.
in a large scale system many logs are produced by geographically distributed components separately and then uploaded to a centralized location for further analysis.
missing duplicated or disordered log messages could be resulted from such a process e.g.
due to network errors limited system throughput storage issues etc.
.
another important source of noise comes from log parsing.
in general an early step of log data analysis is to extract the log events from the raw log messages using a log parser.
however it has been observed that the existing log parsers are not sufficiently accurate which could lead to many misidentified logevents.
the noise in log data hampers the effectiveness of the existing log based anomaly detection approaches.
to investigate the log instability issue we have performed an empirical study on log data produced by a large scale online service system of microsoft.
we find that unstable log data is very common in real world systems.
the details of the study are reported in section .
.
due to the instability of log data the effectiveness of the existing anomaly detection approaches is significantly affected.
the existing approaches are based on a close world assumption the patterns of log events and log sequences are constant which is not practical in real world systems where unstable log events and sequences always appear.
on the one hand a small change to an existing log event as shown in figure can introduce a different but semantically similar one which is recognized by existing approaches as a brand new log event .
therefore existing approaches will either fail to work due to the incompatibility with these unseen log events or result in low performance due to the incorrect classification.
on the other hand log sequences are also likely to be changed due to new execution paths or processing noise.
however traditional approaches only leverage the occurrence information of log sequences and ignore its context attribute and different importance of logs.
it is also worth noting that a large scale online service system is always under active development and maintenance the log data could change frequently.
therefore it is unpractical to update log based anomaly detection tools continuously due to the large amount of effort it requires.
a detailed discussion of existing approaches will be given in section .
.
to overcome the instability issue in this paper we propose logrobust a novel log based anomaly detection approach which can achieve accurate and robust anomaly detection on real world ever changing and noisy log data.
unlike the existing approaches logrobust does not rely on the simple occurrence information of log events.
instead it transforms each log event into a semantic vectorof the fixed dimension.
semantic vectors are capable of capturing the semantic information embedded in log events.
through semantic understanding this representation method is able to identify and handle new but similar log events that emerge from evolving logging statements and parsing errors.
then taking the sequence of semantic vector as input an attention based bidirectional long short term memory neural network bi lstm classification model is applied to detect the anomalies.
the attention based bi lstm model has the ability to capture the contextual information in the log sequences and automatically learn the importance of different log events.
thus it is robust to the variations in the sequences.
we have evaluated the proposed approach using the public log data collected from hadoop.
we inject different ratios of changes to the hadoop log data and evaluate the effectiveness of the proposed approach.
the experimental results show that logrobust is robust when the injection rate is increased from to the f1 score is only slightly decreased from .
to .
.
we have also applied logrobust to the industrial log data collected from a largescale online service system of microsoft with real instability issue.
logrobust achieves f1 score of .
which is higher than other traditional methods.
the experimental results show that the 808robust log based anomaly detection on unstable log data esec fse august tallinn estonia proposed approach can effectively detect anomalies of the online service system with the ever changing and noisy log data.
the main contributions of this paper are as follows we point out the problem of log instability in log based anomaly detection.
we also conduct an empirical study on log instability in real world systems to confirm our findings.
we propose logrobust a new log based anomaly detection approach which is robust to the unstable log data.
to our best knowledge we are the first to address the instability issue of log data in anomaly detection.
we have evaluated logrobust using both public and realworld industrial datasets.
the results confirmed the effectiveness of our approach.
the remainder of this paper is organized as follows we introduce the background of our work and an empirical study of unstable log data in section .
section describes our approach.
section presents our experimental design and results.
section discusses the incremental updating of our logrobust and the threats to validity.
section surveys related work followed by section which concludes this paper.
background and empirical study .
log terminology raw log msg.
seq.
blk 527787503483342113 raw log msg.
seq.
blk 7503483334211331202 raw log msg.
seq.
blk 7503483334202473044 1receiving block blk 7503483334202473044 src .
.
.
dest .
.
.
2receiving block blk 7503483334202473044 src .
.
.
dest .
.
.
3receiving block blk 7503483334202473044 src .
.
.
dest .
.
.
... ... .
.
.
served block blk 7503483334202473044 to .
.
.102log sequence 3log sequence 2log sequence 1receiving block src dest 2receiving block src dest 3receiving block src dest ... ... served block to parsing a raw log messages b log events figure an example of raw log messages log events and log sequence logs which contain abundant information e.g.
events parameters execution details etc.
about the running status of a softwareintensive system play a crucial role in the maintenance of online service systems.
once a problem anomaly occurs engineers often rely on system logs for further investigation.
figure shows an example of log data from the public hdfs dataset .
each line printed to the system console is a log message .
here we omit some fields for clarity.
each log message consists of a constant part log event and a variable part log parameter .
the log parameter records some system attributes e.g.
url file name or ip address etc.
.
the log event is comprised of fixed text strings and is a template of a log message.
a log event is acquired from a log message through log parsing .
alog sequence consists of a sequence of log events which records an execution flow of a specific task.
log events from the same log sequence share the same task id which can be used to link the events chronologically.
figure 1b shows an example of log sequence with the task id blk 7503483334202473044 .
.
an empirical study on log instability in this section we describe an empirical study in real world industrial scenarios which demonstrates the problem of log instability and reveals the subsequent consequences it causes.
.
.
evolution of logging statements.
we study log evolution on a real world online service system service x from microsoft.
we identify evolving log events by analyzing the changes to logging statements in the source code.
we use the statistics of log events in version .
as the baseline and observe the changes in the numbers of new versions.
the results are shown in figure .
it can be seen from figure 2a that there are a lot of newly added log events in newer versions the blue line .
in addition some old log events are also removed from the source code the black line and a part of original log events have been modified in the new versions the red line .
furthermore we can see from the figure 2b that the total number of log events keeps increasing from in version .
to in version .
.
the unchanged log events are becoming less and less in the newer versions.
in the latest version version .
the number of changed log events accounts for .
of the total log events.
.
.
.
.
.
.
.
.
versions log event added removed modified a changed log events1.
.
.
.
.
.
.
.
versions log event total unchanged b all log events figure the evolution of log events across versions figure gives some real cases of evolving log events from service x in microsoft where the two log messages before and after the changes are given.
case creating and opening new channel factory creating and opening new channel factory for bindingid case result basehash uploadhash tiphash newtiphash minimerge result basehash uploadhash tiphash newtiphash case putsharedremoteservicelocation entering with inputs putsharedremoteservicelocation entering with inputs figure examples of evolving log events case two new words for bindingid are added to the original log event as a supplementary explanation for clarity.
809esec fse august tallinn estonia xu zhang et al.
case a keyword minimerge is prepended to the original log event to indicate the program execution stage.
case a keyword isadocumentsession is removed as the related feature is deprecated.
we also study the evolving log sequence in service x. we collect log data from two days spanning about one month.
we notice that the amount of new log sequences accounts for more than of the total dataset which indicates that almost all log sequences have changed during a month time.
after further analysis we found .
new log sequences are caused by a new added log event setting ocssession as .
other new log sequences are caused by brand new execution paths.
.
.
processing noise.
processing noise is introduced during collection retrieval and pre processing of log data which are not truly anomalies of the system.
for example inaccurate log parsing can hamper the performance of an anomaly detection model .
log parsing is one of early steps in log data analysis.
he et al.
evaluated four commonly used log parsers including slct iplom lke and logsig .
zhu et al.
tested log parsers on a total of datasets.
they all found that existing log parsers are not sufficiently accurate.
these log parsers could perform very differently on different datasets.
for example the logsig parser could achieve an accuracy of .
in hdfs dataset but only .
in bgl dataset .
in logsig can achieve accuracy of .
in proxifier dataset but only .
in linux dataset.
more importantly he et al.
also pointed out that log mining is sensitive to some critical events .
they claimed that errors in parsing could even cause an order of magnitude performance degradation in anomaly detection from to .
.
we also perform a study on service x system using a commonlyused log parser drain .
we randomly sample service x log messages generated from log events and apply drain to parse them.
we find that log messages are incorrectly parsed which account for .
of the total log messages.
most of parsing errors are caused by missing adding a few keywords from into log events.
figure illustrates two examples of parsing error in service x log data.
the ground truth log events and parsing results are given respectively.
case log parser omits one keyword resize compared to the ground truth log event.
case log parser misidentifies parameter rtc as a keyword of the log event.
these parsing errors lead to many extra log events which are not conducive to the follow up analysis.
case httprequestasync ensurebuffer allocated resize httprequestasync ensurebuffer allocated case skipping update of with app alias skipping update of with app alias figure examples of log parsing errorraw log messageslog msg.
seq.
receiving block blk .
.
.
packet responder for.
.
.
receiving block blk .
.
.
.
.
.
.
.
.
.
.
.log msg.
seq.
receiving block blk .
.
.
packet responder for.
.
.
receiving block blk .
.
.
.
.
.
.
.
.
.
.
.log msg.
seq.
receiving block blk .
.
.
packet responder for.
.
.
receiving block blk .
.
.
.
.
.
.
.
.
.
.
.parse log sequenceslog event receiving block s packet responder receiving block s served block to .
.
.log event receiving block s packet responder receiving block s served block to .
.
.log event receiving block s packet responder receiving block s served block to .
.
.count log count vectorscount vector count vector count vector train detection modelsim pca svm lr .
.
.
figure the overview of traditional approaches .
limitation of existing methods over the years there are many studies focused on log based anomaly detection .
some exemplary approaches are as follows im lou et al.
proposed invariant mining im to mine the invariants linear relationships among log events from log count vectors.
those log sequences that violate the invariants are considered as anomalies.
pca xu et al.
constructed normal space and anomalous space of log count vectors using principal component analysis pca to detect anomalies.
if a log count vector is far from the normal space it is considered as an anomaly.
svm and lr represented log sequences as log count vectors and applied supervised learning algorithms to detect anomalies.
in their papers they train the classifier models using support vector machine svm and logistic regression lr .
the existing approaches mentioned above have many common characteristics.
figure illustrates the overview of these approaches.
they all transform the log sequences into log count vectors then build unsupervised or supervised machine learning models on it to detect anomalies.
here a log count vector holds the occurrence of each log event in a log sequence.
the dimension of the log count vector is equal to the number of distinct known log events.
due to the log instability issue the limitation of this method is obvious.
firstly it is incompatible with unstable log events.
even if only one unstable log event occurs e.g.
caused by evolving logging statement or parsing error the dimension of log count vector must be changed and the model also needs to be retrained accordingly.
secondly log count vector only counts the number of log events thus it ignores the context information embedded in the log sequences and cannot identify the different importance of various log events.
thirdly to update the log based anomaly detection tools continuous model re training is required which could incur unacceptable cost for a large scale software system that is under active development and maintenance.
therefore it is also impractical to update the existing anomaly detection tools in an online manner.
as a consequence the existing approaches will either fail to work or result in low detection accuracy.
logrobust an approach to robust log based anomaly detection to overcome the instability problem of real world log data we propose logrobust a novel log based anomaly detection approach.
the overview of logrobust is shown in figure .
the first step is log parsing.
after that logrobust does not rely on the log count 810robust log based anomaly detection on unstable log data esec fse august tallinn estonia training log datanew log data log msg.
seq.
receiving block blk .
.
.
packet responder for.
.
.
receiving block blk .
.
.
.
.
.
.
.
.
.
.
.log msg.
seq.
receiving block blk .
.
.
packet responder for.
.
.
receiving block blk .
.
.
.
.
.
.
.
.
.
.
.log msg.
seq.
receiving block blk .
.
.
packet responder for.
.
.
receiving block blk .
.
.
.
.
.
.
.
.
.
.
.log event receiving block s packet responder receiving block s served block to .
.
.log event receiving block s packet responder receiving block s served block to .
.
.log event receiving block s packet responder receiving block s served block to .
.
.parsing log parsing log event receiving block s packet responder receiving block s served block to .
.
.log event receiving block s packet responder receiving block s served block to .
.
.log event receiving block s packet responder receiving block s served block to .
.
.sem.
vector .
.
.sem.
vector .
.
.sem.
vector .
.
.pre processing word vectorization tf idf aggregation semantic vectorization x1 lstm lstm attentionx2 lstm lstm attention attention based bi lstmtrain predict figure the overview of logrobust vector like what the existing methods do.
instead it transforms each log event into a vector by considering its semantic information.
we call it semantic vectorization .
in this way even though a log event is changed during the evolution or introduced by processing noise it can be still represented as a similar vector to the original log event.
therefore our approach is able to handle unstable log events.
after semantic vectorization logrobust leverages the attention based bi lstm neural network to detect the anomalies which can capture the contextual knowledge of log sequence and learn to assign various degrees of importance to different log events.
in this way our approach is able to handle unstable log sequences.
overall logrobust consists of three steps log parsing section .
semantic vectorization section .
attention based classification section .
.
also we present the usage of logrobust in section .
.
.
log parsing since raw log messages are unstructured data and contain much specific information e.g.
ip address file name etc.
that can hinder automatic log analysis logrobust needs to parse each log message to extract its log event by abstracting away the parameters in the message.
in this way the log messages become structured data which facilitates follow up analysis.
considering accuracy and efficiency logrobust adopts drain method to conduct log parsing.
drain is proposed in and is characterized by its high parsing accuracy and efficiency.
in fact it achieves the best performance compared with other related methods evaluated in .
thus we choose this method to conduct log parsing.
for example in figure the first raw log message receiving block blk 7503483334202473044 src .
.
.
dest .
.
.
is parsed into the log event receiving block src dest .
it is worth noting that the parsing process of drain may also introduce the noise due to its inaccuracy but our approach can handle this problem to be described in the following subsections .
.
semantic vectorization logrobust extracts the semantic information of log event and transforms each log event into a fixed dimension vector we call it semantic vector regardless of whether the log event exists before.
the workflow of semantic vectorization is shown in figure which consists of three steps pre processing of log event word vectorization and tf idf based aggregation.
.
.
pre processing of log events.
to capture the semantics of log events logrobust treats a log event eas a sentence in natural language denoted as s where ti i represents thei th token and nis the length of the log event sentence.
most tokens are valid english words which have their own meanings.
however there are also some non character tokens and many variable names in a log event.
logrobust conducts pre processing for each log event sentence as follows.
we firstly remove all non character tokens from log event sentences s such as delimiters operators punctuation marks and number digits.
then we remove all the stop words such as a the etc.
finally some variable names in log events are actually a concatenation of words.
for example the variable name typedeclaration contains two words type and declaration and the variable name iscommitable is composed of two words is and commitable .
logrobust splits these composite tokens into individual tokens according to camel case .
.
.
word vectorization.
after pre processing logrobust transforms each log event sentence sinto a semantic vector v. the transformation should satisfy the following two requirements discrimination semantic vectors should be able to represent different log events with high discrimination.
for example receiving block src dest and packetresponder for block terminating are two different log events thus their corresponding semantic vectors should be different.
formally it means that the cosine similarity between the two vectors should be low.
compatibility semantic vectors should be able to identify unstable log events with similar semantics.
for example when a log event is changed from receiving block src dest to receiving block src dest time content during the evolution both of them actually have similar semantics.
therefore it is important to represent them as similar vectors.
in order to meet the above two requirements logrobust integrates the semantic information of log events into the vectorization.
specifically logrobust leverages off the shelf word vectors which were pre trained on common crawl corpus dataset using the fasttext algorithm to extract the semantic information from the english vocabularies.
fasttext can sufficiently capture the intrinsic relationship i.e.
semantic similarity among words in natural language and map each word to a d dimension vector where d 300in fasttext word vectors .
after replacing words with corresponding vectors a log event sentence sis transformed into word vectors list l where vi rd i denotes the word vector nis the number of tokens in a log event sentence.
811esec fse august tallinn estonia xu zhang et al.
semantic vector seq.
3semantic vector seq.
semantic vector seq.
semantic vector seq.
... ...log sequence log sequence alog sequence log sequence receiving block src dest receiving block src dest served block to ... ... log sequences semantic vector sequencesreceiving block src dest pre processing tf idf dest src block receiving dest src block receiving word vectorization .
.
.
.
.
.
.
.
w4w3w2w1 w4w3w2w1 .
.
.
.
w4w3w2w1 v4v3v2v1 v4v3v2v1 wivi figure the work flow of semantic vectorization .
.
tf idf based aggregation.
next logrobust represents a log event as a fixed dimension vector by aggregating all nword vectors in l. in this way even though the numbers of words i.e.
n in different log events are varying the dimension of the semantic vectors are fixed i.e.
d .
here logrobust applies weighted aggregation using tf idf which is a widely used method in information retrieval.
the tf idf weight can effectively measure the importance of words in sentences which exactly serves the requirement of high discrimination.
for example if the word block appears frequently in a log event it means that this word may be more representative for this log event.
we thus use the term frequency tf to describe its importance where t f word word total word is the number of target word in a log event total is the number of all words in a log event.
on the other hand if the word block appears in all log events it becomes too common to be able to distinguish those log events and thus its weight should be reduced.
therefore we also utilize the inverse document frequency idf as the metric where idf word log l lword lis total number of all log events and lword is number of log events containing target word.
for each word its tf idf weight wis calculated by tf idf.
finally we can obtain the semantic vector v rdto represent a specific log event by summing up the word vectors in lwith respect of tf idf weights according to the eq.
.
v nn i 1wi vi the semantic vector is able to identify the semantically similar log events and also distinguish different log events.
in this way logrobust is able to handle the instability of log data.
.
attention based classification through semantic vectorization each log event eis transformed into a semantic vector v. at the same time each log sequence is accordingly represented as a list of semantic vectors like we call it as semantic vector sequence .
taking such a semantic vector sequence as input logrobust adopts the attention basedsemantic vector sequence pred softmax w tht v1 lstm lstm h1 fc 1hf 1hb 1v2 lstm lstm h2 fc 2hf 2hb 2vt lstm lstm ht fc thf thb tvt lstm lstm ht fc thf thb tforward.
.
.
.
.
.
backward.
.
.
.
.
.
.
.
.
.
.
.
figure attention based bidirectional long short term memory neural network as anomaly detection model bi lstm neural network for anomaly detection to cope with the unstable log sequences.
the lstm model a variant of the recurrent neural network rnn is specifically designed for sequential data.
it is able to capture the contextual information of the sequence because it belongs to a class of artificial neural network where connections between nodes form a directed graph along a temporal sequence.
this allows it to exhibit temporal dynamic behavior.
lstm consists of an input layer a hidden neurons layer and an output layer.
bi lstm splits the hidden neurons layer of a standard lstm into two directions the forward pass and the backward pass in order to capture sufficient information of input log sequences in both directions.
as shown in figure hf tandhb tare hidden state vectors at time step tin forward pass or backward pass respectively where time step trepresents the position of the input log sequence.
we concatenate both hidden states as htto capture the information from both directions i.e.
ht concat hf t hb t .
since different log events have different impacts on the classification result we introduce the attention mechanism to the bi lstm model to assign different weights to log events.
in this way the impact of log data noise can be also reduced since those noisy log events tend to have less importance and are more likely to be given low attention.
the importance can be learned automatically from the attention layer.
more specifically we add a fully connected layer i.e.
fc layer in figure as the attention layer to the concatenated hidden state htand its output is the weight of attention denoted as which reflects the importance of a log event.
the larger the is the more the model pays attention to this log event.
the computation of is shown in eq.
where wa tis the weight of the attention layer at time step t. t tanh w t ht finally we sum all the hidden states with respect to all these and then construct a softmax layer to output the classification result.
as shown in eq.
wis the softmax layer weight and tis 812robust log based anomaly detection on unstable log data esec fse august tallinn estonia the total length of the log sequence.
pred softmax w t t t 0 t ht!!
during the training phase we use the prediction outputs and the ground truth provided by datasets to calculate the cross entropy as the loss function.
through this loss function we utilize the stochastic gradient descent algorithm to train the parameters of the model including the weights of bi lstm and attention layer .
.
usage of logrobust following the above steps logrobust can build a robust model for anomaly detection on unstable log data.
as depicted in figure given a set of new log data we firstly conduct log parsing.
after that for each log sequence to be detected logrobust transforms each log event in the log sequence into a semantic vector.
the log sequence is represented as a list of semantic vectors accordingly.
then we feed it into the trained model.
finally the attention based bi lstm can predict whether this sequence is an anomaly or not.
experiment in this section we evaluate our approach by answering the following research questions rq1 how effective is the proposed logrobust approach on unstable log data?
rq2 how effective is the attention mechanism in the proposed logrobust approach?
rq3 how effective is the proposed logrobust approach on stable log data?
.
experimental design .
.
datasets.
we evaluate our proposed logrobust on three datasets including the original hdfs dataset the synthetic unstable hdfs datasets and the real world industrial dataset collected from microsoft.
hdfs data the hdfs dataset is a commonly used benchmark for log based anomaly detection .
it is produced through running hadoop based map reduce jobs on more than amazon s ec2 nodes and labelled by hadoop domain experts.
in total log messages are generated from log events.
these log messages form different log sequences according to their block id among which about .
indicate system anomalies.
more details of this dataset can be found in .
we randomly collect normal log sequences and anomalous log sequences from the original hdfs dataset as the training set .
the synthetic hdfs data in order to show the effectiveness of our approach in dealing with the instability of log data we have created unstable testing datasets based on the original hdfs dataset.
we mainly simulate two kinds of log instability as illustrated in figure .
we synthesize the unstable log data according to the experience we obtained from our empirical study as described in section .
.
we believe that the synthetic log data can reflect the unstable characteristics of real world logs.
unstable log events as described in section introduction developers often insert remove some words when they update a logging statement in the source code.
thus we willencounter evolving new log events.
furthermore there are also many pseudo new log events produced from the inaccurate log parsing.
we create a set of synthetic log events by randomly inserting removing a few words into from the original log events in advance.
the synthetic log events do not significantly change the semantic meaning of the original ones therefore the corresponding anomaly label status is not affected.
we inject these synthetic log events into the original log data according to a specific ratio.
unstable log sequences log sequences are likely to be changed during the process of log evolution or collection.
in order to simulate the unstable log sequences we randomly remove a few unimportant log events which do not affect the corresponding anomaly status labels from the original log sequences.
we also randomly select an unimportant log event and repeat it several times in a log sequence or shuffle the order of a few events.
we inject these synthetic unstable log sequences into the original log data according to a specific ratio.
packetresponder for block terminatingoriginal log event packetresponder for block terminating time add words to log event packetresponder block terminatingforremove words from log event a synthetic log events event event event event event 5original log sequence event event event event event 5delete events in sequence event event event event event 3shuffle events in sequence event event event event event event 5duplicate events in sequence b synthetic log sequences figure synthetic log examples on hdfs dataset to prepare for the synthetic dataset we randomly collect log sequences from the original hdfs dataset consisting of normal and anomaly sequences.
the percentage of anomalies is which is close to that of the original hdfs dataset.
we inject the unstable log data into it and create two testing sets newtesting and which contains injected unstable log events and unstable log sequences respectively.
table summaries the synthetic unstable hdfs datasets.
table the synthetic hdfs dataset setunstable eventunstable seq.normal anomaly total training no no newtesting1 yes no newtesting2 no yes 813esec fse august tallinn estonia xu zhang et al.
microsoft s data apart from the public hdfs data we also collect the real world industrial log data from microsoft termed as service x dataset.
service x is a geographically distributed webbased online service serving millions of users.
millions of log sequences about hundreds of terabytes of raw log message data are generated online by service x every day.
service x is a fast evolving service which is deployed in a week level frequency and conducts many a b tests for new features in each deployment.
therefore the system constantly introduces new execution paths and yields unprecedented log events.
the service x dataset consists of logging messages collected on two days spanning about one month.
the number of log messages in these two sets is and respectively.
the data is labelled by the support engineers of service x and contains a small percentage of anomalies.
thus it is an imbalanced dataset.
for confidentiality reasons we do not disclose the number of anomalies here.
.
.
implementation and environment.
the neural network of logrobust is trained using the stochastic gradient descent sgd algorithm .
we use a weight decay of .
with a momentum of .
and set the initial learning rate to .
.
we use the crossentropy as the loss function.
the size of mini batches is set to .
we terminate the training process after epochs.
we build our model based on keras toolbox using an nvidia tesla m40 gpu.
.
.
evaluation metrics.
to measure the effectiveness of logrobust in anomaly detection we use the precision recall and f1 score as metrics.
we calculate these metrics as follows precision the percentage of log sequences that are correctly identified as anomalies over all the log sequences that are identified as anomalies by the model precision tp tp fp recall the percentage of log sequences that are correctly identified as anomalies over all abnormal log sequences recall tp tp fn f1 score the harmonic mean of precision andrecall .
tpis the number of abnormal log sequences that are correctly detected by the model.
fpis the number of normal log sequences that are wrongly identified as anomalies by the model.
fnis the number of abnormal log sequences that are not detected by the model.
.
rq1 experiments on unstable log data .
.
experiments on the synthetic hdfs dataset.
we train logrobust on the original hdfs data i.e.
the training set in table .
then we test the trained model on the synthetic dataset with new log events injected i.e.
the newtesting1 set .
we compare the results of logrobust and four traditional approaches including svm lr im and pca .
as the discussion in section .
traditional approaches cannot take new log events as input because the dimension of log count vectors is related to the fixed number of original log events.
even if only one new log event appears these traditional approaches also fail to work.
for the sake of comparison we treat all new log events as an extra special dimension in log count vector as we cannot know the number of new log events in advance.
in this way the related methods could still work on the dataset containing unstable log events.table experiment results on synthetic hdfs dataset of unstable log events the newtesting1 set injection ratio metric lr svm im pca logrobust precision .
.
.
.
.
recall .
.
.
.
.
f1 score .
.
.
.
.
precision .
.
.
.
.
recall .
.
.
.
.
f1 score .
.
.
.
.
precision .
.
.
.
.
recall .
.
.
.
.
f1 score .
.
.
.
.
precision .
.
.
.
.
recall .
.
.
.
.
f1 score .
.
.
.
.
the experimental results on the newtesting1 set are shown in table .
it can be seen that logrobust performs much better than other approaches.
with the increasing injection ratio of unstable log events the performance of the related approaches has declined in different degrees.
however logrobust still maintains a high accuracy even under a high injection ratio.
for example logrobust can still achieve an f1 score of .
when the injection ratio is when of original log events have been replaced by synthetic unstable log events .
it confirms that our approach is robust enough to the unstable log events.
the reason is that logrobust is able to capture the semantic information embedded in log events through semantic vectorization so it can identify unstable log events with similar semantics meaning.
nevertheless traditional approaches cannot support previously unseen log events thus they are not able to achieve satisfying results.
table experiment results on synthetic hdfs dataset of unstable log sequences the newtesting2 set injection ratio metric lr svm im pca logrobust precision .
.
.
.
.
recall .
.
.
.
.
f1 score .
.
.
.
.
precision .
.
.
.
.
recall .
.
.
.
.
f1 score .
.
.
.
.
precision .
.
.
.
.
recall .
.
.
.
.
f1 score .
.
.
.
.
precision .
.
.
.
.
recall .
.
.
.
.
f1 score .
.
.
.
.
secondly we conduct an experiment on the synthetic hdfs dataset with injected unstable log sequences the newtesting2 set .
we train logrobust on the training set and use the trained model to detect anomalies in the newtesting2 set .
the results are shown in table .
logrobust performs best under all injection ratios.
even when the injection ratio is i.e.
of log sequences suffer from missed duplicated or shuffled problem we still get a good 814robust log based anomaly detection on unstable log data esec fse august tallinn estonia f1 score of .
.
the reason is that our approach uses attentionbased bi lstm as the classification model.
it takes the contextual information embedded in a log sequence into account and learns different importance of log events by the means of attention mechanism.
therefore it is robust to small variations in the sequences.
on the contrary the related approaches rely on the number of occurrences of log events.
if there are variations in the quantitative relationship among the log events the performance of the detection model will be seriously affected.
for example im highly depends on the invariants extracted from log sequences so it is very sensitive to changes in sequences.
.
.
experiment on microsoft s industrial log dataset.
in order to verify the effectiveness of logrobust in the real world industrial practice we perform an experiment on service x dataset collected from microsoft.
the service x dataset consists of logging messages of two days with one day in june and one day in july .
during this time period the log statements of service x were updated weekly.
therefore it suffers from the instability problem.
we use the data collected in june as the training set and the data collected in july as the testing set.
table shows the results.
logrobust achieves satisfactory performance with an f1 score of .
.
other approaches achieve f1 score ranging from .
.
which are much lower than that of logrobust.
in the real world scenario traditional approaches cannot cope with the unstable log data generated from the evolving system and processing noise.
logrobust however is capable of maintaining much higher performance which confirmed its practicality.
table results on the microsoft industrial dataset method precision recall f1 score lr .
.
.
svm .
.
.
im .
.
.
pca .
.
.
logrobust .
.
.
.
rq2 experiment on attention mechanism in this section we validate the effectiveness of the attention mechanism in logrobust.
we compare the results of standard bi lstm and attention based bi lstm on synthetic hdfs dataset with injected unstable log sequences i.e.
the newtesting2 set .
we illustrate f1 score in figure .
we can see that under a low injection ratio both standard bilstm and attention based bi lstm have the similar performance.
however as the injection ratio increases the advantage of attentionbased bi lstm becomes more explicit.
for example under the injection ratio of the f1 score of attention based bi lstm is .
which is much better than the f1 score of .
achieved by standard bi lstm.
the standard bi lstm without attention is greatly affected by the changes of log sequence because it cannot learn the importance of different log events while the performance of the attention bi lstm remains relatively stable.
.
rq3 experiments on the stable log data this rq evaluates whether or not logrobust can work effectively with stable log data.
we use the original hdfs dataset as the stable10 .
.
.
injection ratiof1 score bi lstm attention based bi lstm figure f1 score of the attention model on synthetic hdfs dataset of unstable log sequences the newtesting2 set dataset.
the hdfs dataset is collected from unmodified off the shelf hadoop systems so there is no evolution of logging statements in the source code.
in addition all log events of hdfs dataset are directly identified from the source code thus we can rule out the influence of the processing noise such as parsing errors.
we are confident that the original hdfs data can be regarded as a stable log dataset.
we apply logrobust and related approaches to the original hdfs dataset.
we use the same training set as shown in table .
the remaining log sequences in the original hdfs dataset are regarded as the testing set which contains log messages in total among which indicate anomalous behavior.
the experimental results are shown in table .
the recall precision and f1 score achieved by logrobust on this dataset are .
.
and .
respectively.
compared with the two typical unsupervised methods of pca and im logrobust achieves even better results.
lr and svm are two classic supervised classifiers.
they achieve similar precision but lower recall compared to our proposed approach.
the results show that logrobust can work effectively not only on unstable log datasets but also on the stable ones.
table results on the stable hdfs dataset method precision recall f1 score lr .
.
.
svm .
.
.
im .
.
.
pca .
.
.
logrobust .
.
.
discussion .
incremental updating during the system evolution the number of accumulated new log events and new log sequences could become larger and larger.
this may cause significant performance degradation if training the model on a fixed set of historical data.
in logrobust we can leverage newly arriving data to maintain the detection performance of logrobust.
since retraining a model on new data from scratch is costly logrobust can be incrementally updated to achieve consistently good performance with negligible cost.
our attention based bi lstm is trained using a back propagation algorithm and thus it can naturally be updated incrementally through the gradient descent method to fine tune its parameters automatically.
this is also one of the advantages of logrobust 815esec fse august tallinn estonia xu zhang et al.
over existing approaches .
as described in section .
the existing approaches are based on the log count vector whose dimension cannot be changed as it is constrained by the number of known log events.
thus they cannot support incremental updating.
we also conduct exploration on the synthetic hdfs dataset under high injection ratio of unstable log events and sequences.
the performance of the model also drops significantly when there are very large changes in the dataset f1 score from .
to .
.
however after incremental updating the performance of logrobust can be greatly improved to .
.
.
threats to validity we have identified the following threats to validity subjects in this work we only use datasets collected from hdfs and service x system.
although hdfs is a typical open source project and service x is a large scale realworld software system in microsoft the number of subject systems is still limited.
in the future we will experiment with logrobust on a variety of datasets.
drastic changes our approach supports software systems that are under active maintenance including the scenarios of continuous delivery and deployment.
however if there are sudden drastic changes to the entire code base or logging mechanism and logrobust is not updated incrementally the performance of logrobust would drop significantly.
diversity of changes in real world systems there is a wide variety of changes to log events and log sequences.
in our synthetic data creation we only consider some common types occurred in the studied industrial systems.
in our future work we will explore other possible types of changes.
related work .
log based anomaly detection logs are widely used in practice for postmortem analysis.
there have been a lot of studies on log based anomaly detection .
the current approaches are mainly divided into three categories supervised learning methods unsupervised learning methods and deep learning methods.
many supervised learning methods are applied to log based anomaly detection.
for example trained an svm classifier to detect failures using event logs.
leveraged the decision tree model to detect anomalies in application operations.
proposed a regression based method to detect anomalies using log data in cloud systems.
summarized some classical supervised classification models that are applied to log based anomaly detection.
as described in section ii these conventional approaches are not robust to unstable log data which significantly restricts their applicability in real world practice.
apart from supervised learning approaches many unsupervised learning approaches have been proposed.
for example lou et al.
proposed invariant mining im to mine the invariants linear relationships among log events from log event count vectors.
those log sequences that violate the invariant relationship are considered as anomalous samples.
xu et al.
constructed normal space and abnormal space of log event count matrix using principalcomponent analysis to detect anomalies.
lin et al.
and he et al.
designed clustering based methods to identify problems of online service systems.
unsupervised learning approaches have an advantage that they do not require manual labels in the training set.
however as shown in this paper they are not robust enough to the instability of log data either.
the recent rise of deep learning methods has given a new solution for log based anomaly detection.
used lstm to predict the anomaly of log sequence based on log keys.
also used lstm to forecast the next log event and then compare it with the current ground truth to detect anomalies.
trained a stacked lstm to model the operation log samples of normal and anomalous events.
however their input of neural networks is the one hot vector of log events.
thus it cannot cope with evolving log data especially in the scenario when new log events appear.
some studies have leveraged nlp techniques to analyze log data based on the idea that log is actually a natural language sequence.
proposed to use lstm model and tf idf weight to predict the anomalous log messages.
used word2vec and traditional classifiers like svm and random forest to check whether a log event is an anomaly or not.
combined various attention based models and word vector to detect anomalous log events.
however these approaches only focused on the granularity of log events rather than log sequences.
they ignored the contextual information in log sequences.
in our work we take the entire sequence into account and train an attention based bi lstm model on the log sequence rather than a single log event.
.
logging practice and log data quality recently much research has been devoted to the logging practice and log data quality .
for example zhu et al.
have performed an empirical study of logging practice in microsoft.
they found that developers could adopt different ways of performing logging and there are no ground truth logging methods.
yuan et al.
reported the characteristics of logging modifications by investigating the revision histories of open source software systems.
he et al.
analyzed the quality issues of log parsers.
they observed that many existing log parsers are not sufficiently accurate or efficient which make them ineligible for log parsing in modern systems .
moreover the predefined parameters required by the parsers limit the robustness of the online parsers against the logging statement updates.
conclusion over the years many log based approaches have been proposed to detect anomalies of large scale software systems .
however the existing approaches cannot handle the instability of log data which comes from the evolution of logging statements and the processing noise.
to overcome this problem in this paper we propose a new log based anomaly detection approach called logrobust.
our approach represents a log event as a fixed dimension semantic vector and utilizes an attention based bi lstm classification model to detect anomalies.
we have evaluated the proposed approach using both synthetic public log data and real world industrial data from microsoft.
the experimental results confirm the effectiveness of logrobust.
816robust log based anomaly detection on unstable log data esec fse august tallinn estonia