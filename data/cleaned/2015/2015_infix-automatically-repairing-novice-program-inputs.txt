infix automatically repairing novice program inputs madeline endres computer science and engineering university of michigan ann arbor mi usa endremad umich.edugeorgios sakkas computer science and engineering uc san diego la jolla ca usa gsakkas eng.ucsd.edubenjamin cosman computer science and engineering uc san diego la jolla ca usa blcosman eng.ucsd.edu ranjit jhala computer science and engineering uc san diego la jolla ca usa jhala cs.ucsd.eduwestley weimer computer science and engineering university of michigan ann arbor mi usa weimerw umich.edu abstract this paper presents infix a technique for automatically fixing erroneous program inputs for novice programmers.
unlike comparable existing approaches for automatic debugging and maintenance tasks infix repairs input data rather than source code does not require test cases and does not require special annotations.
instead we take advantage of patterns commonly used by novice programmers to automatically create helpful high quality input repairs.
infix iteratively applies errormessage based templates and random mutations based on insights about the debugging behavior of novices.
this paper presents an implementation of infix for python.
we evaluate on unique scenarios with input related errors collected from four years of data from python tutor a free online programming tutoring environment.
our results generalize and scale compared to previous work we consider an order of magnitude more unique programs.
overall infix is able to repair .
of deterministic input errors.
we also present the results of a human study with participants.
surprisingly this simple approach produces high quality repairs humans judged the output of infix to be equally helpful and within of the quality of human generated repairs.
index t erms input repair novice programs human study i. i ntroduction novice programmers are increasingly turning to online resources beyond the traditional classroom to learn computing .
but even as demand soars for such resources the educational support provided by online tools leaves much room for improvement especially for those students who need the most help .
free tutoring environments such as python tutor seek to close this gap by providing educational support beyond structured course assignments.
however such sites can still suffer from low retention section iv reducing their ability to help students in practice.
we hypothesize that one reason for this low retention is the frustration novices face without instructional support the time spent debugging a single error has been shown to correlate with student frustration .
although student errors extend from simple syntactic mistakes to more involved semantic errors we observe that a surprisingly large portion are input related e.g.
entering instead of .
.
therefore we choose to focus on providing novices with rapid debugging hints and support for input related errors to decrease debugging time.
independently there is limited research into using sourcelevel automatic program repair and fault localization techniques for pedagogical purposes .
from reviewing over six million python tutor submissions we observe that of student interactions involve user input as well as source code.
we also find that python tutor users fixed .
of interpreter errors by only modifying input data.
unfortunately heavyweight expert focused automatic program repair tools and their derivatives such as genprog and angelix both focus on source code transformations and also are unhelpful and confusing for novices .
therefore novices facing input related errors are not well served by extant automatic program repair tools and must instead rely on manual debugging a time consuming endeavor.
a technological solution to this problem should find repairs quickly to fit in the student s workflow section iv a cf.
and be helpful for novices.
we observe that novice repairs are generally short and thus propose to use algorithms that explore the search space of nearby edits.
we also note that learners errors are not uniformly distributed and many student programs that show defects with the same error message can be fixed using the same mutation.
therefore a small number of indicative templates can increase search speed.
finally we note that the structure of student inputs can be unexpectedly complex programs often contain interdependent input values making a randomized approach surprisingly effective.
based on these insights this paper presents infix a randomized search algorithm for automatically repairing program inputs for generic novice programs.
infix uses languagespecific error message templates combined with additional randomized mutations to iteratively search for input repairs.
infix does not require test cases or large amounts of training data instead using the implicit specification of eliminating in3992019 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
terpreter errors .
infix also admits a pleasingly parallelizable implementation improving efficiency.
we evaluate a python implementation1of infix.
our error message and mutation templates are developed from our observational study characterizing novice python input patterns and errors.
in total we abstract five error message templates and five additional simple mutations.
each error message template corresponds to a single interpreter message.
we evaluate our implementation on input related scenarios arising from unique programs from four years of python tutor data section vi b .
each scenario contains a program and input that when run generate an input related error.
overall we find that with just five error message templates and five mutations infix repairs .
of input related scenarios.
as infix is aimed toward helping novices debug it is essential that the repairs are helpful and of high quality.
previous work has shown that student generated source repairs and expert human tutor input hints can be helpful for students by decreasing debugging time and increasing learning .
therefore we compare infix s repairs to those developed by the learners themselves.
from a human study involving participants we find that infix produces high quality repairs.
specifically participants find infix s repairs as helpful as human repairs and within of their quality in a statistically significant manner.
we conduct this study with both undergraduates and crowdsourced amazon mechanical turk workers.2we also find that of our participants often experience input related errors in their own programming.
in summary the main contributions of this paper are infix a novel template based search algorithm for repairing erroneous input data for novice programs a characterization of common novice input patterns and input repairs for python programs a python implementation of infix based on our characterization that fixes .
of input related errors in one second each on average the results of an irb approved human study with participants indicating that infix s repairs are within the quality of and equally helpful as student repairs ii.
m otiv a ting examples in this section we present two novice python scenarios with input related errors.
we adapt both examples from actual student programs submitted to python tutor.
these examples demonstrate the difference between syntactic and semantic input related errors providing motivation for infix s hierarchical use of error message templates and random mutations.
the scenario in figure exemplifies a syntactic inputrelated error explainable as a misunderstanding of python language behavior.3the program in this example accepts a float from the user and carries out a calculation based on this input value.
python s input call accepts floats using weimerw data infix 3all code examples in this paper use python .
we note however that input has the same behavior as raw input in python .program code 1x float input 2print x math.e erroneous input student repair infix repair .
.
python error message valueerror could not convert string to float fig.
example of a syntactic input related error.
period based decimal notation.
however the student entered into the interpreter using a comma instead of a period.
because of this the python interpreter is unable to cast the input to a float and throws a valueerror .
notice that in this simple case the input that needs to be modified to fix this error is included in the error message itself.
specifically the error message points out that the program accepts a float but that is not a float.
while this error may appear trivial to fix for expert programmers novices can have surprising difficulty debugging even simple errors .
in fact novices may not even read error messages in some cases using their existence as a boolean indicator of success or failure.
when designing infix therefore we choose to use errormessage templates to take advantage of the copious information some messages include.
in particular we observe that error messages associated with syntactic mistakes contain the richest information for algorithmic repairs.
our implementation includes a template that specifically addresses valueerror s like those in figure .
for this scenario our template is effective participants find infix s repair equal in quality and helpfulness to the human generated repair.
despite the applicability of error message templates to simple errors many input related errors are semantic and program specific rather than syntactic.
figure exemplifies one such semantic error.
the program uses the first two inputs to build a dictionary that is then accessed using the characters in the third input.
unfortunately the novice includes incorrect values in the third input line resulting in a keyerror .
specifically the student flips the lines corresponding to the keys and to the values perhaps indicating a misunderstanding of python dictionaries.
this error is time consuming taking the student almost three minutes to fix.
this error is significantly more complex than the error from the first example any repaired input for figure must satisfy several program specific constraints.
first input b must have at least as many characters as input a .
second all characters in input c must also be in input a .
furthermore notice that unlike the example in figure the error message in figure is not a rich source of debugging hints.
however we observe that repairs similar to the student generated repair could be created by simple mutations of the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
program code 1input a input 2input b input 3input c input 4c array 5dictionary 6for iin range len input a dictionary input b 8for jin range len input c c array dictionary 10print c array erroneous input student repair infix repair abcd d d abcd badc abcd et abcd et python error message keyerror fig.
example of a semantic input related error.
original error generating input.
as a result of these observations we propose using simple mutations to repair erroneous input data when there is not enough information in the error message alone.
for these mutations we consider the input as a whitespace separated list of tokens.
in our evaluation infix finds a solution to the scenario in figure in under seconds and in our human study we find infix s repair to be equivalent in quality and helpfulness to the student s repair.
from our observations these two examples are indicative of the majority of input related errors encountered by novices in online tutoring environments.
for syntactic errors such as the error in figure we observe that novices tend to repair the same error in similar ways and that error messages are rich sources of information.
for semantic errors like the one in figure fixes are more varied and the error messages are more opaque.
these observations motivate our decision to include both error message templates and mutations in infix we use error message templates to quickly repair the most common errors and random mutations to address more complex semantic errors.
our observations also inspire our decision to prioritize one category above the other we only use mutations when there is no applicable error message template.
iii.
i nfixalgorithm infix is a randomized search optimization algorithm that iteratively modifies the original erroneous input until either a correct input is found or the maximum number of probes has been exhausted.
the key insights behind infix are that input repairs are often composed from a small number of common mutations and that these mutations are often heavily correlated to specific error messages.
furthermore for some simple specific errors student edits are highly predictable.
for the purposes of this paper since we target generic novice programs we define a erroneous input as an input that causes a program to raise an interpreter error.
in contrast we define a correct input as one that causes the program to terminate without error.
a user interaction scenario has an input related error if the programmer runs both erroneous and correct inputs with the same program not every error avoiding input need be helpful to novices we formally investigate repair quality via a human study in section vi g .
given two inputs for one program such that the first is erroneous and the second is correct we refer to the latter as a fix or repair.
in section iii a we describe the infix algorithm in section iii b we present information on template selection and in section iii c we describe a parallel version of infix.
a. infix algorithm architecture at a high level infix takes six arguments the program p the erroneous input i the original error message m a template function t a set of mutations r and a maximum number of probes n. the template function t s domain consists of a finite set of error messages which uniquely determine a corresponding input mutation.
ris a set of additional input mutations.
infix iteratively mutates iuntil it either finds a repaired input or has tried nmutations.
when successful infix returns a correct input i prime such that p i prime terminates normally without raising any exception.
the pseudocode for infix can be found in algorithm .
algorithm main infix algorithm type definitions type tokseq sequence of tokens type mutation tokseq tokseq type message error message require program p prog original erroneous input i tokseq error message m message error message template function t message mutation set of mutations r mutation set maximum number of probes n n procedure infix p i m t r n v v is a set of visited inputs fornin do if m domain t i v then mut t m else mut choose r v v i i mut i m run p i ifmisgood then break returnminimize i ifmisgood elsetimeout during each iteration infix mutates iusing either the error message template function tor a random mutation from r. these two sources of modification are hierarchical infix always applies an error message template if possible line authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
.
however if there is no transformer associated with the error message i.e.
mis not in the domain of t line or if the resulting mutation has already been considered a random mutation is applied instead.
this mutation is chosen unweighted from a set of mutation templates line .
the program pis then invoked on the modified input line .
if the run is error free than the input is minimized and the process terminates.
otherwise infix continues iterating until the probe budget has been exhausted.
any minimization approach that finds a correct fix is acceptable.
previous program repair algorithms have used various minimization methods such as delta debugging .
in section vi d we discuss the minimization method in our python instantiation.
b. template selection infix relies on the selection of templates tand mutations r. in practice the domain of tis a small set of the most common error messages.
note that in algorithm there is a single transformation associated with each error message.
infix thus works best when it is possible to identify a highlyeffective transformation for the most frequent messages.
however there are instances where input related errors that yield the same error message cannot be fixed with the same template see section v b .
therefore rshould contain errorindependent transformations associated with common student mutations.
for our python implementation of infix the transformations in tandrwere developed by characterizing novice python input related errors the results of which are discussed in section iv.
the specific templates and mutations of our implementation are described in section v. c. parallelizing infix infix s structure is pleasingly parallel.
running multiple searches in parallel can both decrease the time to first repair and also increase the likelihood of finding a repair.
in infix s parallel form each thread runs the main infix loop.
however as with similar parallel repair algorithms there is a tension between possibly repeating work on independent parallel threads and incurring overtime caused by coordination .
we propose an approach without online coordination instead each thread is seeded with a random mutation of the original erroneous input.
this increases the likelihood but does not guarantee that threads explore different areas of the search space.
our low overhead approach is critical to finding repairs online in the timescales associated with novice interactions see section iv a we empirically evaluate infix s sensitivity to the number of threads in section vi f. if multiple threads find repairs in the same iteration infix selects a repair with the highest statement coverage.
we note that parallelization is especially helpful for finding repairs that rely on random mutations rather than on error templates.
iv .
c haracteriza tion of novice python input errors in this section we present the findings from our observational study to characterize the input structure and associated novice interactions of scenarios with input relatederrors.
each input related scenario consists of a program an erroneous input and a student generated input repair as defined in section iii.
these errors make up approximately .
of all erroneous interactions on python tutor from jan to dec .
the results of this study inform our python adaptation of infix section v .
we restrict our observational study to just one year to mitigate overfitting specifically we will show that the insights and templates derived from yield an input repair strategy that generalizes across all the years from to .
in subsection iv a we present a general analysis of all scenarios to understand better the size of the programs the inputs and the messages most commonly associated with novice input related errors.
in section iv b we present observations from a more in depth manual examination of randomly selected scenarios to better understand the structure of erroneous inputs as well as the repairs students made to fix them.
these analyses find that input related errors are varied and that novice input patterns can be perhaps surprisingly complex.
we also show that some error messages are significantly more common than others and that similar errors are often fixed in similar ways.
a. erroneous input related scenarios quantitative analysis to characterize the scenarios in our data set we consider the average input and program size the number of input calls per program the time it takes users to generate repairs and the prevalence of specific error messages.
in these scenarios there are unique programs.
generally python tutor programs with input related errors are small the average program length excluding blank lines and comments is .
lines.
there is a large range however some instances have up to lines.
the average input size also varies ranging from to characters with an average of .
.
these inputs can typically be interpreted as white space separated token lists though some use custom delimiters.
on average erroneous inputs have .
tokens although we observe examples with up to .
on average there are only .
textual input calls per program although we observe programs with up to calls .
this small number of calls however does not necessarily lead to programs with simple input structures.
loops and type constraints often complicate input calls see section iv b .
befitting their varying complexities input related errors take a wide range of time for python tutor users to resolve.
the median time to solve input related errors is seconds however .
take users over two minutes to solve and .
take over seven minutes.
the most common error message associated with inputrelated errors is valueerror .
out of the scenarios valueerror s are generated in or .
.
the next most common error messages are indexerror scenarios and nameerror scenarios .
we also note that valueerror has several variations differentiated by the trailing error message text.
the most common of these are invalid literal for int with base authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
scenarios could not convert string to float scenarios and not enough too many values to unpack scenarios .
together these valueerror variations account for scenarios .
.
b. erroneous input related scenarios qualitative analysis we now present qualitative analysis of randomlysampled erroneous input related scenarios.
generally we find that inputs consist almost exclusively of string integer and float literals.
we also note that integers and floats in erroneous inputs are typically small and positive of the inputs in this sample with numerical literals only of them involve a number above and only one contains a negative.
this pattern is similar in the student generated fixes only number containing fixes have a number over .
we also note that in our sample more fixes contain numerical values than erroneous inputs do .
therefore we conclude that many errors are caused by omitting a numerical literal required by the program s input structure.
we further observe that such errors are typically resolved through the insertion of a number into the erroneous input.
subjectively we note that many erroneous inputs likely imply a misunderstanding of python data formatting.
these syntactic errors fall into two main groups erroneous stringto type conversions and mistakes involving library functions.
altogether these account for errors in our sample.
the first group involves misunderstanding how literals are represented as strings.
for example we observe novices who use commas instead of periods for decimal notation or who include quotes around numbers.
our data set contains one extreme example where the student entered math.pi to a program expecting an integer.
these errors typically result in a valueerror and students typically fix them by correcting the format.
while students often preserve the original numerical value in the repair this is not always the case see figure .
this indicates that the fix s exact numerical value is not always as important as correcting the formatting.
the second group of syntactic errors involves misunderstanding python s input andsplit behavior.
by default input reads until the next newline and split breaks strings on whitespace.
some students however may be unclear on both default behaviors.
for example we observe student attempts to use comma separated lists or to include the data for multiple input calls on the same line.
the remaining errors in our sample are semantic.
these errors are diverse ranging from simple swapped input orderings to complicated indirections.
for example one program which accepts a list of integers raises an error if there is a duplicate in the list.
student input fixes are similarly diverse.
however we note that elements in the erroneous input are often permuted in the fixed input.
we also notice that other elements are often slightly modified in the fix for example abcb toabdb .
we further observe student fixes that insert strings occurring as literals in the program s source code.
for example one program contains a dictionary with the hard coded key pollution .
however the student entered the misspelled polution inducing a keyerror .
intriguingly we find that despite a small number of static input calls in a given program exhaustively specifying the set of valid inputs can be challenging.
while scenario programs only have one static input call in .
that call is embedded in either a loop or a split resulting in a complex dynamic input structure.
we also observe that the length of one input portion is often dependent on the value of a different input portion.
we consider two indicative cases dependent list lengths and sentinel values.
figure ashows a python program where the number of times the second input is triggered depends on the value of the first input .
we find a version of this value dependence in scenarios.
figure bdepicts a sentinel loop pattern where exiting the loop requires a specific input value.
we observe instantiations of pattern bin of sampled programs.
we further note that errors involving these patterns often relate to value interdependencies.
for example many indexerror s were caused when one input was used as an index into a structure created by another input.
students typically resolve these errors by inserting or deleting tokens.
finally we observe that student fixes for input related python errors are generative as well as corrective.
as python is interpreted no more input is accepted once an uncaught error is thrown.
therefore when a program with multiple input calls aborts after the first call a fix must both correct this first error and also generate additional input data for any remaining calls.
of the scenarios we examine fixes generate additional input.
we sometimes see novices submit multiple erroneous inputs before achieving an error free program run indicating that debugging for novices is an iterative process.
an example of this pattern is shown in figure c. overall novice inputs and input related errors are varied and complex.
input related errors range from simple syntactic mistakes to more insidious semantic errors that often expose tricky interdependencies between input values and calls.
despite this variety we note that fixes often contain similar mutations of the original erroneous input.
we also find that for simple syntactic errors that result in a valueerror the rich error message is highly predictive of the eventual fix.
v. s pecializing infix to python to create python specific error message templates and mutations for infix we exploit patterns discovered in our observational study.
we design error message templates to quickly target syntactic errors and select mutations inspired by common debugging patterns for semantic errors.
section v a discusses the specific error message templates and section v b presents our mutation operators.
in total our infix implementation contains five error message based templates and five mutations.
a. error message templates in our observational study see section iv a we discover that just four subtypes of valueerror account for .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
1num int input 2v a l int x for xin input .split 3for yin len num print val a example where the length of the second input depends on the value of the first list val must be at least as long as num .
e.g.
this program accepts n1 and not n1.1while true value int input number?
ifvalue print found number break b example of a sentinel input pattern.
to terminate the input must be exactly equal to .1name input name?
2age float input age?
3num int input fave num?
try input error bob valueerror bob .
.
valueerror bob no error c example of iterative debugging process.
note for space input shown is whitespace separated.
fig.
common python input and debugging patterns for novice programmers.
of all input related errors encountered by novices on python tutor.
we further find that many of these valueerror s correspond to syntactic errors such as incorrect python type formatting or misunderstood input andsplit behavior noting that students fix these errors in predictable ways.
additionally we realize that novice fixes for input related errors are often necessarily generative.
based on these observations we implement five error message templates four that address the most common valueerror subtypes and one that addresses eoferrors generated when the program runs out of input.
table i shows our error message templates.
b. mutation templates while our error message templates directly address a significant portion of syntactic errors they do not cover all errors.
in our observational study we found that the student fixes could often be generated by applying a small set of mutations.
from this finding we propose four mutations inserting a new token transforming a space separated list into one separated by newlines reordering tokens and deleting tokens.
while many fixes are similar to the original input we also observed instances where they were seemingly unrelated.
as a result we create a fifth mutation that clears the given input.
table i summarizes the mutations in our infix implementation.
vi.
e v alua tion this section contains the experimental setup and results of our infix implementation evaluation.
section vi a outlines our research questions section vi b describes the expanded python tutor data set we used for our evaluation section vi c describes our human study methodology and the remaining subsections present the results of our investigations into each research question.
a. research questions in our evaluation we focus on five research questions rq1 how effective is infix at repairing python inputrelated errors?
rq2 are the assumptions behind infix such as the benefits of the hierarchical ordering between error messages and templates valid?
rq3 how sensitive is infix to available resources?
rq4 what are the quality and helpfulness of the repairs produced by infix as judged by humans?
rq5 how do the perceived helpfulness and quality of these repairs vary with programmer expertise?
b. benchmark python tutor data set our first evaluation benchmark consists of erroneous input related scenarios collected from four years of python tutor data jan to dec .
each scenario consists of a python program an error causing input and a studentgenerated repair.
by year there are scenarios from from from and from .
this data is an expansion of the data used in our observational study described in section iv.
to mitigate overfitting we only used a subset of the data earlier.
across these scenarios there are submissions from unique ip addresses.
we find that .
of users are single time users only ever submitting one input related error to python tutor.
c. benchmark repair quality human study our second data source is an irb approved human study with participants.
this data includes quality and helpfulness ratings for infix and student generated repairs for scenarios randomly selected from the python tutor data without adaptation.
of the participants are undergraduate or graduate students at the university of michigan.
the remaining are workers recruited from amazon mechanical turk mturk .
these participants have varying levels of selfreported python programming experience.
each participant was shown an online series of novice python programs randomly selected from the stimuli corpus.
.
each stimulus consists of a python program erroneous input error message repaired input and repaired output.
there are two versions of each stimulus one with the historical student generated repair and one where the repair is generated by infix.
to avoid training effects a single participant was never shown both the machine and human repair for the same error.
for all stimuli participants were asked to provide a textual description of the cause of the error and to assess the quality and helpfulness of the repair on a likert scale of to .
to collect data that best reflected our participants 4all stimuli are available at weimerw data infix authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
error message template fix valueerror invalid literal for int with base x replace last instance of xwith random integer between and .
note novices mostly use small numbers in their fixes.
valueerror could not convert string to float x replace last instance of xwith random float between .
and .
.
note novices mostly use small numbers in their fixes.
valueerror not enough values to unpack append duplicate of last token.
deliminator is either whitespace or extracted from code.
valueerror too many values to unpack remove last token from input.
deliminator is either whitespace or extracted from code.
eoferror eof when reading a line append a duplicate of a random token from the original input or append a new random three character string.
mutation description insert a token inserts token at random location.
new token is a short random string token from original input or string literal from source.
split whitespace list transforms a line separated by spaces or other split deliminator from source code and into content separated by newlines.
swap a token swaps a random token with either a short random string a token from the original input or a string literal from source code.
remove a token deletes a random token.
a token may be an entire line but we consider whitespace separation when applicable.
empty the input replaces the entire input with an empty sequences.
useful for unhelpful student provided initial inputs.
table i descriptions of error message templates and mutations in infix s python implementation.
subjective human experiences we did not further define quality or helpfulness.
we also gathered self reported estimates of both programming experience and python specific experience as well as qualitative data pertaining to what factors influence a subjective judgment of repair quality.
study stimuli are very similar to figure except that participants were only shown one repair per scenario.
the study takes around minutes to complete.
a participant s response was only considered valid if it correctly identified the cause of errors.
this high threshold is relevant for trusting mturk worker ratings.
previous work shows that much of the data submitted on mturk is of low quality some users even collude to take advantage of the system .
mturk workers were compensated with .
upon successful completion while students could opt to be entered to win one of two amazon gift cards.
d. rq1 how effective is infix?
we evaluate infix on python tutor scenarios with input related errors.
for our initial effectiveness assessment we set the maximum number of probes per thread nto section iii a and the number of threads to five section iii c .
we perform a sensitivity analysis on these input parameters in section vi f. we use a simple brute force minimization technique section iii a due to the typicallyshort input length we find more heavy weight approaches unnecessary.
all experiments were conducted on an ubuntu .
.
lts server with a .
ghz intel i7 7740x quad core cpu with gb of ram.
in general our evaluation demonstrates that infix is highly effective able to repair .
of input related errors.
we also find that infix s repair rate is consistent achieving similar accuracy for each separate year of data.
as infix s templates were developed from observations of the data we believe this consistency indicates our templates generalize.
a detailed breakdown of this analysis can be found in table ii.
we also find that infix is efficient able to repair the majority of errors in under one second of wall clock time.table ii overall infix evaluation results.
all reported results are run with a probe budget and threads.
median and average probe and time costs wall clock are shown.
input error scenarios probes time sec year total repaired med.
avg.
med.
avg.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
total .
.
.
.
this is important because infix is intended to provide realtime debugging hints and repairs to novices.
infix is both highly effective and efficient repairing .
of input related scenarios in a median of .
wall clock seconds vs. median seconds for novices .
e. rq2 v alidating infix s design assumptions beyond assessing the overall effectiveness of infix we also perform an experiment to validate our design assumptions that both error message templates and randomized mutations are helpful and that error message templates should take precedence.
to do so we implement three variants of infix one with only error message templates one with only mutation templates and one with both that uses random selection instead of a hierarchical prioritization see section iii a .
we compare the performance of these variations on the python tutor scenarios using five parallel threads and maximum probes per thread.
we find that infix outperforms all three variations in repair rate average number of probes or both indicating that the error message templates mutations and their associated hierarchy all contribute to infix s high performance.
in particular the error messageonly and mutation only implementations have markedly lower repair rates than infix.
interestingly we observe that the .
of scenarios that the error message only version repairs is similar to the .
of errors we templated see section v indicating that our abstracted error message templates are highly effective.
we also note that while the non hierarchical authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii experimental results for validation of infix s design assumptions.
tested on the scenarios from .
algorithm number of percent average probes v ariation inputs fixed fixed to solve error messages only .
.
mutations only .
.
non hierarchical .
.
infix complete .
.
table iv infix s sensitivity to the maximum number of probes and the number of threads.
each box contains the percentage of the programs solved when infix is run with the specified parameters.
maximum number of probesnumber of threads .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
version s performance is only slightly lower than infix s the average number of probes is greater validating our assumption that the hierarchy between error message templates and mutations leads to increased efficiency.
a detailed breakdown of our results is given in table iii.
error message templates mutations and the hierarchical mutation structure are critical for infix s high performance.
f .
rq3 infix parameter sensitivity to understand infix s parameter sensitivity we evaluate infix with different probe budgets and parallel threads .
we choose to focus on sensitivity with respect to more constrained resources because unlike traditional automatic program repair we target real time repairs for low budget tutoring sites.
we do however include a larger probe budget to compare against previous work.
for probe budgets n we evaluate on all scenarios.
for probes we evaluate only scenarios from the python tutor data.
we find that infix s repair rate is influenced by the values of these two input parameters as the probe budget and threads increase infix s repair rate also increases.
numerical results from our sensitivity analysis are shown in table iv.
we emphasize two of our findings.
first note that even with a single probe infix repairs a large number of input scenarios.
we observe that most of these correspond to instances where the initial error message is templated.
this demonstrates that infix can still be effective even with highly constrained resources.
second even when the probe budget doubles from to the repair rate with a consistent number of threads increases by at most .
.
between and probes an 8x resource increase this pattern is even more pronounced with a maximum repair rate increase of .
.
this indicates that infix is largely insensitive to resource constraints on thetable v quality and helpfulness from human study ratings likert scale .
infix s patches are lower quality than human written patches in a statistically significant manner p .
helpfulness is not statistically distinguishable.
rated patch quality rated patch helpfulness raters human infix p value human infix p value mturk .
.
.
.
.
.
university .
.
.
.
.
.
all raters .
.
.
.
.
.
order of those bounds established by previous work e.g.
up to probes reported for three algorithms on the similarly sized introclass student program repair benchmark .
infix is insensitive to expected resource parameters and is usable even for non parallel architectures and tight resource budgets.
infix also repairs a non trivial amount of inputrelated errors in a single iteration.
g. rq4 what is the quality of infix s repairs?
as human generated inputs and code repairs have been shown to be useful hints for novices we objectively and subjectively investigate how infix s repairs compare to historical repairs made by the python tutor users themselves.
our objective evaluation compares the statement coverage of infix s repairs to the coverage of student repairs.
we choose coverage because it is a well understood and commonly used metric for software engineering quality assurance .
from analysis on the python tutor data we find that the median coverage of infix repairs .
is .
the median coverage of student repairs .
.
infix s coverage is high comparable to that achieved by pex an automated test generation tool evaluated in an educational setting and greater than that of tools such as ka tch that focus on coverage for expert written patches .
in our second evaluation we asked humans for subjective assessments of repair quality see section vi c .
we collected helpfulness and quality scores for machine and student input repairs on a likert scale between low and high .
details of our results are in table v including subgroup breakdowns for university students and mturk workers.
overall infix s repairs were as helpful as student generated repairs we found no statistically significant difference between the helpfulness of student and machine generated repairs using the two tailed mann whitney u test.
we did however find a statistically significant difference for repair quality p .
the quality of human repairs is higher than infix repairs.
this subjective quality assessment is very high compared to previous investigations of automated repairs.
while we deliberately do not define quality and helpfulness to avoid biasing responses we did ask our participants what factors cause a repair to be of high quality .
generally subjects 5for example while not directly comparable humans found par s patches .
as acceptable as human patches and genprog s .
as acceptable .
similarly long and rinard report of prophet s patches to be correct in a manual human assessment with other algorithms such as genprog and kali performing worse.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
program code 1ticket num int input how many?
2cost float input how much each?
3total ticket num cost 4print your cost is total .
erroneous input student repair infix repair input3 .
.
.
output erroryour cost is .
.your cost is .
.
python error message valueerror could not convert string to float .
fig.
example where the machine repair is of higher quality than the student repair by .
p .
.
indicate that high quality repairs are those that help with fault localization.
for example one participant stated a repair is of higher quality the quicker it helped solve the bugged input .
similarly a second participant wrote that high quality repairs provide a valuable clue .
.
.
when you follow the code and use new input the error is easier to spot .
one participant articulated that high quality repairs should exercise as much code as possible instead of giving values that short circuit checks or skip faulty code supporting our use of code coverage as a proxy for repair quality.
to further tease apart this nuanced notion of repair quality we consider two case studies with statistically significant differences between human and machine repairs.
figure shows an example where the machine repair is rated better than the human repair p .
.
this program asks the user to enter a monetary amount.
the erroneous input contains a simple syntactic error the novice includes a dollar sign with the float.
the human repair simply removes the dollar sign.
infix however suggests a different float.
perhaps unexpectedly participants find the machine repair of higher quality than the human repair.
we hypothesize this is because only the machine repair s output reveals floating point precision formatting behavior undesirable for monetary notation.
in contrast figure depicts an example where participants thought the human repair was better than the machine repair p .
.
in this program the input related error is primarily caused by a defect in the code on line the programmer incorrectly calls leap .
the student input fix includes a different year avoiding the defect due to shortcircuit evaluation.
infix however generates 2for the year.
we believe the fact that 2makes no contextual sense as a modern year to be the reason for its lower perceived quality.
in fact one participant singled out this repair as particularly poor noting that a valid date ...w ould give a better example than a wrong logically year value of .
these two examples demonstrate that the stochastic elements of infix can benefit repair quality revealing edge cases that would otherwise be missed.
however they alsoprogram code 1day int input enter a day 2month int input enter a month 3year int input enter a year 5def leap year pass removed for space considerations 8def checkday day month year ifday and month and leap return day return false print checkday day month year erroneous input student repair infix repair python error message typeerror leap missing required positional argument year fig.
example where the human repair is of higher quality than the student repair p .
.
show a limitation of template based repair we deliberately used small numerical values in our repairs based off our observations in section iv.
however this is unhelpful when the student program involves numerical inputs with other contextual constraints figure .
infix s repairs are of high quality attaining .
of the statement coverage of student repairs.
more importantly study participants found infix s repairs to be equally helpful as and to have of the quality of human repairs.
h. rq5 the effect of programmer expertise as infix is designed to help novices we are interested in the effect of programmer expertise on repair helpfulness.
we analyzed the helpfulness scores of experience based subpopulations.
in our human study see section vi c participants were asked to self report their python experience as either minimal less than a semester moderate semesters or expert semesters .
of the respondents are minimal are moderate and are experts.
while we note we use a coarse definition of programmer expertise see siegmund et al.
for a detailed discussion we claim that students with three or more semesters programming experience are relative experts compared to those who have just started learning.
initially we observed that our relative experts rated infix repairs more helpful than novices .
vs. .
out of .
however since the stimuli for our study were randomly sampled from the python tutor data set they vary in difficulty some programs contain python language features that participants with minimal experience may not have encountered.
we authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
participant experience level minimal moderate expert easiest stimuli .
.
.
hardest stimuli .
.
.
all stimuli .
.
.
table vi helpfulness ratings of infix s repairs depending on experience.
scores are on a scale from to .
hypothesize that these hard programs are confusing for our most novice participants leading to lower helpfulness ratings.
we thus analyzed the helpfulness of infix s repairs for the easiest and hardest programs as determined by three expert annotators fleiss .
.
we find that participants with the least experience give repairs for easy programs higher helpfulness ratings than they give repairs for the hardest programs .
vs. .
.
participants with the most experience however rate machine repairs for both easy and hard programs as equally helpful.
for the easiest programs there is no statistically significant difference in the helpfulness scores between novice and relative expert participants indicating that infix is helpful for novice programmers with varying experience levels.
our results are detailed in table vi.
after controlling for program difficulty infix s repairs are rated equally helpful by developers with varying python experience including novices less than a semester .
i. evaluation summary infix is highly effective and efficient enough to help students debug in real time.
infix is relatively insensitive to resource based parameters indicating that input related repair can be cost effective to deploy under constraint.
beyond its high .
repair rate and sub second efficiency we find that infix produces very high quality repairs that are helpful for novices and experts alike.
vii.
t hrea ts to validity although our experiments indicate that infix is effective and efficient our results and subjective quality data may not generalize.
we also consider that novices may feel they rarely encounter input related errors making repairs unnecessary.
we first recognize that while infix is highly effective for python our results may not generalize to other languages.
we find it likely that infix s success depends on the expressiveness of the language s error messages.
we deliberately implement infix for a language widely used by novices.
however investigating cross language effectiveness remains for future work.
to mitigate the possibility of fraudulent mturk data we require workers to correctly identify the cause of at least stimuli errors for payment.
we only considered responses meeting that threshold as assessed through manual analysis in our evaluation.
filtering for data set inclusion based on response quality is a best practice for studies involving crowdsourced participants .
while workers requested payment on mturk only met the validity threshold for inclusion.
this mturk retention rate is similar to that reported by other crowdsourced studies involving debugging .finally to mitigate threats involving problem significance we asked our study participants how strongly they agree with the phrase i often encounter bugs where the input data is part of the problem .
participants report commonly encountering input related errors responses agree or strongly agree while only three participants strongly disagree.
this result indicates that input related errors are a common and memorable challenge faced by novices.
viii.
r ela ted work a. pedagogical automatic program repair previous pedagogically motivated automatic program repair and fault localization work focuses on large course assignments e.g.
moocs rather than support for non traditional students.
for example ahmed et al.
build statistical models to help repair submissions for different problem sets using between and submissions per problem for training .
however as we focus on input related errors for generic student programs infix must operate without a large corpus of fixes for the same program.
yi et al.
study if state of the art program repair tools can feasibly help students repair source level errors .
they find that expert focused tools and their derivatives are unhelpful though they are potentially useful for course graders.
however they did not investigate input related repairs.
b. automatic input rectification sanitization or fuzzing limited work has been done on automatically repairing input data .
extant work focuses on improving security for industrial programs.
in the most related work long et al.
use provided tests to learn what non malicious inputs look like for a program .
they then automatically correct atypical inputs to fit the learned pattern.
for general novice programs however the input format is rarely specified and there are rarely test cases.
to the best of our knowledge there is no prior work on automatically repairing novice input errors or investigating their repair quality.
given a model for functions such as input split and int fuzz testing e.g.
could be applied to the task of generating non erroneous inputs.
however test input generators often struggle with real time answers to semantic or dependent input paths such as the dictionary key value scenario in figure .
while a few algorithms have efficient web deployments such as pex approaches that handle complex input constraints such as exe generally require minutes rather than seconds.
in addition while there are some evaluations of fuzzing in pedagogical contexts e.g.
as a game we are unaware of any work evaluating fuzzing quality for novice input repairs.
we view a more thorough evaluation of fuzz testing in this context as future work.
c. intelligent tutoring systems there exists a large body of work investigating and evaluating intelligent tutoring systems for learning programming .
these systems target a wide range of programming languages and experience levels.
many authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
of these systems provide data driven source code fixes to serve as general hints for learning .
for example hartmann et al.
use crowdsourcing to provide selected solutions to error messages .
others such as singh et al.
take advantage of reference implementation to provide more specific feedback .
others also use static analysis or constraint solving to provide state based hints .
in approaches similar to our own lazar et al.
provide hints using common student edits and berges et al.
characterize common novice error messages .
both however focus solely on source level errors while we focus on input related errors.
to the best of our knowledge we are the first paper to either classify or repair common novice input related errors.
d. input grammar generation while infix uses randomization to generate input repairs it only generates a single fix.
we hypothesize that automatically synthesizing input grammars for student programs could result in richer information for providing hints.
synthesizing generic input grammars remains a challenging task.
however there has been some recent work in this area .
for example bastani et al.
develop an algorithm for synthesizing contextfree input grammars .
unfortunately our characterization of novice input structures in section iv found that many are actually context sensitive.
automatically generating contextsensitive input grammars remains an area for future work.
ix.
c onclusion this paper presents infix a randomized template based approach for automatically fixing erroneous program inputs for novice programmers.
infix repairs input data rather than source code requires no test cases and requires no special annotations.
we take advantage of novice inputs patterns that we characterized in an observational study to automatically create helpful high quality input repairs.
infix iteratively applies prioritized error based templates and random mutations.
we evaluate on unique input related scenarios from over four years of data.
our results generalize and scale compared to previous work we consider an order of magnitude more unique programs.
overall infix repaired .
of input errors.
the majority were repaired in under a second facilitating real time repairs.
we also present the results of a human study with participants.
infix produces high quality repairs humans judged the output of infix to be equally helpful and within of the quality of human generated repairs.
insensitive to expected resource parameters infix is usable even for environments with tight resource budgets.