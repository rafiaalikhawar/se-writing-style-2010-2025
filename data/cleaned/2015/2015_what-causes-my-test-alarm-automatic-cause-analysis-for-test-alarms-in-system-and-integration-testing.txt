what causes my test alarm?
automatic cause analysis for test alarms in system and integration testing he jiang1 xiaochen li1 zijiang yang3 jifeng xuan4 1school of software dalian universi ty of technology dalian china 2key laboratory for ubiquitous network and service software of li aoning province dalian china 3western michigan university kalamazoo mi usa 4state key lab of software engineering wuhan university wuhan china jianghe dlut.edu.cn li mail.dlut.edu.cn zi jiang.yang wmich.edu j xuan whu.edu.cn abstract driven by new software development processes and testing in clouds system and integration testing nowadays tends to produce enormous number of alarms.
such test alarms lay an almost unbearable burden on software testing engineers who have to manually analyze the causes of these alarms.
the causes are critical because they decid e which stakeholders are responsible to fix the bugs detect ed during the testing.
in this paper we present a novel approach that aims to relieve the burden by automating the procedu re.
our approach called cause analysis model exploits information retrieval techniques to efficiently infer test alarm causes based on test logs.
we have developed a prototype and evaluated our tool on two industrial datasets with more than test alarms.
experiments on the two datasets show that our tool achieves an accuracy of .
and .
respectively which outperforms the baseline algorithms by up to .
.
our algorithm is also extremely efficient spending about .1s per cause analysis.
due to the attractive experimental results our industrial partner a leading information and communication technology company in the world has deployed the tool and it achieves an average accuracy of after two months of running nearly three times more accurate than a previous strategy based on regular expressions.
keywords software testing system and integration testing test alarm analysis multiclass classification i. introduction system and integration testing sit is necessary immediately after the integration of various software components.
with increasing number of companies advocating to conduct continuous integration by following modern software development practices such as devops the frequency of sit has significantly increased.
fortunately emerging techni ques such as testing in the cloud have dramatically improved the efficiency of such testing.
for example a cloud based system is able to run test scripts in less than minutes.
in the past the same amount of testing required hours .
since running test scripts has an average failure rate of approximately the frequent automated sit produces tremendous number of test alarms that have to be analyzed by testers.
there are various causes that may lead to test alarms such as product code defect test script defect and device anomaly.
each type of cause has its unique way to handle including submitting bug reports to devel opers correcting the test scripts and submitting excepti on messages to instrument suppliers.
therefore the analysis of test alarms is critical as it determines who is responsible to fix the potential bugs.
in order to figure out the causes testers have to carefully read test logs each of which may consist of hundreds of test steps and thousands of lin es of text .
considering the fact that thousands of test alarms may be produced per day for a production line with seve ral similar products as we have observed during collaboration with our industrial partner huawei tech inc. a leading information and communication technology company in the world test alarm cause analysis lays an almost unbearable burden on testers and has become a bottleneck in sit.
realizing the urge nt need to alleviate the burden of cause analysis our collaborators manually build regular expressions over the test logs to analyze test alarm causes.
the accuracy of their approach is about on different projects.
in this paper we present a novel approach named cause analysis model cam that infers test alarm causes by analyzing test logs.
the test logs generated by test scripts record important runtime information during testing.
the basic idea of cam is to detect the test logs of historical test alarms that may share the same causes with the new test logs.
cam first utilizes natural language processing nlp techniques to partition test logs in to terms.
next cam selects partial historical test logs for further processing with function point filtering.
thirdly cam constructs attribute vectors based on test log terms.
the cause of a new alarm is predicted according to the ranked similarity between a new test log and each historical one.
finally cam reports the causes along with the difference between the new and historical test logs.
cam is efficient as it is an information retrieval based algorithm without the overhead of training.
in the experiments we collect more than test logs forming two datasets from two industrial proj ects at huaweitech inc. cam achieves accuracy rates of .
and .
respectively outperforming baseline algorithms by up to .
.
for more than one th ird of the testing days the accuracy of cam is over .
in addition cam is very efficient taking on average about .1s per test alarm analysis with 4gb memory.
after deploying cam at huawei tech inc. it achieves an average accuracy of after two months of running which is nearly three times more accurate than their previous strategy based on regular expressions.
in summary this study makes the following contributions we propose a new approach to address the challenge of automatically analyzing the test alarm causes in sit.
ieee acm 39th international conference on software engineering ieee acm 39th international conference on software engineering .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
we construct two indu strial datasets with more than test logs.
the failure cau ses of these test alarms are manually labeled and veri fied by testers.
we conduct a series of experiments to investigate the performance of our approach.
experi mental results show that cam is both effective and efficient.
we deploy and evaluate cam at huawei tech inc. in a real development scenario.
this paper is structured as follows.
in section we introduce the background of th is study.
we describe the overall framework of cam in section .
the experimental setup and research questions are introduced in section .
we experiment to answer the research questions in section .
section and show the threats to validity and related work respectively.
finally section concludes this paper.
ii.
background in this section we present relevant backgr ound regarding system and integration testi ng and the cause analysis problem.
a. system and integration testing sit sit is performed immediat ely after various components are integrated to form an entire system.
the system under test is more complex than those individual components considered in the unit testing.
therefore sit uses a new set of test drivers for revalidation with black box testing strategies .
the function points are a set of functional requirements for a software project .
in sit testers play the role of users to work through a variety of scenarios for covering the required function points .
the functi on points of test scripts are predefined when testers develop te st scripts .
for example if a test script is designed to ve rify the function of configure network proxy testers may add netconf proxy func as the function po int of the test script.
test logs record the runtime information in software testing.
in sit testers develop test scripts also called test codes to check for system functions performance etc.
each test script contains a sequence of test steps with numerous logging statements.
test logs are generated by these logging statements when running test scripts.
a test alarm is an alarm to warn th e failure of a test script.
each test alarm is associated with a failure cause.
testers are responsible to analyze the causes of test alarms.
b. cause analysis process cause analysis for test alarms is critical due to its effect on both testers and developers .
the overall analysis procedure is depicted in fig.
.
in a software company sit is conducted over the code changes in each branch to reduce software bugs .
before developers merge code changes into a trunk branch testers select test script s of some given function points to verify the correctness of these code changes fig.
.
during the testing test scripts au tomatically log important runtime information to form test logs.
code changes are merged into a trunk branch only if they pass all the test scripts.
if a test script fails testers are required to analyze the cause to the failure fig.
.
testers analyze failure causes by examining test logs fig.
.
after detecting failure causes testers submit the test logs with the corresponding causes to the software repository for unified management fig.
.
after that different stakeholders e.g.
testers developers instrument suppliers etc.
have to resolve the failures depe nding on the types of the causes fig.
.
if a cause indicates product code defect testers need to submit a bug re port to developers and request them to fix the bug .
if it is a defect in test scripts testers need to correct test scripts.
for other causes testers may either adjust the c onfiguration files or request instrument suppliers to diagnose the infrastructures etc.
the above process may repeat several times before code changes are merged into a trunk branch fig.
.
c. cause analysis problem as shown in fig.
each test alarm a is associated with a test log l and its failure cause c which forms a triple g1731 g1827 g481 g1838 g481 g1829 g1732.
when a set of test scripts completes running several g1731 g1827 g481 g1838 g481 g491 g1732 g3020 arise for analysis.
testers analyze the causes of test alarms with their test logs and then continuously submit the g1731 g1827 g481 g1838 g481 g1829 g1732 g3020 to the software repository alo ng with the testing days.
we represent g1731 g1827 g481 g1838 g481 g1829 g1732 g3020 for analysis as g1731 g1827 g481 g1838 g481 g491 g1732 g3020 g3289 g3280 g3298 and those in the softwa re repository as g1731 g1827 g481 g1838 g481 g1829 g1732 g3020 g3283 g3284 g3294 g3295 g3290 g3293 g3300 .
following this representation the cau se analysis problem is to predict c in g1731 g1827 g481 g1838 g481 g491 g1732 g3020 g3289 g3280 g3298 with the assistance of g1731 g1827 g481 g1838 g481 g1829 g1732 g3020 g3283 g3284 g3294 g3295 g3290 g3293 g3300 which can be viewed as a multiclass classification problem due to the various failure causes c for test alarms.
the multiclass classification problem aims to classify instances into one out of more than two classes.
in this study the new test logs of test alarms are instances for classifying and their causes are the multiple classes.
despite previous studies attempt to classify test alarms into product code defect and non product code defect these techniques are not suitable for this problem since they either require expensive costs to collect complex information in test alar m a test lo g l failure cause c g1731 g1827 g481 g1838 g481 g1829 g1732 g3020 g1731 g1827 g481 g1838 g481 g1829 g1732 g3020 g3283 g3284 g3294 g3295 g3290 g3293 g3300 g1731 g1827 g481 g1838 g481 g1829 g1732 g3020 g3283 g3284 g3294 g3295 g3290 g3293 g3300 g1731 g1827 g481 g1838 g481 g1829 g1732 g3020 g3283 g3284 g3294 g3295 g3290 g3293 g3300 g1731 g1827 g481 g1838 g481 g491 g1732 g3020 g3289 g3280 g3298 anal yze g6 g2870 g1731 g1827 g481 g1838 g481 g491 g1732 g3020 g3289 g3280 g3298 g6 g2869 g6 g3015 g146 g148 g135 g134 g139 g133 g150 g135 g134 g133 g131 g151 g149 g135 figure .
cause analysis problem.
chan ge bug fix chan ges change branches build build build build product code defect device anomaly re testing select test scripts analyze test alar ms by test logs 2tester find and submit causes handle failures trunk branch figure .
cause analysis process.
teste r developer instrument supplier ... software repository software testing cloud computing authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
large integrated system or need additional efforts to decide how to deal with each non product code defect .
d. test logs and failure causes we exhibit some examples of test logs and failure causes from the industrial projects at huawei tech inc. to better understand the cause analysis problem.
the projects are launched to test the codes of tw o communication systems.
test logs logging is a conventional programming practice to record important runtime information .
in some open source software there is one line of logging code in less than lines of code on average .
when testers develop test scripts they also insert a mass of logging stat ements .
during the runs of test scripts these logging statements record some critical information to test logs.
fig.
exhibits a snippet of test logs.
test logs in these projects are bilingual documents with english and chinese terms.
in practice non english speaking testers prefer adding some native terms e.g.
chinese terms to better understand test logs.
apart from the la nguages test logs in these projects contain all the information that a test log needs .
the contents of the industrial test logs can be summarized in four types including test steps echo messages exception messages and miscellaneous messages.
a test step segment is a command or code snippet to display or verify some specific steps of the software under test.
a test script contains a sequence of test steps simulating the operations of a user to cover the required function points.
an echo message segments and is a feedback of the test step which may contain out put actions state of object environment variables etc.
exception messages segment record the critical information when a test script fa ils which often contain the functions or files being called during the test alarm.
all the segments excep t the test steps echo messages and exception messages are classified as miscellaneous message segment which may include prompt messages and messages from related infrastructures.
in conclusion test logs record information about testing activities including the state of test scripts the software under test and related infrastructures etc.
however it is a nontrivial work to fully distinguish all the information since the distribution of the informatio n varies over distinct projects.
testers peruse the entire test logs to analyze testing activities.
failure causes table exhibits the explanations of the test alarm causes in the two projects.
we also presen t the solutions to these test alarms namely how testers deal with each test alarm.
there are seven types of causes in the projects.
we find that handling test alarms in sit is a complex process.
on the one hand different causes lead to distinct solutions.
debugging or locating bugs in test scripts c4 is not enough for testers to handle test alarms.
testers may conduct obsolete test c1 wrongly configure some files c3 or face several environment issues c6 etc.
on the other hand testers also need to cooperate with distinct stakeholders to handle test alarms.
testers send all the product code defects c2 to developers.
some device anom alies c5 also require the instrument suppliers to deal w ith.
site reliability engineers are responsible for fixing third party software problems c7 .
hence automatically deciding the type of causes can help testers focus on some specific resources.
for example if it is already known that a test alarm is caused by the test script defect testers can further run some bug location and fixing tools for deeper analysis.
in addition many types of causes in table also exist in open source software.
after investigating the causes for false test alar ms all test alarms caused by non pr oduct code defects of apache software we find that causes c1 c3 c4 and c6 are also detected in .
iii.
caus e analysis model in this section we present our cause analysis model cam in detail.
the basic idea of cam is to search the test logs of historical test alar ms that may have the same failure cause with the new test log.
as shown in fig.
cam first pre processes test logs with b ilingual nlp techniques.
then historical test logs are selected according to the function points.
third cam predicts th e cause of a new test alarm based on similarity between new and historical test logs.
finally both the cause and the difference between new and historical test logs are presented to facilitate the examination of prediction results.
cam is efficient as it is an information retrieval based algorithm without the overhead of training models.
besides table .
causes for test alarms and solutions idtype of cause explanation solution c1 obsolete test test scripts or product codes are obsolete when continuous integration e.g.
testers conduct testing with out of date test scripts.
testers update test scripts or product codes.
c2product code defect defects in product code e.g.
the product code does not meet the requirement of a function point.
testers submit bug reports to developers c3configuration error configuration files are incorrectly edited e.g.
testers set conflict parameters in configuration files.
testers correct configuration files c4test script defect faults in assertion expression arguments statement of test scripts e.g.
quotation marks mismatch in test script.
testers debug test scripts c5device anomaly defects exist in the devices for running the test bed e.g.
the interface board of running the communication system breaks down.
testers submit bug reports to instrument suppliers c6environment issue environment issues include the problems of the network cpu memory etc.
e.g.
the space of hard disk is not enough for executing test scripts.
testers diagnose the environment c7third party software problem defects or incompatible issues exist in the third party software e.g.
there are problems for the automatic testing system.
testers ask site reliability engineers to diagnose the third party software test step echo message miscellaneous message exception messa ge figure .
a snippet of test logs.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
testers could better understand and verify the prediction results after examining the info rmation presented by cam.
we exhibit a running example to predict th e cause of the test log snippet in fig.
.
the test log is generated by a test script for verifying the fu nction point auto update schema aus for short .
the test log shows that time out while waiting for more data .
in addition testers use some chinese messages to warn that exception happens continuously for more than times .
we translate and present the chinese part in bold.
a. test log preprocessing in this study test logs are bilingual documents which makes test log preprocessing more complex than that in a single language.
cam preprocesses these test logs with a series of bilingual nlp techniques.
for test logs written in merely english only english nlp is needed.
language detection .
we first distinguish the texts in a test log by the language ty pe.
since languages are located at independent areas in the utf encoding table we apply a regular expression to detect the areas.
terms matching the regular expression are classified as chinese terms.
the remaining ones are english terms.
therefore a test log is separated in to two independent parts namely the english part and the chinese part.
english nlp .
we apply three widely used english nlp steps to preprocess the english part including tokenization stop words removal and stemmi ng.
first the english part is tokenized with a regular expression .
.
string meets the regular expression is tokenized as a term.
second we consider the single letter terms punctuation marks and numbers as stop words and remove them e.g.
e and in fig.
.
third porter stemming algorithm is employed to stem each term.
chinese nlp .
word segmentation is a major difference between chinese and english nlp steps since chinese documents are written without any spaces between terms .
we utilize ikanalyzer an open source nlp tool for chinese word segmentation.
in fig.
ikanalyzer detects several terms in the chin ese part including exception happens continuously for more than times .
term integration .
after transforming the test log into terms we merge english and chinese parts together according to their original order for unified operations in fig.
.
b. historical test log selection historical test log selection aims to select a subset of historical test logs for efficient cause prediction as it is time consuming and noise overwhelming to search through all the historical test logs.
for example there are seven historical test logs in fig.
3a .
after selection only test logs his1 to his5 are used for cause analysis.
cam selects historical test l ogs by examining the function points of test scripts since we find that test scripts with the same function point usually ta rget the same functionalities to check.
they are more likely to fail with the same cause as we investigate in section v d .
fo r a project under test test specifications can be an effective material to extract the function points of test scripts.
in this study we extract the function point of each test script directly from the automa tic testing system of our industry partner.
we associate the function poi nts with test logs by matching the test script id.
for example in fig.
3a the function points of test logs his1 to his5 are aus while that of his6 and his7 are npf netconf proxy func .
when analyzing the test log of a new test alarm cam attempts to select historical test logs for the same function point as the new one.
if no such historical test log is available ca m utilizes all the historical test logs to conduct the prediction.
c. cause prediction after historical test log selection cam predicts the cause of a new test alarm by first ranking the selected historical test logs according to their similarities with the new test log and then analyzing the ranking list to achieve the possible cause.
the basic hypothesis is that the possibility of two test logs implying the same cause increases along with the growth of the similarity between two test logs.
log similarity calculation cam calculate s the similarity betw een test logs by cosine similarity measurement .
the inputs of this measurement figure .
framework of cam for test alarm analy sis with a running example cause analysis model new test log snippet with function point auto update schema e exception happens continuously for more than times .
time out while waiting for more data historical test logs logs func.
point sim logcause his3 aus .
c2 his4 aus .
c3 his1 aus .
c3 his2 aus .
c3 his5 aus .
c2 his6 npf c1 his7 npf c3 3a historical test log real causetop sim logtop cause h1 c2 .
c2 h2 c2 .
c2 h3 c2 .
c2 h4 c1 .
c2 h5 c1 .
c2 h6 c3 .
c2 3b test log preprocessing exception happens continuously for more than times time out while wait for more data t .
k his3 sim log .
threshold .
c2 s summed sim log .
c3 s summed sim log .
the type of cause is c3.
3c new test log cd opt vnfp bash cd imagevmnpso assertion fails historical test log rm opt vnfp imagevmnpso assertion fails ranking list analysis a l c s history failure causes historical test logs selection test log preprocess language detection english nlp chinese nlp term integration a l ?
s new integrated test logs selected historical test logs ranking list prediction result presentation d cause prediction log similarity calculation ranking list analysis c b a authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
are the attribute vectors of test logs.
we construct attribute vectors with the shingling strategy in which each 2shingling is an attribute.
more specifically if we view a document as a sequence of terms a contiguous subsequence in the document is referred to as a shingle.
the shinglings are defined as the set of all unique shingles of size in this document .
the shingling strategy has been successfully applied in chinese grammar detection information retrieval etc.
for example exception happens and happens continuously are two shinglings of the snippet in fig.
.
cam calculates the we ight of attributes with tfidf term frequency inverse document frequency .
for an attribute a in the test log t its tf idf value is defined as tf idf g3002 g481 g3021 g3404 g1858 g3002 g481 g3021 g1499 g142 g145 g137 g3015 g3041 g3250 where g1858 g3002 g481 g3021 denotes the number of times that a occurs in t g1866 g3002 denotes the number of te st logs containing a and n denotes the number of test logs in a project.
with the attribute vectors of two test logs the cosine similarity is measured as g1845 g1861 g1865 g3039 g3042 g3034 g4666 g1848 g4652 g1318 g883 g481 g1848 g4652 g1318 g884 g4667 g3404 g2906 g4652 g4652 g1318 g2869 g942 g2906 g4652 g4652 g1318 g2870 g3627 g2906 g4652 g4652 g1318 g2869 g3627 g3627 g2906 g4652 g4652 g1318 g2870 g3627 where g25 g4652 g4652 g1318 g883 g942 g25 g4652 g4652 g1318 g884 is the inner product of two vectors and g3627 g25 g4652 g4652 g1318 g883 g3627 g3627 g25 g4652 g4652 g1318 g884 g3627 is the product of norm for these vectors.
cam ranks the selected historical test logs in descending order by sim log.
in fig.
3a the test l og his3 has the largest sim log .
and its cause is c2 .
ranking list analysis it is reasonable to predict the test alarm cause w ith the top1 cause1 in the ranking list.
howeve r we find that the top sim log may be very low in some ranking lists which can be interpreted as that the top test log has a small possibility to share the same cause with the new one.
considering such a situation cam analyzes the test alarms as follows.
for a new test log if its top sim log is larger than a cause specific threshold cam adds the new test log to a high similarity set.
otherwise cam adds it to a low similarity set.
for a test log in the high similarity set cam predicts the new log s cause as the top cause in the ranking list.
for a test log in the lowsimilarity set a knn strategy is conducted.
algorithm cause specific threshold calculation input the set d of all historical test logs with the top cause i the target value t output threshold g2016 g3036 let threshold g2016 g3036 g3404 g883 g484 g882 for x x x x .
do achieve the test logs with the top g1845 g1861 g1865 g3039 g3042 g3034 g3408 g1876 from d if the portion of the above test logs with the real cause i t. threshold g2016 g3036 g3404 g1876 break else continue end return threshold g2016 g3036 cause specific threshold .
we employ a cause specific threshold g2016 g3036 for cause i to differentiate the hi gh similarity set and the low similarity set .
since we could achieve a ranking list for a historical test log by calculating the sim log for every the top cause refers to the top test log s cause while the top sim log refers to the top test log s sim log in a ranking list.
test log prior to this historical test log each historical test log is associated with a top cause and top sim log.
given a new test log with top cause i the value of g2016 g3036 is defined as the minimum value between and such that over t of the causes of all the historical test logs with the top cause i and the top sim log g2016 g3036 are correctly predicted as cause i where t is a parameter named ta rget value the impact of t is discussed in section v .
in algorithm we elaborate the details on how to determine the threshold g2016 g3036 for cause i. taking fig.
3b as an example there are six historical test logs with the top cause c2 in the set d. we increase x from to .
when x .
only h1 h5 are considered.
the top causes of h1 h3 equal the real cause therefore the portion is .
.
when x .
only h1 h4 are considered among which the portion reaches .
larger than the target value .
.
at last the threshold of cause c2 is .
.
knn strategy .
for a test log in the low similarity set we sum up the sim log values of the top k test logs by their causes.
then the cause of the new test log is assigned to the cause with the largest summed sim log.
as the sim log of his3 is smaller than c2 s threshold .
cam obtains top k test logs in the ranking list fig.
3c .
after being summed cause c3 achieves the largest summed sim log .
which are summed by the sim log of his4 his1 and his2 .
hence cam predicts the cause of the new test log as c3.
d. prediction result presentation cam helps testers understand and verify the prediction results by presenting the causes as well as the differences between test logs.
for test logs in the high similarity set cam shows the differences between th e new and the top test log in the ranking list.
for test logs in the low similarity set cam shows the differences between the new test log and the first historical test log with the pr edicted cause e.g.
his4 in fig.
3a .
since historical test logs have been analyzed by testers testers may easily know whether two test logs implying the same failure causes after perusing the differences.
to show the differences cam first removes all the numbers in test logs since such information usually indicates time ip address and numeric counter etc.
which may be different in all the test logs.
then cam compares the differences between test logs with javadiffu tils an open source tool to compare differences between texts .
the tool shows all the change delete and insert operations between texts.
at last we high light the different lines of the two test logs.
for the change operations the lines in both two test logs are highlighted.
for the delete or insert operations only the lines with more information are highlighted.
for example in fig.
only the first two lines are different.
instead of com prehending the entire contents of the test log testers can focus on the first two lines to verify the prediction result with th e assistance of the historical test log.
iv.
experimental setup in this section we detail th e experiment related issues for evaluating cam.
the datasets and evaluation metrics are first authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
presented followed by a discussion of the baseline algorithms and the research questions rqs .
a. datasets we collect test logs from tw o industrial testing projects at huawei tech inc. to build two datasets denoted as ds1 and ds2.
in the datasets each test log corresponding to a test alarm is associated with a failure cause manually labeled by the testers.
table exhibits the statistical information of the datasets.
rows to show th at there are more than test logs including fo r ds1 and for ds2.
these test logs are generated during and valid testing days due to vacations and other testing activities.
on average testers are requested to analyze and test alarms per day in the two projects.
as shown in rows and each test log contains more than lines of texts including about and test steps.
the total size of the datasets exceeds 10gb.
hence test logs are a relatively large software artifact which may consume considerable time for analysis.
rows to present the number of test logs with respect to each type of cause.
there are and types of causes detected in ds1 and ds2 respectively.
obsolete test c1 never occurs in ds2 during the time frame.
based on the statistic information we have the following observations.
besides product code defect c2 and test script defect c4 other causes still cover of test alarms.
as shown in row nearly four types of causes occur per day on average.
these factors make testers have to decide the exact type of cause for each test alarm before fixing these alarms.
another finding is that no test alarm is caused by more than one type of cause in th e datasets.
one possible reason is that the causes defined in this study are for classifying the candidate part with defects e.g.
devices environment etc.
a single buggy part can directly lead the test script to fail.
in addition after studying the daily work of testers in the projects we find that if te sters confirm the part causing the test alarm they seldom diagnose the remaining parts unless the test script fails again.
thus au tomatically identifying the most possible cause for test alarms is beneficial for testers.
b. evaluation method we utilize the incremental framework to run algorithms in all th e experiments which can better simulate the daily work of a tester.
more specifically we partition the datasets by the testing day.
when analyzing the test logs in day t t we regard test logs in day to day t as historical test logs.
since the framework conducts prediction from the second day there are and testing days with and test logs in the datasets to be predicted respectively.
under the incremental framework we evaluate the overall performance of algorithms with accuracy and auc area under roc curve .
g1827 g1855 g1855 g1873 g1870 g1853 g1855 g1877 g3404 g3015 g3048 g3040 g3278 g3290 g3293 g3293 g3280 g3278 g3295 g3015 g3048 g3040 g3276 g3289 g3276 g3287 g3300 g3301 g3280 g3279 accuracy can be interpreted as the portion of correctly predicted test logs among all the predicted ones.
g1827 g1847 g1829 g3036 g3404 g3505 g1846 g1842 g1844 g3036 g4666 g1846 g4667 g1832 g1842 g1844 g3036 g4593 g4666 g1846 g4667 g1856 g1846 g2879 g2998 g2998 g481 g1875 g1860 g1857 g1870 g1857 g3 g1846 g1842 g1844 g3036 g3404 g851 g3043 g3042 g3046 g3036 g3047 g3036 g3049 g3032 g3046 g3284 g3 g3030 g3042 g3045 g3045 g3032 g3030 g3047 g3039 g3052 g3 g3030 g3039 g3028 g3046 g3046 g3036 g3033 g3036 g3032 g3031 g851 g3047 g3042 g3047 g3028 g3039 g3 g3043 g3042 g3046 g3036 g3047 g3036 g3049 g3032 g3046 g3284 g1832 g1842 g1844 g3036 g3404 g851 g3041 g3032 g3034 g3028 g3047 g3036 g3049 g3032 g3046 g3284 g3 g3036 g3041 g3030 g3042 g3045 g3045 g3032 g3030 g3047 g3039 g3052 g3 g3030 g3039 g3028 g3046 g3046 g3036 g3033 g3036 g3032 g3031 g851 g3047 g3042 g3047 g3028 g3039 g3 g3041 g3032 g3034 g3028 g3047 g3036 g3049 g3032 g3046 g3284 auc is the area of the two dim ensional graph in which tpr i is plotted on the y axis and fpr i is plotted on the x axis over distinct threshold t of possibility values .
a possibility value of cam s prediction is the sim log of the first historical test log with the pr edicted cause in a ranking list.
we calculate auc in a one against all class strategy.
when calculating auc of cause i all the test alarms predicted as causes i are considered as positives i while the other types of causes are considered as negatives i. auc can avoid inflated performance evaluation on imbalanced data.
for example a classifier that always predicts product code defect c2 achieves .
accuracy on ds1 but results in an auc of which is the same auc as a random guess classifier.
in addition we also evaluate the resource consumption of these algorithms.
resource consumption is critical in industry projects.
on the one hand computation resources e.g.
cpu memory are limited in real scenarios.
on the other hand testers may conduct software testing several rounds per day.
low resource consumption mak es algorithms timely update models with the information in the latest round.
thus we evaluate the time and minimal memory for completing the prediction of each algorithm.
c. baseline algorithms to the best of our knowledg e no studies directly utilize test logs to predict the causes of test alarms in sit.
we implement three baseline al gorithms to study the characteristics of cam.
lazy associative classifier lac .
a similar work by herzig et al.
detects false test alarms with association rules mined from test steps .
since the mining algorithm in is not suitable for multiclass classification we employ lazy associative classifier to predict the causes which uses association rules to execute multiclass prediction .
following the strategies in we extract test steps from test logs.
in our datasets test steps can be identified since they are marked with timestamps.
we bu ild attribute vectors for lac table .
statistic of test logs and causes in the datasets dataset info ds1 ds2 test logs size .72gb .06gb time frame june 1st july 30th oct. 26th nov. 16th testing day day day test logs per day avg.
lines lines lines avg.
test steps test steps test steps obsolete test c1 .
product code defect c2 .
.
configuration error c3 .
.
test script defect c4 .
.
device anomaly c5 .
.
environment issue c6 .
.
software problem c7 .
.
avg.
type of causes per day .
per day .
per day authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
based on the test steps.
each entry in the vector represents whether a test step exists in a test log.
the parameters of association rules namely minimum confidence and support values are set to .
and .
respectively .
we implement lac with an open source to ol shared by federal university of minas gerais .
best first tree bft .
hao et al.
classify test alarms into product code defect and obsolete test script at the unit testing stage with bft classifier.
since the attributes related to test complexity and program execution measurements are expensive to collect in large softw are systems we examine whether the bft classifier is suitable for cause analysis.
bft is implemented with the widely used machine learning tool weka .
we alternatively use the tf idf values of terms in a test log as attributes for bft s input.
topic model tm .
as a popular way to analyze a large scale of documents tm can be us ed to predict the test alarms causes.
tm first extracts several topics from test logs by mining co occurrence terms.
next each test log is expressed by a series of topics with di fferent probabilities.
thirdly we construct attribute vectors w ith these topics and utilize the cosine similarity measurement to rank historical test logs.
at last the top cause in the ranking list is used for the prediction.
we implement one type of tm namely latent dirichlet allocation with an open source tool mallet .
we set the parameter of topic number to alpha to .
and beta to .
according to the suggestion by mallet.
in the experiments we set the test logs in day as the initial training set and incrementally train models after each testing day such that these algorithms can fully learn all the information from history.
d. research questions rq1 are the test logs with the same causes more similar than those with different causes?
rq2 how do the parameters influence cam s performance?
rq3 how does cam perform against baseline algorithms?
rq4 how does historical test log selection influence the performance of cam?
rq5 how does cam perform in a real development scenario?
v. experimental results all the algorithms are implemen ted in java jdk1.
.0 31 and run on a pc with intel core tm i7 cpu .6ghz and 24g memory.
a. answer to rq1 cam predicts test alarm causes by the similarity between test logs.
in this rq we ve rify cam s hypothesis namely test logs with the same causes ar e more similar than those with different causes.
we calculate the pairwise sim log of test logs on the two datasets and collect all the test log pairs with sim log between x to x .
where x ranges from to .
with a step size of .
.
in fig.
the dark blue part of a bar in the bar chart presents the ratio of test log p airs with the same causes in distinct similarity range.
as shown in fig.
the ratio of test logs with the same causes gradually increases along with the increment of the sim log.
more than of test log pairs are with the same causes when sim log .
on ds1 and sim log .
on ds2.
test logs with the same causes tend to have a higher sim log than those with different causes.
answer to rq1 .
the possibility of two test logs implying the same causes increases along with the growth of the similarity between two test logs .
test logs with the same causes are more similar than those with different causes.
b. answer to rq2 influence of parameters cam requires a target value t to determine the causespecific threshold for each type of cause.
meanwhile for the new test logs in low similar ity set cam employs knn to analyze their causes.
the influences of these parameters are investigated in this subsection.
target value t. we evaluate the relationship between cam s accuracy with respect to different target value t. to tune the parameter t we set the number of neighbors k to a fixed value k in this experiment and vary t from to with a step size of .
.
when t the high similarity set is empty.
in contrast the low similarity set is empty when t .
two curves in fig.
show that as t is small t .
the accuracy is low.
when t increases the accuracy increases as well.
the stable ranges are slightly different cam s accuracy is stable when t ranges from .
to .
on ds1 while the figure .
causes vs. cosine similarit y of test lo gs.
.
.
.
.
.
.
.
.
.9ratio of same and different causes cosine similarityds1 same different0 .
.
.
.
.
.
.
.
.
cosine similarityds2 different samefigure .
accuracy with varied target value t. .
.
.
.
.
.
.
.
.
.
.
.
.
.
1accuracy target value t ds1 ds2 figure .
accurac y with varied number of nei ghbors k. .
.
.
.
0accuracy number of neighour kds1 ds2 figure .
accuracy changes before after knn strategy.
low similairty sethigh similairty setlow similairty sethigh similairty set ds1 ds2 of test logs accuracy before .
.
.
.
accuracy after .
.
.
.
.
.
.
.80accuracy authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
accuracy is more stable on ds2 when t varies.
we set t .
in the following experiments.
number of neighbors k. to investigate cam s accuracy with respect to varied k we set t .
and k varies from to with a step size of .
when k it means cam conducts the prediction only w ith the top cause.
the tendencies of the curves in fig.
are similar to that in fig.
.
as k is small k the accuracy is low and then the accuracy gradually rises to be stable along with the growth of k. the accuracy turns to be stable when k on ds1 and k on ds2.
we set k in the following experiments.
we do not set an extremely large k to avoid introducing noisy neighbors.
deep analysis of parameter setting we investigate why such parameters t .
k work for cause prediction.
the target value t splits the new test logs into the high similarity and l ow similarity sets.
the value of k controls the prediction strategy.
fig.
shows the total size of both high similarity and low si milarity sets on the two datasets.
the light and dark blue bars show the accuracy before and after applying the knn strategy respectively.
in fig.
cam successfully splits the new test logs into two sets.
the high similarity set of ds1 and ds2 both cover more than of the new test logs in which cam achieves an accuracy of .
and .
respectively.
while the accuracies of the low similarity sets are only .
and .
for the two datasets.
af ter deciding the low similarity sets knn strategy improves the accuracy of these sets by up to .
namely from .
to .
on ds1 and from .
to .
on ds2.
however if we also apply knn to the high similarity set the accuracy drops on both datasets.
the reason is that according to the hypothesis of cam when the top sim log is greater than the threshold the top cause is likely to be the ground tru th cause of the new test alarm.
answer to rq2 .
we set t .
and k in this study.
cam achieves an accuracy around for the highsimilarity set.
knn improves the accuracy in the lowsimilarity set by up to .
.
c. answer to rq3 accuracy and auc evaluation we summarize the experimental results in terms of accuracy in fig.
.
cam achieves an accuracy of .
and .
on the two datasets.
it outperforms the baseline algorithms by up to .
on ds1 and .
on ds2.
out of the three baseline algorithms no one is superior to the others since lac performs well on ds1 while loses its dominance to bft on ds2.
comparing with lac which mines local patterns from test steps cam shows its robustness over different datasets as it compares test logs from an overall perspectiv e. since a random prediction for more than classes can only achieve an accuracy below the accuracy of cam shows its ability in cause analysis.
we also present the accuracy with respect to the testing days in fig.
and fig.
.
out of the and predicted testing days cam performs best on and of them respectively.
cam achieves an accuracy over for more than one third testing days namely out of on ds1 and out of on ds2.
we introduce the paired wilcoxon signed rank test to explore the statistical significance between the performance of cam and baseline algorithms.
the p values are .
.
on ds1 and .
.
.
on ds2 when comparing the accuracy of cam against lac bft and tm respectively.
therefore cam is superior to the baseline algorithms in terms of accuracy with p .
.
auc values of the algorithms are presented in table .
a value in bold deno tes a result which is be tter than the other algorithms on the same failure cause.
table shows that some failure causes are difficult to be discovered e.g.
environment issue c6 since no algorithm is superior to a random classifier on ds1.
cam and bft have similar performance which outperforms the other algor ithms on out of types of causes on ds1.
on ds2 cam performs better on the majority of causes than the baseline algorithms.
for some causes cam has an auc value beyond .
showing that it could provide excellent discrimination on these causes.
computation resources evaluation table .
comparison on computation resources consumption algorithmtime in minutes memory ds1 test logs ds2 test logs ds1 ds2 training test total training test total lac .
.
.
.
gb gb bft .
.
.
.
.
gb gb tm .
.
.
.
.
gb gb table .
comparison on auc cause algorithm c1 c2 c3 c4 c5 c6 c7 ds1 lac .
.
.
.
.
.
.
bft .
.
.
.
.
.
.
tm .
.
.
.
.
.
.
cam .
.
.
.
.
.
.
ds2 lac .
.
.
.
.
.
bft .
.
.
.
.
.
tm .
.
.
.
.
.
cam .
.
.
.
.
.
figure .
accuracy for algorithms on two datasets.
ds1 ds2 lac .
.
bft .
.
tm .
.
cam .
.
.
.
.
.
.65accuracy datasets figure .
accurac y per testing day on ds1.
.
.
.
.
.
.
39accuracy testing daylac bft tm cam figure .
accurac y per testing day on ds2.
.
.
.
.
.
.
1accruacy testing daylac bft tm cam authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
cam .
.
.
.
gb gb we compare the computation time and memory consumption in this subsection.
to compare the computation time we directly allocate 22gb memory for the algorithms 2gb is reserved for the operating system .
under the incremental framework the computation ti me is calculated as the sum of the time for training or testing models after each incremented testing day.
to compare the memory consumption we allocate the memory increasingly from 2gb to 22gb with a step size of 1gb and observe th e minimal memory for algorithms to accomplish the prediction.
in table the co lumns refer to the algorithm name the computation time and the memory consumption on the two datasets.
for each dataset we present the training time testing time and total time in the sub columns of time .
cam conducts testing without spendi ng time on training models.
as shown in table lac runs extre mely fast as it only mines local patterns of test steps.
bft takes the longest time on ds1 since it frequently conducts garbage collection even with 22gb memory.
when comparing cam against bft and tm in terms of total time cam runs nearly to times faster than bft and times faster than tm.
cam takes .
and .
minutes in analyzing an d test logs over the two datasets respectively which means that it takes only .06s to .13s on average in helping testers analyze one test log.
the training time is the main overhead for most algorithms.
for example it takes .
minutes for bft to incrementally train models on test logs.
as a result when the test logs continue increasing it may take days to update models.
in contrast a new test log can be immediately updated to cam once the tester verifies the cause of that log.
for memory consumption most algorithms consume no more than 8gb for prediction except bft.
bft cannot complete predictions until we allocate 22gb memory.
in contrast cam only takes less than 4gb memory.
the underlying reason is that ca m conducts prediction without the need of allocating huge memory to train models.
in addition along with the grow th of historical test logs we can set a time frame of histo rical test logs to limit the computation time and memory consumption of cam.
answer to rq3 .
cam outperform s the baseline algorithms over distinct evaluation metrics.
meanwhile cam is resources saving as it takes about .1s and less than 4gb memory to process a test log.
d. answer to rq4 cam utilizes function points to conduct historical test log selection.
in a well planned testing process the function points are predefined when testers develop test scripts .
however function points may be unavailable if testers do not organize test scripts with them in some projects.
we propose an algorithm named cam fp to simulate such a situation.
cam fp searches among all the historical test logs to conduct prediction without historical test log selection.
we show the auc accuracy total time and memory for cam fp and cam over the two dat asets in table and table .
we find that historical te st log selection could remove considerable noisy test logs since cam outperforms camfp by .
.
in terms of accuracy.
cam also outperforms cam fp on the majority of causes in terms of auc.
besides historical test log selection can significantly shorten the running time of cam.
after selection cam shortens the running time from .
to .
on ds1 and from .
to .
on ds2 in minutes.
we find that although the function points of test scripts are removed cam fp still achieves co mpetitive performance against the baseline algorith ms. for several types of causes the auc value of cam fp is equa l to or slightly better tha n cam.
it shows the robustness of cam on different situations.
answer to rq4 .
historical test log selection reduces the noisy test logs and shortens th e running time for cam.
without the function points of test scripts cam still achieves competitive performance.
e. answer to rq5 we integrate cam into the automatic testing system of our industry partner.
cam achieves an average accuracy of after two months of running.
this version cam is better than the intelligent an alysis tool of last version manually building regular expressions .
an interesting finding is that cam performs better in a real development scenario than in the experiments.
the reason is that testers tend to conduct software test ing several rounds per day.
the causes between each round m ay be similar.
after testers verify the causes in a round cam immediately utilizes the information of the test logs to predict the causes in the next round.
however it is hard to decide the round for each test log in the experiment.
when we split the test logs according to the testing days it may limit the information available to cam.
in addition instead of simply presenting the causes for test alarms cam also presents the differences between test logs.
i think cam is accurate.
actually i will not believe in an automatic tool.
however after presenting the historical test logs i can quickly decide whether the prediction is correct.
cam accelerates my work.
the presentation of results is important since good presentations may make the prediction easy to comprehend .
such human factor namely the influence of different presentations is not the focus of this study.
we leave it as a future work.
some testers also suggest new features for cam including labeling the defect related snippets from the lengthy test logs provide suggestions on how to fix different types of defects etc.
these features drive us to continue improving cam.
answer to rq5 .
cam achieves an average accuracy of after two months of running in a real development scenario.
cam accelerates testers work with accurate result and comprehensible presentation.
table .
auc values for cam and cam fp cause algorithm c1 c2 c3 c4 c5 c6 c7 ds1 cam fp .
.
.
.
.
.
.
cam .
.
.
.
.
.
.
ds2 cam fp .
.
.
.
.
.
cam .
.
.
.
.
.
table .
accuracy total time and me mory for cam and cam fp algorithm ds1 ds2 accuracy total time memory accuracy total time memory cam fp .
.
min 4gb .
.
min 4gb cam .
.
min 4gb .
.
min 4gb authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
vi.
threats to validity a. experiment construction the generality of the cause pr ediction algorithm in cam should be further studied since the algorithms may be sensitive to the datasets.
to al leviate this threat we evaluate cam over two industry datasets with more than test logs and deploy it in a real development scenario.
as the ground truth causes of the test alarms are manually labeled by testers there may induce some errors.
however industries have a strict process to control the quality of software testing.
the error rate is usually under control.
b. method construction in this study the quality of test logs may influence the results of cam.
currently researchers study how to automatically decide where to log and what to log for developers .
these techniques may improve the quality of test logs for cam to conduct the prediction.
in addition we mainly focu s on analyzing test alarms with test logs.
in order to detect the exact causes of test alarms testers may go through various software artifacts e.g.
test logs test script codes etc.
the accuracy of cam may be further improved by levera ging more software artifacts.
however test logs are still an effective debugging tool .
vii.
related work a. test alarm classification rogstad et al.
disti nguish test code obsoleteness from regression faults for database applications with a set of attributes e.g.
table names sql statements etc.
hao et al.
classify test alarms into product code def ect and obsolete test at the unit testing stage with complex attributes related to test complexity and program execution measurements.
however these techniques are either unique to database products or expensive to collect complexity information in large software systems.
herzig and nagappan classify test alarms in sit .
they detect all false test alarms in microsoft products with association rules since the number of false test alarms is a measurement to measure test quality .
different from detecting false test alarms cam analyzes the test alarms causes which is more complex than the binary classification.
recently several patents are filed to construct systems to analyze test results bucket failure messages and analyze error logs with regular expression .
however technical details and evaluations are not presented in these patents.
as to a survey w ith testers in our industry partner they manually build regular expressions to classify test alarms and achieve an accuracy of over distinct projects.
b. log analysis previous work mainly analyzes two types of logs namely the logs generated by the released software product system log and the logs generated in the testing activity test log .
for the system log analysis oliner et al.
discuss the advances and challenges in system log analysis .
shang et al.
conduct program veri fication for big data analytic applications with test logs.
fu et al.
and xu et al.
conduct anomaly detection throug h log analysis.
besides system logs are also used to diagnose the underlying causes of system anomaly .
however such logs may lack information to analyze the defects in testing activities.
for the test log analysis previous work mainly utilize test logs to solve the oracle problem.
oracle problem is to check whether a test result reveals a failure or not.
andrews et al.
and tu et al.
analyze test logs for oracle problem with state machine based models.
yantzi et al.
conduct an industrial evaluation of methodologies for oracle problem.
recent work by anderson et al.
constructs attributes from test logs to predict the oracle.
in conclusion system logs analysis is a post process of software testing.
our work fal ls into test log analysis.
instead of solving the oracle problem we predict the underlying causes for test alarms.
c. failure clustering most studies in failure clust ering detect failures in product code with execution prof iles.
execution profiles capture the execution of basic blocks and conditional branches of software.
liu et al.
cluster failures with fault location techniques.
dickson et al.
cluster program executions and identify failures among the clusters with unusual profile hypothesis.
digiuseppe et al.
utilize latent semanticanalysis techniques for more precise failure clustering.
besides podgurski et al.
train pattern classifiers to group similar failures in product code.
francis et al.
refine the failure cluster results with two tree based techniques.
lo et al.
capture program execution profiles to train machine learning models for identif ying failures in software product.
clustering failures in pr oduct codes is a subsequent process of test alarm analysis.
approaches in analyzing product codes may fail to identify the failure causes in software testing.
meanwhile execu tion profiles are hard to collect for testers in sit.
different from clustering failures in product code we classify the causes of test alarms in sit.
viii.
conclusion and future work in this study we present ou r attempt towards predicting the multiple causes for test alarms in sit.
our model leverages the test logs of historical test alarm to analyze the new test alarm.
we evaluate our model over two industrial projects with more than test alarms.
our model shows an accuracy of .
and .
respectively.
we deploy cam for our industry partner and achieve an accuracy of after two months of running nearly three times better than their previous strategy with regular expressions.
our technique provides a directi on for industry to analyze test alarms.
in the future we plan to employ more software artifacts to improve cam and verify cam over more software projects.
acknowledgment we greatly thank our industrial companion for sharing their datasets for research.
this work is supported in part by the new century excellent talen ts in university under grant ncet in part by the fundamental research funds for the central universities under grant dut14yq203 and in part by the national natu ral science foundation of china under grants and .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.