bigdebug debugging primitives for interactive big data processing in spark muhammad ali gulzar matteo interlandi seunghyun y oo sai deep t etali t yson condie t odd millstein miryung kim university of california los angeles abstract developers use cloud computing platforms to process a large quantity of data in parallel when developing big data analytics.
de bugging the massive parallel computations that run in today s data centers is time consuming and error prone.
to address this challenge we design a set of interactive real time debugging primitives for big data processing in apache spark the next generationdata intensive scalable cloud computing platform.
this requires re thinking the notion of step through debugging in a traditional debugger such as gdb because pausing the entire computation across distributed worker nodes causes significant delay and naively inspecting millions of records using a watchpoint is too time con suming for an end user.
first b igdebug s simulated breakpoints and on demand watchpoints allow users to selectively examine distributed inter mediate data on the cloud with little overhead.
second a usercan also pinpoint a crash inducing record and selectively resumerelevant sub computations after a quick fix.
third a user can determine the root causes of errors or delays at the level of individual records through a fine grained data provenance capability.our evaluation shows that b igdebug scales to terabytes and its record level tracing incurs less than overhead on average.
itdetermines crash culprits orders of magnitude more accurately and provides up to time saving compared to the baseline replay debugger.
the results show that b igdebug supports debugging at interactive speeds with minimal performance impact.
categories and subject descriptors d. .
testing and debugging debugging aids distributed debugging error handling and recovery keywords debugging big data analytics interactive tools data intensive scalable computing disc fault localization and recovery .
introduction an abundance of data in many disciplines of science engineering national security health care and business has led to the permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citationon the first page.
copyrights for components of this work owned by others than theauthor s must be honored.
abstracting with credit is permitted.
to copy otherwise orrepublish to post on servers or to redistribute to lists requires prior specific permissionand or a fee.
request permissions from permissions acm.org.
icse may austin tx usa c circlecopyrt2016 copyright held by the owner author s .
publication rights licensed to acm.
isbn .
.
.
.
field of big data analytics that run in a cloud computing environment.
to process massive quantities of data in the cloud developers leverage data intensive scalable computing disc sys tems such as google s mapreduce hadoop and spark .
these disc systems expose a programming model for authoring data processing logic which is compiled into a directed acyclic graph dag of data parallel operators.
the root dagoperators consume data from some input source e.g.
gfs or hdfs while downstream operators process the intermediateoutputs from dag predecessors.
scaling to large datasets is handled by partitioning the data and assigning tasks that execute the operator on each partition.
currently developers do not have easy means to debug disc applications.
the use of cloud computing makes application development feel more like batch jobs and the nature of debugging is therefore post mortem.
developers are notified of runtime failures or incorrect outputs after many hours of wasted computingcycles on the cloud.
disc systems such as spark do provide exe cution logs of submitted jobs.
however these logs present only the physical view of big data processing as they report the number of worker nodes the job status at individual nodes the overall job progress rate the messages passed between nodes etc.
these logsdo not provide the logical view of program execution e.g.
system logs do not convey which intermediate outputs are produced fromwhich inputs nor do they indicate what inputs are causing incor rect results or delays etc.
alternatively a developer may test theirprogram by downloading a small subset of big data from the cloudonto their local disk and then run the disc application in a local mode.
however using a local mode she may not encounter the same failure because the faulty data may not be included in thegiven data sample.
the vision of b igdebug is to provide interactive real time debugging primitives for big data processing.
designing b igdebug requires re thinking the traditional step through debugging primi tives as provided by tools such as gdb.
pausing the entire computation across distributed worker nodes causes significant delay andreduces overall throughput.
naively inspecting millions of records flowing through a data parallel pipeline is too time consuming and infeasible for an end user.
b igdebug must tag how individual records are flowing through individual worker nodes and transferthe requested debug information from the distributed worker nodes to the driver in an efficient manner.
in other words b igdebug must meet the requirements of low overhead scalability and finegranularity while providing expressive debugging primitives.
to solve these technical challenges b igdebug provides simulated breakpoints which create the illusion of a breakpoint withthe ability to inspect program state in distributed worker nodes and to resume relevant sub computations even though the pro2016 ieee acm 38th ieee international conference on software engineering g3 g4 g65 g79 g64 g77 g3 g60 g71 g71 g3 g68 g78 g3 g78 g60 g68 g63 g3 g60 g73 g63 g3 g63 g74 g73 g64 g3 g72 g74 g77 g64 g3 g68 g78 g3 g78 g60 g68 g63 g3 g79 g67 g60 g73 g3 g63 g74 g73 g64 g79 g67 g60 g73 g3 g63 g74 g73 g64 g63 g74 g73 g64 g3 g72 g74 g77 g64 g3 g68 g78 g3 g78 g60 g68 g63 g3 g4 g65 g79 g64 g77 g3 g60 g71 g71 g3 g68 g78 g3 g78 g60 g68 g63 g3 g60 g73 g63 g4 g65 g79 g64 g77 g3 g60 g71 g71 g68 g78 g3 g78 g60 g68 g63 g3 g60 g73 g63 g3 g63 g74 g73 g64 g72 g74 g77 g64 g3 g68 g78 g3 g78 g60 g68 g63 g3 g79 g67 g60 g73 g3 g63 g74 g73 g64 g159 g4 g65 g79 g64 g77 g134 g188 g160 g3 g159 g60 g71 g71 g134 g188 g160 g159 g68 g78 g134 g188 g160 g159 g78 g60 g68 g63 g134 g188 g160 g3 g159 g60 g73 g63 g134 g188 g160 g159 g63 g74 g73 g64 g134 g188 g160 g3 g159 g72 g74 g77 g64 g134 g188 g160 g3 g159 g68 g78 g134 g188 g160 g159 g78 g60 g68 g63 g134 g188 g160 g159 g79 g67 g60 g73 g134 g188 g160 g159 g63 g74 g73 g64 g134 g188 g160 g159 g4 g65 g79 g64 g77 g134 g188 g160 g3 g159 g60 g71 g71 g134 g188 g160 g159 g68 g78 g134 g189 g160 g159 g60 g73 g63 g134 g188 g160 g159 g72 g74 g77 g64 g134 g188 g160 g159 g78 g60 g68 g63 g134 g189 g160 g159 g79 g67 g60 g73 g134 g188 g160 g159 g63 g74 g73 g64 g134 g189 g160 g4 g65 g79 g64 g77 g134 g188 g3 g60 g71 g71 g134 g3 g188 g68 g78 g134 g3 g189 g60 g73 g63 g134 g3 g188 g72 g74 g77 g64 g134 g3 g188 g78 g60 g68 g63 g134 g3 g189 g63 g74 g73 g64 g134 g3 g189 g79 g67 g60 g73 g134 g3 g188 g9 g71 g60 g79 g16 g60 g75 g16 g60 g75 g21 g64 g63 g80 g62 g64 g5 g84 g14 g64 g84 g6 g74 g71 g71 g64 g62 g79 g23 g64 g83 g79 g9 g68 g71 g64 g23 g60 g78 g70 g3 g188 g23 g60 g78 g70 g3 g190 g23 g60 g78 g70 g3 g188 g23 g60 g78 g70 g3 g190 g23 g60 g78 g70 g3 g189 g23 g60 g78 g70 g3 g189 g22 g79 g60 g66 g64 g3 g188 g22 g79 g60 g66 g64 g3 g189 figure data transformations in word count with tasks gram is still running in the cloud.
to help a user inspect millions of records passing through a data parallel pipeline b igdebug provides guarded watchpoints which dynamically retrieve only those records that match a user defined guard predicate.
b igdebug supports fine grained forward and backward tracing at the level of individual records by leveraging prior work on data provenance within spark .
to avoid restarting a job from scratch incase of a crash b igdebug provides a real time quick fix and resume feature where a user can modify code or data at runtime.
it also provides fine grained latency monitoring to notify a user which records are taking much longer than other records.
we evaluate b igdebug in terms of performance overhead scalability time saving and crash localizability improvement on threespark benchmark programs with up to one terabyte of data.
with the maximum instrumentation setting where b igdebug is enabled with record level tracing crash culprit determination and latency profiling and every operation at every step is instrumented withbreakpoints and watchpoints it takes .
times longer than the baseline spark.
.5x is a very conservative upper bound as a developer typically would not need to monitor the latency of each recordat every transformation and would not need to set breakpoints onevery operator.
if we disable the most expensive record level la tency profiling b igdebug introduces an overhead less than on average.
we also measure the time saving achieved since itsquick fix and resume feature enables a user to recover appropriatelyfrom a crash and resume the rest of computation.
we also com pare b igdebug s capability to determine crash inducing records against baseline spark which reports failures at the level of tasks each of which handles millions of records.
bigdebug offers the following contributions bigdebug provides breakpoints and watchpoints with minimal performance impact.
bigdebug exhibits less than overhead for record level tracing overhead for crash monitoring and for on demand watchpoint on average.
b igdebug s quick fix and resume feature allows a user to avoid re running a program from scratch resulting in up to100 time saving.
b igdebug narrows down the scope of failure inducing data by orders of magnitude through fine grained tracingof individual records within the distributed data processing pipeline.
several frameworks have been proposed to debug disc applications but none provides a comprehensive set of realtime inspection features for disc systems.
existing debuggers are either a post mortem replay debugger or data provenance solutions that keep track of which intermediate outputs are producedfrom which inputs by storing metadata in external storage .
our recent study finds that these solutions are limited in terms of scalability and performance .
newt can scale up to only100gb and this takes 85x of baseline spark in comparison to b igdebug handling up to 1tb with only .5x .
bigdebug is available for download at miryung software.html.
the website also includes b igdebug s manual and api description.
the rest of the paper is organized asfollows.
section discusses the background on large data parallelprocessing in apache spark and why we chose spark.
section introduces a set of interactive debugging primitives with a running example.
section describes the design and implementation ofindividual features.
section describes evaluation settings and theresults.
section describes related work.
section concludes withfuture work.
.
background apache spark apache spark is a large scale data processing platform that achieves orders of magnitude better performance than hadoopmapreduce for iterative workloads.
b igdebug targets spark because of its wide adoption with over developers and companies leveraging its capabilities and support for interactivead hoc analytics allowing programmers to explore the data as theyrefine their data processing logic.
spark s high level programming model provides over 80types of data manipulating operations and supports language bindings for scala java python and r. furthermore a variety of domain specific extensions have been built onspark including mllib for machine learning graphx for graph processing sparksql for relational queries and statistical analysis in r. spark can consume data from a variety of data sources including distributed file systems like hdfs objectstores like amazon s3 key value stores like cassandra andhbase and traditional rdbms like mysql and postgres .
the spark programming model can be viewed as an extension to the mapreduce programming model that includes direct supportfor traditional relational algebra operators e.g.
group by join fil ter and iterative computations through a for loop language con struct.
these extensions offer orders of magnitude better performance over previous big data processing frameworks like apache hadoop for iterative workloads like machine learning.
sparkalso comes with a relaxed fault tolerance model based on work flow lineage that is built into its primary abstraction resilient distributed datasets rdds which exposes a set of data processing operations called transformations e.g.
map reduce filter group by join and actions e.g.
count collect .
spark programmers leverage rdds to apply a series of transformations to a collection of data records or tuples stored in a 785distributed fashion e.g.
in hdfs .
calling a transformation on an rdd produces a new rdd that represents the result of applying the given transformation to the input rdd.
transformations are lazily evaluated.
the actual evaluation of an rdd occurs when anaction is called.
at that point the spark runtime executes all trans formations leading up to the rdd on which it then evaluates theaction e.g.
the count action counts the number of records in the rdd.
a complete list of transformations and actions can be foundin the spark documentation .
1val textfile spark.textfile hdfs ... 2val counts textfile .flatmap line line.split .map word word .reducebykey .collect figure scala word count application in apache spark figure shows a word count program written in spark using scala.
the frequency of each unique word in the input text file iscalculated.
it splits each word using a space as a separator andmaps each word to a tuple containing the word text and the ini tial count .
the reducebykey transformation groups the tuples based on the word i.e.
the key and sums up the word counts in the group.
finally the collect action triggers the evaluation of the rdd referencing the output of the reducebykey transformation.
the collect action returns a list of tuples to the driver program containing each unique word and its frequency.
the spark platform consists of three modules a driver a master and a worker.
a master node controls distributed job execution and provides a rendezvous point between a driver and the workers.the master node monitors the liveliness of all worker nodes andtracks the available resources i.e.
cpu ram ssd etc.
.
worker nodes are initiated as a process running in a jvm.
figure shows an example spark cluster containing three worker nodes a masternode and a driver.
a spark job consists of a series of transformations that end with an action.
clients submit such jobs to the driver which forwardsthe job to the master node.
internally the spark master translatesa series of rdd transformations into a dag of stages where each stage contains some sub series of transformations until a shuffle step is required i.e.
data must be re partitioned .
the spark scheduler is responsible for executing each stage in topological order with tasks that perform the work of a stage on input partitions.
each stage is fully executed before downstream dependent stagesare scheduled.
the final output stage evaluates the action.
the action result values are collected from each task and returned via the master to the driver program which can initiate another series oftransformations ending with an action.
figure represents the execution plan stage dag for our word count example in figure .
the input text is divided into three partitions.
the driver compiles the program into two stages.
stage applies the flatmap andmap transformations to each input partition.
a shuffle step is then required to group the tuples by the word.
stage 2processes the output of that shuffle step by summing up the counts for each word.
the final result is then collected andreturned to the driver.
in this example both stages are executed bythree tasks.
it is also worth noting that each task runs on a sep arate thread so each worker may run multiple tasks concurrentlyusing multiple executors based on resource availability such as the number of cores.
.
motiv ating scenario this section overviews b igdebug s features using a motivating g16 g60 g78 g79 g64 g77 g26 g74 g77 g70 g64 g77 g26 g74 g77 g70 g64 g77 g7 g77 g68 g81 g64 g77 g26 g74 g77 g70 g64 g77 g189 g138 g3 g10 g64 g79 g3 g82 g74 g77 g70 g64 g77 g78 g24 g78 g64 g77 g3 g19 g77 g74 g66 g77 g60 g72 g188 g138 g3 g21 g80 g73 g3 g19 g77 g74 g66 g77 g60 g72 g3 g190 g138 g3 g6 g74 g73 g73 g64 g62 g79 g3 g79 g74 g3 g82 g74 g77 g70 g64 g77 g78 g3 g60 g73 g63 g3 g60 g78 g78 g68 g66 g73 g3 g79 g60 g78 g70 g78 g190 g190 g190 g191 g191 g191 g191 g138 g3 g22 g64 g73 g63 g3 g79 g67 g64 g3 g77 g64 g78 g80 g71 g79 g3 g79 g74 g3 g79 g67 g64 g3 g63 g77 g68 g81 g64 g77 g192 g138 g3 g22 g67 g74 g82 g3 g74 g80 g79 g75 g80 g79 g3 g79 g74 g3 g79 g67 g64 g3 g80 g78 g64 g77 g3 g5 g68 g66 g7 g64 g61 g80 g66 g3 g6 g74 g73 g79 g77 g74 g71 g78 g136 g3 g7 g64 g61 g80 g66 g66 g64 g77 g3 g12 g73 g78 g79 g60 g73 g62 g64 g189 g136 g3 g6 g74 g72 g72 g80 g73 g68 g62 g60 g79 g68 g74 g73 g3 g6 g67 g60 g73 g73 g64 g71 figure architecture of spark with b igdebug example.
suppose that alice writes a spark program to parse and analyze election poll logs.
the log consists of billions of log entriesand is stored in amazon s3.
the size of the data makes it difficult to analyze the logs using a local machine only.
each log entry contains the phone number the candidate preferred by the callee thestate where the callee lives and a unix timestamp for example clinton texas 1val log s3n xcr wjy ws logs poll.log 2val text file spark.textfile log 3val count text file .filter line line.contains texas .filter line line.split .toint .map line line.split .reducebykey .collect figure election poll log analysis program in scala figure shows the program written by alice which totals the number of votes in texas for each candidate across all phone calls that occurred after a particular date.
line loads the log entry data stored in amazon s3 and converts it to an rdd object.
line 4selects lines containing the term texas.
line selects lines whosetimestamps are recent enough.
line extracts the candidate nameof each entry and emits a key value pair of that vote and the number .
line counts the votes for each candidate by summing by key.
alice already tested this program by downloading the first million log entries from the amazon s3 onto a local disk and running the spark program in a local mode.
when she tests her program with the subset of the data using a local mode there is no failure.
however when she runs the same program on a much big ger data stored in s3 using a cluster mode she encounters a crash.spark reports to alice the physical view of the crash only the typeof crash in this case numberformatexception with a stack trace the id of a failed task the id of an executor node encoun tering the crash the number of re trials before reporting the crash etc.
however such physical layer information does not help al ice to debug which specific input log entry is causing the crash.
though spark reports the task id of a crash it is impossible for alice to know which records were assigned to the crashed executorand which specific entry is causing the crash.
even if she identifiesa subset of input records assigned to the task it is not feasible forher to manually inspect millions of records assigned to the failed task.
she tries to rerun the program several times but the crash is persistent making it less probable to occur due to a hardware fail ure in the cluster.crash culprit determination.
with b igdebug alice is provided with a specific record causing the crash in this casea record trump illinois .
b igde786bug first reports a specific transformation responsible for the crash as well line in figure at time.toint which tries to change the timestamp from string to integer causing a numberformatexception.
using b igdebug s backward tracing feature alice then locates the specific log entry in the s3input causing the crash.
she then sees that this log entry uses atimestamp in date format rather than unix format.
realtime code fix and resume.
without b igdebug alice can only modify the input data and restart the job from scratch incurring wasted computation in running amazon ec2 services.
using bigdebug alice can fix the code on the fly by replacing the original filter at line number with the following one and resuming thefailed computation.
1filter line var time line.split val date new date yyyy mm dd if date.checkformat time time date.gettimeinunix time time.toint guarded watchpoint.
even after fixing the crash alice sees that the total number of votes found by the program is greater than whatshe expected to find.
using b igdebug s breakpoint alice investigates the intermediate result right after the second transformationat line number in figure .
without b igdebug investigating such intermediate result is not possible because spark com bines all transformations within a single stage and evaluates themall at once.
b igdebug allows a user to set a breakpoint at any transformation step and investigate the intercepted intermediate re sults.
she suspects that some unix timestamps may be in the 13digit millisecond format while the code assumes timestamps are in the digit second format.
she therefore installs a watchpointguarded by the following predicate an ordinary function line line.split .length .b igdebug dynamically retrieves the data matching this guard and she can con tinue to modify the guard iteratively in order to investigate further.
.
debugging primitives to provide interactive step wise debugging primitives in spark bigdebug must address three technical challenges.
first it must bescalable to handle large data sets on the order of terabytes.
second since the debugger process on the driver must monitor andcommunicate with a large number of worker nodes performing tasks on the cloud b igdebug must have a low overhead minimizing unnecessary communication and data transfer.
third to help localize the cause of errors b igdebug must support finegrained data inspection and monitoring capabilities at the level of individual records rather than tasks.
currently spark reports failures at the level of tasks only.
since a single task processes millions of records locating a failed task is inadequate as it is impossiblefor a user to manually inspect millions of records.
b igdebug tackles these challenges by adopting a tight integration approach with spark s runtime.
instead of creating a wrapperof existing spark modules to track the input and output of eachstage b igdebug directly extends spark to monitor pipelined intra stage transformations.
to maximize the throughput of bigdata processing b igdebug provides simulated breakpoints that enable a user to inspect a program state in a remote executor nodewithout actually pausing the entire computation.
to reduce de velopers burden in inspecting a large amount of data on demand watchpoints retrieve intermediate data using a guard and transfer the selected data on demand.
these primitives are motivated byprior user studies in inspector gadget where they interviewed1abstract class rdd .... 2def watchpoint f t boolean rdd 3def breakpoint 4def breakpoint f t boolean 5def enablelatencyalert set boolean 6def setcrashconfiguration set crashconfiguration 7def setfunction f t u 8def gobackall lineagerdd 9def gonextall lineagerdd 10def goback lineagerdd 11def gonext lineagerdd .... figure b igdebug s api disc developers in yahoo and found that disc developers wantstep through debugging crash culprit determination and tracing features.
inspector gadget proposes desired primitives but does not implement them.
g41 g41 g1 g33 g33 g33 g33 g1 g41 g42 g1 g27 g9 g18 g1 g25 g13 g29 g25 g4 g16 g18 g13 g1 g51 g24 g22 g9 g23 g17 g1 g1 g1 g1 g1 g1 g41 g43 g1 g1 g1 g1 g1 g1 g1 g1 g1 g1 g33 g25 g13 g29 g25 g4 g16 g18 g13 g37 g40 g15 g12 g14 g24 g32 g36 g36 g33 g33 g33 g40 g38 g1 g41 g44 g1 g27 g9 g18 g1 g11 g21 g26 g20 g25 g24 g1 g51 g1 g25 g13 g29 g25 g4 g16 g18 g13 g1 g41 g45 g1 g1 g1 g1 g1 g1 g1 g1 g1 g33 g14 g18 g9 g25 g8 g9 g22 g37 g18 g1 g51 g52 g1 g18 g33 g24 g22 g18 g16 g25 g37 g40 g1 g40 g38 g38 g1 g41 g46 g1 g1 g1 g1 g1 g1 g1 g1 g1 g33 g19 g9 g22 g37 g28 g21 g23 g12 g1 g51 g52 g1 g37 g28 g21 g23 g12 g31 g1 g41 g38 g38 g1 g41 g47 g1 g1 g1 g1 g1 g1 g1 g1 g1 g33 g23 g13 g12 g26 g11 g13 g3 g30 g6 g13 g30 g37 g39 g1 g50 g1 g39 g38 g1 g41 g48 g1 g11 g21 g26 g20 g25 g24 g33 g11 g21 g18 g18 g13 g11 g25 g1 g41 g49 g1 g33 g33 g33 g33 g1 g1 g1 g1 g41 g46 g1 g1 g41 g47 g1 g1 g41 g48 g11 g41 g49 g1 g3 g13 g6 g7 g12 g9 g1 g5 g6 g17 g8 g10 g15 g14 g11 g13 g17 g1 g1 g19 g20 g1 g9 g51 g52 g9 g33 g11 g21 g20 g25 g9 g16 g20 g24 g37 g34 g41 g42 g35 g38 g1 g33 g33 g33 g33 g1 g27 g9 g18 g1 g11 g21 g26 g20 g25 g24 g1 g51 g1 g25 g13 g29 g25 g4 g16 g18 g13 g1 g1 g1 g1 g1 g33 g14 g18 g9 g25 g8 g9 g22 g37 g18 g1 g51 g52 g1 g18 g33 g24 g22 g18 g16 g25 g37 g40 g1 g40 g38 g38 g1 g1 g1 g1 g1 g33 g19 g9 g22 g37 g28 g21 g23 g12 g1 g51 g52 g1 g37 g28 g21 g23 g12 g31 g1 g41 g38 g38 g1 g1 g1 g1 g1 g33 g23 g13 g12 g26 g11 g13 g3 g30 g6 g13 g30 g37 g39 g1 g50 g1 g39 g38 g1 g33 g33 g33 g33 g1 g1 g1 g1 g1 g33 g33 g33 g33 g1 g27 g9 g18 g1 g28 g21 g23 g12 g1 g51 g1 g25 g13 g29 g25 g4 g16 g18 g13 g1 g1 g1 g1 g33 g14 g18 g9 g25 g8 g9 g22 g37 g18 g1 g51 g52 g1 g18 g33 g24 g22 g18 g16 g25 g37 g40 g1 g40 g38 g38 g1 g1 g1 g1 g1 g28 g21 g23 g12 g33 g13 g20 g9 g10 g18 g13 g7 g9 g25 g13 g20 g11 g30 g2 g18 g13 g23 g25 g37 g38 g1 g27 g9 g18 g1 g11 g21 g26 g20 g25 g24 g1 g51 g1 g28 g21 g23 g12 g1 g1 g1 g1 g33 g28 g9 g25 g11 g15 g22 g21 g16 g20 g25 g37 g9 g51 g52 g9 g33 g11 g21 g20 g25 g9 g16 g20 g24 g37 g34 g41 g42 g35 g38 g38 g1 g1 g1 g1 g33 g19 g9 g22 g37 g28 g21 g23 g12 g1 g51 g52 g1 g37 g28 g21 g23 g12 g31 g1 g41 g38 g38 g1 g1 g1 g1 g33 g23 g13 g12 g26 g11 g13 g3 g30 g6 g13 g30 g37 g39 g1 g50 g1 g39 g38 g1 g33 g33 g33 g33 g1 g1 g1 g5 g20 g24 g25 g23 g26 g19 g13 g20 g25 g1 g1 g41 g43 g1 g41 g44 g27 g41 g45 g1 g1 g3 g13 g6 g7 g12 g9 g1 g4 g6 g17 g9 g13 g8 g18 g1 g2 g12 g9 g16 g17 figure b igdebug instruments a program automatically based on debugger control commands entered by a user.
the api for b igdebug is shown in figure and targets scala.
a user may also use a web based debugger ui to automatically insert corresponding api calls in the code.
for example the in strumented code on the bottom right of figure is automaticallygenerated from the debugger commands at the top of the figure.
.
simulated breakpoint a step by step execution to inspect intermediate outputs is a common debugging strategy.
there are two technical challengesin implementing breakpoints in spark.
first traditional breakpointswill pause the entire execution at the breakpoint while a user in vestigates an intermediate program state.
if we naively implementa normal breakpoint a driver will communicate with all executor nodes so that each executor will process data until the breakpoint in the dag and pause its computation until further debug commandsare provided.
this naive approach causes all the computing re sources on the cloud to be temporarily wasted decreasing throughput.
second spark optimizes its performance through pipelining transformations in a single stage.
therefore there is a mismatchbetween the logical view of data transformation and the physicalview of data processing during debugging.
for example whentwo transformations t1andt2are applied to x these are combined to t2 t1 x in a single stage and the intermediate result oft1 x is not viewable as it is not materialized or stored.
simulated breakpoint.
a simulated breakpoint enables a user to inspect intermediate results at a given breakpoint and resume the execution to create an illusion of a breakpoint even though the program is still running on the cloud in the background.
when g22 g79 g60 g66 g64 g3 g188 g22 g79 g60 g66 g64 g3 g189 g3 g16 g60 g75 g22 g79 g60 g66 g64 g3 g190 g21 g64 g63 g80 g62 g64 g5 g84 g14 g64 g84 g6 g74 g71 g71 g64 g62 g79 g9 g71 g60 g79 g16 g60 g75 g21 g64 g63 g80 g62 g64 g5 g84 g14 g64 g84 g5 g77 g64 g60 g70 g75 g74 g68 g73 g79 g22 g188 g22 g189 g22 g190 g22 g191 figure illustration of simulated breakpoint a simulated breakpoint is hit b igdebug spawns a new process to record the transformation lineage of the breakpoint while letting the executors continue processing the task.
for example in figure when a user sets a breakpoint afterflatmap program states2is captured from the original workflow without affecting its execution.
therefore setting a simulated breakpoint has almostzero overhead as it only retains information to re generate the program state from the latest materialization point i.e.
the last stage boundary before the simulated breakpoint in this case s1.
when a user requests intermediate results from the simulated breakpoint b igdebug then recomputes the intermediate results and caches the results.
if a user queries data between transforma tions such as flatmap andmap in figure within the same stage b igdebug forces materialization of intermediate results by inserting abreakpoint andwatchpoint described in section .
api call on the rdd object to collect the intermediate results.
resume and step over.
when a user enters a resume command using b igdebug s user interface b igdebug will automatically jump to the original workflow running in the background.
this procedure improves the overall throughput of the distributed processing.
when a user enters a step over command to investigate the state after the next transformation in the ui b igdebug replays the execution to the next instruction only from the latest materialization point.
this feature differentiates b igdebug from an existing replay debugger such as arthur which restarts a job from the beginning.
in figure when a user selects step over a new workflow will start from the nearest possible materialization point in this case s1.b igdebug uses the materialized states1and executes later operations while capturing s3on the go.realtime code fix.
when a user finds anomalies in intermediate data currently the only option is to terminate the job and rewrite theprogram to handle the outliers.
terminating a job at a later stagewill waste all computations before.
because running tasks on cloud costs lots of money and even days to process billions of records we hypothesize that developers are less likely to terminate the programafter inspecting it at the breakpoint.
to save the cost of re run b igdebug allows a user to replace any code in the succeeding rdds after the breakpoint.if a user wants to modify code b igdebug applies the fix from the last materialization point rather than the beginningof the program to reuse previously computed results.
assum ing that a breakpoint is in place a user submits a new function i.e.
a data transformation operator at the driver.
the function is then compiled using scala s nsc library andshipped to each worker to override the call to the original func tion when the respective rdd is executed.
suppose a user sets a breakpoint after flatmap and the program is paused in figure .
a user can replace the function in the map transformation from word word to word if word!
null word else word .
when a user resumes after the fix at s2 a new workflow will start from s1and later rdds including modified ones will be computed until the end of the workflow is reached and the background job of theoriginal workflow is terminated.
b igdebug checks whether the supplied function has the same type as the original function through static type checking.
there fore a user cannot provide a code fix that breaks the integrity ofthe used data type.
when a user replaces function fwith a new function g b igdebug applies gto all records from the last materialization point to ensure consistency.
.
on demand watchpoint with guard similar to watching a variable in a traditional debugger like gdb b igdebug provides a watchpoint to inspect intermediate data.
because millions of records are passing through a data parallel pipeline it is infeasible for a user to inspect all intermediaterecords.
such data transfer would also incur high communication overhead as all worker nodes must transfer the intermediate results back to the driver node.
to overcome these challenges b igdebug provides an on demand watchpoint with a guard closure function.
a user can provide a guard to query a subset of data matchingthe guard.
for example r r is an anonymous function that takes ras input and returns true if r is greater than and rdd.watchpoint r r sets a watchpoint to retrieve all records greater than .
b igdebug automatically compiles such user provided guard and distributes it to worker nodes to retrieve the matching data.
on demand batch transfer.
to reduce communication overhead b igdebug batches intermediate data and sends them to the driver when needed.
if no request is made for the watchpointeddata from the user it will be kept at the workers until the end of the stage or the next breakpoint in the same stage if there is any.
when the computation passes the end of the stage the remainingfiltered data are flushed so that there are enough memory availablefor other spark operations.
dynamic guard modification.
a user can modify a guard function to narrow down the scope of captured data.
this feature is called a dynamic guard as the function can be refined iteratively while the spark job is executing.
a watchpoint guard is builton top of scala and java and is written like a normal spark program.
for example an expression value.
1.length value.
2 filters values where the length of first element in tuple is greater than and the value of second element in tuple is .
when a user updates a guard during a spark job b igdebug uses scala s nsc library to compile the guard at the driver node and ships it to all workers.
individual workers thenload the new guard at each executor.
using a dynamic guard a usercan tune the amount of data being transferred and presented.
.
crash culprit and remediation disc systems are limited in their ability to handle failures at runtime.
in spark crashes cause the correctly computed stages to sim ply be thrown away.
remediating a crash at runtime can save timeand resources by avoiding a program re run from scratch.
b igdebug leverages fine grained tracing to be discussed in section .
to identify a crash inducing input not just a crash culprit record in the intermediate stage.
while waiting for a user intervention b igdebug runs pending tasks continuously to utilize idle resources and to achieve high throughput.
crash culprit determination.
when a crash occurs at an executor b igdebug sends all the required information to the driver so that the user can examine crash culprits and take actions as depictedin figure .
when a crash occurs b igdebug reports a crash culprit an intermediate record causing a crash a stack trace a crashed rdd and the original input record inducing a crash by leveraging backward tracing in section .
.
remediation.
b igdebug avoids the re generation of prior stages by allowing a user to either correct the crashed record skip thecrash culprit or supply a code fix to repair the crash culprit.
a g21 g64 g62 g74 g77 g63 g147 g194 g195 g148 g147 g195 g196 g104 g148 g147 g200 g201 g148 g147 g199 g148 g21 g64 g62 g74 g77 g63 g3 g3 g194 g195 g3 g3 g200 g201 g3 g3 g199 g72 g60 g75 g159 g77 g3 g211 g213 g3 g77 g137 g79 g74 g12 g73 g79 g160 g16 g60 g75 g6 g77 g60 g78 g67 g3 g21 g64 g75 g74 g77 g79 g3 g79 g74 g3 g7 g77 g68 g81 g64 g77 g147 g195 g196 g148 g7 g77 g68 g81 g64 g77 g147 g195 g196 g104 g148 g189 g190 figure an intermediate record 23s at the map transformation causes a crash.
b igdebug reports the crash culprit 23s to the user and a user supplies the corrected record .
naive resolution approach is to pause the execution report the crash culprit to the driver and wait until a resolution is provided from theuser.
this method has the disadvantage of putting the workers in the idle mode reducing throughput.
therefore b igdebug provides the following two methods.
lazy repair of records.
in case of a crash the executor reports a crash culprit to the driver but continues processing the remaining pending records.
once the executor reaches the end of the task it then waits for a corrected record from the user.
this approach parallelizes the crash resolution with out holding back the executor.
if there are multiple crashculprits b igdebug accumulates the crashed records at the driver and lets all executors terminate except the very lastexecutor.
the last executor on hold then processes the groupof corrected records provided from the user before the end ofthe stage.
this method applies to the pre shuffle stage only because the record distribution must be consistent with existing record to worker mappings.
this optimization of replacing crash inducing records in batch improves performance.
lazy code fix.
b igdebug accumulates crash culprits at the driver and lets all executors continue processing the pending records.
it then asks a user to supply a code fix to repair the crash culprits i.e.
a new repair function to applyto the crash culprits.
our assumption is that the new func tion extends the original function to clean the crash inducingrecords and a user would like to see some results rather than nothing because the current spark does not provide any output to the user when a task crashes even for successful in puts.
if a user wants to apply a completely different functiongto all records she can use our realtime code fix at simulated breakpoint instead.
similar to the above lazy repair ofrecords the last executor on hold applies the supplied func tion to the crash culprits in batch before the end of the stage.
.
forward and backward tracing bigdebug supports fine grained tracing of individual records by invoking a data provenance query on the fly.
the data provenance problem in the database community refers to identifying the origin of final or intermediate output.
data provenance sup port for disc systems is challenging because operators such asaggregation join and group by create many to one or many to many mappings for inputs and outputs and these mappings are physically distributed across different worker nodes.
b igdebug uses data provenance capability implemented through an extension of spark s rdd abstraction called lineagerdd that leverages spark s built in fault tolerance and datamanagement infrastructure .
the lineagerdd abstraction provides programmers with data provenance query capabilities.
prove nance data is captured at the record level granularity by taggingrecords with identifiers and associating output record identifierswith the relevant input record identifier for a given transformation.
from any given rdd a spark programmer can obtain a lin0 after is said14 and18 done more is said than done ... after is ... is ... done after is ... done offset text key partial count key total count agent1 input output a b c ... ...agent2 input output a x a y ... ... c w ... ... c zagent3 input output x y w ... ... z input output a y c wstep input output a cstep figure a logical trace plan that recursively joins data lineagetables back to the input lines eagerdd reference and use it to perform data tracing i.e.
the ability to transition backward or forward in the spark program dataflow at the record level.
b igdebug instruments submitted spark programs with tracing agents that wrap transformations at stage boundaries.
these agents implement the lineagerdd abstraction and have two responsibilities tag input and output records with unique identifiers for a given transformation and store the associations between input and output record identifiers as a data provenance table in spark snative storage system.
for example figure shows intermediateresults and a corresponding data provenance table for each agent.
for example the first line at offset after is said is assigned with a unique output identifier a. as this line is split into multiple records such as after and is unique output identifiers suchasxandyare assigned to the corresponding key and partial count pairs.
b igdebug utilizes specific tracing agents based on the type of transformation e.g.
data ingested from hadoop distributed file system hdfs and amazon s3 and all native spark transforma tions agents are pluggable making it easy to support other inputdata storage environments e.g.
cassandra hbase and rdbms.
once the provenance data is recorded tracing queries can be issued using the lineage related methods of the api in figure .
thegobackall andgonextall methods are used to compute the full trace backward and forward respectively.
that is given someresult record s gobackall returns all initial input records e.g.
in hdfs that contributed in the generation of the result record s gonextall returns all the final result records that a starting input record s contributed to in a transformation series.
a single stepbackward or forward is supported by the goback andgonext respectively.
at any given point in the trace the user can interac tively issue a native spark collect action to view the raw data referenced by the lineagerdd.
when a tracing query is issued b igdebug logically reconstructs the path connecting input to output records by recursively joining the provenance tables generated by the tracing agents as shown in figure using the word count example.
after executing aword count job a user may want to perform a full backward tracingfrom the output value is .b igdebug does this by first retrieving the identifier for output is from the output provenance table.
figure shows that identifier to be equal to and the corresponding mappings have two input identifiers yandw.
tracing proceeds by recursively joining the provenance tables at neighboring cap ture agents along the output and input values.
1represents a join operation of provenance tables.
for instance the join of agent g21 g64 g75 g74 g77 g79 g3 g79 g74 g3 g63 g64 g61 g80 g66 g66 g64 g77 g21 g64 g62 g74 g77 g63 g77 g188 g77 g189 g77 g190 g77 g191 g21 g64 g62 g74 g77 g63 g23 g68 g72 g64 g3 g159 g72 g78 g160 g77 g188 g120 g192 g77 g189 g120 g189 g187 g187 g77 g190 g120 g195 g77 g191 g120 g193 g21 g7 g7 figure when latency monitoring is enabled straggler records are reported to the user.
and agent 2produces the step 1trace result.
subsequently joining step 1with agent 1produces the step 2trace result which is the final trace result referencing hdfs input records at offsets and18 both containing the word is .
supporting data provenance while logically simple is difficult to achieve in a disc environment such as spark because the size of input output identifier mappings is as large as all intermediate re sults and data provenance tables are physically distributed acrossworker nodes.
for these reasons spark s internal storage service is used for storing provenance data and a distributed join implementation uses partition information embedded into the record identi fiers to optimize the shuffle step.
implementing an optimized distributed join of data provenance tables in spark is the subject of another paper which details the advantage of an optimizeddistributed join in spark over storing data provenance tables in ex ternal storage services e.g.
hdfs mysql .
.
fine grained latency alert in big data processing it is important to identify which records are causing delay.
spark reports a running time only at the level of tasks making it difficult to identify individual straggler records records responsible for slow processing.
to localize performance anomalies at the record level b igdebug wraps each operator with a latency monitor.
for each record at each transformation b igdebug computes the time taken to process each record keeps track of a moving average and sends to the monitor if the time is greater than kstandard deviations above the moving average where default kis .
figure shows an example of how straggler records are tagged and reported to the debugger.
as we show in section record level latency alert poses the highest overhead among b igdebug s primitives due to the cost of taking a timestamp for processing each record and computing the movingaverage among millions of records per executor.
.
ev aluation we evaluate b igdebug s scalability performance overhead localizability improvement in determining crash inducingrecords and time saving w.r.t to an existing replay debug ger.
the main purpose of our evaluation is to investigate whether b igdebug keeps performance similar to the original spark and retains its scalability while supporting interactive debugging.
how does b igdebug scale to massive data?
what is the performance overhead of instrumentation and ad ditional communication for debugging primitives?
how much localizability improvement does b igdebug achieve by leveraging fine grained record level tracing?
how much time saving does b igdebug provide through its runtime crash remediation in comparison to an existing re play debugger?
we use a cluster consisting of sixteen i7 machines each running at .40ghz and equipped with cores hyper threadsper core 32gb of ram and 1tb of disk capacity.
the oper ating system is a 64bit ubuntu .
.
the datasets are all storedon hdfs version .
.
with a replication factor of one copy of each dataset across the cluster.
we compare b igdebug s per formance against the baseline spark version .
.
.
the level of parallelism was set at two tasks per core.
this configuration allowsus to run up to tasks simultaneously.
we use three spark programs for our performance experiments wordcount grep and pigmix query l1.
wordcount computes the number of word occurrences grouped by unique words.
wordcount comprises of transformations which are divided into stages thefirst stage loads the input file textfile splits each line into a bag of words flatmap creates a tuple of word map andthe second stage reduces the key value pairs using each word as thekey reducebykey .
grep finds the lines in the input datasets that contain a queried string.
grep comprises of only stage with transformations textfile reads the input file line by line and filter applies a predicate to see if the line contains a substring.
pigmix s latency query l1 is a performance benchmark for discsystems in which an unstructured data is transformed and analyzed.l1 comprises of stages the first stage contains textfile maps flatmap followed by maps and the second stage has reducebykey followed by a map.
for our performance experiment we vary input size from 500mb to 1tb by using an unstructured data made up of zipf distribution over a vocabulary of terms.
all runs are repeated10 times.
we compute a trimmed mean by removing the shortest runs and the longest runs because the running time of discprograms depends on various factors such as a warm up of hdfscache garbage collection and network i o. in big data systems a variation of is considered noise because of these factors.
.
scalability we perform experiments to show that b igdebug scales to massive data similar to spark.
more specifically we assess the scale up property b igdebug can ingest and process massive data and its running time increases in proportion to the increase in data size.
we assess the scale out property as the number of worker nodes the degree of parallelization increases the running time decreases.scaling up.
figure a shows the scale up experiment while varying the data size from a few gigabytes to one terabyte.
in thisexperiment b igdebug is used with the maximum instrumentation where breakpoints and watchpoints are set in every line and latency monitoring crash remediation and data provenance are enabled for every record at every transformation.
the running timeof b igdebug grows steadily in proportion to the running time of spark.
as the data size increases b igdebug s running time also increases because large input data requires the scheduler to createmore tasks assign the tasks to workers and coordinate distributedexecution to improve data locality throughput and hdfs caching.
with such maximum instrumentation b igdebug scales well to massive data 1tb .
figure b shows the overhead in wordcount.b igdebug takes .5x longer on average in comparison to the baseline spark.
this .5x overhead is a very conservativeupper bound as a developer may not need to monitor the latency ofeach record at every transformation and may not need to set breakpoints on every operator.
when disabling the most expensive feature record level latency monitoring b igdebug poses an average overhead of only.
in grep and l1 the overheads are .76x and .38x with the maximum instrumentation and and respectively when latency monitoring is disabled see table .
overheadbenchmark dataset gb max w o latency pigmix l1 .38x .29x grep ... .76x .07x word count .
to increment with a log scale .5x .34x table performance evaluation on subject programs dataset gb ti m e s bigdebug maximum apachespark a scale up1 .
.
.
.
.
.
dataset gb times of apache sparkbigdebug s maximum overhead b maximum instrumentation234567850100150200250300350400baseline spark 10gb baseline spark 30gb baseline spark 50gb wo rke rsti m e s 10gb 30gb 50gb c scale out instructions with breakpoint out of ti m e s traditional breakpoint simulated breakpoint d traditional vs. simulated breakpoint0 data captured with watchpoint mb overhead watchpoint overhead on 40gb dataset e watchpoint20 dataset gb overhead tracing crash monitoring watchpoint tracing crash monitoring watchpoint f overhead per feature figure b igdebug s scalability and overhead scaling out.
the property of scaling out is essential for any disc system.
we change the number of worker nodes essentially the total number of cores on a fixed size dataset and record a running time for both b igdebug and the baseline spark.
as the number of cores increases the running time should decrease.
figure c shows that b igdebug does not affect the scale out property of the baseline spark.
when there are too many workers it makes it hardfor the spark scheduler to retain locality and b igdebug also ends up moving data around at stage boundaries between workers.
.
overhead we measure the overhead as the increase in the running time of b igdebug w.r.t the baseline spark.
the performance overhead comes from instrumentation and communication between thedriver and the workers and data transfer to carry the requested de bug information to the user.
b igdebug works at the record level.
therefore the overhead increases in proportion to the number of records in each stage.
for example when the wordcount program splits input lines into a bag of words at the flatmap transformation the overhead increases.simulated breakpoint.
b igdebug offers almost overhead to pause and instantly resume.
this overhead is the same across all subject programs because instantly resuming simulated break point does not involve any additional instrumentation.
on the otherhand a traditional breakpoint adds overhead because it must pause and cache the intermediate state entirely.
figure d shows the comparison results between simulated breakpoints and traditionalbreakpoints while setting a breakpoint at different transformations.traditional breakpoints incur orders of magnitude higher overhead since they materialize all intermediate results regardless of whether a user requests them or not.
for example a traditional breakpoint soverhead rises sharply after a flatmap transformation that emits a large number of unaggregated records.on demand watchpoint.
we place a watchpoint between two instructions such that a custom guard reports only a certain percent age of intermediate results e.g.
etc.
once captured the intermediate data is sent to the driver for a user to inspect.
in figure e the x axis shows the amount of captured data rang ing from 500mb to 3gb when the total intermediate data size is 40gb.
when 500mb of data is transferred to the user at the watchpoint it incurs only overhead.
this is a very conservative setup since the user is unlikely to read 500mb records at once.
to iso late the overhead of setting watchpoints from the overhead of data transfer we set a watchpoint at every transformation with a guard that always evaluates to false.
among all subject program wordcount poses the maximum overhead of on average whereas the overhead for grep and l1is and respectively.
crash monitoring.
we enable crash monitoring for every transformation and vary the data size from 20gb to gb to measureoverhead.
see figure f .
crash monitoring imposes over head in l1.
wordcount and grep incur a lower overhead of and respectively.
this overhead comes from monitoring every record transformation for any kind of a failure and checking if there are any pending crashed records to be repaired at the end oflocal execution in a task.backward and forward tracing.
we enable record level data provenance capturing for every transformation and vary the data size from 20gb to 90gb.
see figure f .
the tracing primitive poses on average overhead over the baseline spark in l1.
wordcount and grep pose on average and overhead respectively.
the majority of the overhead comes from the generation of the data provenance tables maintaining the associations between input and output record identifiers.
the cost of storing each dataprovenance table into spark s storage layer is small because it isperformed asynchronously.fine grained latency alert.
we enable record level latency monitoring on every transformation.
record level latency moni toring incurs the highest overhead among b igdebug s features.
latency monitoring alone takes .4x longer than baseline spark ondata size varying from 500mb to 1tb see figure a in word7911 .
.
.
.
.
.
datasize gb t imesofapachesparklatency alert a latency alert overhead2 number of tasks in jobaverage records per task millions records per task execution time ti m e s b localizability vs. performances1 s2 s3 s4050100150200 location of crash stage ti m e s bigdebug arthur c time saving figure latency monitoring overhead time saving through crash remediation and localizability improvement count.i n grep and l1 the overhead is .74x and .24x.
the significant overhead comes from performing a statistical analysis for each record transformation.
a timestamp is recorded before and after the transformation of each record to see if its latency lies within standard deviations from the average.
the overhead is alsoincurred by updating a moving average and a standard deviation.if we disable record level latency monitoring from the maximum instrumentation setting the overhead decreases from i.e.
.5x of baseline spark to i.e.
.34x of baseline spark inthe case of the wordcount program.
.
crash localizability improvement spark reports failures at the level of tasks while b igdebug reports specific failure inducing inputs.
b igdebug also detects specific straggling records while spark only reports a stragglertask causing delay.
by construction our record level tracing approach has accuracy with zero false positive because it leverages data provenance to identify all inputs contributing to a failure.
delta debugging can further isolate this combination of multi ple failure inducing inputs but will require a larger number of runs as opposed to b igdebug that requires only a single run.
therefore we measure the improvement in localizing failed or delayed records in comparison to the baseline spark.
when debugging a program using spark a developer may want to increasethe number of tasks to improve fault localizability at the cost of running time.
note that the running time increases as you increase the number of tasks since more resources are used for communica tion and coordination among distributed worker nodes.
to quantifythis we vary the number of tasks and measure the average number of records per task and the total running time.
see figure b .
when configuring spark with tasks and 60gb dataset eachtask handles .
million records on average.
if a single recordamong .
million records crashes the task it is impossible for ahuman being to identify a crash culprit.
to improve fault localizability if a developer increases the number of tasks from to to reduce the number of records assigned to each task eachtask still handles .
million records and additional communica tion and coordination increases the total running time by .
times.
as figure f shows b igdebug incurs less than overhead for reporting crash culprits and takes only .4x time for latency alert while improving fault localizability by orders of millions.
.
time saving through crash remediation when a task crashes the current spark does not provide any output to the user even for successful inputs and terminates the taskimmediately.
on the other hand b igdebug allows a user to remove or modify a crash culprit at runtime to avoid termination.
therefore b igdebug avoids additional runs when a user tries toremove crash inducing records from the original input.
for the same scenario using a post hoc instrumentation replay debugger arthur requires at least three runs.
in the first run a programcrashes and spark reports failed task ids.
in the second run a user must write a custom post hoc instrumentation query a new data flow graph with those failed task ids and run the query to recom pute the intermediate results for the failed tasks.
in the third run auser removes the crash inducing records and re runs the job again.
crashes in later stages result in more redundant work in the second and third runs and hence more time for completion.
when a crashoccurs at the 9th stage of a stage job arthur must recompute thefirst stages twice while b igdebug avoids such re computation completely by allowing a user to resolve crashes in the first run.
figure c shows our experiment result on time saving.
we compare b igdebug s time saving with a post hoc instrumentation replay debugger like arthur.
we conservatively estimatearthur s performance by running the original spark for its first and third runs.
we measure time saving by dividing the additional time required by arthur by b igdebug s completion time.
we seed crashes in different transformation locations byupdating an original program to throw an exception at a givenstage because the magnitude of time saving depends on which stage a crash occurs.
for example we replace the map function word word with word if word crash crash word where crash always throw a nullpointerexception.
s1 is a program where crashes are seeded in the first stage s2 is a program where crashesare seeded in the second stage etc.
b igdebug saves the execution time by on average and reaches up to after s2.
in theexperiment the most time consuming stage is s2 and a crash in s2or later saves a large amount of time.
.
related work debugging needs for disc.
fisher et al.
interviewed data analysts at microsoft and studied the painpoints of big data analyt ics tools.
their study finds that a cloud based computing solutionmakes it far more difficult to debug.
data analysts often find them selves digging through trace files distributed across multiple vms.
zhou et al.
manually categorize randomly sampled escalation of a big data platform at microsoft.
the study finds that .
of in field failures i.e.
escalations are caused by system side de fects which include logical and design errors of disc applications.
job failures and slowdowns are common in disc applications accounting for and of the escalations.
these findings mo tivate b igdebug .
execution log analysis of disc applications.
several approaches help developers debug disc applications by collecting 792and analyzing execution logs.
boulon et al.
monitor hadoop clusters at runtime and store the log data.
their system collects logs for understanding a runtime failure but does not provide realtime debug primitives.
developers typically develop disc appli cations using a small sample of data in a local mode or a pseudocloud environment first and deploy the application on a larger cloudwith a considerably larger data set and processing power.
shang et al.
compare the execution log captured on the cloud with the log captured using a local mode.
their system abstracts the exe cution logs recovers the execution sequences and compares the se quences between the pseudo and cloud deployments.
tan et al.
analyze hadoop logs to construct state machine views of the program execution to help a developer understand a hadoop execu tion log.
their approach computes the histogram of the duration ofeach state and detects anomalies in the program execution.
xu etal.
parse console logs and combine source code analysis to detect abnormal behavior.
fu et al.
map free form text messages in log files to logging statements in source code.
none of thesepost mortem log analysis approaches help developers debug discapplications realtime.
debuggers for disc applications.
inspector gadget is a framework proposal for monitoring and debugging data flow programs in apache pig .
the proposal is based on informal in terviews with ten yahoo employees who write disc applications.while inspector gadget proposes features such as step through debugging crash culprit determination tracing etc.
it simply lists desired debug apis but leaves it to others to implement the pro posed apis.
the tracing api proposed by inspector gadget targetscoarse grained off line tracing using a centralized server falling behind b igdebug s ability to trace individual records at runtime.
arthur is a post hoc instrumentation debugger that targets spark and enables a user to selectively replay a part of the original execution.
however a user can only perform post mortem analysis and cannot inspect intermediate results at runtime.
it also requires a user to write a custom query for post hoc instrumentation.
to localize faults arthur requires more than one run.
for example toremove crash inducing records from the original input in the firstrun a program crashes and spark reports failed tasks ids.
in the second run a user must write a custom post hoc instrumentation query a new data flow graph with those failed task ids and runthe query to recompute the intermediate results for the failed tasks.in the third run a user removes the crash inducing records and re runs the job again.
such post hoc instrumentation incurs significant debugging time as demonstrated by section .
.
recon opts for a post hoc instrumentation strategy like arthur.
graft is a debugger for a graph based disc computing framework apache giraph .
graft requires a user to select vertices similar to executors in spark to capture events and replay the execution later.
graft assumes that a user has adequate priorknowledge to know buggy vertices.
similar to arthur graft is apost hoc instrumentation debugger.
moreover graft is built forprocessing graphs only and is not applicable to a data flow framework like spark.
daphne lets users visualize and debug dryadlinq programs.
it provides a job object model for viewing the running tasks and enables a user to attach a debugger to a remote process on the cluster.
this approach works in dryadlinq because all communications between tasks is through disk.
such approach could workfor hadoop or mapreduce that persist intermediate results in thefile system but does not work for an in memory processing frame work such as spark that achieves orders of magnitude better performance through in memory processing and lazy evaluation.
data provenance for disc applications.
there is a large body ofwork that studies techniques for capturing and querying data prove nance in data oriented workflows .
ramp instrumentsapache hadoop with agents that wrap the user provided map and reduce functions.
ramp agents store data provenance tables in hdfs and enable a user to query data provenance data usingapache hive and pig .
newt captures data prove nance tables and stores in mysql clusters.
a user must submitsql queries to join data provenance tables in an iterative loop.
because both ramp and newt do not store the referenced raw data a user can see record identifiers only and cannot view intermediateresults on the fly.
b igdebug supports fine grained tracing by leveraging prior work on data provenance within spark .
the data provenance capability used by b igdebug is orders of magnitude faster because it stores data provenance tables in memory within spark s runtime and performs an optimized distributed join of the prove nance tables.
the design of an optimized distributed join algorithm in spark is a subject of another paper and is described else where .
data provenance alone cannot support realtime debugging since a user needs primitives such as simulated breakpointsand guarded watchpoints to interact with a data parallel pipeline atruntime and data provenance queries must be invoked in the context of crashes failures or a breakpoint.
replay debugger.
replay debugging for distributed systems has been extensively studied through systems such as li blog r2 and dcr .
these systems are designed to replay general distributed programs and thus recording all sources of non determinism including message passing order across nodes system calls and accesses to memory shared across threads.
theirgoal is to reproduce errors using the captured events.
these replaydebuggers incur significant overhead at runtime and even larger slowdown at replay time.
in contrast b igdebug leverages the structure of a data flow graph to replay sub computations and a partial replay is to support step through debugging while a program is still running.
frameworks like d3s maceodb and aguilera et al.
are distributed debuggers for finding framework bugs not application bugs.
.
conclusion big data debugging is currently a painstakingly long and expensive process.
b igdebug offers interactive debugging primitives for an in memory data intensive scalable computing disc frame work.
to emulate traditional step wise debugging in the contextof in memory big data processing b igdebug offers simulated breakpoints and guarded watchpoints with little performance over head.
b igdebug enables a user to determine crash culprits and resolve them at runtime avoiding a program re run from scratch.by leveraging fine grained data provenance b igdebug reports the origin of a crash culprit and supports tracing intermediate re sults forward and backward at the record level.
it scales to massive data in the order of terabytes improves fault localizability by orders of millions than baseline spark and provides up to timesaving with respect to a posthoc instrumentation replay debugger.
in terms of future work we plan to construct spark program benchmarks and conduct user studies with professional software engineers.
instead of having a user specify a guard for an ondemand watchpoint extracting data invariants from intercepted in termediate results may be useful for helping the user debug a discprogram.
another area for future work is tool assisted automated fault localization in b igdebug .
for example with the help of automated fault localization we envision that a user can isolate the trace of a failure inducing workflow diagnose the root cause of anerror and resume the workflow for only affected data and code.
793acknowledgements we thank the anonymous reviewers for their comments.
participants in this project are in part supported through nsf ccf1527923 ccf iis cns and nih u54eb020404.
we would also like to thank our industry partnersat ibm and intel for their gifts.
.