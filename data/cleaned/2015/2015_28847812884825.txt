decoupling level a new metric for architectural maintenance complexity ran mo yuanfang cai drexel university philadelphia pa usa rm859 yc349 drexel.edurick kazman university of hawaii sei cmu honolulu hi usa kazman hawaii.edulu xiao qiong feng drexel university philadelphia pa usa lx52 qf28 drexel.edu abstract despite decades of research on software metrics we still cannot reliably measure if one design is more maintainable than another.
software managers and architects need to understand whether their software architecture is good enough whether it is decaying over time and if so by how much.
in this paper we contribute a new architecture maintainability metric decoupling level dl derived from baldwin and clark s option theory.
instead of measuring how coupled an architecture is we measure how well the software can be decoupled into small and independently replaceable modules.
we measured the dl for open source projects and industrial projects each of which has multiple releases.
our main result shows that the larger the dl the better the architecture.
by better we mean the more likely bugs and changes can be localized and separated and the more likely that developers can make changes independently.
the dl metric also opens the possibility of quantifying canonical principles of single responsibility and separation of concerns aiding cross project comparison and architecture decay monitoring.
ccs concepts software and its engineering !software architectures keywords software architecture software quality software metrics .
introduction despite decades of research on software metrics we still cannot reliably measure if one design is more maintainable than another.
software managers and architects need to understand whether their software architecture is good enough whether it is decaying and if so by how much.
most wellknown metrics focus on measuring the complexity and quality of code such as mccabe s cyclomatic complexity permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may austin tx usa c acm.
isbn .
.
.
.
chidamber kemerer s metrics .
these metrics have been used for defect prediction and localization but not for comparing design alternatives or indicating architecture decay.
maccormack et al.
proposed propagation cost pc to measure how tightly source les are coupled.
pc has been used by multiple companies to monitor coupling variation in a system.
but as we will show pc is sensitive to the number of source les and does not always capture architecture level variations.
in this paper we contribute a new architecture maintainability metric decoupling level dl .
instead of measuring the level of coupling we measure how well the software is decoupled into small and independently replaceable modules.
this metric is derived from baldwin and clark s design rule theory which suggests that modularity creates value in the form of options each module creates an opportunity to be replaced with a better version hence improving the value of the system.
accordingly small independently changeable modules are most valuable and the more such modules there are the higher the system s value.
the implication of option theory in software design is signi cant an independent module in software implies that its bugs can be xed locally and changing it will not cause any ripple e ects.
the smaller the module the easier it is to improve and the more such small independent modules there are the more developers can contribute to the system independently and in parallel.
in our prior work we created adesign rule hierarchy drh clustering algorithm to identify independent modules.
we now calculate the decoupling level of a system from its drh.
we measured the dl for open source and industrial projects each having multiple releases.
we observed that of them have dls between and .
to evaluate if dl can be used as a real metric we use it both to compare multiple snapshots of the same project and to compare dls collected across multiple projects.
the results showed that the dl values extracted from consecutive non refactoring snapshots are very stable and non trivial variation of dl indicates major architectural degradation or improvement.
to evaluate if projects with higher dl have higher maintainability we contribute a suite of maintainability measures that can be extracted from the revision history of a project.
these measures indicate the extent to which les were changed separately and to what extent committers have to change the same set of les.
our results show that the larger the dl the more likely bugs and changes can be localized and separated and the more likely that developers can make changes independently.
ieee acm 38th ieee international conference on software engineering with intellectual roots in design rule theory dl quantitatively measures canonical principles of single responsibility and separation of concerns.
although the ability to support parallelized development and localized changes is just one of many aspects of software maintainability it is a critical one that needs to be measured and monitored continuously.
.
background in this section we introduce the fundamental concepts behind our new metric including design rule theory design structure matrix and design rule hierarchy drh .
design rule theory.
baldwin and clark proposed design rule theory that explains how modularity adds value to a system in the form of options.
their theory suggests that independent modules are decoupled from a system by the creation of design rules drs .
the independent modules should only depend on drs.
as long as the drs remain stable a module can be improved or even replaced without a ecting other parts of the system.
since sullivan et al.
introduced design rule theory to software design we have observed that design rules are usually manifested as interfaces or abstract classes.
for example in an observer pattern the observer interface decouples the subject and concrete observers into independent modules.
as long as the interface is stable the subject shouldn t be a ected by the addition removal or changes to concrete observers.
in this case the observer interface is considered to be a design rule decoupling the subject and concrete observers into two independent modules .
design rule hierarchy drh .
to detect design rules and independent modules within a software system our prior work de ned a clustering algorithm design rule hierarchy drh which clusters a system s les into a hierarchical structure with nlayers where layer contains the most in uential les typically interfaces or abstract classes that have many dependents.
files in layer ishould only depend on les in higher layers i.e.
layer j where j i but not depend on les in lower layers.
the unique feature of a drh clustering is that les in the same layer are decoupled into modules that are mutually independent from each other.
independence here means that changing or replacing one module will not a ect other modules in the same layer.
the modules in layer n that is the bottom layer of the drh are truly independent modules because each can be improved or replaced without in uencing any other parts of the system.
design structure matrix dsm .
design rules and modules as well as the structure of the design rule hierarchy can be visualized using a design structure matrix dsm .
a dsm is a square matrix its rows and columns are labeled with the same set of les in the same order.
take the dsm in figure 1a as an example.
the columns and rows are labeled with the names of java classes reverse engineered from the source code of a student project submission.
a marked cell in row x column y cell x y means that the le in row x depends on the le in column y. the marks in the cell are re ned to indicate di erent types of dependencies.
for example in figure 1a cell is labeled with ex cl which is short for extend call .
this cell indicates that question is an abstract class and match a question matching class extends it and calls one of its methods.
the cells along the diagonal represent selfdependency.
in this paper we use x to denote an unspec i ed dependency type in a dsm.
the dsm in figure 1a is an automatically generated drh structure with layers.
layer has one class ui file which is the interface that decouples the module with two user interface classes textfileui file and commandlineui file from their clients that is various question and answer classes file to file .
layer has one module with two les the abstract question file and answer file classes.
the third layer contains one module with one le survey file that aggregates a collection of objects of type question .
these four les in the topmost three layers completely decoupled the rest of the system into independent modules in the last layer layer from file to file .
consider the module containing choice file andchoiceanswer file .
the designer could choose better data structures for these multiple choice question classes without in uencing any other classes.
independence level.
based on design rule theory the more independent modules there are in a system the higher its option value.
in our prior work we proposed a metric called independence level il to measure the portion of a system that can be decoupled into independent modules within the last layer of its drh.
for example the il in the dsm of figure 1a is .
because out of the les are in the last layer.
the decoupling level metric we propose here improves on the il metric.
propagation cost.
maccormack et al.
s propagation cost metric also calculated based on a dsm aims to measure how tightly coupled a system is.
given a dsm they rst calculate its transitive closure to add indirect dependencies to the dsm until no more can be added.
given the nal dsm with all direct and indirect dependencies pc is calculated as the number of non empty cells divided by the total number of cells.
for example the pcs of the three dsms in figure are and respectively.
the lower the pc the less coupled the system.
the problem with il is that it doesn t consider the modules in the top layers of a drh nor does it consider the size of a module.
working with our industrial partners we observed cases where the lowest layer contained very large modules.
in these cases even through the il appeared to be high the system was notwell modularized.
in other cases we observed that even though the number of les decoupled in the last layer were not large the modules in upper layers had few dependents.
in this case a system may not experience maintenance problems despite its low il.
the problem with pc is that it is sensitive to the size of the dsm the greater the number of les the smaller the pc.
for example from the open source projects with more than les of them have pcs lower than .
for the other projects with less than les however about of them have pcs lowers than .
more importantly as we will see later sometimes an architecture can change drastically without signi cantly changing its pc.
.
decoupling level in this section we introduce the rationale and formal definition of decoupling level dl using several examples.
baldwin and clark s theory suggests that a module with high option value should be small easy to change with high technical potential active and with few dependents.
the drh structure allows us to assess a software architecture in terms of its potential to generate option values because it a submission dl pc il b submission dl pc il c submission dl pc il figure design rule hierarchy samples.
t typed cl call ex extend ct cast u use x any dependency explicitly identi es modules their sizes and how decoupled are they from each other.
we thus propose decoupling level dl to measure how well an architecture is decoupled into modules.
concretely since drh separates modules into layers we calculate the dl of each layer.
because the modules in the last layer do not have any dependents we treat them di erently.
note that we cannot quantify technical potential as dl only measures source code.
next we introduce the formal de nitions of dl based on the above rationale.
.
formal definitions we de ne allfiles as the total number of les in the system and files mj as the number of les within a drh module mj.
given a drh with nlayers its dl is equal to the sum of the dls of all the layers dl nx li 1dlli since the last layer of a drh is special in that it contains truly independent modules that can be replaced or changed without in uencing any other parts of the system we calculate the dl for the last layer di erently.
for an upper layer li i n with kmodules we calculate its dl as follows dlli kx j files m j allfiles deps mj lowerlayerfiles where deps mj is the number of les within lower layer modules that directly or indirectly depend on mj.
if a module in uences all other les directly or indirectly in lower layers its dl is the more les it in uences in lower layers the lower its dl the larger a module the more likely it will in uence more les in the lower layer and hence the lower its dl.
based on the de nition of drh a module in upper layers must in uence some les in lower layers.
we calculate the dl of the last layer ln based on the following rationale the more modules in last layer and the smaller each module the better the system can support module wise evolution.
our earlier work of independence level il only considers the proportion of les within the last layer the better modularized a system is the larger the proportion of les in the last layer.
however we have seen some very large modules in the last layer of some projects which can skew the il metric.
ideally we want modules to be small.
but how small?
from an analysis of projects we calculated the averagenumber of les in last layer modules and in all drh modules.
the results are .
and .
les respectively meaning that the average drh module has just a few les.
prior work on cognitive complexity also shows that people can comfortably process approximately chunks of information at a time.
accordingly we consider a drh module with les or fewer to be a small module.
if the last layer ln has kmodules then its dl is dlli n kx j 1sizefactor mj if a module mj has les or fewer we calculate its sizefactor based on its relative size sizefactor mj files m j allfiles ifmjhas more than les we add a penalty to re ect the limits of human cognitive capabilities sizefactor mj files m j allfiles log5 files mj figure illustrates how the size of modules in uences dl.
suppose a system only has layer with les.
if they form modules each having les its dl is figure 2a meaning that each module can improve its value by being replaced with a better version and thus increase the value of the whole system easily.
as the size of each module grows it becomes harder for them to change.
if each module has les figure 2b then its dl decreases to and then to if each module has les figure 2c .
if the les only form one module then it only has dl figure 2d .
if the system only has layer containing module with multiple les then its dl decreases with the number of les.
if all the modules in the last layer have fewer than les then the dl of the last layer equals the proportion of last layer les to the total number of les which is equivalent to the independence level metric proposed in our prior work .
dl is however di erent from il rst because dl considers modules in allthe layers and second because it takes the size of a module into consideration.
.
an example now we use the example shown in figure to illustrate how dl can manifest design quality.
the three dsms were reverse engineered from student submissions for the same class project used in a software design class o ered at drexel 501university.
the students were given weeks to create a questionnaire management system so that a user can create a questionnaire that can be graded and a respondent can complete a given questionnaire.
the basic types of questions supported include multiple choice true false matching ranking short answer and essay.
the software was to be designed for easy extension such as adding new types of questions adding a gui in addition to a console ui and supporting di erent display and printing formats.
the students were supposed to achieve this objective by properly designing question and answer classes and applying appropriate design patterns.
the three dsms revealed drastically di erent designs for the same project.
all three students designed an abstract question class to abstract the commonality among question classes.
since a true false question can be seen as a special type of multiple choice question ranking is a type of matching and short answer is a type of essay both submission and submission have types of question and corresponding answer classes.
but submission has the highest dl and submission has the lowest dl .
there are several major di erences among these designs.
first both survey.java in submission and as well as form.java in submission aggregate a collection of questions.
in the rst two designs however both survey classes only interact with the question base class.
that is as a design rule question decouples survey from concrete question and answer classes and each type of question and its answer classes form a module that can be changed independently.
in submission however form depends on every question and answer class leaving only two classes that can change independently survey and test.
in addition submission applied a bridge pattern correctly so that the user can choose to use a textfileui or commandlineui at runtime creating more independent modules.
submission also attempted to apply a bridge pattern but didn t do it correctly.
in addition although submission has types of question and answer classes they are not independent of each other.
for example the ranking class extends the matching class.
consequently the dl of submission is lower than that of submission .
.
tool support we have created a program to calculate decoupling level which we are integrating into titan .
our dl algorithm takes the following inputs a dsm le that contains the dependency relations among project source les.
currently titan creates dsm les from xml les generated by a commercial reverse engineering tool called understandtm.
a clustering le that contains the drh clustering information for the source les.
we generate this using the drh clustering function of titan.
given these inputs our tool calculates thedecoupling level of a software system.
.
ev aluation now we evaluate dl based on how a metric should behave using the concept of a centimeter a commonly used metric as an analogy.
concretely we investigate the following questions rq1 if alice a young girl is measured two weeks in a row her height measures should be the same or very close to each other.
our analogous research question is if a project is being revised for a limited period of time say throughtwo releases but doesn t go through major refactoring for example reorganizing the code base by applying some new architecture patterns are the dl measures of these releases close to each other?
rq2 if alice measured cm.
and is subsequently measured one year later we expect her height to be signi cantly more than cm.
by analogy our research question is if a project is successfully refactored and its modularity has truly improved will its dl re ect the improvement?
or if the architecture of a project has degraded over time does its dl re ect this degradation?
positive answers to the above two questions imply that it is possible to quantitatively monitor architecture degradation or assess the quality of refactoring using the dl metric.
rq3 if alice measures cm.
tall and bella measures cm.
we can de nitively state that bella is taller than alice.
our analogous question is if project a has a higher dl than project b is project a more maintainable than project b?
a positive answer to this question means that it is possible to quantitatively compare the maintainability of di erent projects or di erent designs for the same project.
if cm.
is above the 50th percentile for a girl of alice s age her mom would know that her height is above average .
similarly if we measure the dl for a large number of projects perhaps considering project characteristics a manager could consult this dataset and determine the relative health of a speci c project from a maintainability perspective.
since both propagation cost pc and independence level il purport to measure software architecture s maintainability we would like to know whether dl is more reliable than pc and il.
therefore we will investigate the rst three questions using dl pc and il comparatively.
because we need to analyze multiple versions of the same project to answer rq1 and rq2 we call this vertical evaluation section .
.
for rq3 we need to compare di erent projects with various domains sizes and ages and so we call this horizontal evaluation section .
.
section .
presents the measures we extracted from projects and section .
summarizes the results.
.
subjects and their metrics to minimize the possible bias caused by speci c project characteristics such as domain or size we randomly chose open source projects from open hub1 and collected industrial projects from of our collaborators.
due to space limitations we placed all the data on our website2.
a brief inspection of this data shows that their domains sizes and implementation languages vary drastically.
to calculate the dl pc and il values of these projects we chose the latest version of each project downloaded its source code and then reverse engineered this code using understandtm which can output an xml le containing all the le level dependency information for a project.
given these xml les we used our tool titan to calculate the dsm les and drh clustering les.
finally for each project the dl pc and il values are calculated based on the project s dsm and drh clustering les.
table reports the statistics of these metric values obtained from these subjects.
this table shows that the average dl of all open source projects and industrial projects a les are decoupled into modules each having les dl b les are decoupled into modules each having les dl c les are decoupled into modules each having les dl d les are decoupled into module with les dl figure one layer drh with di erent modular structure are and respectively.
less than of open source projects have a dl lower than or higher than and these numbers are slightly lower for commercial projects.
it also shows that the project with the best dl is a commercial project even though commercial projects have lower dl values in general.
we will discuss the characteristics of pc and il later.
it is obvious that the data in table will vary if we examine a wide variety of subjects this is the expected behavior with all metrics.
for example child growth charts di er for girls and boys and vary with other factors such as diet and region.
we will continue collecting project data in the future and observe how these values vary and cluster.
table metric summary for projects pt percentile stats open source commercial all projects dl pc il dl pc il dl pc il avg median max min 20th pt 40th pt 60th pt 80th pt .
vertical evaluation we rst evaluate if these metrics are stable for multiple consecutive non refactoring releases.
we expect to observe little variation if a metric is reliable.
after that we apply these metrics to a commercial project that experienced serious architectural problems over a long period of evolution which resulted in a signi cant refactoring to restore modularity.
we are interested to know if the variation of these metrics can re ect the variations in the architecture.
.
.
the stability of dl to evaluate the stability of these metrics we selected out of the projects and a series of releases for each of them.
our selection was based on the following criteria a each project should have at least sprints meaning that it is revised during the chosen time period b these multiple snapshots are consecutive e.g.
multiple sprints from the same release and c a major refactoring among these snapshots was unlikely either based on our prior study or on our communication with the architects.
we chose the out of the commercial projects which we call comm comm and comm because we know that there was no refactoring during these selected snapshots based on our communication with their architects andthey all have su ciently long revision histories.
we chose out of open source projects because we had analyzed their structure before and have prior knowledge that these selected snapshots do not contain a major refactoring.
table reports the statistics of dl pc and il respectively.
take openjpa for example even though the number of les increased from to during the snapshots its dl increased from to .
the standard deviation and coe cient of variation cv of dl are only and respectively meaning that even though it went through signi cant changes its architecture as re ected by dl does not vary drastically.
by contrast the cvs of pc and il are and respectively meaning that these two metrics are more unstable.
over the projects we employ paired t test to test whether the cv of dl is signi cantly di erent with the cvs of pc and il.
the results are as follows dl pc p value .
dl il p value .
.
the results indicate that cv of dl is signi cantly di erent from the cvs of pc and il showing much higher stability.
table coe cient of variation cv of dl pc and il avg cv rls rls range fl range dl dl pc il avro .
.
.
.
camel .
.
.
cassandra .
.
.
.
cxf .
.
.
.
derby .
.
hadoop .
.
.
.
httpd .
.
.
.
mahout .
.
openjpa .
.
.
.
pdfbox .
.
.
.
pig .
.
.
.
tika .
.
wicket .
.
.
.
comm .
.
comm .
.
.
.
comm .
.
avg cv .
.
the variation of dl to evaluate if non trivial variation in a metric can faithfully indicate architecture variation we need to understand what happens to an architecture when the metric value increases or decreases considerably.
ideally we would want to collect the in ection points from multiple projects and talk to their architects to verify what happened between these snapshots where the metric value changes noticeably.
given time and resource constraints nding such candidates from open source projects was unrealistic.
for commercial projects out of the have snapshots or fewer.
for the remaining commercial projects our analysis for 503two of them were previously published .
one of them was just refactored and we don t have the latest data yet and the other was sold and we no longer have access to it.
comm and comm are the remaining commercial projects we can explore.
comm is one of the best modularized projects of all the projects we have analyzed and their architect con rmed that no major architectural degradation or refactoring had occurred.
project comm by contrast exhibits a more typical evolution path.
table displays the metric values for all snapshots collected since and figure depicts the trends of the three metrics over these snapshots.
the dl value indicates major in ection points where its value increased or decreased more than points from version .
to .
the dl increased from to from .
to .
dl decreased from to from .
to .
dl decreased from to and from .
to .
its dl increased from to .
by contrast although both pc and il re ect the rst in ection point the pc doesn t change in the other points.
that is if we only consider pc as an architecture metric it will indicate that the architecture didn t change at the later points.
il missed the 3rd and 4th points but indicates an architecture degradation from .
to .
where it dropped points while pc also increased .
we presented this data to the architects and asked them what happened during these transitions.
in other words did the architecture actually improve when the dl increased and did it degrade when the dl decreased?
did dl miss the 5th point where il indicates a degradation?
next we discuss these in ection points.
transition from .
to .
all three metrics indicate a signi cant improvement.
according to the architect when version .
was released it had been evolving for a year as a prototype.
from .
to .
released in april the product was refactored signi cantly and multiple design patterns were applied for the purpose of transforming it into a commercial product.
the architecture indeed improved signi cantly.
the transferring and refactoring process was accomplished by sept when the commercial project was released as version .
.
transition from .
to .
since the architecture was stable the management was eager to add new features which was the main objective in the next years.
we were told that during these years the developers were aware that to meet deadlines architecture debts were introduced and the project became harder and harder to maintain.
we can see that the dl decreased from to and remained around till .
when the dl dropped to .
the pc values by contrast decreased only slightly.
transition from .
to .
we observed a signi cant drop of dl from to .
referring to the dataset of all projects the maintainability of this product decreased from the 80th percentile to around the 20th percentile.
when we presented the data to the architect unlike the previous two points when the dl changes were expected this transition point was a small surprise indeed there was a signi cant refactoring in .
in which new interfaces were introduced to decouple several highly coupled parts.
this was the time when the developers were unable to tolerate the technical debt anymore and decided to clean up while continuing to add new features.
after adding the new interfaces the architect expecteda signi cant improvement in .
but the dl showed the opposite.
we then examined the drspaces led by these new design rules and tried to understand what happened.
it turns out that out of the new design rules had very minor impact only in uencing a few other les.
the other new design rule however was very in uential in uencing other les.
examining the drspace formed by the les we observed that these les were not as decoupled as the architect expected.
there existed several large dependency cycles that should have been decoupled.
the architect con rmed that since this was a signi cant refactoring combined with the addition of new features the refactoring wasn t completely nished by .
.
instead the cycles we observed in .
were removed gradually from .
to .
.
thus the surprising decrease of dl was caused by a significant but incomplete refactoring the new design rule introduced many more dependents but many modules were inadequately decoupled hence the signi cant decrease of dl.
transition from .
to .
we observed a signi cant increase in dl but still not as high as the dl for .
when the product was rst refactored.
the architect told us that the objective of .
was to conform to a new third party library and to improve unit testing.
to do so the major activities in .
were clean up that is reducing technical debt.
several big cycles we observed in .
were completely decoupled in this process which explains the increase of dl.
transition from .
to .
we observed that hundreds of les were added to the project and both pc and il indicate a degradation.
the pc increased because of the extra dependencies to these new les and il decreased because the new les are all at the higher levels of the drh.
we examined the drspace formed by the new les and found these les to be well decoupled.
we also examined drspace led by each of new les.
all these drspaces are small the mean and median of their sizes are .
and les respectively meaning that these newly added les are well decoupled and have little in uence on the other les.
we presented the results to the architect and asked if the architecture degraded due to these newly added les.
the architect explained that these new les were added because a new spreadsheet component was integrated to the system.
this component interacts with the rest of the system through apis but it was designed to be architecturally isolated and didn t change the structure of the rest of the system.
the system didn t experience any degradation due to the integration either.
therefore we believed that il and pc reported false positives.
the architect con rmed that since the project has been accumulating debt for years not all architecture issues were completely resolved.
and they are again facing the ubiquitous dilemma shall we refactor or keep adding new features?
.
the architect told us that the dsm analysis made it very clear where the debts are located and hence what should be done to remove them.
in summary we observed that in this project the variation in dl measures not only indicated successful refactoring and architecture degradation but also revealed an unsuccessful refactoring.
neither of the other metrics could provide such insights.
using drspace analysis we were able to identify why the modules were not decoupled as expected.
.
horizontal evaluation our horizontal evaluation aims to investigate if software 504table snapshots of comm index release dl pc il files .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
figure the dl variation for comm systems with higher dls are easier to maintain.
the root question is how to measure maintainability ?
in this section we rst propose a suite of maintainability measures that can be extracted from the revision history of a software system.
after that we introduce the subjects we used in this evaluation and the correlation between dl pc il and these maintainability measures.
.
.
maintainability measures ideally maintainability should be measured by the e ort in terms of hours spent on each task.
but such data is virtually impossible to obtain.
we thus propose a suite of maintainability measures that can be extracted from software revision history.
we illustrate the rationale using the cases depicted in figure .
suppose there are three committers for a project and each of them committed one revision each changing a set of les.
if each of the committers changed completely di erent sets of les figure 4a it means that these les can be changed independently and in parallel and it is unlikely that the committers need to expend e ort communicating with each other.
on the other extreme if all three committers changedexactly the same set of les figure 4d these les cannot be changed in parallel and it is highly likely that the committers have to communicate to resolve con icts.
if these are bug xing changes then the rst case implies that the bugs were localized and separated while the second case implies that all the bugs are in the same set of les.
it is obvious that in the rst case these les have the best maintainability and in the second case the worst maintainability.
we thus propose the following measures to quantify maintainability from the revision history of a project .
commit overlap ratio cor measures to what extent changes made to les are independent from each other.
that is the total number of les revised in all commits divided by the number of distinct les commitoverlapratio pm 1jfcij jfc1sfc2s sfcmj where mis the total number of commits jfcij i m is the number of les changed in a speci c commit i and jfc1sfc2s sfcmjis the total number of distinct les involved in all commits.
in figure the cor is in case a the best case and is in case d the worst case.
if these are bug xing commits a larger cor means that more bugs are xed by changing the same set of les indicating that these les are harder to maintain.
we further distinguish cor for bug xing only commits and all commits using bcor and ccor respectively.
.
commit fileset overlap ratio cfor measures to what extend the lesets managed by di erent committers overlap.
suppose that a committer ci makes multiple commits and fsiis the set of all the les cirevised then we calculate cfor for all the committers as follows cfor pm 1jfsij jfs1sfs2s sfsmj where mis the number of committers and the denominator is the total number of distinct les committed by all committers.
the larger the cfor the fewer les can be changed in parallel by di erent committers indicating lower maintainability.
we similarly distinguish cfor for bug xing and all commits using bcfor and ccfor respectively.
.
pairwise committer overlap pco measures the likelihood that two committers have to communicate with each other to resolve con icts.
suppose committer ca changed leset fsa and committer cbchanged leset fsb.
we measure their communication need as the number of les they both changed divided by the total number of les changed by either of them.
for each committer ci we thus use jaccard index to calculate her potential interaction with all other committers as committeroverlap i mx jjfsitfsjj jfsisfsjj where i6 jandmis the number of committers.
then we de ne pairwise committer overlap pco as the average ofcommitteroverlap among all committers.
the higher the number the more likely the committers have to communicate.
in figure the pco of case a is meaning that there is no need to communicate at all.
case b and case c have the same cor and cfor but in case c p3 may need to talk to both p1 and p2 hence case c has higher pco.
in case d each committer may have to talk a case cor cfor pco b case cor .
cfor .
pco .
c case cor .
cfor .
pco .
d case cor cfor pco figure layers with di erent modular structure to other committers pco .
we similarly distinguish pco into bpco for bug xing commits and cpco for all commits .
.
.
evaluation strategy ideally to fairly evaluate whether dl can be used to indicate maintainability di erences we should rst chose a set of projects of various sizes and domains each having multiple maintainers being well managed using proper version control and issue tracking systems where most commits are explicitly linked with issues.
more importantly none of these projects should go through signi cant architecture or design level refactoring.
given these projects we should measure dl values from their earlier versions that re ect a stable design then monitor and collect their maintenance measures for a long enough period of time until we can get a statistically valid sample set.
ideally these projects should have similar numbers of changes and bug xes of similar levels of complexity so that we can compare if higher dl values indeed correlate with lower maintenance e ort.
in reality nding such projects is not realistic.
even though there are large numbers of open source projects it is unrealistic for us to interview each open source team to determine at which release the architecture was stable for measurement purposes.
moreover given the frequent addition and removal of features even though there is no signi cant refactoring it is possible that the architecture may change during the project s evolution intentionally or unintentionally which will in turn in uence its subsequent maintainability.
maintainability will also be a ected by other factors such as the popularity of a project which will in uence the number of bugs that are found and xed.
consequently our idealized evaluation is not feasible.
as a result we decided to investigate as many projects as we could and include all the history of each to minimize the di erences caused by the number of revisions.
since we don t know from which release the dl pc and il should be measured to indicate a stable design we measure multiple snapshots of each project and calculate average values.
we are con dent that average dl should faithfully re ect the architecture of a project according to the stability analysis and from the fact that large scale refactorings in open source projects are extremely rare .
we are less con dent with using average pc since we have observed that pc varies drastically even for versions known to be stable making it a less quali ed architecture measure.
to make a fair com parison however we still calculate the average pc for each project and calculate the correlation between average dl pc and dl on the one hand and project maintainability measures on the other hand.
next we introduce the subjects we selected and the analysis results.
.
.
horizontal evaluation subjects we chose out of the open source projects and out of the commercial projects as the subjects for horizontal evaluation.
as shown in table these projects are implemented using di erent languages and have di erent sizes ages and domains.
we chose those projects rather than using all projects mainly because their revision histories are well managed using well known tools from which we can extract data.
more importantly in these projects we are able to extract the linkage between commits and issues reliably.
in other words most committers in these projects labeled their commits with the id of the issue that each commit addresses so that we can distinguish bug xing commits from other changes.
for each snapshot of each project we downloaded the source code reverse engineered the code transformed the le dependencies into a dsm le using titan and generated its drh clustering le.
using the dsm and drh clustering les we computed each dl pc and il and calculated their averages over multiple snapshots.
we used the revision history of each project to extract maintainability measures as introduced in section .
.
.
table horizontal evaluation subjects projects languages java c c c files cloc 10k .7m committers commits .
.
analysis given the following measures of maintainability change commit overlap ratio ccor bug commit overlap ratio bcor bug commit fileset overlap ratio bcfor change commit fileset overlap ratio ccfor pairwise committer overlap based on both all changes cpco and bug xing commits bpco we conducted a pearson correlation analysis between these measures and dl pc and il respectively.
we report the pearson values pv and pvalues in table .
all the p values are less than .
meaning these correlated relationships are statistically signi cant.
506this table shows that these measures have the highest negative correlation with dl meaning the higher the dl the better the maintainability.
il similarly showed negative correlation but the correlation was much weaker than with dl.
pc displays positive correlation with these maintenance measures meaning that the more tightly coupled a system is the harder it is to maintain.
although pc has relatively high correlations with ccor bcor ccfor and bcfor its correlations with cpco and bpco are much lower meaning that this coupling measure is less correlated with how well people can make changes independently from each other.
table pearson correlation analysis dl pc il pv p value pv p value pv p value ccor .
.7e .
.7e .
.
bcor .
.1e .
.1e .
.
ccfor .
.1e .
.6e .
.
bcfor .
.6e .
.2e .
.
cpco .
.6e .
.
.
.
bpco .
.2e .
.
.
.
.
evaluation summary so far we have answered all four questions positively.
compared with pc and il dl appears to be a more reliable metric in that it remains stable over subsequent releases reveals architecture degradation and major refactorings and has signi cant correlations with maintenance measures.
.
discussion in this section we discuss the threats to validity of the research the interpretation of dl and future work.
threats to validity.
although our evaluation showed that dl is a promising metric we understand that the evaluation su ers from several threats.
first even though we measured projects these projects use c c c and java only.
we thus cannot claim that dl can be applied to projects written in other languages such as perl or javascript.
second since all the dls are calculated based on the static information extracted by understandtm for projects in which major dependencies are not statically detectable such as service oriented architecture we may not be able to calculate their dls accurately.
third the maintenance measures we proposed in section may not re ect the true maintenance e ort.
we will keep looking for better approximations and for actual e ort data.
fourth as we explained at the beginning of section to evaluate how well dl can predict maintainability we should have used the dl collected at the beginning of a release and then measured the subsequent maintenance e ort until the dl changed signi cantly.
we tried this strategy for a few projects but the history data between releases is too small for us to form statistically meaningful results.
luckily we have several industrial collaborators who have refactored their code based on our previous analyses.
now that we have observed increased dls we will follow the newly refactored projects for at least a year and track how maintenance e ort changes.
the interpretation of dl.
it is possible that variance in dl may be due to reasons other than the inherent architectural complexity.
the domain and technology in used for example can both in uence dl values.
mahout3has the highest dl among all open source projects we have studied.
mahout is a machine learning library including techniques for classi cation recommendation and clustering.
the techniques and algorithms in mahout s library are inherently separate from each other.
so its domain leads naturally to a higher dl.
however the other two projects that have very high dls and are actually commercial projects with complex domains.
one of them has a size similar to mahout while the other is times larger meaning that even a system with a complex domain can have a well modularized design.
in addition we have shown in section that even in student projects di erent developers may create dramatically di erent designs.
a low dl may also have more than one explanation.
one possibility is that the system is stable and seldom needs to change.
or there could be very few developers maintaining it so that they can work closely without the need to program in parallel.
for example for the three open source projects with the lowest dls gnu screen4 dl rsync5 dl and imagemagick6 dl open hub7reported that in the past months they all have just developers and their activities were either described as has not seen any change in activity or has seen a substantial decrease in development activity .
the fact is that these projects currently do not support large numbers of developers or parallel development.
the reason could be either that their low dls make changes extremely di cult or the projects have been stabilized and there is no need to change.
our industrial experience shows that a low dl usually indicates signi cant maintenance di culty caused by decayed architecture.
in fact several industrial projects with low dls that we are engaged with are currently being refactored.
a high dl however doesn t always mean that the architecture is high quality.
of all the industrial projects we have analyzed two of them have dls higher than the 80th percentile.
but they still experienced considerable maintenance di culties.
our detailed analysis using titan revealed that the major issues in these projects are caused by unstable interfaces and modularity violations .
in other words implicit dependencies are the main causes of their maintenance problems .
as a result even though a project may have excellent scores as measured by static analysis tools it may still su er from maintenance di culties.
thus an architecture should be analyzed using both structure and history information.
also it should be noted that although we intended to create a metric and dl has demonstrated crucial properties of a metric it is certainly not as precise and sensitive as for example a centimeter.
our experience has shown that if the dl varies points or more it is almost certain that some architecturally signi cant changes have occurred.
a smaller variation may however just be noise.
as we have reported a software project that has been refactored does notnecessarily have a high dl.
a complex refactoring may take a while to complete and the dl may actually decrease 507while the refactoring is in progress.
future work.
one of our ongoing tasks is to keep collecting more data from more open source and industrial projects and form a broader architecture health chart .
we plan to make dl calculation a service so that anyone can measure their own project and add their data to our dataset.
we will also explore how to make the implicit run time dependencies among services explicit and explore the applicability of dl for these systems.
it would also be interesting to investigate the three research questions using traditional code based metrics.
also thus far we have only investigated the correlation between dl and software maintainability.
one of our future tasks therefore is to explore their causal relationships.
finally a sensitivity analysis of the value at which modules are considered small is also part of our future work.
.
related work in the past decades researchers have proposed various metrics to measure software quality.
we now discuss three categories of work on software metrics.
code quality measures.
numerous metrics are well known measures to software quality such as mccabe cyclomatic complexity halstead metrics lines of code.
various metrics were proposed to measure oo programs such as ck metrics lk metrics and mood metrics .
these metrics have been used to predict quality issues or to locate error prone les .
combining code quality metrics to measure software maintainability has also been widely studied .
oman and hagemeister proposed maintainability index mi a composite number based on multiple metrics to determine software maintainability.
bijlsma et al.
and heitlager et al.
also used combined metrics to rate software maintainability.
similarly commercial tools such as sonarqube8and infusion9 also provide a single composite number sonarqube s technical debt and infusion s quality de cit index qdi for management to monitor software quality.
the problem is as nagappan et al.
reported even though these complexity metrics have demonstrated to be useful for defect prediction the most predictive metrics are di erent in di erent projects.
menzies also pointed out the di culty of using the measures collected from one set of projects to predict issues for another set of projects.
di erent from these existing metrics our dl is the only metric that measures how a software system is decoupled quantifying canonical single responsibility and separation of concern principles.
our objective is not defect prediction but architecture comparison and degradation monitoring.
to the best of our knowledge we are not aware of an existing metric or a metric suite that has been successfully used to compare software architecture among large number of di erent projects or demonstrates similar behavior as a real metric such as a centimeter.
history measures and quality.
similar with structural metrics above most measures extracted from history were proposed for bug location and error prediction.
there have been numerous studies of the relationship between evolutionary coupling and error proneness .
for example cataldo et al.
s work reported a strong correlation density of change coupling and failure proneness.
ostrand et al.
s study demonstrated le size and le change information were very useful to predict defects.
again we are not aware of a history measure that has been used to compare software architecture like a real metric.
even though history can most faithfully re ect architecture quality we prefer to access architecture design quality before history and penalty accumulate.
using dl the designer can judge if the maintainability is below average at early stages of software development.
architecture metrics.
although by far less well known than the metrics mentioned above maccormack s propagation cost pc has been used by most of our industrial collaborators.
independence level il as a simpli ed option based metric also has been accepted by one of our industrial collaborators to compare hundreds of projects using il values of open source projects as benchmarks.
the most comprehensive formula is the option valuation proposed by baldwin and clark which we rst used to quantitatively compare the two variations of kwic .
as we explained earlier in the paper option valuation requires the estimation of the technical potential of each module and the problems with pc and il have been discussed.
bouwers et al.
reported that three of twelve systemlevel architecture metrics for encapsulation are correlated with the ratio of local change sets higher ratio of local change sets indicates better encapsulation.
by contrast our dl measures how well a software architecture can support parallelized distributed development and localized changes.
in summary to the best of our knowledge decoupling level is the only software maintainability metric that bears similarity with other metrics we use in everyday life such as the centimeter in that it allows managers to monitor evaluate and compare software projects and their evolution.
.
conclusion in this paper we proposed a new metric decoupling level to measure software architecture maintainability.
based on baldwin and clark s design rule theory this metric aims to measure to what extent an architecture is decoupled into small independent modules that can be changed separately and in parallel by multiple developers.
our evaluation has revealed that dl values remain stable through multiple non refactoring releases of projects and its non trivial variation indicates severe architecture decay or the occurrence of signi cant refactoring activities.
we also extracted a suite of measures from a project s revision history to indicate maintainability and analyzed how the dl pc and il correlated with these measures using data from projects.
the results showed that dl has a much stronger negative correlation with project maintainability than the other architecture metrics.
our investigation suggests that dl has the potential to be a valuable metric for measuring comparing and monitoring software maintainability.