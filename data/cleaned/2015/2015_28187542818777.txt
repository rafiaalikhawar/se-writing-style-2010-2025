gray computing an analysis of computing with background javascript tasks yao pan jules white v anderbilt university usa yao.pan jules.white vanderbilt.eduy u sun california state polytechnic university pomona usa yusun cpp.edujeff gray university of alabama usa gray cs.ua.edu abstract websites routinely distribute small amounts of work to visitors browsers in order to validate forms render animations and perform other computations.
this paper examines thefeasibility cost effectiveness and approaches for increasing theworkloads offloaded to web visitors browsers in order to turnthem into a large scale distributed data processing engine whichwe term gray computing.
past research has looked primarilyat either non browser based volunteer computing or browser based volunteer computing where the visitors keep their browsersopen to a single web page for a long period of time.
this paperprovides a deep analysis of the architectural cost effectiveness user experience performance security and other issues ofgray computing distributed data processing engines with highheterogeneity non uniform page view times and high computingpool volatility.
i. i ntroduction every website visitor does some computational work on the website s behalf such as validating input values of a web form before submission to the server harvesting information relatedto the client s browser animating page elements or displayingcomplex data analytics.
website visitors implicitly opt intoperforming computational work for the website owner withoutknowing exactly what work their browsers will perform.
theline between what computational tasks should or should not beoffloaded to the visitor s browser is not clear cut and createsa blurred boundary which we term gray computing.
the only hard line defining acceptable and unacceptable computingtasks is drawn by the security controls in the browser thatprotect the user from malicious logic.
the successful offloading of smaller tasks such as form validation motivates the question of whether or not there is po tential to perform more computational work in this gray com puting area of website visitors browsers.
previously browser based javascript applications were single threaded preventingcomplex calculations from being performed without the user sknowledge since they would directly impact user interactionwith the web page.
the emergence of new standards such as web workers which allow background javascript threadson web pages offer the potential for much more significantbackground usage of visitor s computing resources withouttheir knowledge which motivates the study of this new graycomputing power.
for example big data processing where organizations process website visitor logs user photos social networkconnections and other large datasets for meaningful infor mation has become commonplace for large websites.
couldorganizations offload these big data processing tasks to clientweb browsers in background javascript threads?
how muchcomputational power could be harnessed in this type of model both for computations directly beneficial to the web visitor such as product recommendations as well as for attackers thatcompromise a website?
there are clearly significant privacy and ethical questions around this concept of gray computing but before deeplyexploring them it is important to ensure that there is actuallysignificant potential computational value in gray computing.for example if the costs such as added load to the webserveror reduced website responsiveness reduce web traffic thenclearly gray computing will not be exploited.
further if thecomputational power of browsers is insignificant and can notperform sufficient work to outweigh the outgoing bandwidthcosts of the data transferred to clients for processing then nouser incentives or other models to entice users into opting intocomputationally demanding tasks will be feasible.
scientists have effectively applied volunteer computing which is a distributed computing paradigm in which computerowners donate their computing resources to research projects to reduce costs and increase their computing power.
v olunteercomputing differs from gray computing in that volunteerstypically directly opt in by installing a client and have ful l knowledge of the computational tasks that they will beperforming.
code obfuscation techniques such as javascriptminimization typically make web visitors unaware of thecomputational tasks that they are performing on behalf of awebsite.
moreover few website visitors look at the sourcecode of webpages that they are visiting in order to determinethe computational work their browsers are performing.
how much gray computing power is currently untapped?
as we will show from empirical results presented in thispaper there is significant untapped gray computing power.for example over billion hours of video are watched eachmonth on y outube .
assume that each client computerhas an average processing ability of gflops whichis equivalent to an intel i3 .
further assume thateach client computer is only utilized by gray big datacomputing tasks offloaded by y outube such as the client side face detection task we present in section iv b. the ieee acm 37th ieee international conference on software engineering .
ieee ieee acm 37th ieee international conference on software engineering .
ieee ieee acm 37th ieee international conference on software engineering .
ieee ieee acm 37th ieee international conference on software engineering .
ieee icse florence italy combined processing power of the client computers would be pflops.
tianhe the fastest super computer on recordtill june has a computing speed of .
pflops .
inthis example the combined processing power of the monthlyweb visitors to y outube is 3x the largest supercomputer in the world.
even if the true gray computing power is only1 of this estimate the processing power would still makegray computing among the top50 fastest super computers inthe world .
open question is it feasible and cost effective to build a gray computing data processing infrastructureusing website visitors browsers?
although prior research on volunteer computing has investigated browser based volunteercomputing engines past research has focused on scenarioswhere website visitors keep specialized web pages open forlong periods of time.
the prior research has not considered thecost effectiveness of building browser based distributed dataprocessing engines with gray computing or gray computing simpact on the user experience of a normal website.
if graycomputing can be done in a manner that is both cost effectiveand does not impact user browsing experience it warrantsboth significant concern and study.
a number of key researchchallenges exist toward adopting gray computing that stemfrom the volatility security user experience considerations and cost of performing distributed data processing with graycomputing.
this paper does not investigate the ethical andprivacy concerns of gray computing in depth but instead aimsto determine if gray computing is an appealing enough targetfor both legitimate website operators and attackers to warrantfurther research into these issues.
this paper presents an architectural solution which addresses these research challenges and proves the feasibilityof performing distributed data processing tasks with graycomputing.
there is a need for further study on the importanceof the security ethical and other considerations of this type ofcomputing resource utilization.
to prove the feasibility of thegray computing concept we built empirically benchmarked and analyzed a variety of browser based distributed dataprocessing engines for different types of gray computing tasks ranging from facial recognition to rainbow table generation.we analyzed the computational power that could be harnessedboth for valid user oriented concerns as well as by cyber attackers.
the same experiments were performed using ama zon s cloud services for distributed data processing in order tocompare the performance and cost of gray computing to cloudcomputing.
contributions although there are several past implementations of browser based distributed data processing engines past work has not investigated the potential for more signifi cant usage of website visitor computing resources with back ground javascript threads.
this paper s main contributions which differentiate it from previous work are as follows a complete architecture for gray computing is providedthat improves cost effectiveness by exploiting asymme tries in cloud pricing models.
experiments are presented that investigate the impact onuser experience of popular websites such as facebook.
past research treated the computing resources of browserclients as free.
the additional cost load on the networkand server when distributing data processing tasks tobrowser clients is considered in this work and shown tobe a critical component of determining which tasks arecost effective to use for gray computing.
this paper considers more realistic browsing behaviorversus past approaches that assumed the browser wasopen for long periods of time and did not considersituations where the user was only computationwhile a visitor was reading web pages on a site.
a variety of real world distributed data processing appli cations ranging from computer vision to machine learn ing are benchmarked and assessed for cost effectivenesswith gray computing.
the remainder of this paper is organized as follows section ii presents the key research challenges investigated in thispaper section iii presents our proposed solutions to addressthese challenges and the corresponding empirical verifications section iv examines some practical applications of distributeddata processing with gray computing and analyzes their suit ability for this computing model section v describes relatedwork section vi presents concluding remarks and futurework.
ii.
r esearch questions in order to determine if it is feasible to tap into gray computing a number of key research questions need to beaddressed question does javascript provide sufficient performance for computationally intensive tasks?
question does gray computing impact website performance in a user perceptible manner?
question what are the mechanisms for handling malicious clients and how will they impact gray computingperformance?
question how cost effective is gray computing versus commodity public clouds?
question how do you effectively allocate tasks to clients with unknown page view times?
the remainder of this paper presents the results of experiments and analyses that we performed to answer each of thesekey questions.
as will be shown there are practical solutionsto the challenges of gray computing and it can be a cost effective approach to perform a number of complex tasks suchas image processing.
iii.
a nswering the research questions to answer the key research questions we built an implementation of a distributed data processing engine that cantap gray computing power benchmarked it and performedcost performance analysis on a variety of different data pro cessing tasks.
we set out to build the most optimized graycomputing engine that we could design to ensure that our icse florence italycost benefit analyses were realistic.
initially we used past research on volunteer computing to guide our gray computingarchitecture but found a number of architecturalassumptions in volunteer computing that did not hold in graycomputing hosted in cloud based websites.
in particular wefound architectural issues discussed in prior work such asmismatches between past architectures and cloud computingpricing that we sought to address.
our design focused on optimizing the gray computing distributed processing engine architecture for websites servedout of cloud computing environments such as amazon ec2.based on our analysis of the architectures used in past re search the key architectural limitations of pastresearch when applied to a cloud computing environment areas follows prior approaches assume a fixed sunk cost for thecomputing time of the task distribution server.
in a cloud computing environment the cost of the computingtime for the task distribution server is not fixed andscales with load.
therefore past architectures need to bereassessed to minimize the distribution server load anddecrease cost.
otherwise gray computing is less cost effective.
pricing asymmetry of cloud computing resources canbe exploited to reduce gray computing costs.
for example amazon s3 and microsoft azure only chargefor data transfer out of their storage services and notdata transfers into their storage services.
prior work didnot optimize the data distribution and results reportingarchitectures to take advantage of pricing asymmetry.
task distribution servers were reported as the bot tleneck in some prior work .
the load on task distribution servers can be substantially reduced by of floading data distribution to content delivery networks which provide better cost performance ratios than servingthe same data out of a cloud based server.
all results were reported directly back to the taskdistribution server in prior work.
server load can be reduced by allowing browser based clients to bypassthe task distribution server and directly write results tocloud based storage using a temporary url authorizationmodel such as the one provided by amazon s3.
to address the architectural issues described above we developed a novel architecture for browser based data processingengines hosted out of cloud computing environments suchas amazon ec2.
our architecture was focused on providingwebsites with a mapreduce interface to gray computing which is commonly used for data processing tasks .
to reduce the workload of the task distribution server we utilize the cloud provider s storage service to serve datafor computing directly out of the storage service.
the taskdistribution server is only responsible for determining whattasks should be given to a client.
the task distribution serverhandles http get requests and responds with a json basedstring containing the task id and data location url for eachtask.
each client calls the api once before working on anindividual task.
our empirical analysis shows a relatively smallworkload is added to the task distribution server and the costis negligible.
besides reducing the workload of the task distribution server serving data directly out of the cloud based storageserver also exploits the pricing asymmetry in cloud storageservices.
in amazon s3 and microsoft azure only outbounddata transfer is charged.
data transferred into the storageservice is free.
this means clients can report the results ofcomputations directly to the storage service for free.
this setupallows the data processing engine to reduce bandwidth costs.
since the clients are highly volatile and typically only have short page view times improving data transfer speedsis critical to the overall performance.
it is essential to max imize the time that clients spend executing computationaltasks and minimize the time spent waiting for input data.one optimization that has been applied in our architectureis the use of a content delivery network cdn .
insteadof serving the input data through a single storage server acdn works by serving the content with a large distributedsystem of servers deployed in multiple data centers in multiplegeographic locations.
requests for content are directed to thenodes that are closest and have the lowest latency connectionto the client.
take amazon s cdn service cloudfront as anexample.
the original version of the data is stored in an originserver such as s3.
amazon copies the data and producesa cdn domain name for the data.
the clients request thedata using the cloudfront domain name and cloudfront willdetermine the best edge location to serve the contents.
anoverview of this proposed architecture is shown in figure .
in order to manage and coordinate the computational tasks we built a cloud based task distribution server using node.jsand the express framework.
the task distribution server par titions the data into small chunks and assigns them to clientsfor processing.
we use amazon s3 as our storage server tostore input data for clients and receive results from clients.an ec2 server is used to subdivide the tasks and maintaina task queue to distribute tasks to clients.
in our proposedarchitecture tasks are distributed to clients as follows the client requests an html webpage from the server.
the server injects an additional javascript file into the webpage that includes the data processing task.
a javascript web worker which executes in a background processing thread after the page is fully loaded isused to perform the heavy data processing computationwithout impacting the foreground javascript rendering.
the client sends ajax http requests to retrieve the input data from the cdn.
once it receives the inputdata it runs a map or and reduce function on the data.
the client issues an http put or post of the results directly back to the cloud storage server.
aftersubmitting the results the web worker messages themain thread to indicate completion.
upon receipt of thismessage the main thread sends a new http request tofetch another data processing task from the server.
icse florence italy g94 g410 g381 g396 g258 g336 g286 g3 g94 g286 g396 g448 g286 g396 g3 g894 g4 g373 g258 g460 g381 g374 g3 g94 g1007 g895 g3 g18 g381 g381 g396 g282 g349 g374 g258 g410 g349 g374 g336 g3 g94 g286 g396 g448 g286 g396 g3 g894 g4 g373 g258 g460 g381 g374 g3 g28 g18 g1006 g895 g3 g18 g367 g349 g286 g374 g410 g855 g3 g116 g286 g271 g3 g271 g396 g381 g449 g400 g286 g396 g3 g116 g286 g271 g3 g449 g381 g396 g364 g286 g396 g3 g24 g349 g400 g393 g367 g258 g455 g3 g272 g381 g374 g410 g286 g374 g410 g3 g258 g374 g282 g3 g346 g258 g374 g282 g367 g286 g3 g437 g400 g286 g396 g3 g349 g374 g410 g286 g396 g258 g272 g410 g349 g381 g374 g3 g100 g258 g400 g364 g3 g87 g396 g381 g272 g286 g400 g400 g349 g374 g336 g3 g90 g286 g410 g437 g396 g374 g3 g90 g286 g400 g437 g367 g410 g400 g3 g271 g455 g3 g44 g100 g100 g87 g3 g87 g104 g100 g3 g100 g258 g400 g364 g3 g38 g349 g374 g349 g400 g346 g286 g282 g3 g39 g286 g374 g286 g396 g258 g410 g286 g3 g410 g286 g373 g393 g381 g396 g258 g396 g455 g3 g104 g90 g62 g3 g296 g381 g396 g3 g349 g374 g393 g437 g410 g3 g282 g258 g410 g258 g856 g3 g116 g286 g271 g3 g100 g258 g400 g364 g367 g258 g455 g272 g100 g258 g400 g364 g346 g410 g373 g367 g3 g58 g381 g271 g3 g89 g437 g286 g437 g286 g3 g361 g381 g271 g1005 g3 g361 g381 g271 g1005 g3 g361 g361 g381 g271 g1006 g3 g361 g381 g271 g1006 g3 g361 g361 g381 g271 g1007 g3 g361 g381 g271 g1007 g3 g361 g361 g381 g271 g1008 g3 g361 g381 g271 g1008 g3 g361 g857 g3 g18 g24 g69 g3 g94 g286 g396 g448 g286 g396 g3 g3 g894 g4 g373 g258 g460 g381 g374 g3 g18 g367 g381 g437 g282 g38 g396 g381 g374 g410 g895 g3 g90 g286 g395 g437 g286 g400 g410 g3 g410 g258 g400 g364 g3 g24 g349 g400 g410 g396 g349 g271 g437 g410 g286 g3 g410 g258 g400 g364 g3 g258 g374 g282 g3 g349 g410 g400 g3 g349 g374 g393 g437 g410 g3 g282 g258 g410 g258 g3 g104 g90 g62 g3 g87 g90 g286 g410 g437 g396 g374 g90 g286 g400 g437 g367 g410 g400 fig.
architecture of the proposed distributed data processing engine for gray computing.
a. answering question benchmarking javascript performance on computationally intensive tasks current javascript performance popular browsers such as chrome firefox and safari have made great efforts tooptimize their javascript computing engines and have pro duced significant increases in javascript computational speed.further a variety of highly optimized tools have been devel oped to automatically port c c programs to javascript.
em scripten is an llvm low level virtual machine basedproject that compiles c c code into highly optimizablejavascript in the asm.js format.
entire c c code bases suchas the unity 3d gaming engine and the ffmpeg video process ing library have been ported to javascript with emscripten.emscripten works by first compiling c c code throughclang into an intermediary representation in llvm.
it isclaimed the optimized javascript code can achieve near nativeperformance to c code.
we conducted some benchmarkexperiments to analyze the performance of the javascriptproduced by emscripten.
the results are quite startling theported javascript codes significantly outperformed the hand written javascript code and were within 2x performance ofthe original native c code.
table i e xecution time comparison of na tive c na tive js and ported js from c .
j ava script is running with v8 java script engine .
.
benchmark c native js ported js nbody .6s .9s .2s fasta .3s .4s .1s table i presents the results from the comparison between native c code hand written javascript code andported javascript code using emscripten.
the hand writtenjavascript code is written with the same algorithm as thec version.
the ported javascript code is produced usingemscripten .
.
with the o3 optimization flag.
the c code was compiled using llvm gcc .
with the o3 optimization parameter.
the reason for the performance boostis that emscripten uses asm.js which is a highly optimizedlow level subset of javascript.
the types in javascript areimplicitly declared and thus take time for large applicationsto decide at runtime.
asm.js utilizes typed arrays for speedimprovements.
it also eliminates javascript s object orientedconstructs and thus eliminates many hard to optimize codeconstructs.
b. answering question benchmarking background web worker computational task impact on user experience one concern with deploying gray computing to websites is whether the background computing tasks will affect the website s foreground user interaction tasks.
gray computingwill be of little interest to website owners and visitors if thecomputing tasks affect the user experience heavily.
web workers is a new feature introduced in html5 which can create a separate javascript thread in the background.using web workers background computing tasks and fore ground rendering user interaction tasks can run in separatethreads allowing the main javascript needed to render thepage and validate forms to run unimpeded without affectinguser experience.
to test whether computationally intensive tasks in the background will affect the user experience we conductedexperiments with greasemonkey to inject data processingtasks into popular websites.
greasemonkey is a browser exten sion which allows users to install scripts that make on the flychanges to web page content before or after the web page isloaded in the browser.
the first concern regarding user experience is page load time.
however web workers spawn threads in the onload function of a web page which is executed only after thehtml page finishes loading.
background web worker threadscan be run without affecting page load time.
the second concern is the page responsiveness.
many metrics exist for page responsiveness.
in this project we choosethe search box item suggestion or auto completion whichis a pervasive and time critical feature of many websites forthe user experience evaluation.
search box auto completionis a time critical task since auto completion results must becomputed faster than the user types in order to be helpful.
ouraim was to test if the generation of search suggestions wouldbe slowed down by a computationally intensive backgroundtask.
two scripts were written in greasemonkey.
script wassetup to inject a search term into the search bar and record howlong it took for the web page s foreground client javascriptto generate the search result html elements and producea baseline time for comparison.
script spawned a webworker to perform a computationally intensive task requiredby the applications we proposed in section iv such as facerecognition and image scaling and then injected the searchterms.
the time to generate auto completion suggestions whilerunning the computationally intensive background tasks wasmeasured.
we compared the time to generate the searchsuggestions with and without the web worker task to see ifthe background computation would impact the user experienceon a number of popular websites.
we used different keywords as search inputs and for each keyword we ran trials and averaged the load times.as the results shown in table ii illustrate there was nodiscernible difference in the search suggestion load time with icse florence italyand without a web worker computational task running.
the results show that background gray computing tasks would notbe easily discernible by the user causing both alarm dueto the potential for misuse and potential for distributed dataprocessing.
table ii a verage search box hint results genera tion time t and cpu utiliza tion .
website with web worker without web worker t avg.
cpu util t avg.
cpu util twitter .58s .59s wikipedia .47s .46s gmail .75s .73s c. answering question mechanisms for handling malicious clients and their associated overhead since clients in a gray computing distributed data processing engine are not guaranteed to be trustworthy it is possible thatthe results returned from them can be fraudulent.
a simplebut effective approach to handle malicious clients it to use atask duplication and majority voting scheme.
more complexstrategies such as credibility based approaches exist but arenot necessarily a good fit for the highly volatile browser based data processing environment.
many visitors may accessa website only once and the website may have no historystatistics to derive their visitor reputation scores.
suppose theoverall application consists of ndivisible task units.
there arecconcurrent clients and among them mare malicious clients which will send falsified results.
f c n is the fraction of malicious clients in total clients.
our duplicationapproach works by assigning identical task units to kdifferent clients and verifying the results returned from different clients.if the computation results from different clients are the same then the result will be accepted.
if not a majority votingstrategy is applied and the result from the majority of clientsis accepted.
notice that the server randomly distributes tasksto clients while ensuring no client receives both a task andits duplicate and that clients have no control over which taskunits they will get.
the duplication approach fails only whena majority of a task s duplications are sent to a collaborativegroup of malicious clients.
error rate the error rate epsilon1is defined as the fraction of the final accepted results that are malicious.
as described bysarmenta et al.
the error rate can be given by epsilon1 2d summationdisplay j d parenleftbigg2d j parenrightbigg fj f 2d j wheredis the minimum number of matching results needed for the majority voting scheme.
for example for d the server accepts a result when it gets two identical results.
whenthe server gets two different results it keeps reassigning thetask until identical results are received.
redundancy r i s defined as the ratio of the number of assigned task units to thenumber of total task units.
it can be proved that r d f .
although simple the duplication approach is robust even with small d. for large websites with millions of visitors it is difficult for attackers to gain control over a large portion of theclients.
suppose a hacker has a botnet of a thousand machines f .
for a website with a million total clients.
as shown in figure even with d we can ensure .
of all the accepted results are correct.
the choice of dis also dependent on the type of application.
some applications aregenerally less sensitive and even no duplication is acceptablesince the falsified results do little harm.
g1005 g856 g1004 g28 g882 g1004 g1012 g1005 g856 g1004 g28 g882 g1004 g1011 g1005 g856 g1004 g28 g882 g1004 g1010 g1005 g856 g1004 g28 g882 g1004 g1009 g1005 g856 g1004 g28 g882 g1004 g1008 g1005 g856 g1004 g28 g882 g1004 g1007 g1005 g856 g1004 g28 g882 g1004 g1006 g1005 g856 g1004 g28 g882 g1004 g1005 g1005 g856 g1004 g28 g1085 g1004 g1004 g1005 g1006 g1007 g28 g396 g396 g381 g396 g3 g396 g258 g410 g286 g3 g282 g3 g296 g1089 g1004 g856 g1004 g1005 g296 g1089 g1004 g856 g1004 g1004 g1005 fig.
error rates for different parameter values of d and f. d. answering question a cost model for gray computing another challenge that must be dealt with is the costeffectiveness of gray computing.
to assign tasks to a large number of browser clients a coordinating server is neededto send the input data to clients and collect the computingresults from them.
the extra data transferred in addition tothe original web page will consume additional bandwidth andprocessing power of the server.
it is possible that this extracost can be more than the value of the results computed bythe clients for some types of applications.
therefore we needa cost model to help developers decide whether it is cost effective to deploy an application through gray computingand estimate the cost savings.
in our cost model we choosecloud computing services as our benchmark.
we chose tocompare against cloud computing because cloud computinghas a quantifiable computing cost that we can compare against.
for our gray computing distributed data processing engine the cost can be broken down as c browser ctransfer crequest cdistribute ctoclient ctoserver crequest cdistribute ctoclient ctoserver crequest ctransfer is the data transfer cost calculated on volume of data crequest is the http request cost incurred based on the number of get or post requests cdistribute is the virtual compute units consumed by task distribution.
we have shownin our empirical analysis that c distribute is nearly zero for our architecture because the data is served directly out of the cloudstorage service and the task distribution server only needs todo a small amount of work.
c transfer can be further broken down into ctoclient and ctoserver .
icse florence italyctoclient k i ptransferout k i porigint ocdn pcdnout ctoserver o ptransferin o ptocdn ptoorigin ctoclient is the cost to fetch input data for computing from an s3 storage server iis the original size of input data to be transferred to clients for processing porigint ocdn is the unit price to transfer data from an origin server s3 in our case to cdn servers cloudfront in our case pcdnout is the unit price to transfer data from cdn servers to the clients.c toserver is the cost to return computation results from clients to the storage server ois the size of data to be returned ptocdn is the unit price to transfer data from clients to cdn servers and ptoorigin is the unit price to transfer data from cdn servers to the origin server.
the actual data transferred from server to client will be more than the original size of input data i. one reason is as described in section iii c the same task needs to be sent toddifferent clients to be robust to malicious clients.
another reason is the clients may leave before the computation finishes.the data transferred to these clients is wasted.
the actual vol ume of data transferred out will be ktimes the data needed for one copy of the task where k d summationtext n 1d n d .
dis the duplication factor.
the variable is the successful task completion ratio of browser clients i.e.
the percentage of distributed tasks that are successfully processed before abrowser client leaves the site .
in section iii e we discuss the estimation of value and its relationship with average page view duration task granularity and task distribution algorithms.
c request k d n prequest nis the number of tasks that need to be processed.
prequest is the unit price of http requests to the cdn server.
for eachtask distribution session the client needs one http requestto fetch the data and one http request to return the results.since each task needs to be duplicated dtimes and not all the tasks are completed successfully more fetch requests areneeded than return requests.
that is knrequests to fetch input data and dnrequests to return the results.
the cost to run the tasks in the cloud is given by c cloud ccloud tcloud i punit wherepunit is the cost per hour of a virtual compute unit.
for example amazon provides an ecu virtual computingunit measure that is used to rate the processing power of eachvirtual machine type in amazon ec2.
virtual compute unitsare an abstraction of computing ability.
t cloud is the computing time to process unit of data with one virtual compute unitin the cloud.
the proposed distributed data processing engine built on gray computing is only cost effective when c cloud c browser that is the cost to distribute and process the data in thebrowsers must be cheaper than the cost to process the datain the cloud.
t cloud i punit k i porigint ocdn pcdnout o ptocdn ptoorigin k d n prequest since prices are all constant for a given cloud service provider the key parameters here are t cloud which can be computed by a benchmarking process and k which can be computed based on the voting scheme for accepting results and averagepage view time of clients.
the cost saving uis defined as u c cloud cbrowser a positive uindicate an application is suitable for gray computing.
e. answering question an adaptive scheduling algorithm for gray computing the clients in gray computing are website visitors browsers which are not static and unreliable.
the clients join and leave the available computational resource pool frequently.the result is that a client may leave before its assignedcomputational task is finished adding extra data transferand producing no computational value.
we define as the successful task completion ratio.
the value of is important and directly influences the cost of gray computing.
the higher is the less data transferred is wasted and thus more costeffective gray computing will be.
there are two factors affecting the successful task completion ratio the page view duration of the client and the computing time of the assigned task.
the relationship between average page view duration and task sizes is depicted in figure .
assume assigned task size is proportional to thecomputing time needed.
for a fixed task chunk size the longerthe page view duration the fewer chances the client will leavebefore the computation completed.
the distribution of the pageview durations of a website s clients is determined by thewebsite s own characteristic.
but the computing time of theassigned tasks is what we can change.
assume the wholeapplication can be divided into smaller chunks of arbitrarysize.
reducing the single task size assigned to the clients willincrease the task completion ratio but result in more task units.more task units means more cost on the requests to fetch dataand return results.
therefore there is a tradeoff between usingsmaller tasks and larger tasks.
instead of treating all the clients as the same and assigning them tasks of the same size we developed an adaptivescheduling algorithm.
we utilize the fact that website visitors dwell time follows a weibull distribution the prob ability a visitor leaves a web page decreases as time passes.this implies most visits are short.
our adaptive schedulingalgorithm works by assigning tasks of smaller size to clientsfirst and increasing the subsequent task sizes until a threshold icse florence italy40 .
.
.
.
.
.
.951successful task completion ratio average dwell time in second task size task size task size fig.
the relationship between average dwell time and successful task completion ratio for different task sizes.
is reached.
in this way we achieve higher successful timecompletion ratio than fixed task size while also reducing thenumber of task requests.
for comparison we also implemented a hadoop scheduler.
the hadoop scheduler works as follows the entire jobis partitioned into task units of the same size.
the servermaintains a queue of tasks to assign.
when a client connectsto the server the server randomly selects a task from the queueand assigns it to the client.
if a task fails the failed task will bepushed onto the back of the queue and wait to be reassignedto other clients.
to derive values for the schedulers we ran simulations in matlab with varying average page view times and taskdurations.
we assumed the page view duration follows aweibull distribution with average page view duration equaltot apv.
we used a duplication factor of d for a voting scheme that required matching results.
the results are shownin table iii.
we can see that increases as the average page view time increases i.e.
the successful task completionrate goes up and the adaptive scheduler achieves higher and faster task completion times compared to the hadoop scheduler.
table iii c omparison of different schedulers in terms of task completion success ra tio .
average page view duration scheduler 30s 1min 2min 3min hadoop .
.
.
.
adaptive .
.
.
.
iv .
e xamples of gra y computing applica tions in this evaluation section we examine some practical applications of distributed data processing with gray computing.we compute the cost saving ufor each application to find out which applications are cost effective in a gray computingmodel.
a. experiment environment browser client we used a desktop computer for our experiments with an intel core i5 cpu clocked at .5ghz and 8gb of memory.
the computational tasks were implementedin javascript and run on chrome .
.cloud instance we use an amazon ec2 m3.medium instance ecu and ubuntu .
64bit operating systemfor benchmarking cloud data processing performance.
price for cloud computing see table iv and v. p unit .
hour .
we choose a duplication factor d for most use cases because it guarantees a relatively high accuracy aswe have shown in section iii c. we also choose a conservativesuccessful task completion ratio of .
which is representative of the hadoop scheduler efficiency with an averagepage view time of roughly 30s.
we compute the cost saving u t cloud i punit ctoclient ctoserver crequest for each task as shown in table vi.
a data processing task is cost effective for distributedprocessing with browsers if the uis a positive number.
table iv a subset of amazon cloud service type and price aso f february service subtype price ecu ec2 t2.micro .
hr varied m3.medium .
hr m3.large .
hr .
c3.large .
hr table v a subset of amazon cloud service type and price aso f february service subtype price s3 transfer in to s3 free s3 to cloudfront free cloudfront to origin .
gb to internet first 10tb .
gb to internet next 40tb .
gb to internet next 100tb .
gb to internet next 350tb .
gb http requests .
per b. use cases scenario face detection overview.
for the first gray computing case study we chose face detection which is a data processing task that facebook runs at scale on all of its photo uploads to facilitate usertagging in photos.
face detection is a task that most facebookusers would probably consider to be beneficial to them.
facedetection also has an interesting characteristic in that it canbe run on the data that is already being sent to clients as partof a web page e.g.
the photos being viewed on facebook .
notice in our cost model a large portion of the cost comesfrom sending input data from the server to clients.
this costbecomes prohibitively expensive for data intensive tasks.
datatransfer costs appear inevitable since it is impossible for the table vi c omparison of cost sa ving per gb for different tasks .
the higher this v alue the more computa tionally intensive the task and the more suitable the task for distributed da ta pro cessing with browsers .
task cost savings per gb rainbow table face detection .
sentiment analysis .
image scaling .
word count .
icse florence italyclients to compute without input data.
however there are some circumstances that the input data needs to be sent tothe client anyway such as when the data is an integral partof the web page being served to the client.
in these cases noextra data transfer costs are incurred.
facebook has million as of photos uploaded every day and face detectionin photos is a relatively computationally intensive computervision task.
our hypothesis was that significant cost savingscould be obtained through gray computing.
experiment setup.
to test whether offloading face detection tasks to gray computing is cost effective we conductedexperiments for face detection tasks on both javascript in thebrowser and opencv which is a highly optimized c c computer vision library in the cloud.
for the javascript facialdetection algorithm we used an opensource implementation ofthe viola jones algorithm .
for the amazon cloud com puting implementation we use the same algorithm but a highperformance c implementation from opencv .
.
.
there aremore advanced face detection algorithms that achieve betteraccuracy but need more computing time which would favorbrowser based data processing.
empirical results.
the results of the experiment are shown in table vii.
we can see that the computation time is approxi mately linear to the total number of pixels or image size.
thisresult is expected because viola jones face detection uses amulti scale detector to go over the entire image which makesthe search space proportional to the image dimensions.
analysis of results.
facebook has million photos uploaded by users each day.
suppose the average resolutionof the photos being uploaded is million pixels which isless than the average resolution of most smartphone cameras such as the million pixel megapixel iphone camera.it takes .7s .
3600h 278h of computing time for an ec2 m3.medium instance to process these photos.with our utility function u t cloud i punit d i ptransferout o ptransferin crequest where ptransferout because the photos already exist in clients so there is no additional cost for transferring data to the clients.for each photo the returned computing result should be anarray of coordinations of rectangles which is rather small soocan be ignored.
the client does not need to fetch the photos only needs to issue one http request to return theresults.
we assume the client processes photos at a time which takes around 7s on a browser with average hardware .we choose a duplication factor d since this application is not very sensitive to security.
thus c request .
and u .
could be saved each day by distributing this task to browserclients rather than running it in amazon ec2.
that is ayearly cost savings of roughly .
million dollars.
the actualalgorithm facebook uses is likely to be more complex than thealgorithm we used and a larger t cloud is expected.
therefore the cost savings could be even larger than we calculated.
scenario image scalingoverview.
another example of a useful gray computation that can be applied to data already being delivered to clients is image scaling.
websites often scale image resources suchas product images to a variety of sizes in order to adapt themto various devices.
for instance desktop browsers may loadthe image at its original size and quality while mobile clientswith smaller screens may load the compressed image withlower quality.
the amazon kindle browser talks directly toec2 rather than target websites and receives cached mobile optimized versions of the site and images that are producedfrom large scale image processing jobs.
we can offload image compression tasks to gray computing.
similar to scenario the photos are already being deliveredto the clients so there is no added cost for transferring datafrom the server to clients.
after loading the images each clientwill do the scaling and compression work and then send thescaled images for mobile devices back to the server.
whenthe number of images to process becomes huge the savedcomputation cost could be substantial.
experiment setup.
there are many image scaling algorithms and they achieve different compression qualities withdifferent time consumptions.
we wanted to measure javascrip t s computation speed on this task in the real world and did notwant to focus on the differences in efficiency of the algorithms.we chose bicubic interpolation for image scaling on both thebrowser and server.
bicubic interpolation is implemented injavascript and c for both cloud and browser platforms.
empirical results.
the experiment results in terms of computation speed are shown in table viii.
analysis of results.
to get a quantitive understanding of the cost savings by offloading the image scaling tasksto gray computing we again use facebook as an exampleand make several assumptions suppose the average resolutionof the photos being uploaded is million pixels and theaverage scaling ratio is .
million uploaded photosdaily take .55s .
3600h 472h for one ec2 m3.medium instance to scale and compress.
with our utilityfunctionu t cloud i punit k i ptransferout o ptransferin crequest .
againptransferout because the photos already exist in clients so there is no additionalcost for transferring data to the clients.
we assume the clientprocesses photos at a time which takes around 10s ona browser with average hardware .
we choose a duplicationfactord since this application is not very sensitive to security.
o .
80k .
104gb .
crequest .
.
therefore u .
.
.
is saved each day by distributing this task to browsers whichaggregates to dollars a year.
there are many techniques that can be used to further improve the quality of the resized images such as sharpening filtering etc.
these techniques require additional computationtime larger t cloud so the amount of money saved could be further increased if these techniques are applied to uploadedimages.
icse florence italytable vii c omputing time comparison for face detection tasks .
image dimension number of pixels size js computing time s ec2 computing time s .
million 82kb .
.
.
million 156kb .
.
.
million 277kb .
.
table viii c omputing time comparison for image resize tasks .
image dimension sizejavascript computing time s ec2 computing time s scaling ratio 82kb .
.
.
.
.
.
156kb .
.
.
.
.
.
277kb .
.
.
.
.
.
scenario sentiment analysis overview.
sentiment analysis refers to using techniques from natural language processing and text analysis to identify the attitude of a writer with respect to some sourcematerials.
for instance amazon allows customers to reviewthe products they purchased.
it would be useful to automat ically identify the positive negative comments and rank theproducts based on the customers attitude.
since the commentcontext is sent to the websites visitors anyway there will beno extra cost incurred due to the data transferred from serversto clients.
experiment setup.
there are many machine learning algorithms proposed for sentiment analysis in the literature .we implemented a naive bayes classifier a simple but quiteeffective approach for sentiment analysis.
the classifier takesas input a user review and predicts whether the review is posi tive or negative.
the classifier is implemented with javascriptfor the browsers and python for ec2.
we trained our classifierwith a movie review dataset containing positive and1000 negative reviews.
we collected movie reviews from theinternet as test data and partitioned them into files each ofsize 200kb.
then we used the trained classifier to predict theattitude of each review item and record the time needed.
empirical results.
for an input size of 200kb the prediction time of browsers with javascript is .3s and for the sametask running on an ec2 m3.medium instance the predictiontime is .7s.
analysis of results.
for 1gb input data the cost savings is1000 .
.
.
.
.
to get a quantitive understanding of how much money can be saved by offloadingthe sentiment analysis tasks to gray computing we use face book as an example and make several assumptions supposeeach user uploaded photo has an average of 1kb comments.since facebook has million photos uploaded daily thereis350 350gb comments in total.
if facebook wants to analyze the attitude of every comment of the photos the cost is .
a year.
scenario word count overview.
word counting is the classic use case that is used to demonstrate big data processing with mapreduce.word counting requires determining the total occurrence ofeach word in a group of web pages.
it is a task that requiresa relatively large amount of textual input with a very smallamount of computational work.experiment setup.
we compared the cost effectiveness of running a word count task in javascript versus the amazonelastic map reduce emr service using a script written inpython .
the amazon emr experiment was configured with1 master node m1.medium and slave nodes m1.medium .
table ix c omputing time comparison for wordcount .
input size browser javascript amazon emr 18mb .7s 94s 380mb 3min 6min empirical results.
the experiment results in terms of computing speed are shown in table ix.
for 1gb of input c request is very small the cost savings is ccloud ctoclient .
.
.
.
.
.
analysis of results.
as can be seen the cost saving is a negative number which means the value of the computedresults is less than the cost of transferring the text data tothe clients.
word counting is not an ideal application fordistributed data processing with gray computing because itis a data intensive but computationally simple task.
however if the word count was being run on web pages delivered tothe clients the data transfer cost would not be incurred and itwould be as cost effective as scenarios and .
scenario rainbow table generation overview.
the final use case was designed to analyse the gray computing power that could be wielded by an attacker if they compromised a large number of websites and begandistributing malicious data processing tasks in their web pages.for the malicious use case we focused on password crackingwith rainbow tables which has become a common typeof work performed by cyber attackers on data stolen fromwebsites.
a rainbow table is a precomputed table forreversing cryptographic hash functions usually for crackingpassword hashes.
these tables are used to recover the plaintextpasswords that are stored as hashes in databases.
to distribute the tasks the central server only needs to send a start string and end string of a range of the password to theclients.
the size of input is extremely small and can almostbe ignored.
therefore c toclient .
experiment setup.
the browser environment is the same as described in section iv a. for the cloud instance weimplemented the rainbow table generation algorithm in c and compiled with g .
.
.
icse florence italyempirical results.
the results of the experiment are shown in table x. generating hashes on a desktop s browser using ported javascript took 4s.
generating the same hasheson an amazon ec2 m3.medium using c took 3s.
table x c omputing time comparison for genera ting a rainbow table .
input size browser native js ec2 c browser ported js 9e5 90s 3s 4s analysis of results.
suppose an attacker compromises websites with a cumulative traffic of million visitors perday and an average browsing time of minutes.
the attackerwould be able to wield the equivalent of .25hour .
365day .
107computing hours a year in a browser.
to accomplish the same task with an amazonec2 m3.medium instance for a year the yearly cost wouldbe3.
.
hour .crequest .
.
.
with our utility function u ccloud o ptransferin crequest the yearly cost saving uis around .8million.
the nearnative computing speed of javascript with browser and thelow data transferred per unit of computational work makesthis an effective processing task to distribute.
v. r ela ted work the term volunteer computing was first used by luis f. g. sarmentan .
he developed a java applet basedvolunteer computing system called bayanihan in .seti home was one of the earliest projects to prove thepracticality of the volunteer computing concept.
seti homewas designed to use the numerous personal computers of thepublic to process vast amounts of telescope signals to searchfor extraterrestrial intelligence.
some of seti home s suc cessors include folding home which simulates proteinfolding for disease research and boinc which is aplatform to hold various themes of research projects.
whatthese projects share in common is that they all focus on non profit scientific computing and they all require users to installa specialized client side software in order to participate in theprojects.
whereas in our paper we analyze the highly volatilebrowser based data processing domain and extend beyondscientific computing to websites business operations.
a number of researchers have investigated volunteer computing with browsers.
the primary advantage of a browser based approach is that the user only needs to open a web pageto take part in the computation.
krupa et al.
proposeda browser based volunteer computing architecture for webcrawling.
finally konishi et al.
evaluated browser basedcomputing with ajax and the comparative performance ofjavascript and legacy computer languages.
in our work weadd a comprehensive analysis of the architectural changesto optimize this paradigm for websites served from cloudenvironments the impact of page view time on schedulerefficiency and data transfer a cost model for assessing thecost effectiveness of distributing a given task to browser basedclients as opposed to running the computation in the cloud and4 present a number of practical examples of data processingtasks to support website operators particularly social networksand e commerce.
mapreduce is a programming model for large parallel data processing proposed by google which has beenadapted to various distributed computing environments suchas volunteer computing .
there are some early prototypesimplementing mapreduce with javascript .
lin et al.
observed that the traditional mapreduce is proposed for ho mogeneous cluster environments and performs poorly on vol unteer computing systems where computing nodes are volatileand with high rate of unavailability as we also demonstratedwith our derivation of in section iii e. they propose moon mapreduce on opportunistic environments which extendshadoop with adaptive task and data scheduling algorithmsand achieves a fold performance improvement.
however moon targets institutional intranet environments like studentlabs where computer nodes are connected with a local networkwith high bandwidth and low latency.
we focus on cloud andinternet environments with highly heterogeneous computationability widely varying client participation times and non fixedcosts for data transfer and task distribution resources.
vi.
c onclusion every day millions of users opt into allowing websites to use their browsers computing resources to perform computa tional tasks such as form validation.
in this paper we explorethe feasibility cost effectiveness user experience impact andarchitectural optimizations for leveraging the browsers ofwebsite visitors for more intensive distributed data processing which we term gray computing.
although previous researchhas demonstrated it is possible to build distributed data pro cessing systems with browsers when the web visitors explicitlyopt into the computational tasks that they perform no detailedanalysis has been done regarding the computational power user impact and cost effectiveness of these systems whenthey rely on casual website visitors.
the empirical resultsfrom performing a variety of gray computing tasks rangingfrom face detection to sentiment analysis show that there issignificant computational power in gray computing and largefinancial incentives to exploit it.
due to these incentives andthe vast potential for misuse we believe that much moreresearch is needed into the security and ethical considerationsaround gray computing.
as part of the analysis in this paper we derived a cost model that can be used to assess the suitability of differentdata processing tasks for distributed data processing with graycomputing.
this cost model can aid future discussions of waysof legitimately incentivizing users to opt into gray computing.further we pinpointed the key factors that determine whethera task is suitable for gray computing and provide a processfor assessing the suitability of new data processing task types which can help aid in guiding the design of gray computingsystems and user incentive programs.
we also presented anumber of architectural solutions that can be employed to ex ploit cost and performance asymmetries in cloud environmentsto improve the cost effectiveness of gray computing.
icse florence italyreferences j. dean and s. ghemawat mapreduce simplified data processing on large clusters communications of the acm vol.
no.
pp.
.
l. f. sarmenta v olunteer computing ph.d. dissertation citeseer .
y outube statistics intel processors cs .htm.
top500 supercomputer s. ryza and t. wall mrjs a javascript mapreduce framework for web browsers url cs.
brown.
edu courses csci2950u f11 papers mrjs.
pdf .
r. cushing g. h. h. putra s. koulouzis a. belloum m. bubak and c. de laat distributed computing on an ensemble of browsers internet computing ieee vol.
no.
pp.
.
j. j. merelo a. m. garc a j. l. j. laredo j. lupi on and f. tricas browser based distributed evolutionary computation performance andscaling behavior in proceedings of the gecco conference companion on genetic and evolutionary computation.
acm pp.
.
p .
langhans c. wieser and f. bry crowdsourcing mapreduce jsmapreduce in proceedings of the 22nd international conference on world wide web companion.
international world wide webconferences steering committee pp.
.
emscripten clang greasemonkey l. f. sarmenta sabotage tolerance mechanisms for volunteer computing systems future generation computer systems vol.
no.
pp.
.
c. liu r. w. white and s. dumais understanding web browsing behaviors through weibull analysis of dwell time in proceedings of the 33rd international acm sigir conference on research and development in information retrieval.
acm pp.
.
w. weibull wide applicability journal of applied mechanics .
p .
viola and m. jones rapid object detection using a boosted cascade of simple features in computer vision and pattern recognition .
cvpr .
proceedings of the ieee computer society confer ence on vol.
.
ieee pp.
i .
b. pang and l. lee opinion mining and sentiment analysis f oundations and trends in information retrieval vol.
no.
pp.
.
b. pang and l. lee a sentimental education sentiment analysis using subjectivity summarization based on minimum cuts in proceedings of the 42nd annual meeting on association for computational linguistics.association for computational linguistics p. .
p .
oechslin making a faster cryptanalytic time memory trade off in advances in cryptology crypto .
springer pp.
.
l. f. sarmenta bayanihan web based volunteer computing using java in worldwide computing and its applications wwca .
springer pp.
.
d. p .
anderson j. cobb e. korpela m. lebofsky and d. werthimer seti home an experiment in public resource computing communications of the acm vol.
no.
pp.
.
folding home d. p .
anderson boinc a system for public resource computing and storage in proceedings of fifth ieee acm international workshop on grid computing .
ieee pp.
.
t. krupa p .
majewski b. kowalczyk and w. turek on demand web search using browser based volunteer computing in sixth international conference on complex intelligent and software intensive systems cisis .
ieee pp.
.
f. konishi s. ohki a. konagaya r. umestu and m. ishii rabc a conceptual design of pervasive infrastructure for browser computingbased on ajax technologies in seventh ieee international symposium on cluster computing and the grid .
ccgrid .
ieee pp.
.
h. lin x. ma j. archuleta w. c. feng m. gardner and z. zhang moon mapreduce on opportunistic environments in proceedings of the 19th acm international symposium on high performance distribut ed computing.
acm pp.
.
icse florence italy