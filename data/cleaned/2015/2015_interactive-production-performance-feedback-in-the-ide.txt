interactive production performance feedback in the ide j urgen cito mit cambridge ma usa jcito mit.eduphilipp leitner chalmers university of gothenburg gothenburg sweden philipp.leitner chalmers.semartin rinard mit cambridge ma usa rinard mit.eduharald c. gall university of zurich zurich switzerland gall ifi.uzh.ch abstract because of differences between development and production environments many software performance problems are detected only after software enters production.
we present performancehat a new system that uses profiling information from production executions to develop a global performance model suitable for integration into interactive development environments.
performancehat s ability to incrementally update this global model as the software is changed in the development environment enables it to deliver near real time predictions of performance consequences reflecting the impact on the production environment.
we build performancehat as an eclipse plugin and evaluate it in a controlled experiment with professional software developers implementing several software maintenance tasks using our approach and a representative baseline kibana .
our results indicate that developers using performancehat were significantly faster in detecting the performance problem and finding the root cause of the problem.
these results provide encouraging evidence that our approach helps developers detect prevent and debug production performance problems during development before the problem manifests in production.
index t erms software performance engineering ide user study i. i ntroduction because of differences between development and production execution environments many software performance problems are detected only after software enters production .
to track down and correct such performance problems developers are currently faced with the task of inspecting monitoring data such as logs traces and metrics and must often run additional performance tests to fully localize the root cause of the problem.
problems associated with this situation include the deployment of software with performance problems the time required to identify and correct performance problems and the need for developers to function effectively in two very different environments specifically the software code view environment during development and the monitoring based environment during deployment.
previous studies have shown that switching between separate views makes it difficult for developers to maintain a clear image of the overall context of runtime behavior .
this separation is particularly problematic when developers are immersed in the context of a particular task .
we present a new tool performancehat that integrates production monitoring information directly into the source code view.
instead of requiring developers to manually analyze monitoring data and perform additional test runs to obtain relevant information about performance problems per formancehat works with a performance model derived from monitoring data collected during production runs.
as the developer modifies the code performancehat incrementally updates the model to provide the developer with performance feedback in near real time.
because the model is derived from production monitoring data the performance feedback accurately reflects the performance consequences of specific developer changes in the production environment not in the development environment.
benefits of this system include the early detection and correction of performance problems in development before the software enters production and a tight integration of performance feedback into the development process.
we evaluate performancehat in the eclipse ide with java.
in a controlled experiment with professional software developers the data show that developers using performancehat find the root cause of production performance problems significantly faster than a control group using standard techniques.
at the same time developers using performancehat when working on non performance relevant tasks experienced no drop in productivity compared with the control group.
ii.
b ackground r ela ted work we introduce some background on profiling and monitoring discuss related work around software performance prediction and source code view augmentation.
profiling vs monitoring.
profiling is a form of dynamic program analysis that measures performance aspects of a running program.
software performance is highly dependent on the execution environment and workload of a system.
therefore information provided by profilers executed locally is often not enough to identify performance issues in production environments.
a performance monitoring tool continuously monitors components of deployed software systems.
it collects several performance metrics such as response time or cpu utilization from the monitored application and usually displays them in form of time series graphs in dashboards.
a recent study has shown that monitoring tools exhibit enough information to be used to identify performance regressions .
our approach performancehat utilizes information collected in state of the art monitoring tools to build a performance model that is integrated in the developer workflow in the ide.
impact analysis performance prediction.
change impact analysis supports the comprehension evaluation and implementation of changes in software .
most of the work ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
that is related to change impact analysis and performance operates on an architectural level and is not supposed to be triggered during software development.
recent work by luo et al.
uses a genetic algorithm to investigate a large input search space that might lead to performance regressions.
our approach for impact analysis is applied live i.e.
during software development and leverages an initially build performance model from monitoring data to incrementally reflect software changes to provide early feedback to software developers.
augmenting source code views.
several papers have proposed augmenting source code views to support program analysis efforts.
hoffswell et al.
introduce different kinds of in situ visualizations related to runtime information in the source code to improve program understanding.
lieber et al.
augment javascript code in browser debugging tools with runtime information to support reasoning on asynchronous function interaction.
beck et al.
augment method definitions in the intellij ide with in situ visualizations obtained by sampling stack traces .
our approach goes beyond augmenting local runtime information e.g.
generated by tests to deal with distinct scalability challenges that we describe in our approach in section iv c. further we go beyond visualizing runtime information insitu and provide early feedback on source code changes by leveraging incremental performance analysis.
iii.
s cope our research targets systems with particular properties online services we target online services that are delivered as a service saas applications typically deployed on cloud infrastructure and accessed by customers as web applications over the internet.
specifically we do not consider embedded systems big data applications scientific computing applications or desktop applications.
while an approach similar to ours could also be used for a subset of those the concrete modeling and performance related challenges would change taking them out of scope for the present study.
software maintenance we assume that the application is already deployed to production and used by real customers.
however given the ongoing industrial push to lean development methodologies and continuous deployment many saas applications are in maintenance mode for the vast majority of their lifetime.
system level performance performancehat supports performance engineering on a systems level rather than improving e.g.
the algorithmic complexity of a component.
hence a focus is put on component to component interactions e.g.
remote service invocations database queries as these tend to be important factors contributing to performance regressions on a system level while also being hard to test without knowing production conditions.
production observable units our approach is limited to modeling methods that are actively measurable by existing monitoring tools.
thus methods that might have suboptimal theoretical computational complexity but do not exhibit any significant overhead that is captured by monitoring will not be modeled.
iv .
i nteractive monitoring feedback in the ide our theory is that software developers are enabled to identify and prevent performance issues faster when source code is augmented with monitoring data and developers receive immediate i.e.
near real time feedback on code changes.
we state the goals for our approach performancehat and describe the models and techniques that allow us to achieve them.
to guide the design of our approach we formulate two goals based on our theory operational awareness we want to provide operational awareness to developers by tightly integrating monitoring data into the development workflow.
by that developers should become more aware of the operational footprint of their source code.
contextualization at the same time we aspire to provide contextualization of these operational aspects.
monitoring data should be available in the context of current development task to minimize context switches to other external monitoring tools.
based on these principles we implement performancehat a s part of the eclipse ide with java figure .
whenever a class is loaded we construct a performance model and display it as in situ annotations yellow highlighting in the source code.
when hovering over these annotations a box appears that provides performance information from the model retrieved from production monitoring.
performance analysis is built into the incremental build process i.e.
we incrementally update the performance model every time a developers saves new changes by triggering the incremental build of eclipse .
through this process we provide interactive performance analysis updates so that developers retrieve immediate feedback of the impact of their changes.
we now discuss more formally how we achieve our goals by deriving an in ide performance model.
a. in ide performance model we construct our model by establishing a relation from programs to datasets retrieved from monitoring tools.
we then describe how we can incrementally update our performance model by reflecting and propagating changes in the model.
we consider programs pas syntactically valid programs of a language p. a program p pconsists of a set of methods m m p where every method mis uniquely identifiable through id m e.g.
fully qualified method names in java organized in classes or any other unit of organization for methods e.g.
modules .
a syntactically valid program p p can be transformed into an abstract syntax tree a tree representation of the source code of p denoted by ast p .
we consider a simplified ast model where the smallest entity nodes are method declarations and statements within a method method invocations branching statements and loops .
formally an ast is a tuple a a wherea authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a displaying operational footprint b inferring the performance of newly written code fig.
a performancehat in action displaying execution times in production contextualized on method level.
the box is displayed as the developer hovers over the marker on the this.retrieve method invocation.
b after introducing a code change the inference function attempts to predict the newly written code.
further it is propagated over the blocks of foreach loops.
the box is displayed when hovering over the loop over the task collection showing the prediction with supporting parameters.
is set of nodes in the ast source code artifacts a0 a is the root node and a mapsto a is a total function that given a node maps it to a list of its child nodes.
each nodea i ahas a unique identifier id ai .
for convenience we also define a function astm m that returns the ast for a method m m p .
formally mast past where mast astm m past ast p m m p and id m id a0 inmast .
trace data model our approach relies on execution traces that have been collected at runtime either throughobservation instrumentation or measurement.
while this datacould potentially have different types the focus of this paperis on runtime data that is relevant to software performance e.g.
execution times workload .
let us consider a dataset d to be the set of trace data points.
the model of a data pointd i d is illustrated in table i. the illustrative trace shown in the table is an actual trace type used in the evaluation throughthe monitoring tool kieker .
we model the point in timethe data point has been measured or observed and the runtimeentity that produced the data point the granularity rangesfrom methods to api endpoints to operating system processes .
we assume every trace has a numerical or categorical value of the observation.
many traces are also associated with somekind of label that is part of the trace meta data e.g.
methodname .
the context is a set of additional information thatsignifies for instance on which infrastructure the entity wasdeployed on or which log correlation id in the example it iscalled sessionid was involved .
table i trace data model abstract description illustrative trace t recorded time logging timestamp e observed entity java method trace type execution time et primitive v alue measured time e.g.
250ms l label method name e.g.
us.ibm.map.put c context stack size sessionid host ... trace mapping in an initial step ast nodes are combined with the dynamic view of runtime traces.
this mappingconstitutes a relation between nodes in the ast and a setof trace data.
on a high level this process is inspired byprevious work in software traceability using formal conceptanalysis which is a general framework to reason about binary relationships.
a set of traces can be mapped to different ast node types e.g.
method invocations loop headers based on different specification criteria in both node and trace.
in this particularcase of response times we map data based on the fully qualified method name in java that is both available as partof the ast node and in the trace data.
specifications definedeclarative queries about program behavior that establish thismapping relationship.
specification queries.
we model the relation between source code artifacts a ainast p to trace data points d d is authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
loginsimplified abstract syntax tree ast distributed runtime traces vtextbox.
getpassword ....... responsetime datamanager.start responsetime datamanager.start .
responsetime vtextbox.getpassword responsetime paymentservice cpuutilization connectionsvm1 .
ids ids ............... responsetime datamanger.start responsetime setconnectionimage responsetime vtextbox.getpassword cpuutilization connectionsvm1 .
vmbilled customerservicevm1 .
ids ids ............... responsetime connection.login responsetime setconnectionimage cpuutilization connectionsvm1 .
responsetime connection.login responsetime connection.login ........datamanager.
start fig.
an illustration of the performance model construction process based on a specification function in this particular case identity matching multiple data points from the dataset are mapped to an ast node.
modeled as a mapping s a mapsto d .
the mapping is directed by a declarative specification predicate sp a d mapsto true false .
the predicate decides based on an expression on attributes within the source code entity and data point whether there is a match.
while the specification can take many different forms depending on the application domain we illustrate this concept by briefly outlining two typical examples for online services entity level specification.
in its simplest form the predicate returns true if information of one trace can be exactly mapped to one source code entity based on an exact attribute matching.
let us again look at our the example of response time traces sprt a d id a l d d et.
this is a common use case for execution times or methodlevel counters that can be mapped to method definitions and method invocations.
measurements of multiple threads are attributed to the same method declaration when encountered in the ast.
system level specification.
a more complex specification could take the form of mapping memory consumption which is usually measured at the system or process level to method invocations through program analysis and statistical modeling.
the idea is to sample system level runtime properties over time and at the same time through instrumentation sample how long each method has been executed at recorded points in time.
overlaying both measurements can approximate the impact of certain method invocations to the system level property e.g.
memory consumption .
this approach has been already shown to work well when mapping system level energy consumption on source line level .
b. incremental performance analysis during software maintenance software developers change existing code to perform change requests.
one of our goals is to provide early feedback regarding software performance so that software developers can make data driven decisions about their changes and prevent performance problems from being deployed to production.
to achieve this we design an incremental analysis for software performance when addingmethod invocations and loops into existing methods.
we focus on these particular changes because existing work by sandoval et al.
has shown that they have the most significant effect on software performance.
changes to source code are reflected as additions or deletions in the ast.
existing work supports formal reasoning of these changes through tree differencing .
while this technique would also be a viable option for our approach to trigger an update in our performance model we apply a slightly different procedure that enables faster analysis.
since in addition to static source code we also have access to a datasetdand a specification function s we can distinguish between ast nodes that have data attached to them and new nodes without information.
algorithm presents an overview on a holistic approach that combines constructing a performance model for new classes through mapping inference of changes and intra procedural propagation within the ast of a particular method m m p in linear time with respect to the number of ast nodes in m we iterate through every node in the method ast in bfs order and attempt to associate data from the trace data set dto the node through the specification query s. if the node cannot be associated with existing data we assume it to be newly added code and add it to a stack toinfernodes .
nodes in the stack without data are iterated and attempted to be inferred by a given inference function .
because we pushed nodes from the stack from the previous bfs iteration outside in we now infer nodes inside out .
every newly inferred method is added to a context set that is passed to the inference model and can be used for nodes that are higher in the hierarchy.
example let us assume we have a new method invocation within a for loop.
the new method invocation is inferred before we reach the loop node thus we add its information to the context.
as we reach the loop node the loop inference model can use this newly inferred information in the context to adjust its prediction.
passing up the context is how we achieve propagation.
incremental update through partial inference we integrate our analysis into the build process and thus into the development workflow.
hence a sense of immediacy is required.
existing performance prediction methods range from analytical models to very long running simulation models .
to obtain a result in an appropriate time frame our approach requires an analytical performance inference model.
to illustrate how a possible inference model can look like we combine an analytical model with a learning model inspired by didona et al.
.
however our work is general in the sense that a different analytical model for performance inference could be integrated as well.
we briefly illustrate two models the we implemented in our tool performancehat .
formally an inference model is a function a a mapsto d wherea ast p .
this function attempts to infer new information based on existing matched data i.e.
its inference context .
different source code artifact types require different inference models.
we authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
code change ?
?inference propagation fig.
sequence of incremental performance analysis and propagation code is changed and nodes are inferred from the bottom up and propagated up to the method declaration.
algorithm matching inferring and propagating runtime information to ast nodes in method m data a method m m p a dataset d a specification function s an inference function result all relevant ast nodes in mannotated with data in dor with a prediction inferred through toinfernodes iterator goes through method ast through bfs i.e.
outside in for node in astm m do node.data visit node s trigger node type dependent visitor to match dto node based ons ifnot node.data then toinfernodes toinfernodes node adding nodes unknown to din this context to be inferred end end context while not empty toinfernodes do currentnode toinfernodes.pop infer information about nodes from the inside out currentnode.data currentnode context context context currentnode newly inferred data is propagated through passing context up end consider addition of method invocations and loops method invocation inference methodinvocation the most atomic change we consider for our analysis is adding a new method invocation.
to infer new information about the response time of this method invocation in the context of the parent method we require information about it from an already existing context i.e.
the method being invoked in a different parent method .
further we want to adjust the response time based on the different workload parameters of the parent method.
thus we learn a model mwl m p m p mapsto d any viable regression model that represents how response times of invocations are affected by different workloads wl .
from this learned model we can infer new method invocations as methodinvocation m mwl parent m m where parent m p mapsto m p is a function that returns the parent method of an invocation.
loop inference loop when adding new loops entirely or adding new method invocations within the body of aloop we consider a simple non parametric regression model i.e.
an additive model to infer the average execution time of the loop.
let l astm m be a loop node in methodm m p .
we build an additive model over the mapped or inferred execution times of all statements method invocations or other blocks in the loop body of lmultiplied by the average number of iterations size l .
more formally t l summationtext n l sn n.data is a model of the execution time of the loop body where the functions snare unknown smoothing factors that we can fit from existing data in d. thus loop l size l t l .
in case of a foreach loop over a collection the number of iterations size can either be retrieved from instrumenting the collections in the production environment or by allowing the software developer to provide an estimate for this parameter.
figure illustrates incremental analysis with an example a developer changes the code and adds a method invocation task.gethours within a loop over collection of type tasks .
the nodes of the new method invocation and its surrounding loop do not have any information attached to them.
first the newly introduced method is inferred through methodinvocation and attached to the node.
the new information is propagated and used in loop to approximate the new loop execution time.
all new information is then propagated to all nodes up until the method declaration.
resulting performance model after describing all steps of construction we formally summarize our performance model as the tuple angbracketlefta d s angbracketright where ais a simplified ast mostly on method level dis a trace dataset retrieved from a monitoring tool sis a specification query that establishes a relation from ast nodes to traces in the dataset is an inference function that employs lightweight prediction models to update information on ast nodes we now describe the implementation of performancehat and discuss scalability concerns.
c. scalability design and implementation given that our performance analysis is integrated into the development workflow it is important that it does not introduce significant delays and interrupt the workflow.
while we authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
presented how to overcome conceptual impediments to enable immediate analysis earlier i.e.
incremental inference and propagation we now want to discuss design considerations that reflect on architectural scalability of our approach.
we implemented performancehat as a combination of an eclipse plugin for java and further components that deal with collecting and aggregating runtime performance information.
when a program is loaded in the ide we construct our initial performance model.
performancehat then hooks our incremental performance analysis into eclipses incremental builder.
this means every time a file is saved we start the analysis for that particular file.
in initial versions of performancehat every build process triggered fetching new data from a remote instance e.g.
the performance monitoring server introducing network latency as a bottleneck.
an iterative process to improve scalability resulted in the the high level architecture depicted in figure .
it bundles application logic for performance model construction i.e.
specification and inference as an extension to the ide and retrieves data from a local component local feedback handler that synchronizes with a remote component deployed feedback handler e.g.
a monitoring tool server located closer to the running application in production.
we provide a brief overview on the efforts that were required to enable scalability to construct and incrementally update a performance model in the ide local feedback handler feedback handler is implemented as a java application exposing a rest api with a document data store mongodb .
the deployed feedback handler is installed on the remote infrastructure close to the deployed system and has an interface to receive or pull runtime information from monitoring systems after transforming it into a local model that is subsequently understood by the extension in the ide .
the local feedback handler runs as a separate process on the software developer s local workstation.
the reason for this split is that constructing the performance model and performing incremental analysis in the ide requires fast access to monitoring data.
the local feedback handler deals with periodically fetching data from potentially remote systems over an http interface.
local ide cache additionally we reduce the feedback loading latency by introducing a local ide cache so that each entry has to only be retrieved once per session.
we used an lru cache with a size of maximal entries and a timeout of minutes after minutes the cache entry is discarded however both these entries are configurable .
this reduced build time significantly as discussed in section vi.
bulk fetching a significant improvement also occurred when for an empty cache we registered all nodes that required information from the feedback handler and then loaded all information in bulk.
the prototype implementation including documentation is available as an open source project on github .
local workstation local feedback handler local ide cachedatastore httpinfrastructure deployed feedback handlerdeployed systemtransform ide infrastructure deployed feedback handlerdeployed systemtransform infrastructure n deployed feedback handlerdeployed systemtransform....http stream apache kafka broker httpspecification functioninference function registered filters fig.
resulting scalable architecture for performancehat v. u ser study ev alua tion to evaluate whether the proposed approach has a significant impact on decisions made in the course of software maintenance tasks specifically related to performance we conduct a controlled experiment with professional software developers as study participants.
in the following we describe the study in detail outlining our hypotheses describing programming tasks measurements and the study setup.
we then present the results of the study and discuss threats to validity.
a. hypotheses the goal of our study is to empirically evaluate the impact our approach has on software maintenance tasks that would introduce performance issues.
to guide our user study we formulate the following hypotheses based on this claim.
h01 given a maintenance task that would introduce a performance problem software developers using our approach are faster in detecting the performance problem we are interested in knowing whether the presence of performance data and inference on code level supports software developers in detecting performance issues in code faster.
h02 given a maintenance task that would introduce a performance problem software engineers using our approach are faster in finding the root cause of the performance problem additionally when a high level problem has been detected we are interested to see whether our approach allows software developers to find the root cause of the issue faster i.e.
does it improve debugging of the performance issue .
h03 given a maintenance task that is not relevant to performance software engineers using our approach are not slower than the control group in solving the task as not all software maintenance tasks potentially introduce a performance problem we are equally interested whether our approach introduces overhead into the development process and thus increases development time.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
b. study design overview the broad goal of our experiment is to compare the presence of our approach to a representative baseline that illustrates how software developers currently deal with handling performance problems in industry.
investigating performance regressions from production is done through performance monitoring tools.
we considered using a conventional local profiler as a baseline but discarded that option because we want to investigate scenarios that are caught using production workloads.
thus local workloads generated by test suites would not surface in the local profiling tool e.g.
visualvm and would also not reflect the reality of how these kinds of performance issues are identified .
we design our user study as a controlled experiment using a between subject design a common approach in empirical software engineering studies .
in betweensubject design study participants are randomly assigned in one of two groups a treatment group and a control group.
both groups have to solve the same programming tasks.
the control group uses a common tool to display runtime performance information kibana in combination with eclipse to solve the tasks.
the treatment group uses our approach within eclipse to solve the tasks.
as the study application we make use of agilefant a commercial project management tool whose source code is entirely available as open source because it represents a non trivial industrial application that exhibits real life performance issues which have already been discussed in previous work .
c. study participants many empirical software engineering studies rely on students as study participants to evaluate their approaches.
for our approach however this was not an option as our study requires understanding and experience of runtime performance issues which are usually only encountered when deploying production software.
thus we recruited study participants from industrial partners through snowball sampling .
the study participants have at least years of professional software development experience and who have previously worked with java.
performance skill level was self assessed by the participants based on a likert scale question on their software performance ability.
we equally distributed the participants between control and treatment group based on their experience and performance skill level.
d. programming tasks and rationale when designing programming tasks for controlled experiments in software engineering research we are faced with the trade off of introducing a realistic scenario and minimizing task complexity and duration to properly capture the effects between the groups and avoiding unnecessary variance .
with agilefant as our study application we aim for a more realistic scenario in an industrial application.
we introduce two types of tasks in the controlled experiment.
we present the study participants with software maintenance tasks that are relevant to software performance but also with tasks that donot have an impact on performance.
the rationale for mixing the task types has two particular reasons which are also reflected in our hypotheses.
first we want to understand to what extent the augmentation of source code with performance data in our approach is a distraction that introduces additional cognitive load into tasks not relevant to performance see h03 .
second we want to avoid learning effects after initial tasks in study participants i.e.
them knowing that looking at performance data is usually a way to solve the task .
we now give a brief description of the tasks and types used in the study.
we briefly describe the tasks t1 to t4 in the text below.
a more detailed description of the tasks with corresponding source code can be found in our online appendix1.
performance relevant tasks t2 and t4 work by discovered code changes in our case study application that lead to performance problems.
we extracted two relevant change tasks from these changes for t2 and t4.
in t2 the study participants retrieve a collection from a method within a field object to extract object ids and add them to an existing set in a loop.
however this method is quite complex and introduces a performance problem.
the participants need to investigate the issue over multiple class files and methods and reason over performance data to find the root cause of the performance problem.
t4 requires the study participants to iterate over an existing collection to retrieve a value and compute a summary statistic that should then be attached to the parent object.
the method to retrieve the lower level value is lazily loaded and thus slower than maybe expected.
additionally the new code is located within two nested for loops.
participants need to retrieve performance information on all newly introduced statements and then reason about the propagation up to the method definition.
non performance tasks t1 and t3 we designed the regular tasks non performance tasks to be non trivial i.e.
that a performance problem might hide in the added statements.
for t1 study participants need to set a particular object state based on a value retrieved from a method call on an object passed as a parameter in which the underlying computation is unknown .
for t3 the study participants need to iterate over a collection and compute the sum of a value that needs to be attached to a transfer object similar to t4 .
e. measurements in the experiment we performed a number of different measurements depending on the task type.
for every task we measure a total time required to solve the task.
for performance relevant tasks we distinguish two more measurements total time t we measure the time it takes our study participant to solve the task.
beware that we only start measuring when the participants signaled that they understood the task and navigated to the correct location in the code to conduct the maintenance task.
we decided for this protocol to avoid measuring the time it takes for task comprehension and task authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
navigation which is not the aim of our research and would introduce unnecessary variance into our experiment .
first encounter fe for tasks involving performance problems we measure the first encounter of the study participant with the realization that a performance problem has been introduced.
this realization can come through inspecting performance data to the newly introduced artifact either in kibana by the control group or in the ide with our approach in the treatment group or by attempting to deploy the new code and receiving feedback from performance tests see section v f for details on the setup .
root cause analysis rca t fe starting from the time of the first encounter of the introduced performance problem fe we measure the time until the participant finds the root cause of the performance problem rca .
we consider the root cause found when the participant can point to the lowest level artifacts i.e.
method invocations available in the code base that are the cause for the degraded performance effect and can back their findings with performance data.
performance data can be queried through the provided tools in each group.
f .
study setup we conducted the experiments on our own workstation onsite with each study participant.
we now describe the technical environment for the experiments and the protocol.
environment the study application was deployed within docker containers on our own workstation.
performance data was collected through the performance monitoring tool kieker .
for the control group we also deployed the elk stack a common setup that collects distributed log files with logstash stores them centrally in the database elasticsearch to finally display them in the dashboard visualization tool kibana.
the participants in the control group solely interact with kibana.
since the standard setup for kibana only displays the raw form of the collected logs we provided standard dashboards for the participants that displayed average execution times for each method which is the same information provided by our approach in the treatment group.
for the treatment group we deploy the feedback handler component that pulls performance data from kieker directly and converts them into our model with an adapter.
the participants were given a standard version of eclipse neon in version .
.
which included a text editor and a treeview and no further installed plugins except of course our approach in the treatment group .
participants were able to hot deploy the classes of single tasks separately by executing a console script.
when executing the script performance tests relevant to the task were simulated and the participants were given feedback whether their changes introduced a performance problem and to what extent response time in seconds .
protocol in the first to minutes the study participants were given an introduction into our study application and its data model the provided tools and into the task setting.
if needed the participants were given an introduction to the eclipse ide.
the control group was given an introduction tokibana and how it can be used in the experiment setting to potentially solve the programming tasks.
the same was done for our approach with the treatment group.
over the course of solving the tasks participants were encouraged to verbalize their thoughts i.e.
think aloud method .
all sessions were recorded for post analysis with consent of the study participants.
participants were given thorough task descriptions and were encouraged to ask questions to properly understand the questions.
when participants signaled that they understood the task and they started programming we started collecting our measurements.
after completing all tasks we debriefed the participants and asked about their study experience and collected feedback about content and process.
g. study results table ii shows the mean and standard deviation of results for all tasks and measurements grouped in control and treatment group.
we first use the shapiro wilk test to test whether our samples come from a normally distributed population.
we were not able to verify the normality hypothesis for our data.
thus we perform a mann whitney u test a non parametric statistical test to check for significant differences between the population of the two groups and cliff s delta to estimate the effect size similar to other comparable works in empirical software engineering .
table ii results in seconds over all tasks and measures for treatment and control presented as mean standard deviation together with the p value resulting from mann whitney u statistical tests.
fe and rca are only given for the performance relevant tasks t2 and t4.
p values .
are marked with.
treatment control mann whitney u t1 total .
.
.
.
t2 total .
.
.
t2 fe .
.
.
.
.
t2 rca .
.
.
.
.
t3 total .
.
.
.
.
t4 total .
.
.
.
t4 fe .
.
.
.
.
t4 rca .
.
.
.
.
the descriptive statistics suggest that the treatment group requires less time to complete each performance relevant task total measurements task and .
to ease the reading of the empirical measurements figure presents the experiment results in form of box plots comparing the time required by control and treatment group over all task and measures side by side.
in the following we investigate the results of the experiment with respect to our formulated hypotheses.
to avoid losing perspective in further aggregation we analyze the tasks relating to our hypotheses separately.
timing in performance relevant tasks h01andh02 looking at performance relevant tasks t2 and t4 both in table ii and figure the treatment group performs better in absolute terms for all measurements.
for both total times the difference is significant p value .
effect sizes cliff s delta .
and .
.
we now go into more detail between both measures for performance relevant tasks authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
total times spent on individual tasks for nonperformance tasks t1 and t3 and performance relevant tasks t2 and t4 in control kibana and treatment performance hat group.
detecting performance problems h0 for the fe first encounter measures see figure we see a significantdifference in t2 fe effect size cliff s delta .
.
in t4 fe however the difference is not significant.
a possible explana tion for this difference lies in the structure of the task t4 see section v d .
in t4 the code change occurs alreadyin two nested loops.
so even without direct presence ofperformance data in the process a software developer caneasily speculate that introducing yet another loop leads toano n time complexity.
in t2 however the introduced performance problem was not as obvious by simply inspectingcode without performance data.
root cause analysis h for the measure rca root cause analysis see figure both t2 rca and t4 rcashow significant differences effect sizes cliff s delta .68and .
between treatment and control group.
even inthe case of t4 where the first encounter was more easilyattainable through code inspection alone the analysis didrequire querying performance data to pinpoint the root causeof the performance problem.
fig.
times in performance relevant tasks broken down intofirst encounter of performance problem fe and root causeanalysis rca .
overhead in non performance tasks h for both regular non performance maintenance tasks t1 and t3 wewere not able to reject the null hypothesis.
beware that thisonly means not enough evidence is available to suggest thenull hypothesis is false at the given confidence level.
thus we have a strong indication that there are no significantdifferences between the treatment and control group for thesetasks.
in the context of our study this is an indication thatour approach does not introduce significant cognitive overheadthat distracts software developers from regular maintenancetasks.
h. threats to v alidity external v alidity refers to threats to the generalizability of the presented results.
in this regard our user study has a threat to external validity with regards to our selection ofstudy participants.
given that participation in such a study isnecessarily voluntary it is possible that our study participantsare not representative of the general population of developersof cloud applications.
another issue is our usage of thesnowballing sampling technique which may lead to an overlyhomogeneous participant population.
we have mitigated thisthreat by carefully supervising our participant demography and ensuring that e.g.
participants are not from the samecompany or close circle of collaborators.
internal v alidity describes the extent to which conclusions are justified by the data.
we are aware of two major threatsto the internal validity of the user study.
firstly given thecomplex domain of our approach some aspects of our studysetup were necessarily artificial i.e.
participants did not com mit to a real production environment monitoring data camefrom a pre established feedback dataset .
secondly given thenature of our study participants were aware of or couldat least suspect that the study was related to performance.this may have influenced their behavior to be more carefulregarding performance.
consequently it is possible that theeffect of performancehat outside of a study setting may be more pronounced.
vi.
ide o verhead analysis our performance analysis in performancehat is hooked into the incremental build process of the ide.
this meansevery time a file is saved the analysis process is triggered.anecdotally none of our study participants remarked uponany visible delays introduced through our analysis.
however to gain a more formal picture of its overhead we numericallystudy the build time impact of performancehat with two case studies.
while we deem a small overhead i.e.
increasein build times in the ide unavoidable we need to studywhether the additional effort for constructing the performancemodel i.e.
loading production data matching to the ast andrendering warnings and in situ visualizations does not undulyslow down the ide.
a experiment setup we analyze the build time impact in four different experimental settings as shown in table iii.we use two different case study applications agile fant a commercial project management tool whose sourcecode is entirely available as open source and an existingresearch prototype.
for both applications we established a authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
baseline of production feedback data by generating a representative workload on the deployed code.
for both applications we evaluate full project builds i.e.
a build of all java files as well as an incremental build that re builds only a single file fi.hut.soberit.agilefant.model.story in the case of agilefant and a web service controller in the case of the research prototype .
we find these applications interesting as they represent two orthogonal but typical use cases a large monolithic web application in case of agilefant and a small service in a much larger composite system in case of the research prototype.
table iii also lists the total number of ast nodes for each setting after applying ast construction as in section iv.
note that these are the simplified asts and not the entire asts of the original java source code file.
table iii summary of experimental settings.
application build type ast nodes agilefant full build agilefant incremental build story research prototype full build research prototype incremental build controller for each of those settings we further evaluate four different scenarios with or without cache as discussed in section iv c and with a cold or warm build environment.
we refer to the build environment as cold directly after re starting the ide and as warm after the respective build has run at least once.
we have chosen these two parameters as preliminary experimentation has shown that both the cache and whether the ide has already executed the same build before have a significant impact on the total build time as well as the build time impact of performancehat .
we executed all experiments on a lenovo thinkpad x1 laptop with an intel core i5 6200u cpu using a clock speed of .40ghz and gb of ram.
the laptop was running eclipse neon in version .
.
as the ide.
background applications and system services have been disabled to the extent possible for the duration of our measurements.
we repeated each individual experiment i.e.
each of the settings in every scenario times to account for natural variability in build times.
b results figure depicts the total build overhead in seconds in boxplots introduced by performancehat in all settings and for all scenarios.
note that the y axis scale differs for all four settings.
this is to be expected as incremental builds are substantially faster than full builds.
similarly builds of the large and monolithic agilefant application take much longer than builds of the research project.
we observe that the overhead for incremental builds never exceeds seconds even in the worst case no cache cold ide large code base .
using caching alone the overhead can be reduced to .
seconds.
after the ide is warmed up which we expect to be the most common scenario in practical usage the build overhead of performancehat for incremental builds becomes completely negligible.
for full builds the overhead depends on the size of the project.
for the research prototype even in case of a full build the overhead is around seconds in the worst case.
for the monolithic agilefant application cache coldcache warmno cache coldno cache warmbuild time overhead agilefant full .
.
.
.
.
cache coldcache warmno cache coldno cache warmresearch project full .
.
.
.
.
.
cache coldcache warmno cache coldno cache warmstory single file .
.
.
.
.
cache coldcache warmno cache coldno cache warmcontroller single file fig.
total build time overhead of all settings and scenarios in seconds.
performancehat introduces a significant overhead of up to seconds in the worst case for full builds.
however by introducing caching this overhead can be reduced to less than seconds.
and generally a full build is usually not done more than once per start of the workstation.
we conclude that performancehat only introduces a substantial overhead on the total build time in the absolute worst case.
this overhead depends on the size of the code base on the availability of a cache on whether the ide is warm and most importantly on whether a full project build or an incremental file build is conducted.
in the most important use case incremental builds with cache and warm ide the introduced overhead is negligible and not noticeable to developers.
vii.
c onclusion we present a system performancehat that contextualizes the operational performance footprint of source code in the ide and raises awareness of the performance impact of changes.
our results from a controlled experiment with practitioners working on different software maintenance tasks indicate that developers using performancehat were significantly faster in detecting and finding the root cause of performance problems.
they additionally indicate that when working on non performance relevant tasks they did not perform significantly different illustrating that performancehat does not lead to unnecessary distractions.
when designing and researching programming experience we want to lower cognitive overhead in the software development process.
thus in the future we want to introduce smart thresholds that learn normal behavior from production and adjust the visibility of performance in code continuously.
further we want to tackle the potential problem of early optimization by introducing different usage profiles for our approach that show a different amount and level of detail e.g.
debugging vsdesign profile .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.