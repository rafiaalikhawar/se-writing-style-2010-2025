parameter free probabilistic api mining across github jaroslav fowkes charles sutton school of informatics university of edinburgh edinburgh eh8 9ab uk jfowkes csutton ed.ac.uk abstract existing api mining algorithms can be di cult to use as they require expensive parameter tuning and the returned set of api calls can be large highly redundant and di cult to understand.
to address this we present pam probabilistic api miner a near parameter free probabilistic algorithm for mining the most interesting api call patterns.
we show that pam signi cantly outperforms both mapo and upminer achieving test set precision at retrieving relevant api call sequences from github.
moreover we focus on libraries for which the developers have explicitly provided code examples yielding over loc of hand written api example code from the client projects in the data set.
this evaluation suggests that the hand written examples actually have limited coverage of real api usages.
ccs concepts software and its engineering !documentation keywords api mining sequential pattern mining .
introduction learning the application programming interface api of an unfamiliar library or software framework can be a signi cant obstacle for developers .
this is only exacerbated by the fact that api documentation can often be incomplete or ambiguous .
fortunately an opportunity to address this problem has arisen out of the simultaneous growth in the amount of source code that is available online and the growth of large scale data mining and machine learning methods.
this con uence has enabled the development of api mining methods which aim to automatically extract a set ofapi patterns which are lists of api methods that are usually used together and that together characterize how an api is used.
despite a number of interesting proposed tools including well known ones such as mapo and upminer so far api mining tools have not yet gained wide spread adoption in development environments such as eclipse and visual studio.
we suggest that the fundamental reason for this is that the quality of the extracted patterns is not yet high enough the patterns returned by current methods are numerous and highly redundant section .
for example figure shows the top ten api patterns extracted by two state of the art methods.
to a large extent the patterns from both methodsare variations on a theme repeating di erent variations of ways to repeat the same few api methods.
the fundamental reason for this we argue is that current api mining methods are built on statistically shaky ground.
speci cally api mining algorithms largely employ frequent sequence mining which is a family of techniques from the data mining literature that takes as input a database of sequences and from those attempts to identify a set of patterns that frequently occur as subsequences.
in the context of api mining each sequence in the database is the list of api methods called by some client method and the subsequence patterns that are mined are candidates for api patterns.
frequent sequence mining methods are very good at their intended purpose which is to e ciently enumerate subsequences that occur frequently.
but they are not suitable for pattern mining all by themselves for a simple reason frequent patterns are often uninteresting patterns.
this problem is well known in the data mining literature chap.
and the technical reason for it is easy to understand.
events that are individually frequent but unrelated will also tend to frequently occur in a sequence simply by chance.
imagine running a frequent sequence mining algorithm on the sequence of events that occur in the day of a typical academic.
perhaps the most frequent individual events would be sendemail anddrinkcoffee .
then a frequent sequence miner may well return the pattern sendemail drinkcoffee even if the two actions have no direct statistical relationship because frequent sequence miners do not consider any notion of statistical correlation or independence among items in the sequence.
within the api mining literature methods like mapo and upminer apply clustering methods precisely to reduce the number of redundant api patterns that would be returned from the frequent sequence miner however as we show in section even with this step substantial redundancy remains.
we address this problem by developing new a mining algorithm that returns api patterns that not only occur often but also that occur in a sequence more often than expected by chance that is the most interesting sequences chap.
.
in order to quantify what most interesting means we employ a powerful set of techniques from statistics and machine learning called probabilistic modelling.
that is we design a probability distribution over the list of api methods called by client methods and we evaluate a proposed api pattern by whether it improves the quality of the model.
the quality of a probabilistic model can be measured simply by whether it assigns high probability to a training set.
while numerous sequential probabilistic models of source 1arxiv .05558v2 nov 2016code have appeared in the literature these all rely onn gram language models.
but an n gram in this context is a contiguous sequence of method calls whereas api patterns contain gaps that is the methods in an api pattern do not always occur contiguously in a client method but instead can have other method calls interspersed between them.
for example consider the methods in figure which show real world usage examples of a four line call sequence to set up a twitter client.
the rst four lines of the last method shows the basic call sequence but the other two methods have additional calls interspersed between them e.g.the rst method also turns on ssl support.
although frequent sequence mining algorithms handle gaps in sequences automatically previous probabilistic models of code do not so we need to introduce a new probabilistic model.
to address this unsatisfactory state of a airs we propose pam probabilistic api miner a near parameter free probabilistic algorithm for mining the most interesting api patterns.
pam makes use of a novel probabilistic model of sequences based on generating a sequence by interleaving a group of subsequences.
the list of component subsequences are then the mined api patterns.
this is a fully probabilistic formulation of the frequent sequence mining problem is able to correctly represent both gaps in sequences and unlike api mining approaches based on frequent sequence mining largely avoids returning sequences of items that are individually frequent but uncorrelated.
furthermore another drawback of current methods for api mining is that they have multiple parameters that are very hard to tune in practice rendering them di cult to use without expensive pre training.
first frequent sequence mining methods depend on a user speci ed dataset speci c minimum frequency threshold .
this threshold is extremely di cult to tune in practice as it is prone to exponential blow up setting the threshold too low leads to billions of patterns.
conversely setting it too high leads to no patterns at all.
api mining algorithms also tend to employ hierarchical clustering techniques in an attempt to reduce the inherent redundancy of frequent sequence mining which tends to produce highly repetitive patterns .
choosing the number of clusters cuto is considered somewhat of a black art and can only be reliably inferred through computationally expensive training procedures on a validation set .
pam on the other hand is near parameter free in the sense that our two user speci ed parameters are independent of the dataset have simple intuitive interpretations and sensible default values.
to showcase pam we apply it to mining api patterns for of the most popular java libraries on the github hosting service.
we collect all usages of these apis on github yielding a data set of client methods from client projects totalling over million lines of code loc .
we focus on libraries that contain a speci c examples subdirectory which allows us to automatically collect api usage examples that the libraries developers thought were most salient.
this yields a corpus of java les comprising loc solely of hand written api usage examples.
these two sources of information allow us to perform extensive evaluation of pam compared to mapo and upminer.
we show that pam signi cantly outperforms both mapo and upminer achieving precision on our test set.
moreover the set patterns that pam returns is dramatically less redundant than those from mapo or upminer.
to the best of our knowledge we are the rst to mine api calls across the entirety of github from a data set of client methods totalling over million loc.
we examine whether the api patterns mined by pam could be used to augment api examples that are provided with popular projects.
this is a new method of evaluating api mining algorithms.
we show that there is substantial overlap between mined patterns and developer written examples providing evidence that the mined patterns are meaningful but that even though our corpus averages example loc per project the mined patterns describe many new patterns that appear in practice but are not covered by the examples.
.
related work the rst algorithm for mining api usage patterns from source code was mapo proposed by xie and pei and extended by zhong et al.
.
mapo mines method call sequences that call a desired api from code snippets retrieved by code search engines.
sequences of called api methods are rst extracted from the code snippets and then clustered according to a distance metric computed as an average of the similarity of method names class names and the called api methods themselves.
for each cluster mapo mines the most frequent api calls using spam and feeds them to an api usage recommender that ranks them based on their similarity with the developer s code context.
up miner extends mapo in an attempt to further reduce the redundancy of the mined api call sequences.
this is achieved through three principal approaches using the bide closed frequent sequence miner that returns only the frequent sequences that have no subsequences with the same frequency using a clustering distance metric based on the set of all api call sequence n grams and an additional clustering step on the mined call sequences from each cluster.
unlike mapo the api call sequences are presented as probabilistic graphs ranked according to their frequency.
acharya et al.
extract api usage scenarios among multiple apis as partial orders.
frequent api calls are rst mined from inter procedural program traces api call ordering rules are then extracted from the frequent calls and presented as partial order graphs.
buse and weimer generate api usage documentation by extracting and synthesizing code examples from api method call graphs annotated with control ow information.
their approach rst extracts method control ow paths from les that use a desired api.
path predicates are then computed for each statement relevant to a static instantiation of a desired api type and assembled along with the statements into api usage graphs.
the api usage graphs are then clustered based on their statement ordering and type information and an abstract usage graph is extracted from each cluster by merging its concrete api usages.
finally each abstract api usage graph is transformed into a code fragment by extracting the most representative abstract statement ordering from the graph and assigning each abstract statement a concrete name according to heuristics.
muse uses static slicing to extract and rank code examples that show how to use a speci c method.
for each relevant method invocation muse extracts an intra procedural backward slice representing a raw code example.
the raw 2code examples are then clustered using type clone detection and the resulting groups of clones ranked according to their popularity.
a representative code example from each group of clones is selected based on its readability and ease of reuse and each example is annotated with inline descriptions of method parameters mined from the javadoc.
other api mining papers include uddin et al.
who detect api usage patterns in terms of their time of introduction into client programs and li and zhou who automatically extract implicit programming rules in large software code.
a related approach that has been well studied in the literature is api code search where text matching is used to nd api usage examples from a large often online corpus.
sniff nds abstract code examples relevant to a natural language query expressing a desired task.
sniff annotates publicly available source code with api documentation and the annotated code is then indexed for searching.
in response to a query matching code examples are retrieved and concise examples are extracted via a syntax aware intersection clustered and ranked based on their frequency of occurrence.
similarly keivanloo et al.
nd abstract code examples by combining textual similarity and clone detection techniques ranking the returned examples according to their similarity to the query and the completeness and popularity of their concrete usage patterns.
bajracharya et al.
nd concrete api usage examples by combining heuristics based on structural and textual aspects of the code based on the assumption that code containing similar api usages is also functionally similar.
strathcona is a code example recommendation tool that automatically generates queries from the developer s current code context.
the code examples relevant to the queries are identi ed using six heuristics that compare the structural context of the query against the structural context of the classes and methods within an example repository.
the resulting code examples are ranked according to their frequency in the nal set of top examples returned by each heuristic.
prompter takes a similar approach matching the generated context speci c queries to stack over ow discussions and automatically recommending the discussions most relevant to the developers task.
the api mining problem we consider section is specifically to return sequences of api methods that are used together.
other important but distinct data mining problems that are related to api usage include mining preconditions of api methods and mining code changes .
an important recent piece of infrastructure for large scale mining work on code is boa .
another interesting line of work is to mine existing code to measure the popularity of language constructs and apis our work considers a di erent mining problem that of discovering combinations of api methods that are used together rather than measuring the popularity of known language features.
sequential pattern mining is an extremely well studied problem with a huge number of published papers on the topic.
the problem was rst introduced by agrawal and srikant in the context of market basket analysis which led to a number of other algorithms for frequent sequence mining including gsp pre xspan spade and spam .
frequent sequence mining su ers from pattern explosion a huge number of highly redundant frequent sequences are retrieved if the given minimum support threshold is too low.
one way to address this is by mining frequent closedsequences i.e.
those that have no subsequences with the same frequency such as via the bide algorithm .
more recently there has been work on sequence mining that directly addresses the pattern explosion issue such as sqs search and gokrimp algorithm .
our proposed approach falls into this class of probabilistic sequential pattern mining algorithms and returns patterns that are of a quality that is comparable to if not better than both sqs and gokrimp see for details .
there have also been sequential probabilistic models of source code proposed in the software engineering literature.
hindle et al.
develop an n gram language model for source code and apply it to a code completion task.
allamanis et al.
use ann gram language model to learn and suggest variable naming conventions.
raychev et al.
develop an n gram language model that synthesizes code completions for programs using apis.
.
extracting api calls first we describe the speci c api mining problem that we consider in this paper.
for every client project that uses a given api we extract the sequence of api calls used by each method in the project.
the problem of mining api calls is then to infer from these sequences of api calls those subsequences that represent typical usage scenarios for the api.
these could then be either supplied in example documentation to client developers or suggested in real time as developers type.
for the purposes of this paper we use a best e ort approach to extract api call sequences directly from source code les.
following mapo we parse each java source le of interest using the eclipse jdt parser and extract method calls to api methods from a speci ed library using a depth rst traversal of the ast.
for simplicity unlike mapo we do not attempt to enumerate all possible branches of conditional statements.
for example for the code snippet if m1 m2 else m3 our method returns the call sequence m1 m2 m3 whereas mapo would return the call sequences m1 m2 and m1 m3 .
if this example were indeed a common api usage pattern we would argue that returning m1 m2 m3 is better in principle because a subsequence like m1 m2 would provide only incomplete information about what the developer should write next.
furthermore unlike mapo we only consider method invocations and class instance creations and approximately resolve their fully quali ed names from the le import statements.
for simplicity superclass methods and their return types super method constructor invocations and class cast expressions are not considered.
we use an approach similar to the original mapo paper to approximately resolve fully quali ed method names.
we keep track of eld and local variable declarations so that we can resolve the fully quali ed name of a method call on the eld variable.
we also keep track of import statements so that we can resolve fully quali ed names of classes that are explicitly imported as well as those imported using a wildcard by scanning for speci c imports from the wildcarded package in the corpus in a pre processing step.
additionally in the pre processing step we nd the return types of locally declared methods so that we are able to subsequently resolve any calls on them.
finally we lter out any method names that cannot be fully resolved.
each api call sequence is then the sequence of fully quali ed api method names that are 3private finchtwitterfactory context context mcontext context installhttpresponsecache configurationbuilder configurationbuilder new configurationbuilder configurationbuilder.setoauthconsumerkey consumerkey.consumer key configurationbuilder.setoauthconsumersecret consumerkey.consumer secret configurationbuilder.setusessl true configuration configuration configurationbuilder.build mtwitter new twitterfactory configuration .getinstance public twitter gettwitterinstance configurationbuilder cb new configurationbuilder cb.setoauthconsumerkey keys.consumerkey cb.setoauthconsumersecret keys.consumersecret cb.setoauthaccesstoken msettings.getstring accesstoken null cb.setoauthaccesstokensecret msettings.getstring accesssecret null twitterfactory tf new twitterfactory cb.build return tf.getinstance private void startoauth configurationbuilder configurationbuilder new configurationbuilder configurationbuilder.setoauthconsumerkey const.consumer key configurationbuilder.setoauthconsumersecret const.consumer secret twitter new twitterfactory configurationbuilder.build .getinstance try requesttoken twitter.getoauthrequesttoken const.callback url toast.maketext this please authorize this app!
toast.length long .show this.startactivity new intent intent.action view uri.parse requesttoken.getauthenticationurl force login true catch twitterexception e e.printstacktrace figure three real world usage examples of a twitter4j api pattern that sets up a twitter client with oauth authorization.
called by a method in the source le.
for example consider the client methods in figure that all share the common twitter4j api call sequence configurationbuilder.
init configurationbuilder.setoauthconsumerkey configurationbuilder.setoauthconsumersecret configurationbuilder.build twitterfactory.
init twitterfactory.getinstance this is the minimum api call sequence required to set up oauth authorization for a twitter client.
all the methods in figure have added extra api calls for optional functionality e.g.ssl encryption but all contain the minimal api call sequence as a subsequence.
there are of course limitations to this approximation as noted in the original mapo paper .
in particular it is not possible to resolve external nested method calls i.e.
in the call method1 .method2 we cannot resolve method2 unless method1 is declared locally .
however for the purposes of this paper we are primarily interested in assessing the performance of pam.
moreover it is important to note that pam is exible and supports any api call extractor that returns sequences of calls making it applicable to dynamically inferred call sequences as well as other programming languages.
while we mine possibly incomplete api calls that are inferred statically from .java les in this paper one can in principle extract fully resolved static or dynamic api call sequences using the bcel bytecode library .
the reason we did not perform dynamic call sequence extraction is that the idiosyncratic build process of most java projectswould have made compiling all 967open source java projects that used our chosen libraries in our dataset see table prohibitive.
finally note that any api call extractor that only extracts sequences of calls cannot handle conditional statements properly as it is trying to approximate a graph with a sequence.
.
mining api call patterns in this section we will describe our novel probabilistic model for api mining.
our model is a joint probability distribution over the list of api calls in a client method which we observe in the data and the underlying api patterns that the programmer intended to use which we never observe directly.
the model de nes this probability assuming that the set of all possible true api patterns is known.
then learning involves working backward given the client methods that were observed what set of true api patterns might have generated them?
speci cally we measure the quality of a proposed set of api patterns by supposing those were the true patterns and measure the probability that the model assigns to all client methods in the database.
we search for the set of api patterns that maximizes this probability speci cally we perform this search under the framework of a celebrated algorithm from statistics called expectationmaximization em which has seen an enormous number of applications.
we use a particular variant called structural em as this deals with the speci c setting of learning via search through a large combinatorial space in our case the space of all possible sets of api patterns.
4in the next sections we give a high level overview of each of the aspects of the model.
for ease of exposition we do not describe some of the more theoretical aspects for those we refer the reader to our paper that describes a similar model and algorithm for general sequential pattern mining.
.
probabilistic model in this section we describe the probabilistic model that pam is based on.
the model is a probability distribution that based on a set of api patterns de nes a distribution over all possible api patterns present in client code.
when a probabilistic model becomes more complex than one of the well known standard families then it is often easiest to explain by describing an algorithm to sample from it.
this is how we will proceed in this section.
the model has two di erent types of parameters a set of api patterns and a set of probabilities.
the api patterns are straightforward each api pattern is a sequence sa a1 an of method names from the api.
we allow patterns to occur more than once in the same client method.
therefore for each api pattern sa the model also includes a probability distribution over the integers mwhich represents how likely a client method is to include the pattern szero times one time etc.
we de ne ito be the set of all api patterns sain the model.
we assume that ialso contains singleton sequences m for every method min the api although an api pattern with only one method call is not very useful so we never return such patterns to the user we will see in section .
that these are a technical device that is necessary for the inference procedure.
now we present an algorithm that will draw samples from our model which we call the generative algorithm .
the generative algorithm says hypothetically speaking if our model were correct how would each client method be generated assuming that the api patterns and probabilities are known?
we emphasize that the generative algorithm is simply an explanatory tool that helps in understanding our approach and is never used while performing the api mining.
the algorithm has two main phases first from the set of all interesting api patterns we sample which ones will appear in the client method that we are about to generate and how many times they will be used which yields a multiset that we calls.
then we randomly sample a way to interleave the sampled api patterns and this results in a hypothetical client method.
more formally .for each unique api pattern sin the set of interesting api patternsi decide independently the number of times sshould be included in the client api sequence x i.e.
draw the count csfrom a suitable probability distribution over the integers.
.setsto be the multiset with counts csof all the api patternssselected for inclusion in x that is s fs cs 1g assis a multiset a single api pattern can occur more than once in s. .setpto be the set of all possible sequences that can be generated by interleaving together the api patterns in the multisets i.e.
p fx spartition of x s x8s2sg see the discussion below for an illustrative example .
.
samplexuniformly at random from p. this algorithm de nes a probability distribution over client methods which we can sample from simply by executingit.
first let us clarify the meaning of the set pthrough an example.
to interleave two sequences s1ands2 we mean the placing of items from s1into the gaps between items in s2.
for example if s1 m1 m2 ands2 m3 m4 then the set of all ways to interleave s1ands2is p f m3 m4 m1 m2 m3 m1 m4 m2 m3 m1 m2 m4 m1 m3 m4 m2 m1 m3 m2 m4 m1 m2 m3 m4 g it is possible to uniformly sample from pe ciently by merging in subsequences one at time but we omit the details as it is unnecessary in practice.
at this point the generative algorithm may seem contrived.
certainly we hope that developers do not write code in a manner that is anything like this.
to assuage the reader s conscience we point out that popular methods such as the n gram language model and latent dirichlet allocation which have been widely applied both to natural language and programming language text also have generative algorithms and those algorithms are similarly contrived.
the reason that these models are useful anyway is that we are primarily interested not in the forward generative direction in which we use api patterns to generate client methods but in the reverse direction in which we run the generative algorithm backward to use client methods to infer api patterns.
as we will see in a moment the backward version of this model is much more intuitive and natural.
we have so far de ned a probability distribution implicitly using a generative algorithm however we can now de ne it explicitly by giving a formula for the probability of a client methodxunder our model.
to do this we need to introduce notation to handle the fact that our model allows the same api pattern to occur multiple times in a single client method.
we will consider each occurrence of an api pattern sin a client api sequence xseparately let s denote the n th occurrence of sinxi.e.
by the notation m1 m2 we mean the 3rd time the api pattern m1 m2 occurs in a client sequence .
for example x m1 m2 m3 m1 m2 contains the api patterns m1 m2 and m1 m2 i.e.
the rst and second occurrences of m1 m2 .
in light of this p m1 m2 2x is naturally de ned as the probability of seeing m1 m2 for the 3rd time given that we ve seen it for the 2nd time i.e.
p m1 m2 2xj m1 m2 x p m1 m2 2xj m1 m2 m1 m2 2x since we are allowing gaps in api patterns and so m1 m2 2x m1 m2 m1 m2 2x .
formally the associated probability s for then th occurrence s is simply the conditional probability of seeing s in a client sequence xgiven the previous occurrence s i.e.
s p s p s .
we also introduce the binary variable zs 1fcs ngto indicate whether s is included in xor not.
for clarity of exposition we will drop the explicit occurrence superscript in the sequel.
now we can give an explicit formula for the probability of a client method xunder our model.
given a set of informative api patternsi letz denote the vectors of zs sfor all api patterns s2i.
assuming z are fully determined the generative model implies that the probability of generating a client api sequence xis p x zj jpjq s2i zs s s zsifx2p otherwise 5calculatingjpjmay seem problematic however it turns out to be rather straightforward.
pick an arbitrary ordering s jsjfor the selected api pattern s2sand observe that in our iterative sampling algorithm when merging sainto sb we havejsbj points to splice jsajelements into that isjsbj 1multichoosejsaj denoted jsbj jsaj jsbj jsaj jsbj .
to see how to compute this consider a sequence of k jsaj stars andn jsbjbars j .
the number of ways that these two sequences can be spliced together is equal to the number of ways to place the kstars inton 1bins delimited by the bars this is by de nition n multichoose k .
this can be equivalently viewed as the number of ways to arrange thenbars amongst the kstars andnbars which is clearly n kchoosen .
applying this formula iteratively we obtain jpj jsjy s ps t 1jstj jssj jsjy s ps t 1jstj !
jssj!
ps t 1jstj !
.
inference inference in a probabilistic model is the task of running a generative procedure backwards.
in our model this is the task of given a client method x infer the vector z which indicates which api patterns were used in the method.
while the generative algorithm provides a way to sample from the joint distribution p x zj inference is focused on the conditional distribution p zjx essentially the inference procedure amounts to computing a partition of the api calls in xaccording to the mined api patterns.
at this point the reader may well be wondering why we need to do inference at all?
for a speci c client method x can t we just look up the api patterns that xsubsumes?
the answer is that the mined api patterns overlap and this is what we want because we would like to be able to learn more general and more speci c versions of the same pattern.
consider the case where we have to choose between returning a more general pattern such as builder.
init builder.setcommonproperty builder.build and a more speci c one such as builder.
init builder.setcommonproperty builder.setrarevalue builder.build we want these two patterns to compete with each other to explain each client method and the fact that we use a partitioning approach means that each api call in a client can be explained by at most one api pattern.
at the end of the day the e ect is that the more speci c pattern will only survive into the nal list of mined patterns if it manages to be used to explain enough client methods.
in other words because we have probabilities associated with each of the two api patterns we are able to choose the more interesting of the two in a well de ned and rigorous statistical way.
more formally the inference procedure assumes that the vector of probabilities is known we learn it in the next section .
to infer the best zfor a client api sequence x the natural probabilistic way of asking this question is using the conditional distribution p zjx speci cally we search for the vector zthat maximizes logp zjx .
sadly it can be shown that this problem is np hard in general.
happily this problem can be approximately solved using a simple greedyalgorithm cf.algorithm in and we nd that the greedy algorithm works well in practice.
the greedy algorithm repeatedly chooses an api pattern sthat maximizes the improvement in log probability divided by the number of methods in sthat have not yet been explained.
in order to minimize cpu time we cache the api patterns and coverings for each api client sequence as needed.
now we can see why we have insisted on including singleton sequences for api patterns into ieven though we would never return them to a user.
including the singleton sequences and allowing them to be repeated arbitrarily many times ensures that every client method has at least one valid partitioning.
.
learning given a set of interesting api patterns i consider now the case where both variables z in the model are unknown.
there is an obvious chicken and egg problem here the most interesting api patterns sare determined by maximizing logp zjx forzs which means we we need to know s. but the probability sof an api pattern sdepends on how often that pattern is used in a client api sequence x. to get round this we can use the expectation maximization em algorithm which is an algorithm for estimating parameters in a model that has unobserved variables.
the em algorithm is a clever way to get around the chicken and egg problem by iteratively solving for the best zgiven the current value of then solving for the best value of .
of course em needs an initial guess for the unobserved variables and a good rst guess is simply the relative support i.e.
relative frequency of occurrence of each api pattern in i in fact this guess is correct if all the api patterns in iare independent of each other .
for the mathematical details we refer the interested reader to algorithm in .
.
inferring new api patterns now that we have shown how to learn the parameters of our probabilistic model the astute reader may well note that we still haven t got anywhere as we have no way of inferring which sequences to include in the set of interesting api patternsi.
however we can once again turn to the statistical community for a solution in the form of the structural em algorithm .
as the name suggests this is a variant of the em algorithm that lets us grow the set of interesting api patternsi.
in particular we can add a candidate api patterns0to the set of interesting api patterns iif so improves the value of logp zjx averaged across all client api sequences x. to get an estimate of maximum bene t to including candidates0 we must carefully choose an initial value of s0 that is not too low to avoid getting stuck in a local optimum.
to infer a good s0 we force the candidate s0to explain all api client sequences it supports by initializing s0 tand update s0with the probability corresponding to its actual usage once we have inferred all the zs.
as before structural em also needs an initial guess for the set of interesting api calls iand associated probabilities and we can simply initialize iwith all the api methods in the dataset and with their relative supports.
once again we omit the mathematical details of the algorithm but refer the interested reader to algorithm in .
.
candidate generation however we are not quite done yet.
the structural em 6algorithm requires a method to generate new candidate sequencess0that are to be considered for inclusion in the set of interesting api patterns i. one possibility would be to use a standard frequent sequence mining algorithm to recursively suggest larger api patterns starting from all the api methods in the dataset however preliminary experiments found this was not the most e cient method.
for this reason we take a somewhat di erent approach and recursively combine the interesting api patterns in iwith the highest support rst.
in this way our candidate generation algorithm is more likely to propose viable candidate api patterns earlier and we nd that this heuristic works well in practice.
although the algorithm is straightforward it adds little to the exposition and we omit the details here and refer the interested reader to algorithm in .
.
mining interesting api patterns finally we can present our complete probabilistic api mining pam algorithm in algorithm .
as all operations algorithm probabilistic api miner pam input client method api call sequences x x m initializeiwith singleton api patterns and with their supports while not converged do add api patterns to i using structural em algorithm in optimize parameters for i using em algorithm in end while remove all singleton api patterns from i returni on api client sequences in our algorithm are independent and so trivially parallelizable we perform the eandm steps in both the em and structural em algorithms in parallel.
we can rank the retrieved api patterns according to their interestingness that is how likely they are under the probabilistic model and therefore we rank the api patterns s2i according to their probabilities sunder the model.
an important property of formulating pam as a pattern covering problem on each api client sequence is that it strongly favours adding only api patterns of associated methods i.e.
methods that largely co occur in the code.
.
experiments in this section we perform a comprehensive evaluation of pam across github comparing and contrasting it against mapo and upminer.
in particular we aim to answer the following three research questions.
rq1 are the api patterns mined by pam more prevalent?
this research question evaluates the quality of the api call sequences mined by pam as we would expect a set of more representative api call sequences to be more prevalent in a held out corpus of code.
by performing a random split of a suitable api call dataset we can see if sequences mined from one half of the dataset are prevalent on the other half and thus if they are representative.
note that performing such a test train split is standard practice in the evaluation of machine learning algorithms.rq2 are the api patterns mined by pam more diverse?
we would also expect a more representative set of api patterns to have lower redundancy as a list in which every pattern uses the same few methods will be both redundant and non diverse.
however a redundancy of zero is not necessarily desirable as mentioned previously section .
a good list of patterns may contain both more general and more speci c versions of the same pattern.
that said a highly redundant list is clearly problematic.
rq3 could the api patterns mined by pam supplement existing developer written api examples?
finally we investigate if the mined api patterns can be useful in practice.
to do so we look at libraries and frameworks that explicitly contain directories of api examples provided by the library s developers.
this allows us to measure whether api call sequences present in api example directories are returned by pam and also vice versa i.e.
whether the handbuilt example directories can be improved by including api patterns mined from client code by pam.
we will show both that a there is substantial overlap between the mined patterns and the developer written examples indicating that pam does indeed nd patterns that the project developers believe are meaningful but also b pam identi es a large number of patterns that do not occur in examples which could serve as a useful supplement.
evaluation metrics a good measure of the quality of mined api call sequences is to see what proportion are contained in a set of relevant gold standard sequences a measure we term sequence precision .
this allows us to measure the degree to which the mined patterns represent relevant api call sequences.
similarly we also de ne sequence recall as the proportion of relevant gold standard sequences that contain a mined call sequence.
this allows us to measure the degree to which the api miner is able to retrieve relevant api call sequences.
in other words sequence precision measures the percentage of mined sequences that are somewhere used and sequence recall measures the degree to which the mined sequences cover the usages in the gold standard data set.
we present these metrics in a precision recall curve as is standard practice in the information retrieval literature .
each point on the precision recall curve corresponds to a di erent point in the ranked list of api patterns returned by each method and indicates what the sequence precision and sequence recall would be if we forced the method to stop returning patterns at that point in the list.
in a precision recall curve being up and to the right is better as it means that the system returns more accurate results for any xed recall value.
for mapo and upminer we rank the list of api patterns by support whereas for pam we rank the patterns by their probability under pam s statistical model.
as for redundancy we measure how redundant the set of mined sequences is by calculating the average over each api pattern of the number of other larger api patterns that contain it we call this no.
containing sequences .
dataset in order to asses the performance of pam and perform a thorough comparison with mapo and upminer we assemble a data set of target libraries and frameworks from the github java corpus .
we focus on those projects that contain an examples directory of code examples so that we can compare mined patterns to those written by the library s developers.
we include in our data set all java projects on github that are su ciently popular imported by a 7table example dataset extracted from the github java corpus.
each row is a separate library or framework for which we mine a set of api patterns.
each client le set contains all source les that import a class belonging to the respective package or one of its subpackages.
each example le set contains all source les that are present in the project s example directory.
note that both le sets exclude duplicate les.
project package name client loc example loc description andengine org.andengine android 2d opengl game engine apache camel org.apache.camel enterprise application integration framework cloud9 edu.umd.cloud9 cloud based ide drools org.drools business rules management system apache hadoop org.apache.hadoop map reduce framework hornetq org.hornetq embeddable asynchronous messaging system apache mahout org.apache.mahout scalable machine learning environment neo4j org.neo4j graph database netty io.netty network application framework resteasy org.jboss.resteasy restful application framework restlet framework org.restlet restful web api framework spring data mongodb org.springframework .data.mongodb16 spring framework mongodb integration spring data neo4j org.springframework .data.neo4j6 spring framework neo4j integration twitter4j twitter4j twitter api project wonder com.webobjects webobjects frameworks weld org.jboss.weld contexts and dependency injection api apache wicket org.apache.wicket web application framework total su cient number of other projects and that contain a su ciently large examples directory.
speci cally we rst nd all java projects in the corpus that have an example directory i.e.
matching example example containing more than 10k loc.
from these projects we then select those that are in the top projects in the corpus ranked according to popularity.
popularity in the github corpus is calculated as the sum of the number of project forks and watchers where each is separately normalized into a z score.
from these top projects we determine which of these are called from or more methods belonging to other projects in the corpus leaving us with the projects in table .
we call this set of projects and associated client code the example dataset to emphasize the fact that we focus on libraries and frameworks that include examples.
each of these projects is a library or framework which we will call a target project for which we wish to extract api patterns.
for each target project we perform api mining separately and all results are reported as an average over the target projects.
to extract a set of client methods for each target project we search the entire github java corpus for all source les that import a class belonging to the respective package or one of its subpackages and this set of les excluding duplicates formed the client le set .
extracting for each project all source les in the aforementioned example directory excluding duplicates formed the example le set .
statistics on both le sets are given in table .
experimental setup as public implementations were unavailable we implemented mapo and upminer based on the descriptions in their respective papers.
we used a clustering threshold of for mapo as this gave consistent performance and for upminer as this matched the natural cuto in the dendrogram.
the minimum support thresholds for both algorithms were set as low as was practically feasible for each run.
we ran pam for 000iterations with a priority queue size limit of candidates.
rq1 are the api call sequences mined by pam more prevalent?
as previously mentioned in an attempt to .
.
.
.
.
.
.
.
.
average sequence recall0.
.
.
.
.
.
.
.
.8interpolated average sequence precisionpam mapo upminerfigure average test set precision against recall for pam mapo and upminer on the example dataset using the top kmined train set sequences as a threshold.
answer this question we divide our dataset of api calls in half and see if sequences mined from one half of the dataset are prevalent in the other half.
speci cally we randomly divide the client le set cf.table into two roughly equal train and test sets.
this enables us to mine api call subsequences from the training set and evaluate them using the sequence precision and recall metrics against the api call sequences in the test set.
figure shows the sequence precision against recall averaged across all projects in the dataset.
it is evident that pam has signi cantly higher precision and recall than both mapo and upminer reaching a precision of .
mapo performs especially poorly as its precision degrades signi cantly as the recall increases.
we can therefore say with certainty that the api call sequences mined by pam are more prevalent.
note that while the best recall that pam achieves is this is actually close to the theoretical maximum recall on the test set.
this can be approximated by the proportion of test set sequences that contain training set sequences which is around .
rq2 are the api call sequences mined by pam more top n02468101214average no.
containing subsequencespam mapo upminerfigure average no.
containing sequences for pam mapo and upminer on the example dataset using the top kmined sequences.
diverse?
we now turn our attention to the complete dataset and mine call sequences from the entire client le set for each project cf.table .
we can then use the no.
containing sequences metric to determine how redundant the set of mined call sequences is.
figure shows the average no.
of sequences containing other sequences in the set of top k mined sequences as kvaries.
one can see that pam has consistently the lowest gure around showing that it is the least redundant and therefore most diverse.
one of the key motivations of our method is that the list of patterns returned by sequence mining methods is redundant.
this gure shows that even after the extra steps that mapo and upminer take to reduce the redundancy of the raw output of frequent sequence mining the patterns returned by pam are still less redundant.
rq3 could the api patterns mined by pam supplement existing developer written api examples?
we measure whether the mined api patterns correspond to handwritten examples in the dataset.
we therefore mine for each project call sequences from the client le set and evaluate them against call sequences in the example le set.
figure shows the sequence precision against recall averaged across all projects.
again pam has evidently better precision and recall than mapo and upminer.
the best recall achieved by pam is signi cantly better than the other methods and for any xed recall value pam has higher precision than the other methods.
this suggests that the api patterns returned by pam could better supplement developer written examples than those returned by mapo or upminer.
in an absolute sense the level of agreement between pam and the hand written examples although substantial might not seem especially high.
this raises an interesting question does this level of disagreement occur because the pam patterns are not representative of the client code they were mined from orbecause the hand written examples themselves are not fully representative of the client code?
although previous work has explored what it means for a single api example to be useful there seems to be much less work about what it means for a set of examples to be useful and how well example directories in popular projects re ect all of the actual common uses of an api.
2this gure excludes hadoop as we had problems with our implementation of mapo and upminer running out of memory hadoop has around million client loc .
while pam had no issues we excluded it for a fair comparison.
.
.
.
.
.
.
average sequence recall0.
.
.
.
.
.5interpolated average sequence precisionpam mapo upminerfigure average example set interpolated precision against recall for pam mapo and upminer on the example dataset2 using the top kmined sequences as a threshold.
we can however make an initial assessment of this question by going back to the held out test set of client code that we used in rq1.
we can measure how well the client test set and the handwritten examples agree by measuring sequence precision and recall if we take the handwritten examples as if they were api patterns and the client test set as the gold standard.
when we do this we nd that the handwritten examples have a recall of meaning that three quarters of client api method sequences are not contained within any of the handwritten examples.
turning to precision the handwritten examples have a precision of meaning that two thirds of api sequences from the example code are not used by any client method where used by means fully contained by .
this is signi cantly lower than the precision between the training set of client methods and the test set suggesting that the training set is more representative of the test set than the handwritten examples are.
although this might be seen as a suggestive result we caution that this has an important threat to validity handwritten examples may include sca olding code that is unnecessary in client methods.
for this reason we advise caution about drawing strong conclusions from the precision of handwritten examples but we note that this threat does not apply to the recall.
these results suggest that even in very well documented projects with extensive sets of examples the api usage examples written by developers are still incomplete.
while it may not seem surprising that developer written example directories would be incomplete recall that we speci cally chose our data set to consist only of popular libraries with extensive handwritten examples indeed our data set averages lines of example code per target api .
it is striking that even with projects that are so extensively documented pam is still able to infer a list of coverage with substantially greater coverage of the api.
to gain further insight into this issue we randomly selected three projects from our dataset and looked at the top ve api patterns returned by pam that were not present in any example call sequence.
we found that the selected api patterns fell into the following three categories referred to an api method not in any of the examples referred to an api class not in any of the examples and referred to an api pattern that was not contained in any api example although its methods were present in the examples .
this provides some support for the hypothesis that the api 9twitterfactory.
init twitterfactory.
init twitterfactory.
init twitterfactory.getinstance twitterfactory.getinstance twitterfactory.getinstance status.getuser twitterfactory.getinstance twitterfactory.
init status.gettext twitter.setoauthconsumer twitterfactory.getinstance twitter.setoauthconsumer configurationbuilder.
init twitterfactory.
init twitter.setoauthaccesstoken configurationbuilder.build twitterfactory.getinstance twitter.setoauthconsumer status.getuser configurationbuilder.
init status.gettext twitterfactory.
init status.getuser status.gettext auth.accesstoken.gettoken configurationbuilder.
init auth.accesstoken.gettokensecret configurationbuilder.setoauthconsumerkey twitter.setoauthconsumer twitter.setoauthaccesstoken configurationbuilder.
init configurationbuilder.build configurationbuilder.build twitterfactory.
init twitterfactory.
init twitterfactory.
init twitterfactory.getinstance twitterfactory.getinstance configurationbuilder.
init twitter.setoauthaccesstoken configurationbuilder.build status.getid twitterfactory.
init configurationbuilder.
init status.getid twitterfactory.
init configurationbuilder.
init configurationbuilder.
init configurationbuilder.setoauthconsumerkey configurationbuilder.
init configurationbuilder.setoauthconsumerkey configurationbuilder.build twitterfactory.
init configurationbuilder.setoauthconsumersecret twitterfactory.getinstance configurationbuilder.build configurationbuilder.setoauthconsumerkey twitterfactory.
init configurationbuilder.build auth.accesstoken.
init twitterfactory.getinstance twitter.setoauthaccesstoken user.getid http.accesstoken.gettoken user.getid twitterfactory.
init http.accesstoken.gettokensecret twitterfactory.getinstance twitter.setoauthconsumer twitter.getoauthaccesstoken twitter.setoauthaccesstoken auth.accesstoken.gettoken auth.accesstoken.gettokensecret configurationbuilder.setoauthaccesstoken configurationbuilder.setoauthaccesstokensecret figure top twitter4j.
api patterns mined by mapo left upminer middle and pam right .
patterns document part of the api that are used in client code but for which the original developers have not chosen to write speci c examples.
overall these results suggest that the patterns returned by pam could serve as a useful supplement to code examples written by api developers.
indeed these results raise the question of whether in future work pam could be used to help detect novel and undocumented api usages and feed them back to library and framework maintainers.
qualitative evaluation to provide further support to rq3 whether the mined patterns from pam could be useful we qualitatively compare and contrast the top sequences returned by pam mapo and upminer on an example target api.
figure shows the top ten mined api patterns from twitter4j returned by pam mapo and upminer on theexample dataset.
one can clearly see that the api calls found by mapo are extremely repetitive in fact most of the top ten calls are just combinations of subsequences of the following pattern configurationbuilder.
init configurationbuilder.setoauthconsumerkey configurationbuilder.setoauthconsumersecret configurationbuilder.build twitterfactory.
init twitterfactory.getinstance which according to our manual inspection occurs commonly in client code but does not appear anywhere in the top ten patterns returned by mapo.
similarly the majority of the top ten upminer patterns are combinations of subsequences of the pattern twitterfactory.
init twitterfactory.getinstance twitter.setoauthconsumertwitter.setoauthaccesstoken despite the full version of this sequence appearing as the 10th pattern returned by upminer.
pam on the other hand retrieves both of these full patterns within the top ten.
one might think that the configurationbuilder pattern without oauth returned by pam is redundant however not all clients use oauth.
moreover the sequences returned by pam clearly display a more diverse selection of api methods the top ten pam sequences use unique api methods compared to only for both mapo and upminer.
.
conclusions we presented a parameter free probabilistic api mining algorithm that makes use of a novel probabilistic model to infers the most interesting api call patterns and demonstrated the e cacy of our approach on dataset of several hundred thousand api client les from github.
through our experiments we found suggestions that api calls are not well documented in example code and in future we would like to verify this through a large scale empirical study.
.