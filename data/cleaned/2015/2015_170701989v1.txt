cooperative kernels gpu multitasking for blocking algorithms extended version tyler sorensen imperial college london london uk t.sorensen15 imperial.ac.ukhugues evrard imperial college london london uk h.evrard imperial.ac.ukalastair f. donaldson imperial college london london uk alastair.donaldson imperial.ac.uk abstract there is growing interest in accelerating irregular data parallel algorithms on gpus.
these algorithms are typically blocking so they require fair scheduling.
but gpu programming models e.g.
opencl do not mandate fair scheduling and gpu schedulers are unfair in practice.
current approaches avoid this issue by exploiting scheduling quirks of today s gpus in a manner that does not allow the gpu to be shared with other workloads such as graphics rendering tasks .
we propose cooperative kernels an extension to the traditional gpu programming model geared towards writing blocking algorithms.
workgroups of a cooperative kernel arefairly scheduled and multitasking is supported via a small set of language extensions through which the kernel and scheduler cooperate.
we describe a prototype implementation of a cooperative kernel framework implemented in opencl .
and evaluate our approach by porting a set of blocking gpu applications to cooperative kernels and examining their performance under multitasking.
our prototype exploits no vendor specific hardware driver or compiler support thus our results provide a lower bound on the efficiency with which cooperative kernels can be implemented in practice.
ccs concepts software and its engineering multiprocessing multiprogramming multitasking semantics computing methodologies graphics processors keywords gpu cooperative multitasking irregular parallelism introduction the needs of irregular data parallel algorithms.
many interesting data parallel algorithms are irregular the amount of work to be processed is unknown ahead of time and may change dynamically in a workload dependent manner.
there is growing interest in accelerating such algorithms on gpus .
irregular algorithms usually require blocking synchronization between workgroups e.g.
many graph algorithms use a level by level strategy with a global barrier between levels work stealing algorithms require each workgroup to maintain a queue typically mutex protected to enable stealing by other workgroups.
to avoid starvation a blocking concurrent algorithm requires fairscheduling of workgroups.
for example if one workgroup copyright held by the owner author s .
publication rights licensed to association for computing machinery.
killkill forkgather time timeexecution time3 graphics request 2cu graphics terminatedhost cpu gpu compute units cu graphics graphicsfigure cooperative kernels can flexibly resize to let other tasks e.g.
graphics run concurrently holds a mutex an unfair scheduler may cause another workgroup to spin wait forever for the mutex to be released.
similarly an unfair scheduler can cause a workgroup to spin wait indefinitely at a global barrier so that other workgroups do not reach the barrier.
a degree of fairness occupancy bound execution.
the current gpu programming models opencl cuda and hsa specify almost no guarantees regarding scheduling of workgroups and current gpu schedulers are unfair in practice.
roughly speaking each workgroup executing a gpu kernel is mapped to a hardware compute unit .1the simplest way for a gpu driver to handle more workgroups being launched than there are compute units is via an occupancy bound execution model where once a workgroup has commenced execution on a compute unit it has become occupant the workgroup has exclusive access to the compute unit until it finishes execution.
experiments suggest that this model is widely employed by today s gpus .
the occupancy bound execution model does not guarantee fair scheduling between workgroups if all compute units are occupied then a not yet occupant workgroup will not be scheduled until some occupant workgroup completes execution.
yet the execution model does provide fair scheduling between occupant workgroups which are bound to separate compute units that operate in parallel.
current gpu implementations of blocking algorithms assume the occupancy bound execution model which they exploit by launching no more workgroups than there are available compute units .
resistance to occupancy bound execution.
despite its practical prevalence none of the current gpu programming models actually mandate occupancy bound execution.
further there are reasons why this model is undesirable.
first the execution model does not enable multitasking since a workgroup effectively owns a compute unit until the workgroup has completed execution.
the 1in practice depending on the kernel multiple workgroups might map to the same compute unit we ignore this in our current discussion.arxiv .01989v1 jul 2017gpu cannot be used meanwhile for other tasks e.g.
rendering .
second energy throttling is an important concern for battery powered devices .
in the future it will be desirable for a mobile gpu driver to power down some compute units suspending execution of associated occupant workgroups if the battery level is low.
our assessment informed by discussions with a number of industrial practitioners who have been involved in the opencl and or hsa standardisation efforts including is that gpu vendors will not commit to the occupancy bound execution model they currently implement for the above reasons yet will not guarantee fair scheduling using preemption.
this is due to the high runtime cost of preempting workgroups which requires managing thread local state e.g.
registers program location for all workgroup threads up to on nvidia gpus as well as shared memory the workgroup local cache up to kb on nvidia gpus .
vendors instead wish to retain the essence of the simple occupancy bound model supporting preemption only in key special cases.
for example preemption is supported by nvidia s pascal architecture but on a gtx titan x pascal we still observe starvation a global barrier executes successfully with workgroups but deadlocks with workgroups indicating unfair scheduling.
our proposal cooperative kernels.
to summarise blocking algorithms demand fair scheduling but for good reasons gpu vendors will not commit to the guarantees of the occupancy bound execution model.
we propose cooperative kernels an extension to the gpu programming model that aims to resolve this impasse.
a kernel that requires fair scheduling is identified as cooperative and written using two additional language primitives offer killand request fork placed by the programmer.
where the cooperative kernel could proceed with fewer workgroups a workgroup can execute offer kill offering to sacrifice itself to the scheduler.
this indicates that the workgroup would ideally continue executing but that the scheduler may preempt the workgroup the cooperative kernel must be prepared to deal with either scenario.
where the cooperative kernel could use additional resources a workgroup can execute request fork to indicate that the kernel is prepared to proceed with the existing set of workgroups but is able to benefit from one or more additional workgroups commencing execution directly after the request fork program point.
the use of request fork andoffer killcreates a contract between the scheduler and the cooperative kernel.
functionally the scheduler must guarantee that the workgroups executing a cooperative kernel are fairly scheduled while the cooperative kernel must be robust to workgroups leaving and joining the computation in response tooffer killandrequest fork.
non functionally a cooperative kernel must ensure that offer killis executed frequently enough such that the scheduler can accommodate soft real time constraints e.g.
allowing a smooth frame rate for graphics.
in return the scheduler should allow the cooperative kernel to utilise hardware resources where possible killing workgroups only when demanded by other tasks and forking additional workgroups when possible.
cooperative kernels allow for cooperative multitasking see sec.
used historically when preemption was not available or too costly.
our approach avoids the cost of arbitrary preemption as the state of a workgroup killed via offer killdoes not have to be saved.
previous cooperative multitasking systems have provided yield semantics where a processing unit would temporarily give up its hardware resource.
we deviate from this design as in the case of a global barrier adopting yield would force the cooperative kernel to block completely when a single workgroup yields stalling the kernel until the given workgroup resumes.
instead our offer killallows a kernel to make progress with a smaller number of workgroups with workgroups potentially joining again later via request fork.
figure illustrates sharing of gpu compute units between a cooperative kernel and a graphics task.
workgroups and of the cooperative kernel are killed at an offer killto make room for a graphics task.
the workgroups are subsequently restored to the cooperative kernel when workgroup calls request fork.
the gather time is the time between resources being requested and the application surrendering them via offer kill.
to satisfy soft real time constraints this time should be low our experimental study sec.
.
shows that in practice the gather time for our applications is acceptable for a range of graphics workloads.
the cooperative kernels model has several appealing properties by providing fair scheduling between workgroups cooperative kernels meet the needs of blocking algorithms including irregular data parallel algorithms.
the model has no impact on the development of regular noncooperative compute and graphics kernels.
the model is backwards compatible offer killandrequest fork may be ignored and a cooperative kernel will behave exactly as a regular kernel does on current gpus.
cooperative kernels can be implemented over the occupancy bound execution model provided by current gpus our prototype implementation uses no special hardware driver support.
if hardware support for preemption isavailable it can be leveraged to implement cooperative kernels efficiently and cooperative kernels can avoid unnecessary preemptions by allowing the programmer to communicate smart preemption points.
placing the primitives manually is straightforward for the representative set of gpu accelerated irregular algorithms we have ported so far.
our experiments show that the model can enable efficient multitasking of cooperative and non cooperative tasks.
in summary our main contributions are cooperative kernels an extended gpu programming model that supports the scheduling requirements of blocking algorithms sec.
a prototype implementation of cooperative kernels on top of opencl .
sec.
and experiments assessing the overhead and responsiveness of the cooperative kernels approach over a set of irregular algorithms sec.
including a best effort comparison with the efficiency afforded by hardware supported preemption available on nvidia gpus.
we begin by providing background on opencl via two motivating examples sec.
.
at the end we discuss related work sec.
and avenues for future work sec.
.
background and examples we outline the opencl programming model on which we base cooperative kernels sec.
.
and illustrate opencl and the scheduling requirements of irregular algorithms using two examples a work stealing queue and frontier based graph traversal sec.
.
.1kernel work stealing global task queues int queue id get group id while more work queues task t pop or steal queues queue id if t process task t queues queue id figure an excerpt of a work stealing algorithm in opencl .
opencl background an opencl program is divided into host anddevice components.
a host application runs on the cpu and launches one or more kernels that run on accelerator devices gpus in the context of this paper.
a kernel is written in opencl c based on c99.
all threads executing a kernel start at the same entry function with identical arguments.
a thread can call get global idto obtain a unique id to access distinct data or follow different control flow paths.
the threads of a kernel are divided into workgroups .
functions get local idandget group idreturn a thread s local id within its workgroup and the workgroup id.
the number of threads per workgroup and number of workgroups are obtained via get local size andget num groups .
execution of the threads in a workgroup can be synchronised via a workgroup barrier.
a global barrier synchronising all threads of a kernel is notprovided as a primitive.
memory spaces and memory model.
a kernel has access to four memory spaces.
shared virtual memory svm is accessible to all device threads and the host concurrently.
global memory is shared among all device threads.
each workgroup has a portion of local memory for fast intra workgroup communication.
every thread has a portion of very fast private memory for function local variables.
fine grained communication within a workgroup as well as inter workgroup communication and communication with the host while the kernel is running is enabled by a set of atomic data types and operations.
in particular fine grained host device communication is via atomic operations on svm.
execution model.
opencl and cuda specifically make no guarantees about fair scheduling between workgroups executing the same kernel.
hsa provides limited one way guarantees stating work group a can wait for values written by work group b without deadlock provided ... if a comes after b in work group flattened id order .
this is not sufficient to support blocking algorithms that use mutexes and inter workgroup barriers both of which require symmetric communication between threads.
.
motivating examples work stealing.
work stealing enables dynamic balancing of tasks across processing units.
it is useful when the number of tasks to be processed is dynamic due to one task creating an arbitrary number of new tasks.
work stealing has been explored in the context of gpus .
each workgroup has a queue from which it obtains tasks to process and to which it stores new tasks.
if its queue is empty a workgroup tries to steal a task from another queue.
figure illustrates a work stealing kernel.
each thread receives a pointer to the task queues in global memory initialized by the1kernel graph app global graph g global nodes n0 global nodes n1 int level global nodes in nodes n0 global nodes out nodes n1 int tid get global id int stride get global size while in nodes.size for int i tid i in nodes.size i stride process node g in nodes out nodes level swap in nodes out nodes global barrier reset out nodes level global barrier figure an opencl graph traversal algorithm host to contain initial tasks.
a thread uses its workgroup id line as a queue id to access the relevant task queue.
the pop or steal function line pops a task from the workgroup s queue or tries to steal a task from other queues.
although not depicted here concurrent accesses to queues inside more work andpop or steal are guarded by a mutex per queue implemented using atomic compare and swap operations on global memory.
if a task is obtained then the workgroup processes it line which may lead to new tasks being created and pushed to the workgroup s queue.
the kernel presents two opportunities for spinwaiting spinning to obtain a mutex and spinning in the main kernel loop to obtain a task.
without fair scheduling threads waiting for the mutex might spin indefinitely causing the application to hang.
graph traversal.
figure illustrates a frontier based graph traversal algorithm such algorithms have been shown to execute efficiently on gpus .
the kernel is given three arguments in global memory a graph structure and two arrays of graph nodes.
initially n0contains the starting nodes to process.
private variable level records the current frontier level and in nodes andout nodes point to distinct arrays recording the nodes to be processed during the current and next frontier respectively.
the application iterates as long as the current frontier contains nodes to process line .
at each frontier the nodes to be processed are evenly distributed between threads through stride based processing.
in this case the stride is the total number of threads obtained via get global size.
a thread calls process node to process a node given the current level with nodes to be processed during the next frontier being pushed to out nodes .
after processing the frontier the threads swap their node array pointers line .
at this point the gpu threads must wait for all other threads to finish processing the frontier.
to achieve this we use a global barrier construct line .
after all threads reach this point the output node array is reset line and the level is incremented.
the threads use another global barrier to wait until the output node is reset line after which they continue to the next frontier.
the global barrier used in this application is not provided as a gpu primitive though previous works have shown that such a global barrier can be implemented based on cpu barrierdesigns .
these barriers employ spinning to ensure each thread waits at the barrier until all threads have arrived thus fair scheduling between workgroups is required for the barrier to operate correctly.
without fair scheduling the barrier threads may wait indefinitely at the barrier causing the application to hang.
the mutexes and barriers used by these two examples appear to run reliably on current gpus for kernels that are executed with no more workgroups than there are compute units.
this is due to the fairness of the occupancy bound execution model that current gpus have been shown experimentally to provide.
but as discussed in sec.
this model is not endorsed by language standards or vendor implementations and may not be respected in the future.
in sec.
.
we show how the work stealing and graph traversal examples of figs.
and can be updated to use our cooperative kernels programming model to resolve the scheduling issue.
cooperative kernels we present our cooperative kernels programming model as an extension to opencl.
we describe the semantics of the model sec.
.
presenting a more formal operational semantics in appendix a and discussing possible alternative semantic choices in appendix b use our motivating examples to discuss programmability sec.
.
and outline important nonfunctional properties that the model requires to work successfully sec.
.
.
.
semantics of cooperative kernels as with a regular opencl kernel a cooperative kernel is launched by the host application passing parameters to the kernel and specifying a desired number of threads and workgroups.
unlike in a regular kernel the parameters to a cooperative kernel are immutable though pointer parameters can refer to mutable data .
cooperative kernels are written using the following extensions transmit a qualifier on the variables of a thread offer killand request fork the key functions that enable cooperative scheduling andglobal barrier andresizing global barrier primitives for interworkgroup synchronisation.
transmitted variables.
a variable declared in the root scope of the cooperative kernel can optionally be annotated with a new transmit qualifier.
annotating a variable vwith transmit means that when a workgroup uses request fork to spawn new workgroups the workgroup should transmit its current value for vto the threads of the new workgroups.
we detail the semantics for this when we describe request fork below.
active workgroups.
if the host application launches a cooperative kernel requesting nworkgroups this indicates that the kernel should be executed with a maximum ofnworkgroups and that as many workgroups as possible up to this limit are desired.
however the scheduler may initially schedule fewer than nworkgroups and as explained below the number of workgroups that execute the cooperative kernel can change during the lifetime of the kernel.
the number of active workgroups workgroups executing the kernel is denoted m. active workgroups have consecutive ids in the range .
initially at least one workgroup is active if necessary the scheduler must postpone the kernel until some compute unit becomes available.
for example in fig.
at the beginningof the execution m while the graphics task is executing m after the fork m 4again.
when executed by a cooperative kernel get num groups returns m the current number of active workgroups.
this is in contrast to get num groups for regular kernels which returns the fixed number of workgroups that execute the kernel see sec.
.
.
fair scheduling isguaranteed between active workgroups i.e.
if some thread in an active workgroup is enabled then eventually this thread is guaranteed to execute an instruction.
semantics for offer kill.theoffer killprimitive allows the cooperative kernel to return compute units to the scheduler by offering to sacrifice workgroups.
the idea is as follows allowing the scheduler to arbitrarily and abruptly terminate execution of workgroups might be drastic yet the kernel may contain specific program points at which a workgroup could gracefully leave the computation.
similar to the opencl workgroup barrier primitive offer kill is a workgroup level function it must be encountered uniformly by all threads in a workgroup.
suppose a workgroup with id mexecutes offer kill.
if the workgroup has the largest id among active workgroups then it can be killed by the scheduler except that workgroup can never be killed to avoid early termination of the kernel .
more formally if m m 1orm 1then offer killis a no op.
if instead m 1and m m the scheduler can choose to ignore the offer so that offer killexecutes as a no op or accept the offer so that execution of the workgroup ceases and the number of active workgroups m is atomically decremented by one.
figure illustrates this showing that workgroup 3is killed before workgroup .
semantics for request fork.recall that a desired limit of nworkgroups was specified when the cooperative kernel was launched but that the number of active workgroups m may be smaller than n either because due to competing workloads the scheduler did not provide nworkgroups initially or because the kernel has given up some workgroups via offer killcalls.
through the request fork primitive also a workgroup level function the kernel and scheduler can collaborate to allow new workgroups to join the computation at an appropriate point and with appropriate state.
suppose a workgroup with id m mexecutes request fork.
then the following occurs an integer k is chosen by the scheduler knew workgroups are spawned with consecutive ids in the range the active workgroup count mis atomically incremented by k. theknew workgroups commence execution at the program point immediately following the request fork call.
the variables that describe the state of a thread are all uninitialised for the threads in the new workgroups reading from these variables without first initialising them is an undefined behaviour.
there are two exceptions to this because the parameters to a cooperative kernel are immutable the new threads have access to these parameters as part of their local state and can safely read from them for each variable vannotated with transmit every new thread s copy ofvis initialised to the value that thread in workgroup mheld forvat the point of the request fork call.
in effect thread of the forking workgroup transmits the relevant portion of its local state to the threads of the forked workgroups.figure illustrates the behaviour of request fork.
after the graphics task finishes executing workgroup 0calls request fork spawning the two new workgroups with ids 2and .
workgroups and 3join the computation where workgroup 0called request fork.
notice that k 0is always a valid choice for the number of workgroups to be spawned by request fork and is guaranteed if mis equal to the workgroup limit n. global barriers.
because workgroups of a cooperative kernel are fairly scheduled a global barrier primitive can be provided.
we specify two variants global barrier andresizing global barrier .
our global barrier primitive is a kernel level function if it appears in conditional code then it must be reached by allthreads executing the cooperative kernel.
on reaching a global barrier a thread waits until all threads have arrived at the barrier.
once all threads have arrived the threads may proceed past the barrier with the guarantee that all global memory accesses issued before the barrier have completed.
the global barrier primitive can be implemented by adapting an inter workgroup barrier design e.g.
to take account of a growing and shrinking number of workgroups and the atomic operations provided by the opencl .
memory model enable a memory safe implementation .
theresizing global barrier primitive is also a kernel level function.
it is identical to global barrier except that it caters for cooperation with the scheduler by issuing a resizing global barrier the programmer indicates that the cooperative kernel is prepared to proceed after the barrier with more or fewer workgroups.
when all threads have reached resizing global barrier the number of active workgroups m is atomically set to a new value m say with m n. ifm mthen the active workgroups remain unchanged.
if m m workgroups are killed.
ifm mthen m mnew workgroups join the computation after the barrier as if they were forked from workgroup .
in particular thetransmit annotated local state of thread in workgroup is transmitted to the threads of the new workgroups.
the semantics of resizing global barrier can be modelled via calling request fork andoffer kill surrounded and separated by calls to a global barrier .
the enclosing global barrier calls ensure that the change in number of active workgroups from mtom occurs entirely within the resizing barrier so that mchanges atomically from a programmer s perspective.
the middle global barrier ensures that forking occurs before killing so that workgroups are left intact.
because resizing global barrier can be implemented as above we do not regard it conceptually as a primitive of our model.
however in sec.
.
we show how a resizing barrier can be implemented more efficiently through direct interaction with the scheduler.
.
programming with cooperative kernels a changing workgroup count.
unlike in regular opencl the value returned by get num groups is not fixed during the lifetime of a cooperative kernel it corresponds to the active group count m which changes as workgroups execute offer kill and request fork.
the value returned by get global sizeis similarly subject to change.
a cooperative kernel must thus be written in a manner that is robust to changes in the values returned by these functions.in general their volatility means that use of these functions should be avoided.
however the situation is more stable if a cooperative kernel does not call offer killandrequest fork directly so that only resizing global barrier can affect the number of active workgroups.
then at any point during execution the threads of a kernel are executing between some pair of resizing barrier calls which we call a resizing barrier interval considering the kernel entry and exit points conceptually to be special cases of resizing barriers .
the active workgroup count is constant within each resizing barrier interval so that get num groups andget global size return stable values during such intervals.
as we illustrate below for graph traversal this can be exploited by algorithms that perform strided data processing.
adapting work stealing.
in this example there is no state to transmit since a computation is entirely parameterised by a task which is retrieved from a queue located in global memory.
with respect to fig.
we add request fork andoffer killcalls at the start of the main loop below line to let a workgroup offer itself to be killed or forked respectively before it processes a task.
note that a workgroup may be killed even if its associated task queue is not empty since remaining tasks will be stolen by other workgroups.
in addition since request fork may be the entry point of a workgroup the queue id must now be computed after it so we move line to be placed just before line .
in particular the queue id cannot be transmitted since we want a newly spawned workgroup to read its own queue and not the one of the forking workgroup.
adapting graph traversal.
figure shows a cooperative version of the graph traversal kernel of fig.
from sec.
.
.
on lines and we change the original global barriers into a resizing barriers.
several variables are marked to be transmitted in the case of workgroups joining at the resizing barriers lines and level must be restored so that new workgroups know which frontier they are processing in nodes andout nodes must be restored so that new workgroups know which of the node arrays to use for input and output.
lastly the static work distribution of the original kernel is no longer valid in a cooperative kernel.
this is because the stride which is based on m may change after each resizing barrier call.
to fix this we re distribute the work after each resizing barrier call by recomputing the thread id and stride lines and .
this example exploits the fact that the cooperative kernel does not issue offer killnorrequest fork directly the value of stride obtained from get global sizeat line is stable until the next resizing barrier at line .
patterns for irregular algorithms.
in sec.
.
we describe the set of irregular gpu algorithms used in our experiments which largely captures the irregular blocking algorithms that are available as open source gpu kernels.
these all employ either work stealing or operate on graph data structures and placing our new constructs follows a common easy to follow pattern in each case.
the work stealing algorithms have a transactional flavour and require little or no state to be carried between transactions.
the point at which a workgroup is ready to process a new task is a natural place for offer killandrequest fork and few or no transmit annotations are required.
figure is representative of most level by level graph algorithms.
it is typically the case that on completing a level of1kernel graph app global graph g global nodes n0 global nodes n1 transmit int level transmit global nodes in nodes n0 transmit global nodes out nodes n1 while in nodes.size int tid get global id int stride get global size for int i tid i in nodes.size i stride process node g in nodes out nodes level swap in nodes out nodes resizing global barrier reset out nodes level resizing global barrier figure cooperative version of the graph traversal kernel of fig.
using a resizing barrier and transmit annotations the graph algorithm the next level could be processed by more or fewer workgroups which resizing global barrier facilitates.
some level specific state must be transmitted to new workgroups.
.
non functional requirements the semantics presented in sec.
.
describe the behaviours that a developer of a cooperative kernel should be prepared for.
however the aim of cooperative kernels is to find a balance that allows efficient execution of algorithms that require fair scheduling and responsive multitasking so that the gpu can be shared between cooperative kernels and other shorter tasks with soft real time constraints.
to achieve this balance an implementation of the cooperative kernels model and the programmer of a cooperative kernel must strive to meet the following non functional requirements.
the purpose of offer killis to let the scheduler destroy a workgroup in order to schedule higher priority tasks.
the scheduler relies on the cooperative kernel to execute offer killsufficiently frequently that soft real time constraints of other workloads can be met.
using our work stealing example a workgroup offers itself to the scheduler after processing each task.
if tasks are sufficiently fast to process then the scheduler will have ample opportunities to de schedule workgroups.
but if tasks are very time consuming to process then it might be necessary to rewrite the algorithm so that tasks are shorter and more numerous to achieve a higher rate of calls to offer kill.
getting this non functional requirement right is gpu and application dependent.
in sec.
.
we conduct experiments to understand the response rate that would be required to co schedule graphics rendering with a cooperative kernel maintaining a smooth frame rate.
recall that on launch the cooperative kernel requests nworkgroups.
the scheduler should thus aim to provide nworkgroups if other constraints allow it by accepting an offer killonly if a compute unit is required for another task and responding positively to request fork calls if compute units are available.
prototype implementation our vision is that cooperative kernel support will be integrated in the runtimes of future gpu implementations of opencl with driver support for our new primitives.
to experiment with our ideas on current gpus we have developed a prototype that mocks up the required runtime support via a megakernel and exploits the occupancy bound execution model that these gpus provide to ensure fair scheduling between workgroups.
we emphasise that an aim of cooperative kernels is to avoid depending on the occupancybound model.
our prototype exploits this model simply to allow us to experiment with current gpus whose proprietary drivers we cannot change.
we describe the megakernel approach sec.
.
and detail various aspects of the scheduler component of our implementation sec.
.
.
.
the megakernel mock up instead of multitasking multiple separate kernels we merge a set of kernels into a megakernel a single monolithic kernel.
the megakernel is launched with as many workgroups as can be occupant concurrently.
one workgroup takes the role of the scheduler 2and the scheduling logic is embedded as part of the megakernel.
the remaining workgroups act as a pool of workers.
a worker repeatedly queries the scheduler to be assigned a task.
a task corresponds to executing a cooperative or non cooperative kernel.
in the noncooperative case the workgroup executes the relevant kernel function uninterrupted then awaits further work.
in the cooperative case the workgroup either starts from the kernel entry point or immediately jumps to a designated point within the kernel depending on whether the workgroup is an initial workgroup of the kernel or a forked workgroup.
in the latter case the new workgroup also receives a struct containing the values of all relevant transmit annotated variables.
simplifying assumptions.
for ease of implementation our prototype supports multitasking a single cooperative kernel with a single non cooperative kernel though the non cooperative kernel can be invoked many times .
we require that offer kill request fork andresizing global barrier are called from the entry function of a cooperative kernel.
this allows us to use goto andreturn to direct threads into and out of the kernel.
with these restrictions we can experiment with interesting irregular algorithms see sec.
.
a non mock implementation of cooperative kernels would not use the megakernel approach so we did not deem the engineering effort associated with lifting these restrictions in our prototype to be worthwhile.
.
scheduler design to enable multitasking through cooperative kernels the runtime in our case the megakernel must track the state of workgroups i.e.
whether a workgroup is waiting or computing a kernel maintain consistent context states for each kernel e.g.
tracking the number of active workgroups and provide a safe way for these states to be modified in response to request fork offer kill.
we discuss these issues and describe the implementation of an efficient resizing barrier.
2we note that the scheduler requirements given in sec.
are agnostic to whether the scheduling logic takes place on the cpu or gpu.
to avoid expensive communication between gpu and host we choose to implement the scheduler on the gpu.we describe how the scheduler would handle arbitrary combinations of kernels though as noted above our current implementation is restricted to the case of two kernels.
scheduler contexts.
to dynamically manage workgroups executing cooperative kernels our framework must track the state of each workgroup and provide a channel of communication from the scheduler workgroup to workgroups executing request fork andoffer kill.
to achieve this we use a scheduler context structure mapping a primitive workgroup id the workgroup s status which is either available or the id of the kernel that the workgroup is currently executing.
the scheduler can then send cooperative kernels aresource message commanding workgroups to exit at offer kill or spawn additional workgroups at request fork.
thus the scheduler context needs a communication channel for each cooperative kernel.
we implement the communication channels using atomic variables in global memory.
launching kernels and managing workgroups.
to launch a kernel the host sends a data packet to the gpu scheduler consisting of a kernel to execute kernel inputs and a flag indicating whether the kernel is cooperative.
in our prototype this host device communication channel is built using fine grained svm atomics.
on receiving a data packet describing a kernel launch k the gpu scheduler must decide how to schedule k. suppose krequests nworkgroups.
the scheduler queries the scheduler context.
if there are at least navailable workgroups kcan be scheduled immediately.
suppose instead that there are only na navailable workgroups but a cooperative kernel kcis executing.
the scheduler can use kc s channel in the scheduler context to command kcto provide n naworkgroups via offer kill.
once nworkgroups are available the scheduler then sends nworkgroups from the available workgroups to execute kernel k. if the new kernel kis itself a cooperative kernel the scheduler would be free to provide kwith fewer than nactive workgroups initially.
if a cooperative kernel kcis executing with fewer workgroups than it initially requested the scheduler may decide make extra workgroups available to kc to be obtained next time kccalls request fork.
to do this the scheduler asynchronously signals kc through kc s channel to indicate the number of workgroups that should join at the next request fork command.
when a workgroup wofkcsubsequently executes request fork thread of wupdates the kernel and scheduler contexts so that the given number of new workgroups are directed to the program point after the request fork call.
this involves selecting workgroups whose status is available as well as copying the values of transmit annotated variables to the new workgroups.
an efficient resizing barrier.
in sec.
.
we defined the semantics of a resizing barrier in terms of calls to other primitives.
it is possible however to implement the resizing barrier with only one call to a global barrier with request fork andoffer killinside.
we consider barriers that use the master slave model one workgroup master collects signals from the other workgroups slaves indicating that they have arrived at the barrier and are waiting for a reply indicating that they may leave the barrier.
once the master has received a signal from all slaves it replies with a signal saying that they may leave.table blocking gpu applications investigated app.
barriers kill fork transmit loc inputs color mis p sssp bfs l sssp octree game pannotia lonestar gpu work stealing incorporating request fork andoffer killinto such a barrier is straightforward.
upon entering the barrier the slaves first execute offer kill possibly exiting.
the master then waits for mslaves the number of active workgroups which may decrease due to offer kill calls by the slaves but will not increase.
once the master observes thatmslaves have arrived it knows that all other workgroups are waiting to be released.
the master executes request fork and the statement immediately following this request fork is a conditional that forces newly spawned workgroups to join the slaves in waiting to be released.
finally the master releases all the slaves the original slaves and the new slaves that joined at request fork.
this barrier implementation is sub optimal because workgroups only execute offer killonce per barrier call and depending on order of arrival it is possible that only one workgroup is killed per barrier call preventing the scheduler from gathering workgroups quickly.
we can reduce the gather time by providing a new query function for cooperative kernels which returns the number of workgroups that the scheduler needs to obtain from the cooperative kernel.
a resizing barrier can now be implemented as follows the master waits for all slaves to arrive the master calls request fork and commands the new workgroups to be slaves the master calls query obtaining a value w the master releases the slaves broadcasting the value wto them workgroups with ids larger than m wspin calling offer killrepeatedly until the scheduler claims them we know from query that the scheduler will eventually do so.
we show in sec.
.
that the barrier using query greatly reduces the gather time in practice.
applications and experiments we discuss our experience porting irregular algorithms to cooperative kernels and describe the gpus on which we evaluate these applications sec.
.
.
for these gpus we report on experiments to determine non cooperative workloads that model the requirements of various graphics rendering tasks sec.
.
.
we then examine the overhead associated with moving to cooperative kernels when multitasking is notrequired sec.
.
as well as the responsiveness and throughput observed when a cooperative kernel is multi tasked with non cooperative workloads sec.
.
.
finally we compare against a performance model of kernel level preemption which we understand to be what current nvidia gpus provide sec.
.
.
.
applications and gpus table gives an overview of the irregular algorithms that we ported to cooperative kernels.
among them are graph algorithms based on the pannotia and lonestar gpu application suites using global barriers.
we indicate how many of the original number of barriers are changed to resizing barriers all of them and how many variables need to be transmitted.
the remaining two algorithms are work stealing applications each required the addition ofrequest fork andoffer killat the start of the main loop and no variables needed to be transmitted similar to example discussed in sec.
.
.
most graph applications come with different data sets as input leading to application input pairs in total.
our prototype implementation sec.
requires two optional features of opencl .
svm fine grained buffers and svm atomics.
out of the gpus available to us from arm amd nvidia and intel only intel gpus provided robust support of these features.
we thus ran our experiments on three intel gpus hd hd and iris .
the results were similar across the gpus so for conciseness we report only on the iris gpu driver .
.
.
with a host cpu i3 5157u.
the iris has a reported compute units.
results for the other intel gpus are presented in appendix c. .
sizing non cooperative kernels enabling rendering of smooth graphics in parallel with irregular algorithms is an important use case for our approach.
because our prototype implementation is based on a megakernel that takes over the entire gpu see sec.
we cannot assess this directly.
we devised the following method to determine opencl workloads that simulate the computational intensity of various graphics rendering workloads.
we designed a synthetic kernel that occupies all workgroups of a gpu for a parameterised time period t invoked in an infinite loop by a host application.
we then searched for a maximum value for tthat allowed the synthetic kernel to execute without having an observable impact on graphics rendering.
using the computed value we ran the host application for xseconds measuring the time y xdedicated to gpu execution during this period and the number of kernel launches nthat were issued.
we used x 10in all experiments.
the values x y nandx n estimate the average time spent using the gpu to render the display between kernel calls call this e and the period at which the os requires the gpu for display rendering call this p respectively.
we used this approach to measure the gpu availability required for three types of rendering light whereby desktop icons are smoothly emphasised under the mouse pointer medium whereby window dragging over the desktop is smoothly animated and heavy which requires smooth animation of a webgl shader in a browser.
forheavy we used webgl demos from the chrome experiments .
our results are the following p 70msande 3msfor light p 40ms e 3msfor medium and p 40ms e 10msfor heavy.
for medium and heavy the 40msperiod coincides with the human persistence of vision.
the 3msexecution duration of both light and medium configurations indicates that gpu computation is cheaper for basic display rendering compared with more complex rendering.
.
the overhead of cooperative kernels experimental setup.
invoking the cooperative scheduling primitives incurs some overhead even if no killing forking or resizingtable cooperative kernel slowdown w o multitasking overall barrier wk.steal.
mean max mean max mean max .
.
.
.
.
.
octree color g3 circuit actually occurs because the cooperative kernel still needs to interact with the scheduler to determine this.
we assess this overhead by measuring the slowdown in execution time between the original and cooperative versions of a kernel forcing the scheduler to never modify the number of active workgroups in the cooperative case.
recall that our mega kernel based implementation merges the code of a cooperative and a non cooperative kernel.
this can reduce the occupancy for the merged kernel e.g.
due to higher register pressure this is an artifact of our prototype implementation and would not be a problem if our approach was implemented inside the gpu driver.
we thus launch both the original and cooperative versions of a kernel with the reduced occupancy bound in order to meaningfully compare execution times.
results.
tab.
shows the geometric mean and maximum slowdown across all applications and inputs with averages and maxima computed over runs per benchmark.
for the maximum slowdowns we indicate which application and input was responsible.
the slowdown is below .
even in the worst case and closer to on average.
we consider these results encouraging especially since the performance of our prototype could clearly be improved upon in a native implementation.
.
multitasking via cooperative scheduling we now assess the responsiveness of multitasking between a longrunning cooperative kernel and a series of short non cooperative kernel launches and the performance impact of multitasking on the cooperative kernel.
experimental setup.
for a given cooperative kernel and its input we launch the kernel and then repeatedly schedule a noncooperative kernel that aims to simulate the intensity of one of the three classes of graphics rendering workload discussed in sec.
.
.
in practice we use matrix multiplication as the non cooperative workload with matrix dimensions tailored to reach the appropriate execution duration.
we conduct separate runs where we vary the number of workgroups requested by the non cooperative kernel considering the cases where one a quarter a half and all but one of the total number of workgroups are requested.
for the graph algorithms we try both regular and query barrier implementations.
our experiments span pairs of cooperative kernels and inputs classes of non cooperative kernel workloads quantities of workgroups claimed for the non cooperative kernel and variations of resizing barriers for graph algorithms leading to configurations.
we run each configuration times in order to report averaged performance numbers.
for each run we record the execution time of the cooperative kernel.
for each scheduling of the non cooperative kernel during the run we also record the gather time needed by the scheduler to collect workgroups to launch the non cooperative kernel and the non cooperative kernel execution time..1110high low n n n 1time ms number of non cooperative workgroupsiris octree gather time light task heavy task .1110high low n n n 1time ms number of non cooperative workgroupsiris bfs usa gather time w o query gather time light task heavy task .1110high low n n n 1time ms number of non cooperative workgroupsiris color g3 circuit gather time w o query gather time light task heavy taskfigure example gather time and non cooperative timing results responsiveness.
figure reports on three configurations the average gather and execution times for the non cooperative kernel with respect to the quantity of workgroups allocated to it.
a logarithmic scale is used for time since gather times tend to be much smaller than execution times.
the horizontal grey lines indicates the desired period for non cooperative kernels.
these graphs show a representative sample of our results the full set of graphs for all configurations is provided in appendix c. the left most graph illustrates a work stealing example.
when the non cooperative kernel is given only one workgroup its execution is so long that it cannot complete within the period required for a screen refresh.
the gather time is very good though since the scheduler needs to collect only one workgroup.
the more workgroups are allocated to the non cooperative kernels the faster it can compute here the non cooperative kernel becomes fast enough with a quarter resp.
half of available workgroups for light resp.
heavy graphics workload.
inversely the gather time increases since the scheduler must collect more and more workgroups.
the middle and right graphs show results for graph algorithms.
these algorithms use barriers and we experimented with the regular and query barrier implementations described in sec.
.
.
the execution times for the non cooperative task are averaged across all runs including with both types of barrier.
we show separately the average gather time associated with each type of barrier.
the graphs show a similar trend to the left most graph as the number of non cooperative workgroups grows the execution time decreases and the gather time increases.
the gather time is higher on the rightmost figure as the g3 circuit input graph is rather wide than deep so the graph algorithm reaches resizing barriers less often than for the usa road input of the middle figure for instance.
the scheduler thus has fewer opportunities to collect workgroups and gather time increases.
nonetheless scheduling responsiveness can benefit from the query barrier when used this barrier lets the scheduler collect all needed workgroups as soon as they hit a resizing barrier.
as we can see the gather time of the query barrier is almost stable with respect to the number of workgroups that needs to be collected.
performance.
figure reports the overhead brought by the scheduling of non cooperative kernels over the cooperative kernel execution time.
this is the slowdown associated with running the .
.
.
.
.
.
.
n n n 1median overhead workgroups n n n 1median period ms workgroupsheavy medium lightfigure performance impact of multitasking cooperative and non cooperative workloads and the period with which non cooperative kernels execute cooperative kernel in the presence of multitasking vs. running the cooperative kernel in isolation median over all applications and inputs .
we also show the period at which non cooperative kernels can be scheduled median over all applications and inputs .
our data included some outliers that occur with benchmarks in which the resizing barrier are not called very frequently and the graphics task requires half or more workgroups.
for example a medium graphics workload for bfs on the rmat input has over an overhead when asking for all but one of the workgroups.
as figure shows most of our benchmarks are much better behaved than this.
in future work is required to examine the problematic benchmarks in more detail possibly inserting more resizing calls.
we show results for the three workloads listed in sec.
.
.
the horizontal lines in the period graph correspond to the goals of the workloads the higher resp.
lower line corresponds to a period of 70ms resp.
40ms for the light resp.
medium and heavy workload.
co scheduling non cooperative kernels that request a single workgroup leads to almost no overhead but the period is far too high to meet the needs of any of our three workloads e.g.
a heavy workload averages a period of 939ms.
as more workgroups are dedicated to non cooperative kernels they execute quickly enough to be scheduled at the expected period.
for the light and medium workloads a quarter of the workgroups executing the non cooperative kernel are able to meet their goal period and 40msresp.
.
however this is not sufficient to meet the goal for the heavy workloadtable overhead of kernel level preemption vs cooperative kernels for three graphics workloads g. workload kernel level cooperative resources light .
.
n medium .
.
n heavy .
.
n giving a median period of 104ms .
if half of the workgroups are allocated to the non cooperative kernel the heavy workload achieves its goal period median of 40ms .
yet as expected allocating more non cooperative workgroups increases the overhead of the cooperative kernel.
still heavy workloads meet their period by allocating half of the workgroups incurring a slow down of less than .
median .
light and medium workloads meet their period with only a small overhead .
and .
median slowdown respectively.
.
comparison with kernel level preemption nvidia s recent pascal architecture provides hardware support for instruction level preemption however preemption of entire kernels but not of individual workgroups is supported.
intel gpus do not provide this feature and our opencl prototype of cooperative kernels cannot run on nvidia gpus making a direct comparison impossible.
we present here a theoretical analysis of the overheads associated with sharing the gpu between graphics and compute tasks via kernel level preemption.
suppose a graphics workload is required to be scheduled with period pand duration d and that a compute kernel requires time cto execute without interruption.
if we assume the cost of preemption is negligible e.g.
nvidia have reported preemption times of .
msfor pascal because of special hardware support then the overhead associated with switching between compute and graphics every ptime steps is p p d .
we compare this task level preemption overhead model with our experimental results per graphics workload in tab.
.
we report the overhead of the configuration that allowed us to meet the deadline of the graphics task.
based on the above assumptions our approach provides similar overhead for low and medium graphics workloads however has a higher overhead for the high workload.
our low performance for heavy workloads is because the graphics task requires half of the workgroups crippling the cooperative kernel enough that request fork calls are not issued as frequently.
future work may examine how to insert more resizing calls in these applications to address this.
these results suggest that a hybrid preemption scheme may work well.
that is the cooperative approach works well for light and medium tasks on the other hand heavy graphics tasks benefit from the coarser grained kernel level preemption strategy.
however the preemption strategy requires specialised hardware support in order to be efficient.
related work irregular algorithms and persistent kernels.
there has been a lot of work on accelerating blocking irregular algorithms using gpus and on the persistent threads programming style for longrunning kernels .
these approaches rely on the occupancy bound execution model flooding available compute units with work so that the gpu is unavailable for other tasks and assuming fair scheduling between occupant workgroups which is unlikely to be guaranteed on future gpu platforms.
as our experiments demonstrate our cooperative kernels model allows blocking algorithms to be upgraded to run in a manner that facilitates responsive multitasking.
gpu multitasking and scheduling.
hardware support for preemption has been proposed for nvidia gpus as well as sm draining whereby workgroups occupying a symmetric multiprocessor sm a compute unit using our terminology are allowed to complete until the sm becomes free for other tasks .
sm draining is limited the presence of blocking constructs since it may not be possible to drain a blocked workgroup.
a follow up work adds the notion of smflushing where a workgroup can be re scheduled from scratch if it has not yet committed side effects .
both approaches have been evaluated using simulators over sets of regular gpu kernels.
very recent nvidia gpus i.e.
the pascal architecture support preemption though as discussed in sec.
and sec.
.
it is not clear whether they guarantee fairness or allow tasks to share gpu resources at the workgroup level .
cuda and opencl provide the facility for a kernel to spawn further kernels .
this dynamic parallelism can be used to implement a gpu based scheduler by having an initial scheduler kernel repeatedly spawn further kernels as required according to some scheduling policy .
however kernels that uses dynamic parallelism are still prone to unfair scheduling of workgroups and thus does not help in deploying blocking algorithms on gpus.
cooperative multitasking.
cooperative multitasking was offered in older operating systems e.g.
pre windows and is still used by some operating systems such as risc os .
additionally cooperative multitasking can be efficiently implemented in today s high level languages for domains in which preemptive multitasking is either too costly or not supported on legacy systems .
conclusions and future work we have proposed cooperative kernels a small set of gpu programming extensions that allow long running blocking kernels to be fairly scheduled and to share gpu resources with other workloads.
experimental results using our megakernel based prototype show that the model is a good fit for current gpu accelerated irregular algorithms.
the performance that could be gained through a native implementation with driver support would be even better.
avenues for future work include seeking additional classes of irregular algorithms to which the model might be extended to apply to investigating implementing native support in open source drivers and integrating cooperative kernels into template and compilerbased programming models for graph algorithms on gpus .