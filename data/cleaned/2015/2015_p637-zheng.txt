a method to identify and correct problematic software activity data exploiting capacity constraints and data redundancies qimu zheng1 zheng.qm .comaudris mockus2 audris utk.eduminghui zhou1 zhmh pku.edu.cn 1school of electronics engineering and computer science peking university key laboratory of high confidence software technologies ministry of education beijing china 2university of tennessee middle drive knoxville tn usa abstract mining software repositories to understand and improve software development is a common approach in research and practice.
the operational data obtained from these repositories often do not faithfully represent the intended aspects of software development and therefore may jeopardize the conclusions derived from it.
we propose an approach to identify problematic values based on the constraints of software development and to correct such values using data redundancies.
we investigate the approach using issue and commit data of mozilla project.
in particular we identied problematic data in four types of events and found the fraction of problematic values to exceed and rapidly rising.
we found the corrected values to be closer to the most accurate estimate of task completion time.
finally we found that the models of time until x changed substantially when data were corrected with the corrected data providing a better t. we discuss how the approach may be generalized to other types of operational data to increase delity of software measurement in practice and in research.
categories and subject descriptors d. .
metrics process metrics general terms measurement human factors keywords data quality mining software repositories capacity constraint data redundancy corresponding author.
.
introduction operational support tools such as issue tracking system its version control system vcs mailing lists forums and others are broadly adopted by software projects and the operational data produced and consumed by these tools are crucial for the e ective operation of software development.
for example dabbish et al.
argue that the data in the operational systems such as github are actively being used by developers seeking to learn share code and for other key tasks such as assessing the quality and activity of software projects.
operational data are also heavily used in software engineering research for example in the eld of mining software repositories and in empirical software engineering.
however as extensively documented in the section on related work operational data often do not faithfully represent the intended aspects of software development and therefore may jeopardize the conclusions derived from it.
such problematic data a ects the conclusions presented in the academic work see e.g.
and leads to poor decisions in software development see e.g.
showing that mozilla had over rate of mistaken product assignments a ecting the quality of software and increasing e ort and lead times.
its for example contains activities of individuals initiating and completing software project tasks.
virtually every its produces reports on issue resolution.
that information is commonly used in practice to measure progress and select issues to be worked on.
for example the time until x is obtained as the duration of time between issue creation and resolution date recorded in the its.
the values recorded in the its however tend to strongly vary with the practices used by projects and individuals.
for example an individual may write a script to clean its by closing a large number of dormant issues.
such cleanup would produce questionable x dates for the issues involved and an exceptionally high productivity for the individual recorded as the resolver.
in this study we propose a method to identify and correct such problematic activity data.
the rst premise of this study is that physical constraints tend to bound what could be accomplished by any individual or group over a xed period of time.
for example an individual may be able to complete only a limited number of tasks in any given time interval.
the mere existence of such constraints provides information that can help us identify erroneous data that permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
copyright is held by the owner author s .
publication rights licensed to acm.
esec fse august september bergamo italy acm.
... .
637violate them.
the bounds on actor productivity even if unknown can be estimated from the operational data as described below.
the second premise is that the operational data tend to be highly redundant with numerous types of events that could be used to measure the quantity of interest.
this redundancy as in the information theory can help us to correct erroneous data using redundant events.
we therefore use the existence of real constraints in software development to identify problematic values and to correct such values by selecting redundant observations that are more likely to be correct and evaluate our approach on mozilla its and vcs.
in particular we answer the following research questions.
rq1 is it possible to identify and x incorrect task completion dates in software development?
we propose to use a poisson distribution for individuals productivity to identify likely inaccurate task completion dates based on issue resolution events and replace such problematic data with redundant events associated with the same task.
rq2 what are the speci c mechanisms in mozilla that result in erroneous issue x completion dates?
manual analysis of issues with erroneous dates suggests three common mechanisms issues tracked in other system long dormant issues and issues linked to committed patches in the vcs tend to be batch xed.
rq3 can redundancies in operational data provide alternative task completion dates?
we used alternative observations of the last comment posted for an issue to correct the issue resolution dates and authors.
rq4 are corrected data more accurate than uncorrected data?
by comparing corrected and uncorrected values to the last commit date in the vcs we found corrected values to be more accurate.
rq5 how redundant are the alternative observations?
we found that when the primary observations are incorrect the redundant observations are also more likely to be incorrect but despite that they provide ample redundancy with more than of them being nonproblematic.
rq6 does the data correction matter in common models of time until x?
we t a model commonly found in the literature and found both its structure and the t to change substantially after data correction.
rq7 can the approach be generalized to other types of events and tasks?
we identify likely incorrect observations for four types of events and discuss how to apply the approach more broadly.
the rest of the paper is organized as follows.
we describe the related work in section .
we propose a method to identify and correct problematic values in section and apply the approach to correct issue x dates in section .
we elaborate on the limitations in section and discuss how to generalize the method to other types of operational data in section .
.
related work as operational data in software development has been increasingly used in research and practice the concerns aboutits accuracy and completeness have been drawing an increasing attention.
we consider related work from two aspects the extensive use of its data to conduct analysis and research on operational data quality.
being one of the most important software repositories its has been widely mined to measure a variety of aspects in software development.
some of the studies use the number of issues to measure e ort or performance e.g.
using the number of completed issues to measure team e ort and productivity or using the number of defects xed to measure individual performance .
other studies focus on improving issue resolution practices.
for example mining the history to recommend right issues for people and comparing the similarity between issue reports to detect duplicate reports .
bug xing is an important topic with a substantial amount of literature.
common questions include which bugs get xed ?
how long it takes to x a bug ?
guo et al.
performed an empirical study to characterize factors that a ect which bugs get xed in windows vista and windows .
kim et al.
studied the bug x time of les in argouml and postgresql by identifying when bugs are introduced and when they are xed.
however as highlighted by this study the above metrics derived from operational data may su er from the variation of how people practice software development.
the problems with operational data have been an increasing focus of attention.
for example bird et al.
studied the linkages between vcs and its and their e ects on research result.
they investigated historical data from several software projects and found strong evidence of systematic bias.
the biases found by bird et al.
were later veri ed by thanh et al.
for commercial projects.
kim et al.
proposed approaches to deal with the unlinked bugs.
they measured the impact of noise in the data and proposed an classi cation algorithm for identifying such noise.
bachmann et al.
presented tools for reverse engineering link data.
their tool enables users to quickly nd and examine relevant changes and annotate them as desired.
errors in issue tracking data and their e ects on defect prediction models are investigated in and .
their ndings show that issue report classi cations are unreliable.
due to the noise in issue tracking data les tend to be wrongly marked to be error prone and wrongly predicted to be error prone.
the noise in code commit data is studied in e.g.
which investigated whether a single commit includes several tasks.
the use of constraints in commercial projects was exploited in for example where an algorithm that distributes monthly developer e ort to individual tasks is evaluated.
the accuracy of a triage activity in its is evaluated in .
a method that compares an intermediate value of an attribute in its to its nal value is used to estimate the odds that a product attribute in the its is incorrect.
this method is used to illustrate how the incorrect product attribute sends the issue to the wrong product team leading to wasted e ort and issue resolution delays .
various research challenges associated with operational data are outlined in for example .
a comprehensive review of outlier detection is presented by hodge and austin .
our approach of using a mixture model is similar to approaches proposed by yamanishi et al.
.
the proposed approach incorporates actual data generation mechanism in operational data within software development context to construct more accurate models that 638are based on the understanding of the processes generating the data.
this makes it possible to identify the outliers more precisely.
furthermore the outlier detection work typically assumes that the outliers will be removed from the sample or accommodated as in robust statistical methods .
removing or ignoring outliers would unfortunately bias the sample and analysis results as we show in this study.
we therefore unlike in the prior work focus on providing a theoretical basis for the methods used to identify and correct problematic observations in operational data.
.
method in this section we attempt to answer rq1 by relying on the premise that constraints may serve as an information source and that operational data are highly redundant.
while the idea of using constraints as an information source is very general we discuss a speci c application of it to identify problematic values in activity streams represented by di erent types of events in software operational data.
it is common for operational data to include activity streams sequences of events that represent actions.
posts to a mailing list status changes to an issue or commits to a version control system represent such event streams.
considering the complexity of software development process and the variation of operational support tools an event in one system is often associated with events in other systems.
for example before closing an issue in its a code commit to resolve the issue may be submitted in vcs and the code review discussion may have been through the mailing list.
these additional events in any of the associated operational support tools may provide redundant events in case the primary events are identi ed as problematic.
in software mining certain events from activity streams are used to estimate the task completion time.
in cases when the task represents a nontrivial amount of e ort as for example in code changes or in issue xes it is reasonable to expect that an individual will have a limited capacity to perform such tasks and the completion times if accurate can not happen simultaneously or be very close to each other for two distinct and nontrivial tasks.
however this constraint while enforced in the real world often does not hold in the operational data because the event timestamps are imperfect representations of task completion times.
more speci cally the events representing the task completion date may contain a date far from the actual task completion date or the event may be associated with a person who was not involved in the task.
such inaccuracies may jeopardize conclusion drawn from the analysis relying on these events.
the existence of constraints in the physical world can help us identify such clearly erroneous data by assuming or estimating productivity bounds of actors.
we describe an approach to determine such constraints below.
lets denote the operational data event time that we use to approximate the completion time of task iby actorjas tij.
assuming the productivity of actor jaspjand the difculty of task iaseiwe would have the duration between the events be tij ti j ei pjif the tasks are done in sequence1.
the formula shows that the duration between such events can not approach zero unless the e ort for the task 1while the actors in software development tend to engage in several tasks at a time this is often caused by the need to wait on input from other parties or tasks of higher priority interrupting tasks of lower priority.approaches zero ei!
or the productivity of the actor tends to in nity pj!
.
for simplicity assuming an exponential distribution of the task completion inter arrival times the count of events over any speci c interval of time would follow a poisson distribution with the intensity dened by some function of the ratioei pjfor the events occurring during that interval.
we therefore expect the count of task completions to follow a poisson distribution2.
if we observe extreme outliers in these counts it is reasonable to suspect that the data may be incorrect.
we can use various approaches to detect outliers for example by assuming that the operational data values come from a mixture distribution with probability the observation is accurate and comes from a poisson distribution parameterized by good and with the observation is inaccurate and comes from a poisson distribution parameterized by bad .
to get some idea of what good would look we need to gauge the distribution ofei pjbased on the understanding of real world of actors and their tasks.
to gauge the distribution of and bad we need to investigate the mechanisms by which the operational data get corrupted i.e.
do not re ect the intended task completion times .
such investigation may also reveal more precisely the distributions of normal and erroneous completion times.
if the direct access to the actors and the ability to understand what happens in reality is limited we can still rely on the statistical properties of the observed distribution and use mixture models like described above to estimate the probability of erroneous observation and the goodand bad.
for simplicity lets assume that bad good3.
the following simple rule can be used to identify erroneous values isproblematic nj 1ifp ii tij2d cut 0ifp ii tij2d cut wherep ii tij2d is the number of task completions by developerjduring daydandcutis a large enough number to ensure that non problematic observations are highly unlikely to be marked as erroneous.
unfortunately once the erroneous data are identi ed we often can not simply discard it the subsequent analysis frequently assumes that data are complete.
for example if we want to know the number of issues xed by each developer and understand what a ects their performance discarding problematic data would make the analysis biased.
removing incorrect data would for example a ect the estimates of each developer s performance by a varying amount.
once the errors are identi ed we therefore need to correct the data.
to do that we propose to use redundant observations from the event streams that are associated with the same task.
as we mentioned above the redundant observations could come from other systems used in software development.
the redundant observations should be able to avoid the problems that the original data have while being a reasonable approximation of the needed measures.
for example the redundant observations should provide redundancy i.e.
they should not be uniformly problematic in cases when the original observations are problematic.
the choice of redundant observations should also be based on the 2albeit of varying intensity that depends on the di culty of the tasks conducted during that interval.
see section .
.
3evaluation described in section .
suggests this to be a reasonable assumption.
639understanding of the problems in the original data and of the nature of the desired measures.
it is not unusual to have numerous redundant observations and measures in software mining.
for example there are numerous events in its such as issue status change comment report attachment and numerous other events with timestamps.
version control system and mailing list may contain events associated with the same issue.
developers may also use other means such as twitter to announce that they have managed to x a problem.
as with the identi cation of problematic observation we may be able to select redundant observations based on available data.
for example any measure de ned as a function of the redundant observations that well approximates non problematic primary observations may be selected as a suitable alternative.
the identi cation of erroneous data and the correction process can be described as follows.
.
gather events from available sources and link them by using tasks and individuals involved in these events.
.
choose the primary event type to represent the desired task completion times.
.
choose a set of redundant event types that approximate the desired task completion times and provide redundancy are at least some times non problematic when the primary event is problematic .
.
obtain event times tikfor taskiand event type k k represents the primary event and k6 represents redundant events .
use the distribution of tikfor eachk to identify problematic values4.
denote the identi cation method as isproblematic tik .isproblematic tik should return the likelihood that the observed value tik is incorrect.
.
for task i obtain values of isproblematic tik for each redundant observation type k. .
choose from the alternative observations via the following rule correct ti arg min k isproblematic tik if isproblematic ti1 ti1if!isproblematic ti1 where the alternative kthat has the lowest likelihood of being inaccurate is chosen to represent the task completion time.
.
an illustration using mozilla data in this section we illustrate how to apply the approach described in section to identify and correct problematic issue x events.
we introduce mozilla data used in this study in section .
.
in section .
we investigate the error mechanisms in mozilla data in particular how the problems in the data can be detected and what mechanisms cause these abnormalities.
based on that understanding we describe how we identify problematic issue x dates of mozilla in section .
and how we correct such values in section .
.
we investigate the extent of data redundancy in section .
.
we compare the accuracy and the impact on model t for uncorrected and corrected values in section .
and section .
.
4for simplicity of description we talk about a single individualjikand omit the index jikfrom the discussion here.table quantiles of productivity .
mozilla data mozilla uses bugzilla as the issue tracking system its .
we use the o cial bugzilla dump provided by the mozilla community in january to conduct our analysis5.
the data includes issues reported to mozilla from september to january .
it records all activities conducted on these issues from the time somebody reported an issue until the time somebody closed it it also may remain open at the time when the bugzilla dump was created .
for each issue a sequence of events take place issue is created assigned submitted tested and resolved6.
the issue may also be reassigned its attributes changed comments debugging traces etc.
added.
each such event has an associate date time the type of action and the person performing the action.
mozilla uses mercurial version control system vcs to manage code.
we cloned code bases of all products in mozilla community and gather all available code commits in feb .
the extracted commit logs include the code committer commit time changed les and revision information of every commit.
.
investigation of error mechanism in this section we discuss quantitative and qualitative approaches to discover the mechanisms by which erroneous data can get introduced to answer rq2.
in particular we discuss how to determine if there may be a violation of productivity bound.
we also describe manual inspection of a sample of problematic events to identify probable mechanisms by which errors are introduced.
.
.
the existence of problematic data we use issue resolution events in bugzilla to calculate individual s productivity by counting the number of issues she xed each day.
more speci cally we count the number of events in which she changed the resolution of an issue to fixed.
overall we obtain positive observations each observation is a person day i.e.
the number of issues xed by an individual in one of her days active in the project and table shows the quantiles of these counts.
based on the assumption of physical constraints on individual s productivity an individual can only accomplish a certain amount of tasks in a certain time unit i.e.
a day here we are expecting a relatively low number of xes per active day with a median of and th percentile of xes per day.
however as shown in table the bugzilla records an extremely high number of xes for some individual day combinations suggesting there is a violation of the assumption of limited productivity.
it is important to note that a large fraction of all issues are xed during these most productive days.
for example the 5this bugzilla dump provided by the mozilla community has a higher quality than other snapshots retrieved on line by people .
6each resolved issue has a resolution e.g.
fixed duplicate.
as the name suggests fixed means that the issue is xed duplicate means that the reported issue is a duplicate of another issue.
640table exceptionally productive individuals based on issue fix events date userid count top of most productive person days xed issues counting for of all the issues xed.
we also calculate the number of person days during each month when one person xes ten or more issues on a single day.
as the batch xes curve in figure shows there is a signi cant increase in the number of outliers in recent years.
the kinds of charts presented in this section can be used to illustrate the extent of problems associated with any event type.
while in this section we consider issue resolution events in the later section we present the quality of last comment events and last vcs commit events.
.
.
experiment to identify causes of problematic data in this section we investigate issues where extremely high numbers of xes on a single day by a single individual are recorded.
speci cally we sample issues that are xed in top most productive person days shown in table .
we manually check the comments of these issues and try to nd out the reasons why these individuals can achieve such high productivity.
according to our investigation we nd three possible reasons for these outliers.
development process tracked by other system.
one possible reason for the outliers is that some issues are tracked in other system.
speci cally in mozilla the development process of many issues is tracked by an agile development system called pivotal tracker.
when the issue is marked as accepted in pivotal tracker it indicates the completion of xing activities.
at the same time a comment suggesting that the work on the issue has been accepted is automatically created in bugzilla but the issue status is not immediately changed.
instead the issue status in bugzilla is changed manually to fixed some time after it is accepted in pivotal tracker.
in this case there is no more work on the issue after the issue is accepted in pivotal tracker.
therefore the timestamp of the last comment before the issue is marked as xed in bugzilla i.e.
the timestamp of the comment created by pivotal tracker is a more accurate estimation of issue x date.
on bugzilla recorded that the person with id xed issues.
it turns out that most of these issues are tracked by pivotal tracker and they have been accepted in that system.
the person id simply changed the resolution of these resolved issues to fixed.
so the logins and timestamps that mark the issues as xed recorded in the its system do not represent actual xers or x dates for those issues.
dormant issues.sometimes when an issue receives no attention for a long time bugzilla administrators will decide to re evaluate the issue and possibly change the state to xed.
in this case the real working process stops at the time when the last comment before the issue is marked as xed is created.
therefore the timestamp of the last comment before the issue is marked as xed is a more accurate estimation of issue x date.
on bugzilla recorded that the person with id changed the resolution of issues to fixed.
when we check these issues we nd that these issues have not been touched for years.
this person was actually re evaluating the issues and changing the status to fixed.
closing issues with committed patches.
in some cases a patch is rst checked into the vcs and the committer leaves a link in the associated issue of bugzilla pointing to this code commit.
after that someone not the committer adds another comment noting the fact that the issue was xed with the same link pointing to the code commit and then mark the issue as fixed.
in this case the timestamp of the prior to the last comment appears to be closer to the real x date.
on bugzilla recorded that the person with id marked issues as fixed.
these issues are xed with a link pointing to a commit in the vcs.
the patches for these issues were rst checked in and later these issues were marked as xed with a link pointing to those patches.
again the logins and timestamps that are recorded to mark the issues as xed do not represent the actual xer and x date for those issues.
in summary we found three mechanisms that led to incorrect date or actor.
in all of these cases a better proxy for task completion time appears to be the date of the last comment or of the last code commit date associated with the issue.
following the method described in section we use the last comment as the alternative event to perform correction described in the next sections.
.
method to identify problematic data in this section we elaborate on implementing the problematic data identi cation method introduced in section for issue x dates in mozilla.
we model the number of issues xed by an individual each day using a mixed poisson distribution.
as discussed in section the intensity of the task completion events may vary depending on the task di culty therefore each day dwould produce a poisson random variable with a di erent intensity d. our observations therefore would represent a mixture of poisson random variables with di erent intensity parameters d. furthermore we do not consider inactive days7 obtaining a zero truncated distribution.
a commonly used mixing distribution for the intensity of poisson random variables is gamma distribution.
the resulting mixture distribution is a zero truncated negative binomial distribution.
we use r function zanbi from package gamlss.dist to match quantiles of the observed person day x counts to the truncated negative binomial distribution and found that parameters and with 1e to en7many participants in open source project such as mozilla may not work full time on the project and there are many other tasks apart from xing issues so counting days with zero xes would also include days when the individual may not have spent any e ort on xing issues.
641figure batch x batch last comment batch code commit in each month able zero truncation matched closely the th and th quantiles corresponding to counts of two and three respectively .
based on these parameters for the truncated negative binomial distribution the probability of observing a count of or larger is less than e .
we therefore use as the bound to determine physical productivity i.e.
we consider all events where an individual xes more than issues in one day as problematic.
using this approach we identify of observations as problematic.
.
the choice of redundant observations for data correction once the problematic data are identi ed we try to answer rq3 by selecting redundant observations for issue x dates.
the choice of redundant observations is based on the available data and on the investigations described in sections and .
.
we found two types of events containing redundant information related to the x date the last comment and the last commit.
figure shows the di erences among redundant observations.
note that the curves batch xes and batch last comments present the number of persondays per month where a single developer is associated with ten or more issue resolution and last comment events respectively.
batch commits counts person days with or more commits in vcs see section for the identi cation of problematic commits .
since we use the last commit date to evaluate the performance of the correction technique later in section4.
here we only use the last comment as the redundant event.
the last comment event itself may be problematic as described in the method section.
table summarizes problematic values obtained for the last comment events.
we can observe that while it also has some clearly problematic counts only the top two person day combinations exceed while nine such combinations exceed in the issue x events shown in table .
whats more important none of the actors in the two tables overlap suggesting that problematic observations based on the resolution change events ti1do not always coincide with problematic 8we follow johnson s recommendation to use a higher p value for statistical evidence instead of the commonly used value of because using the latter value often leads to unreproducible results .
here we use e .table exceptionally productive individuals based on last comment events date userid count observations from the last comment event ti2.
we therefore would expect that isproblematic ti2to be small for cases when isproblematic ti1is high thus providing reasonable levels of redundancy.
based on the analysis described in section .
the negative binomial distribution for the last comment event has and with the probability to observe the count of ten or more such events per personday being less than e .
.
evaluation of accuracy about of the issues are xed with a link pointing to the commit in the vcs.
the timestamp of the commit is more likely to re ect the completion of the x activities as there are no subsequent modi cations to the code in relation to the speci c x. to answer rq4 we evaluate the accuracy of corrected data by comparing the timestamp with or without the proposed correction to the timestamp in the vcs.
we apply our correction method on the issues that can be linked to code commits and calculate quantiles of two metrics to evaluate the accuracy of uncorrected data and corrected data relative to timestamps in the vcs.
we calculate both absolute errors and relative errors of uncorrected data and corrected data where absolute error jtimestamp vcs timestampj 642table absolute error quantile uncorrected corrected .
days days .
days days .
days days .
days days .
days days table relative error quantile uncorrected corrected .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
and relative error jtimestamp vcs timestampj vcs timestamp issue creation time table and show that the corrected values have lower relative or absolute errors are about more accurate than the uncorrected ones.
.
investigation of data redundancies although we get a more accurate estimation of the task completion time as discussed in section .
a better understanding of the redundant data is needed.
in this section we investigate the extent of data redundancy to answer rq5.
we use comment activities in bugzilla to calculate individual s productivity by counting the number of last comments she made the number of comments she made that are the last comment of an issue each day.
the number of last comments per day per person like the number of issues xed should not exceed a certain level.
we consider the last comment date as problematic if the commenter made or more last comments per day.
among the issues issues .
are identi ed to have problematic last comment dates.
among the issues whose x dates are identi ed as problematic .
have problematic last comments.
the result shows that the redundant observations are more likely to be problematic if the primary observations are problematic fisher test p val 1e .
despite that they provide ample redundancy with more than observations not marked as problematic when the primary events are problematic.
.
impact on model of issue resolution interval we further evaluate the approach by modeling time until x using corrected and uncorrected data.
while we already established that the errors cover a substantial portion of all data and are relatively large the statistical models tend to be quite robust and may not change substantially even if the data were to be corrected.
if we observe substantial di erences in the model after data correction this would suggest a positive answer to rq6.
we t two models to conduct the evaluation.
first we use a variety of predictors reported in the literature to explain the time until x and report how the results change after correction.
second we replicate an existing study that reported four predictors to be statistically signi cant.
the observations for the models are issues resolved withthe resolution fixed and with their x dates identi ed as problematic by the approach described in section .
.
we use this subset to focus on revealing the potential problems resulting from uncorrected data.
for all problematic issue x dates the redundant observations i.e.
the timestamps of the last comment were available and were used in data correction as shown in equation in section .
we also remove issues created after as many of them may still be open at the time mozilla dump was created.
finally we remove outlier issues that take more than one year to x. after these lters are applied we have issues for the regression models.
prior studies have used various issue report attributes for predicting time until x. developer reputation issue severity number of assignees attachments and dependencies are used by bhattacharya et al .
the number of comments is used by hooimeijer et al .
issue priority issue severity are used by giger et al .
to replicate previous work on predicting time until x we t the following multivariate linear regression model ln days severity ln attachments reputation ln assignee ln depends priority late ln comments resolver lastcommenter the response variable is the natural logarithm of time until x measured in days.
the predictors include severity the issue severity recorded in bugzilla.
the issue severity shows the impact of an issue with seven levels blocker critical major normal minor trivial and enhancement.
more impactful issues are more likely to be resolved sooner so we expect that as severity decreases the issue resolution time progressively increases as well with larger estimated coe cients.
attachments the number of attachments associated with the issue.
more attachments may indicate a better documentation for the problem resulting in a shorter time until x. depends the number of other issues that the issue depends on.
if an issue depends on other issues the processing of the issue may be blocked until other issues are resolved.
such blocking should increase the time until x. assignee the number of assignee changes associated with the issue.
an increase in the number of assignee changes should increase the issue x time as observed in e.g.
.
priority the issue priority recorded in bugzilla.
this metric indicates the priority of the issue including six levels unassigned p1 p2 p3 p4 and p5.
higher priority should be associated with shorter time until x. reputation reputation of the reporter which is de ned as reputation issues xed reported by the person issues reported by the person issues reported by reputable people tend to take less time to x. late whether the issue is reported in the later period late 1if create date 0otherwise this variable is used to describe how issue x time may vary over time.
643comments the number of comments associated with the issue.
the number of comments re ects the attention that the issue receives which should a ect the time until x. more comments may indicate complicated discussion or difculty replicating the issue resulting in longer time until x. resolver whether the reporter closed the issue herself.
the fact that the reporter resolved the issue herself may indicate that she created the issue once she came up with a x. we expect such issues to take less time to x as part of the work may have occurred before the issue was reported.
lastcommenter the author of the last comment before the issue is marked as fixed.
this variable is used to adjust for the variations in practices random e ects among participants resolving the issue.
the values of predictors attachments depends assignee comments were highly skewed in our dataset we therefore take the natural logarithm of these predictors.
table regression with uncorrected data estimate p value intercept .
.
critical .
.
major .
.
normal .
.
minor .
.
trivial .
.
enhancement .
.
ln attachments .
.
ln depends .
.
ln assignee .
.
reputation .
.
p1 .
.
p2 .
.
p3 .
.
p4 .
.
p5 .
.
ln comments .
.
resolver .
.
late .
.
regression results are shown in tables and .
the second column shows the estimated coe cients for the predictors and the third column shows the p values.
the values of coe cients are the estimated e ects that di erent predictors have on the issue x time measured by days.
for example as shown in table the regression model predicts that a decrease in the reputation by from the median reputation of will result in an increase of time until x by exp .
the positive coe cients for depends assignee comments the negative coe cients for reputation andresolver and the coe cients for di erent priority and severity levels match our expectation of the model as discussed above.
note that the negative coe cient for attachments suggests that an increase in the number of attachments will reduce the time until x supporting our hypothesis that many attachments may make the issue easier to reproduce and therefore to x. also the severity coe cients are more reasonably ordered for the corrected data than for the uncorrected data with the order blocker critical major normal minor trivial andenhancement taking increasingly longer times to resolve.
for uncorrected data we seetable regression with corrected data estimate p value intercept .
.
critical .
.
major .
.
normal .
.
minor .
.
trivial .
.
enhancement .
.
ln attachments .
.
ln depends .
.
ln assignee .
.
reputation .
.
p1 .
.
p2 .
.
p3 .
.
p4 .
.
p5 .
.
ln comments .
.
resolver .
.
late .
.
table comparison of statistical signi cance predictor original uncorrected corrected assignee signi cant signi cant signi cant severity signi cant signi cant signi cant depends signi cant signi cant signi cant attachments signi cant signi cant signi cant trivial severity issues taking less time to resolve than minor or normal severity issues a questionable result.
we perform the diagnostics on the regression with corrected data and the quantile quantile plot shown in figure suggests approximate normality of the residuals.
the residual plot not provided for the lack of space does not indicate non homogeneous variance.
the correction of data makes a substantial di erence when modeling the time until x. after applying the correction the coe cients for four variables switch between being significant and not being signi cant for p value implying that the correction of data changes the model substantially.
the adjusted r square increases from to suggesting the corrected model ts about better than the uncorrected one.
for comparison table shows the published models of time until x that are replicated in this study.
only reputation predictor becomes statistically signi cant in our model the ndings for the remaining predictors are replicated.
however it s di cult to replicate the studies that did not report the e ects of the predictors.
the e ects of attachements andcomments reported by are in contrast to ndings in our model perhaps because of di erent practices of projects studied there.
to replicate the results on the same dataset we use the model presented in .
predictors used in this model include the number of assignees issue severity issue dependencies and the number of attachments.
we t the model with uncorrected and corrected data with results shown in table .
all four predictors are signi cant replicating the original results.
however the sign of the coe cient for attachments which were not reported in the original study ip from negative to positive after applying our data correction method 644table prior work on time until fix paper studied projects predictor e ect on time until x signi cance chrome eclipse firefox assignee na yes seamonkey thunderbird severity na yes attachments na yes dependencies na yes reputation na low correlation firefox attachments yes comments yes severity higher sev indicates shorter time yes reputation na no eclipse priority na na severity na na dependencies na na comments na na eclipse mozilla gnome priority na di er in projects severity na di er in projects comments na di er in projects figure quantile quantile plot of regression residuals indicating an opposite e ect of this predictor.
furthermore the r square increases from .
to .
after applying the correction method suggesting a much better t of the corrected data.
study by hooimeijer et al.
however shows that the presence of attachments is correlated with a longer time until x. .
limitations our approach has a number of limitations.
we discuss issues related to the ability to apply our method limitation related to the identi cation of incorrect values and other potential issues.
general method.
in this study we use physical constraints e.g.
individual productivity bound that exist in software development to identify problematic values and to correct such values by using redundant observations that are more likely to be correct.
however not all data may have such physical bounds or the analysts may not be aware of the bounds even if they exist.
as noted in the section describing the method such bounds may be estimable from the observations directly.
furthermore not all events may have redundant counterparts in operational data.method for identifying problematic data.
we discussed the method of identifying problematic issue x dates in section .
.
however the cut o bound is set based on the likelihood of observing problematic data.
although this is good enough for identifying major outliers it may fail to detect minor outliers in our dataset.
moreover a universal bound for all users in all periods of time may not be optimal.
using a mixture model described in section may help identify the bounds more precisely and potentially suggest other distributions as being more appropriate than the truncated negative binomial distribution we used.
besides we only consider a single event type and a single task issue x. other tasks also require e ort and may be done by the same individual.
using that additional information may help further tighten the capacity constraints resulting in an even more accurate identi cation of problematic data.
insights for practice.
despite the dramatic di erences between the models trained with corrected and uncorrected data practical predictability of the two models have not been tested.
further empirical investigation is needed to fully assess the di erences between models trained with corrected uncorrected data.
645table exceptionally productive individuals based on code commit events date userid count bobby holley ms2ger gregory szorc b2g bumper bot ms2ger ms2ger xbld xbld xbld xbld finally our method only corrects task completion time after identifying problematic task completion time and individuals.
a careful selection of redundant data is needed for correcting problematic task completion individuals.
.
generalizations in this section we investigate rq7 in particular we discuss how the method may apply to event streams other than its event streams and how the redundant observations may be chosen from an event stream associated with another system.
if an individual performs a task that takes nonzero effort we can use an event corresponding to the time of task completion recorded in operational data as a proxy.
an arbitrary event or a function calculating a metric from a single or multiple events recorded in operational data could be chosen with the best proxy depending on the type of task the practices of using tools and the nature of the operational data.
for example the issue reporting is also a task that requires nontrivial e ort.
the task completion time in issue reporting is the date when the issue is created if we ignore subsequent interactions with developers or triagers .
we can look at issue reporting event stream and determine problematic values as well.
the number of issues reported per day per person like the number of issues xed should not exceed a certain level.
an extremely high number of issue reports may indicate that the reported issues were imported from other its and therefore the recorded reporters and recorded timestamps may be unreliable.
in mozilla community based on this constraint we count the number of issues reported by each user on each day and the ten most productive person days are shown in table .
while we argued that the last commit date better represents issue x completion it is also not free from problems.
in particular we count the number of code commits by each person on each day and present the results in table .
it shows that code commit data from vcs also violates the capacity constraints.
the truncated negative binomial distribution with parameters and matches the observed quantiles of the commit date reasonably well.
in figure we used the daily commit count of as a cuto representing tail probability of less than e forbatchcommits .
the identi cation of problematic issue report data and code commit data show that the approach may indeed be generalizable to other types of events and tasks.
note however that the alternative ways to identify problematic data should also be employed.
for example many of top commit days are associated with login ffxbld firefox build .table exceptionally productive individuals based on issue report events date userid count clearly this administrative login does not make all these commits manually.
these are likely to be mostly scriptgenerated commits that should be identi ed prior to applying our data correction approach.
in summary based on the capacity constraint we identify likely incorrect data for four types of events the issue report last comment issue x and code commit.
.
conclusions operational data may not always accurately re ect the phenomena of interest thus misleading research wasting developers e ort and time and causing quality problems in software development.
in this study we proposed an approach to identify and correct problematic event data based on the individual capacity constraints and redundancies present in operational data and used mozilla its and vcs to illustrate how the approach could be applied in practice.
in particular we found that batch xing is a common mechanism for erroneous data and we used alternative observations of the last comment posted for an issue to correct the issue resolution dates.
by comparing corrected and uncorrected values to the last commit date in the vcs we found corrected values to be more accurate.
we replicated the model commonly found in the literature to predict time until x and found both its structure and the t to change substantially after data correction.
we identi ed likely incorrect observations for other types of events e.g.
issue reports and code commits to illustrate how the approach could be generalized.
it is important to note that capacity constraints are not the only constraints that exist in software development.
it may be possible to exploit other constraints to identify and correct problematic data and through that to improve e ectiveness of software development.
detailed