precise semantic history slicing through dynamic delta refinement yi li university of toronto toronto on canada liyi cs.toronto.educhenguang zhu university of toronto toronto on canada czhu cs.toronto.edujulia rubin mit cambridge ma usa mjulia csail.mit.edu marsha chechik university of toronto toronto on canada chechik cs.toronto.edu abstract semantic history slicing solves the problem of extracting changes related to a particular high level functionality from the software version histories.
state of the art techniques combine static program analysis and dynamic execution tracing to infer an over approximated set of changes that can preserve the functional behaviors captured by a test suite.
however due to the conservative nature of such techniques the sliced histories may contain irrelevant changes.
in this paper we propose a divide and conquer style partitioning approach enhanced by dynamic delta re nement to produce minimal semantic history slices.
we utilize deltas in dynamic invariants generated from successive test executions to learn signi cance of changes with respect to the target functionality.
empirical results indicate that these measurements accurately rank changes according to their relevance to the desired test behaviors and thus partition history slices in an e cient and e ective manner.
ccs concepts software and its engineering !software con guration management and version control systems dynamic analysis software evolution keywords semantic history slicing program analysis dynamic invariants software con guration management.
.
introduction software con guration management systems scms are widely used in software development practices.
these systems e.g.
git svn mercurial are useful for capturing incremental changes made by developers examiningor reverting changes identifying developers responsible for a speci c change creating development streams and more.
incremental changes are manually grouped by developers to form commits a.k.a.
change sets .
commits are stored sequentially and ordered by their time stamps so that it is convenient to trace back to any version in the history.
yet the sequential organization of changes is in exible and lacks support for many tasks that require high level semantic understanding of program functionality .
for example developers often need to locate and transfer functionality from one branch to another either for porting bug xes or for splitting large chunk commits into multiple functionally independent pull requests.
identifying failureinducing changes in version histories is another challenge that developers face in their work.
semantic history slicing.
semantic history slicing identi es a set of commits in a change history that relate to each other based on a certain criterion.
for example cslicer identi es and extracts a set of functionally related commits that correspond to a speci c high level functionality.
the functionality is de ned by a test suite and the sliced history has to be perfectly functional and pass all the tests in the suite.
git bisect and delta debugging isolate failure inducing changes in version histories using a divideand conquer style re nement approach where a set of commits is partitioned and tested separately until a minimal subset that exposes the test failures is found.
the biggest challenge for precisely solving the semantic slicing problem lies in the very large number of possible programs in the search space i.e.
those that are induced by all subsets of commits in the history.
a valid semantic history slice has to be e ciently discovered from the exponential number of candidates.
existing solutions approach this problem from two di erent angles.
cslicer analyzes the latest program version to collect test coverage information and then computes an over approximated set of commits that include changes to the covered elements.
cslicer trades accuracy for e ciency it executes the test suite only once but it conservatively assumes that all changes traversed by the test execution can potentially alter the test results.
this assumption results in potential inclusion of unnecessary or irrelevant changes into the history slice.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
partition execute learnhistory testsminimal slice sub history change significance dynamic invariantssignals figure dynamic delta re nement loop.
divide and conquer style techniques such as delta debugging guarantee accuracy of the result.
yet they can be very expensive to run as they execute the test suite multiple times depending on the way history is partitioned and on the order in which partitions are tested.
dynamic delta re nement.
in this paper we propose a precise semantic history slicing technique based on iterative re nement and change signi cance ranking .
we discover relevance of changes to the target tests through successive test runs and utilize this information to guide the history partition and speed up the re nement process.
we refer to this technique as dynamic delta re nement .
its key insight is that by comparing the runtime executions of two program versions before and after a change we can extract information about the precise impact of the changes at various program points.
by combining impact information with test outcomes pass or fail we are able to accurately infer the signi cance of changes with respect to the target tests.
in particular if the tests still pass after removal of a change then the removed change and its family of related changes are insigni cant to the tested functionalities.
we give more details on how such families of changes can be detected using dynamic program invariants generated by daikon in sec.
.
fig.
shows an overview of the delta re nement loop.
using the signi cance measurements of changes dynamic delta re nement is able to e ciently nd minimal semantic history slices through well informed partition schemes.
with much higher con dence changes of less signi cance are removed rst and upon success the analysis scope is reduced and the re nement continues recursively.
the results of test executions either success or failure are used to update signi cance ranking of the remaining changes.
the ranking accuracy is improved with each execution and the re nement loop terminates when the minimality condition is met.
note that the algorithm maintains a valid semantic slice throughout this process so it can be interrupted at any time and return a valid best e ort result.
contributions.
.we show how dynamic delta re nement can learn signi cance of changes with respect to a speci c high level functionality.
.we de ne minimal semantic slices and describe an algorithm for computing them based on dynamic delta re nement.
.we report on an implementation of a fully automated precise semantic history slicing tool that operates on java projects hosted in git repositories.
.we compare our technique with previous work in terms of precision and e ciency.
based on empirical evaluation our technique achieves improvement in accuracy compared with cslicer .
we also demonstrate the advantage of using change signi cance to speed up the basic partition scheme used by delta debugging.
organization.
the rest of this paper is organized as follows.
sec.
illustrates how dynamic invariants are used for learning change signi cance and how signi cance ranking is used to guide history partition.
sec.
provides the necessary background for the rest of the paper.
in sec.
we formalize the delta re nement algorithm for nding minimal semantic slices and prove its correctness.
in sec.
we describe our implementation and experimental results.
finally we discuss related work and conclude in sec.
and respectively.
.
overview of the approach in this section we illustrate our approach on two simple examples.
.
changes and dependencies example .
fig.
2a shows two versions of a java program foo.java base and nal .
the nal version introduces a few modi cations to the class bthrough a series of atomic changes .
atomic changes are de ned over the abstract syntax trees asts of the program as insertions ins deletions del or updates upd of ast nodes e.g.
elds methods etc.
speci cally there are six atomic changes between the base and the nal versions listed in no particular order an update to the eld b.x an insertion of a new eld yinto the class b an update to the eld b.s an update to the method b.g which adds an additional statement z lib ?
z m conditionally assigning the returned value of m to the local variable z an update to method b.h which replaces by !
and an insertion of a new method m into the class b. the lib method called in g represents an external library whose returned value is only known during runtime.
we do know that the library method behaves deterministically but cannot predict its return value without executing it.
the desired functionality of the program is captured by a unit test for foo.java which asserts that the returned value of the method a.f should be equal to see fig.
2b .
we denote this test by t. note that the test assertion holds in the nal version of the program but fails in the base one.
a semantic history slice is a subset of the changes which produces a well formed and fully functional program that can still pass the test.
since we only care about a subset of the program behaviors captured by the test some atomic changes are unnecessary.
in our example the minimal set of changes which quali es as a valid semantic slice isf2 6g.
the testtfails when any of these three changes is missing and passes whenever all of them are present.
other changes are either never executed or do not alter the asserted values.
interestingly the test passing property is not monotone i.e.
adding modi cations may change the tests from passing to failing.
change dependencies.
atomic changes are not completely independent from each other.
in order to construct a well formed program some changes have to be applied 496final base class a int f return new b .g class b int x string s null int g int z h s x return z int h string v int t return v null ?
t upd b.x class a int f return new b .g class b int x int y string s abc int g int z h s x z lib ?
z m return z int h string v int t return v !
null ?
t int m return y upd b.s ins b.y upd b.g upd b.h ins b.m a atomic changes.
t est class testfoo test void test a a new a assertequals a.f b target test.
c change dependencies.
figure atomic changes between the base and nal versions of foo.java .
as prerequisites for others .
for example ins b.m depends on ins b.y since the method b.m accesses the eld b.yand requires the declaration of the eld in order to compile and upd b.g depends on ins b.m since the new version of the method b.g invokes b.m see fig.
2c .
we are only interested in producing well formed programs.
the partition of changes thus has to obey the dependency relations.
that is reverting a subset of atomic changes results in a well formed program only if the remaining changes have all their dependencies satis ed.
the change dependencies can be computed systematically as we describe in sec.
.
.
.
learning change significance we now show how delta information observed from successive test runs can be used to learn signi cance of atomic changes with respect to a target test suite.
in example the target test tpasses in the nal version.
we can use this information to establish facts about the program variables at various program points by generatingdynamic invariants likely invariants that may not generalize but that hold for the exercised test executions.
for simplicity we refer to them as invariants from now on.
for instance b x is a eld invariant which indicates that the value of the eld b.xequals to during the initial execution.
another example is a.f return which is a method post condition asserting that the return value of a.f is .
we denote by h the set of reverted changes ithe initial set of invariants for the nal version of the program and i0the invariants after changes are reverted.
the row h of fig.
shows four possible cases of reverted changes in example .
the deltas in the generated invariants before and after reverting changes is shown in row ini0 of the table.
the rows t h and signals show the test outcomes and signi cance signals learned for each case respectively.
we discuss each case in turn below.
case test passing without extra signal.
suppose a singleton atomic change set h f1gis reverted during the rst partition step and the new program is now equivalent to applyingh f2 6gto the base version.
the declarations and initializations of xare reverted to the base version xinitialized to instead of .static analysis is unable to determine whether this change would a ect the test results due to the undetermined returns oflib .
but we are able to precisely detect the impact of reverting 1by comparing the new set of invariants i0 generated during the actual execution of the new program to the original invariants i. in this case we observe that only one invariant disappears after reverting namely b x see row ini0 in fig.
.
this indicates that the impact of reverting 1is local to the change itself and does not ow into other program points.
in fact lib returns false at runtime and thus the change on b.xdoes not propagate through the returned value from h s x is overwritten by m which is independent of the change.
the test outcome is unchanged and therefore the value of b.xis considered insigni cant.
we decrease the signi cance score of denoted by in fig.
row signals .
case test passing with extra signals.
now suppose that two atomic changes h f1 3g are reverted together.
the initial values of both xand sare a ected x taking value instead of and sbeing initialized to null instead of abc .
this time we observe three invariants disappearing after the revert b x b s !
null and b.h i s return which involve an additional method h i s whose return value is a ected by the revert.
since the test passes again none of the three invariants inini0is consequential for the target functionality.
these include the return of b.h i s .
apart from 1and 3which are obviously insigni cant we could also infer that 5is insigni cant a local impact analysis shows that 5only a ects the return of b.h i s at this point we have determined that the change set f1 5gis insigni cant for the target test.
this information can be used to guide the partition in the next iteration by prioritizing reverting 5over the rest.
case test failing by determined causes.
when 4is reverted the conditional assignment statement z lib ?
z m in g is removed.
the test fails because now the value from h s x ows through which is di erent from the old value from m .
since an atomic change is already the smallest unit in our analysis we can pinpoint 4as the de nite cause of the test failure.
all invariants violated by the revert are directly impacted by the change and most likely cause the failure.
they are 497case case case case h ini0b x 2b x b s !
null b.h i s return 0b y one of b.g return b.m return a.f return 3b s !
null b.h i s return b y one of b.g return b.m return a.f return t h signals figure change signi cance learning case by case.
as follows b y one of which asserts that the eld yused to take both and now ycan only be and b.m return which asserts that the return of m used to be now m does not return at all .
therefore we consider both 2and which are associated with b.yand b.m respectively signi cant for the test.
case test failing by undetermined causes.
when the test fails after reverting multiple atomic changes as happens in this case the causes for the failure are undetermined.
the actual cause can be any one in the reverted changes or their arbitrary combination.
for example when h f3 4gis reverted the test fails only due to the absence of 3but we cannot infer useful signi cance information in this case.
.
history partition by significance ranking the basic idea of history partition is inspired by delta debugging .
in the rst iteration the history is split into two halves which are then tested individually.
if one of the partitions passes the test then the process continues recursively on the successful partition.
otherwise less aggressive partitions are produced by reverting fewer changes and keeping more.
for example we can split the history into four similar size change sets and revert each of them one at a time.
if none of the attempts are successful then ner grained partitions are produced until we reach a point where only a single atomic change is reverted at a time.
then we are able to classify the change precisely according to the test results.
the process terminates when a minimal history slice is found reverting any single change in the slice fails the test.
in this paper we make two enhancements to the basic partition scheme before attempting basic partitions we prioritize removal of low signi cance changes whenever possible and by precisely analyzing dependencies between changes we predict compilation errors without needing to compile the program.
example .
we use another example with a slightly more complex change history to illustrate our enhanced history partition scheme.
in this example there are eight atomic changesf1 8g adding two non essential changes and 8on top of the history in example .
the set of essential changes is stillf2 6g.
due to space limitations we omit the details of the additional changes1.
the actual steps taken when analyzing this example are shown in fig.
.
during the rst step n the history 1the code and detailed changes for example can be found at bitbucket.org liyistc gitslice wiki ase16 e2.n partition h h t signals figure enhanced history partition scheme.
is partitioned into two equal halves i.e.
f1 4gand f5 8g.
we keep one set and revert the other but only to nd that the dependencies !
2and !
are violated.
in fig.
change dependency violations are represented by in column t .
no test run is needed so far.
during the second step n we increase the partition granularity and revert two changes at a time.
reverting f3 4gproduces a well formed program but the test fails since 4is an essential change.
no signi cance signal is learned since the cause of the failure is not determined.
the test passes whenf7 8gis reverted.
the extra signals for 3and 5that we learn from the passing test allow us to lower their signi cance as well.
during the third step n we revertf3 5gas suggested by their signi cance measurements and successfully reduce the scope down to only four atomic changes.
similarly to the rst step neither half of the partition produced during step is a valid semantic slice.
therefore we increase the partition granularity again in step reverting a single change at a time.
this time 1can be reverted which leaves a valid minimal history semantic slice f2 6g.
during the nal step n the delta re nement loop terminates because none of the changes can be successfully reverted.
for this example six test runs are needed for nding the minimal solution using the enhanced partition scheme.
in contrast the basic partition scheme from without signi cance learning or change dependency analysis requires thirteen test runs and twelve additional failed compilations.
498p l l classcextendsc c f km k c c f super f this f f m c m c x returne e x e.f e.m e newc e c e figure language syntax rules .
y2v r ins x n v y v r0 v r fxgparent x y id x n x v x2v r del x v r0 v r nfxgx2v r upd x v x v figure types of atomic changes .
.
preliminaries in this section we provide background and de nitions for the rest of the paper.
language syntax.
to keep the presentation concise we step back from the complexities of the full java language and concentrate on the core object oriented features.
we adopt a simple functional subset of java from featherweight java denoting it by p. the syntax rules of the language pare given in fig.
.
many advanced java features e.g.
interfaces abstract classes and re ection are removed from p while the typing rules which are crucial for the compilation correctness are retained .
a syntactically correct program p2pconsists of a list of class declarations l where the overhead bar lstands for a possibly empty sequence l1 ln.
we usehito denote an empty sequence and for sequence concatenation.
every class declaration has members including elds c f methods m and constructors k .
a method body consists of a single return statement the returned expression can be a variable a eld access a method invocation an instance creation or a type cast.
abstract syntax tree.
a valid program p2pcan be parsed as an abstract syntax tree ast denoted by ast p .
we adopt a simpli ed ast model where the smallest entity nodes are elds and methods.
formally r ast p is a rooted tree with a set of nodes v r .
the root of ris denoted byroot r which represents the compilation unit i.e.
the programp.
each entity node xhas an identi er and a value denoted by id x and x respectively.
in a valid ast the identi er for each node is unique e.g.
fully quali ed names in java and the values are canonical textual representations of the corresponding entities.
we denote the parent of a nodexbyparent x .
fig.
shows an ast for the base version of the program foo.java from fig.
2a.
change and change history.
let be the set of all asts.
an atomic change operation !
is either an insert delete orupdate see fig.
.
it transforms r2 producing a new ast r0such thatr0 r .
an insertion ins x n v y inserts a node xwith an identi er nand a valuevas a child of a node y. a deletion del x removes the nodexfrom the ast.
an update upd x v replaces the nodexwith the node v. a change operation is applicable on an ast if its preconditions are met.
for example the insertion ins x n v y is applicable on rif and only if y2v r .
insertion of an existing node is treated the same as an update.
denotes the set of all atomic changes.
foo b a f x i s s g h i s figure ast of foo.java at the base version.
ahistory is a sequence of changes h h ki.
a sub history is a sub sequence of a history i.e.
a sequence derived by removing changes from hwithout altering the ordering.
we write h0 hindicating that h0is a subhistory ofhand refer toh i jiashi j. a change historyh h 1isapplicable to rif 1is applicable to randh 1is applicable to r .
test cases.
we assume that semantic functionalities can be captured by test cases and the execution trace of a test case is deterministic.
a test casetis a function t p!b such that for a given program p2p t p is true if and only if the test succeeds and false otherwise.
a test suite is a collection of unit tests that can exercise and demonstrate the functionality of interest.
let test suite tbe a set of test casesftig.
we write pj tif and only if program ppasses all tests in t i.e.
8t2t t p .
dynamic invariants.
dynamic invariants are likely invariants that are discovered from program executions.
they assert predicates that hold during the execution at speci c program points including procedure entries and exits and aggregate program points of multiple class instances.
we are particularly interested in three types of predicates method preconditions asserting values of input parameters method postconditions asserting returned values and all values taken by elds throughout the execution.
a wide range of dynamic invariants is detected and reported by daikon but not all of them are useful for inferring change signi cance.
in particular a failing invariant which depends on two or more variables has ambiguous causes altering either one of them can break the invariant.
therefore we only consider a subset of the invariants which involve a single program variable including comparisons with constants e.g.
x k x k1 mod k2 k1 x k2 x !
null single valuedness e.g.
x has only one value and value range e.g.
x one of a b .
given two invariant sets iandi0 the invariant delta ini0 consists of all invariants in ithat are not implied by any invariant in i0.
formally ini0 fi2ij 9i02i0 i0 i g. precise semantics preserving slice.
consider a programp02pand itsksubsequent versions p1 pksuch that they are all well formed.
let hbe the original change history from p0topk i.e.
h1 i p0 pifor all integers i k. lettbe a set of tests passed by pk i.e.
pkj t. definition .
semantics preserving slice .
a semantics preserving slice of historyhwith respect to t denoted byh th is a sub history h0 hsuch that h0 p0 j t. of course his a semantics preserving slice of itself.
shorter slicing results are preferred over longer ones and the optimal slice is the shortest sub history that satis es the above prop499h texecute h i h ih i i learn h h h h i i i i i i h !
!
!n !n !i !i ... ...h h partition !
!
!n !n... i i signals s figure dynamic delta re nement overview.
erties.
however the optimality of the sliced history cannot always be guaranteed by polynomial time algorithms nding it requires 2jhj tests in general.
therefore we aim at computing an approximation of the optimal solution which still has good practical precision guarantees.
we say that a sub history h ofhis a 1minimal semantic slice ifh is semantics preserving and reverting any single change in h would break the semantic properties.
definition .
minimal semantic slice .
let h be semantics preserving i.e.
h th.h is a minimal semantic slice ofhif8 2h h nf g 6j t. .
algorithm in this section we present the dynamic delta re nement algorithm for precise semantic history slicing in detail.
.
algorithm description given a history hand a test suite t to compute a 1minimal semantic slice h our algorithm iteratively goes through three phases partition execution and learning as shown in fig.
.
to implement each phase the delta re nement algorithm maintains three data structures h the current minimal semantics preserving history slice which is always an over approximation of the minimal solution and can be returned as a sub optimal solution if the re nement process terminates prematurely i the set of dynamic invariants generated from the last successful test execution and updated after every successful run s !r the change signi cance ranking a function from atomic changes to real numbers updated according to the outcomes from the execution phase.
the algorithm is presented in fig.
as a set of generic rules specifying the minimal requirements for each phase.
initialization.
theinit rule executes tests on the nal versionh p0 and collects dynamic invariants i inv h t .
it also initializes h to be the input history h and initializes signi cance scores for all atomic changes in hto zero.
partition.
this phase receives a history hand splits it into two non empty sub histories h andh .
the split can be either random or guided by a signi cance ranking of atomic changes.
the two rules for this phase par rand andpar sig govern the behaviors of two di erent partition schemes.initialization init h t h h8 2h s 0i inv h t partition jh j 1par rand h h h h h h h h i j2h s i s j par sig h s h s s s i h h nh execution and learning h j t i0 inv h t pass h h t ini0 h h i i0 h 6j t i0 inv h t jh j 1fail h h t ini0 h h i i h 6j tjh j 1fail h h t h h i i figure the dynamic delta re nement algorithm.
the random partition splits the current minimal semantic sliceh into two non empty sub histories h andh randomly when the length of h is greater than one.
then h is kept while h is reverted.
we adjust the relative sizes ofh andh to balance between progressions upon test success and chances of successes.
for example a smaller h can reduce a larger chunk of non essential changes if the tests pass but it usually has a lower chance of success assuming essential changes are uniformly distributed.
in the actual implementation we gradually increase the size of h when test fails and decrease it otherwise.
the signi cance guided partition scheme splits the history according to signi cance ranking of changes such that all changes inh have higher or equal signi cance score than those inh .
with accurate signi cance ranking reverting non essential changes can be very e ective.
in practice we apply par sig rst whenever possible as it has a higher chance to produce more accurate splits.
execution.
the execution phase receives a valid partition h h and executes tests tonh p0 written as h afterwards .
the dynamic invariants i0generated from the execution are compared with iwhich is generated from the last successful test run.
an invariant delta ini0and a test signal are passed on to the learning phase.
learning.
the learning phase infers signi cance of individual atomic changes according to the invariant deltas and the test signals.
there are three rules for this phase pass fail andfail controlling how the signi cance ranking is updated under di erent circumstances.
whenh passest the pass rule applies.
we use the invariant deltas to match each a ected variable and program point involved with atomic changes that might be the cause.
this matching step is performed using a simple local static change impact analysis .
for each a ected method postcondition we collect all statements within the method body that have potential impacts on the method return e.g.
changed value ows into the return .
for instance using a simple backward dataow analysis the invariant b.g return in example is matched to 4which 500directly updates the returned variable z. similarly for each method precondition we consider every call site and collect statements preceding the method invocation which potentially impacts the corresponding input parameters.
finally for invariants on elds we analyze all eld access sites and perform a similar backward analysis.
the signi cance of each matched change is decreased.
we update h toh and recursively apply partition rules on h .
whenh failst either fail orfail applies depending on the size of h .
ifjh jis greater than one as discussed before the cause of test failure is not determined.
we do not infer change signi cance in this case fail .
otherwise we perform a similar analysis as in pass and increase the signi cance scores of the related changes fail .
termination condition.
the algorithm never attempts the same partition h twice and it terminates whenever h becomes empty or the minimal condition de ned in de nition is met 2h h nf g 6j t. .
soundness and completeness the following theorem states that the algorithm is sound.
theorem .
soundness .
given a history hand a test suitet if the delta re nement algorithm terminates then h is a minimal semantics preserving slice of hwith respect tot.
the soundness of the algorithm is straightforward.
since h is only updated when tis passed h is always a valid semantics preserving slice.
the termination condition guarantees that it is also minimal.
as presented the generic partition rules are non deterministic.
to ensure termination we impose a notion of fairness on partition schemes.
a fair partition scheme guarantees that a singleton partition for every atomic change in h is eventually reverted after very update of h .
the following theorem states completeness of the algorithm.
theorem .
completeness .
given a history hand a test suitet the algorithm using fair partition schemes always terminates with nitely many rule applications.
to see this suppose the algorithm does no terminate.
since h has nite number of changes initially and its length is monotonically decreasing jh jhas to eventually stay constant.
because of the fairness condition every atomic change inh is eventually reverted and tested.
if none of the tests pass then the minimal condition is met.
if one of the tests passes then jh jshould decrease.
both cases lead to contradictions.
.
ev aluation in this section we describe our implementation of a minimal semantic slicing tool based on dynamic delta re nement algorithm.
we evaluate our implementation w.r.t.
both its precision and performance using a benchmark suite obtained from real open source software repositories.
.
implementation we implemented the delta re nement algorithm as a fullyautomated precise semantic history slicing tool definer for java projects hosted in git repositories.
we describe the implementation and some of the applied optimizations below.
p0 p1 p2 p3 a.f a.g b.h b.g del3 upd2 ins1 updfigure analyzing change dependencies.
change dependency analysis.
to avoid running into compilation errors we perform a pre analysis for each version in the history and compute direct dependencies for all changed ast nodes.
this analysis produces a multi version change dependency graph as shown in fig.
.
in this example there are four program versions i.e.
p0 p1 p2andp3 all of which are well formed.
there are three changed nodes i.e.
methods a.f anda.g which belong to class a as well as b.g which belongs to class b. there is also a xed node b.h which stays unchanged.
class bis a sub class of a. each node has a separate time line on which its changes are labeled.
in particular a.f has an update betweenp1andp2 a.g is inserted between p1andp2 and b.g is updated after p0but deleted after p2.
in fig.
solid arrows represent necessary dependencies while empty arrows represent su cient dependencies.
for instance a method invocation of g inb.h makes b.h necessarily depend on b.g beforep2.
but when g is introduced in the super class ain versionp2 both de nitions of g are su cient dependencies of b.h i.e.
existence of either one of them would satisfy the compilation requirement due to method inheritance.
this graph is useful for predicting compilation failures without actually compiling the program as long as atomic changes for an ast node are reverted sequentially.
for example 2cannot be reverted from p3alone because both a.f and b.h necessarily depend on it.
but f1 4g can be reverted together since a.f no longer depends on a.g and the recovered b.g substitutes the dependency forb.h .
we build the multi version dependency graph incrementally.
a complete dependency graph is built by rst analyzing the base version and for subsequent versions it su ces to analyze only the changed classes.
git adaptation.
the generic algorithm discussed in sec.
operates on the level of atomic changes.
to work with git we treat the set of atomic changes belonging to the same commit as a bundled group.
the partition algorithm is adjusted such that changes in the same group always stay together and the signi cance score for a group is computed as the sum of the scores of its members.
apart from dependencies between atomic changes we also analyze dependencies between commits which are also known as the hunk dependencies .
hunk dependencies for a commit are prerequisites which have to be in place so that the target commit can be applied without causing a git merge con ict.
partitions of commits which do not comply with the hunk dependencies are discarded immediately without running any tests.
lazy dynamic tracing.
the software projects we experimented with are relatively large in size to kloc .
instrumenting the entire project and tracing test execution 501table statistics of tested software projects.
projects files loc c 1y c 4m io .
collections .
math .
end to end is often impractical.
but since we are only interested in the local direct impacts of changes which give clearer signals for signi cance we can simply trace classes that have changed during the input history.
tooling.
we use jgit a java implementation of git for repository manipulation and commit level hunk dependency analysis .
we use a modi ed version of changedistiller for extracting ast level atomic changes from git commits.
we also use the apache byte code engineering library bcel to analyze dependencies among atomic changes daikon for dynamic invariant detection and soot for performing local change impact analysis.
definer is written in java and yields fully automated analysis of projects built with maven .
the source code ofdefiner and all benchmarks used in our experiments are available online bitbucket.org liyistc gitslice.
.
experiments the goal of our empirical evaluation of definer is to answer the following research questions rq1 how does the precision of history slices produced by definer compare with those produced by cslicer ?
rq2 how e ective is change signi cance ranking for guiding history partitions when compared with the basic partition scheme used by delta debugging?
rq3 how do di erent partition schemes a ect the performance ofdefiner ?
.
.
subjects we tested definer on a benchmark consisting of eight target functionalities selected from three open source projects namely apache commons io library io apache commons collections library collections and apache commons mathematics library math .
these projects are all written in java and their development histories are freely accessible online.
they are also actively developed and maintained so that there are an abundance of new functionalities e.g.
features bug xes and improvements to choose from.
statistics about each project is shown in table .
columns files and loc show the number of java les and the total lines of code respectively.
column c 1y shows the number of commits between and for each project.
column c 4m shows the average number of commits over a month period.
in order to test the history slicing capabilities of definer all of the experiments require an original history segment h and target test suite t designated for certain high level functionality.
we randomly selected target functionalities and corresponding history segments based on commit messages and testing documents.
in particular we looked for commits which are accompanied by test suites intending to validate a functionality and selected such commits as the end points of the history analysis scope.
according to our experience the lifetime of a functionality typically spans around commits which correspond to a period of months for a project under active development.
i1 i2 i3 c1 c2 m1 m2 m305101520253035relative slice sizes definer default cslicerfigure sizes of slices de ner vs. cslicer.
therefore the length of hwas decided based on two factors each program version in the chosen history should be wellformed and compilable as assumed by our change dependency analysis and the average history length of a subject matches the average number of commits during a month development period .
commits .
surprisingly enough many versions in math and collections did not compile.
even though non compilable code does not a ect the the correctness of definer it could a ect the soundness of the change dependency analysis and thus definer s e ciency.
because of that we test definer under the well formedness assumption and merge problematic commits with their children whenever possible to form a larger commit that lead to compilable versions.
the details about each experimental subject are given in table .
column id lists the subject identi ers.
column end point shows the target commits which correspond to the nal version in our analysis scope.
columns jhj and jtj show the length of the original history segments and the sizes of the target test suites respectively.
column jh j shows the length of the veri ed minimal history slice.
.
.
results we conducted three experiments to address our research questions.
the experiments were conducted on a desktop computer running linux with an intel i7 .4ghz processor and 16gb of ram.
the results of the experiments are described below.
experiment .
the rst experiment aims to compare definer with cslicer in terms of the precision of the produced history slices.
we use the default con guration of definer default which adopts a simple partition scheme that reverts negative scored commits rst whenever possible.
the relative slice size for each subject is computed as the length of the produced history slice divided by the original history length i.e.
jh j jhj.
the results of the comparison are shown in fig.
.
the history slices found by definer are always shorter or equal to those computed by cslicer on average .
shorter .
in fact all slices produced by definer are veri ed to be minimal while cslicer does not guarantee minimality.
for example out of commits from the subject c2 definer nds a slice of length which is much shorter than reported by cslicer .cslicer took .6s to nish on average while definer took .7s.
we consider this performance overhead to be reasonable since history slicing is often performed as an o line maintenance task.
experiment .
the second experiment evaluates the effectiveness of using change signi cance ranking and change 502table experimental subject details and descriptions.
projects id functionality descriptions end point jhj jtj jh j ioi1 byte array output stream i2 identi es broken symlink les b9d4976 i3 le name utilities 63cbfe70 collectionsc1 index of function in iterable utilities 90509ce8 c2 union function in set utilities 9314193c mathm1 error conditions in continuous output eld model 6e4265d6 m2 construct median with speci c estimation type a f37e0 m3 large samples in polynomial tter b07ecae3 i110 i220 i350 c1 c225 m15 m25 m3basic learn default figure history reduction per test run.
dependency analysis in speeding up the delta re nement loop.
we compared three con gurations of definer in terms of the number of test runs needed to achieve minimal solutions default as described before learn which only enables change signi cance learning and disables compilation failure prediction based on change dependency analysis and basic which also disables signi cance learning and thus is e ectively equivalent to delta debugging which applies the basic random partition scheme2.
all con gurations still apply hunk dependency analysis which predicts git merge failures without actually picking the commits.
the results of the comparisons are shown in fig.
where the length of h y axis is plotted as a function of the number of test runs x axis used so far.
in general default andlearn require fewer test runs than basic to reach the minimal solution.
in most of the cases the advantage of signi cance learning is obvious especially for cases such as i1 m1andm2where learn requires on average only about of test runs compared with basic .
in addition change dependency analysis which prevents test from running on noncompilable programs helped extend this advantage further it only takes about of test runs.
there is only a single case i3for which the basic partition scheme performed much better it was able to reduce a large chunk of commits quickly at the 17th test while the other two con gurations did not reach minimal solution until the 88th test run.
a closer look at i3reveals that the minimal slice contains only out of commits and most of the random partitions ended up being successful.
in contrast thedefault partition scheme is more conservative and progresses more slowly in this case.
yet when the minimal slice is relatively large for example in i1 out of and m1 out of the signi cance guided partition is much more e ective.
2we could not directly compare with the original implementation in which does not work with git repositories.table comparisons of di erent partition schemes.
id neg nonpos low combined i1 i2 i3 c1 c2 m1 m2 m3 experiment .
we also experimented with three di erent partition schemes namely neg nonpos low and their combination combined .
recall that commits with positive signi cance scores are likely relevant to the target tests and vice versa commits which do not have a score assigned cannot be classi ed until new signals become available.
all schemes follow the general steps described in sec.
.
with di erent partition priorities at the beginning of each iteration.
the neg scheme only reverts commits which have negative scores.
it is the most conservative one among the three.
nonpos is the most aggressive one which reverts all commits with non positive scores.
low always reverts the lowest one third of the commits according to their signi cance ranking.
combined attempts all three partitions whenever possible.
the results of this experiment can be seen in table where each column lists the number of test runs required to reach the minimal solution and the best con gurations for each row are in bold.
all three partition schemes perform well on some of the subjects.
for example low required the smallest number of test runs for i1 i2and c2.
the combined scheme achieved the best overall performance by winning in out of examples.
.
.
summary to summarize we evaluated the precision and performance ofdefiner empirically on a benchmark set of real world 503software projects.
we demonstrated that definer produces more precise history slices than existing state of the art techniques such as cslicer .
moreover in the majority of cases it outperforms the basic partition scheme used by delta debugging thanks to the change signi cance ranking learned during the re nement process.
with all optimizations combined definer achieves precise slicing results in an e cient manner.
.
related work our work intersects with di erent areas of research.
in this section we compare our dynamic delta re nement algorithm with related work.
history understanding and manipulation.
there is a large body of work on analyzing and understanding software histories.
the basic research goals are retrieving useful information from change histories to help understand development practices localize bugs and support predictions .
li et.
al de ned the problem of semantic history slicingand proposed an algorithm cslicer which conservatively computes a sub history that preserves the desired test properties.
the advantage of cslicer is its e ciency it only executes the tests once and assumes all code entities touched by the tests can potentially a ect the test results.
our algorithm has stronger guarantees on slice quality and always returns minimal solutions within a reasonable amount of time.
in fact the two techniques can be combined together so that the output from cslicer is used as an input to definer to achieve both precision and e ciency.
the goal of delta debugging is to simplify and isolate a small test case from a large set of changes which can still manifest the target failures.
this problem can be considered as semantic slicing with respect to the failureinducing properties.
our delta re nement techniques use a similar divide and conquer partition process but improve its performance with the signi cance ranking inferred from previous iterations.
another interesting take on history analysis is to create exible views of the change history at various granularities instead of using the xed revision centric representation.
some notable approaches include history slicing and history transformation .
the promise of these techniques is to provide users the most convenient and e ective ways of interacting with change histories and better facilitate the speci c software evolution tasks at hand.
for example mu slu et.
al introduced the concept of semantics summarization view which clusters original sequence of commits into semantically related high level logical groups.
such history transformation operators can be instantiated with our techniques to produce high quality history visualization and understanding tools.
dynamic behavioral analysis.
through program instrumentation and execution tracing dynamic analysis techniques allow the comparison of precise runtime program behaviors.
daikon is one example of such techniques which discover likely program invariants from runtime executions.
daikon instruments the target program traces variables of interest and infers likely invariants for them.
it has been widely used for many software developing tasks including debugging regression testing bug prevention and more.diduce is another tool for dynamic invariant detection.
it trains a model for the target program by formulating hypotheses of invariants obeyed by the program and re ning hypotheses dynamically through presumably good runs.
the produced model can be used to check for potential errors in other test runs.
we use a similar idea of forming and updating hypotheses dynamically with multiple test executions.
the key di erence is that our goal is to infer change signi cance rather than program invariants.
therefore we can exploit useful information from both passing and failing runs to improve the accuracy of our signi cance model.
our work is also related to behavioral regression testing w.r.t.
the usage of test executions for exposing behavioral di erences across program versions.
we di er in how the di erences are used report comparison results to users in order to help them complete and improve the quality of existing regression test suites whereas our goal is to identify signi cant changes with the guidance from the behavioral di erences.
change impact analysis.
change impact analysis ia solves the problem of determining the effects of source code modi cations.
it usually means selecting a subset of tests from a regression test suite that might be a ected by a given change or given a test failure deciding which changes might be causing it.
research on ia can be roughly divided into three categories thestatic dynamic and combined approaches.
the work most related to ours is on the combined approaches to ia.
ren et al.
introduced a tool called chianti for ia of java programs.
chianti takes two versions of a java program and a set of tests as the input.
first by tracing test executions it builds dynamic call graphs for both versions before and after the changes.
then it compares the classi ed changes with the old call graph to predict the a ected tests and it uses the new call graph to select the a ecting changes that might cause the test failures.
faulttracer improved chianti by extending the standard dynamic call graph with eld access information.
the invariant deltas we used for locating precise impacts of changes can be viewed as a dynamic ia technique.
in fact we are not limited to using daikon for this purpose.
the performance of definer can be further improved with a custom lighter weight runtime tracing technique.
moreover the backward analysis which matches a ected program points to other related changes belongs to the static ia technique category.
a whole range of static analyses with di erent levels of precision can be integrated into our algorithm to trade o between ranking accuracy and performance.
.
conclusion we proposed the dynamic delta re nement algorithm for nding minimal semantic history slices.
we have implemented the algorithm as a prototype tool definer which operates on java projects hosted in git.
definer largely improves the precision of the history slices over state of the art techniques.
the change signi cance learning techniques are also shown to be e ective in speeding up the slicing process when applied to large scale software projects.
for future work we would like to explore the possibility of applying delta re nement to debugging and fault localization.
we also see room for improvement in terms of performance by combining di erent slicing approaches and parallelizing test executions as much as possible.
.