a synergistic analysis method for explaining failed regression tests qiuping yi zijiang yang jian liu chen zhao and chao wang institute of software chinese academy of sciences beijing china university of chinese academy of sciences beijing china department of computer science western michigan university kalamazoo michigan usa department of electrical and computer engineering virginia tech blacksburg virginia usa abstract we propose a new automated debugging method forregression testingbasedon asynergisticapplicationof both dynamic and semantic analysis.
our method takes a failureinducing test input a buggy program and an earlier correct version of the same program and computes a minimal set of code changes responsible for the failure as well as explaining howthecodechangesleadtothefailure.althoughthisproblem hasbeenthesubjectofintensiveresearchinrecentyears existing methodsarerarelyadoptedbydevelopersinpracticesincethey do not produce sufficiently accurate fault explanations for real applications.
our new method is significantly faster and more accurate than existing methods for explaining failed regression testsinrealapplications duetoitssynergisticanalysisframework that iteratively applies both dynamic analysis and a constraint solver based semanticanalysis to leverage theircomplementary strengths.wehave implementedournew method ina software toolbasedonthellvmcompilerandthekleesymbolicvirtual machine.ourexperimentsonlargereallinuxapplicationsshow thatthenewmethodisbothefficientandeffectiveinpractice.
i. introduction experiencehasshownthatsoftware updatesoftenintroduce new bugs.
therefore it is good practice to conduct regression testing during software development which determines whether new bugs have been introduced into the code with previously working functionality.
although there exist many tools to automate this process in practice e.g.
re running regression tests periodically and reporting failures as soon as they occur detecting these failures is only the first step.
the more challenging task is to identifythe relevant code changes andexplainwhy these changes lead to the failure.
this is where existing methods fall short.
althoughtherehasbeenalargebodyofworkonautomated debugging in the context of regression testing few of the existing methods are actively used by developers in practice for several reasons.
first they are not accurate enough in that faulty code changes are either missed or buried in a large number of irrelevant ones.
second the causal relationship between faulty code changes and the manifested failures are not explainedwellenough.third in manycases merelyreverting the faulty code changes is not enough because other code changesmaybe neededaswell to makethe modifiedprogram compile successfully.
due to these problems developers are forced to rely on manual efforts to interpret the failures.
we propose a new synergisticanalysis frameworkto significantly improve the accuracy of the automatically computed faultexplanations byleveragingare executionbaseddynamic analysis together with a constraint solver based semanticreplay diff code causes semantic analysis critical predicate identificationdynamic analysisfailed test t program p program p root fig.
.
synergistic dynamic and semantic analysis framework.
analysis to take advantage of their complementary strengths.
specifically dynamic analysis is effective in identifying the correlation between code changes and the manifested failure i.e.
by reverting some of these changes and re executing the program to see if it still fails but this is ineffective in identifying the causal relationship between the code changes and the failure.
in contrast semantic analysis is effective in identifying the causal relationship between the code changes and the failure but is ineffective in identifying the actually faulty code changesfrom a large number of possible ones.
by leveragingbothtypesof analysis we can locatethe rootcause more accurately as well as more quickly.
fig.
shows the overall flow of our method.
given a correct program p its faulty evolution p and a failed test case t our method first computes the code difference betweenpandp denoted .then itreplaysthe erroneous execution and obtains the failed assert condition .
here we assume the failure is modeled as a failed assertion.
once and are available our method starts the iterative steps of applying semanticanalysis and dynamicanalysis which are connected with a third component called critical predicate identification .
initially the critical predicate fed to the semantic analysis is the failed assert condition based on which our semantic analysis computes the cause causal chain of events responsible for .
in the subsequent dynamic analysis we identify among the code changes in a subset root thatis responsibleforthe criticalpredicate .
if root can be found we are done.
otherwise we identify another critical predicate from the current causes and try again.
in the end we report roottogether with all related causes and ieee acm 37th ieee international conference on software engineering .
ieee ieee acm 37th ieee international conference on software engineering .
ieee ieee acm 37th ieee international conference on software engineering .
ieee ieee acm 37th ieee international conference on software engineering .
ieee icse florence italy find4.
find.c find4.
find.c ifdef o nofollow options.open nofollow available check nofollow else options.open nofollow available false endif fig.
.
incorrect code changes reported by add on find a. present them in a tree like structure to highlight the causal relationships between the changes and the failure.
our new method has significant advantages over existing methodssuchasdeltadebugging dd anditsvariantssuch asaugmenteddeltadebugging add duetoitssynergistic application of both semantic analysis and dynamic analysis.
delta debugging in contrast relies on dynamic analysis only.
as an example consider a regression test failure in version .
.
of linux application find which has 24k lines of code and code changes since the last correct version .
.
.
fig.
2showsthecodechangeslocalizedbyadd whichcompletely missed the real bug.
although reverting these changes can modify the value of open nofollow available in functioncheck nofollow therebyavoiding the buggy functionsafely chdir nofollow it merely dodges the failure for the given test input without fixing the bug.
in contrast our new method would report the changes ch1andch2as shown in fig.
.
a careful study of the bug fix provided by the developers shows that change ch1matches the actual bug fix.
the bug is due to the fourth argument symlink handling of the function safely chdir nofollow which was ignored in the new version.
the developers fixed the bug by adding a switch statementtohandlethepreviouslyignoredargument.although changech2does not need to be reverted or modified in order to fix the bug it is still important since it explains why the faulty function is invoked in the first place.
therefore the failure explanation computed by our method is more accurate and helpful to debugging.
in addition our method identifies auxiliary code changes that need to be reverted together withch1andch2to make the modified program compile successfully in previous methods this time consuming step requires the developers manual effort.
although dd reports the two faulty code changes ch1and ch2 which is better than add since it missed them these two changes are buried among eight other code changes that areirrelevanttothefailure.thedevelopershavetosiftthrough these other changes manually to understand the root cause.
furthermore the program obtained by reverting only these ten changes cannot be compiled successfully which prevents the developers from quickly checking the correlation between them and the failure.
our new method in contrast can automatically identify the auxiliary changes needed to be reverted tomaketheprogramcompilesuccessfully.finally neitherdd nor add can guarantee that there is a causality relationship between the reportedcode changesand the manifested failure whereas our new method can.
find4.
find.c find4.
find.c ch1 ... static enum safechdirstatus safely chdir nofollow const char dest ... static enum safechdirstatus safely chdir const char dest ... ch2 enum safechdirstatus status safely chdir name traversingdown stat buf enum safechdirstatus status safely chdir name traversingdown stat buf symlinkhandledefault fig.
.
correct code changes reported by after on find a. there is also a large body of work on identifying the root cause of a manifestedfailure basedon semantic analysis only.
however the problem with these methods is that they focus only on the failures without considering the code changes between two versions and therefore do not leverage the fact that the original version can serve as a modelof the intendedprogrambehavior.
furthermore some existing methods rely on a test suite to provide sufficiently manypassingandfailingtestruns whichmaynotalwaysexist in practice.
without a golden model or formal specification the number of possible causes of a manifested failure tends to be very large since changing any part of the control or data flow along the faulty execution trace could lead to the flip of the assertion condition.
our new method in contrast can mitigate the potential explosion of possible causes of a manifested failure by restricting the analysis only to code changes committed between the two versions.
finally ournewmethodcanguaranteethatthereisnotonly correlation but also causality relation between the reported codechangesandthemanifestedfailure.wepresentthecauses causalchainsofevents thatconnectthecodechangesandthe failurein a tree likestructureforease of comprehension.each event in the causal chain corresponds to a program statement responsibleforpropagatingthefault.suchexplanationismore informativethana rankedlist ofwarningsreportedbyexisting methods it is the reason why we call our approach fault explanation rather than fault localization.
we have implemented our method in a tool built upon llvm and klee and evaluated it on a set of linux applications such as find bc make gawk anddiff.
our experimental results show that the new method is both accurate and efficient in computing faulty code changes and explainingthecausalityrelationbetweenthemandthefailures.
to summarize this paper makes the following contributions we propose a new synergistic analysis method for explaining failed regression testes by leveraging both semantic and dynamic analysis in a unified framework.
weimplementthemethodinasoftwaretoolbasedonthe llvmcompilerandthekleesymbolicvirtualmachine.
we evaluate the new method on a set of large linux applicationsand demonstrateits effectivenessin practice.
the remainder of the paper is organized as follows.
we present the overall algorithm in section ii which is then icse florence italyfollowed by a detailed description of each major component.
we illustrate our method using an example in section iii.
we discuss implementation details in section iv.
we present the experimental results in section v review related work in section vi and finally give our conclusions in section vii.
ii.
thesynergistic analysismethod algorithm shows the overall flow of our method based on three inputs the correct program p the faulty revision p and the failure inducing test input t. let be the set of code changes between pandp be the faulty execution trace and 0be the failed assertion.
the main part of the algorithm is a loop with three steps the semantic analysis the dynamic analysis and the extraction of critical predicates.
these three steps are described as follows in the semantic analysis our goal is to compute a causal chain of events explaining why the path leads to the critical predicate .
the result is a subset of executed statements denoted called a cause that forces to become valid.
we use to denote the accumulative set of all causes computed by this analysis.
the detailed algorithm is presented in section ii a. inthe dynamicanalysis ourgoalisto determinewhether is indeed the root cause.
in the context of regression testing we assume that a root cause must be one that involves some of the code changes committed between pandp .
giventhe codechangesidentifiedby semantic analysis uniontext we repeatedly execute p with different subsets of these code changes reverted to see if it can avoid the failure.
the detailed algorithm is presented in section ii b. if the current iteration of semantic dynamic analysis fails to locate the root cause we need to continue the analysis in the upstream of the cause .
toward this end we identify the set p of critical predicates which are branching conditions along the faulty execution trace that determine whether the current cause can occur.
these critical predicates will in turn be used as seeds for the next round of semantic dynamic analysis.
the detailed algorithm is presented in section ii c. if the root cause is found we return the code changes in roottogether with the relevant subset of causes in .
the reasonwhythisisabetterfailureexplanationresultisbecause in many cases merely pointing out the faulty code changes is not enough for the developers to understand how they lead to the manifested failure.
therefore we augment rootwith the relevantcausestoillustratetheircausalityrelationshipbetween the code changes and the manifested failure.
a. semantic analysis given the faulty path and the predicate the negated assert condition the objective of semantic analysis is to find out why holds in .
in other words why the execution would not lead to passing of the assertion .
our semantic analysis is based on computing the weakest preconditionof along the faulty execution path.
following thenotationofdijkstra wedefinetheweakestprecondition of a predicate with respect to an instruction sas a function mapping to the formula wp s such that wp s is thealgorithm1 explain program p program p test input t let be the set of code changes between pandp let be the faulty execution trace of p under input t let 0be the first critical predicate failed assertion initialization predicate set p 0 cause set whilep e atio slash do remove a predicate fromp semanticanalysis root dynamicanalysis p uniontext if root e atio slash then return roottogether with the relevant causes in endif p extractcriticalpredicates p p p endwhile weakestcondition satisfied before executing sthat guarantees to be satisfied after executing s. formally the weakest precondition wp overan assignment a branchingstatement and a sequence of instructions are defined as follows assignment x expr we define wp x expr .
that is each appearance of xin is replaced by the right hand side expression expr.
branchif c we define wp if c c .
sequence of instructions we define wp s1 s2 wp s1 wp s2 .
during dynamic analysis assume that the faulty execution trace is a bracketle ts1...sn a bracketri htand is the critical predicate that holds at the end of our wp computation is an iterative procedure wp s1 ...wp sn .
since actually led to the logical formula representing the intermediate result of the wp computation must become false at some point.
as soon aswp si ...wp sn becomes false we stop the wp computation and invoke a satisfiability modulo theory smt solver to compute the minimal unsatisfiable unsat core.modern smtsolverssuchas yices andz3 can produce an unsat core for a unsatisfiable logical formula which is a minimal subset of the conjunctive constraints that is still unsatisfiable.
in the context of fault localization the unsat core succinctly explains why cannot lead to .
unfortunately there still is a gap because the unsat core and the cause causal chain of events for critical predicate becausethe unsatcoreitself doesnottelluswhichprogram statements are responsible for generating the constraints in the unsat core.
therefore we need to map the unsat corebacktotheoriginalprogramstatementsbyleveragingthe so called generator instructions .
given the unsat core the generator instructions of the unsat core are all the assignmentsthatparticipatein the creationof the manifestedfailure.
intuitively these statements collectively are responsible for causing to become invalid at the end of the execution .
definition agenerator instruction of a predicate is either an assignment v exprsuch that vappears in the transitive support of or a branching condition where the predicate originally comes from.
itisworthpointingoutthat duringthewpcomputation an if c conditioncanonlyaddanewconjunctiveconstraint c to the existing formula but not transform an existing predicate.
in the running example to be introduced in section iii the generator instructions of the predicate sum at line icse florence italyalgorithm2 dynamicanalysisrecur program p set sizen if nthen execute the program p with changes in reverted return if the execution passes and otherwise endif partition intonsubsets ... n foreach subset ido ifthe execution of p with ireverted passes then returndynamicanalysisrecur p i elseifthe execution of p with i reverted passes then returndynamicanalysisrecur p i endif endfor returndynamicanalysisrecur p 2n are s2 s3 s7 s10 s13 the bold line numbers in fig.
.
we callthesestatementscollectivelya causeastheyforma causal chain of events that eventually triggers the failure.
however the first cause computed from the failed assertion 0may not be the root cause.
in regression testing if is not triggered by some of the code changes committed between the correct program pand the buggy version p we would not consider as a root cause.
the assumption is that the failure manifested in p but not in pis triggered by some of the recent code changes.
in general it is possible for the root cause to trigger the manifested failure indirectly by transitively affecting the direct cause computed from the failedassertion 0.therefore followingthesemanticanalysis we shall apply the dynamic analysis to determine whether is a root cause and if the answer is no we shall identify the new starting points for the subsequent semantic analysis.
b. dynamic analysis after the semantics analysis in section ii a produces a new cause and updates the set of causes the objective of dynamic analysis is to determine which cause together with the related code changes is the root cause.
the idea which is similar to the trial and error approach used in delta debugging is to execute the buggy program p with various combination of code changes reverted to see if the execution passes or fails.
given the set of causes discovered so far algorithm presents our approach to finding the smallest set of changes suchthattheexecutionof p withthechangesrevertedpasses.
the algorithm is recursive with an initial set of changes being uniontext i.e.
any change that appears in at least one cause and the value n .
in other words the call dynamicanalysis p in algorithm is implemented as dynamicanalysisrecur p .
specifically at line the set of changes is first partitioned into nsubsets.
then for each subset i we first executep with ireverted.iftheexecutionpasses wewould like to find out if an execution with a reverted subset of i can still pass thus we invoke the recursive call at line .
if theexecutionwithreverted ifails wetryitscomplementset i at lines and .
if no subset or its complement can make an execution pass we double the value of nso that biggersubsets can be tested.
eventually nbecomeslarger than the size of itself and it becomes the base case of the recursive function.
in lines to if the number of changes1intx 2inty c1 int m ... x ... y ... c2 m ... ... z x y c3 z x m assert z fig.
.
codesnippet cannot becompiled after reverting only c3.1inta b c d 2if a 3if b 4if c!
c end if l4 d c end if l3 end if l2 assert d fig.
.
an example for computing the critical conditions.
underconsiderationis less than n the dynamicanalysis either returnsallthechangesiftheexecutionwithreverted passes or returns if the execution still fails.
algorithm differs from the approach used in delta debugging in that it is asymmetric we consider passing executions only.asymmetric approachwouldbetoconsiderbothpassing and failing executions if an execution with reverted change cpasses cis fault related but if an execution with reverted changecstill fails cis irrelevant to the failure.
although this approachhas been employedin many priorworkssuch as deltadebugging itmayleadtosomefaultycodechangestobe missed especially whenthe failure is caused by multiplecode changes.
consider the program in fig.
as an example.
the buggyprogram p canbeexecutedwithoutfailureonlyifboth changesc2andc3are reverted.
however since a re run with c2reverted fails a symmetric approach would wrongly claim thatc2is irrelevant.
in contrast our asymmetric approach would not exclude such faulty code changes.
finally it is worth pointing out our method presented in algorithm2maynotobtaintheoptimalsolution.itisdesigned in this way to improve the runtime performance in practice because re runs can be expensive especially when the faulty code changes are far away from the failure in terms of the controlflow distance.
therefore in practice we need to make atrade offbetweentheaccuracyofthecomputationresultand the runtime overhead.
c. critical predicate identification the dynamic analysis presented in section ii b may not discover the set of code changes responsible for the failure in one shot.
in such case the current cause merely propagates the fault instead of causing the failure.
therefore we need to examine the upstream of this cause along the faulty path .
recall that in section ii a the generator instructions identified from a cause mandate the validity of a critical predicate which initially is the failed assertion .
if we make changes to at least one generator instruction the predicates can be evaluated differently.
given a generator instruction s v expr there are two ways to change s. one is to change the conditional statement that sdepends on so that s may not be executed.the other one is to change the values of the variables in expsovcan be evaluated differently.
based on this observation we now define the critical predicateswith respect to a generator instruction s. definition the predicate in a branching statement bis called acritical predicate if it has potential impact on a generator instruction tby icse florence italy directinfluence b squigglerightt bimmediatelydetermineswhether twill be executed or indirect influence b curlyrightt bdetermines whether an unexecuted statement swill be executed and sin turn redefines a variable read by t. our use of indirect influence in the above definition is similar to the potential dependence used in relevance slicing .
based on the definition the set of critical predicates of p in a cause isp uniontext t b b squigglerightt b curlyrightt wheretis a generator instruction.
consider the program in fig.
as an example.
it has an assertion failure along the execution path a bracketle t1 a bracketri ht because the value of dis not at line .
using the algorithm presented in section ii a we can obtain the first cause 0 .
assuming 0is not the root cause now we need to look for the critical predicates with respect to the generator instruction at line .
based on definition the predicate at line is critical sinceitcontrolswhetherline6willbeexecuted.thepredicate atline3isalsocritical wereitevaluatedto true thestatement at line would have been executed and cwould have been redefined.
note that we only consider the executed predicates.
the predicate at line is not considered critical because it is not executed during the test.
the critical predicates serve as new starting points for the subsequent semantics analysis as indicated in algorithm .
algorithm3 extractcriticalpredicates cause foreachsl do let sjbe the closest enclosing branch of slnot post dominated by sl p .add sj direct influence foreach variable varused bysldo let sbe the last instruction that defines varbeforesl forevery branch sjbetweensandsldo ifindirectinfluence sj var then p .add sj indirect influence endif endfor endfor endfor the pseudo code for computing the critical predicates is shown in algorithm which follows definition with the following modification.
for each generator instruction sl we choose only the immediate preceding instance of slthat is not post dominatedby sl lines because other critical predicates will be computed during the subsequent iterations of our analysis.
therefore no root cause will be missed due to this simplification.
in algorithm lines deal with the indirect influence .
here sdenotes the last instruction found in that assigns a value to the variable var.
only direct influence in a prefix denoted s1 ... sx can affect the causes generated in suffix sx ... sn .
ifsisthe laststatementbefore slthatassignsthe variablevarused bysl then during the backward analysis sandslrepresent the upper and lower bound of indirect influence respectively.
for each varused by sl the loop at line identifies every branch instance sjbetweensand sl which can indirectly affect the value of varas a critical predicate.
an example of such instance is the one at line in fig.
.
bool sorted true 2voidf intx inty intz 3intsum 4if !sorted 5if x y sum x else sum y else sum x 9if z sum z else sum z printf sum d n sum assert sum bool sorted false c1 2voidf intx inty intz 3intsum 4if !sorted 5if x y c2 sum x else sum y else sum x 9if z sum z c3 else sum z c4 printf sum d n sum assert sum fig.
.
correct and buggy programs with four code changes c1 c2 c3 c4 and failure inducing test input x y z .
stepline num.
weakest precondition wp satisfiability sum sat sum z sat z sum z sat z sum y z sat x y z sum y z sat sorted x y z sum y z sat sorted x y z y z sat sorted unsat fig.
.
applying our semantic analysis to the example in fig.
with the faulty execution trace and the critical predicate sum e atio slash .
iii.
therunningexample and comparison to existingmethods in this section we use an example to further illustrate the three steps of our new method.
we also compare our method with existing approaches to show that it can return better failure explanations in regression testing.
fig.
shows side by side a correct program left that computes max x y z andabuggyrevisionoftheprogram right with four code changes denoted c1 c2 c3andc4 respectively.
in this example the variable sortedindicates whether the three input variables have been sorted in the descendingorderand thevariable sumstoresthe computation result.
due to the code changes at lines and executing the revised program under the test input a bracketle t3 a bracketri ht forx y z leads to a wrong result sum instead of sum .
the actual code changes responsible for this failureare c2andc3.
however existing methodssuch as delta debugging may either report redundant code changes or miss the faulty code changes.
our new method in contrast can identify the faulty code changes precisely as well as explain why they are responsible for the failure.
a. applying our new method our method starts by replaying the failed test case a bracketle t3 a bracketri ht on the buggy program right .
based on the failed execution trace a bracketle ts1 s2 s3 s4 s5 s7 s9 s10 s12 s13 a bracketri ht our method performs the first semantic analysis to identify the causeof the failed assert condition sum e atio slash .
the cause returned by the semantic analysis is a minimal set of events a causal chain linking some code changes to the failed assertion.
specifically we negate the initial predicate sum e atio slash at line and compute the weakest precondition of sum icse florence italy 1 c2 2 0 c3 fig.
.
our tree likestructure for explaining the cause of the failure.
along the erroneous execution trace backwardly.
since the execution led to sum e atio slash the weakest precondition of sum is guaranteed to become falseat some point during the backward traversal.
fig.
shows the steps of this computation where the wp becomes an unsatisfiable formula at line after steps and the unsat core is shown as follows x y z sum0 sum1 sum0 y sum2 sum1 z sum2 e atio slash for ease of comprehension we have used the static single assignment ssa form in the above formula to differentiate multiple occurrences of sum.
next we map the constraints in this unsatisfiable subformula unsat core back to the program statements that produce them generator instructions .
we get a causal chain of events or a cause 0 s2 s3 s7 s10 s13 which explains why the assert condition sum failed.
constraints that are notin the unsat core are deemed as irrelevant.
however a cause returned by the first semantic analysis may not be the rootcause of the failure.
in the subsequent dynamic analysis we check if it is root cause by inspecting the code changes committed between the correction program pandthebuggyrevision p .sincec3istheonlycodechange in 0 to decide if 0is the root cause we revert the change c3 re compile andre executetheprogram.sincetheassertion still fails after c3is reverted we conclude that 0is not the rootcause otherwise reverting c3would have fixed the bug.
unlike existing methods such as delta debugging our use of thistrial and error style dynamic analysis is guided by the cause computed by the preceding semantic analysis.
since 0is not the root cause but a link between the root causeandthefailure weneedtoanalyzethechainofeventsin 0to identify other critical predicates.
a critical predicate is a branching condition whose value determines whether events in 0can occur duringthe execution.in this example the two critical predicates come from s5ands9 respectively since they determine whether assignments at lines and can be executed.
these critical predicates are new starting points for the next round of semantic analysis based on the weakest predication computation.
the two new causes returned by the subsequent semantic analysis are 1 s2 s5 and 2 s2 s9 the first of which is triggered by the code change c2.
furthermore both changesc2andc3are included in the accumulative set of discovered causes.
a subsequent dynamic analysis confirmed that reverting both c2andc3would make the failure go away.
therefore 1is the root cause.
in contrast 2 s2 s9 is irrelevant.stepc1 c2 c3 c4 p fmaxpass min fail diff 0p c1 c2 c3 c4 c1 c2 c3 c4 f f c3 c4 c3 c4 p c4 c3 c4 c3 fig.
.
steps of applying delta debugging to the example in fig.
.
to report the code changes responsible for the failure we presentc2andc3 as well as the causal chains of events causes ina tree likestructureshownin fig.
.inthisfigure nodes are the code changes responsible for triggering the manifested failure and the causal chains of events whereas edges are the critical predicates linking the causal chains together.
specifically the result in fig.
shows that the cause 1 which includes the change c2 leads the incorrectoutcome at line and the cause 0propagates the effect of c2to the failure at line which includes the code change c3.
b. comparing to other methods the reason why our method is more robust than existing methods is because of its use of semantic analysis to guide dynamic analysis and vice versa.
therefore our method can identifynotonlythe correlation butalso the causality relation between the faulty code changes and the manifested failure.
to illustrate this advantage we apply some of the existing methods to the example in fig.
and compare the results.
fig.
shows the results of applying delta debugging dd to the running example.
in columns c1 c4 the symbol means that a code change is applied to the correct versionpand indicates that the change is omitted.
therefore represents the previously correct program pand representsthe buggyprogram p .column p fshows whether the execution passed without failure or failed.
columnmaxpassshows the maximal set of changes applied to pwhile the execution still passes whereas column minfail shows the minimal set of changes applied to pwhile the execution still fails.
delta debugging starts with the correct program and the faulty program for which maxpassis empty whereas minfailis the complete set of changes.thegoalistoiterativelyreduce minfailandenlarge maxpasssuch that the difference shown in column diff is minimized.
when diffcan not be reduced further it contains the explanation for the failure.
initially the set of changes is partitioned into subsets c1 c2 and c3 c4 .
since applying c3 c4 to the correct program pcauses the execution to fail delta debugging assumesthatthe faultychangesare inside c3 c4 .therefore it decreases minfailfrom c1 c2 c3 c4 to c3 c4 and partitions c3 c4 into c3 and c4 .
since applying c4 to program pavoids the failure c4is added to maxpass.
delta debugging terminates after this step as c3 cannot be partitioned any further.
therefore diff c3 is reported as the explanation.
however this is not the correct result because if we keep the changesof c1 c2andc4in the revised version and only revert c3 the execution still leads to the assertion failure.
therefore the code changes localized by delta debugging is not accurate in this example.
icse florence italybesides delta debugging there are also fault localization methods based on dynamic slicing and symbolic techniques such as darwin .
our method is also moreaccuratethanthesemethods.dynamicslicing eliminates programstatementsthatareirrelevanttothemanifestedfailure based on computing the data and control dependence.
it is a popular technique because of its low cost but unfortunately is not accurate .
in the running example dynamic slicing wouldnotbeabletopruneawayanyoftheprogramstatements in the faulty trace because the failed assertion transitively depends on all the statements.
darwin mayproducea betterresultthandynamicslicing buttheresultisstillinferiortotheonereturnedbyourmethod because darwin does not apply semantic and dynamic analysissynergistically.specifically darwintriestoexplain failurebycomparingthe weakestpreconditionofthe assertion along the execution paths in the correct program and its faulty version.
applied to the running example it would generatetheweakestprecondition wp1inpassingexecutionas sorted z x z and the weakest precondition wp2 in failing execution as sorted x y z y z .
since all conditions in wp1 wp2 areunexplained bywp2 wp1 except for z darwin would report almost all the executed statements in both the passing and the failing executionsasfailure related.asaresult thedeveloperswould have to sift through the irrelevant code changes and program statements in order to understandthe root cause of the failure.
compared with our method exploits the use of unsat cores to prune away redundant predicates and stops the backward exploration as soon as error inducing code changes are identified.
in addition our method presents a tree based structure to illustrate the fault propogation.
iv.
computing auxiliary codechanges besides passing or failing an execution of the program p with some reverted code changes may have a third possibility theprogrammaynotbe compiledsuccessfully.considerthe example in fig.
.
when only the change c2is reverted the resulting program cannot be compiled because the variable m has not been declared before its usage.
during our study of real world regression test examples we have found that such cases are common in practice and it is time consuming for the developersto identify such code changesmanually.in this section we present a solution to this problem.
the problemwe wantto solveis formallystated asfollows.
assumep can no longer be compiled with the code changes in reverted our goalin algorithm is to find an augment set of code changes denoted such that if is reverted together with the resulting program can be compiled.
whilecomputing weensurethat remainsthesetof codechangesthathavecausedthecompilationerrorinthefirst place.
in contrast we decrease the set monotonicallyduringtherecursiveapplicationofthefunction findauxchange .
initially the valueof is the set of allchangesin p except .
since is the same as reverting these changes in p leads to p which can be compiled successfully.
the goal of each recursive call is thus trying to find a subset of that can still work togetherwith to make the program compile.
this is achieved by partitioningalgorithm4 findauxchange set set sizen assert program p with reverted can be compiled if nthen return endif partition into n subsets ... n foreach ido ifprogram p with i reverted can be compiled then returnfindauxchange i n elseifp with i reverted can be compiled then returnfindauxchange i n endif endfor returnfindauxchange 2n filea fileb begina spana beginb spanb line a 1 ... line a spana line b 1 ... line b spanb fig.
.
the template of the unified code changes.
and trying out each subset.
the invariantthat solves the compilation problem is always maintained.we note that all changes in the program p have to be considered not just the changes appearing in the faulty path .
to improve the performance we rely on the program structure to partition the change set.
for example a change often grammatically depends on the changes within the same file or function so we partition accordingly.
furthermore in algorithm we cache the results of findauxchangeto prevent redundant computation.
for example after findauxchange is computed we need to find an auxiliary set for .
in this case we invokefindauxchange becausetheprogramwith reverted can always be successfully compiled.
the set of all code changes in the program is computed using the linux utility application diff which happens to be a benchmark application used in our experimental evaluation of the new method which is used with the option uto compute the difference between two versions.
fig.
shows the template of such comparisons.
it starts with two lines of thetwo file namesundercomparison the timepartis omitted and then describes the change hunks.each change hunk starts withaline begina spana beginb spanb that specifies the starting and ending line numbers of the changes between the two files.
following the ranges are the detailed differences.that is lines begina begina spana from fileais replaced by lines beginb beginb spanb fromfileb.
during the implementation we have chosen the smallest granularity possible because the size of the change hunks may affect the precision of the subsequent analysis.
v. experiments we have implemented our method in a tool based on the llvm compiler and the klee symbolic virtual machine .
the tool called after automated fault explanation for regression testing can handle c c applications icse florence italytable i characteristicsof the benchmark applications used inourexperiments .
nameloccorrect p buggy p changefailure description reported site find a 24kv4.
.
v4.
.
71using l h produces wrong output find b 40kv4.
.
v4.
.
using mtime produces wrong output find c 40kv4.
.
v4.
.
using size produces error message bc 10kv1.05a v1.
argument processing error bug.cgi?id make 23kv3.
v3.
using r produces wrong output gawk 37kv3.
.
v3.
.
use of strtonum causes abort diff 20kv2.
.
v2.
adds additional newline that work on the llvm klee platform.
we use the yices smt solver to implement the computation of unsat cores.
during our experiments we have evaluated the failure explanation capability of our method and compared it with two existing techniques the classic delta debugging dd and a recent improvement called augmented delta debugging add in the same tool.
our experimental evaluation was designed to answer the following research questions how accurately can our new method localize the set of code changes responsible for the manifested failure?
specifically we wanttoknow a whetherthefaultycode changes will be missed and if the answer is no then b whether the faulty code changes will be buried in a large number of irrelevant code changes.
how scalable is our new method in handling real applications?
compared to existing methods such as dd and add which use dynamic analysis but not semantic analysis our use of the smt solver may bring some overhead but may also reduce the number of redundant tests.
we want to know if our method has a good overall runtime performance.
we conducted experiments on seven widely used linux applications such as find bc make gawk anddiff.
each application has tens of thousands of lines of c code and hundreds of code changes committed between the correct and buggy versions.
table i shows their characteristics including the name the number of lines of code the versions of the correctandbuggyprograms adescriptionoftheerror andthe website on which the bug was reported.
by studying the bug reports and patches proposed by the developers we identified the minimal set of faulty code changes responsible for each failureandunderstoodhowtheytriggeredthefailure.then we ran our tool on these benchmarksand compared the results of the following three methods dd add and after.
all the experiments were conducted on a computer with a .66ghz intel dual core cpu and gb ram.
a. accuracy of the failure explanation in computer aided debugging we generally expect the analysis tool to provide only hints as to where the faulty code changes are and then rely on the developers to identify the root cause from the reported changes.
therefore the quality of a fault explanation method is evaluated using the following criteria a whether the faulty code changes are included in the set of reported changes and if the answer to the previous question is yes then b whether the faulty code changes are buried in a large number of irrelevant ones.to compare the performance of different methods for each benchmark and each method we classified the result into one of the following three categories matched meaning that the reported code changes matched the actual bug fixes provided by the developers.
inthiscase thedevelopersproposedtoeitherrevertthese code changes or revise them in order to fix the reported bug.
missed meaning that reverting the code changes would not avoid the failure or merely dodge it since a repaired program simply chose a different branch and could no longer reach the buggy code.
in this case the result is not helpful to debugging.
partial meaning that reverting the code changes would make the failure disappear but the developers have decided that these are necessary changes.
instead other parts of the code should be revised to accommodate these changes.
for example in the code snippeta b assert a b changingb 3tob would cause an assertion failure.
although b 2is the actualrootcauseofthisfailure thedevelopermaydecide that the fix should be to revise a 2toa .
table ii shows the results of comparing the code changes returned by the three methods.
columns and show the name of the benchmark and the total number of code changes between the correct and buggy versions.
column shows the number of code changes localized by dd.
column shows whether the root cause is included in the reported .
columns show the result of add and columns show the result of after in the same format.
column shows the actually faulty codechangesfor each benchmarkprogram obtainedbyourinspectionofthesoftwarecodeandcomments from the developers.
the results in table ii indicate that our method is more accurate in localizing the faulty code changes.
in all cases the changes localized by after include the actual bug fixes provided by the developers.
in contrast add missed the actual bug in find a and both dd and add reported partial results on diff.
furthermore the false positives of after are significantly fewer than the other two methods.
on average among the .
code changes between the two versions our method will report only .
code changes among which .
are the actual faulty code changes.
b. comparing the runtime performance our new method relies on a synergistic analysis framework that leverages both the trial and error style dynamic analysis icse florence italytable ii relevant codechangescomputed by dd add andafter.
name changesdd add after actual match match match find a yes 1missed yes find b yes yes yes find c yes yes yes bc yes yes yes make yes yes yes gawk yes yes yes diff 1partial 1partial yes avg.
.
.
.
.
.
table iii comparing the runtimeperformanceof dd add andafter.
namedd add after speedup test time s test time s test time s s1 s2 find a .7x .1x find b1 .7x .2x find c .9x .3x bc .9x .1x make2 .1x .9x gawk .4x .6x diff .8x .5x avg.
.4x .4x which is similar to dd and add and the smt solver based semantic analysis.
therefore a natural question is whether the use of the smt solver would slow down the method.
to answer this question we compared the runtime performance of the three methods.
table iii shows the result.
columns compare the number of test runs explored by the dynamic analysis component of each method and the total execution time.
columns and show the speedup of our new method after over the other two methods.
specifically we define s1 timedd timeafterand s2 timeadd timeafter.
the result in table iii shows that although after spends additional time on the semantic analysis whereas the other two methods do not the runtime overhead often is more than compensated by the smaller number of test runs needed.
on average our method is .
times faster than dd and .
times faster than add.
the use of semantic analysis can drastically reduce the number of re executions needed by dynamic analysis.
this is especially important for programs where the code changes are far away from the manifested failures for which re execution takes a long time.
c. statistics of our synergistic analysis recall that our method has two additional features that dd and add do not have.
the first one is the capability of computing aux the set of code changes that must be reverted together with rootto make it compile.
the second one is the capability of reporting a tree of causal event chains to explain how the faulty code changes lead to the failure.
table iv shows the statistics of running our method on the benchmarkprograms.webreakdown total thetotalnumber of code changes reported by our tool into two parts.
that is total aux root where auxis the number of addition changes that must be reverted to make the program compile and rootis the set of changes responsible for the failure.
one main advantage of our method over existing methodsis the capability of computing aux.
we also report table iv statistics of runningafter on thebenchmark programs .
name changesafter total aux rootsmt time s causes find a find b find c bc make gawk diff avg.
.
.
.
.
.
diffutils .
src io.c diffutils .
.
src io.c for p0 !
beg0 p0 p1 if p0 !
p1 while p0 !
beg0 if p0 !
p1 fig.
.
thecodechange ofprogram diffreportedbydd add andafter.
among the total execution time how many seconds are spent on running the smt solver based semantic analysis.
the last column shows the number of causes causal chains of events that link the faulty code changes to the manifested failure.
the causes computed by our smt solver basedanalysiscanhelpthedevelopersunderstandthecausality relationship between the code changes and the failure it is the main reason why we call our approach fault explanation instead of fault localization.
besides establishing the causality relationship the reported tree of causes can providehints to programmerson how to fix the faulty program.
consider the diffbenchmark program where all three methods reported the code change in fig.
.
however the result reported by after is a matchwhereas the results reported by the other two methods are partial for the following reasons.
first reverting the change in fig.
can indeed make the failure go away.
however based on the comments from the developers this change itself was not faultysinceitwasintroducedtofixabugappearedinanearlier version id 58d0483b621792959a485876aee05d799b6470de and the bug eventually was fixed by adding another condition to catch and correct the affected variable.
our method correctly explainedthisfailurebecause inadditiontothisfaultychange our method also reported causes which formed a chain of propagation.
the actual bug fix proposed by the developers was on our causality chain consider another example make which is the benchmark application on which dd and add reported significantly more code changes than after.
for this example the bug fix provided by the developers is to remove the check of f is target shown in fig.
.
although this change is also reported by by dd and add which earns them a match this faulty code change is buried among and irrelevant changes respectively.
in practice it would be too time consuming for the developers to sift through such large numbers of potential code changes.
in contrast our method icse florence italy make .
implicit.c make .
implicit.c if lookup file p !
dep changed check is disabled.
if f lookup file name !
f is target fig.
.
actual bug fix reported by after for the makebenchmark.
reported only code changes together with auxiliary code changes to make the program compile successfully.
vi.
relatedwork there is a large body of work on debugging evolving programs based on the trial and error style analysis as pioneered by zeller et al.
.
the approach commonly known as delta debugging dd has been combined with other techniques including execution coverage also known as theaugmenteddelta debugging add observation based slicing dualslicing andhierarchicalinformation toimproveprecision.however thesemethodsrelyondynamic analysis only whereas in our synergistic analysis framework wealsousesmtbasedsemanticanalysistocomputecausality chains and provide guidance to the dynamic analysis.
another line of influential work on debugging evolving programs is based on symbolic techniques.
for example the darwin method relies on the assumption that the path conditions of buggy and correct executions often differ from each other.
by comparing the difference darwin is effective in finding control logic errors.
however it is less effective in finding errors in other parts of the code such as the assignments since they do not alter the control flow.
banerjee et al.
proposesa remedialmethodfor darwin by comparing the erroneous instructions against a golden program.
however neither method is based on the synergistic application of both dynamic analysis and semantic analysis.
furthermore neither provides the tree like explanation of the fault propagation as in our method.
the concept of as correct as the previous version has become popular in the subfield of regression verification .althoughtheseworksdonotdirectlyfocusonexplaining failed regression tests as in this paper they complement our work at the high level.
dynamic slicing together with many variations isanotherwidelyusederrortriagingtechnique.however it may not be able to remove many of the semantically irrelevant program statements thereby limiting its usefulness .
therefore in practice slicing typically is used as an auxiliary technique to complement other methods in automated debugging.forexample theproblemtackledinthispapercannotbe solved by simply combining dynamic slicing with a weakest precondition computation over the backward slice the use of re execution based analysis is also crucial to identify the actual causality relationship between the recent code changes and the observed failure.
bugassist isabooleansatsolverbasedtoolforlocalizing potentially faulty program statements in a buggy c programs.thetoolleveragesasatsolver scapabilitytocompute minimal unsatisfiability core in the cbmc verification tool.ermiset al.
propose a graig interpolant based method for computing error invariants which are then used to identify portions of a faulty trace that are irrelevant.
there are also other fault localization methods based on weakest precondition and inductive interpolant to explain the encountered failure.
however none of these methodsis gearedtoward regression testing and as such they donotutilize the codechangesbetween the correctandbuggy versions.
there are bug triaging methods based on comparing the passing and failing execution traces .
for a given failing execution they find passing executions that are as similar to the failing one as possible.
then they identify the difference between passing and failing executions and present them as an explanation of the failure.
methods based on the use of dynamically learned likely program invariants can also be efficient for catching the differences between failing and passing executions.
however the effectiveness of these methods is limited by the quality and sometimes the availability of the test suite.
fault localization methods based on identifying anomalous events in program executions rely on the assumption that rarely occurring events are likely faulty.
a representative tool that falls in this category is radar which derives multiple models from the base version of the program and compares them with the failed execution to identify a chain of suspicious anomalous events.
such techniques differ from our method in that they do not rely on semantic analysis and therefore can only discover correlation between the anomalous events and the failure but not the causal relationship .
nevertheless they are complementary to our method in that the anomalous events can be used by our method to further prune the irrelevant predicates.
vii.
conclusions we have presented a new synergistic analysis method for localizing faulty code changes in the context of regression testingandexplaininghowthesecodechangesleadtothemanifested failure.
the method relies on an iterative framework that leverages dynamic analysis to identify the correlation between the code changes and the failure and also leverages semanticanalysistoidentifythecausalityrelationshipbetween them.
our experiments on widely used linux applications show that the new method is effective in localizing relevant code changes in practice.
furthermore our method can report a tree of causes to help explain the chain of fault propagation events from the code changes to the manifested failure.