software documentation issues unveiled emad aghajani csaba nagy olga lucero v ega m rquez mario linares v squez laura moreno gabriele bavota michele lanza software institute universit della svizzera italiana usi switzerland systems and computing engineering department universidad de los andes colombia department of computer science colorado state university usa abstract good software documentation provides developers and users with a description of what a software system does how it operates and how it should be used.
for example technical documentation e.g.
an api reference guide aids developers during evolution maintenance activities while a user manual explains how users are to interact with a system.
despite its intrinsic value the creation and the maintenance of documentation is often neglected negatively impacting its quality and usefulness ultimately leading to a generally unfavorable take on documentation.
previous studies investigating documentation issues have been based on surveying developers which naturally leads to a somewhat biased view of problems affecting documentation.
we present a large scale empirical study where we mined analyzed and categorized documentation related artifacts stemming from four different sources namely mailing lists stack overflow discussions issue repositories and pull requests.
the result is a detailed taxonomy of documentation issues from which we infer a series of actionable proposals both for researchers and practitioners.
keywords documentation empirical study i. i ntroduction good old documentation the ideal companion of any software system is intended to provide stakeholders with useful knowledge about the system and related processes.
depending on the target audience the contents of documentation varies.
for example technical documentation e.g.
api reference guides describes information about the design code interfaces and functionality of software to support developers in their tasks while user documentation e.g.
user manuals explains to end users how they should use the software application.
despite the undeniable practical benefits of documentation during software development and evolution activities its creation and maintenance have been often neglected leading to inadequate and even inexistent documentation.
these and other aspects of documentation e.g.
needs learning obstacles have been investigated through interviews with and surveys of practitioners with the general goal of identifying the root causes of documentation issues e.g.
inaccuracy outdatedness .
to address these issues at least partially different approaches and tools have been proposed to aid developers during software documentation including its automatic generation .
for example a recent proposal by robillard et al.
suggests a paradigm shift towards systems that automatically generate documentation in response to a developer s query while considering her working context.however to achieve high quality automatic documentation systems we require first a deep understanding of software practitioners needs.
although existing studies have revealed some of these needs their results are limited by the low number and lack of diversity of practitioners questioned and documentation artifacts analyzed.
to overcome these limitations we qualitatively analyzed different types of artifacts from diverse data sources and identified the issues that developers face when dealing with documentation .
specifically we mined open source software repositories and examined a set of artifacts corresponding to development emails programming forum discussions issues and pull requests related to software documentation.
for each artifact we determined the reported issue the type of documentation presenting it and the proposed solution as well as the documentation tools discussed.
based on our analysis we built a comprehensive taxonomy consisting of types of documentation issues linked to i the information it contains ii how the information is presented iii the documentation process and iv documentation tool support.
we describe and exemplify each category of documentation issues.
we also discuss their implications in software research and practice deriving actionable items needed to address them.
ii.
r ela ted work software documentation has inspired research mainly on two fronts tools approaches and empirical studies.
tools approaches.
there has been much work on building tools to support the automated generation of documentation.
software summarization has shown progress with different techniques and tools for generating abstractive and extractive summaries of diverse software artifacts such as bug reports classes and methods unit tests commit messages release notes user reviews code examples and user stories .
approaches aimed at supporting developers during coding have also been developed.
code search engines and recommendation systems are available for retrieving api and code usage examples code fragments implementing specific features and crowd knowledge .
despite these efforts automated documentation is still wishful thinking.
a first research road map to enable automated on demand documentation has however been drawn by the recent proposal of robillard et al.
.
ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
t able i summary of previous studies on softw are document a tion aspects .
study artifacts summary of findings related to concerns and quality attributes f orward and lethbridge questionnaire with participants from sw industry research peers and mail lists members .software documentation regularly used by participantsdespite documentation being outdated practitioners learn how to deal with it.
software documentation tools should seek to better extract knowledge from core resources.
software professionals value technologies that improve automation of the documentation process and its maintenance.
kajko mattsson exploratory study with swedish organizations.maintenance related documentation artifacts documentation within corrective maintenance is still a very neglected issue.
chen and huang questionnaire with project managers and sw engineers of the chinese information service industry association of taiwan.software documentation regularly used by participantsmost typical problems in software documentation quality for maintenance are that software documentation is untrustworthy inadequate incomplete or does not even exist lacks traceability does not include its changes and lacks integrity and consistency .
robillard personal interviews with professionals at microsoft.api documentation and source codethe top obstacles for api learning are resources for learning documentation examples etc.
api structure background t echnical environment process.
api documentation must include good examples be complete support complex usage scenarios be organized and have better design .
dagenais and robillard i a qualitative study with contributors and users of open source projects and ii an evolution analysis of documents from open source projects.open source projects documentation in a repository or wiki e.g.
django firefox and eclipse in open source projects knowing the relationships between documentation and decisions of contributors help to define better techniques for documentation creation and maintenance.
when a wiki is selected to host documentation its quality is threatened by erroneous updates sp am or irrelevant content urls included .
this requires more effort for maintaining wikis.
robillard and deline i an initial questionnaire ii a set of qualitative in person interviews and iii a questionnaire with developers at microsoft.api documentation regularly used by participantsrelevant issues in the documentation that affect the developers learning experience documentation of intent code examples cookbooks for mapping usage scenarios to api elements penetrability of the api and format and presentation of the documentation.
pl sch et al.
online questionnaire with software professionals mainly german speakers.software documentation regularly used by participantsthe most important attributes are accuracy clarity consistency readability structuredness and understandability.
there is a need for automatic analysis of software documentation quality.
zhi et al.
mapping study about a set of papers from to .n a documentation quality attributes that appear in most of the papers are completeness consistency and accessibility .
more empirical evidence is required involving large scale development projects or larger samples of participants from various organizations more industry academia collaborations are also required and more estimation models or methods to assess documentation.
garousi et al.
industry case study with analysis of documentation using the taxonomy by zhi and a questionnaire with employees of novatel inc.source code and a sample of software documentation design tests and processes technical documentation is preferred during development than during maintenance tasks the preferred source of information for maintenance is the source code other sources of information have no significant impact on developers preferences.
uddin and robillard a case study and a questionnaire with software professionals from ibm.api documentation the top problems in api documentation are i incompleteness ii ambiguity iii unexplained examples iv obsoleteness v inconsistency and vi incorrectness while in presentation are vii bloat viii fragmentation ix excess structural information and x tangled information.
alhindawi et al.
topic modeling based study.kde koffice source base and its external documentationa novel approach for evaluating documentation quality.
tools that can automatically assess the software documentation quality in an are highly demanded.
labeling and grouping documentation would impact its quality positively.
sohan et al.
controlled study with software engineers.wordpress rest api documentationdevelopers feel more satisfied when having examples.
when documentation lacks examples developers spend more time on coding execute more trial attempts and have lower success rates.
empirical studies.
software documentation has been analyzed in diverse empirical studies that i report evidence of its importance and impact in the software life cycle ii describe problems that developers face when dealing with it iii list quality attributes required in documentation iv provide recommendations for constructing it including standards and v propose frameworks and tools for evaluating documentation concerns such as cost benefit and quality attributes .
due to space limitations we summarize the closest ones to our study in table i. the mapping study by zhi et al.
is notable as it reviews about documentation related papers and reports that the most frequently discussed quality attributes are completeness consistency and accessibility .
zhi et al.
conclude that software documentation is an immature area and that stronger empirical evidence is needed to gain a deeper understanding of it.
most of the aforementioned studies gathered information directly from participants and used practitioner samples re stricted to a specific context e.g.
a company .
these studies are therefore not diverse enough in terms of analyzed artifacts and programming languages used by developers and the largest samples reported in the studies are robillard and deline and practitioners uddin and robillard .
to avoid some of the limitations imposed by interviews and surveys we opted for an approach that allowed us to study a wider population in terms of number and types of artifacts by mining different data sources.
our results complement previous categorizations of documentation issues with a taxonomy that considers documentation content processes and tools.
ours is the first mining based study focused on identifying documentation issues as discussed by practitioners in software repositories.
previous studies following a mining based strategy are more general identifying topics discussed by developers or by apps users .
iii.
e mpirical study design our goal is to answer the following research question rq what are the documentation issues faced by developers?
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a. data collection our data collection consists of two steps.
first we adopt an automatic process based on keyword matching to mine candidate artifacts related to documentation from the four analyzed sources i.e.
emails issues and pull requests of open source projects and stack overflow threads .
then we manually analyze a statistically significant sample of artifacts to categorize them based on the issues they discuss the solutions they propose and the type of documentation they involve.
identification of candidate artifacts related to documentation issues table ii summarizes the artifacts automatically collected from the four sources see column candidate artifacts .
we discuss the process adopted in each case.
t able ii study da t aset sourcecandidate manually false valid labeled artifacts analyzed posit.
artifacts sentences issues mailing lists pull requests stack overflow overall stack overflow so .
we mined from the official so dump of june all discussions having a question labeled with a documentation related tag.
to determine these tags we searched for all tags related to documentation and documentation tools in the so tag page by using the keywords doc documentation and documentor .
the latter term is known to be part of the name of tools supporting software documentation.
one author then inspected all the tags resulting from these three searches to identify the ones actually related to software documentation and or documentation tools.
during the inspection the author read the tag name the tag description and some of the questions in which the tag was used.
this process resulted in the selection of tags e.g.
code documentation phpdocumentor design documents that were used to search for the related discussions in so.
the first results discussions returned by the searches were manually inspected to look for additional documentation related tags missed in the first step.
the process was iterated with the newly founded tags until no new tags were found in the top results of the tag searches.
this resulted in a total of documentation related tags available in our replication package .
next we queried the so dump to extract all discussions having a question with a non negative score and tagged with one or more of the relevant tags.
we removed questions with a negative score to filter out irrelevant discussions.
this process resulted in the selection of discussions.
for each of them we kept the question the two top scored answers and the accepted answer if any .
github issues and pull requests.
we downloaded the github archive containing every public github event occurring between january and april .
while older data is available we excluded it since some of the information needed for our study was only archived starting from .
we extracted all events of type issuesevent issuecommentevent pullrequestevent and pullrequestreviewcommentevent .these events capture the opening closing of issues and pull requests as well as all the discussion held for them through comments.
a detailed description of these event types is available online .
then we selected issues and pull requests from projects having at least ten forks and or stars to exclude toy projects.
finally we adopted a keywordmatching approach to extract issues and pull requests related to documentation.
we started from the so tags previously mentioned and converted them into keywords .
this means for example that the so tag design documents was converted into design doc to match design document design documents and design doc while tags including the word documentation e.g.
xml documentation were replaced with the keyword documentation since matching this keyword will also match the more specific ones.
we also added keywords that we considered relevant but were not derived from any of the so tags.
for example while the keyword api doc was derived from the so tag api doc we also added api manual .
in total we defined documentation related keywords .
we extracted all the issues pull requests having at least one of the keywords in their title and or in their first post i.e.
the one opening the issue or the pull request .
this resulted in the selection of issues and pull requests.
mailing lists.
we built a crawler to mine the mail archives of the apache software foundation asf .
the asf archives all emails exchanged in the mailing lists of the projects it runs.
each of its projects has several mailing lists focused on different topics.
we mined all mailing lists named docs discussions related to documentation dev discussions among developers and users discussions involving both users and developers for a total of mailing lists.
for the threads extracted from the docs mailing lists we did not apply any filter.
for the threads extracted from the dev and the users mailing lists we only selected those containing in the subject at least one of documentation related keywords we previously defined.
this resulted in the extraction of email threads each one composed by one or more messages.
manual classification of documentation issues once we collected the candidate artifacts we manually analyzed a statistically significant sample ensuring a confidence level .
this resulted in the selection of artifacts for our manual analysis out of the artifacts collected from the four sources.
since the number of collected artifacts is substantially different between the four sources table ii we decided to randomly select the artifacts by considering these proportions.
a simple proportional selection would basically discard so and mailing lists from our study since issues and pull requests account for over of our dataset.
indeed this would result in the selection of pull requests issues so discussions and mailing list threads.
for this reason we adopted the following sampling procedure for so and mailing lists we targeted the analysis of artifacts each ensuring a confidence level within those two sources.
for issues and pull requests we adopted the proportional selection as explained above.
this resulted in artifacts to be manually analyzed confidence .
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the selected artifacts were manually analyzed by six of the authors with the goal of classifying them as false positive i.e.
unrelated to documentation issues or assigning a set of labels describing i the documentation issue discussed ii the solution proposed adopted iii the type of the documentation and iv the documentation tools discussed.
each of these labels was optional.
for example it is possible that only the issue type and the solution were labeled for an artifact.
for two of the four categories namely issue type and documentation type we started from a predefined list of labels.
for the issue types we used the quality attributes defined by zhi et al.
.
for the type of documentation we had predefined labels that we selected based on our experience e.g.
code comments .
see for the list of predefined labels.
the labeling was supported by a web app that we developed for this task and for conflict resolution.
each author independently labeled artifacts randomly assigned to her by the web app selecting a proper label among the predefined ones or defining a new label when needed.
to assign a label the author inspected the whole artifact and in the case of issues and pull requests also the related commits.
every time an author had to label an artifact the web app also showed the list of labels created by all taggers so far.
the labeling was performed at sentence level the web app allowed the author to select one sentence from the artifact at a time and assign labels to it.
this means that multiple sentences could be labeled for each artifact and hence multiple labels could be assigned to it.
this allowed us to create a database publicly available of labeled sentences related to documentation artifacts.
each artifact was assigned to two authors by the web app.
in case both of them classified the artifact as a false positive the artifact was replaced with another one randomly selected from the same source e.g.
a false positive email thread was replaced with another email thread .
for each artifact xin which there was a conflict in the assigned labels a third author not previously involved in the labeling of x w a s assigned to solve it.
a conflict in this scenario can happen for many reasons.
first the two authors could label different sentences in the artifact.
second assuming the same sentences are selected different categories of labels could be assigned to the sentences e.g.
one author labels a sentence as discussing the issue one as presenting a solution .
third assuming the same sentences and the same categories of labels are selected the label values differ e.g.
different solutions indicated for the same sentence .
fourth one author could classify the artifact as a false positive while the other could label it.
for these reasons we had a high number of conflicted artifacts out of .
.
we solved some specific cases automatically.
in particular if two authors i labeled for the same artifact two different sentences siandsjwhere siis a substring of sj or vice versa and ii had no conflicts between the label values we automatically solved the conflict by selecting the longest sentence as the valid one.
this reduced the number of conflicted artifacts to which were manually reviewed by a third author who could accept a conflicting sentence and apply minor modifications if necessary or discard it.in this final process artifacts were discarded as false positives.
the final number of sentences labeled for each type of artifact is reported in table ii.
b. data analysis we answer our rq by presenting a taxonomy of the types of documentation issues found in our analysis.
such a taxonomy was defined in an open discussion involving all the authors and aimed at merging similar labels and hierarchically organizing them see figure .
we focus our qualitative analysis on specific categories of issue types.
for each category we present interesting examples and common solutions and discuss implications for researchers and practitioners.
c. replication package all the data used in our study is publicly available .
iv .
r esul ts discussion as a result of the labeling process we obtained artifacts including a total of labeled sentences.
figure shows the hierarchical taxonomy of the documentation issue types that we identified.
they are grouped into four main categories i problems related to the information content of the documentation describe issues arising from what is written in the documentation ii issues classified under the information content how category focus on how the content is written and organized iii the process related category groups issues related to the documentation process and iv tool related matters originate from the usage of a documentation tool.
the number shown in the main categories of figure represents the number of artifacts related to that issue e.g.
artifacts were related to process related issues .
note that a single artifact might discuss multiple types of issues.
figure also shows the distribution of the analyzed artifacts among the four sources we analyzed.
interestingly problems related to the content and how it is presented organized are mostly discussed in issues and pull requests and discussions about the documentation process and tools related issues are mainly held in mailing lists and so respectively.
for each category we next describe representative examples and discuss implications for researchers indicated with the beakericon and or practitioners code forkicon derived from our findings.
a. information content what a total of artifacts discuss issues related to the information content i.e.
what is written in the documentation.
correctness .
correct documentation provides accurate information in accordance with facts .
incorrect documentation might have unforeseen serious consequences going beyond wasted time trying to replicate a wrong code example or following the wrong steps in a tutorial.
this is the case of an issue filed for the acid state project a tool providing acid guarantees to serializable haskell data structures.
as reported in the issue a false claim in the documentation could lead to data loss this could easily cause permanent data loss if the user then proceeds to remove the archive folder which is claimed to be safe by the documentation .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
process relatedusefulness usability maintainability readabilityinformation content how development issues caused by documentationtraceability internationalizationcontributing to documentationdoc generator configuration wrong missing author informationdifficulty in translating into a languageissues with character encodingmissing translation for a languagehow to write documentationhelp needed due to lack of timewhere the documentation should be placed in the repohow to report issues found in the documentationhow to support external documentation contributors discussing what is missing needs improvementwhere to add a new piece of documentationfeedback required for first drafthow to contribute to doc as an outsiderbuild configuration ignores javadoc warningsbuild tools are misplaced in the infrastructure difficulty in automating document generationproblems introduced by auto generated commentsused ci cd tools are not documenteddevelopment issues caused by restructuring of documentation filesexcessive website loadtimeformat presentationwarning the user about copy paste counterexamplesinformation organizationaccessibility findability official doc for a library framework ... documentation of a terminated projectsource of primary doc project site or wiki content browseability searchability how to find the doc of the actual version on websiteuser confused on where to start in the dochow to find the doc for a speci fic thingpoor support for navigating the documentationmultiple readme filesinconsistent formattingpoor formatting styleformat wiki website interactive to adopt style of urls in documentationlicense copyright formattinginconsistent stylinginconsistent terminologyneed to split the documentation too large cloned documentationsuper fluous documentation super fluous code commentssuper fluous instructionsempty javadoc commentsempty files super fluous autogenerated code commentsconciseness clarityspelling and grammar too verbose too much details noisy with super fluous partsconfusing method names acronyms used in commentsconfusing documentation titleunclear code exampledoc needs improvement fix to become useful code example needs improvement to become usefulinformation content what correctness completeness up to dateness inappropriate installation instructionswrong translationerroneous code exampleswrong code commentsfaulty tutorial failing installation process for a userimproperly described installation process violation of best practices in the example codecode doc inconsistencyscreenshot does not reflect current guioutdated obsolete referencesmissing documentation for a new release e.g.
v2.
outdated translationsmissing documentation for new feature componentoutdated exampleoutdated version informationoutdated license copyright informationoutdated installation instructions broken linklink to an old version of doc while newer is available code must change to match docbehavior described in documentation but not implementedmissing poor documentationmissing diagrams missing performance informationmissing introduction sectiondocumentation for usersinstallation deployment releasemissing supported versionsmissing linksmissing license copyright informationmissing configuration instructionsmissing compatibility informationmissing code commentsdeveloper guidelinesmissing code behavior clari fications missing required librariesmissing information about error warning messagesdefault code behavior is not documentedmissing style conventionsmissing contribution guidelines missing information regarding testing debuggingmissing information regarding javadoc policymissing release notes missing deployment documentationmissing build guidemissing installation guidemissing alternative solutions missing best practicesmissing unrecommended usage missing api documentationmissing library usage informationmissing user manualmissing code example missing tutorial step by step guides missing faqmissing link from source code to doc tool related bug issuesupport expectationshelp required tool migration outdated documentation toolexcessive output sizebug in the toolrequest for new featureautomatization ide integrationide support for autoformatting comment blocks modifying multiple page content togetherupdate multiple files together with a speci fic format warning about doc issues ahead of timeability to document more code elements missing editor for writing documentationsupport for reusing part of existing documentationpoor quality of generated commentsautomatic documentation generationneed to automatize doc deployment publishing by script toolneed to automatically update doc after changes license copyright auto generationexclude speci fic entities from auto generated dochow to do usereceiving error warning messageslicense of documentation tool is expiredasking the existence of a documentation tool format relatedconfiguration miscon figuration broken formattingproblems to convert migrate doc format21 assumption of existing knowledge255485 stack over flow issues pull requests mailing lists stack over flow issues pull requests mailing lists stack over flow issues pull requests mailing lists11 stack over flow issues pull requests mailing lists61 availability documentation not available due to website migration fig.
.
documentation issues taxonomy authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the type of documentation most frequently impacted by correctness issues was code examples e.g.
accounting for of the cases in which we labeled a documentation type followed by installation guidelines e.g.
.
correctness issues in code examples include syntactic mistakes e.g.
the documentation gives the following example but running it leads to error syntax is not a symbol as well as more serious programming errors e.g.
one of the example fixture files in the documentation would not work because it contains