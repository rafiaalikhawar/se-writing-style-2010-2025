a novel automated approach for software effort estimation based on data augmentation liyan song southern university of science and technology china university of birmingham uk songly sustc.edu.cnleandro l. minku school of computer science university of birmingham uk l.l.minku cs.bham.ac.ukxin yao southern university of science and technology china and university of birmingham uk xiny sustc.edu.cn abstract software effort estimation see usually suffers from data scarcity problem due to the expensive or long process of data collection.
as a result companies usually have limited projects for effort estimation causing unsatisfactory prediction performance.
few studies have investigated strategies to generate additional see data to aid such learning.
we aim to propose a synthetic data generator to address the data scarcity problem of see.
our synthetic generator enlarges the see data set size by slightly displacing some randomly chosen training examples.
it can be used with any see method as a data preprocessor.
its effectiveness is justified with state of the art see models across see data sets.
we also compare our data generator against the only existing approach in the see literature.
experimental results show that our synthetic projects can significantly improve the performance of some see methods especially when the training data is insufficient.
when they cannot significantly improve the prediction performance they are not detrimental either.
besides our synthetic data generator is significantly superior or perform similarly to its competitor in the see literature.
therefore our data generator plays a non harmful if not significantly beneficial effect on the see methods investigated in this paper.
therefore it is helpful in addressing the data scarcity problem of see.
ccs concepts computing methodologies supervised learning by regression bayesian network models ensemble methods software and its engineering software creation and management keywords software effort estimation data scarcity synthetic data generation acm reference format liyan song leandro l. minku and xin yao.
.
a novel automated approach for software effort estimation based on data augmentation.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa association for computing machinery.
acm isbn .
introduction software effort estimation see is the process of predicting the effort e.g.
in person month or person hour required to develop a software system.
it often takes place in the very early stage of software development and is an important task in software project management.
over and under estimation can cause either waste of resources or result in compromising the product quality .
one of the core challenges of see is the high cost associated with data collection .
the collection of software projects is very costly and may require considerable amount of time and workload .
consequently companies usually have small numbers of completed projects to estimate the effort of new projects.
it would be hard to make accurate estimates with inadequate see data because the information contained in such small data probably cannot support training of see models .
existing work has frequently attempted to tackle this issue by creating advanced predictors that are more suitable for this problem .
rather than introducing sophisticated see models or collecting as many completed projects as possible we can augment see data set by generating synthetic projects based on the existing data.
however little work has been done to investigate such strategies.
this paper proposes an automated data augmentation approach that can be used as a preprocessor for any see method.
our data generator produces additional synthetic projects by slight displacing some randomly chosen completed projects with each synthetic data associated with one existing project.
though the synthetic projects are not real they can enrich the representativeness of the area they are generated and potentially enhance the effort prediction.
our data generator provides a second and much cheaper way to tackle see data scarcity problem compared to proposing sophisticated models or strategies of real data collection.
to evaluate its effectiveness we investigated the following research questions rq1 given an see predictor can our synthetic data generator help improving prediction performance over the baseline that does not use synthetic data?
when?
could it be detrimental?
rq2 given an see predictor if our synthetic projects are helpful to prediction performance why are they helpful?
if they are detrimental why are they detrimental?
rq3 how well our data generator performs compared against other existing data generators in the see literature?
experimental studies based on six state of the art see models show supportive results of our data generator.
our synthetic projects always have positive effect on and are rarely detrimental to the baseline performance of the investigated see models especially when the training data is insufficient.
besides our data generator is similar or significantly superior to its only competitor in see .
esec fse november lake buena vista fl usa liyan song leandro l. minku and xin yao the main contribution of this paper is to propose and validate a novel synthetic data generator and provide the understanding of when and why the synthetic projects generated by this approach can help improving the baseline performance of the see model.
related work .
data augmentation for classification there are many studies in machine learning ml that augment the data set size for better performance.
imbalance classification is a typical example where the difference between the numbers of data samples in different categories is huge .
one problem of learning from imbalanced data is that the classifiers would often predict a new sample with the majority label though its label should equal to the minority .
data over sampling is a popular and effective approach to tackle the imbalance classification problem where synthetic data is generated in the minority class to form a more balanced data set for performance improvement .
software defect prediction sdp is a typical counterpart of imbalance classification in the context of software engineering se since the defect modules are much less likely to happen than the non defect ones.
data imbalance usually undermines the performance of sdp methods where the defect predictors often rarely predict the faulty modules .
to tackle the imbalance problem of sdp a few methods that augment the data set size of the minority class i.e.
the faulty class have been proposed .
for instance employed several data augmentation methods in ml such as random over sampling that reproduces the data of minority class randomly and smote synthetic minority over sampling technique that produces new samples based onk nearest neighbours to enlarge the data set size of the minority class.
their experimental results showed promising or better effect of the augmented data in performance improvement.
another genetic algorithm based data augmentation method was proposed in which outperformed the predictor without the augment data and the predictor with other augmentation methods.
.
data augmentation in see literature the augmentation methods designed for classification cannot be directly used for see since by nature there is no minority majority class in regression.
section .
is discussed for being among the most related to our work.
despite many studies on synthetic oversampling for classification there have been few for regression e.g.
smoter and its adaptation for see .
this may be due to the difficulty in defining minority and majority values for regression.
some studies generate only synthetic inputs .
however they are either only applicable to images or require large training sets which are unavailable for see .
to our best knowledge there has been only one work in the see community that tackles the data scarcity problem by generating synthetic data .
their proposed approach extended smote from classification to regression by attributing class imbalance from the most predictive numerical feature which is usually a sizerelated feature such as functional size .
after casting the entire data samples into three classes small medium and large according to e.g.
functional size conventional smote was used to generate synthetic projects to small and medium classes to balance the data distribution.
the entire data set size was thus increased.then these synthetic projects together with the real see data were passed to k nearest neighbours k nn for the purpose of getting better performance.
their experiments showed promising results based on desharnais data set from seacraft repository.
despite that the data generator of was designed for k nn it can be easily extended for other see models as a data preprocessor.
we will compare the effect of this data generator with ours in term of improving the performance of the baseline see models in sec.
.
.
our synthetic data generator different from the synthetic data generator in the literature where a synthetic project was generated by a combination of two existing projects our approach produces a synthetic project by displacing one existing project that is randomly selected.
consider a training set of nsoftware projectsd xn yn n n where an input vector xn rdincludes software features such as software development type team expertise andfunctional size and ynis the actual effort for developing this software.
our synthetic data generator will produce n synthetic projects to enlarge the training set size and tackle the see data scarcity problem where is the synthetic rate and denotes the upward rounding operator e.g.
.
.
the synthetic rate should not be too large in order to retain the synthetic projects in good quality.
in this paper is chosen from .
.
.
as shown in table .
overall based on randomly selected training examples from the data setd the proposed data generator will produce n synthetic projects one by one each of which consists of two steps synthetic feature generation and synthetic effort generation.
.
synthetic feature generation see features can be categorized into three classes according to the types of feature values categorical features with discrete nominal values such as enhancement re development andnew development forsoftware development type ordinal features with discrete ordinal values such as very low low normal andhigh for team expertise and numerical features with continuous values such as functional size andline of codes .
given a randomly chosen training example x d a synthetic project x syn is generated feature by feature by displacing each training feature individually.
the generation approach varies depending on the types of feature values as follows.
.
.
categorical feature.
for a categorical feature xc xwith kvalues vc1 vck our proposed approach will generate its synthetic counterpart x syn c by uniformly sampling a new categorical value from the set vc1 vck vc xc wherevc xcdenotes the categorical feature value of the chosen training project.
we assign a model parameter 1to the synthetic categorical feature generation such that with probability the synthetic feature retains the training value vc xc and with probability the synthetic feature randomly takes a value from vc1 vck vc xc having the same probability for each value to be taken.
the process can be formulated as x syn c vc xc if u vc1 vck vc xc if0 where is a random variable uniformly taken from and u denotes a discrete uniform distribution function.
to retain 469a novel automated approach for see based on data augmentation esec fse november lake buena vista fl usa a moderate shift on the synthetic feature we adopt small changing probability as listed in table .
taking the categorical feature development type with values of enhancement re development and new development as an example if the training example is re developed the synthetic feature will stay the same with probability or be uniformly chosen from enhancement new development with probability .
.
.
ordinal feature.
for an ordinal feature xo xwith k values vo1 vok wherevoi vojfor1 i j k our approach will generate its synthetic counterpart x syn o according to binomial distribution.
binomial distribution b n p is frequently used to model the number of successes in a sequence of nindependent experiments each of which succeeds with probability por fails with p .
for random variable b n p itsexpectation equals to e np.
binomial distribution is suitable to model ordinal features because it is a discrete distribution and can manifest the ordered relationship between feature values.
figure a illustrates the histogram of a binomial distribution b n p .
we use an example to demonstrate our procedures in deciding the parameters of binomial distribution b n p of a training project.
given an ordinal feature team expertise with values of very low low normal and4 high if the team expertise of the training example is normal the synthetic feature should have the highest chance for taking normal the second highest and the same chance for4 high and low and the lowest chance for very low .
to guarantee the expectation to be normal the binomial parameters should satisfy n p .
to guarantee the same chance of taking low and4 high pshould be .
combining the two equations the binomial distribution should be b n p .
figure b shows a solution of the binomial distribution for team expertise .
it is noteworthy that to retain feature value normal situating at the distribution centre three dummy values are added.
a synthetic ordinal feature is sampled from b n p .
if we get a dummy value resume the sampling process until acquiring a valid feature value.
the process can be formulated as x syn o b n vo xo p wherevo xois the ordinal feature value of the training example.
.
.
numerical feature.
for a numerical feature xf xwith continuous values xf r1 our proposed approach will generate its synthetic counterpart x syn fby adding a zero mean gaussian variable n 2 to its baseline value xfas x syn f xf f f n 2 .
usually the numerical features are size related.
here we normalize each numerical feature to have zero mean and unit variance and assign gaussian 2with small values .
.
.
as shown in table to restrict the impact of gaussian displacement.
overall all numeric ordinal features change with large probability based on gaussian binomial distribution and each categorical feature has some chance to change based on the probability .
.
synthetic effort generation denoteyas the actual effort of training example x the aim of synthetic effort generation is to assign a proper value y syn to the synthetic feature x syn .
a binomial b n p .
b synthetic team expertise .
figure binomial distribution and its ordinal feature modelling similar to the numerical feature generation our approach assigns the synthetic effort by adding a zero mean gaussian variable n to its baseline effort value as y syn y si n f n where si n f is the positive negative sign of the injected gaussian variable of the numerical feature in eq.
.
when there are more than one numerical features fis their summation.
by so y syn y and x syn f xf can have the same increasing decreasing direction catering the well known fact that numerical size related features are positively correlated with effort values .
in this work we confine for simplicity.
exploration of a separate parameter can be conducted in future.
.
further discussions and summary there are several research lines that may further enhance the effectiveness of our proposed synthetic generator as our ordinal categorical feature modelling may not fit reality perfectly.
for instance a newly developed software project would be more likely to be enhanced rather than re developed employees with normal expertise would be more likely to evolve to high rather than lowexpertise.
thus it is interesting to study whether other nonsymmetric distributive modellings of ordinal categorical features would improve performance further.
this would depend on expert knowledge of the data distribution.
our approach assumes that synthetic efforts are only affected by the change in numerical features.
assigning synthetic effort from the changes of ordinal categorical features is very challenging as it requires expert knowledge or data analyses with large training sets.
this is potentially a harder problem than see itself.
since our strategy has achieved good results we did not investigate effects of changes in categorical ordinal features on synthetic effort.
nevertheless it is an interesting research direction.
in summary our data augmentation approach generates synthetic projects individually each of which is based on slight displacement of a training example that is chosen randomly.
thus the produced synthetic projects can only impact the local areas they are generated.
besides our synthetic data generator is data driven and does not depend on any effort estimator.
thus it can be used as a preprocessor with any see model.
experimental design .
data sets the experiments are based on data sets from the software engineering artifacts can really assist future tasks seacraft former promise and the international software benchmarking standards group isbsg release .
to investigate the effect of the training set size the data sets are grouped into small medium and large according to the ratio of the number of data over the number of features.
table contains the basic description of the investigated data sets.
470esec fse november lake buena vista fl usa liyan song leandro l. minku and xin yao maxwell contains projects from one of the biggest commercial banks in finland covering the years from to and both in house and outsourced development.
we removed the input features start year syear and duration syear .
syear was removed because it was found to have no significant effect on the dependent effort according to one way anova .duration was removed since it was unknown in reality during effort prediction process.
after the removal of features input features were left.
cocomo81 andnasa93 were collected in the cocomo data format which has features consisting of cost drivers lines of codes anddevelopment type .
we used the cocomo numeric values for the cost drivers.
cocomo81 has projects.
nasa93 contains projects developed between s and .
albrecht contains projects developed in ibm using the third generation languages in the 1970s .
eighteen out of projects were written in cobol four were written in pl1 and two were writen in dms languages.
seven input features were used.
the dependent effort is recorded in hours.
kemerer contains projects donated by dr. jacky w. keung in .
we use input features and remove the feature project id since it is irrelevant to the effort prediction.
desharnais contains projects with nine features from a canadian software company.
four projects contained missing values so they were excluded from our investigation.
the input features in use are teamexp managerexp transactons entities pointsnonadjust adjustment pointsajust and language .
the depended feature effort is recorded in hours.
kitchenham contains projects undertaken between and by a single software development company .
we removed the input features project id actual start date actual duration estimate completion data first estimate and first estimate method .project id was removed because it was irrelevant with see prediction.
actual start date was removed following the same preprocessing as .completion date together with start date would give the duration of the project and duration was removed because it was considered as a dependent variable of see process.
the other features were removed because they were themselves estimations of completion date or effort or represent the method used for such estimations.
this feature preprocessing led to remaining features adjusted function points project type andclient code .
isbsg release contains a large body of software projects projects covering many different companies several countries organisation types application types etc.
we preprocessed isbsg repository with the same procedures as .
we maintained projects by only keeping projects with relatively high quality.
they were grouped into several data sets according to the organisation type and only the groups with at least projects were maintained following isbsg s data size guidelines.
the resulting organisation types are shown in table .
the isbsg suggests that the most important criteria for estimation purpose are functional size development type new development enhancement or re development primary programming language 3gl 4gl or apg and development platform mainframe midrange or pc .
as development platform is missing in more than of the projects for two organisation types the remaining three criteria were used as input features.table see data sets that are cast into groups representing small medium andlarge data set sizes according to the ratio of the number of data over the number of features.
three sets of holdout values are assigned to three groups of data sets respectively.
size data set fea data fea data small medium large smallmaxwell .
.
.
loococomo81 .
nasa93 .
albrecht .
kemerer .
mediumdesharnais .
.
.
.7org2 .
org5 .
org6 .
org7 .
largekitchenham .
.
.
.7org1 .
org3 .
org4 .
table isbsg data sets grouped according to organization type and only the groups with at least projects were maintained following isbsg s data size guideline.
id organisation type data financial property business services banking communications government manufacturing transport storage ordering billing note that all projects of org6 had the same development type andprogramming language sofunctional size was used as a single feature.
in org7 all projects had the same development type and programming language with only one exception.
removing the exception we had projects with a single input feature.
data preprocessing.
for each data set in table we apply the logarithm to the numerical features making them less skewed and more gaussian distributed.
exponential distributions of numeric features are often observed in defect and effort prediction data sets which are usually composed of many small values combined with a few much larger values .
logarithm preprocessor has shown to be non harmful to or even sometimes improve the performance of the defect prediction .
our preliminary experiments on see have also shown either similar or better performance when using the logarithm scales of the numeric features compared to using their original values.
using the logarithm preprocessor all numeric features are replaced with their natural logarithm values.
this procedure also minimizes the effects of the occasional very large feature values.
furthermore each feature was normalized to be zero mean and unit variance to avoid scalability problem.
for the dependent outputs we converted the numerical efforts into their logarithm scales to make the effort distribution more gaussian.
this procedure can also alleviate the prediction problem when treating test sample with very large effort.
.
performance evaluation there are several performance metrics for see evaluation .
popular examples are mean absolute error mae mean magnitude 471a novel automated approach for see based on data augmentation esec fse november lake buena vista fl usa of the relative error mmre percentage of estimations within n of actual values pred n logarithmic standard deviation lsd and standardised accuracy sa .
different performance metrics emphasize different factors and can behave differently in effort model evaluation .
for instance mmre was shown to be biased towards prediction systems that underestimate effort .
underestimation over optimism is the direction of the error that practitioners are more unwilling to see so we did not use mmre in our investigation.
the performance metric used in this paper is mae defined aspn i yi dyi n whereyi dyiis the actual estimated effort and n is the number of testing data.
mae was recommended by shepperd and macdonell for see studies for being symmetric and not bias towards under or overestimation .
as the effort is in the logarithm scale this metric becomes less affected by project size.
we apply holdout evaluation to control the training set size deliberately and evaluate the effects of synthetic data when training set size is small medium and large respectively.
we randomly split the data set into training and testing subsets.
each see model is trained from the training set and its performance is evaluated from the testing set.
this process is repeated times and the average mae is reported.
.
baseline see predictors investigated we investigate see models linear regression lr automatically transformed linear model atlm k nearest neighbour k nn relevance vector machine rvm regression tree rt and support vector regression svr since they are among the state of the art see predictors .
each of them is used as a baseline model to investigate whether or not the generated synthetic data can improve its prediction performance.
these models are implemented in matlab and specified if otherwise.
lr and atlm are chosen because they have been shown to be good baselines after appropriate data transformations .
r.matlab package was used to configure the r implementation of atlm into the matlab framework.
k nn is chosen for being among the simplest prediction model and due to its intuitive interpretation that mimics the human instinctive decision making .
some empirical studies have showed that k nn is comparable and sometimes superior to other see models .
to predict the effort of a testing project the distances of this data to all training examples are computed in euclidean metric.
based on them knearest neighbours to the testing project are determined and their median is returned as the estimated effort of this testing project .
rvm is chosen because it has been shown to be very competitive compared to other state of the art see models and can be used to provide uncertain effort prediction .
in rvm each training data is associated with one basis function measuring the distance of this training project to the testing project.
there are several choices for the basis function.
we employ non normalized gaussian kernel j x exp x j 2s2 as our basis function for its locality property where the jis the j th training sample and the width scontrols their spatial scale.
rt is chosen for being among the most frequently used see models which has presented potential advantage for see .
rt is a rule based hierarchical model where software data featurestable parameter values of the see models investigated.
id method parameters lr no tuning parameter atlm no tuning parameter k nn k neighbour rvm s width .
.
rtsl max tree depth m min node per leaf e stopping error .
.
.
svrkernel linear c regularization .
.
slack variables .
.
.
syn.our synrate .
.
.
categorical .
.
2 gaussvar .
.
.
syn.cmp k neighbours in smote are used to split projects into to small groups and this process is recursively repeated to form a regression tree .
svr is designed for small data problems which seems suitable to effort estimation.
however svr has not been popularly used in see community partially because of the contradictory conclusions drawn from previous studies .
some claimed its superior performance in see while others claimed inferior performance of svr compared with other see models .
there are several choices for svr kernel.
we use linear kernel that has been shown to be a better choice .
parameter settings.
the parameter values of the see models investigated in this paper are listed in table .
for rt the maximum tree depth of means unlimited tree depth.
for svr we investigate the conventional settings for regularization parameter cand slack variable .
for the model that has more than one parameters we investigate its all parameter combinations.
our discussion is based on the performance of the best parameter settings with which the see predictors can achieve their best performance.
result and discussion this section aims to evaluate our synthetic data generator by comparing the performance of the see models with and without using the generated synthetic projects.
for simplicity the performance of the see model that does not use synthetic data is represented by bsl.seer and the performance of the see model that uses the synthetic data generated by our approach is represented by syn.seer where seer is one of the see models discussed in sec.
.
.
for a more thorough assessment sec.
.
compares our synthetic generator against its competitor in the see literature .
the performance of see models that uses the synthetic projects generated by this generator is represented by syn.cmp.seer .
.
effect of synthetic data on performance this subsection aims to answer rq1 given an effort model can our synthetic data generator help improving prediction performance over the baseline that does not use synthetic data?
when?
could it be detrimental?
to answer rq1 we investigate the effect of the synthetic data by comparing the performance of syn.seer against bsl.seer across data sets with small medium and large training set sizes respectively.
table lists the performance comparisons in 472esec fse november lake buena vista fl usa liyan song leandro l. minku and xin yao all settings.
we can see that the synthetic data generated by our approach can usually improve the performance.
to investigate whether the improvement is significant the effect size between syn.seer andbsl.seer across runs of each data set is checked.
effect size is a simple way of quantifying the size of the difference between two methods with multiple runs .
the vargha and delaney s a12is a non parametric effect size that makes no assumptions about the underlying distribution which is interpreted in terms of vargha and delaney s categories small .
medium .
and large .
.
in table large medium small effect size is highlighted in orange bold yellow bold bold indicating the performance improvement of using the synthetic projects generated by our approach.
we perform wilcoxon signed rank tests with holm bonferroni correction at the significance level .
to judge whether performance difference between bsl.seer andsyn.seer is statistically significant across all data sets.
wilcoxon signed rank tests are typically used to compare the performance of two models across multiple data sets .
the null hypothesis h0 states that the two models are equivalent.
the alternative hypothesis h1 states that they differ significantly.
wilcoxon signed rank tests also provide the average ranks of bsl.seer vssyn.seer across data sets calculated as rj np ir i j where r i jis the rank of the jthmethod on the ithdata set j bsl.seer syn.seer i n and n 14is the number of data sets.
the average rank averank provides a reasonable comparison between bsl.seer vssyn.seer given rejection of the null hypothesis .
.
.
lr and atlm.
since atlm is a variant of lr using the automatic data transformation mechanism we discuss the effect of our synthetic projects on them together.
for small training set size we can see from table a that the synthetic projects generated by our approach can drastically improve the performance of lr atlm with large effect size in five out of seven seacraft data sets.
the synthetic data never hurts the performance of lr atlm in any see data set investigated.
wilcoxon signed rank tests with holm bonferroni correction at the significance level .
across all see data sets detect significantly better performance of syn.lr syn.atlm over bsl.lr bsl.atlm .
it is noteworthy that the performance of lr atlm is unstable in some data sets.
for example atlm performs extremely bad in org1 with very large mae mean mae of runs .
.
.
further investigation found that atlm performed extremely bad on one of the runs with mae .
.
removing this outlier the mean mae across the remaining runs reduced to .
.
forsyn.atlm vs .
.
for bsl.atlm with a12 .
.
the unstable performance of lr atlm may be due to the scarcity of training samples.
when the few training samples are close to each other being more likely to happen given inadequate training data lr atlm may suffer from ill conditional problem when matrix inversion in the training process.
another possible reason for atlm is the incorrect statistic estimate on its automatic transformation mechanism caused by insufficient training samples.
for medium training set size we can see from tables a vs b that bsl.lr bsl.atlm can achieve superior and more stable performance using medium compared to small training set sizes indicating that augmenting the training data from an insufficient number can improve the performance of lr atlm.
similar observation can also seen for syn.lr syn.atlm .
we can also see that our synthetic projects can improve the performance of lr atlm especially for seacraft data sets the effect size a12is large in data sets.
wilcoxon signed rank tests with holm bonferroni correction at the significance level .
across all data sets detect significant better performance of syn.lr syn.atlm over bsl.lr bsl.atlm showing an overall superiority when the training set size is medium.
for large training set size the superiority of syn.lr syn.atlm over bsl.lr bsl.atlm becomes smaller than for medium small training set sizes.
for instance effect size a12is medium or small in only two seacraft data sets.
wilcoxon signed rank tests with holmbonferroni correction at the significance level .
across all data sets detect significantly better performance of syn.lr syn.atlm over bsl.lr bsl.atlm with p value .
.
.
summary.
our synthetic data can always improve the baseline performance of lr and atlm and the improvement magnitude is usually significant having large or medium effect size especially when the training data is insufficient.
when the training set size is large our synthetic projects can hardly have detrimental effect and sometimes significantly improve the baseline performance.
.
.
rvm and rt.
table shows that our synthetic data can usually improve the baseline performance of rvm and rt.
for rvm our synthetic data can always improve their baseline performance.
their effect sizes are sometimes large or medium showing substantial performance improvement on rvm.
wilcoxon signed rank tests with holm bonferroni correction at the significance level .
across all data sets detect significant overall superiority of using our synthetic data for all the training set sizes.
for rt our synthetic data can always improve their baseline performance.
when they are helpful with rt the effect size is often large or medium especially when the training set size is not large.
wilcoxon signed rank tests with holm bonferroni correction at the significance level .
across all data sets detect significant overall superiority of using our synthetic data for medium and large training set sizes.
summary.
our synthetic data can enhance the performance of rvm and rt and the improvement is often significant especially when the training set size is not large.
.
.
k nn and svr.
we can see from table that our synthetic projects can usually improve the performance of k nn and svr but the improvement is not very large.
fork nn our synthetic data can usually improve the baseline performance.
wilcoxon signed rank tests with holm bonferroni correction at the significance level .
across all data sets detect significantly better overall performance of using our synthetic data for medium and large training set sizes.
however the effect size shows small or insignificant superiority.
for svr our synthetic data can usually improve the baseline performance.
wilcoxon signed rank tests with holm bonferroni correction at the significance level .
across all see data sets detect significant overall superiority of using our synthetic data for small and medium training set sizes.
however the effect size usually shows insignificant superiority.
473a novel automated approach for see based on data augmentation esec fse november lake buena vista fl usa table performance comparisons between each pair of syn.seer vsbsl.seer across data sets in terms of mae for small medium and large training set sizes respectively.
the different training set sizes refer to different holdout values of table .
the reported values are the mean of runs followed by their standard deviations stds .
the comparison is highlighted in orange dark grey and bold font for large in yellow light grey and bold font for medium and in bold font for small effect size values.
the last two rows of each sub table list the results of wilcoxon tests with bonferroni correction.
the overall comparison between bsl.seer vssyn.seer can be seen from averank average ranks .
the first value or in wilcoxon row means there is or not significant difference detected and its corresponding p value comes the next.
significant difference is highlighted in orange dark grey on this row.
a small training set size.
data syn.lr bsl.lr syn.atlm bsl.atlm syn.k nn bsl.
k nn syn.rvm bsl.rvm syn.rt bsl.rt syn.svr bsl.svr maxwell .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cocomo81 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nasa93 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kitchenham .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
albrecht .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kemerer .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deshar .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org6 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org7 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
averank .
.
.
.
.
.
.
.
.
.
.
.
wilcoxon .
.
.
.
.
.
b medium training set size.
data syn.lr bsl.lr syn.atlm bsl.atlm syn.k nn bsl.
k nn syn.rvm bsl.rvm syn.rt bsl.rt syn.svr bsl.svr maxwell .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cocomo81 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nasa93 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kitchenham .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
albrecht .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kemerer .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deshar .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org6 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org7 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
averank .
.
.
.
.
.
.
.
.
.
.
.
wilcoxon .
.
.
.
.
.
c large training set size.
data syn.lr bsl.lr syn.atlm bsl.atlm syn.k nn bsl.
k nn syn.rvm bsl.rvm syn.rt bsl.rt syn.svr bsl.svr maxwell .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cocomo81 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nasa93 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kitchenham .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
albrecht .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kemerer .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deshar .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org6 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org7 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
averank .
.
.
.
.
.
.
.
.
.
.
.
wilcoxon .
.
.
.
.
.
superior performance of svr.
we can see from table that svr usually outperforms other see predictors.
friedman tests at the significance level .
across all data sets reject the null hypothesis h0 which states that all models are equivalent.
nevertheless our synthetic data can further improve its performance a little when there are insufficient training samples.
factors that impact the prediction performance of seer.
the superiority of svr over see models is consistent with some previous works but contradicts some others .
one of the possible reasons would be the usage of different evaluation approaches that resulted in different training sizes.
the performance of see models can be affected by the training set size.
for instance rvm performed the second best among all baseline models for small and large training size sets but when the training set size was medium it ranked the fourth after svr lr and atlm.
some other factors that may affect the results of see model comparisons include the data sets used in the study the type of preprocessing the performance metrics the model parameter tuning and the amount of fine tuning of the methods .
summary.
our synthetic data can often improve the prediction performance of k nn and svr though the improvement is usually small or insignificant.
at least our synthetic data is not detrimental.
474esec fse november lake buena vista fl usa liyan song leandro l. minku and xin yao .
.
summary.
our synthetic projects are particularly helpful for lr and atlm on small medium data sizes moderately helpful for rvm and rt and not very helpful for k nn and svr.
nevertheless they are rarely detrimental to the baseline performance.
we have also computed the predictive performance based on mae applied to actual efforts not in the logarithm scale .
the supplementary material will be available in leandro minku s homepage.
the key conclusion that syn.seer always performed similarly better than bsl.seer remained the same.
therefore results with mae applied to actual efforts were omitted due to space constraints.
.
underlying reasons for the effect of our synthetic data on prediction performance this subsection aims to answer rq2.
given the results summarised in sec.
.
.
rq2 can be rephrased as rq2.
why do our synthetic projects usually have positive effect to an see model?
rq2.
why do our synthetic projects have different improvement magnitude for different see models?
when the training set size is large an see predictor can usually achieve relatively good performance leaving limited improvement space for using our synthetic projects.
thus our discussion will focus on the case with insufficient training examples.
.
.
positive effect of synthetic data.
this subsection aims to answer rq2.
in view of the augmentation of data set and the enhanced ability to cope with data noise.
the main reason would be the augmentation of the see data by encompassing the synthetic projects into the construction of see models which directly tackles the data scarcity problem of see.
another reason would be the enhanced ability to cope with data noise that can lead to large variations from the actual values i.e.
large noise.
effort values are highly likely to contain noise due to the participation of humans in see data collection .
when training examples are insufficient such noise is more likely to mislead the construction of see models resulting in less correct and unstable performance.
when the training data contains noise and the amount of noise is smaller than the predictive information the synthetic projects can compensate the possibly negative effect and enhance the prediction robustness.
figure illustrates the positive effect of our synthetic data on a linear see model.
our approach emphasizes the more typical areas of learning space helping to avoid being misled by large noise.
specifically the training projects that locate in crowded regions which are less likely to contain large variations are more likely to be chosen for generating our synthetic projects.
in this way our synthetic data emphasizes the space with small or no noise and impacts the neighbourhood of those training projects by encoding more representatives.
this would enhance the robustness of this local area when being used to construct an see model.
on the other hand our synthetic projects can be rarely generated in sparse regions where large variations are more likely to exist.
in this way we can circumvent the issue of introducing data noise that can lead to large variations from the actual values.
in this sense the use of our synthetic data can usually have positive effect to an see model by enlarging the data set and compensating the noisy data.
effort effort featurenoise free training data noisy training data synthetic data linear model trained by noise free data linear model trained by noisy data linear model trained by both noisy and synthetic datafigure illustration that synthetic data can improve the quality of the parameter estimate of a linear see model.
the synthetic project square enhances the robustness of its neighbourhood and alleviates detrimental effect of noisy training examples.
it is noteworthy that data noise can only be filtered out if groundtruth noise free projects are known.
however such ground truth is not known in reality.
therefore coping with noise by filtering would be difficult and our proposed approach can be a good alternative.
moreover our synthetic projects may introduce noise but only in the form of small variations in the projects since our synthetic generator emphasizes the space with smaller variation and generates synthetic data with small change.
.
.
effect of synthetic data on each see model.
this subsection aims to answer rq2.
in view of the locality globality property of each see model.
locality and globality of see models .see approaches that perform estimations based on training examples that are similar to the testing project are referred as local approaches .
the opposite terminology is referred here as globality where the effort estimation is performed based on all training examples regardless of the similarity to the project to be estimated.
recall that our synthetic data can only impact their neighbourhood so the locality globality property of an see model would be a primary avenue to spread the effect of the synthetic data from the neighbourhood to other areas having projects to be estimated.
lr atlm is an example of see models with thorough globality .
all training examples regardless of their similarity to the project to be estimated are used to estimate the model parameters which are then used to predict the effort of the testing project.
therefore the effects of our synthetic data in one area will impact the predictions in the entire space leading to remarkable effect of our synthetic data on the prediction performance.
in particular if synthetic examples are created in an area with several examples where the model can become quite confident in their predictions this could improve the predictions in the areas with less examples where the model would originally not be confident about.
k nn is an example of see models with thorough locality where the prediction of a project is only based on the training examples in its neighbourhood.
therefore the effect of our synthetic data in one area will not impact the predictions in other area causing little effect of our synthetic data.
rtpossesses a hybrid property of globality andlocality .
on the one hand rt has globality .
to construct rt all training examples are used to decide the split features and the corresponding thresholds on which the tree branches are formed.
on the other hand rt haslocality .
to predict the effort of the testing project rt needs to find a branch where the testing project is more similar to the training examples of this branch.
the effort prediction is based on the training subset.
therefore the effect of our synthetic data in one area will impact the predictions in other area to some extent.
475a novel automated approach for see based on data augmentation esec fse november lake buena vista fl usa rvm is another example of see models with a hybrid property of globality andlocality .
on the one hand rvm has globality .
to construct rvm all training examples are used to estimate the optimal model parameters.
on the other hand rvm has locality .
the prediction of rvm is a weighted summation with each weight being the similarity of the testing project to one training example.
in this sense only a subset of training data is used to predict the effort.
therefore the effect of our synthetic data in one area can impact the prediction in the other area in some degree.
svr has a tolerance margin in table with which data noise is tolerant to some extent.
when the synthetic project locates within thetolerance margin it can be seen as a disturbance of its original training example and thus has no effect on the decision of the model parameters.
only when the synthetic projects locates on the tolerance margin namely when it is a support vector it can effect the decision of the model parameters.
in this sense little improvement of using our synthetic projects is probably cased by the much less opportunity for them to be chosen as support vectors .
.
comparison of synthetic generators this subsection aims to answer rq3 how well our proposed synthetic generator perform compared with other synthetic generator in the see literature?
to answer rq3 we compare the performance of our synthetic generator against its only competitor in see denoted by syn.cmp.seer .
.
.
syn.seer vs syn.cmp.seer.
table shows the performance comparisons of the two synthetic generators.
we can see that regardless of the see model the performance using our synthetic generator syn.seer is often better than the performance using the competing synthetic generator syn.cmp.seer especially when the training set size is not large.
the effect size between syn.seer and syn.cmp.seer across runs of each see data set is checked and exhibited on the cells in the columns of syn.seer .
we can see that when the training set size is large syn.seer usually has similar performance to syn.cmp.seer .
when the training set size is not large the superiority of our synthetic generator over its competitor can be considerable depending on see models.
the superiority magnitude of syn.seer over syn.cmp.seer is often large for lr and atlm moderate for rt and rvm and small for k nn and svr.
wilcoxon signed rank tests with holm bonferroni correction at the significance level .
between syn.seer andsyn.cmp.seer show that when the training set size is not large our synthetic generator is always superior to its competitor by having significantly better prediction performance.
.
.
syn.cmp.seer vs bsl.seer.
comparing the performance ofbsl.seer in table and syn.cmp.seer in table we can see that syn.cmp.seer cannot outperform bsl.seer in many cases.
effect size across the runs of each data set between syn.cmp.seer and bsl.seer is always small or insignificant indicating that the synthetic examples generated by the literature do not have considerable impact on the prediction performance.
wilcoxon signed rank tests with holm bonferroni correction at the significance level .
show that syn.cmp.seer is similar to bsl.seer in most cases.
however the competing synthetic generator was claimed to be effective in improving the performance of its baseline model .
further examines found that the experiments of were based ondesharnais data set only.
the reported superiority of using their synthetic data was small and no statistical test was conducted.
we suspect that with more data sets into their experiments and using statistic tests their conclusions would probably be no significant difference with or without using their synthetic data.
threats to validity internal validity.
we did multiple wilcoxon tests to evaluate the statistical significance of the results which may induce type i error.
for instance to answer rq1 we performed wilcoxon post hoc tests between syn.seer andbsl.seer across data sets for see models in training set sizes leading to total comparisons.
however we do not consider it to be very serious to this study because these p values were usually considerably small indicating very confident difference.
besides the size of difference was also checked by effect size alleviating the problem of multiple comparisons.
another potential threat to validity is the three extra parameters when using our synthetic generator.
we did not investigate a very large number of possible values for these parameters.
despite that our synthetic generator showed its effectiveness in improving performance of lr atlm rvm and rt when the training set size is not large.
therefore we do not consider further parameter tuning as essential for this study.
as a future work we will investigate the impact of parameter settings and present guidelines to tune the model parameter.
construct validity.
our analyses are mainly based on mae in the logarithm scale for being not biased towards under or overestimation and for alleviating the dominance of very large effort values.
our preliminary studies showed that using other performance metrics such as median absolute error led to similar results as using mae.
as a future work other performance measures could be investigated.
external validity.
this study has not explored a full range of see models to be used with our synthetic generator in all see data sets.
we may not be able to generalize the obtained findings to other see models or other see data sets.
nevertheless since the chosen see models have been shown to be the state of the art and the data sets covering a wide range of see data this paper offers good support in the effectiveness of our synthetic generator in addressing the data scarcity problem of see.
conclusions we proposed a novel synthetic data generator to address data scarcity problem of see.
our approach produces a similar synthetic project by displacing a training example that is chosen randomly.
the generated synthetic projects are then added to the training data set and used to train see models.
experimental results show positive effect of our approach in improving the baseline performance of see models and its superiority over the only synthetic generator of see literature .
we validate our data generator by answering the three research questions a follows.
ans1.
experiments show that our synthetic projects always have positive effect on and are rarely detrimental to the performance of all see models investigated.
they are particularly helpful for small and medium data set sizes for lr and atlm moderately helpful for rvm and rt and not very helpful for k nn and svr.
nevertheless they are hardly detrimental to the baseline performance.
476esec fse november lake buena vista fl usa liyan song leandro l. minku and xin yao table performance comparisons between syn.seer andsyn.cmp.seer across data sets in terms of mae with small medium and large training set sizes.
the reported values are the mean of runs followed by their stds.
the effect size across runs of each see data set is used to measure the performance difference between syn.seer vssyn.cmp.seer and between syn.cmp.seer vsbsl.seer which is exhibited in the cells associated with syn.seer andsyn.cmp.seer respectively.
the orange dark grey bold yellow light grey bold bold font indicates large medium small effects size.
the last two rows list the results of wilcoxon tests with bonferroni correction across all data sets the rows associated with syn.seer list the wilcoxon results between syn.seer vssyn.cmp.seer and the rows associated with syn.cmp.seer list the wilcoxon results between syn.cmp.seer vsbsl.seer .
significant difference of wilcoxon tests is highlighted in orange dark grey .
a small training set size.
data syn.lr syn.cmp.lr syn.atlm syn.cmp.atlm syn.k nn syn.cmp.
k nn syn.rvm syn.cmp.rvm syn.rt syn.cmp.rt syn.svr syn.cmp.svr maxwell .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cocomo81 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nasa93 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kitchenham .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
albrecht .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kemerer .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deshar .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org6 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org7 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
wilcoxon p value .
.
.
.
.
.
.
.
.
.
.
.
b medium training set size.
data syn.lr syn.cmp.lr syn.atlm syn.cmp.atlm syn.k nn syn.cmp.
k nn syn.rvm syn.cmp.rvm syn.rt syn.cmp.rt syn.svr syn.cmp.svr maxwell .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cocomo81 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nasa93 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kitchenham .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
albrecht .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kemerer .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deshar .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org6 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org7 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
wilcoxon p value .
.
.
.
.
.
.
.
.
.
.
.
c large training set size.
data syn.lr syn.cmp.lr syn.atlm syn.cmp.atlm syn.k nn syn.cmp.
k nn syn.rvm syn.cmp.rvm syn.rt syn.cmp.rt syn.svr syn.cmp.svr maxwell .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cocomo81 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nasa93 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kitchenham .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
albrecht .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kemerer .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
deshar .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org6 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
org7 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
wilcoxon p value .
.
.
.
.
.
.
.
.
.
.
.
ans2.
the positive effect of our synthetic data is mainly due to the data augmentation and the robustness enhancement in the areas that the noise of see projects may injure the quality of see model training.
different see models have different improvement magnitude that can be usually affected by their locality globality .
ans3.
studies show that our synthetic generator is significantly superior to or has no significant difference from its only competitor of see literature .
besides the competing generator probably brings no significant improvement over the baseline see models.
future work includes further investigation of the impact of parameter settings and tuning guidelines investigation of more performance metrics and comparisons against the random strategythat assigns synthetic effort values with the outputs of randomly chosen training projects.