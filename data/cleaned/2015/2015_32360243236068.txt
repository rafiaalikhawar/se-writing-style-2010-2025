deepsim deep learning code functional similarity gang zhao texas a m university college station texas usa zhaogang92 tamu.edujeff huang texas a m university college station texas usa jeff cse.tamu.edu abstract measuring code similarity is fundamental for many software engineering tasks e.g.
code search refactoring and reuse.
however most existing techniques focus on code syntactical similarity only while measuring code functional similarity remains a challenging problem.
in this paper we propose a novel approach that encodes code control flow and data flow into a semantic matrix in which each element is a high dimensional sparse binary feature vector and we design a new deep learning model that measures code functional similarity based on this representation.
by concatenating hidden representations learned from a code pair this new model transforms the problem of detecting functionally similar code to binary classification which can effectively learn patterns between functionally similar code with very different syntactics.
we have implemented our approach deepsim for java programs and evaluated its recall precision and time performance on two large datasets of functionally similar code.
the experimental results show that deepsim significantly outperforms existing state of theart techniques such as deckard rtvnn cdlh and two baseline deep neural networks models.
ccs concepts software and its engineering software libraries and repositories maintaining software software evolution keywords code functional similarity control data flow deep learning classification acm reference format gang zhao and jeff huang.
.
deepsim deep learning code functional similarity.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
introduction measuring similarity between code fragments is fundamental for many software engineering tasks e.g.
code clone detection permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa association for computing machinery.
acm isbn .
.
.
.
search and reuse refactoring bug detection .
while code syntactical similarity has been intensively studied in the software engineering community few successes have been achieved on measuring code functional similarity.
most existing techniques follow the same pipeline first extracting syntactical features from the code in the form of raw texts tokens or ast then applying certain distance metrics e.g.
euclidean to detect similar code.
however the syntactical representations used by these techniques significantly limit their capability in modeling code functional similarity.
in addition such simple distance metrics can be ineffective for certain feature spaces e.g.
categorical .
moreover separately treating each code fragment makes it difficult to learn similarity between functionally similar code with different syntactics.
there exist a few techniques that construct a program dependence graph pdg for each code fragment and measure code similarity through finding isomorphic sub graphs.
compared to syntactical feature based approaches these graph based techniques perform better in measuring code functional similarity.
however they either do not scale due to the complexity of graph isomorphism or are imprecise due to the approximations made for improving scalability e.g.
mapping the subgraphs in pdg back to ast forest and comparing syntactical feature vectors extracted from ast .
in this paper we present a new approach deepsim for measuring code functional similarity that significantly advances the state of the art.
deepsim is based on two key insights.
first a feature representation is more powerful for measuring code semantics if it has a higher abstraction because a higher abstraction requires capturing more semantic information of the code.
control flow and data flow represent a much higher abstraction than code syntactical features such as tokens and ast nodes .
hence deepsim uses control flow and data flow as the basis of the similarity metrics.
more importantly we develop a novel encoding method that encodes both the code control flow and data flow into a compact semantic feature matrix in which each element is a high dimensional sparse binary feature vector.
with this encoding we reduce the code similarity measuring problem to identifying similar patterns in matrices which is more scalable than finding isomorphic sub graphs.
second instead of manually extracting feature vectors from the semantic matrices and calculating the distance between different code we leverage the power of deep neural networks dnn to learn a similarity metric based on the encoded semantic matrices.
in the past decade dnn has led to breakthroughs in various areas such as computer vision speech recognition and natural language processing .
recently a few efforts have also been made to tackle program analysis problems with dnn.
we design a new dnn model that further learns high level features esec fse november lake buena vista fl usa gang zhao and jeff huang figure deepsim system architecture.
from the semantic matrices and transforms the problem of measuring code functional similarity to binary classification.
through concatenating feature vectors from pairs of code fragments this model can effectively learn patterns between functionally similar code with very different syntactics.
as depicted in figure deepsim consists of two main components code semantic representation through encoding control flow and data flow into a feature matrix code similarity measurement through deep learning.
the first component can take code fragments in any language as input in the form of source code bytecode or binary code as long as a data flow graph and a control flow graph can be constructed.
the second component is a novel deep learning model containing two tightly integrated modules a neural network module that extracts high level features from the matrices of a code pair and a binary classification module that determines if the code pair is functionally similar or not.
these two modules work together to fully utilize the power of learning the neural network module extracts latent representation for the classifier and the signal propagated backward from the classifier in turn helps the neural network learn better representation from the input.
finally the trained model can be used to measure code functional similarity.
we have implemented deepsim for java at the method level based on the wala framework and tensorflow .
we evaluated deepsim on two datasets a dataset of google code jam projects and the popular bigclonebench which contains over tagged clone pairs and false clone pairs.
our results show that deepsim significantly outperforms the state ofthe art techniques with f1 score10.
for google code jam and .
for bigclonebench compared to .
and .
the highest for the other techniques such as deckard rtvnn cdlh as well as two stacked denoising autoencoder sda baseline deep learning models.
meanwhile deepsim achieves very good efficiency even faster than deckard .
the contributions of this paper are as follows we present a novel approach for measuring code functional similarity which encodes code control and data flow into a single semantic feature matrix.
we design a new deep learning model that learns high level representation from the semantic matrices and learns a functional similarity metric between code with different syntactics.
we implement a prototype tool deepsim2and evaluate it extensively with two large datasets showing significant performance improvements over the state of the art techniques.
1f1 score recall precision recall precision 2it is publicly available at background our deep learning model is based on feed forward neural networks.
in this section we first review the basic concepts.
.
feed forward neural network feed forward neural network also named multi layer perceptron mlp is an universal yet powerful approximator and has been widely used.
it is essentially a mapping from inputs to outputs f rm0 rmk where m0is the dimensionality of inputs mkis the dimensionality of outputs generated by the last layer lk.
each layer liis a mapping function fli rmi rmi i k.fis hence the composed function of all its layers f flk flk fl1 x .
the mapping function of each layer is composed by its units called neurons .
a neuron aij the j th neuron in layer i takes the outputs generated by previous layer or the initial inputs as inputs and calculates a scalar output value through an activation function aij wt ijai bij ai ai1 ... aimi where wij rmi j mi.
there are various activation functions according to different network designs.
in this work we use elu c cifc ec 1ifc where cis a scalar value .
the structure that several neurons form a layer and several layers form the integrated neural network has been proved effective in learning complicated non linear mapping from inputs to outputs .
moreover with the increasing computational power of modern gpus the depth of neural networks the number of layers can be made very large with a large training dataset the so called deep learning .
a representative feed forward dnn model is deep autoencoder which aims to learn a compact hidden representation from the inputs.
unlike standard encoding approaches e.g.
image audio compression it uses neural networks to learn the target encoding function instead of explicitly defining it.
the goal is to minimize the error e.g.
squared error of reconstructing the inputs from its hidden representation h1 w1x b1 hk wkhk bk h k w hk b k x w 1h b j x x nn i x x 142deepsim deep learning code functional similarity esec fse november lake buena vista fl usa where wi rmi mi 1andbi rmiare the weights and biases of layer li w i wt iif we use tied weights hkthe encoded representation finally obtained xthe input of the model x the reconstruction of x and nthe number of all input samples.
minimizing the reconstruction error forces this model to preserve as much information of the raw input as possible.
however some identity properties existed in the raw input may make the model simply learn a lookup function.
therefore salt noise or corruption process is usually added to the input of the autoencoder which is called stacked denoising autoencoder sda .
the corruption process works as randomly setting some components of the input to which actually increases the sparsity of the autoencoder.
the reconstruction process then tries to use this corrupted input to reconstruct the raw input h w x b x w h b j x x nn i x x where x is the corruption process.
in this way it learns the hidden representation through capturing the statistical dependencies between components of the input.
.
model training the training of neural networks is achieved by optimizing a defined loss function through gradient decent.
for autoencoder the reconstruction error discussed before is used as the loss function.
for binary classification a cross entropy loss function is usually used j nn i 1s i lo s i s i lo s i where w b s i is the ground truth label for sample i and s i is the predicted label.
to optimize j we can apply gradient descent to update w andb nn j here nis the size of batching samples for each update.
the learning rate can be used to control the pace of parameters update towards the direction of gradient descent.
decaying with time is generally a good strategy for finding a good local minimum .
given the large depth of dnn it is ineffective to compute the derivative of the loss function separately for each parameter.
hence back propagation is often applied.
it consists of three stages a feed forward stage to generate outputs using the current weights a back forward stage to compute the responsive error of each neuron using the ground truth outputs and feed forward outputs and a weights updating stage to compute the gradients using responsive errors and update the weights with a learning rate.
encoding semantic features we encode the code semantics represented by control flow and data flow into a single semantic matrix.
in this section we present our encoding process.
.
control flow and data flow control flow analysis and data flow analysis are two fundamental static program analysis techniques .
control flow captures the dependence between code basic blocks and procedures and data flow captures the flow of data values along program paths and operations.
the information derived from these analyses is fundamental for representing the code behavior.
our basic idea is to encode code control flow and data flow as relationships between variables and between basic blocks e.g.
operations control jumps .
to obtain the control flow and data flow we may perform the analysis on either source code binary code or any intermediate representation e.g.
java bytecode llvm bitcode .
in this paper we focus on java bytecode using the wala framework .
a example code b control flow graph of m1 figure control flow and data flow example.
figure illustrates a code example and its control flow graph cfg .
in the cfg each rectangle represents a basic block and in each basic block there may exist several intermediate instructions.
for each basic block a data flow graph dfg can be generated.
note that each variable and each basic block contain its type information.
for example xis astatic integer variable and bb1is a basic block that contains a conditional branch .
we describe how we encode them in more details in the next subsection.
.
encoding semantic matrices we consider three kinds of features for encoding the control flow and data flow information variable features basic block features and relationship features between variables and basic blocks.
variable features.
for each variable its type features contain its data type v t bool byte char double float int long short void other primitive types jdk classes user defined types its modifiers 143esec fse november lake buena vista fl usa gang zhao and jeff huang v m final static none and other information v o basic array pointer reference .
we encode them into a 19d binary feature vector v v m v o v t .
consider the variable xin figure 2a.
it is a static int variable so we have v .
basic block features.
for each basic block we consider the following seven types normal loop loop body if if body switch switch body .
other types of basic blocks e.g.
try catch basic block are regarded as normal basic blocks.
similarly we encode this type information into a 7d one hot feature vector b. for example in figure 2b bb1 is a ifbasic block and its representation b .
relationship features between variables and basic blocks.
we encode data flow and control flow as operations between variables and control jumps between basic blocks.
in total we identify different operation types and use a 43d one hot feature vector ito represent it3.
to preserve the information of each operation between variables in the dfg we derive a 81d feature vectors t vop1 vop2 i where vop1andvop2denote the variable feature vector for operand op1andop2respectively.
to preserve the control flow we extend tto be vop1 vop2 i b of size e .
now we formally define three rules to encode information for each relationship between variables and basic blocks t op1 op2 vop1 vop2 i bbbop1 encodes the operations between two variables op1andop2 as well as the type information of the two variables and the corresponding basic block.
here bbop1denotes the basic block that op1belongs to.
t op bb vop v0 i0 bbb encodes the relationship between a variable and its corresponding block.
here v0 i0 denote zero feature vectors.
t bb1 bb2 v0 v0 i0 bbb2 encodes the relationship between two neighbor basic blocks.
here bb2is a successor of bb1in the cfg.
definition .
semantic feature matrix a .given a code fragment we can generate a matrix using the encoding rules above that captures its control flow and data flow information.
each row and column in ais a variable or a basic block.
their order corresponds to the code instruction and basic block order.
a i j t i j is a binary feature vector of size e that represents the relationship between row iand column j here i j ... n where n nv bb is the summation of variables count and basic blocks count.
example.
consider the example in figure again.
its cfg has variables local variables and static fields and basic blocks including enter return exit block .
from the encoding rules above we can derive a sparse matrix as visualized in figure .
compared to the raw cfg and dfg our semantic feature matrix representation has two advantages.
first it reduces the problem of finding isomorphic subgraphs to detecting similar patterns in matrices.
second this structure i.e.
a single matrix makes it easy to use for the later processes such as clustering or neural networks in our approach.
3in our current implementation we rely on wala s instruction types to distinguish different operations and control jumps.
in particular for some instruction types e.g.
ssabinaryopinstruction that have more than one optional opcodes e.g.
add div etc.
we treat each opcode as a different operation.
figure semantic features matrix generated from method m1in figure .
along zaxis are the 88d binary feature vectors.
the value is represented by a blue dot and represented by empty.
it is also worth noting that for most syntactically similar code that only differ in their identifier names literal values or code layout the generated semantic matrices will be identical because these differences are normalized by cfg and dfg.
this property ensures that our approach can also handle syntactical similarity.
we acknowledge that our matrix representation does not encode all information in a pdg.
however all dependence are implicitly encoded in our representation.
for example the code y x z x contains an input dependence it is encoded as t y x andt z x .
other types of dependence are encoded in similar way.
learning based code similarity measuring from our semantic matrix encoding described in the previous section we can see that the semantic matrices generated from two functionally similar methods with syntactical differences may not be identical.
we cannot directly use simple distance metric e.g.
euclidean distance for measuring their similarity since we have no knowledge about whether some elements i.e.
feature vector t in a semantic matrix is more important than another.
moreover different elements may be functionally similar e.g.
binary add operations between two intandlong variables respectively but it is difficult to detect this similarity directly on the semantic feature matrix.
to address this issue we develop a deep learning model that can effectively identify similarity patterns between semantic matrices.
in particular we train a specially designed feed forward neural network that generates high level features from semantic matrices for each code pair and classify them as functionally similar or not using the classification layer at the end of the neural network.
this design eliminates the need to define a separate distance metric on the learned features.
more importantly this model can be finely trained in a supervised manner to learn better representation from the input through backward propagating the label signal of the classification layer i.e.
see eq.
.
this helps our model effectively learn patterns from similar code with very different syntactics.
144deepsim deep learning code functional similarity esec fse november lake buena vista fl usa .
features learning figure a schematic of our feed forward model for measuring code functional similarity.
the architecture of our model is shown in figure .
as a static neural graph this model requires a fixed size of input semantic feature matrices.
similar to recurrent neural network we process all input matrices to a fixed size k k k 128in this work .
for smaller methods we pad their feature matrices with zero value.
for larger methods we truncate the matrices through keeping all cfg information and discarding part of dfg information that exceeds the matrix size the original feature matrix size is n nv nb after truncation k nv nb where nb nb nv nb .
recall that we encode data flow and control flow into 88d sparse binary feature vectors t. similar to word embedding we first maptinto hidden state h0of size c .
this layer makes it possible to learn similarity between two relationship feature vectors.
for example binary add operations between two intandfloat variables respectively may share some similarities.
we will discuss this further in section .
.
handling code statement re ordering.
we then flatten each row of ato a vector with length of c k. we add two fully connected layers taking the flattened row feature vectors as inputs in order to extract all information related to the variable or basic block represented by each row.
at last a pooling layer is added to summarize all the row feature vectors.
to mitigate the effect of code statement re ordering we use average pooling instead of flattening the entire matrix.
int c int x c int y c x y y x y int c int y c int x c y x y x y consider the two small programs above which have identical code statements but different statement orders at lines and andbetween lines and .
the different statement orders at lines and lead to different xandyvalues at the end of the two programs.
in our matrix representation we encode all the data flow between the three variables and the statement order decides which variable or basic block that each row of the matrix represents.
thus we generate two different as.
however if we only consider the statements at lines the two programs should be equivalent because there are no dependence between the two statements at lines and .
therefore to make our model more robust in handling the statement re ordering we ignore the order of independent variables or basic blocks and keep only the order of variables or basic blocks with dependence between them.
this can be achieved by performing average pooling on all those neighbor rows of athat represent independent variables or basic blocks and flattening the rest rows the order is preserved in the flattened vector .
however this would lead to various flattened vector lengths for different methods which is difficult for a static neural network model to handle.
instead we make approximation by directly performing averaging pooling on all rows and leaving the rest task to our deep learning model.
interestingly another advantage of this design is that it reduces the model complexity because the pooling layer reduces the input size from c k ktoc kwithout any extra parameters similar to pooling layers in convolutional neural network .
thus the training and predicting efficiency of our approach is increased.
similar to the vanilla multi layer autoencoder recall section .
this model can be trained in an unsupervised manner through minimizing the reconstruction error eq.
and eq.
.
the training consists of two phases.
each layer is first pre trained to minimize the reconstruction error and then the entire model is trained through minimizing its overall reconstruction error.
in this way the model can learn a non linear mapping from the input ato a compressed latent representation.
however the model trained by this approach may discriminate two similar but not identical inputs e.g.
two array search methods that respectively handle intandfloat arrays.
to enforce the model to learn similar patterns across different samples we utilize supervised training which is achieved by the binary classification layer that takes as input the concatenation of the final latent representations of two methods.
we describe it in more details in the next subsection.
.
discovering functional similarity given two methods with learned latent representations a straightforward way to measure their functional similarity is calculating their distance in the hidden state space.
this metric is applied by many existing techniques .
however such a simple distance metric is not satisfiable under two considerations.
first users have to perform heuristic analysis to find a suitable distance threshold for every dataset.
euclidean distance for measuring code similarity may not be applicable in the hidden space.
for example in the xor problem if we use euclidean distance the input would be closer to and than which is a wrong classification.
second a deterministic distance metric cannot leverage existing training dataset to finely train the model.
though it is feasible to obtain an optimal distance threshold using a training dataset the parameters of the neural network model itself are not updated in this process.
145esec fse november lake buena vista fl usa gang zhao and jeff huang a sda for learning latent feature representation b measuring code similarity using latent representation figure sda baseline models.
m1is a semantic matrix agenerated from a method c1andc2are two feature vectors generated by sda from a method pair.
for sda base we put the two parts together when training while for sda unsup we do not back propagate the error signal of b into a .
we propose an alternative approach that formulates the problem of identifying functionally similar code as binary classification.
more specifically as shown in figure we concatenate the latent representation vectors h13andh23from methods m1andm2in different orders .
then we apply a fully connected layer on the two concatenated vectors with sharing weights wf c. another average pooling is performed on the output states and finally the classification layers are added.
we can now use crossentropy as the cost function eq.
and minimize it to optimize the entire model.
in this way our model is able to learn similarity between each code pair with different syntactics.
optimization.
the two concatenations with different orders and the average pooling operation are important for optimizing the efficiency of our model.
note that as a similarity measuring task we have a symmetric property s mi mj s mj mi .
if we use only one concatenation or we have to add both mi mj and mj mi into our training dataset to satisfy the symmetric property which will lead to nearly 2x training and predicting time.
when applying this model to discover functionally similar methods on a code repository of size n we would need to run itn n times.
considering that the classification layers only contain very few computations of the entire model less than matrix multiplications for our trained model the efficiency can be significantly improved almost 2x speedup by separating the model into two parts during prediction.
more specifically for the nmethods we run the feed forward phase of the model to generate their latent representation h3.
then for each method pair we only run the classification phase.baseline model.
as a comparison we also train an sda as the baseline model as depicted in figure .
in the baseline we try to flatten each matrix to a long input vector of size k k e i.e.
if k and e .
however even we set a small size of the first hidden layer e.g.
there are over .
108parameters which makes the training difficult unlike sda our model applies learning on each row followed by a row average pooling hence its layer size is limited .
therefore we slightly change the feature vectort.
ast vop1 vop2 i b vop v m v o v t we use a bit integer to represent each component of it thus we drive at of size this can be understood as a handcrafted feature vector as the characteristic vector in .
the other parts follow the standard sda model.
we develop two settings for the sda baseline model sda base andsda unsup .
sda base also adds classification layers on top of it combine the two models in figure taking the concatenation of two latent representation vectors as inputs with only one concatenating order .
thus the fine training step will also optimize the feed forward encoding part of the model.
sda unsup uses a separate classification model with one hidden layer as shown in figure 5b.
it works by estimating an optimal similarity metric in the hidden space.
in both cases we apply the same loss function in eq.
.
the goal of these two settings is to present a comparison between our model and the vanilla sda and to understand the effectiveness of our binary classification formulation.
experimental evaluation .
datasets we evaluated deepsim on two datasets google code jam gcj and bigclonebench .
in the first experiment we follow recent literature and use the programs from the google code jam competition as our dataset.
we collected projects from competition problems as shown in table .
each project for the same problem is implemented by different programmers and its correctness has been verified by google.
we have manually verified that the problems are totally different.
therefore programs for the same problem should be functionally similar i.e.
label and those for different problems should be dissimilar i.e.
label .
to better understand this benchmark we manually inspected projects for each problem and found that very few projects pairs in total each in mushroom monster andrank and file and each in brattleship andsenate evacuation can be classified as syntactically similar4.
note that we did not remove any of these syntactically similar code because our approach is also capable of handling them as explained in section .
in addition we found that more than of the projects contain at least one obvious statement re ordering.
most of them are caused by different orders of variable definitions used in loops or parameters for functions such as min max etc.
in the second experiment we run deepsim on the popular bigclonebench dataset .
bigclonebench is a large code clone benchmark that contains over tagged clone pairs and false clone pairs collected from systems.
each clone 4our criterion for syntactically similar code is very relax the number of different lines of code can be up to and no structural difference.
146deepsim deep learning code functional similarity esec fse november lake buena vista fl usa table the google code jam dataset.
dataset statistics value projects total lines of code tokens vocabulary size similar method pairs dissimilar method pairs pair in bigclonebench is also a pair of methods and is manually assigned a clone type.
the total number of clone pairs obtained from the downloaded database is slightly different from that reported in .
we discard code fragments without any tagged true or false clone pairs and discard methods with loc less than .
in the end we obtained 50k methods including .5m tagged clone pairs and 200k tagged false clone pairs.
most true false clone pairs belong to clone type wt3 t4.
.
implementation and comparisons for deepsim and the sda baseline models we use wala to generate cfg dfg from bytecode of each method and construct the semantic feature matrix as described in section .
for efficiency we store all the matrices in a sparse format.
we use tensorflow to implement the neural networks models.
for the gcj dataset we developed a plug in in the eclipse ide to automatically inline source code file of each project to a single method.
for bigclonebench because it does not provide the dependency libraries for the source code files we modified wala and the polyglot compiler framework to generate cfg dfg for these source code files directly.
for any external class type we replace it with a unique type java.lang.object .
this results in failures to get all fields in some cases.
however our tool is able to process over of the clone pairs in bigclonebench.
we also compared deepsim with the three other state of the art approaches deckard rtvnn and cdlh .
deckard is a classical ast based technique that generates characteristic vectors for each subtree using predefined rules and clusters them to detect code clones.
in our experiment we used the stable version available in github .
rtvnn uses recursive neural networks to measure code similarity using euclidean distance.
different from our approach their model operates on source code tokens and ast and learns hidden representation for each code separately i.e.
unsupervised learning rather than combining each code pair to learn similar patterns between them.
since the rtvnn implementation is not available we implemented the approach following the paper .
cdlh is a recent machine learning based functional clone detector that applies lstm on asts.
the cdlh paper does not provide details about their experimental settings.
to make a fair comparison we compare with their reported numbers.
training.
we apply fold cross validation to train and evaluate deepsim and the two baseline models on the two datasets.
namely we partition the dataset into subsets each time we pick subset as test set the rest subsets as training set.
we repeat this times and each time the picked test subset is different.
the reported result is averaged over the times.table parameter settings for different tools.
tool parameters deckard min tokens stride similarity threshold .
sda base layers size epoch initial learning rate .
for l2 regularization .
corruption level .
dropout .
sda separate feed forward phase layers size epoch 1for l2 regularization .
corruption level .
initial learning rate .
classification phase layers size epoch dropout .
initial learning rate .
2for l2 regularization .
rtvnn rtnn phase hidden layer size epoch 1for l2 regularization .
initial learning rate .
clipping gradient range .
.
rvnn phase hidden layer size epoch initial learning rate .
1for l2 regularization .
distance threshold .
deepsim layers size 128x6 epoch initial learning rate .
for l2 regularization .
dropout .
for rtvnn because it needs to extract all the tokens and build vocabulary before training we have to train and evaluate it using the same full dataset following the same experiment setting as .
we try a range of different super parameters e.g.
learning rate layer sizes regularization rate dropout rate various activation functions and weights initializers etc.
for each model and record their testing errors and f1 scores.
for deckard it has no training procedure but a few parameters.
we also tune its parameters and choose the set of parameters that achieved the highest f1 score on the full dataset.
the optimal super parameters that achieved the highest f1 score for these models are listed in table .
.
results on gcj table results on the gcj dataset.
tool recall precision f1 score deckard .
.
.
sda base .
.
.
sda unsup .
.
.
rtvnn .
.
.
deepsim .
.
.
.
.
recall and precision.
table reports the recall and precision of deepsim on gcj compared to the other approaches.
recall means the fraction of similar method pairs retrieved by an approach.
precision means the fraction of retrieved truly similar method pairs in the results reported by an approach.
overall deepsim achieved .
recall while deckard .
rtvnn .
sda base .
and sda unsup .
and deepsim achieved much higher precision .
than deckard .
and rtvnn .
as well as sda base .
and sda unsup .
.
147esec fse november lake buena vista fl usa gang zhao and jeff huang deckard achieved low recall and precision.
the reason is that to recognize a similar method pair deckard requires the characteristic vectors of parser tree roots for the two methods to be very close which is essentially the syntactics of the entire code.
to better understand this result we picked all solutions from one competition problem and found that more than half of the functionally similar code pairs have diverse parser tree structures making them being predicted into different clusters by deckard.
sda base and sda unsup achieved comparable recall and are better than deckard and sda base s precision is much higher than sda unsup.
deepsim achieved much higher recall than deckard and the two baseline models.
this indicates that deepsim effectively learns higher level features from the semantic matrices than the other approaches and the encoded semantic matrix is also a better representation than the syntactical representation used by deckard.
rtvnn achieved the highest recall but very low precision.
we found that rtvnn almost reports all method pairs as similar.
the reason is that rtvnn relies on the tokens and ast to generate the hidden representation for each method and applies a simple distance metric to discriminate them.
however two functionally similar methods may have significant syntactical differences and two functionally dissimilar methods may share syntactically similar components e.g.
io operations .
after further analyzing the experiment results we found that the distances between most methods are in the range of .
through reducing the distance threshold the precision of rtvnn could be significantly improved up to however its recall also drops quickly down to less than .
as a result it only achieved f1 score .
at the highest.
.
.
false positives false negatives.
the precision of deepsim is .
which still has a large improvement space.
after checking those false positives and false negatives of deepsim we found that this is mainly due to the tool s limitation in handling method invocations .
for example for the problem senate evacuation some solution projects use standard loops and assignment statements while other solution projects employ utility methods such as map andreplace .
because we do not explicitly encode the information inside the callee method into the semantic feature matrices deepsim cannot distinguish these utility methods and their corresponding statements.
nevertheless this is a common limitation for most existing static code similarity measuring techniques .
considering that deepsim could generate a final hidden representation for each method after training incorporating this information to encode method invocation instructions is feasible re training may be necessary .
in addition utilizing constraints solving based approaches as a post filtering to the reported results may also further improve the precision.
.
.
time performance.
we also evaluated the time performance of these approaches on the full dataset in total .4m method pairs .
we run each tool with the optimal parameters on a desktop pc with an intel i7 .0ghz cores cpu and gtx gpu.
for deepsim and the two sda baseline models they need to generate semantic feature matrices from bytecode files so we also include the time of this procedure into the training time.
for deckard we use the default maximal number of processes i.e.
.
we run each tool three times and report the average.table time performance on gcj.
tool prediction time training time deckard 72s sda base 1230s 8779s sda unsup 37s 1482s rtvnn 15s 8200s deepsim 34s 13545s table reports the results.
deckard does not need training so it has zero training time.
sda unsup separates latent representation learning and classification phases thus it takes fewer training epochs to get stable and has the lowest training time among the three models.
while deepsim has fewer neurons than sda base sda base has a huge amount of neurons in the first layer it takes more computation since its first three layers are applied on each element or row of the semantic feature matrices.
thus deepsim takes the longest training time.
although the training phase of deepsim is slower than the other approaches it is a one time offline process.
once the model is trained it can be reused to measure code similarity.
for prediction rtvnn takes the least time as it only needs to calculate the distance between each code pair and compare it with the threshold.
deepsim takes the 2nd least time even faster than deckard because it only needs to generate the semantic feature matrices and the prediction phase is performed by gpu which is fast.
sda unsup takes approximately the same time as deepsim while sda base is 30x slower because it has to run the whole model for each code pair.
.
results on bigclonebench table results on bigclonebench.
tools recall precision f1 score deckard .
.
.
rtvnn .
.
.
cdlh .
.
.
deepsim .
.
.
table f1 score for each clone type.
clone type deckard rtvnn cdlh deepsim t1 .
.
.
.
t2 .
.
.
.
st3 .
.
.
.
mt3 .
.
.
.
wt3 t4 .
.
.
.
tables report the results on bigclonebench.
the recall and precision are calculated according to .
the results of deckard rtvnn and cdlh correspond to that reported in .
for this dataset deepsim significantly outperforms all the other approaches for both recall and precision.
the f1 score of deepsim is .
compared to .
by cdlh.
deepsim does not achieve .
148deepsim deep learning code functional similarity esec fse november lake buena vista fl usa f1 score on t1 st3 clones which should be guaranteed by our encoding approaches as discussed in section because the tool misses some data flow when generating dfg from source code due to the approximation we introduce to handle external dependencies in wala.
since the true false clone pairs from bigclonebench are from different functionalities we run another experiment that use all the true false clone pairs with functionality id as training dataset since it contains approximately true false clone pairs of the whole dataset and the rest data as testing dataset.
the result is shown in table which is consistent with the results reported in tables .
table results of deepsim trained using data from single functionality.
clone type recall precision f1 score t1 .
.
.
t2 .
.
.
st3 .
.
.
mt3 .
.
.
wt3 t4 .
.
.
we also note that for wt3 t4 clone type the f1 score of deepsim on bigclonebench is higher than that on the gcj dataset this is consistent with the comparison result between bigclonebench and another oj dataset ojclone as reported in .
we inspected several wt3 t4 clone pairs and found that although they are less than similar at the statement level which is the criterion for wt3 t4 clone type many of them follow similar code structure and differ only on the sequence of the invoked apis.
in contrast the gcj projects are all built from scratch by different programmers with different code structures.
it is hence more difficult to detect functional clones in the gcj dataset.
.
discussion why deepsim outperforms the other dnn approaches.
the reasons are three fold.
first deepsim is based on the semantic feature matrix that encodes dfg cfg which is a higher level abstraction than the lexical syntactical representation e.g.
source code tokens or ast used by the other dnn approaches such as rtvnn and cdlh.
compared to the two sda baselines deepsim takes the semantic feature matrix as input while for the sda baselines each 88d feature vector tis manually re encoded to 8d and the entire matrix is flattened to a vector.
the manual re encoding may lose information since each element in the 8d vector is actually categorical data and a standard neural network model is limited in this scenario.
in addition flattening the entire matrix aggravates the impact of statement re ordering because the flattened vector is strictly ordered which causes reporting more false positives.
second deepsim and the two sda baselines all contain classification layers to support the classification of a method pair while rtvnn only calculates the distance between the hidden representations of two methods which may not be applicable as we discussed before.
note that the f1 score of deepsim sda base and sda unsup are all higher than rtvnn this should be partly attributed to theeffectiveness of our binary classification formulation and the fine training stage.
third the deepsim model uses two average pooling layers which sda base and sda unsup do not have.
the first one computes the average states along all rows of the matrix which reduces the side effect caused by the statement re ordering.
the second one computes the average states of two different concatenations of final latent representations from a method pair which guarantees the symmetric property of code similarity i.e.
s mi mj s mj mi .
effectiveness of the encoding approach.
in the first layer of deepsim we map each feature vector tof size to a 6d vector in the hidden space.
to analyze our trained model we construct eight different feature vectors each is encoding of a specific operation between variables.
we map them into the embedding space and visual them with pca dimensionality reduction as shown in figure .
figure first layer hidden representation of different input feature vectors generated by deepsim.
we can observe that there are four very close points corresponding to four different instructions int variables addoperation in normal basic block short variables addoperation in normal basic block double variables addoperation in normal basic block int variable and static int variable addoperation in normal basic block.
according to their meanings they are quite similar and should be close in the embedding space.
this manifests that our encoding does effectively preserve the data flow information and our model successfully learned the similarity between these operations.
we also note that the two identical operations from different basic blocks denoted by the plum and cyan points in the figure are not close in the embedding space.
this indicates that the code structure i.e.
control flow information is also well encoded into the feature vector and successfully learned by our model for measuring functional code similarity.
the visualization of the hidden states for the eight instructions shows the effectiveness of our semantic feature matrices that encode data flow and control flow.
more importantly our specially designed dnn model can learn high level features from the input semantic feature matrix and patterns between functionally similar code with different syntactics.
this significantly distinguishes our approach from the other approaches which rely on syntactical representation.
semantic feature matrix size.
in this work we fix the size of the semantic feature matrix to .
it works well for our datasets but such fixed length reduces computation efficiency for methods with smaller length and may lose some information for methods 149esec fse november lake buena vista fl usa gang zhao and jeff huang with larger length.
to support variant matrix length we can integrate our model with recurrent neural network rnn .
however a challenge is that rnn requires a fixed input sequence order which is difficult to handle code statement re ordering.
a stacked lstm with attention mechanism may be a potential solution.
larger dataset.
as a deep learning approach deepsim s effectiveness is limited by the size and quality of the training dataset.
building such a large and representative dataset is challenging.
even bigclonebench only covers functionalities and consists of less than 60k functions which is a tiny proportion of the full ijadataset .
in future work this can be addressed by incrementally building a very large dataset through crowd sourcing platform such as amazon mechanical turk .
we also plan to build a web platform so that users can upload samples to try our tool and verify the result.
the collected data will help improve the accuracy of our model.
other related work code clone detection.
according to carter et al .
roy and cordy code clones can be roughly classified into four types.
type i iii code clones only differ at tokens and statement level while type iv code clones are functionally similar code that usually have different implementations.
existing code clone detectors mainly target type i iii code clones through measuring code syntactical similarity based on representations such as text tokens abstract syntax tree or parse tree .
su et al .
proposed to measure functional code similarity based on dynamic analysis.
this approach captures runtime information inside callee functions through code instrumentation but it may miss similar code due to the limited coverage of test inputs.
several other approaches have focused on scaling code clone techniques to large repositories .
in addition gabel and su studied code uniqueness on a large code repository of 420m loc and observed significant code redundancy at granularity of tokens.
barr et al .
studied patching programs by grafting existing code snippets in the same program and how difficult this grafting process is.
their result indicates a promising application of code similarity measuring techniques.
machine learning for measuring code similarity.
carter et al.
proposed to measure code similarity through extracting handcrafted features from source code and applying self organizing maps on them.
yang et al .
derived more robust features from source code based on inverse frequency reverse document frequency then applied k means to cluster code and predict the class for a new code fragment using the cosine distance metric.
this approach cannot handle code fragments that do not belong to any existing clusters.
li et al .
also applied deep learning on code clone detection.
they first identified tokens that are likely to be shared between programs then computed the frequency of each token in each program.
given a code pair they computed a similarity score for each token based on their frequencies.
the similarity score vector is then fed to the deep learning model to classify the code pair.
since token frequency is still a syntactical representation the ability of this approach for measuring code functional similarity may be limited.semantic code search and equivalence checking .
reiss used both static and dynamic specifications e.g.
keywords class of method signatures test cases contracts to search target code fragments.
stolee et al .
and ke et al .
used symbolic execution to build constraints for each code fragment in the codebase and searched target code with input output specifications.
the returned code fragment should satisfy both the type signature and the conjoined constraints.
this approach may not scale well as limited by symbolic execution and smt solver.
moreover deissenboeck et al.
reported that of program chunks across five open source java projects refer to project specific data types which makes it difficult to directly compare inputs outputs for equivalence checking across different projects.
partush and yahav presented a speculative code search algorithm that finds an interleaving of two programs with minimal abstract semantic differences which abstracts the relationships between all variables in the two programs.
this may not be applicable for non homologous programs since there is not a clear mapping between most statements and variables.
conclusion we have presented a new approach deepsim for measuring code functional similarity including a novel semantic representation that encodes the code control flow and data flow information as a compact matrix and a new dnn model that learns latent features from the semantic matrices and performs binary classification.
we have evaluated deepsim with two large datasets of functionally similar code and compared it with several state of the art approaches.
the results show that deepsim significantly outperforms existing approaches in terms of recall and precision and meanwhile it achieves very good efficiency.