performance prediction of configurable software systems by fourier learning yi zhang university of waterloo y825zhan uwaterloo.cajianmei guo university of waterloo gjm gsd.uwaterloo.caeric blais university of waterloo eric.blais uwaterloo.cakrzysztof czarnecki university of waterloo kczarnec gsd.uwaterloo.ca abstract understanding how performance varies across a large number of variants of a configurable software system is important for helping stakeholders to choose a desirable variant.
given a software system with noptional features measuring all its2npossible configurations to determine their performances is usually infeasible.
thus various techniques have been proposed to predict software performances based on a small sample of measured configurations.
we propose a novel algorithm based on fourier transform that is able to make predictions of any configurable software system with theoretical guarantees of accuracy and confidence level specified by the user while using minimum number of samples up to a constant factor.
empirical results on the case studies constructed from real world configurable systems demonstrate the effectiveness of our algorithm.
i. i ntroduction many software systems allow users to customize the software behavior with a finite set of configuration options which we refer to as features that users may decide to select or deselect.
each feature combination gives rise to a particular variant orconfiguration of the system which in turn has a particular performance measure such as execution time or throughput.
in order for users to decide on an optimal configuration for their particular purpose they first need an understanding of the relationship between feature selection and performance hence giving rise to the fundamental problem of performance prediction of configurable software systems.
take execution time as an example since it is simply infeasible to run all 2n configurations for a system with nfeatures the key challenge is to accurately predict the performance of the system on all configurations by measuring only a small number of sample configurations as is being actively studied in many recent works .
to address the challenge we first formalize it as an equivalent mathematical problem.
the mapping from any particular configuration of a system with nfeatures to its performance value can be formalized as a boolean function f f0 1gn!r for example given n the statement f 22s means that the execution time of the system with its second and fourth features selected is .
seconds.
then for any given system with nfeatures the performance value of allits configurations can be solely and completely captured by a particular function f as in that maps any of its configurations c2f0 1gnto a value in r which we will hereafter refer to as the performance function .
therefore the problem of predicting performance of a configurable system reduces to the problem of learning the performance function with a small number of its values given by the measurements i.e.
producing an estimate of f x for everyx2f0 1gn.
although there are many established learning algorithms it is well known that arbitrary boolean functions of this form simply cannot be learned.
since the domain of such functions is a finite set with an arbitrary function f knowingf a provides practically no information about f b unlessa b. that is to say performance values cannot be predicted by merely taking samples of configurations.
fortunately previous empirical results have shown that performance functions of actual software systems are not arbitrary but rather structured hence can be potentially learned effectively.
but what exactly the structures are that actual performance functions exhibit was not made clear.
in this work we have explored one explicit notion of structure of actual performance functions of being close to fourier sparse functions namely functions that have many s in their fourier decompositions .
furthermore we have proposed an algorithm that is able learn these fourier sparse functions to user specified accuracy and confidence level by taking only a small number of sample configurations such that we in turn achieve accurate predictions of the actual underlying performance function of a configurable system.
in summary the contributions of this work include a theoretical model and formulation of configurable software performance prediction in terms of fourier learning of boolean functions.
an algorithm for learning configurable software performance functions via their fourier decomposition.
an implementation analysis and evaluation of the algorithm with its strengths and relative weaknesses compared to existing performance prediction methods.
the rest of the paper is organized as follows.
section ii introduces the mathematical background of fourier transform and formalizes the problem of performance prediction.sections iii and iv presents the algorithm itself and its implementation considerations.
sections v and vi discuss the experimental results and evaluations of the algorithm.
the paper concludes after the related and future work in section vii.
ii.
p roblem formalization having established software performance functions as boolean functions of the form f f0 1gn!r in this section we introduce the mathematical basis of fourier transform of boolean functions and formalize the problem of software performance prediction in terms of its mathematical equivalence.
a. fourier transform consider a function fof the form f f0 1gn!r f x f x1 x2 x n y2r a natural representation of fis the truth table view shown in table i where each xi a boolean value indicating if a feature is selected represents the ith coordinate of the input vectorx andf x its function value.
table i. t ruth table view of a boolean function f x1x2.
.
.xnf x .
.
.
0y1 .
.
.
0y2 .
.
.
0y3 ............yi .
.
.
1y2n we now explore an alternative view of the function f. given a function f f0 1gn!r we can rewrite it in the following form f x x z2f0 1gn f z z x where f z 2rare the fourier coefficients and z x are thecharacter functions defined as z x ifpn i 1zixi mod 1ifpn i 1zixi mod intuitively this fourier transform of fsimply decomposes it into the sum of 2n simpler functions with coefficients indexed by all vectors z2f0 1gn.
we say the function f ist sparse if at most tout of 2nof the fourier coefficients are non zero.
with a given dimension n since all 2ncharacter functions are fixed the 2nfourier coefficients then uniquely determine the function f. in other words knowing all the fourier coefficients is equivalent to knowing the function fitself since given the fourier coefficients one can exactly calculate any function value f x and perhaps less trivially vice versa.with the inner product defined on functions of interest f g f0 1gn!r hf gi 2nx x2f0 1gnf x g x it can be easily checked that the set of functions f z z2f0 1gng forms an orthonormal basis of the entire set of functions from f0 1gntor.
in other words h z1 z2ievaluates to if and only ifz1 z2and otherwise.
therefore we can derive the following properties for any two vectors z1 z22 f0 1gn we have z1 x z2 x z1 z2 x where isxor of the two bit vectors or equivalently addition modulo .
to calculate any particular fourier coefficient f z we have hf z i x z2f0 1gn f z z z x z2f0 1gn f z h z z i f z h z z i f z by orthonormality of the character functions.
immediately we have parseval s identity hf fi x z2f0 1gn f2 z with the inner product defined in and combining with we define the usual notion of l2norm of functions and distance between them.
l2norm kfk2 p hf fi distance dist f g kf gk2 x z2f0 1gn f z g z from it is now clear that fourier coefficients are uniquely determined by the function f hence we have the equivalence between a function fand its fourier coefficients.
from we define the distance between two functions f andgto be the square of the l2norm of their difference f g which by is the sum of all the differences in their squared coefficients.
coincidentally by this distance is exactly the mean square difference betweenfandgon all inputs.
b. problem formalization recall our goal is to predict the performance of software systems based on their configurations.
in this section we formalize exactly what it means.
for a software system with nfeatures there are overall 2npossible configurations each of which has a particular performance measure.
therefore it is natural to think of theprocess of mapping a configuration to its performance as a boolean function that can take any real number value f f0 1gn!r where any input bit vector x2f0 1gnrepresents a particular configuration in which feature iis selected to be on if and only ifxi and vice versa.
to make performance prediction is to estimate f x given any configuration vector x. this is to say that we need to learn what the function fthat corresponds to the software system produces at any given point.
from the previous section we realize that learning a function fis equivalent to learning all of its fourier coefficients .
in particular as we will discuss in section iii functions constructed from real software systems are typically close to fourier sparse in which case predicting its performances becomes equivalent to estimating only the large fourier coefficients of its corresponding function f. this is exactly what our algorithm does.
the problem of learning a performance function can thus be formalized as follows.
fix a target function f the learning algorithm takes two inputs and and outputs an estimate hoffsuch that dist f h with probability .
iii.
t hefourier learning algorithm a. overview given a target function f a high level summary of the fourier learning algorithm can be described as follows take a random sample of configurations and their performance values of the system.
use the sample to learn the fourier coefficients of f hence reconstructing an estimate hoff.
estimate dist f h if it is larger than increase the sample size and repeat.
with a clear definition of the problem at hand and the tool of fourier transform introduced in the previous section we are now ready to discuss each step of the learning algorithm in more detail.
b. normalization in order to aid error analysis of the algorithm we normalize the original function f f0 1gn!r to f0 f0 1gn!
such that new function f0has a finite range.
in practice it is often feasible to establish perhaps with some domain knowledge the upper and lower bounds on theperformance values of a particular system e.g.
between and max .
thenfcan be normalized by subtracting max 2from eachf x and then dividing the result by max .
from this point on we will then use fas the function after normalization.
c. learning fourier coefficients in this section we describe how fourier coefficients of a functionf f0 1gn!
can be learned.
from the fourier coefficient corresponding to each vectorz2f0 1gncan be calculated as f z hf zi 2nx x2f0 1gnf x z x this is essentially an average of the function values over thef0 1gndomain weighted by the particular character function corresponding to the fourier coefficient that we are trying to estimated.
now when a sample sis taken such that f x is known for allx2s the same formula can be used to estimate all the fourier coefficients.
for each z2f0 1gn f z can be estimated as f z hf zi jsjx x2sf x z x obviously the more samples we take i.e.
the closer our sample set sis to the entire domain the more accurate our estimations of f z will be.
the hoeffding s inequality formalizes this result.
theorem iii.
hoeffding s inequality .
letx1 x2 x mbe independent and identically distributed random variables with range .
let the theoretical expected value of xibe e xi p8i m and let the sum random variable be y mx i 1xi then we have pr jy m pj exp 2m b a intuitively the theorem states that the probability of the sample average i.e.
y m drifting distance apart from the theoretical average pis exponentially small in the number of samplesmand the distance squared .
to apply the theorem to our estimations of fourier coefficients we realize the following for each fourier coefficient f z the random variables arexi f xi z xi and the expectation for eachxiis the actual value of the coefficient therefore we have e xi f z according to .
we normalized the function values to be all between 1and1 therefore by parseval s identity we have all fourier coefficients in the range .
we demand each estimations of our fourier coefficients to be accurate within with probability 2n such that the probability of allfourier coefficients are accurate within will be at least 2n 2n then we can rewrite the inequality as pr exp 2m 2n where f0 z is the approximation of f z .
rearrange and solve for m we have m log n log to summarize the relationship between the number of samples and the accuracy is as follows theorem iii.
.
given any function f f0 1gn!
with1 probability all 2nfourier coefficients of fcan be learned with at most additive error using log n log number of samples.
d. error analysis in this section we describe how the algorithm guarantees the accuracy of its estimate h by constructing a series of estimates of fand bounding their distances from f. step given a function f letgbe the function obtained fromfby only keeping its tlargest fourier coefficients and set the rest to such that dist f g d. then by construction gist sparse.
step leth1be the function obtained from gby replacing all its fourier coefficients smaller than d tby namely h1 x x z g z d t g z z x noticeh1is also at most t sparse.
theorem iii.
.
letf gandh1be defined as above then dist f h1 d d2 tstep leth2be an estimate of h1with each of its nonzero coefficients accurate to d 4t i.e.
h1 z h1 z h2 z d 4t then obviously dist h1 h2 x z2f0 1gn h1 z h2 z t d 4t d2 16t step lethbe our estimation of f obtained by learning each off s fourier coefficients to accuracy d 4twith probability as described in section iii c. if the estimation of any coefficient is less than 3d 4t we set it to .
notice according to theorem iii.
this can be done with 32t2 d2 log n log number of samples.
in this case for any zsuch that the original fourier coefficient f z d t we have an estimate of it h z accurate tod 4t.
on the other hand for all zsuch that f z d 2t h z .
then we realize that dist h f dist h2 f since h2 z is set to whenever f z d t which is a coarser cut.
now we can establish dist h f dist h2 f kh2 fk2 kh2 h1k2 kh1 fk2 dist h2 h1 dist h1 f 2kh2 h1k2 kh2 h1k2 d2 16t d d2 t d 4p t r d d2 t d o d2 t sinced as distance between fandg is bounded as tgets large dist h f is bounded above by o d .
in this case we havedist h f 4d 3as soon ast which is a very small sparsity number.
to summarize this step we realize that as long as there exists at sparse function gthat is within d close tof we could take the number of samples specified in to construct an estimate hsuch thatdist h f 4d .
step now we would like to estimate dist h f by drawing a sample sfromfand calculate jsjx x2s f x h x again by theorem iii.
using 2d2log samples is sufficient to estimate dist h f withind 3accuracy with probability .ifdist f h 4d our estimate will be larger than d. therefore we are confident that the original function fisnot sufficiently close to any t sparse function.
in this case we need to increasetand draw more samples to guarantee dist f h .
we have found that typical performance functions of software systems are sufficiently close to t sparse functions with smallt hence having a relatively small sample size.
however the algorithm works with functions with generic sparsity since it is determined implicitly within the algorithm.
to conclude the learning algorithm is written out in full inalgorithm .
it applies to any generic function f and with only a linear sample dependency on the dimension n it outputs an estimation hsuch thatdist h f with probability .
algorithm the fourier learning algorithm input n dimension of f. target estimate error.
confidence level parameter.
t0 starting sparsity optional .
inc multiplicative increment of t optional .
output has an estimate of f. initialization t t0or d p m1 32t2 d2 log n log m2 2d2log learn ifm1 m2 2nthen fis computed exactly.
else drawm1random samples to estimate all 2nfourier coefficients of fusing .
these are the coefficients of the estimation h. for each h z 3d 4t let h z .
drawm2more samples and calculate d0 dist f h using .
end if ifd0 dthen returnh else t inc tort t go to line .
end if iv.
i mplementation to evaluate the algorithm we have implemented it in java and run it against data constructed from real world software systems across different domains.
the experimental details will be presented in section v here we discuss a few implementation considerations of the algorithm.a.
feature selection before the algorithm is run we do a preliminary feature selection.
in a software system if a particular set of features must be on or must be off then they have effectively no discriminant power with respect to the software performances across configurations therefore they are simply ignored and the number of features can be thus reduced.
b. partially defined functions so far the algorithm we have described has only dealt with functions defined on the entire domain of f0 1gn i.e.
software systems that every possible configuration is valid and has a performance value.
however this is often not the case in reality.
actual software systems often have features interacting with each other that may result in some combinations of them being invalid.
a simple example could be that turning the debug mode on will force the log to file option being on as well.
therefore any configuration of the software with debug being and log to file being would notbe valid.
our learning algorithm deals with this neatly.
letd f0 1gnthe set of bit vectors representing all valid configurations.
notice the size of dsatisfiesjdj 2n.
in this case since the uniformly random samples are taken from the set dinstead of the entire set of f0 1gn all estimated fourier coefficients simply needs to be rescaled by a factor of jdj 2n.
alternatively scaling the estimated function values constructed from the fourier coefficients by the same factor yields the same results.
in practice valid configurations are often defined in terms of boolean constraints.
therefore obtaining the number of valid configurations may require solving non trivial sat instances expressed in the feature model namely counting the number of satisfiable assignments of a boolean satisfiability sat problem which is outside the scope of discussion of this work.
empirical results from both exact solver and approximate solver have suggested that this is usually feasible up to a relatively large number of features so it is not a primary concern here.
v. e valuation a. subject systems for evaluation of our algorithm we used the public data sets of five software systems used in and .
they are software systems in different domains and written in different languages.
a brief summary of the subject software systems is shown in table ii as briefly mentioned in section iii the number of samples the algorithm requires is o n for a given set of accuracy parameters.
although this dependency on nis very good for large systems for the small data sets we have the required number of samples is more than the entire valid domain for any reasonable accuracy parameters due to the constant hidden in theo n notation.table ii.
s ummary of original software systems system domain lang.jdjn apache web server c x264 encoder c llvm compiler c berkeley db database c berkeley db database java lang.
language of the software system.
jdj number of valid configurations.
n number of features after trivial selection.
to still use the data sets for meaningful empirical evaluations of the algorithm we constructed four hybrid systems by combining some of the original systems together in a natural way.
letf f0 1gm!randg f0 1gn!rbe the performance functions of two systems we simply construct f g f0 1gm n!r f g x f g x1 x m n f x1 x2 x m g xm x m n to be the function of total performance of the combined configurations in f andg.
and the hybrid systems corresponding to the performance of running sequentially each of the two involved systems once assuming no interactions between them are summarized in table iii.
table iii.
s ummary of constructed hybrid systems system component jdjn a apache x264 b llvm x264 c x264 x264 d llvm llvm b. experimental setup recall the algorithm essentially establishes a relationship between the number of samples it takes and its prediction accuracy and confidence level.
for our experiments we will fix the confidence level at i.e.
2and vary the desired accuracy parameters on each system to attempt to verify the theoretical error bounds of the algorithm.
our initial sparsity parameter was set to be t0 and the multiplicative increase between iterations was set to be a conservative .
each experiment will be repeated times and the results are presented in section v c. the experiments are run on a single ubuntu .
machine with intel core i7 cpu .
ghz and gb of ram.
c. experimental results here we present the results of our experiments on the data sets described in section v a.a high level summary of the experimental results in terms of the desired error the actual error and the number of samples taken for each system is presented in table iv.
table iv .
s ummary of experimental results systemn samples mean max a .
.
.
.
.
.
.
.
.
b .
.
.
.
.
.
.
.
.
.
.
.
c .
.
.
.
.
.
.
.
.
.
.
.
d .
.
.
.
.
.
.
.
.
.
.
.
n number of features of system.
user specified maximum error.
samples number proportion of samples used.
mean average actual error from the runs.
max maximum actual error from the runs.
as shown the actual prediction errors of the algorithm has fallen within the specified bound on all instances.
the small difference between the maximum error over the runs and the mean error on all systems also suggests a very small variance that the algorithm produces on a given system hence promoting its stability and reliability.
on the other hand the prediction errors across different systems may vary widely even with the same specified level of accuracy principally due to the different intrinsic structures of systems.
for example system b has an average error of when the user desires an error of whereas that of system c is only .
furthermore although the maximum error for all these systems have fallen within .
even when is set to be .
the algorithm still decides to increase the number of samples when is decreased.
this is because the number of samples is derived primarily from to guarantee the error and confidence interval for allsystems of given size.
again better thanguaranteed errors on these systems show their further structure but the algorithm is designed to work with full generality.
notice the number of samples the algorithm used across different systems and accuracy parameters it is evident that the sample complexity of the algorithm is o and only linear inn.
therefore in much larger systems the sample complexity of the algorithm would have great benefits in making performance predictions within bounded errors.
systems b c and d are all consistent in achieving the desired accuracies with less than of all samples.
however for system a the number of valid configurations is a smaller subset of the entire domain hence giving rise to the number of required samples occupying a larger proportion of the space of all valid configurations.vi.
d iscussion a. performance measure as briefly mentioned in section ii a we have used the notion of distance between two functions defined as dist f g kf gk2 2nx x2f0 1gn f x g x to be the error of our learning algorithm throughout.
notice this definition coincides exactly with the standard mean square error as a common performance measure for machine learning algorithms.
some previous studies have adopted the average relative error defined as 2nx x2f0 1gnf x g x f x as the performance measure which is known to produce disproportionately large value of errors when f x is close to0.
since our algorithm normalized all function values to be around using this performance metric here clearly does not make much sense.
b. comparative analysis there has been two studies by guo et al.
and siegmund et al.
on software performance prediction that have used the same data sets for evaluating their algorithms.
we give a brief comparative analysis between our algorithm and the previous ones in this section.
algorithms splc onqueror is a deterministic sampling method that focuses on predicting software performances by measuring specific samples according to some heuristics and understanding the interactions between different features in particular pairwise feature interactions.
cart on the other hand utilizes statistical methods and random sampling techniques in order to categorize unknown configurations into groups of known configurations according to their containment of certain features and hence predict their performance.
both these methods employ a top down work flow where a number of samples are taken from the system then the performance values of each configuration are predicted before they are compared to actual values for their accuracy.
the fourier learning algorithm on the other hand attempts to determine the number of samples necessary for any desired accuracy in an adaptive progressive sampling manner which may be especially beneficial when the costs of sampling or measurements are high and prediction accuracy critical.
given the different natures and work flows of the algorithms it is not immediately obvious how they could be directly comparable to each other hence the rest of the section will only attempt to give a qualitative account of several aspects of their relative strengths and weaknesses.
sample complexity requirements splc onqueror requires at least n2 designated samples just to cover the feature wise and pairwise measurements where nagain is the number of features.
cart does not have explicit sample complexity constraints therefore employs a progressive sampling scheme to double the number of random samples until the prediction accuarcy becomes satisfactory.
with an implicit progressive random sampling scheme similar to cart used in but with the theoretical underpinnings of fourier learning in contrast to cart we can explicitly bound our sample complexity to o n log to be confident in achieving an error within .
parameter tuning although all three algorithms seem to be fully automated cart seems to have the largest number of parameters to consider such that the algorithm does not over fit the sample.
the fourier learning algorithm has two optional parameters t0andinc to control the initial and incremental number of samples respectively.
smaller t0and an inccloser to represents more conservative approaches which is more suitable if measurements are costly.
splc onqueror on the other hand relies more on heuristics rather than explicit parameters.
execution time for performance prediction purposes execution time of a learning algorithm is typically not an important concern.
since the process of gathering the data of software performances is typically much more costly in terms of time and effort the minimum number of necessary measurements namely the sample complexity therefore is of greater importance here.
splc onqueror and cart are both fairly fast.
the fourier learning algorithm having to approximate all 2n fourier coefficients potentially more than once can be relatively slow.
however some basic knowledge of the specific data sets may allow us to infer properties of their fourier spectrum and hence dramatically speed up its execution time.
for instance for the four hybrid systems we constructed we know they consist of two independent components with no interactions between them.
this saves the algorithm the effort of estimating many coefficients that we know are .
furthermore we used the most naive method for estimating large coefficients in the algorithm for conceptual simplicity.
more sophisticated and much faster methods such as do exist and can be readily substituted as a subroutine into our algorithm with minimal alteration.
summary all three proposed algorithms have their distinct features and characteristics.
for larger systems with many features and a relatively full set of valid configurations where measurement costs are potentially high and accuracy critical the fourier learning algorithm is a suitable choice.
on the other hand for smaller systems e.g.
n or systems with known low degree feature interactions then cart and splc onqueror respectively might be more appropriate interms their trade off between accuracy and measurement efforts.
the main characteristics of the methods are summarized in table v. table v .
m aincharacteristics of the learning algorithms splc onqueror cart fourier accuracy any sample size o n2 anyo n sampling specific random random error control no no yes c. threats to validity potential threats to the validity of our results primarily comes from two angles the model and the datasets.
our model for configurable software performance prediction is based on a fundamental abstraction of a performance functionf f0 1gn!rfor any given software system which maps a valid configuration to a real number representing the performance value of our interest.
although this abstraction is obviously valid for simple performance measures such as software execution time with fixed workload it might require adjustments for more sophisticated software feature structures or performance objectives.
for systems with non boolean features our approach can not be used directly.
however siegmund et al.
has shown that non boolean features can be easily expressed in terms of boolean features in higher dimensions.
for example a feature having options a b and c could be expanded to features namely isa?
isb?
and isc?
.
then our approach can be readily employed.
similarly with multiple performance objectives the performance function can be viewed as a multi dimensional function f f0 1gn!rm wheremis the number of performance measures.
then each performance measure can be dealt with simultaneously and separately.
since our approach is a black box method that operates on a high level of abstraction more software specific concerns such as varying workload and multi user scenarios might pose more unexpected threats to our model.
however one might be able to see how these variations can be feasibly incorporated into the modelling of features or performance objectives via some transformation such as the ones outlined above and hence assume these threats are minimal.
as for the datasets we used the openly available software system measurements originally generated by siegmund et al.
for our experiments as off the shelf datasets.
we are aware that using the combined hybrid systems as constructed in section v a may lose some representativeness of the systems however the theoretical analysis of our algorithm should provide sufficient confidence that it is applicable to any general function that fits the abstraction regardless of its system structures and the experiments should merely serve as empirical evidence confirming the correctness of the analysis.vii.
r elated and future work one of the first established techniques for solving the problem as we have described in section vi b1 was splc onqueror due to siegmund et al.
which employs various heuristics and considers particular feature combinations and interactions to achieve performance prediction.
guo et al.
thereska et al.
westermann et al.
and sarkar et al.
have all utilized statistical sampling and machine learning approaches in particular cart for configurable software performance prediction.
there are two related fourier learning algorithms that can be viewed as more advanced versions of our algorithm.
the first one due to uses better searching techniques hence reducing the time complexity of estimating the large fourier coefficients to only polynomial time in the number of features n. however it essentially requires the function to be defined on the entiref0 1gndomain.
how it could be modified to be able to deal with partially defined functions is not immediately clear hence can be explored in future work.
another direction of adjustment of our algorithm is proposed by where not only the magnitudes of estimated fourier coefficients are restricted but also their indices giving even tighter probabilities of potential over fitting.
however it is not clear that some of the assumptions it makes fit well in the domain of software systems.
for example it assumes that the indices of large fourier coefficients are mostly of small weight which in software terms means that most significant feature interactions only involve a small number of features.
this method therefore would completely ignore potential feature interactions involving more than a certain number of features.
future work should investigate whether this assumption would hold in practice.
one further future work is related to batory s proposal of quantifying feature interactions in software systems.
we believe that batory s notion of feature interaction can be formalized as derivatives of boolean functions therefore feature interaction detection can be reduced to estimating derivatives of boolean functions which is in turn closely related to estimating their fourier coefficients .
therefore an interesting future work would involve using the tools we proposed so far in estimating fourier coefficients and translating them into a formal model of detecting feature interactions of configurable software systems with minimal extra cost.
viii.
c onclusion software performance prediction is a fundamental problem in software engineering that deserves much attention.
in this work we formalized the model of configurable software performance prediction in terms of learning boolean functions explicitly explored the fourier sparsity property of performance functions of real software systems and proposed and implemented the fourier learning algorithm that is able to make performance predictions with guaranteed accuracy and confidence level.to conclude we have introduced a new perspective on treating the problem of predicting software performance.
with increasingly large software systems desires of guarantees on prediction accuracy and more precise understanding of measurement efforts more formal approaches such as this one may prove beneficial in the long term.