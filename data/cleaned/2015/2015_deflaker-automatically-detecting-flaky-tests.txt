deflaker automatically detecting flaky tests jonathan bell1 owolabi legunsen2 michael hilton3 lamyaa eloussi2 tifany yung2 and darko marinov2 1george mason university fairfax va usa 2university of illinois at urbana champaign urbana il usa 3carnegie mellon university pittsburgh pa usa bellj gmu.edu legunse2 eloussi2 yung4 marinov illinois.edu mhilton cmu.edu abstract developers oftenrun tests tocheck thattheir latest changesto a coderepositorydidnotbreakanypreviouslyworkingfunctionality.
ideally any new test failures would indicate regressions caused by thelatestchanges.however sometestfailuresmaynotbeduetothe latestchangesbutduetonon determinisminthetests popularly calledflakytests.thetypicalwaytodetectflakytestsistorerun failingtestsrepeatedly.unfortunately rerunningfailingtestscan be costly and can slow down the development cycle.
we present the first extensive evaluation of rerunning failing testsandproposeanewtechnique calleddeflaker thatdetects if a test failure is due to a flaky test without rerunning and with very low runtime overhead.
deflaker monitors the coverage of latest code changes and marks as flaky any newly failing test that did not execute any of the changes.
we deployed deflaker live inthebuildprocessof96javaprojectsontravisci andfound87 previously unknown flaky tests in of these projects.
we also ran experiments on project histories where deflaker detected flaky tests from failures with a low false alarm rate .
.
deflaker had a higher recall .
vs. of confirmed flaky tests than maven s default flaky test detector.
ccs concepts software and its engineering software testing and debugging keywords software testing flaky tests code coverage acm reference format jonathanbell owolabilegunsen michaelhilton lamyaaeloussi tifany yung and darko marinov.
.
deflaker automatically detecting flaky tests.inproceedingsoficse 40thinternationalconferenceonsoftware engineering gothenburg sweden may june icse pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
icse may june gothenburg sweden copyright held by the owner author s .
publication rights licensed to association for computing machinery.
acm isbn ... .
introduction automatedregressiontestingiswidelyusedinmodernsoftware development.
whenever a developer pushes some changes to a repository tests are run to check whether the changes broke some functionality.
ideally every new test failure would be due to the latest changes that the developer made and the developer could focus on debugging these failures.
unfortunately some failures are not due to the latest changes but due to flaky tests.
as in previous work we define a flaky test as a test that can non deterministically pass or fail when run on the same version of the code.
flaky tests are frequent in most large software and create problems in development as described by many researchers and practitioners .forexample accordingtoherzigandnagappan the microsoft swindowsanddynamicsproductteamsestimatetheir proportionofflakytestfailurestobeapproximately5 .similarly pivotal developers estimate that half of their build failures involve flaky tests labuschagne et al.
reported that of builds inatraviscidatasetfailedbecauseofflakytests andluoetal.
reportedthatflakytestsaccountedfor73kofthe1.6m .
daily test failures in the google tap system for regression testing.
when a test fails developers need automated techniques that canhelpdeterminewhetherthefailureisduetoaflakytestorto a recently introduced regression .
the most widely used technique to identify flaky test failures rerun is to rerun each failingtestmultipletimesafterwitnessingthefailure ifsomererun passes the test is definitely flaky but if all reruns fail the statusis unknown.
rerun is supported by several testing frameworks e.g.
android jenkins maven spring and the google tap system .
developers do notproactively search forflakytestsasamaintenanceactivity insteadsimplyusingrerun to identify that a given test failure is flaky.
thereislittleempiricalguidancedescribinghowtorerunfailing tests in order to maximize the likelihood of witnessing the testpass.
reruns might need to be delayed to allow the cause of the failure e.g.
anetworkoutage toberesolved.flakytestsarenondeterministic by definition so there is no guarantee that rerunning a flaky test will change its outcome.
the performance overhead of rerunscaleswiththenumberoffailingtests foreachfailedtest rerun will rerun it a variable number of times potentially also injectingadelaybetweeneachrerun.rerunning everyfailedtest is extremely costly when organizations see hundreds to millions of test failures per day.
even google with its vast compute resources doesnotrerunall failing testsoneverycommit butreruns only those suspected to be flaky and only outside of peak test execution times .
acm ieee 40th international conference on software engineering authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden bell et al.
... ... magic .java list of changes to monitorversion control system ... ... public class magic current version of code ... ... public class magic previous version of codedeflaker coverage analyzer ast builderdiff tool ... ... magic .java differential coverage reports one per testcoverage instrumenter coverage recorder test outcome monitor rerunnerdeflaker reporter ... ... testma gic.mag ictest list of likely flaky testscurrent commit last build reporting after test execution coverage collection during test execution differential coverage analysis before test execution inputs outputdeflaker figure high level architecture of deflaker with three phases before during and after test execution.
we performed an extensive evaluation of rerun .
on test failures in historical builds of open source mavenbasedjavaprojects.theflakytestdetectorinmaven whichreruns each test shortly after it failed and in the same jvm in which it failed markedonly23 ofthe5 328testfailuresasflaky.byisolatingeachreruninitsownjvm andfurtherrebootingthebuild system to clean the state between reruns we confirmed that in fact at least of those failing tests were flaky.
maven likely doesnotisolatetestrerunsbecauseofthehighcostofcreatinga process loadingclasses andpreparingcleanstateforeachtest ourprior study found that isolating tests can add a overhead .
hence to effectively find flaky tests rerun introduces substantial performance overhead.
developers should ideally be able to know immediately after a test fails that it is flaky.
even if a developer suspectsatesttobeflaky ourgoalistoprovideevidenceforthat suspicion from the outcome of a single test execution if the test fails is it due to a regression or flakiness?
we propose a new and efficient technique deflaker that is complementary to rerun and often marks failing tests as flaky immediatelyaftertheirexecution withoutanyreruns.recallthatatest isflaky ifit both passesand fails when the codethat is executed by the test did not change moreover a test failure is newif the test passed on the previous version of code but fails in the current version.
a straw man technique to detect flaky tests is to collect complete statement coverage for each test of both the test code and the code under test intersect coverage with the changes and reportasflakynewtestfailuresthatdidnotexecuteanychanged code.however collectingfullstatementcoveragecanbeexpensive.
ourkeyinsightindeflakeristhatoneneednotcollectcoverage of theentire codebase .
instead one can collect only the coverage of the changed code which we call differential coverage.
differential coverage first queries a version control system vcs to detect codechangessincethelastversion.itthenanalyzesthecodeand constructsanabstract syntaxtreeforeachchangedfiletodetermine whereinstrumentationneedstobeinsertedtotrackexecutionof each change.
finally when tests run it monitors change execution generating an individual change coverage report for each test.
wepresentourdeflakertoolthatdetectsflakyteststhrough lightweightdifferentialcoveragetracking.ifatestfailsbutdoesnotcoveranychangedcode deflakerreportsthetestasflaky without requiringanyreruns.ourevaluationof deflakerusesatraditionalexperimental methodology on historical builds on our own servers andwealsoproposeanovelmethodologyforevaluatingtestingand analysis tools on open source projects in real time and in the exact same build environments that the projects developers use.
this new methodology allowed us to evaluate deflaker on complexprojects that we could not easily get to compile and execute in our own local environments the traditional methodology .
replicating software builds in a lab environment can be tricky when complex projectsmayincludeahandfulofmanualconfigurationstepsbefore they can compile.
even then subtle differences in environment e.g.
the exact version of java and the distinction between an oracle jvm and an openjdk jvm can lead to incorrect results.
the marginal human cost of adding a new project to an evaluation canbeveryhigh.incontrast whenprojectsarecurrentlydesignedto be automatically compiled and tested in a standard environment e.g.
travisci it can be much easier to study more projects.
ourexperimentsintheliveenvironmentsinvolved93projects and commits and we found previously unknown flaky tests in of these projects.
we reported of these newly detected flakytests anddevelopershavealreadyfixed7ofthem.inordertoperform a larger evaluation without abusing travisci s resources wealsoperformedatraditionalevaluationon26projectsand5 commits running in our own environment in which deflaker detected4 846flakytestfailuresamong5 328confirmedflakytests .
with a low false positive rate .
.
in comparison the currentreruninmavenfoundonly23 ofthesesametestfailures to be flaky.
for projects with very few test failures deflaker can impose almost no overhead only collecting coverage in a single rerunoffailedtests.forprojectswithtoomanyfailurestorerun we found that deflaker imposes a low enough overhead .
to be used with every test run eliminating the need for reruns.
the primary contributions of this paper are new idea a generalpurpose lightweight techniquefor detecting flaky tests by tracking differential coverage.
robust implementation a description of our open source flaky test detector deflaker .
extensive evaluation experiments with various rerun approaches and deflaker on commits of projects taking cpu years in aggregate to our knowledge this is the first empirical comparison of different rerun approaches.
novel methodology a new research methodology for evaluatingtestingtoolsbyshadowingthelivedevelopmentofopensource projects in their original build environment.
deflaker deflaker detects flaky tests in a three phase process illustrated infigure .
in the first phase differential coverage analysis deflaker usesasyntacticdifffromvcsandanastbuildertoidentifyalistof changes to track for each program source file.
in the second phase coveragecollection deflakerinjectsitselfintothetest execution process monitoringthecoverageofeachchangeidentifiedinthe authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deflaker automatically detecting flaky tests icse may june gothenburg sweden public class superold public void magic public class supernew extends superold public void magic assert false causes test to fail public class appextends superold supernew public class testapp testpublic void testapp newapp .magic unchanged linechanges be havior figure sample change that challenges a syntactic diff prior phase.
finally once tests have finished executing deflaker analyzes the coverage information and test outcomes to determine thesetoftestfailuresthatarelikelyflaky.inprinciple thesereports could also be printed immediately as tests fail but we report them at the end of the test run to conform with existing testing apis.
.
differential coverage analysis deflakeranalyzesprogramcodeandversionhistorytodetermine how to track the impact of changed code combining syntactic changeinformationfromavcs inourcase git withstructural informationfromeachprogramsourcefile.deflakertracksthe coverageofchangesto allprogramsourcefiles includingbothtest codeandprogramcode .theoutputofthisphasearelocationsin the program code in which to add coverage probes that are used at runtime to determine if a test executes changed code.
asotherresearcherspointedout e.g.
inthecontextofregressiontestselection andchange impactanalysis using solely syntactic change information is often insufficientin objectoriented languages.
in other words it is necessary to monitor even somesyntacticallyunchangedlinestodeterminethatachangegets executed.for instance changesthatmodify theinheritancestructure of a class or method overloading may cause dynamic dispatch to occur differently at a call site that itself is unchanged.
figure2showsanexampletestpronetosomeofthesechallenges changingthesupertypeof appfrom superold tosupernew would cause the unchanged magicmethod call to refer to a different implementation causingthetesttofailinsteadofpass.similarly addingan emptyimplementationofthe magicmethodto the app class would change the test behavior as well.
to handle these sortsofchanges priorworkeither performsstaticanalysisto model changes at a fine granularity or instead simply tracks thechangesatacoarsegranularity.traditionalapproaches e.g.
jdiff chianti dejavoo andfaulttracer model the dynamic dispatch behavior of the jvm to infer the exact semanticsof eachchange.
theseapproaches canpreciselyidentify theimpactofaddingorremovingamethodtoorfromaclass or changing a type hierarchy allowing downstream tools to take into account any potential changes to method invocation.
morerecentworkhasshownthatinsomecontextsitcanbemore cost effective to model these changes more coarsely greatly reducingthecostoftheanalysis atthecostofsomelossinprecision .
thesetoolstrackclassfilecoverageratherthanstatementcoverage if a test