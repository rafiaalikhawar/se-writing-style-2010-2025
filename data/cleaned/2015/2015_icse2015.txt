an information retrieval approach for regression test prioritization based on program changes ripon k. saha lingming zhangysarfraz khurshid dewayne e. perry electrical and computer engineering the university of texas at austin usa email ripon utexas.edu khurshid ece.utexas.edu perry ece.utexas.edu ydepartment of computer science the university of texas at dallas usa email lingming.zhang utdallas.edu abstract regression testing is widely used in practice for validating program changes.
however running large regression suites can be costly.
researchers have developed several techniques for prioritizing tests such that the higher priority tests have a higher likelihood of finding bugs.
a vast majority of these techniques are based on dynamic analysis which can be precise but can also have significant overhead e.g.
for program instrumentation and test coverage collection .
we introduce a new approach repir to address the problem of regression test prioritization by reducing it to a standard information retrieval problem such that the differences between two program versions form the query and the tests constitute the document collection .
repir does not require any dynamic profiling or static program analysis.
as an enabling technology we leverage the open source ir toolkit indri.
an empirical evaluation using eight open source java projects shows that repir is computationally efficient and performs better than existing dynamic or static techniques for the majority of subject systems.
index terms regression testing test prioritization information retrieval i. i ntroduction programs commonly evolve due to feature enhancements program improvements or bug fixes.
regression testing is a widely used methodology for validating program changes.
however regression testing can be time consuming and expensive .
executing a single regression suite can take weeks .
regression testing is even more challenging in continuous or short term delivery processes which are now common practices in industry .
therefore early detection of regression faults is highly desirable.
regression test prioritization rtp is a widely studied technique that ranks the tests based on their likelihood in revealing faults and defines a test execution order based on this ranking so that tests that are more likely to find new unknown faults are run earlier .
existing rtp techniques are largely based on dynamic code coverage where the coverage from the previous program version is used to order i.e.
rank the tests for running against the next version .
a few recent techniques utilize static program analysis in lieu of dynamic code coverage .
rtp techniques whether dynamic or static are broadly classified into two categories total or additional depending on how they calculate the rank .
total techniques do not change values of test cases during the prioritization process whereas additional techniques adjustvalues of the remaining test cases taking into account the influence of already prioritized test cases.
although a number of rtp techniques specifically coverage based ones have been widely used they have two key limitations .
first coverage profiling overhead in terms of time and space can be significant.
second in the context of certain program changes which modify behavior significantly the coverage information from the previous version can be imprecise to guide test prioritization for the current version.
although the static techniques address the coverage profiling overhead they simulate the coverage information via static analysis and thus can be also imprecise.
this paper presents repir an information retrieval ir approach for regression test prioritization based on program changes.
traditional ir techniques focus on the analysis of natural language in an effort to find the most relevant documents in a collection based on a given query .
even though the original focus of ir techniques was on documents written in natural language recent years have seen a growing number of applications of ir to effectively solve software engineering problems by extracting useful information from source code and other software artifacts .
the effectiveness of these solutions relies on the use of meaningful terms e.g.
identifiers and comments in software artifacts and such use is common in most real world software projects.
in the context of testing and debugging the application of ir has been primarily focused on bug localization .
our key insight is that in addition to writing good identifier names and comments in the code developers use very similar terms for test cases and we can utilize these textual relationships by reducing the rtp problem to a standard ir problem such that program changes constitute the query and the test cases form the document collection.
our tool repir embodies our insight.
we build repir on top of the stateof the art indri toolkit which provides an open source highly optimized platform for building solutions based on ir principles.
we compare repir against ten traditional rtp strategies using a dataset consisting of eight opensource software projects.
the experimental results show that for the majority of subjects repir outperforms all programanalysis based and coverage based strategies at both testmethod and test class levels.
thus repir provides an ef fective alternative approach to addressing the rtp problem without requiring any dynamic coverage or static analysis information.
furthermore unlike traditional techniques repir can be made oblivious to the program languages at the expense of only of accuracy and may be directly applied to various programs written in different languages.
for reproducibility and verification our experimental data is available online.
this paper makes the following contributions repir .
we introduce a new approach for regression test prioritization rtp based on program changes.
we define a reduction from the regression test prioritization problem to a standard information retrieval problem and present our approach repir based on this reduction.
tool.
we embody our approach in a prototype tool that leverages the off the shelf state of the art indri toolkit for information retrieval.
evaluation .
we present a rigorous empirical evaluation using version history of eight open source java projects and compare repir with rtp strategies.
we also present different variants of repir and provide detailed results how repir can be used more effectively depending on test or program differencing granularities.
ii.
b ackground this section briefly discusses the basic concepts of regression test prioritization working procedure of an ir system and its applications in software engineering.
a. regression test prioritization a test prioritization technique or tool reorders the actual execution sequences of test cases in such a way that it meets certain objectives.
the nature of objectives could be diverse including but not limited to increasing the fault detection or code coverage rate.
rothermel et al.
formally defined the test case prioritization problem as finding t02pt such that 8t00 t002pt t006 t0 .
in the definition ptdenotes the set of all possible permutations of a given test suite t andfdenotes a function from any member ofpt to a real number such that a larger number indicates better prioritization.
in this paper we focus on increasing the likelihood of revealing regression errors related to specific code changes earlier in the regression testing process.
b. ir techniques in software engineering recent years have seen many ir applications applied to solve software engineering problems.
ir is concerned with search given a massive collection of documents ir seeks to find the most relevant documents based on a user query.
generally an ir system works in three major steps text preprocessing indexing and retrieval.
preprocessing involves text normalization stop word removal and stemming.
a text normalizer removes punctuation performs case folding tokenizes terms etc.
in the stop word removal phase an ir system filters out the frequently used terms such as prepositions articles etc.
order to improve efficiency and reduce spurious matches.
finally stemming conflates variants of the same term e.g.
go going gone to improve term matching between query and document.
then the documents are indexed for fast retrieval.
once indexed queries are submitted to the search engine which returns a ranked list of documents in response.
finally the search engine or model is evaluated by measuring the quality of its output ranked list relative to each user s input query.
for a broad overview of ir please refer to .
ir techniques have been applied to over two dozen different software engineering problems many of which are highlighted in two surveys on the application of ir to se problems .
there are two predominant tasks today.
the first task is feature or concept location which consists of locating features described in maintenance requests such as enhancements or faults .
the second task is traceability which links or recovers links between software engineering artifacts .
another closely related task is software reuse where ir is used to identify the reusable software artifacts .
there are also a diverse set of other tasks such as quality assessment change impact analysis in source code restructuring and refactoring defect prediction clone detection and duplicate bug detection .
iii.
rep ir a pproach regression test prioritization rtp and information retrieval ir both deal with a ranking problem albeit in different domains section ii .
while rtp is concerned with test cases written in a programming language ir is concerned with documents written in natural language.
however many human centric software engineering documents are text based including source code test scripts and test documents.
furthermore in real world software projects developers often use meaningful identifier names and write comments which allow solving a number of software engineering problems using information retrieval.
our key insight is that in addition to writing good identifier names and comments in the code developers use very similar terms for test cases and we can utilize these textual relationships using ir to develop effective and efficient rtp techniques.
a. problem formulation we reduce rtp to a standard ir problem where the program difference between two software revisions or versions is the query and the test cases or test classes are the document collection .
therefore for a given test suite ts the prioritized test suite ts0is defined by ranking tests in ts based on the similarity score between the program differences and test cases.
reducing the rtp technique to a standard ir task enables us to exploit a wealth of prior theoretical and empirical ir methodology providing a robust foundation for tackling rtp.
this work primarily focuses on projects with junit tests.
b. construction of document collection the process of constructing documents from test cases varies depending on the information granularity and the choiceof information retrieval techniques.
generally a test suite is a collection of source code files where each source code file consists of one or more test methods functions.
for example junit has two levels of test cases test classes and test methods.
prior research on rtp focused on prioritizing both test methods and test classes.
in this section we describe the construction of three types of document collections.
hereafter we use test class to denote test class file and test method to denote test method function.
at test class level to make a document collection at test class level we first build the abstract syntax tree ast of each source code file using eclipse java development tools jdt and traverse the ast to extract all the identifier names class names attribute names method names and variable names and comments.
the identifier names and comments are particularly important from the information retrieval point of view since these are the places where developers can use their own natural language terms.
alternatively a document collection at the test class level can be also constructed without any knowledge of underlying programming language.
in this case we do not construct any ast for test classes.
rather we read each term in the test class remove all mathematical operators using simple text processing and tokenize them to construct the document collection.
at test method level for constructing a document collection at the test method level we extract all the methods from the ast using jdt and store all the identifiers and comments related to a given method as a text document.
structured document for structured information retrieval it is important to store documents in such a way that they retain the program structure.
in our study we distinguish four kinds of terms based on program constructs i class names ii method names iii all other identifier names such as attribute and variable names api names and iv comments.
to construct structured documents we traverse the ast for either each test class or test method to extract aforementioned terms and store them in an xml document.
c. query construction as we defined in the problem formulation in an ir based rtp differences between two program versions comprise the query.
how to utilize the best query representation e.g.
succinct vs. descriptive is a very well known problem in traditional ir .
while the succinct representation often provides the most important keywords it may lack other terms useful for matching.
in contrast although the more verbose descriptions may contain many other useful terms to match it may also contain a variety of distracting terms.
in our work we experiment with three representations of program differences that can affect the overall results of rtp.
low level differences by low level differences we mean the program differences between two versions at the line level.
we compute the low level differences by applying unix diffrecursively while ignoring spaces and blank lines.
it can also be directly obtained from version control systems e.g.
cvs svn git without additional computation.
we denotetable i highlevel change types no.
type change description cm change in method am addition of method dm deletion of method af addition of field df deletion of field cfi change in instance of field intializer csfi change in static field initializer lc m look up change due to method changes lc f look up change due to field changes this representation of query as ldiff and quantify it in terms of number of lines.
high level differences since ldiff is expected to be noisy e.g.
sensitive to changes in formatting or local changes our goal is to summarize ldiff by abstracting local changes and ignoring formatting differences.
to this end we consider nine types of atomic changes table i that have been used in various studies for change impact analysis .
we use faulttracer a change impact analysis tool to extract these changes.
we denote this high level query ashdiff and quantify it in terms of the number of atomic changes.
compact ldiff or hdiff since the difference between two program versions is often too long e.g.
thousands of lines it is highly likely that they would have many duplicated terms.
in this representation we construct a compact version of ldiff and hdiff by removing all the duplicated words from them.
we denote these compact forms of ldiff and hdiff as ldiff.distinct andhdiff.distinct respectively.
d. tokenization since we are dealing with program source code rather than natural language written in english similar to other ir systems for software engineering our tokenization is different than that of standard ir task.
generally identifier names are a concatenation of words.
dit et al.
compared simple camel case splitting to the more sophisticated samurai system and found that both performed comparably in concept location.
therefore in addition to splitting terms based on period comma white space we also split identifier names based on the camel case heuristic.
e. retrieval model researchers in software engineering have experimented with a number of different retrieval models developed by ir including latent semantic indexing lsi the vector space model vsm and latent dirichlet allocation lda .
however recent research shows that tf.idf term weighted vsm briefly tf.idf model works better than others .
another study shows that although there is a widespread debate on which of three tf.idf bm25 okapi or language modeling traditionallydominant ir paradigms was best all three approaches utilize the same underlying textual features and empirically perform comparably when well tuned .
therefore we chose thetf.idf model for our study.
we elaborate on this tf.idf formulation below.
let us assume that test cases documents and a program difference query are represented by a weighted term frequency vector dand qrespectively of length n the size of the vocabulary i.e.
the total number of terms .
d x1 x2 x n q y1 y2 y n each element xiin drepresents the frequency of term ti in document d similarly yiin query q .
however the terms that occur very frequently in most of the documents are less useful for search.
therefore in a vector space model generally query and document terms are weighted by a heuristic tf.idf weighting formula instead of only their raw frequencies.
inverse document frequency idf diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.
thus weighted vectors for dand qare dw tfd x1 id f t1 tfd x2 id f t2 tf d xn id f tn qw tfq y1 id f t1 tfq y2 id f t2 tf q yn id f tn the basic formulation of idf for term tiisid f ti logn nti where nis the total number of documents in cand ntiis the number of documents with term ti.
therefore in the simplest tf.idf model we would simply multiply this value by the term s frequency in document dto compute the tf.idf score for t d .
however actual tf.idf models used in practice differ greatly from this to improve accuracy .
to date ir researchers have proposed a number of variants of tf.idf model.
we adopt indri s built in tf.idf formulation based upon the well established bm25 model .
this tf.idf variant has been actively used in ir community over a decade and rigorously evaluated in shared task evaluations at the text retrieval conference trec .
in this variant the document stffunction is computed by okapi tfd x k1x x k1 b bld lc where k1is a tuning parameter that calibrates document term frequency scaling.
the term frequency value quickly saturates i.e.
dampens or diminishes the effect of frequency for a small value of k1 whereas a large value corresponds to using raw term frequency.
bis another tuning parameter between and which is the document scaling factor.
when the value of bis the term weight is fully scaled by the document length.
for a zero value of b no length normalization is applied.
ldandlcrepresents the document length and average document length for the collection respectively.
the idf value is smoothed as logn nt 5to avoid division by zero for the special case when a particular term appears in all documents.
the term frequency function of query tfqis defined similarly as tfd.
however since the query is fixed across documents normalization of query length is unnecessary.
therefore bis simply set to zero.
indexer index test source files ast constructor tokenizer tokenizer document collec8on program differences retrieval a single query rank list of test cases parameters k1 k2 b test class test method program p modified program p differencer fig.
.
repir architecture tfq y k2y y k2 now the similarity score of document dagainst query qis given by equation .
s d q nx i 1tfd xi tfq yi id f ti f .
structured information retrieval the tf.idf model presented in equation does not consider source code structure program constructs i.e.
each term in a source code file is considered having the same relevance with respect to the given query.
in our recent work on ir based bug localization we found that incorporating structural information into ir model known as structured retrieval can help improve the results considerably.
in this paper we adapt the structured ir for rtp.
as we described in section iii b3 we distinguish four kinds of program constructs classes methods variables and comments .
therefore to exploit all of this structural information we perform a separate search for each type of terms.
since tf.idf model assumes that there is no statistical dependence between terms we simply sum all the field retrieval scores to calculate the final similarity score.
s0 d q x f2ds df q where fis a particular document field.
the benefit of this model is that terms appearing in multiple document fields are implicitly assigned greater weight since the contribution from each term is summed over all fields in which it appears.
g. architecture figure shows the overall architecture of our ir based rtp prototype repir re gression test p rioritization using information r etrieval .
first repir takes the source code files of tests as input that we would like to prioritize.
next it extracts information from test cases tokenizes the terms and constructs a document collection for a given level of granularity as described in section iii b. repir also extracts program changes ldiff hdiff or compact and tokenizes terms in the same way as tokenizing document terms to construct the query.we adopt the indri toolkit a highly efficient opensource library for indexing and developing our retrieval model.
after documents and queries are created above they are handed off to indri for stopword removal stemming and indexing.
note that we use the default stopword list provided with indri and the krovetz stemmer for stemming.
we also set the values of k1 k2 and bto .
and .
respectively which have been found to perform well empirically .
iv.
e mpirical evaluation to investigate the effectiveness of repir we performed an empirical evaluation in terms of five research questions.
rq1 is repir more effective than untreated or random test orders?
this research question aims to understand whether repir reveals regression faults earlier than when there is no rtp or when test cases are ordered at random.
rq2 do the high level program differences help improve the performance of repir?
low level program differences are expected to produce noisy results since changes to even a single character of a line would be interpreted as deletion of a line followed by an addition of line.
therefore in this research question we investigate whether the high level program differences based on ast help improve the accuracy of repir.
rq3 is structured retrieval more effective for rtp?
in our recent work we showed that incorporating the structural information program into traditional information retrieval model which is known as structured information retrieval can improve the bug localization results considerably .
in this research question we investigate whether the same is true for ir based rtp.
rq4 how does repir perform compared to the existing rtp techniques?
to date researchers have focused on various program analysis either static or dynamic based techniques to propose or improve rtp.
in this research question we are interested in investigating how well repir performs compared to those existing techniques.
rq5 how does repir perform when it is oblivious to language level information?
since repir utilizes only textual information identification of specific programing language constructs is not needed.
lightweight language specific parsing is used only for identifier name extraction for constructing document collection at the method level.
however if we use ldiff to prioritize test classes repir can be made completely oblivious of the underlying programming language.
in this research question we investigate how repir performs for this configuration.
a. subject systems we studied eight open source software systems for our evaluation.
these systems are from diverse application domain and have been widely used in software testing research .
we obtained xml security and apache ant fromthe well known software artifact infrastructure repository sir and extract other subject systems from their host website.
the sizes of these systems vary from .7k loc time and money .7k loc source code and 3k loc test code to .9k loc apache ant .4k loc source code and .5k loc test code .
for each subject system we first extract all the major releases with their test cases and consider each consecutive versions as a version pair.
for each pair we run the old regression test suite on new version to find possible regression test failures.
then we treat the changes causing those test failures as the regression faults.
in this way we were able to identify version pairs with regression faults which are all used in our study.
table ii provides all the details regarding each version pair including the statistics for test methods test classes fault revealing test methods program edits in terms of both ldiff and hdiff and failure inducing edits in terms of hdiff .
it also represents the textual properties of the first version in each version pair such as the number of distinct tokens extracted from source code and test cases as well as the number and proportion of test case tokens that also appear in source code.
the high proportion i.e.
.
for all subjects of test case tokens in source code confirms repir s motivation that developers use similar terms for tests and source code.
b. independent variable since we are interested in investigating the performance of repir for different granularities of test cases different representations of program differences and how it works compared to other rtp strategies we have mainly three independent variables iv1 test case granularity iv2 program differences and iv3 prioritization strategy in section iii we discussed different test cases granularities and program differences.
now we briefly describe test prioritization strategies that we considered for comparison.
untreated test prioritization keeps the original sequence of test cases as provided by developers.
in our discussion we denote the untreated test case prioritization as ut.
we consider this to be the control treatment.
random test prioritization rearranges test cases randomly.
since the results of random strategy may vary a lot for each run we applied random test prioritization times for each subject according to arcuri et al.
s guidelines to evaluate randomized algorithms .
in our discussion we denote the random test prioritization technique as rt.
dynamic coverage based test prioritization varies depending on the types of coverage information e.g.
the method or statement coverage and prioritization strategies e.g.
the total oradditional strategy .
we used the four most widely used variants of coverage based rtp cma cmt csa and cst.
for example cma denotes test prioritization based on themethod coverage using the additional strategy and cst denotes test prioritization based on the statement coverage using the total strategy.table ii description of the dataset no.
project version pair tmeth tclass ftmeth ldiff hdiff fedits srctoken tsttoken comtoken p1time and money .
.
.
p2time and money .
.
.
p3mime4j .
.
.
p4mime4j .
.
.
p5jaxen .0b7 .0b9 .
p6jaxen .1b6 .1b7 .
p7jaxen .0b9 .0b11 .
p8xml security .
.
.
p9xstream .
.
.
p10xstream .
.
.
p11xstream .
.
.
p12xstream .
.
.
p13xstream .
.
.
p14xstream .
.
.
p15commons lang .
.
.
p16commons lang .
.
.
p17joda time .
.
.
p18joda time .
.
.
p19joda time .
.
.
p20joda time .
.
.
p21apache ant .
.
.
p22apache ant .
.
.
p23apache ant .
.
.
p24apache ant .
.
.
jupta is a static analysis based test prioritization approach that ranks tests based on test ability ta .
ta is determined by the number of program elements relevant to a given test case t which is computed from the static call graph of t to simulate coverage information.
ta can be calculated based on two levels of granularity fine granularity and coarse granularity.
ta at the fine granularity level is calculated based on the number of statements contained by the methods transitively called by each test whereas ta at the coarse granularity level is calculated based on the number of methods transitively called by each test.
similar with coveragebased prioritization techniques we also used four variants of jupta jma jmt jsa and jst.
note that we implemented all the static and dynamic rtp techniques using byte code analysis.
more specifically we used the asm byte code manipulation framework2to extract all the static and coverage information for test prioritization.
c. dependent variable we use the average percentage faults detected apfd a widely used metric in evaluating regression test prioritization techniques as the dependent variable.
this metric measures prioritization effectiveness in terms of the rate of fault detection of a test suite and is defined by the following formula apfd pm i 1tfi n m n where ndenotes the total number of test cases mdenotes the total number of faults and tfidenotes the smallest number of test cases in sequence that need to be run in order to expose the ithfault.
the value of apfd can vary from ut.m rt.m ldiff.m ut.c rt.c ldiff.c0.
.
.
.
.
.0apfdfig.
.
accuracy of repir ldiff at test method and test class levels to1.
since nandmare fixed for any given test suite a higher apfd value indicates a higher fault detection rate.
d. study results in this section we present the experimental results which answer our research questions.
repir vs. ut and rt first to understand the performance of repir compared to ut and rt at the test method level repir is set to construct the document collection at test method level and use ldiff as a query.
we select the tf.idf retrieval model and run repir on all version pairs p1 p24 .
we also run rt and ut for the same dataset.
each rtp technique provides a ranked list of test methods for each version pair which we use to calculate apfds.
we also perform the same experiment for test class level.
figure presents the results for all version pairs in the form of boxplot.
in each plot the x axis shows the strategies that we compared and the y axis shows the apfd values.
to name rtp techniques we used mto denote method level and cto denote class level test cases.
each boxplot shows the average dot in the box median line in the box upper lower quartile and 90th 10th percentile apfd values achieved by a strategy.ldiff.m ldiff.dis.m hdiff.m hdiff.dis.m ldiff.c ldiff.dis.c hdiff.c hdiff.dis.c0.
.
.
.
.0apfdfig.
.
impact of program differences at test method m and test class c levels dis distinct from the figure we see that the mean median first and third quartiles apfd of ut rt repir at test method level are .
.
.
.
.
.
.
.
.
and .
.
.
respectively which clearly indicates that repir overall performs better than ut and rt.
we also investigate whether the accuracy of repir varies with the length of program differences or the number of test methods since these are two main inputs for repir.
we compute the spearman correlation between the size of ldiff quantified by number of changed lines and apfd and between the number of test methods and apfd.
the low correlation values for both cases .
and .
indicate that the accuracy of repir is fairly independent of the length of program differences and the number of test methods.
similarly for the test class level we see that the mean median first and third quartiles apfd of repir .
.
.
.
is higher than that of ut .
.
.
.
and rt .
.
.
.
.
these results show that repir performs much better than ut and rt at test class level.
the low spearman correlations between the number of testclasses and apfd .
and between the length of program differences and apfd .
indicate that the accuracy of repir is not dependent either on the the number of testclasses or the size of program differences.
impact of program differences to answer rq2 we run repir with four forms of program differences ldiff ldiff.distinct hdiff hdiff.distinct separately for both at test method level and at test class level.
figures presents the summary of apfd values for test methods and test classes respectively.
results show that at method level hdiff works better than ldiff in terms of both mean hdiff .
vs. ldiff .
and median hdiff .
vs. ldiff .
.
when we take a closer look at our data for individual program versions we find that hdiff improves the apfd values for version pairs while decreases it for version pairs.
however if we further condense the query by removing duplicate terms the accuracy decreases for both hdiff and ldiff and the decrease rate is larger for hdiff than that of ldiff.
we believe that since hdiff is already condensed it is affected more due to the removal of duplicated terms than ldiff.
on the other hand we see that ldiff works slightly betldiff.m ldiff.st.m hdiff.m hdiff.st.m ldiff.c ldiff.st.c hdiff.c hdiff.st.c0.
.
.
.
.
.0apfdfig.
.
impact of structured retrieval at test method m and test class c levels st structured ter than hdiff on average at test class level.
the mean value of apfds for hdiff are .
whereas it is .
for ldiff although interestingly median apfd for hdiff is .
higher than that of ldiff.
we think the reason for getting better accuracies with hdiff at test method level is that since test methods are typically small they are more affected by the noises of ldiff.
from the results of ldiff.distinct and hdiff.distinct we see that like test method level the removal of duplicated terms also slightly hurts the results.
however it should be noted that repir is computationally more efficient when it uses the compact representation of the query and thus may be suitable when the program change is large.
impact of structured retrieval to understand whether the structured retrieval leads to better prioritization we constructed the structured version of document collection at both test method and test class level as described in section iii b3.
then we used the structured retrieval techniques as described in section iii f. from figure we see that structured retrieval works basically the same as unstructured retrieval at both testmethod and test class levels for hdiff.
however it reduces the accuracy slightly for ldiff.
therefore our results suggest that although structured retrieval has been found to improve the accuracy of bug localization results considerably it does not help much for rtp.
repir vs. jupta or coverage based rtp to answer rq4 first we ran all the eight techniques four variants of jupta based on call graphs and four variants of coveragebased technique described in section iv b on each programpair in our dataset.
figures and show the summary of apfd values for each strategy at method level and class level respectively.
results show that for both static and dynamic techniques additional strategies are overall more effective than total strategies.
this is consistent with prior studies .
now we compare all the mean apfds of repir from figure with that of jupta and coverage based techniques from figures and as summarized in table iii.
from the results we see that repir equipped with either ldiff or hdiff overall outperforms all the jupta and coveragebased approaches total oradditional regardless of test case granularities test method test class .
at test method level the mean apfd achieved by repir are .
using ldiff and .
using hdiff whereas the best variants of jupta jma jma jmt jsa jst cma cmt csa cst0.
.
.
.
.
.0apfdfig.
.
accuracy of jupta and coverage based rtp at test method level jma jmt jsa jst cma cmt csa cst0.
.
.
.
.0apfd fig.
.
accuracy of jupta and coverage based rtp at test class level table iii comparison of mean apfd s achieved by different strategies tm t est method tc t est class tm tc tm tc tm tc ldiff .
.
jma .
.
cma .
.
ldiff.dis .
.
jmt .
.
cmt .
.
hdiff .
.
jsa .
.
csa .
.
hdiff.dis .
.
jst .
.
cst .
.
and coverage based technique csa achieve .
and .
respectively.
at test class level repir achieves the mean apfd of .
using ldiff and .
using hdiff whereas the best variants of jupta jsa and coverage based technique cmt achieve .
and .
respectively.
even with the compact representation of queries ldiff.dis and hdiff.dis repir performs better than all total strategies and performs equally well or better than additional strategies.
we further investigate how repir performs for each subject system.
for conciseness we present the average apfd values for each system achieved by repir using hdiff jma jupta based on method coverage using additional strategy which is the best among all the jupta strategies and csa based on statement coverage using additional strategy which is the best among all the coverage based strategies at method level.
similarly for test class level we present the average apfd values for each subject system achieved by repir using ldiff jsa and cmt which are the best in ir jupta and coverage based approach respectively.
table iv shows that repir achieved the best apfd for six five out of eight subjects at test method test class level.
accuracy when repir is oblivious of programming language for this experimental setting repir does not build any ast for source code or test classes while constructing document collections and queries.
documents are made at test class level by simply removing mathematical operators and tokenizing any text that are in the test classes.
so the documents are expected to be noisy because of programming language keywords.
we used ldiff as query which is alsotable iv comparison by subjects test method test class subject hdiff jma csa ldiff jsa cmt time and money .
.
.
.
.
.
mime4j .
.
.
.
.
.
jaxen .
.
.
.
.
.
xml security .
.
.
.
.
.
xstream .
.
.
.
.
.
commons lang .
.
.
.
.
.
joda .
.
.
.
.
.
apache ant .
.
.
.
.
.
program language independent.
then we run repir for all version pairs and calculate the apfd values.
results show that the mean apfd across all version pairs is .
while it was .
when we used only identifiers and comments as document terms.
interestingly it turns out that the median apfd of the language oblivious approach is .
higher .
for languageoblivious configuration vs. .
when we used only identifiers and comments of test classes.
therefore our results show that repir even in its simplest form when it is oblivious of the programming language does not lose any significant accuracy and outperforms all the jupta and coverage based approach highest mean is .
achieved by jsa .
e. qualitative analysis our quantitative results already show that developers tend to use very similar terms in source code and corresponding test cases which is one of our main motivations for developing an ir based rtp.
in this section we illustrate a concrete example to show the usefulness of this information.
when commons lang evolved from version .
to .
test method fastdateformattest.testlang538 failed since the developer incorrectly removed a conditional block for updating the time zone in method fastdateformat.format shown in figure highlighted in red .
if we extract the program differences from this change ldiff produces the following terms time zone forced calendar get etc.
while hdiff produces cm fastdateformat.format .
it should be noted there were also many other non faulty changes in the query.
now let us take a look at the test method that reveals this fault in figure .
interestingly we see many of the terms from faulty edits in the test method highlighted in bold .
furthermore the source code class fastdateformat and the corresponding test class fastdateformattest have similar names.
as a result repir with hdiff ranked this method at 7th and ldiff ranked at 17th position among test methods.
on the other hand the best variants of jupta public stringbuffer format calendar calendar stringbuffer buf f if m timezone forced f calendar .get time inmillis calendar calendar calendar .clone calendar .set timezone mtimezone g return applyrules calendar buf g fig.
.
a failure inducing edit in commons lang .03public void testlang538 f final string date time 16t16 .000z more commonly constructed with cal new gregoriancalendar for the unit test to work in any time zone constructing with gmt rather than default locale time zone gregorian calendar cal new gregorian calendar timezone .get timezone gmt cal.clear cal.set fastdate format format fastdate format .getinstance yyyy mm dd t hh mm ss.sss z timezone .get timezone gmt assertequals date time date time format .format cal g fig.
.
a fault revealing test method for commons lang .
and coverage based technique jma and csa ranked it at 367th and 370th position respectively.
in spite of such good results there was one occasion where repir performed unsatisfactorily when time money evolved from .
to .
.
we found that the fault revealing test method was moneytest.testprint where there was apparently no information of use to ir since the only line in the test method is assertequals usd .
d15.tostring .
however our overall results show that the number of such occasions is very small out of cases in our study .
f .
time and space overhead the running time of repir depends on three parameters the size of vocabulary the number of test cases and the length of program differences.
repir works most efficiently when we use a compact representation of the query.
it takes only a fraction of a second for each version pair to prioritize its test cases.
for example repir took only .
second to prioritize all the test methods of joda time .
which has the largest number of test methods in our study.
the preprocessing and indexing took only three seconds.
when we used the full representation of query repir took seconds.
on the other hand the additional strategy based on statement level coverage information took seconds only for test prioritization excluding instrumentation and coverage collection .
the time complexity of repir and total strategy grows linearly when test suite size increases while the additional strategy grows quadratically .
thus repir is a more cost effective approach.
furthermore note that coveragebased approach is not useful if the coverage is not available from the old version because developers can simply run all the tests instead of spending time for recollecting the coverage.
the space overhead of repir is determined by the requirement of indexing test cases for ir.
for most of the subjects index size is around 1mb.
the index size of joda time .
having the largest number of test methods is 3mb.
on the contrary the data required by the traditional techniques for the same system was .6mb for method coverage matrix and 31mb for statement coverage matrix.
the time and space overhead of jupta is very similar to coverage based approaches since jupta tries to simulate code coverage.
all the experiments were performed on a macbook pro running os x .
with intel core i7 cpu .8ghz and 4gb ram.
g. threats to validity this section discusses the validity and generalizability of our findings.construct validity we used two artifacts of a software repository program source code and test cases which are generally well understood.
our evaluation uses subject systems with both real and seeded for the projects from sir regression faults.
also we applied all the prioritization techniques on the same dataset enabling fair comparison and reproducible findings.
to evaluate the quality of prioritization we chose apfd which has been extensively used in the field of regression test prioritization and is straightforward to compute.
apfd expresses the quality of prioritized test cases based on how early the faulty test cases are positioned in the prioritized suite.
however apfd does not consider either the execution time of individual test cases or the severity of faults.
therefore it may not accurately estimate how much we are gaining from the prioritized test suite in terms of cost and benefits.
internal validity the success of repir vastly depends on the usage of meaningful and similar terms in source code and corresponding test cases which is consistent with programming best practices.
another threat to internal validity is the potential faults in our implementations as well as the used libraries and frameworks.
to reduce it we used mature libraries and frameworks that have been widely used in various software engineering and information retrieval applications e.g.
faulttracer and indri .
external validity our experimental results are based on versions of programs from eight software projects all of them are open source projects written in java with junit tests.
although they are popular projects and widely used in regression testing research our findings may not be generalizable to other open source or industrial projects with other test paradigms.
note however that our technique does not have to rely on language specific features and therefore we expect it to handle programs in other languages.
for example in another study we showed that the information retrieval based bug localization is equally effective in c programs.
the risk of insufficient generalization could be mitigated by applying repir on more subject systems both open source and industrial .
this will be explored in our future work.
v. r elated work reducing the time and cost of regression testing has been an active research area for near two decades.
researchers have already proposed various regression testing techniques such as regression test selection prioritization and reduction .
since our work is for regression test prioritization rtp this section is limited to the relevant work in this area.
for related work regarding ir in software engineering please refer to section ii b.wong et al.
introduced the notion of rtp to make regression testing more effective.
they made use of program differences and test execution coverage from the previous version and then sorted test cases in order of increasing cost per additional coverage.
rothermel et al.
empirically evaluated a number of test prioritization techniques including both the total andadditional test prioritization strategies using various coverage information.
in that work they also proposed the widely used apfd metric for test prioritization.
along the same line elbaum et al.
investigated more code coverage information and incorporated the cost and the severity of each test case for test prioritization .
jones and harrold argued that there are important differences between statement level coverage and modified condition decision coverage mc dc for regression testing and proposed test reduction and prioritization using the mc dc information.
jeffrey and gupta introduced the notion of relevant slices in rtp.
their approach assigns higher weight to a test case that has larger number of statements branches in its relevant slice of the output.
however a common limitation of these techniques is that they require coverage information for the old version which can be costly to collect or may not be available in the repository.
besides investigating different types of coverage information researchers have also proposed various other strategies for rtp.
li et al.
used search based algorithms such as hill climbing and genetic programming for test prioritization.
jiang et al.
used the idea of adaptive random testing for test prioritization.
zhang et al.
recently proposed a spectrum of test prioritization strategies between the traditional total and additional strategies based on statistical models.
however according to the reported empirical results the traditional additional strategy remains one of the most effective test prioritization strategies.
there are also some approaches that do not require dynamic coverage information.
srikanth et al.
proposed a valuedriven approach to system level test case prioritization based on four factors requirements volatility customer priority implementation complexity and fault proneness of the requirements.
tonella et al.
used relative priority information from the user in the form of pairwise test case comparisons to iteratively refine the test case ordering.
yoo et al.
further used test clustering to reduce the manual efforts in pairwise test comparisons.
ma and zhao distinguished fault severity based on both users knowledge and program structure information and prioritized tests to detect severe faults first.
all these approaches require inputs from someone who is familiar with the program under test which may be costly and not always available.
to avoid manual efforts zhang et al.
proposed a static test prioritization approach jupta which extracts static call graph of a given test case to estimate its coverage.
later mei et al.
extended the study and proposed more variants of jupta along the same way.
recently jiang and chan proposed a static test prioritization approach based on static test input information.
however all these approaches try to use static informationto simulate code coverage and thus may be imprecise.
in contrast repir is a fully automated and lightweight does not require coverage collection or static analysis test prioritization approach based on information retrieval and has been shown to be more precise and efficient than many existing techniques.
like our approach there are also some test prioritization techniques that utilize the presence of natural english in source code artifacts.
arafeen and do first clustered test cases based on requirements which are written in english and then prioritized each cluster based on code metrics.
nguyen et al.
used an ir based approach for prioritizing audit tests in evolving web services where they use service change descriptions to query against test execution traces.
both of these approaches require requirement documents or manual change descriptions with past test execution traces which may not be available.
furthermore they are designed and intended to be used when requirements or services change and may not be well suited for day to day regression testing.
thomas proposed a test prioritization approach using a topic model approach where test cases are ordered based on their edit distances.
this technique does not utilize any change information and thus may be imprecise.
vi.
c onclusion to reduce the regression testing cost researchers have developed various techniques for prioritizing tests such that the higher priority tests have a higher likelihood of finding bugs.
however existing techniques require either dynamic coverage information or static program analysis and thus can be costly or imprecise.
in this paper we introduced a new approach repir to address the problem of regression test prioritization by reducing it to a standard ir problem.
repir does not require any dynamic profiling or static program analysis.
we rigorously evaluated repir using a dataset consisting of version pairs from eight projects with both real and seeded regression faults and compared it with rtp strategies.
the results show that repir is more efficient and outperforms the existing strategies for the majority of the studied subjects.
we also show that repir can be made oblivious of the underlying programming language for testclass prioritization seldom losing accuracy.
we believe that this alternative approach to rtp represents a promising and largely unexplored new territory for investigation providing an opportunity to gain new traction on this old and entrenched problem of rtp.
moreover further gains might be achieved by investigating such ir techniques in conjunction with traditional static and dynamic program analysis integrating the two disparate approaches each exploiting complementary and independent forms of evidence regarding rtp.
acknowledgment we thank julia lawall and matthew lease for their input on our draft.
this work was funded in part by the national science foundation nsf grant nos.
ccf and ccf0845628 .