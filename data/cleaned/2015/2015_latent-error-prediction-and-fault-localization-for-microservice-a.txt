singapor e management univ ersity singapor e management univ ersity institutional k nowledge at singapor e management univ ersity institutional k nowledge at singapor e management univ ersity resear ch collection school of computing and information systems school of computing and information systems latent err or pr ediction and fault localization for micr oser vice latent err or pr ediction and fault localization for micr oser vice applications b y learning fr om system tr ace logs applications b y learning fr om system tr ace logs xiang zhou xin peng tao xie jun sun singapor e management univ ersity junsun smu.edu.sg chao ji see next page for additional authors follow this and additional works at https ink.libr ary.smu.edu.sg sis r esear ch part of the information security commons citation citation zhou xiang peng xin xie t ao sun jun ji chao liu dewei xi ang qilin and he chuan.
latent err or prediction and fault localization for micr oser vice applications b y learning fr om system tr ace logs.
.
27th a cm joint e uropean softwar e engineering conf erence and symposium on the f oundations of softwar e engineering t allinn est onia a ugust .
available at available at https ink.libr ary.smu.edu.sg sis r esear ch this conf erence p aper is br ought t o you for fr ee and open access b y the school of computing and information systems at institutional k nowledge at singapor e management univ ersity .
it has been accepted for inclusion in resear ch collection school of computing and information systems b y an authoriz ed administr ator of institutional knowledge at singapor e management univ ersity .
for mor e information please email cher ylds smu.edu.sg .
author author xiang zhou xin peng t ao xie jun sun chao ji dewei liu qilin xi ang and chuan he this conf erence paper is a vailable at institutional k nowledge at singapor e management univ ersity https ink.libr ary.smu.edu.sg sis r esear ch latent error prediction and fault localization for microservice applications by learning from system trace logs xiang zhou fudan university chinaxin peng fudan university chinatao xie university of illinois at urbana champaign usa jun sun singapore management university singaporechao ji fudan university chinadewei liu fudan university china qilin xiang fudan university chinachuan he fudan university china abstract in the production environment a large part of microservice failures are related to the complex and dynamic interactions and runtime environments such as those related to multiple instances environmental configurations and asynchronous interactions of microservices.
due to the complexity and dynamism of these failures it is often hard to reproduce and diagnose them in testing environments.
it is desirable yet still challenging that these failures can be detected and the faults can be located at runtime of the production environment to allow developers to resolve them efficiently.
to address this challenge in this paper we propose mepfl an approach of latent error prediction and fault localization for microservice applications by learning from system trace logs.
based on a set of features defined on the system trace logs mepfl trains prediction models at both the trace level and the microservice level using the system trace logs collected from automatic executions of the target application and its faulty versions produced by fault injection.
the prediction models thus can be used in the production environment to predict latent errors faulty microservices and fault types for trace instances captured at runtime.
we implement mepfl based on the infrastructure systems of container orchestrator and service mesh and conduct a series of experimental studies with two opensource microservice applications one of them being the largest open source microservice application to our best knowledge .
the x. zhou x. peng c. ji d. liu q. xiang and c. he are with the school of computer science and shanghai key laboratory of data science fudan university china and the shanghai institute of intelligent electronics systems china.
x. peng is the corresponding author pengxin fudan.edu.cn .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august tallinn estonia association for computing machinery.
acm isbn .
.
.
.
indicate that mepfl can achieve high accuracy in intraapplication prediction of latent errors faulty microservices and fault types and outperforms a state of the art approach of failure diagnosis for distributed systems.
the results also show that mepfl can effectively predict latent errors caused by real world fault cases.
ccs concepts software and its engineering cloud computing testing and debugging .
keywords microservices error prediction fault localization tracing debugging machine learning acm reference format xiang zhou xin peng tao xie jun sun chao ji dewei liu qilin xiang and chuan he.
.
latent error prediction and fault localization for microservice applications by learning from system trace logs.
in proceedings of the 27th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august tallinn estonia.
acm new york ny usa pages.
introduction industrial microservice applications have dozens to thousands of microservices running on hundreds to tens of thousands machines.
in the production environment microservice applications typically are fragile often failing due to infrastructure e.g.
network hardware failures or application faults and it is challenging to reproduce and diagnose these microservice application failures in testing environments.
infrastructure failures can be predicted at the cloud infrastructure layer and handled via infrastructure level adaptations such as the allocation and migration of virtual machines .
application faults traditionally rely on developers to detect and fix via code review testing and debugging.
however microservice application faults are complicated due to the complex and dynamic interactions and runtime environments .
a microservice can 683esec fse august tallinn estonia xiang zhou xin peng tao xie jun sun chao ji dewei liu qilin xiang and chuan he have several to thousands of instances that are dynamically created and destroyed according to the scaling requirements.
these instances run with complex environmental configurations such as memory cpu limits of microservices and containers .
microservice applications usually have complex invocation chains each involving several to dozens of microservice invocations most of which are asynchronous.
improper environmental configurations or coordination of microservice instances or asynchronous interactions may result in failures at runtime.
to allow developers to resolve microservice application failures efficiently in the production environment it is desirable and yet challenging that these microservice application failures can be detected and the faults can be located at runtime of the production environment e.g.
based on application logs or system logs.
application logs record the internal status and events during the execution of an application.
these application logs are produced by logging statements written by developers in the application.
however application logging is often done in an arbitrary and ad hoc manner and the log messages often contain limited information for failure diagnosis .
on the other hand some existing approaches use system logs to detect anomalies or predict failures of cloudbased systems.
these system logs are produced by infrastructure systems such as operating systems e.g.
centos distributed file systems e.g.
hdfs and container orchestrators e.g.
kubernetes .
however these approaches can only uncover anomalies or failures within the cloud infrastructure such as computing nodes and storage .
to address this challenge in this paper by learning from system trace logs in short as trace logs a special type of system logs we focus on predicting three common types of microservice application faults that are specifically relevant to microservice interactions and runtime environments i.e.
multi instance faults configuration faults asynchronous interaction faults.
according to an existing empirical study almost half of all studied microservice application faults are of these three types.
different from internal logic faults which are localized often based on analyzing internal states and logics these three types of faults are usually relevant to external characteristics of trace logs such as the number of microservice instances accesses of global variables local cache resource consumption invocation and execution orders.
recently pham et al.
propose a failure diagnosis approach for distributed systems.
this approach locates faults by matching similar failure traces produced using fault injection.
this matching based approach has very limited generalization ability and cannot support failure recognition.
in contrast in our work we extract a set of features that reflect the dynamic environments and interactions of microservices from trace logs and train a set of prediction models based on the features.
the prediction models can be used to not only locate the faults but also recognize the failures by predicting latent errors caused by the faults.
in particular in this paper we propose mepfl microservice errorprediction and faultlocalization a novel approach for latent error prediction and fault localization of microservice applications by learning from trace logs.
we design mepfl to predict latent errors of a microservice application caused by the preceding three common types of microservice application faults.
mepfl further predicts the locations i.e.
microservices and types of the faults.
figure basic concepts mepfl combines trace level and microservice level prediction.
the trace level prediction utilizes application specific context e.g.
microservices involved in an application in its feature vectors and treats a trace instance which records the process of microservice invocations for a user request as a whole during training and prediction.
it includes three models to predict latent errors relevant microservices and fault types respectively.
the microservice level prediction uses a set of general features for microservices in a context free way.
it is based on a model to predict what type of fault the target microservice has.
all these preceding models used in trace level or microservice level prediction are defined on a set of trace log features selected by correlation analysis.
with two open source microservice applications sock shop and trainticket which is the largest open source microservice application to our best knowledge we conduct a series of experimental studies to evaluate the effectiveness of mepfl1.
the results show that mepfl can achieve high recall precision .
.
and .
.
and low false positive rate .
and .
in latent error prediction high top accuracy .
and .
in faulty microservice prediction and high recall precision .
.
and .
.
in fault type prediction.
mepfl substantially outperforms pham et al.
s approach in terms of the accuracy of the prediction of faulty microservices and fault types.
with the increase of the coverage of trace types in the training data the overall prediction more and more relies on the trace level prediction and the prediction accuracy substantially increases.
to further evaluate the effectiveness of mepfl in localizing real world microservice application faults we conduct an experimental study with the fault cases provided by the trainticket benchmark these fault cases are replicated from industrial fault cases.
the results show that mepfl can accurately predict the latent errors caused by these fault cases with a recall of .
.
.
on average and a precision of .
.
.
on average .
in this paper we make the following main contributions.
a set of features and prediction models for predicting latent errors faulty microservices and fault types for microservice applications based on system trace logs.
a learning based approach for latent error prediction and fault localization of microservice applications based on fault injection and testing.
a series of experimental studies with two open source microservice benchmarks to evaluate the effectiveness of the approach.
preliminaries our approach is defined based on a series of concepts about microservice.
figure shows the relationships between these concepts.
1all the data of the studies can be found in our replication package .
684latent error prediction and fault localization for microservice applications esec fse august tallinn estonia table fault cases reported by our previous work fault type fault cases example monolithic f6 f9 f10 f1422f22 a wrong column name included in the constructed sql statement multi instance f8 f11 f12 f12 unexpected output of a microservice when it is in a special state configuration f3 f4 f5 f7 f3 improper configurations of jvm and docker asynchronous interactionf1 f2 f13 f1 asynchronous message delivery that lacks sequence control a microservice application includes a set of scenarios and each scenario includes multiple types of user requests e.g.
different clicks on the same ok button on the same web page result in different user requests of the same user request type .
each user request type is mapped to a trace type.
each trace instance records the process of microservice invocations for a user request.
the trace type for the trace instance is the trace type mapped from the type of the user request.
the trace instance includes a series of spans and each span represents an invocation between two microservice instances i.e.
a caller and a callee .
for example the trainticket application has a ticket reservation scenario that includes several user request types i.e.
trace types such as query available tickets confirm ticket selection and pay.
the trace instances of confirm ticket selection include a series of microservice invocations e.g.
the reserve service invokes the ticket info service food service and several other services and the ticket info service in turn invokes the price service and other services.
a recent trend of microservice applications is the introduction of container orchestrator and service mesh.
a container orchestrator such as kubernetes k8s dynamically schedules and manages a large number of container instances.
in our system implementation kubernetes is used to dynamically deploy microservice applications and manage microservice instances and their environmental configurations during the offline training phase.
service mesh is introduced as a separate layer that handles service to service communication.
the most recognized implementation is istio .
our system implementation uses istio to manage asynchronous microservice interactions during offline training and collect trace logs during both offline training and online prediction.
prediction models .
fault types collected in an industrial survey our previous work reported representative microservice fault cases see table .
these fault cases can be categorized into the following four types.
monolithic faults .
these faults can cause failures even when the application is deployed in a monolithic mode i.e.
all the microservices are deployed on one node each microservice has only one instance and different microservices interact synchronously.
these faults are usually due to faults in the internal implementation of microservices and are irrelevant to the runtime environments of the application.
multi instance faults .
these faults are related to the existence of multiple instances of the same microservice at runtime.
they are often due to lacking coordination among different instances of the same microservice e.g.
to keep them in consistent states .
configuration faults .
these faults are related to the environmental configurations of microservices such as theresource e.g.
memory and cpu limits.
they are often due to improper or inconsistent configurations of microservices and or their residing environments e.g.
containers and virtual machines vms .
asynchronous interaction faults .
these faults are related to the asynchronous invocations among microservices and may cause failures when asynchronous invocations are executed or returned in an unexpected order.
they are often due to missing or improper coordination of the sequences of asynchronous invocations.
the mechanisms of monolithic faults are similar to those of faults in monolithic applications.
for example f22 is a monolithic fault caused by an incorrect sql statement.
the failures caused by these faults can be reproduced and debugged on a single machine.
multi instance faults are mainly related to inconsistent states between different instances of the same microservice.
for example f12 is caused by an inconsistent state of a microservice.
when an invocation chain involves two instances of the same service and their states are different the request will be denied with an unexpected output.
to fix the fault the developers can add a synchronization mechanism for the involved state variable or move it to external storage e.g.
redis .
statelessness is a recommended practice for microservice development.
however many microservice applications in practice are migrated from legacy monolithic applications which are often stateful with state variable definitions i.e.
in memory session states .
due to the difficulty of refactoring the migrations are often halfway and stateful microservices are common in enterprise applications.
configuration faults are related to the complex environmental settings of microservices.
a microservice is associated with a series of application level environmental configurations such as the configurations of jvm e.g.
max min heap memory stack memory garbage collection strategy thread pool and database connection pool.
a microservice is also associated with a series of infrastructure level environmental configurations such as the configurations of containers e.g.
memory limit cpu limit health check communication security and vms.
reconciling these configurations is a challenging task and often incurs mistakes.
for example f3 is due to a conflict between the memory limits of jvm and docker this conflict can cause the jvm process to be killed.
asynchronous interaction faults are usually caused by an unexpected order of executing asynchronous microservice invocations.
when a microservice invokes several other microservices asynchronously the orders of the requests executions and responses of the invoked microservices are uncertain.
if the developers have unrealistic assumptions e.g.
the invoked microservices shall be executed and returned in the same order of invocations the developers may introduce asynchronous interaction faults into the applications.
for example f1 lies in an unexpected order of executions and returning of multiple asynchronously invoked microservices and can cause an invoked microservice to be in an abnormal status.
table shows that out of the fault cases .
are monolithic faults while the other fault cases .
are related to multi instance environmental configuration or asynchronous interaction.
monolithic faults can be found on a single machine by 685esec fse august tallinn estonia xiang zhou xin peng tao xie jun sun chao ji dewei liu qilin xiang and chuan he unit or integration testing and can be located using traditional techniques of fault localization such as assertions and breakpoints .
therefore we focus on the other three types of faults in this paper.
.
feature definition mepfl aims to predict whether a latent error has occurred the faulty microservice and the fault type.
mepfl systematically extracts a set of features from trace instances.
figure shows an example of trace instance which starts from a root microservice r. in the figure each arrow represents an invocation i.e.
span and different subscripts of the same capital letter e.g.
d1andd2 represent different invocations of the same microservice the same or different instances .
for this trace instance the microservice features are calculated for each of the microservice invocations e.g.
r a1 d1 and d2 .
figure an example of trace instance based on the data collected in a previously reported empirical study we summarize a comprehensive set of microservice features from different aspects i.e.
configuration resource instance and interaction .
these features can be extracted from trace logs.
a feature is defined in one of three scopes global indicates that the feature is a global property of the microservice and is irrelevant to the current trace instance trace indicates that the feature is an overall property of the microservice in the current trace instance invocation indicates that the feature is a property of the current microservice invocation.
for example for the trace instance shown in figure a global feature of d1depends on all the instances of the microservice dat that time a trace feature of d1depends ond1andd2in the trace instance an invocation feature of d1 depends on only d1itself in the trace instance.
table shows a complete list of the candidate features.
detailed descriptions of the features can be found in our replication package .
configuration features reflect the environmental configurations of the microservice instance.
resource features reflect the resource consumptions e.g.
memory and cpu of the microservice instance and its residing node.
these features may be relevant to the validity of microservice configurations.
volume support which enables the microservice to access the persistent data of the node may also be relevant to multi instance faults.
instance features reflect the status of the deployment of the instances of the microservice and their involvement in the current trace instance.
these features may be relevant to multi instance problems.
interaction features reflect the status of interactions especially asynchronous interactions with other microservices.
among these features et and rsc reflect the status of the current microservice invocation itself ait and ceo reflect the status of asynchronous invocations to other microservices.
for example for a1in figure the et and rsc features reflect the execution time and response code of a1itself the ait and ceo features reflect the status of the invocations which are asynchronous invocations from a1tod1 e1 and f1.
to provide a unified encoding for different microservices we develop the following convention for the ait and ceo features assuming that there are nmicroservices asynchronously invoked by the current microservice sort the invoked microservices by the invocation order and number them from to n for each invoked microservice extract a feature ait i i n for capturing its execution time for each pair of invoked microservices set a feature ceo i j i j n for capturing whether their execution order is consistent with their invocation order.
for example for a1in figure there are ait features and ait 1for capturing the execution time of d1 there are ceo features and ceo 2for capturing whether the execution order of d1ande1 is consistent with their invocation order.
these features can be relevant to asynchronous interaction faults.
the preceding features can be used in the microservice level prediction model.
to support trace level prediction we further derive a set of trace level features for each trace instance by absorbing the features of each microservice involved in the trace instance.
if a microservice is invoked multiple times in the trace instance we treat the feature values of the last invocation as the values of the corresponding trace level features.
because the influences of many faults are cumulative and some other faults terminate traces at the last invocation the last invocation of a microservice may provide the most significant indication for latent errors.
for example for the trace instance in figure there are rsc features i.e.
r.rsc a.rsc b.rsc c.rsc d.rsc e.rsc f.rsc and g.rsc each corresponding to an involved microservice and d.rsc is the rsc value ofd2.
these trace level features incorporate application specific context and thus can better predict latent errors and fault locations if the models have been trained with trace instances of similar trace types.
in addition to these derived features we also define tracelevel features that aggregate the execution information of involved microservices i.e.
tet the execution time of the current trace instance tmn the number of microservices that are invoked in the current trace instance and tin the number of microservice instances that are invoked in the current trace instance .
.
model design mepfl supports four prediction models which allow us to combine both trace level and microservice level prediction.
trace level prediction includes latent error prediction faulty microservice prediction and fault type prediction each supported by a prediction model i.e.
the le model fm model and ft model .
microservicelevel prediction includes a microservice status prediction model in short as an ms model .
trace level prediction models are defined on trace level features.
these models incorporate application specific microservices and their interactions in their feature definitions and thus can utilize application specific context in the prediction when similar trace instances e.g.
trace instances that share common microservices with the current trace instance have been included in the training corpus.
the microservice level prediction model on the other hand is defined on microservice level features in an applicationindependent way and thus generalizes across different applications.
all the four prediction models are classification models.
the three trace level models treat the given trace instance as a whole.
the le model is a binary classification model that classifies the 686latent error prediction and fault localization for microservice applications esec fse august tallinn estonia table feature definition category feature description scope configurationml memory limit of the current microservice relative to the node memory limit invocation cl cpu limit of the current microservice relative to the node cpu limit invocation vs whether volume support is enabled for the current microservice global resourcemc memory consumption of the current microservice instance relative to its memory limit invocation cc cpu consumption of the current microservice instance relative to its cpu limit invocation nmc memory consumption of the current node relative to its memory limit invocation ncc cpu consumption of the current node relative to its cpu limit invocation instancein number of instances for the current microservice in the whole system global iin number of instances for the current microservice invoked in the current trace instance trace sva ratio of the shared variables that are accessed in the current invocation invocation ca whether the cache is accessed in the current invocation invocation sa whether the third party storage e.g.
database is accessed in the current invocation invocation tn number of threads of the current microservice instance invocation lt life time of the current microservice instance since its creation invocation nin number of microservice instances residing in the current node invocation nmn number of microservices whose instances reside in the current node invocation interactionet execution time of the current microservice invocation invocation rsc http response status code of the current microservice invocation invocation ait time of an asynchronous invocation in the current microservice execution invocation ceo whether the execution order of a pair of asynchronous invocations is consistent with their invocation order invocation trace instance into two classes with error or not the fm model is a multi label classification model that predicts one or multiple microservices in the trace instance to be faulty and the ft model is a multi label classification model that predicts one or multiple fault types.
the microservice level model treats the microservices involved in the given trace instance individually.
the ms model is a single label classification model that predicts an error status which can be one of the three fault types or a special type no fault .
approach figure provides an overview of mepfl which includes an offline training phase and an online prediction phase.
the offline training phase executes the target microservice application in the testing environment and trains the prediction models based on the collected trace logs.
to train the models we need not only trace logs under successful executions but also trace logs under erroneous executions.
therefore we design a series of fault injection strategies to systematically obtain a series of faulty versions of the application.
we use existing automated test cases to drive the application and its faulty versions to execute and produce trace logs.
these test cases simulate user requests from the clients e.g.
browsers and mobile apps and trigger microservice invocation chains.
to ensure the diversity of traces an additional control is imposed on the runtime environment to make the application executed under different settings.
we then prepare a training corpus for the prediction models by collecting and analyzing trace logs from the executions.
for each trace instance we automatically construct a trace level training sample and a set of microservice level training samples by feature extraction and error fault labeling.
based on the training corpus we use machine learning techniques e.g.
random forests k nearest neighbors multi layer perceptron to train three prediction models that can predict for each trace whether a latent error occurs which microservice the fault resides in and what type of fault it is respectively.
the online prediction phase monitors the execution of the application in the production environment and predicts latent errors and fault locations faulty microservices and fault types .
it continuously collects and analyzes trace logs from the running application.
for each trace instance it extracts features in a similar way to what has been done in preparing the training corpus and then uses the prediction models to predict whether a latent error occurs which microservice the fault resides in and what type of fault it is.
figure approach overview .
fault injection fault injection produces a series of faulty versions of the target microservice application by introducing different types of faults into different parts of the application.
for each faulty version we inject a specific type of fault into a specific microservice in a semiautomatic way.
we design a fault injection strategy for each type of fault.
each fault injection strategy is parameterized with the following information a precondition that specifies the conditions to be met by the location of a microservice where the fault is injected a code transformation method that specifies how to generate a patch to transform a microservice into a faulty one an expected failure symptom that specifies the expected symptom if the injected fault has caused a failure.
we next present details of the parameters for each fault type.
multi instance faults.
the precondition is that the microservice accesses third party cache databases such as redis .
the code transformation method is to replace the data accesses to thirdparty cache databases with data accesses to global variables or local cache.
as multi instance faults usually cause incorrect data accesses the expected failure symptom includes injected failure response incorrect output or various assertion failures or exceptions e.g.
null pointer exceptions .
configuration faults.
configuration faults are usually related to consumption of various resources e.g.
memory and cpu and irrelevant to the business logics of microservices.
therefore configuration faults can be injected into any part of any microservice.
the code transformation method is to inject resource intensive code into target locations of microservices e.g.
memory intensive code that loads a lot of data into memory or computing intensive code that includes a lot of iterations.
the expected failure symptom includes microservice instance restarted timeout exception or resource consumption related exceptions e.g.
out of memory exceptions .
asynchronous interaction faults.
the precondition for asynchronous interaction faults is that the microservice invokes multiple 687esec fse august tallinn estonia xiang zhou xin peng tao xie jun sun chao ji dewei liu qilin xiang and chuan he figure injection of asynchronous interaction fault microservices in succession.
the code transformation method is to introduce data dependencies between different microservice invocations.
if the invocations are synchronous we need to first transform them into asynchronous ones.
the expected failure symptom includes injected failure response or various assertion failures or exceptions e.g.
null pointer exceptions .
figure shows an example of injecting an asynchronous interaction fault where the red code is introduced for fault injection.
this example includes two synchronous microservice invocations.
the code transformation changes the invocations to asynchronous ones using java completablefuture and introduces objects crlist andcolist to store the return values.
it introduces a data dependency between the two asynchronous invocations via the variable checkresult .
due to the dependency the program can enter an unexpected state if the two invoked microservices are executed in a different order.
to inject a fault of a specific type into the target application we first identify a location in the application that satisfies the precondition and use the code transformation method to introduce a fault at the location.
then we check whether the resulting faulty microservice can be successfully compiled and if not we manually repair the compilation problem.
after that we further verify the fault injection result by executing the corresponding test cases multiple times e.g.
times .
as the failures caused by these faults are probabilistic the fault injection is regarded to be successful if the expected failure symptom is observed in one of the executions.
each successfully injected fault is incorporated into the target application to produce a faulty version.
.
execution control the execution controller automatically deploys the target application and its faulty versions executes different application versions with existing automated test cases and collects trace logs.
for faulty versions the controller further manipulates the runtime environment of the application so that it is executed under different settings including different numbers of microservice instances environmental configurations and asynchronous interaction sequences.
for each setting the controller executes the test cases involving thefaulty microservice for a given number e.g.
of times to get multiple trace instances under the same setting.
the automatic deployment execution manipulation and trace log collection are implemented based on the infrastructure support of container orchestrator and service mesh see section .
for a faulty version with an injected multi instance fault the controller manipulates the number of instances of the faulty microservice.
the controller executes the faulty microservice with to a given number e.g.
of instances while the other microservices running with the default numbers of instances.
for a faulty version with an injected configuration fault the controller manipulates the environmental configurations e.g.
memory limit cpu limit of the faulty microservice e.g.
with the memory limit or cpu limit increased from to while the other microservices running with default configurations.
for a faulty version with an injected asynchronous interaction fault the controller manipulates the execution sequences of asynchronous invocations.
the controller causes the microservices that are asynchronously invoked by the faulty microservice to be executed in different orders.
for example for a microservice athat asynchronously invokes three microservices b c and din that order the controller first forces the invoked microservices to be executed in the same order of invocation i.e.
b c d and then forces them to be executed in other orders e.g.
d c b .
as the microservices that are asynchronously invoked by a microservice in a trace instance are usually not so many e.g.
fewer than the controller can try different execution orders to achieve certain coverage e.g.
covering all the partial orders of any pair of microservices.
during the executions of the application and its faulty versions the controller continually collects the trace logs.
to provide the features required by the prediction models the trace logs record not only the sequence of the spans i.e.
microservice invocations of each trace instance but also the environmental configurations resource consumptions and number of the instances of the involved microservices.
.
preparation of training corpus for each trace instance we generate a trace level training sample by extracting trace level features and labeling its error status fault type and fault location.
for each microservice involved in the trace instance we generate a microservice level training sample by extracting microservice level features and labeling its fault status.
we extract the features based on the feature definitions given in section .
.
we first extract microservice level features based on the spans included in the trace instance and then aggregate these microservice level features into trace level features.
the error status of a trace instance can be or .
indicates that the trace instance has a latent error caused by one of the three types of faults and indicates the other cases.
note that the other cases include that the trace instance has no errors or has other unknown errors e.g.
errors caused by monolithic faults .
we label the error status of a trace instance as if the following four conditions are satisfied.
first the result of the trace instance is not consistent with the expected result in the test case.
second the trace instance is produced by a faulty version of the application and the faulty microservice is included in the trace instance.
third 688latent error prediction and fault localization for microservice applications esec fse august tallinn estonia the trace instance is produced in a faulty setting i.e.
for a multiinstance fault the faulty microservice has more than one instance for a configuration fault the faulty microservice runs with resource limits e.g.
memory or cpu limit for an asynchronous interaction fault the execution order of the microservices asynchronously invoked by the faulty microservice is not the same as their invocation order.
fourth the error is not caused by monolithic faults.
to check whether the fourth condition is satisfied we automatically deploy the same application version in a monolithic environment and run the same test case if the test case passes we conclude that the error is not caused by monolithic faults.
if the trace error status is we label the error status of the corresponding trace level training sample as and the fault status of each microservice level training sample to no fault .
if the trace error status is we label the error status of the corresponding trace level training sample to its fault type to the injected fault type and its fault location to the faulty microservice.
for each microservice level training sample we label its fault status to the injected fault type if it has an injected fault and no fault if it has no injected fault.
.
training to select relevant features for the training we conduct a correlation analysis between the errors caused by the three types of faults and the candidate features see table based on trainticket .
for each trace type we use the corresponding test cases to run the original application and its faulty versions with different types of injected faults times.
during the process we manipulate the running system so that it is executed with different settings e.g.
the numbers of instances environmental configurations and execution orders of asynchronous invocations .
table lists the pearson correlation coefficient between the errors caused by different types of faults and the candidate features for five trace types.
the complete correlation analysis results can be found in our replication package .
we choose the features whose correlation coefficients are higher than .
for at least one fault type and one trace type.
based on the criterion we select all the candidate features except vs sa lt nmn and their corresponding trace level features for the training.
before training the prediction models we follow a standard process to preprocess the training data including imputation category encoding discretization and feature scaling.
imputation replaces missing data in the training samples with typical values i.e.
the means for continuous values and the most frequent values for other kinds of values.
category encoding transforms categorization values e.g.
rsc into integer values from to n nis the number of categories of a feature .
discretization transforms continuous values e.g.
tn into integer values from to k by dividing the range of the values into kpartitions.
feature scaling normalizes the ranges of feature values into .
we choose the following three machine learning techniques to train the prediction models.
random forests rf an ensemble learning technique for classification that constructs a multitude of decision trees at training time and outputs the class that is the mode of the classes classification of the individual trees.
figure mepfl prediction process k nearest neighbors knn a non parametric classification technique that determines the class of an object by a plurality vote of its neighbors.
multi layer perceptron mlp a feed forward artificial neural network model that maps a set of input data onto a set of appropriate outputs.
to optimize these classification models we use the grid search technique to search the hyper parameter e.g.
hidden layer sizes for mlp space for the best cross validation score.
the technique exhaustively generates candidates from a grid of parameter values specified with some initial parameters and then scans all the possible combinations to find the best cross validation score.
.
feature extraction and prediction the online prediction phase includes two steps.
the feature extraction step extracts microservice and trace level features from trace logs.
the prediction step uses the prediction models to predict latent errors fault locations and types.
figure shows the prediction process which combines the traceand microservice level prediction models.
given a trace instance captured online mepfl first uses the le model to predict whether the trace instance has latent errors and determines whether tracelevel prediction has the required confidence level obtained along with the prediction from the le model .
if the confidence is not lower than a predefined threshold mepfl uses the fm model and the ft model to predict the faulty microservices and the fault types respectively.
otherwise mepfl uses the ms model to predict latent errors fault locations and types.
note that the ms model predicts the fault status of each microservice involved in the given trace instance.
if any microservice is predicted to be faulty mepfl regards the trace instance as erroneous and all the faulty microservices together with their fault types as the results of fault location and type prediction.
implementation our implementation includes four major components the fault injector execution controller log processor and predictor.
the fault injector currently supports the fault injection of java microservices.
it uses javaparser .
to parse and manipulate the source code of microservices to implement fault injection.
other parsers can be introduced in the future to support fault injection of microservices implemented in other languages.
the execution controller integrates testng .
to implement automated scheduling and execution of test cases.
at the system infrastructure level the execution controller integrates kubernetes .
and istio .
to implement automatic microservice application deployment configuration and runtime manipulation in the offline training.
it uses kubernetes rest apis to dynamically deploy microservice applications and manage microservice instances and their environmental configurations.
it customizes the istio implementation to control the execution returning sequences 689esec fse august tallinn estonia xiang zhou xin peng tao xie jun sun chao ji dewei liu qilin xiang and chuan he table the pearson correlation coefficient of microservice level features and latent errors trace typesfeatures configuration resource instance interaction ml cl vs mc cc nmc ncc in iin sva ca sa tn lt nin nmn et rsc ait ceo trace type 1instance .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
config .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
interaction .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
trace type 2instance .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
config .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
interaction .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
trace type 3instance .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
config .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
interaction .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
trace type 4instance .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
config .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
interaction .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
trace type 5instance .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
config .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
interaction .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
of synchronous microservice invocations based on its sidecar.
istio sidecar is a kind of intelligent proxies that mediate and control the network communication between microservices .
the log processor uses kubernetes and istio to capture trace logs.
to capture information about variable accesses required by some features such as sva and ca we use jdk to capture the runtime jvm thread dump and heap dump and analyze the call stack and variable values in the dumps.
the log processor includes a pipeline to collect and process trace logs.
the pipeline runs a restful service based on spring boot .
.
to collect distributed log data produced by istio and uses kafka .
.
to stream the log data.
it then uses spark .
.
to process the data to produce training samples and uses hdfs hadoop distributed file system .
to store the processed data.
we implement the predictor based on scikit learn a machine learning library for python.
the threshold of the confidence score for adopting the trace level prediction is .
evaluation to evaluate the effectiveness of mepfl we conduct a series of experimental studies to answer the following research questions.
rq1 how accurate is mepfl in predicting latent errors faulty microservices and fault types of microservice applications?
rq2 how does the training coverage of trace types influence the prediction accuracy?
rq3 how effective is mepfl when it is used to predict latent errors caused by real fault cases?
these studies are based on two open source microservice applications sock shop and trainticket .
sock shop is a small scale benchmark application that is widely used in microservice research .
it has microservices and trace types.
trainticket is a medium scale benchmark application that has been recently released having microservices and trace types.
it replicates a variety of faults based on a study of industrial fault cases in microservice applications.
all the studies are conducted on a private cloud with virtual machines.
.
prediction accuracy rq1 to answer rq1 we evaluate the accuracy of mepfl using a corpus of trace logs with automatically injected faults.
for sock shop we inject faults including multi instance faults configuration faults and asynchronous interaction faults into trace types.
we obtain trace instances by test driven automatic execution .
of which have latent errors.
for trainticket weinject faults including multi instance faults configuration faults and asynchronous interaction faults into trace types.
we obtain trace instances .
of which have latent errors.
the fault injection process is automated.
but when the injected faults cause compilation failure or the tests fail in unexpected ways developers are required to manually analyze edit code.
among the faults injected into the two applications .
are injected automatically without human efforts .
involve human efforts.
for each application we randomly divide the trace instances equally into subsets and perform fold cross validation to evaluate the accuracy.
in every cross validation we use subsets as training data and the remaining subset as testing data.
for the prediction of latent errors we evaluate the recall precision and f1 measure the harmonic mean of precision and recall of identifying trace instances that have latent errors.
as the number of successful trace instances is much larger than that of erroneous ones we also evaluate the false positive rate fpr i.e.
the rate of error free traces that are predicted to be erroneous among all the successful traces.
for the prediction of faulty microservices we evaluate the top k with kbeing accuracy of identifying a faulty microservice from an erroneous trace instance i.e.
the probability that the faulty microservice is included in the top k prediction results.
for the prediction of fault types we evaluate the recall precision and f1 measure of the predicted fault types of erroneous trace instances.
to our best knowledge there are no existing approaches of tracelog based error prediction or fault localization for microservice applications.
therefore we compare mepfl with a state of theart approach for trace based failure diagnosis in distributed systems.
the approach uses fault injection to populate the database of failures for a target distributed system and locate the root causes of reported failures by matching against the failures in the database.
in this study we use the trace instances in the training data as the failure database for the approach to identify the faulty microservices and fault types.
note that this state of the art approach cannot identify latent errors as it assumes that failures can be reported from the production environment.
for mepfl we evaluate three versions each with a different machine learning technique i.e.
mepfl rf mepfl knn and mepfl mlp.
table shows the accuracy of intra application error prediction and fault localization.
it can be seen that for both applications mepfl achieves very high accuracy in the prediction of latent errors faulty microservices and fault types.
in particular mepflmlp performs the best.
mepfl mlp predicts latent errors with a recall of .
.
a precision of .
.
and a false positive 690latent error prediction and fault localization for microservice applications esec fse august tallinn estonia table accuracy of intra application error prediction and fault localization methodssock shop trainticket latent error faulty microservice fault type latent error faulty microservice fault type recall precision f1 fpr top1 top3 top5 recall precision f1 recall precision f1 fpr top1 top3 top5 recall precision f1 mepfl rf .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mepfl knn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mepfl mlp .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
approach in n a n a n a n a .
.
.
.
.
.
n a n a n a n a .
.
.
.
.
.
rate .
.
faulty microservices with a top accuracy of .
.
and fault types with a recall of .
.
and a precision of .
.
for sock shop trainticket .
the overall prediction accuracy of sock shop is slightly higher than that of trainticket.
this result may be explained by the fact that sock shop has much fewer microservices and trace types.
for both applications mepfl substantially outperforms the state of the art approach in predicting faulty microservices and fault types.
we also evaluate the accuracy of mepfl for cross application latent error prediction and fault localization.
in this evaluation we use the trace instances of an application sock shop or trainticket as training data and the trace instances of the other application as testing data.
note that the state of the art approach cannot be used for cross application prediction as it relies on the trace instances of the same application for matching.
for mepfl only the microservice level prediction works for this prediction.
table shows the accuracy of cross application error prediction and fault localization.
it can be seen that the accuracy is much lower than that of intra application prediction.
for example for mepfl mlp the recall precision of latent error prediction drop from .
.
and .
.
to .
.
and .
.
.
using trainticket data to predict sock shop has a higher accuracy than predicting the other way around.
this result may be explained by the fact that trainticket has much more microservices and trace types than sock shop.
.
influence of trace type coverage rq2 fault injection and trace collection cost time and effort.
therefore a practical problem is how to decide whether enough training data have been collected i.e.
whether the coverage of trace types in the training data substantially affects the prediction results.
in this evaluation we use the best machine learning technique for mepfl i.e.
mepfl mlp and trainticket as the target application as it is much larger than sock shop.
we randomly divide the trace types of trainticket into subsets and use a part of the trace types for training and the others for testing.
we start with one subset for training and gradually increase the number of subsets for training.
for each setting we calculate the accuracy of latent error prediction and fault localization along with the rate that trace level prediction is adopted t rate and report the training time tt and prediction time pt .
table shows the results of the evaluation.
the general trend is that the prediction accuracy substantially increases with the increase of trace type coverage.
for example when of the trace types are covered by the training data the recall and precision of latent error prediction the top accuracy of faulty microservice prediction and the recall and precision of fault type prediction are all higher than .
with the increase of trace type coverage the overall prediction more and more relies on the trace level prediction.
when the trace type coverage reaches the trace level prediction dominates the overall prediction.
when the coverage reaches .
of the prediction is made by the trace level prediction.
apossible explanation for the trend is that with the increase of trace type coverage it is more likely to find similar trace instances in the training data.
with the increase of trace type coverage the training time linearly increases from 8m37s to 35m35s while the prediction time keeps stable.
.
effectiveness for real fault cases rq3 the purpose of this evaluation is to assess the effectiveness of mepfl for real world microservice faults.
to this end we evaluate the accuracy of mepfl for the fault cases provided by the trainticket benchmark.
these fault cases are replicated from industrial fault cases by transferring the fault mechanisms from the original applications to the benchmark application.
from fault cases we choose the fault cases that belong to the three fault types for the evaluation.
we use the trace instances produced with injected faults to train the prediction models using mepfl mlp and apply the models on the fault cases.
for each fault case we apply the patch into the application to produce a faulty version and then deploy the version.
five student volunteers play the role of users and manually execute the scenarios that may involve the target microservice.
for each execution we manually check the system logs and execution results to confirm whether it involves a latent error and whether the error is caused by the fault case.
we collect all the produced trace instances and apply the prediction models on each trace instance.
the evaluation results are shown in table .
for each fault case we provide the fault type multi instance configuration or asynchronous interaction the numbers of trace instances tr and erroneous ones et and the accuracy metrics.
it can be seen that mepfl accurately predicts the latent errors caused by these fault cases with a recall of .
.
.
on average a precision of .
.
.
on average and a false positive rate of .
.
.
on average the faulty microservices with a top accuracy of .
.
.
on average and the fault type with a recall of .
.
.
on average and a precision of .
.
.
on average .
it can be seen that the prediction accuracy for configuration faults f3 f4 f5 f7 is lower than the accuracy for multi instance faults f8 f11 f12 and asynchronous interaction faults f1 f2 f13 .
this result may be due to that execution traces are less sensitive to latent errors caused by configuration faults.
multi instance faults and asynchronous interaction faults have direct influences on execution traces e.g.
involved microservice instances interaction sequences or accesses of shared variables and cache.
in contrast configuration faults are usually related to environmental configurations and resource consumption and have no direct influences on execution traces.
.
threats to validity there are three major threats to the internal validity of the studies.
the first one lies in the correctness of fault injection and trace 691esec fse august tallinn estonia xiang zhou xin peng tao xie jun sun chao ji dewei liu qilin xiang and chuan he table accuracy of cross application error prediction and fault localization methodstrainticket model predicting sock shop sock shop model predicting trainticket le fm ft le fm ft recall precision f1 fpr top1 top3 top5 recall precision f1 recall precision f1 fpr top1 top3 top5 recall precision f1 mepfl rf .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mepfl knn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mepfl mlp .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table influence of trace type coverage coverage tt pt t ratelatent error faulty microservice fault type recall precision f1 fpr top1 top3 top5 recall precision f1 8min37s 12s .
.
.
.
.
.
.
.
.
.
.
11min58s 12s .
.
.
.
.
.
.
.
.
.
.
14min15s 12s .
.
.
.
.
.
.
.
.
.
.
16min24s 12s .
.
.
.
.
.
.
.
.
.
.
19min58s 12s .
.
.
.
.
.
.
.
.
.
.
23min39s 12s .
.
.
.
.
.
.
.
.
.
.
27min26s 12s .
.
.
.
.
.
.
.
.
.
.
30min42s 12s .
.
.
.
.
.
.
.
.
.
.
35min35s 12s .
.
.
.
.
.
.
.
.
.
.
table effectiveness for real world fault cases fault cases type tr etle fm ft recall precision f1 fpr top1 top3 top5 recall precision f1 f1 ai .
.
.
.
.
.
.
.
.
.
f2 ai .
.
.
.
.
.
.
.
.
.
f3 c .
.
.
.
.
.
.
.
.
.
f4 c .
.
.
.
.
.
.
.
.
.
f5 c .
.
.
.
.
.
.
.
.
.
f7 c .
.
.
.
.
.
.
.
.
.
f8 mi .
.
.
.
.
.
.
.
.
.
f11 mi .
.
.
.
.
.
.
.
.
.
f12 mi .
.
.
.
.
.
.
.
.
.
f13 ai .
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
labeling.
the fault injection implemented by automatic code transformation may fail to introduce a fault as expected and the error status of a trace instance may be incorrectly labeled.
the second one lies in unknown problems within the two benchmark applications.
these problems may cause unexpected errors of the applications thus influencing the error status of trace instances.
the third one lies in the uncertainties of automatic deployment and runtime management.
there are two major threats to the external validity of the studies.
the first one lies in the limitation of two target applications used in the studies.
sock shop and trainticket are much smaller than complex industrial microservice applications.
the second one lies in the limitation of the fault cases used in the studies.
the trainticket fault cases may be simpler than industrial faults and represent only a limited part of different types of faults.
thus it is not clear whether the approach can be effectively applied for much larger industrial applications and more complex fault cases.
related work traditional fault localization approaches include slice based fault localization e.g.
spectrum based fault localization e.g.
fault localization based on analyzing program states e.g.
fault localization based on data mining e.g.
and model based fault localization e.g.
.
in recent years machine learning techniques have been applied for fault localization in different systems .
these approaches focus on traditional programs and thus are not applicable to microservices.
indeed these approaches can be potentially applied to application faults in microservice applications whereas our approach focuses on microservice specific faults.
recently multiple approaches on fault localization for distributed systems or cloud native systems have been reported.
whittaker et al.
presented an approach that enables the developers to reason about the causes of events in distributed systems.
leonardo et al.
proposed a lightweight approach of fault localization for cloud systems.
wang et al.
presented an online approach of incremental clustering for fault localization in web applications.
phamet al.
proposed an approach to automating failure diagnosis in distributed systems by combining fault injection and data analytics.
compared to our work the preceding previous approaches are designed for traditional distributed systems or cloud systems lacking consideration of complex environmental factors that are essential in microservice applications e.g.
auto scaling of the instances of each service tremendous asynchronous interactions.
there are previous approaches on fault analysis for cloud systems by considering the high complexity and dynamism of cloud computing environments.
lin et al.
proposed an approach for predicting the failure proneness of a node in a cloud service system based on historical data.
pitakrat et al.
proposed an approach named hora which combines component failure predictors with architectural knowledge and predicts failures caused by three representative types of faults memory leak system overload and sudden node crash.
ahmed et al.
proposed a machine learning approach based on detecting metric correlation stability violations csv for automated localization of performance faults for datacenter services running under dynamic load conditions.
dean et al.
presented perfcompass an online debugging approach for performance anomaly faults.
perfcompass can quantify whether a production run performance anomaly has a global impact or local impact.
the preceding previous approaches share some similar ideas as our approach.
however these approaches do not support latent error prediction and they do not focus on microservices.
although hora can be applied for microservice applications it is used to predict violations of expected qos levels caused by system level faults e.g.
memory leak system overload and sudden node crash .
in contrast our approach is tailored to microservices and predicts latent errors caused by three types of microservice specific faults in the implementation and configuration of microservices.
conclusion in this paper we have proposed mepfl an approach for latent error prediction and fault localization of microservice applications by learning from system trace logs.
it supports three types of microservice application faults that are specifically relevant to microservice interactions and runtime environments i.e.
multi instance faults configuration faults and asynchronous interaction faults.
based on a set of features defined on system trace logs mepfl trains prediction models at both the trace level and microservice level using the system trace logs collected from automatic executions of the target application and its faulty versions produced by fault injection.
our experimental studies have demonstrated the effectiveness of mepfl with two open source microservice benchmarks and real world fault cases.