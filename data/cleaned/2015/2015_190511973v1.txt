empirical review of java program repair tools a large scale experiment on bugs and repair attempts thomas durieux inesc id and ist university of lisbon portugal thomas durieux.mefernanda madeiral federal university of uberl ndia brazil fer.madeiral gmail.com matias martinez university of valenciennes france matomartinez gmail.comrui abreu inesc id and ist university of lisbon portugal rui computer.org abstract in the past decade research on test suite based automatic program repair has grown significantly.
each year new approaches and implementations are featured in major software engineering venues.
however most of those approaches are evaluated on a single benchmark of bugs which are also rarely reproduced by other researchers.
in this paper we present a large scale experiment using java test suite based repair tools and benchmarks of bugs.
our goal is to have a better understanding of the current state of automatic program repair tools on a large diversity of benchmarks.
our investigation is guided by the hypothesis that the repairability of repair tools might not be generalized across different benchmarks of bugs.
we found that the tools are able to generate patches for of the bugs from the benchmarks and have better performance on defects4j compared to other benchmarks by generating patches for of the bugs from defects4j compared to of bugs from the other benchmarks.
our experiment comprises repair attempts in total which we used to find the causes of non patch generation.
these causes are reported in this paper which can help repair tool designers to improve their techniques and tools.
acm reference format thomas durieux fernanda madeiral matias martinez and rui abreu.
.
empirical review of java program repair tools a large scale experiment on bugs and repair attempts.
in proceedings of the 27th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse .
acm new york ny usa pages.
introduction software bugs decrease the quality of software systems from the point of view of the software system users.
manually repairing bugs is well known as being a difficult and time consuming task.
to address this activity automatically a new field of research has emerged named automatic program repair .
automatic program repair consists of automatically finding solutions executable permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august tallinn estonia association for computing machinery.
acm isbn x xxxx xxxx x yy mm.
.
.
.
software bugs without human intervention .
the most popular approach to automatically repair bugs is to create a patch using the test suite of the program as the specification of its expected behavior.
this type of approach is known as test suite based program repair which has been applied in several repair tools in the last decade .
the repair tools are used in empirical evaluations so that the repairability of the repair approaches they implement is measured.
these evaluations are reported in the literature in two ways when a new repair approach is proposed e.g.
or when a dedicated full contribution on evaluating existing repair tools is reported e.g.
.
the evaluations consist of four main aspects in general the selection of benchmarks of bugs the collection of data by executing repair tools on the selected bugs an investigation on the effectiveness of the repair approach regarding some criteria e.g.
repairability correctness and repair time and finally the comparison of repair approaches and discussion.
a major problem with all previous evaluations focusing on repair for java programs is that they are widely performed on the same benchmark of bugs defects4j .
in theory this should not be a problem if defects4j is not biased however no benchmark is perfect .
benchmarks should reflect the representativeness of the bugs and the projects they come from in the real world.
the extent of the representativeness of benchmarks for real world bugs is unknown because even the distribution of the real world bugs is unknown.
therefore by using a single benchmark when evaluating repair tools a bias can be introduced which makes hard to generalize the performance of repair tools.
in this paper we report on a large experiment conducted on test suite based repair tools for java using other benchmarks of bugs than defects4j.
the primary goal of this experiment is to investigate if the existing repair tools behave in a similar way across different benchmarks.
if a repair tool performs significantly better on one benchmark than on others we say that the repair tool overfits the benchmark .
the secondary goal is to understand the causes of non patch generation from a practical view which to the best of our knowledge has not been subject of investigation by the repair community.
to achieve our goals we designed our experiment considering three out of the four main aspects usually used to evaluate repair tools a on benchmark we use benchmarks including defects4j totaling bugs b on execution we run repair tools on the bugs using a framework we developed to automatize andarxiv .11973v1 may 2019esec fse august tallinn estonia durieux et al.
simplify the execution of repair tools on different benchmarks c on observed aspect we analyze the repairability of the tools focusing on their performance across the different benchmarks.
we do not target the fourth aspect of evaluations which is about comparing repair tools.
our goal is not to compare repair tools among themselves but to compare the behavior of each tool among different benchmarks.
through our experiment we first observed that all repair tools are able to generate a test suite adequate patch for bugs from each of the benchmarks.
however when analyzing the proportion of bugs patched by the repair tools per benchmark we found that indeed the repair tools perform better on defects4j than on the other benchmarks.
finally we found six main reasons why repair tools do not succeed to generate patches for bugs.
for instance we observed that incorrect fault localization and multiple fault locations have a significant impact on patch generation.
these reasons are valuable for repair tool designers and researchers to improve their tools.
to sum up our contributions are a large scale experiment of repair tools on bugs from benchmarks this is the largest study on automatic program repair ever i.e.
x repair attempts a repair execution framework named repairthemall that adds an abstraction around repair tools and benchmarks which can be further extended to support additional repair tools and benchmarks a novel study on the repairability of repair tools across multiple benchmarks where the goal is to investigate if there exists the benchmark overfitting problem i.e.
that the repair tools perform significantly better on the extensively used benchmark defects4j than on other benchmarks a thorough study on the non patch generation cases on the repair attempts that did not result in patches.
the remainder of this paper is organized as follows.
section presents the literature on the evaluation of test suite based repair tools for java which grounds the motivation of our experiment.
section presents the design of our study including the research questions and the data collection and analysis.
section presents the results followed by the discussion in section .
finally section presents the related works and section presents the final remarks.
state of affairs on test suite based automatic repair tools for java automatic repair tools meet benchmarks of bugs when they are evaluated.
in this section we present a review of the literature on the existing evaluations of repair tools which is based on a two step protocol gathering repair tools and gathering information on their evaluations focusing on the used benchmarks and the number of bugs given as input to the repair tools.
to gather repair tools we searched the existing living review on automatic program repair for test suite based repair tools for java which is the focus of this work we found repair tools that meet this criterion.
then we gathered scientific papers containing evaluations of these tools.
there are two types of papers that are interesting for us the first type consists of the presentation of a new repair approach which also includes an evaluation conducted using a tool that implements the approach e.g.
and the second typetable test suite based program repair tools for java.
repair toolbenchmark used bugs patcheda fixedb in evaluation generate and validate acs defects4j arja defects4j quixbugs defects4j 22capgen introclassjava cardumen defects4j quixbugs deeprepair defects4j elixir defects4j bugs.jar genprog a defects4j hdrepair defects4j jaid defects4j jgenprog defects4j defects4j quixbugs defects4j defects4j jkali quixbugs jmutrepair defects4j quixbugs kali a defects4j lsrepair defects4j par pardataset rsrepair a defects4j quixbugs simfix defects4j sketchfix defects4j sofix defects4j ssfix defects4j xpar defects4j semantics driven defects4j dynamoth quixbugs nopol conditiondataset defects4j quixbugs metaprogramming based npedataset npefix quixbugs aapatched bug means that a repair tool fixed it with a test suite adequate patch.
bafixed bug means that a repair tool fixed it with a test suite adequate patch that was confirmed to be correct.
consists of an empirical evaluation carried out on already created tools which is a specific work to evaluate repair tools by running them on benchmarks of bugs e.g.
.
we gathered papers from the first type of papers more than one tool can be presented in the same paper and two papers from the second type.
table summarizes our review on the existing evaluations of the repair tools based on the scientific papers.
the first column presents the repair tools which are grouped by the well known categories generate and validate andsemantics based approaches plus metaprogramming based .
we named the latter category for repair tools that first create a metaprogram of the program under repair and then explore it at runtime which in the end uses the runtime information to generate patches.
each repair tool is associated with one or more benchmarks used in its evaluation in the table.
when a repair tool has been evaluated in more than one benchmark or more than once in the same benchmark we place first the benchmark used in the paper that presented the tool i.e.
first evaluation followed by the other benchmarks with the reference for the posterior studies.empirical review of java program repair tools esec fse august tallinn estonia for instance in the paper that jgenprog is presented there is an evaluation on defects4j this evaluation has no citation in the second column of the table because the evaluation is in jgenprog s paper.
later it was evaluated again on defects4j and also on quixbugs which contain citations of the empirical evaluation papers in the table.
the table also presents additional information on the evaluations which are the number of bugs given as input to the repair tools and the number of bugs for which the tools generated a test suite adequate patch i.e.
patched bugs and a correct patch i.e.
fixed bugs reported by the gathered papers.
in total we found evaluations of the repair tools.
out of repair tools were evaluated on a subset of bugs from defects4j and nine of them were recently evaluated on the quixbugs benchmark .
in some exceptional cases the benchmarks bugs.jar and introclassjava were also used.
however the number of existing evaluations in terms of number of repair tools versus number of benchmarks is low compared to all possible combinations.
there are some benchmarks that were rarely used or never used so far this is partially explained by the fact that some benchmarks were recently published e.g.
bears thus they were not available when some repair tools were published.
we also observe that three repair tools were originally evaluated on datasets that were not presented in the literature in a research paper dedicated for them i.e.
pardataset conditiondataset and npedataset .
this is the case of the first evaluations of par nopol and npefix.
however these repair tools were later evaluated on formally proposed benchmarks except for par which is not publicly available.
par was later reimplemented resulting in the tool xpar which was then evaluated on defects4j.
study design the extensive usage of defects4j at evaluating repair tools motivates our study.
our goal is to investigate if the repair tools have a similar performance on other benchmarks of bugs.
in this section we present the design of our study including the research questions the systematic selection of repair tools and benchmarks of bugs and the data collection and analysis.
.
research questions rq1 .
to what extent do test suite based repair tools generate patches for bugs from a diversity of benchmarks?
this research question guides us towards the investigation on the ability of the existing repair tools to generate test suite adequate patches for bugs from the selected benchmarks.
rq2 .
is the repair tools repairability similar across benchmarks?
the repair tools have been extensively evaluated on defects4j.
our goal in this research question is to investigate if they repair bugs from other benchmarks to a similar extent than they repair bugs from defects4j.
rq3 .
what are the causes that lead repair attempts to not generate patches?
existing evaluations focus on the successful cases i.e.
the bugs that a given repair tool generated patches for.
however to the best of our knowledge there is no study that investigates the unsuccessful cases i.e.
a repair tool tried to repairtable selected repair tools based on our inclusion criteria.
non fulfillment criteria repair toolsexcluded not public c1 elixir par sofix xpar not working c2 acs capgen deeprepair only compatible with defects4j c3 lsrepair simfix faulty class method required c4 hdrepair jaid sketchfix others ssfixincluded arja cardumen dynamoth jgenprog genprog a jkali kali a jmutrepair nopol npefix rsrepair a a bug but no patch was generated.
our goal in this research question is to find the causes of non patch generation so that the repair community can focus on practical limitations and improve their repair tools.
.
subject repair tools to include a java test suite based program repair tool in our study it must meet the following four inclusion criteria criterion .
the repair tool ought to be publicly available our study involves the execution of repair tools therefore tools that are not publicly available are excluded.
we exclude tools with this criterion when the paper where the tool was described does not include a link for the tool we cannot find the tool on the internet and we received an answer by email from the authors of the tool explaining why the tool is not available e.g.
elixir has a confidentiality issue or no answer at all.
criterion .
the repair tool ought to be possible to run some tools are publicly available but they are not possible to run for diverse issues e.g.
acs uses github which recently changed its interface and does not allow programmed queries .
criterion .
the repair tool ought to be possible to run on bugs from different benchmarks beyond the one used in its original evaluation we cannot run tools in other benchmarks if they are hardcoded to specific ones e.g.
simfix is currently working only for defects4j .
criterion .
the repair tool ought to require only the source code of the program under repair and its test suite used as oracle.
these two elements are the two inputs specified in the problem statement of test suite based automatic program repair .
after checking on all repair tools presented in table we found tools that meet the inclusion criteria outlined.
one of them ssfix was further excluded because we had issues to run it so we ended up with repair tools for our experiment.
table presents the excluded and included tools and for the excluded ones it also shows the criterion they did not meet.
note that among the included tools we have eight generate and validate tools the two semantics based tools and the only metaprogramming based tool.
therefore we cover the three approach categories.
we briefly describe each selected repair tool in the remainder of this section.
jgenprog and genprog a are java implementations of genprog which is for c programs.
genprog is a redundancybased repair approach that generates patches using existing code aka the ingredient from the system under repair i.e.
it does not synthesize new code.
genprog works at the statement level esec fse august tallinn estonia durieux et al.
and the repair operations are insertion removal and replacement of statements.
jkali and kali a are java implementations of kali .
kali was conceived to show that most of the patches synthesized by genprog over the manybugs benchmark consist of avoiding the execution of code.
the operators implemented in kali are removal of statements modification of ifconditions to true andfalse and insertion of return statements.
jmutrepair is an implementation of the mutation based repair approach presented by debroy and wong for java.
it considers three kinds of mutation operators relational e.g.
logical e.g.
and unary i.e.
addition or removal of the negation operator !
.
jmutrepair performs mutations on those operators in suspicious ifcondition statements.
nopol is a semantics based repair tool dedicated to repair buggyifconditions and to add missing ifpreconditions.
nopol uses the so called angelic values to determine the expected behavior of suspicious statements an angelic value is an arbitrary value that makes all failing test cases from the program under repair pass.
nopol then collects those values at runtime and encodes them into a satisfiability modulo theory smt formula to find an expression that matches the behavior of the angelic value.
when the smt formula is satisfiable nopol translates the smt solution into a source code patch.
dynamoth is a repair tool integrated into nopol that also targets buggy and missing ifconditions.
the difference between dynamoth and nopol is that instead of using an smt formula to generate the patch it uses the java debug interface to access the runtime context and collects variable and method calls.
then dynamoth combines those variables and method calls to generate more complex expressions until it finds one that has the expected behavior.
this allows the generation of patches containing method calls with parameters for instance.
npefix is different from the generate and validate and semantics based tools it is a metaprogramming based tool.
it means that npefix modifies the program under repair to include several repair strategies that can be activated during the runtime.
npefix repairs programs that crash due to a null pointer exception.
npefix runs the failing test case several times and activates a different repair strategy for each execution.
in the end knowing the repair strategies that have worked together with information of the context that they worked a patch is created.
note that npefix works in a similar way than semantics based tools in this last step if a patch is found it means the patch is already satisfactory.
arja is a genetic programming approach that optimizes the exploration of the search space by combining three different approaches a patch representation for decoupling properly the search subspaces of likely buggy locations operation types and ingredient statements a multi objective search optimization for minimizing the weighted failure rate and for searching simpler patches and a method variable scope matching for filtering the replacement inserted code to improve compilation rate.
cardumen is a test suite based repair approach that works at the level of expressions.
it synthesizes new expressions that are used to replace suspicious expressions as follows.
first it mines templates i.e.
piece of code at the level of expression where the variables are replaced by placeholders from the code under repair.table selected benchmarks of bugs and their sizes.
benchmark projects bugs loc java bears bugs.jar defects4j introclassjava quixbugs total then for creating a candidate patch that replaces a suspicious expression se cardumen selects a compatible template i.e.
the evaluation of the template and the sereturn compatible types and creates a new expression from it by replacing all its placeholders with variables frequently used in the context of se.
rsrepair a is a java implementation of rsrepair .
rsrepair repairs is a test suite based repair approach for c that has been created to compare the performance between genetic programming genprog and random search in the case of automatic program repair.
it showed that in rsrepair finds patches faster than genprog.
.
subject benchmarks of bugs to select benchmarks of bugs for our study we defined the following three inclusion criteria criterion the benchmark must contain bugs in the java language this criterion excludes benchmarks such as manybugs introclass codeflaws and bugsjs .
criterion the benchmark must be peer reviewed presented in the literature in a research paper dedicated for it this criterion excludes benchmarks such as pardataset conditiondataset and npedataset .
criterion the benchmark must include for each bug at least one failing test case this criterion excludes benchmarks such as ibugs .
after searching the literature for benchmarks that meet our criteria we ended up with benchmarks.
table summarizes them by presenting their sizes in number of projects bugs and lines of code.
we present a brief description of them as follows.
defects4j contains bugs from six widely used open source java projects with an average size of lines of java code.
the bugs were extracted by the identification of bug fixing commits with the support of the bug tracking system and the execution of tests on the bug fixing program version and its reverse patch buggy version .
despite the fact this benchmark was first proposed to the software testing community it has been used for several works on automatic program repair.
bugs.jar contains bugs from eight apache projects with an average size of lines of java code.
it was created using the same strategy than defects4j.
its main contribution is the high number of bugs.
bears contains bugs from different github projects with an average size of lines of java code.
it was created by mining software repositories based on commit building state from travis continuous integration.
bears has the largest diversity of project compared to previous bug benchmarks.empirical review of java program repair tools esec fse august tallinn estonia introclassjava contains bugs from six different student projects.
it is a transpiled version to java of the bugs from the c benchmark introclass .
in the transpiled version the projects have on average lines of code.
quixbugs contains single line bugs from programs which are translated into both java and python languages.
each program corresponds to the implementation of one algorithm such as quicksort and contains on average lines of code.
this is the first multi lingual program repair benchmark.
.
building a repair execution framework to run the repair tools on different benchmarks of bugs we created an execution framework named repairthemall which provides an abstraction around repair tools and benchmarks.
figure illustrates the overview of the framework.
it is composed of three main components repair tool plug in where there is the abstraction around repair tools allowing the addition and removal of tools benchmark plug in where there is the abstraction around benchmarks also allowing the addition and removal of benchmarks and repair runner which works as a fa ade for the execution of repair tools on specific bugs.
for the creation of the framework we performed three main tasks one for each component.
first for repair tool plug in we identify the common parameters that are required by the repair tools which we refer to as abstract parameters .
we then map these abstract parameters to the actual parameters of each repair tool.
this is necessary because the repair tools use different parameter names and input formats.
for instance an abstract parameter is the source code folder path and the actual parameter for source code folder path in arja is dsrcjavadir but in jgenprog is srcjavafolder .
we identify eight common parameters for the repair tools source code path test path binary path of the source code binary path of the tests the classpath the java version compliance level the failing test class name and workspace directory.
repairthemall also supports the setting of actual parameters existing in the repair tools to tune specific executions i.e.
actual parameters that are not mapped to any abstract parameter.
then for benchmark plug in we identify the abstract operations that should be performed to use the bugs from benchmarks e.g.
check out a buggy program given a bug id .
we map these abstract operations to the actual operations of each benchmark when they are available.
we define three required bug usage operations to be able to use the benchmark with repair tools to check out a specific bug buggy source code files at a given location to compile the buggy source code and the tests and to provide information on the bug to be given as input to repair tools i.e.
the eight parameters previously mentioned except workspace directory .
if the bug is from a multi module project the source code and test paths should be related to the module that contains the bug.
only defects4j provides bug usage operations that fully covers the required abstract operations.
consequently we had to build above the four other benchmarks the missing operations e.g.
to check out a bug from the quixbugs benchmark.
finally for the repair runner we design the input and output in a simple way so that one can easily interact with the repairthemall framework as well as interpret the results.
for the input the repairthemall frameworkabstract operationsbenchmark plugin abstract parametersconcrete parametersconcrete operations repair runner tool plugincheckout compilea benchmark operations bug info checkout compilebugs a bug repair attempts concrete tool outputa repair tool input repair tool name benchmark name bug id output normalised output repair attempt log figure the repairthemall framework.
one can start an execution of repair tools on benchmarks as a simple command line for instance the command .
repair.py nopol benchmark defects4j id chart starts the execution of nopol on the bug chart of defects4j.
at the end of this execution the repair runner generates a standardized output divided into two files the log of the repair attempt execution repair.log and the normalized json file results.json containing the location of the patches generated by the tools if any and the textual difference between the patches and their buggy program versions.
this standardized output is due to the abstraction around the output format output normalization we create to simplify the analysis and the readability of the results from the different repair tools.
therepairthemall framework currently contains repair tools and benchmarks of bugs and it allows the plug in of other ones to help the repair community to compare different approaches.
moreover the framework allows users to set repair tool executions such as the timeout and the limit of generated patches.
it is publicly available at which includes a tutorial with all the steps to use it and to integrate new repair tools and new benchmarks.
.
data collection and analysis to answer our research questions we executed the repair tools on the benchmarks using repairthemall resulting in patches that are further used for analysis.
in this section we describe the repair tools setup section .
.
and their execution section .
.
and the analysis we performed on the repair attempts to determine the possible causes of non patch generation section .
.
.
.
.
repair tools setup.
for this experiment we set the time budget to two hours per repair attempt a repair attempt consists of the execution of one repair tool over one bug.
we also configured the repair tools to stop the execution of repair attempts when they already generated one patch.
however arja genprog a kali a rsrepair a and npefix do not have this option they stop their repair attempts when they consume their own tentative budget or by timeout.
moreover we configured repair tools to run on one predefined random seed due to the huge computational power required for this experiment we were not able to run the repair tools with additional seeds.
finally table presents the version of each repair tool that we used in this study.
the logs of the repair attempts are available at .esec fse august tallinn estonia durieux et al.
table the used version of each repair tool.
repair tool framework github repository commit id arja genprog a kali a rsrepair aarja yyxhdy arja e60b990f9 cardumen jgenprog jkali jmutrepairastor spoonlabs astor 26ee3dfc8 dynamoth nopol nopol spoonlabs nopol 7ba58a78d npefix spirals team npefix 403445b9a .
.
large scale execution.
to our knowledge our experimental setup is the biggest one on patch generation studies in terms of number of repair tools and bugs and also in execution time.
in total we executed repair tools on bugs from open source projects that the selected benchmarks provide.
this represents repair attempts which took days and .
hours of combined execution almost a year of continuous execution.
this experiment would not be possible without the support of the cluster grid that provided us the required computing power to conduct this work.
.
.
finding causes of non patch generation.
prior studies on patch generation mainly focused on the ability of approaches to generate patches and do not investigate the reasons why non patch generation happens.
the study on non patch generation is important to make research progress so that authors of repair tools can improve their tools.
since there is a lack of knowledge on that subject we are not able to automatically perform the detection of the reasons why patches are not generated for bugs and therefore manual analysis is required.
due to the scale of our experiment setup including different patch generation attempts it is unrealistic to manually analyze each attempt log to understand what happened.
we identify the major causes of non patch generation by analyzing a sample of the repair attempt logs.
we do not predefine the sample because we observe during preliminary investigation that identical behaviors happen for groups of repair attempts.
for instance we found that for all bugs from a specific project all the repair tools have the same issue in the fault localization.
for that reason predefining a sample is not optimal because we would analyze attempt logs that we already know what is the cause of non patch generation.
results the results of our empirical study as well as the answers to our research questions are presented in this section.
.
repairability of the repair tools in this research question we analyze the repairability of the repair tools on the total of bugs.
for that we calculated the number of patched bugs and also the number of bugs that are commonly patched by tool.
figure presents the repairability of the repair tools in descending order by the number of patched bugs.
for each tool it shows the number of unique bugs patched by the tool dark grey the number of patched bugs that other repair tools also patched light grey and the total number of patched bugs with the proportion over all included in this study.
for instance nopol synthesizes patches for0 250nopol dynamoth arja kali a rsrepair a genprog a jgenprog jmutrepair jkali cardumen npefix156 .
.
.
.
.
.
.
.
.
patched bugsunique overlapped figure repairability of the repair tools on bugs.
bugs in total .
of all bugs where are uniquely patched by nopol and are patched by nopol and other tools.
we observe that nopol dynamoth and arja are the three tools that generate test suite adequate patches for the highest number of bugs with respectively and patched bugs in total.
npefix on the other hand generates patches for the fewest number of bugs .
it can be explained by the narrow repair scope of this tool i.e.
bugs exposed by null pointer exception.
on bugs uniquely patched by tools we observe that only jkali failed to generate patches for bugs that are not patched by other tools and that dynamoth is the tool that patches the highest number of unique bugs.
however npefix is the tool that has the highest proportion of unique patched bugs i.e.
of the bugs patched by npefix are unique.
the overlapping between each pair of repair tools in number of bugs is presented in table .
in the case where the column name and the line name are the same main diagonal it presents the number of uniquely bugs patched by the tool.
for instance bugs have been uniquely patched by arja which repairs other bugs that are also patched by genprog a. we observe a large overlap between repair tools that share the same patch generation framework i.e.
the framework where the repair tools are implemented see table .
for instance arja has an overlap of with genprog a with kali a and with rsrepair a all implemented in the arja framework.
however arja has an overlap ranging from to with the other repair tools.
dynamoth has an overlap of with nopol but only to with the other tools.
each tool implemented in the astor framework e.g.
jgenprog has a big overlap with other tools in astor .
moreover the tools in astor also present high overlapping with the tools in the arja framework which are tools sharing similar repair approaches.
answer to rq1 .to what extent do test suite based repair tools generate patches for bugs from a diversity of benchmarks?
the repair tools are able to generate patches for bugs ranging from to bugs from a total of bugs.
they are complementary to each other because repair tools fix unique bugs all but jkali .
we also observe that the overlapped repairability of the tools is impacted by their similar implemented repair approaches and also by the patch generation framework where they are implemented.
1the full list of the patched bugs and the textual patches are available in .empirical review of java program repair tools esec fse august tallinn estonia table the number of overlapped patched bugs per repair tool.
each row rpresents the percentage of overlapped patched bugs of one tool trwith the rest of the tools.
for instance of the bugs patched by arja row are also patched by genprog a column .
on the contrary of the bugs patched by genprog a row are also patched by arja column .
arja genprog a kali a rsrepair a cardumen jgenprog jkali jmutrepair nopol dynamoth npefix arja genprog a kali a rsrepair a cardumen jgenprog jkali jmutrepair nopol dynamoth npefix .
benchmark overfitting in this research question we compare the repairability of the repair tools on the bugs from the extensively used benchmark defects4j with their repairability on the other benchmarks included in this study which are bears bugs.jar introclassjava and quixbugs.
table shows the number of bugs that have been patched by each repair tool per benchmark.
we first observe that defects4j is the benchmark with the highest number of unique patched bugs which represents .
of all defects4j bugs.
the next most patched benchmarks are quixbugs with and introclassjava with .
of their bugs.
this difference can also be observed in the total number of generated patches per benchmark defects4j is still dominating the ranking with generated patches even that it contains fewer bugs than bugs.jar versus bugs .
to test if the repairability of the repair tools is independent of defects4j we applied the chi square test on the number of patched bugs for defects4j compared to the other benchmarks.
the null hypothesis of our test is that the number of patched bugs by a given tool is independent of defects4j .
we observed in table that the p value is smaller than the significance level .
for all repair tools.
hence we reject the null hypothesis for those tools and we conclude that the number of patched bugs by them is dependent of defects4j.
therefore repair tools overfit defects4j .
the repairability of the repair tools on defects4j cannot be only explained by the repair approaches.
we raised three hypotheses that can potentially explain the repairability difference between defects4j and the other benchmarks there is a technical problem in the repair tools the bug fix isolation performed on defects4j has an impact on repairing defects4j bugs and the distribution of the bug types in defects4j is different from the other benchmarks.
.
in rq1 we observed the importance of the implementation of the tools for the repairability.
one hypothesis that can explain the fact that repair tools overfit defects4j is that the authors of the repair tools have debugged and tuned their frameworks for defects4j and consequently improved significantly the repairability of their tools for this specific benchmark.
for instance they may have paid attention to not let the dependencies of the repair tools to interfere with the classpath of the defects4j bugs in order to preserve the behaviorof test executions on the defects4j bugs.
however this issue can affect the bugs of other benchmarks.
.
the second hypothesis is related to the way that defects4j has been created.
a bug fixing commit might include other changes that are not related to the actual bug fix.
then given a bug fixing commit the authors of defects4j recreated the buggy and patched program versions so that the diff between the two versions contains only changes related to the bug fix this is called bug fix isolation.
the resulted isolated bug fixes facilitate studies on patches .
however such a procedure can potentially have an impact on the repairability of the repair tools.
for instance by comparing the developer patch with the defects4j patch for the bug closure we observe that the method isnegativezero has been introduced in the buggy program version which contains part of the logic for fixing the bug.
the presence of this method in the buggy program version can simplify the generation of patches by the repair tools or introduce an ingredient for genetic programming repair approaches.
.
our final hypothesis is related to the distribution of the bugs in the different benchmarks.
defects4j might contain more bugs that can be patched by the repair tools compared to the other benchmarks.
for that reason the bug type distribution of each benchmark should be further analyzed and correlated with the repairability of the tools.
to our understanding the first hypothesis is more plausible since we observe in rq1 that the implementation of the repair tools has an impact on their repairability.
however additional studies should be designed to identify which hypothesis or a combination of hypotheses has an impact on the repairability of the repair tools on defects4j compared to other benchmarks.
answer to rq2 .is the repair tools repairability similar across benchmarks?
there is a difference in the repairability of the repair tools across benchmarks.
indeed the repairability of all tools is significantly higher for bugs from defects4j compared to the other four benchmarks therefore we conclude that they overfit defects4j.
in addition we raised three hypotheses that might explain this difference.
the confirmation of those hypotheses are full contributions themselves therefore our study opens the opportunity for several future investigations.esec fse august tallinn estonia durieux et al.
table number of bugs patched by at least one test adequate patch and the p value of the chi square test of independence between the number of patched bugs from defects4j compared to the other benchmarks.
repair toolbenchmark bears bugs.jar defects4j introclassjava quixbugs total p value arja .
genprog a .
kali a .
rsrepair a .
cardumen .
jgenprog .
jkali .
jmutrepair .
nopol .
dynamoth .
npefix .
total total unique .
.
.
.
.
table percentage of repair attempts that failed by error.
repair toolbenchmark bears bugs.jar defects4j introclassjava quixbugs average arja .
.
.
.
genprog a .
.
.
.
.
kali a .
.
.
.
rsrepair a .
.
.
.
.
cardumen .
.
.
.
.
jgenprog .
.
.
.
.
jkali .
.
.
.
.
jmutrepair .
.
.
.
.
.
nopol .
.
.
.
.
dynamoth .
.
.
.
npefix .
.
.
.
.
average .
.
.
.
.
.
.
causes of non patch generation in this final research question we analyze the repair attempts that did not result in patches and we identify the causes of non patch generation.
the goal of this research question is to provide highlights to the automatic repair community on the causes of non patch generation so that authors of repair tools can improve their tools.
table and table present the percentage of repair attempts that finished due to an error and by timeout respectively.
they show that the repair attempts in error or timeout represent the majority of all repair attempts .
.
the bugs.jar benchmark is the main contributor to this percentage.
the size and complexity of the bugs.jar projects show the limitation of the current automatic patch generation tools.
moreover table shows that npefix is the tool with the highest error rate but this tool crashes when no null pointer exception is found in the execution of the failing test case that exposes a bug.
regarding the timeout in table jgenprog and cardumen are more subject to reach timeout.table percentage of repair attempts that failed by timeout.
repair toolbenchmark bears bugs.jar defects4j introclassjava quixbugs average arja .
.
.
.
genprog a .
.
.
.
.
kali a .
.
.
rsrepair a .
.
.
.
.
cardumen .
.
.
.
.
jgenprog .
.
.
.
.
.
jkali .
.
.
.
jmutrepair .
.
.
.
nopol .
.
.
dynamoth .
.
.
npefix .
.
.
average .
.
.
.
.
.
we then manually analyzed the execution trace of the repair attempts to identify the causes of non patch generation.
the methodology for this analysis is described in section .
.
and by following it we identified six causes of non patch generation.
.
a logical problem is that the repair tools do not have a patch that fixes the bug in their search space.
for instance npefix is not able to generate patches for bugs that are not related to null pointer exception.
jgenprog is not able to generate a patch when the repair ingredient is not in the source code of the application which happens frequently for small programs like the ones in quixbugs.
new repair approaches should be created to handle this cause of non patch generation.
.
when the fault localization does not succeed to identify the location of the bug the repair tools do not succeed to generate a patch for it.
this can be due to a limitation of the fault localization approach or to the suspiciousness threshold that the repair tools use.
moreover we identified that test cases that should pass are failing and consequently there is a misleadingempirical review of java program repair tools esec fse august tallinn estonia fault localization.
for instance the fault localization fails on all bugs from the inria spoon project from the bears benchmark because the fault localization does not succeed to load a test resource and consequently the passing test cases fail.
.
developers frequently fix a bug at more than one location in the source code we refer to this type of bug as multi location bug.
however most current repair tools and fault localization tools do not support multi location bugs.
for instance the bug math from defects4j has to be fixed in the exact same way at two different locations and the two locations are specified by two failing test cases.
the current tools consider that the two failing test cases specify the same bug at the same location and consequently do not succeed to generate a multi location patch.
.
we observe that some of the repair attempts finish the execution by consuming all the time budget.
considering the size of this experiment it is not realistic to increase drastically the time budget.
however new approaches and optimizations can minimize this problem.
in this study we detected repair attempts that failed by timeout.
it is not possible to predict the outcome of those attempts but a previous study showed that additional time budget might result in a higher number of generated patches by genetic programming approaches.
however in our experiment the repair tools require .
minutes on average to generate a patch which is significantly lower than the allocated time budget two hours .
.
we also observe that the repairthemall framework does not succeed to correctly compute parameters from some bugs to give as input to the repair tools such as compliance level source folders and failing test cases.
this results in failing repair attempts which can be due to a bug in repairthemall or an impossibility to compile the bug.
for instance npefix fails times because of issues related to classpath.
.
the final cause of non patch generation is related to other technical limitations that cause the nonexecution of the repair tools.
one of them is about too long command lines.
the repair tools are executed from the command line which means that all parameters must be provided in the command line.
however the size of the command line is limited and in the case of projects that have a long classpath the operating system denies the execution of the command line which results in failing repair attempts.
on bugs.jar for instance repair attempts finished with the error argument list too long .3finally there are also other diverse issues that cause the repair tools to crash.
for instance jgenprog finished its repair attempt on the bug flink 6bc6dbec from bugs.jar with a nullpointerexception .
answer to rq3 .what are the causes that lead repair attempts to not generate patches?
through an analysis on logs of repair attempts we identified six causes of non patch generation such as incorrect fault localization.
each cause should be investigated in detail in new studies.
moreover repair tools designers are also stakeholders on those causes which inform them what are the weakness of their tools and help them to understand their previous evaluations results.
2all the occurrences of invalidclasspathexception in our execution 3repair attempts that end with argument list too long 4log file of the repair attempt discussion diversity of program repair benchmarks.
in rq2 we found that all java repair tools included in this study perform significantly better on the bugs from defects4j than on the bugs from other benchmarks.
indeed repair tool evaluations that only use defects4j have a threat to the external validity since the repairability results cannot be generalize for other benchmarks.
we then conclude that future tools should be evaluated on diverse benchmarks to mitigate that threat.
impact of the repair tools engineering on the repairability.
during the conduction of this study we observed that the implementations of the repair approaches play an important role in their ability to repair bugs.
for instance jkali and kali a share the same approach but they neither have the same implementation nor the same results see table .
kali a fixes bugs while jkali fixes with the same input.
note that this observation has also been correlated with the analysis of non patch generation where a significant number of causes is not related to the repair approaches themselves but to their implementations.
this observation highlights a potential bias in empirical studies on automatic program repair that compare the repairability of different repair approaches.
based on this observation those studies only compare the effectiveness of repair tools not the approaches themselves.
challenges of creating repairthemall .the main challenges we faced to run repair tools are related to the creation of the repairthemall framework.
first we checked all test suite based repair tools for java based on our criteria e.g.
availability so that we could find the suitable tools for our study see table .
then we had to understand the repair tools we finally gathered where manual source code analysis was required so that we could compile them and find their inputs and requirements the tools are diverse sometimes not documented and implemented by different researchers.
once we understood the repair tools we could plug them in the repairthemall framework which contains the abstraction around the tools.
those challenges are mainly due to lack of well organized open science repositories for all the repair tools.
good documentation examples and instructions on how to compile the tools can speed up the process of learning on how to execute repair tools.
the observed repairability compared to the previous evaluations.
table shows the test suite based repair tools for java and the repairability results from their previous evaluations.
those results are difficult to compare with the results of our study because the previous evaluations on defects4j did not consider all bugs from the benchmark.
on defects4j only cardumen fixes fewer bugs in this study compared to the previous evaluation.
this can be explained by the difference of the setup such as the number of random seed considered in the study and potential bugs in the version of cardumen we use.
according to those results repairthemall configures correctly the repair tools to generate patches since no major drawbacks have been observed.
threats to validity.
as with any implementation the repairthemall framework is not free of bugs.
a bug in the framework might impact the results we reported in section .
however the framework and the raw data are publicly available for other researchers and potential users to check the validity of the results.esec fse august tallinn estonia durieux et al.
this study focuses on test suite adequate patches which means that the generated patches make the test suite pass yet there is no guarantee that they fix the bugs.
studying patch correctness is out of the scope of this work.
our goal is to analyze the current state of the automatic program repair tools and identify potential flaws and improvements.
the conclusions of our study do not require the knowledge on the correctness of the patches.
our goal is to have a full picture of test suite based repair tools for java.
in our literature review presented in section we found repair tools that compose the full picture.
our study was conducted considering only of them note that this is the largest experiment in terms of number of repair tools and benchmarks .
however we do not have the full picture we wanted to which is a threat to the external validity of our results.
most of the repair tools that we did not include in our study are just not possible to be ran for instance par is not even available.
open source tools allow the community to generate knowledge in several directions.
in our work open source tools allowed us to perform a novel evaluation on the state of the repair tools.
another direction is to help the development of new tools for instance deeprepair is built over astor which is a library for repairing.
related works the works related to ours are empirical studies on the repairabilityof multiple automatic program repair tools.
repair tools for c programs were the subject of investigation in the first empirical studies on automatic program repair.
qi et al.
introduced the idea of plausible i.e.test suite adequate versus correct patch.
they studied the patch plausibility and correctness of four generate andvalidation repair tools on the bugs from the genprog benchmark which later became a part of the manybugs benchmark .
they found that a small number of bugs are fixed by correct patches.
anincorrect test suite adequate patch is known as an overfitting patch because it overfits the test suite.
such problem was named asthe overfitting problem by smith et al.
who studied it in the context of two generate and validate c repair tools on the bugs from the introclass benchmark .
they found that even using highquality high coverage test suites results in overfitting patches.
later le et al.
analyzed the overfitting problem for semantics based repair techniques for the c language.
the study also investigates how test suite size and provenance number of failing tests and semantics specific tool settings can affect overfitting.
for java martinez et al.
reported on a remarkable large experiment where three repair tools were executed on the bugs from defects4j.
the focus of their study was to measure the repairability of the repair tools and to find correct patches by manual analysis.
they also found that a small number of bugs could be repaired with a test suite adequate patch that is also correct.
ye et al.
presented a study where nine repair tools were executed on the bugs from quixbugs.
they used automatically generated test cases based on the human written patches to identify incorrect patches generated by the repair tools.
motwani et al.
reported on an empirical study that included seven repair tools for both java and c languages where the defects4j and manybugs benchmarks were used.
they had a different focus they investigated if the bugs repaired by repair tools are hard and important.
to do so they used the repairability data fromtable empirical studies on repair tools.
work language tools bench main focus c plausible vs. correct patch c patch overfitting c patch overfitting java patch overfitting java patch overfitting java c repairability vs. bug related measures ours java benchmark overfitting non patch generation previous works and they performed a correlation analysis between the repaired bugs and measures of defect importance the human patch complexity and the quality of the test suite.
table summarizes the mentioned studies.
the main difference between our study and all the previous ones is the goal.
previous works focused on the patch overfitting problem and advanced correlation analysis between the repairability of tools and bug related characteristics.
we introduce the benchmark overfitting problem which is investigated in this paper as well as the causes of non patch generation.
moreover the scale of our study is much larger than previous studies on repair tools and benchmarks.
conclusions in this paper we presented an empirical study including repair tools and bugs from benchmarks.
in total repair attempts were performed this is the largest experiment to our knowledge.
the goal of our experiment is to obtain an overview of the current state of the repair tools for java in practice.
for that we scaled up the previous experiments by considering more benchmarks of bugs which combined have bugs from projects collected with different strategies.
we found that the repair tools are able to repair bugs from benchmarks that were not initially used for their evaluations.
however our results suggest that all repair tools overfit defects4j.
finally we analyzed why the repair tools do not succeed to generate patches this study resulted in six different causes that can help future development of repair tools.
our study opens several opportunities for future investigations.
first our hypotheses on why the repair tools perform better on defects4j can be further confirmed.
for instance one of the hypotheses is the fact that the buggy program versions were changed in defects4j due to the bug fix isolation.
a study to confirm this hypothesis is a full contribution itself.
second other repair tools can also be executed to aggregate and scale up our study.
ssfix for instance is possible to run despite the fact we had issues for it which lead to its exclusion in this work.
moreover the tools that are hardcoded to be ran on defects4j could also be adapted to work for other benchmarks of bugs.
finally an investigation on the bug type distribution in the benchmarks should also be conducted.
this would provide the information on how many bugs a repair tool is actually able to fix i.e.
by finding the bugs that meet the repair tools bug class target.