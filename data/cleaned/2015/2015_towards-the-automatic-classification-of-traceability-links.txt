towards the automatic classification of traceability links chris mills department of computer science florida state university tallahassee usa chris.mills0905 gmail.com abstract a wide range of text based artifacts contribute to software projects e.g.
source code test cases use cases project requirements interaction diagrams etc.
.
traceability link recovery tlr is the software task in which relevant documents in these various sets are linked to one another uncovering information about the project that is not available when considering only the documents themselves.
this information is helpful for enabling other tasks such as improving test coverage impact analysis and ensuring that system or regulatory requirements are met.
however while traceability links are useful performing tlr manually is time consuming and fraught with error.
previous work has applied information retrieval ir and other techniques to reduce the human effort involved however that effort remains significant.
in this research we seek to take the next step in reducing it by using machine learning ml classification models to predict whether a candidate link is valid or invalid without human oversight.
preliminary results show that this approach has promise for accurately recommending valid links however there are several challenges that still must be addressed in order to achieve a technique with high enough performance to consider it a viable completely automated solution.
index terms software traceability traceability link recovery machine learning i. i ntroduction software projects are made up of different types of artifacts such as source code files requirements specifications legal regulations design documents etc.
which contain important knowledge about the different facets of a system.
information about the relationship between related artifacts of different types can be leveraged in various software tasks such as program comprehension and impact analysis.
it can also serve as a means of ensuring that regulations and requirements are met as the project evolves.
traceability link recovery tlr is the task associated with deriving links between relevant documents in different artifact sets for the purpose of improving the software development process.
unfortunately performing tlr manually is arduous and error prone.
in fact despite extensive research efforts adoption of traceability in industry is still limited due to the perception that its benefits are outweighed by the associated costs .
this is especially true for small teams or projects without rules imposed by a regulatory body.
from the perspective of a software company performing traceability recovery represents an immense investment of human capital.
however software traceability is widely accepted as an indicator of a well constructed system in the research community has been shown to improve software maintenance activities and benefit even small project teams .
therefore lowering the barrier to adoption by further reducing the required human effort is an important research task.
a large body of existing work has focused on partially automating tlr through applying information retrieval ir and machine learning ml as well as model and rule based approaches .
while these techniques reduce the human effort involved in tlr the process still relies on significant human intervention.
for example most state ofthe art ir approaches require a human operator to manually inspect documents or links in a ranked list to establish traceability.
these approaches also frequently require a threshold of similarity above which links are considered to be valid in practice this is a difficult parameter to tune.
existing ml approaches determine similarity using the presence of indicator words mined from requirements or another highlevel source e.g.
architectural tactics or legal requirements .
this process can be difficult when documents in the training set of artifacts have few words in common such as code classes test cases or use cases.
additionally model based approaches are only applicable in specific scenarios where artifacts can be represented as even informal models.
finally rule based approaches require human operators to create and maintain rules which are often project specific and apply only to artifacts comprised of structured language.
as an alternative we propose a completely automated ml classification approach that does not require a predefined similarity threshold and is applicable to any text based software artifact without assumptions about internal structure.
the ideal version of such an approach would produce a perfect prediction of the valid and invalid links therefore completely automating tlr without human intervention.
while this perfect approach may be hard to reach intermediary versions that are able to accurately reveal at least a subset of the valid links automatically would still be beneficial.
for example small teams working on unregulated projects that currently have no established tlr process can benefit from even an imperfect starting point for adopting software traceability.
for large safety critical stringently regulated projects on the other hand complete automation might not be possible or even advisable.
because the model would need to meet .
c ieeease urbana champaign il usa doctoral symposium1018 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
prohibitively stringent provable accuracy criteria some degree of human oversight will always be required for verification.
however even in these cases we seek an approach that further minimizes effort through additional automation.
the next sections provide an overview of our proposed approach show the results of an initial evaluation outline existing challenges and ways to address them in future work and discuss related research.
ii.
a pproach and preliminary evaluation our proposed approach considers all possible links between two sets of artifacts simultaneously and based on a ml classification model and a set of descriptive features determines which of those links are likely to be valid and which are likely to be invalid.
the main contribution of this research compared to previous work is a completely automated blackbox approach to tlr that accepts two sets of software artifacts and then provides a set of recommended candidate links that are expected to be valid.
the next subsection describes the feature set used to represent traceability links in the model.
a. traceability link representation a potential traceability link consists of two documents.
when the link is valid these documents are related to one another.
for example one document could be a use case and the other a code class that implements it.
when the link is invalid the documents are not related to one another.
in the proposed approach we represent each potential link with an extensive set of features designed to capture the similarity between the two artifacts as well as information on the internal quality of either artifact.
in our initial implementation we used features that can be divided into three main categories ir ranking features building on the long standing success of ir based techniques in capturing the semantics of software artifacts we make use of several such approaches for generating features that reflect the semantic relationship between each pair of artifacts.
for each ir technique considered and each pair of documents d1andd2in artifact setsd1andd2respectively we compute two features.
the first is the rank of d2in the list of results from d2when usingd1as a query and the other is the rank of d1in the list of results from d1when using d2as a query.
by including both features we exploit asymmetries in ir based document similarity that have been shown to impact ir results .
document statistics features the two artifacts in a potential link also have basic statistical properties that reveal information about both similarity and document terseness which has been shown to be a factor in hard to trace artifacts .
there are five features in this category representing a pair of artifacts the number of unique terms in each of the two documents the total number of terms in each of the documents and the jaccard measure between the terms in the two documents.
query quality qq features we have previously used qq features in our work to capture the internal quality of software engineering artifacts and to identify hard to traceartifacts in tlr.
here we make use of the set of preand post retrieval qq metrics used in our previous work and consider each of them as individual features for each artifact.
qq features help the classification model differentiate cases when similarity is low between a pair of artifacts because they are unlikely to be related from cases where similarity is dampened because one or both of the documents is of poor semantic quality i.e.
a hard to trace artifact .
b. preliminary implementation using the aforementioned features we constructed a preliminary implementation of our ml classification approach as a proof of concept in order to gauge its potential and obtain a baseline for performance.
in this implementation we consider four tr approaches traditional vsm with cosine similarity bm25 and two methods based on language model smoothing techniques jelinek mercer and dirichlet.
because we employ four different tr approaches our link representations are quite large including different features.
we also investigated four classification algorithms random forest rf classification trees j48 k nearest neighbors knn k and n aive bayes to understand which of these is most suitable for classifying potential traceability links.
we use the default weka1implementations of all four algorithms to establish baseline performance without any special algorithm tuning.
furthermore because the approach considers all possible links between two sets of artifacts we expect the data to be highly imbalanced i.e.
there should be many more invalid than valid links .
therefore there is an intrinsic class imbalance which poses an obstacle for accurate link classification that must be addressed .
in this implementation we apply synthetic minority oversampling technique smote and random majority undersampling to balance the data.
c. evaluation a preliminary evaluation was conducted on eleven datasets from six software projects involving six different types of artifacts.
table i shows the details of each dataset.
this data was selected because it contains projects that are frequently used to evaluate new techniques for tlr .
as anticipated we found a significant class imbalance in this data at an average valid to invalid link ratio of .
for the evaluation we had access to an oracle for each dataset that contains known links between documents established by the original designers of the system.
using these oracle files as a ground truth we employed ten fold crossvalidation to compute performance metrics for models using each classification algorithm trained on data balanced with either smote or undersampling.
note that the testing set was not rebalanced as so would bias the experiment by artificially boosting performance.
table ii shows the results of this evaluation expressed in true positive rate tpr and false positive rate fpr .
tpr is equivalent to recall and shows the percentage of valid links that were recovered by the classifier.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i software systems used in evaluation system possible links valid links artifact types cm .
high r low r eanci .
uc cc etour .
uc cc smos .
uc cc itrust .
uc cc easyclinic .
uc cc easyclinic .
tc cc easyclinic .
id tc easyclinic .
id uc easyclinic .
tc uc easyclinic .
id cc total .
high r high level requirements low r low level requirements uc use cases cc code classes id interaction diagrams tc test cases table ii average tpr and fpr for each classification algorithm using either smote or majority undersampling smote undersampling algorithm tpr fpr tpr fpr j48 .
.
.
.
knn .
.
.
.
n aive bayes .
.
.
.
random forest .
.
.
.
fpr is the number of invalid links that were predicted to be valid.
a perfect model would get .
tpr i.e.
the model identifies all of the valid links with .
fpr i.e.
there are no invalid links predicted to be valid .
typically recall is the primary metric by which tlr approaches are measured as even with human intervention an approach cannot fully support tlr unless it is capable of identifying all or a vast majority of the traceability links .
in our case because we ultimately seek a fully automated system it is imperative that we also minimize fpr otherwise the approach could recommend a set of links containing a prohibitively large amount of noise that must be manually validated.
that is to say a model with extremely high tpr but moderate fpr is not useful in a practical sense.
our data shows that a baseline model with no feature selection applied and no parameter tuning is capable of achieving an average tpr of .
i.e.
it recovers .
of the valid links on average across projects with a fairly low fpr of .
when using rf with only smote balancing.
furthermore using only majority undersampling the same model is capable of identifying .
of the valid links but at the cost of a much higher fpr .
.
this illustrates the importance of data balancing when addressing tlr as a classification problem.
future work will investigate how parameter tuning and hybrid balancing approaches can be used to further optimize a model to maximize tpr for a minimal fpr.
iii.
c hallenges and future work while our proposed approach has several advantages compared to existing techniques it also has several unique challenges.
first because we use ml classification we rely onexisting historical data to train our models.
therefore the approach is not directly applicable to greenfield projects for which historical data is extremely limited or not available.
to address this shortcoming future work will focus on applying cross project training in which similar projects are used to train models.
because project specific configurations are usually able to outperform generic ones we will also utilize techniques from cold start software analytics to achieve near optimal configurations without the need for project specific data.
additionally we will investigate the use of genetic algorithms for deriving optimized parameter configurations for a set of classification algorithms rather than relying on defaults.
moreover the feature set used in the initial implementation is extremely large features and will only grow if additional ir techniques are included.
therefore it is important to incorporate an appropriate feature selection process capable of extracting features that most efficiently differentiate valid and invalid links while introducing minimal noise.
finally we compare this approach to other state of the art semi automatic approaches.
we will also perform a user study to empirically establish if this approach is truly able to reduce the human effort required for tlr and if so to what degree.
iv.
r elated work the first area of related work focuses on applying ir approaches to automate tlr.
these include probabilistic and vector space models vsm latent semantic indexing lsi and latent dirichlet allocation lda among others.
due to space constraints we direct the interested reader to borg et.
al.
which provides a detailed systematic mapping study of the area.
additionally binkley and lawrie applied learning to rank to traceability which is similar to this approach in that they too combine multiple ir approaches to better estimate document similarity.
gethers et.
al.
also combine ir approaches in their work leveraging orthogonal information for tlr.
duan and cleland huang combined clustering and ir to develop an approach that recommends groupings of artifacts that are likely related to a query document.
these works differ from ours in that they follow a standard process that requires a user to inspect a list of results for each document in a query set or specify a similarity threshold for classification while ours does not.
in addition to ir based approaches there have also been various automation efforts based on other techniques including annotated dependency graphs model driven engineering and rules based approaches using requirement to object model rules .
however these approaches differ significantly from our work.
rules based approaches require significant effort to establish rules and discipline to maintain artifacts that adhere to those rules.
finally the approach proposed by egyed and grunbacher relies on mapping artifacts to shared common ground which may not be applicable for all artifact types.
alternatively our approach is broadly applicable to any text based software artifact.
also related to this research are other classification approaches to tlr.
cleland huang et.
al.
first proposed authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a probabilistic classifier trained on a set of indicator words for non functional requirements which was subsequently used for linking regulatory codes with project requirements and architectural tactics with source code .
this work differs from ours in that our approach does not rely on the extraction of identifier word sets as part of training instead ir ranks between two documents in a potential link are used to express document similarity.
therefore our approach is applicable when tracing two artifact sets with many individual documents such as test and use cases without the need to map them first to indicator words.
finally falessi et al.
applied classification for predicting the number of true links left in an ir result list.
this work differs from ours in that we are predicting the validity of individual links.
v. c onclusion given that traceability has the capacity to improve the development process resulting in software that is higher quality and projects that are more likely to be on time and budget reducing the required investment of human effort is an important research task.
here we propose a machine learning classification approach based on an extensive feature set that leverages document similarity via several ir engines in the context of query quality to guage which artifacts are hard totrace.
as shown in our preliminary evaluation the approach has promise but additional work is required to boost performance improve support for greenfield projects and build sufficient tooling for a full scale side by side comparison with existing state of the art approaches.