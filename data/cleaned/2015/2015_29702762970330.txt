evaluating the evaluations of code recommender systems a reality check sebastian proksch sven amann sarah nadi mira mezini software technology group technische universit t darmstadt germany proksch amann nadi mezini st.informatik.tu darmstadt.de abstract while researchers develop many new exciting code recommender systems such as method call completion code snippet completion or code search an accurate evaluation of such systems is always a challenge.
we analyzed the current literature and found that most of the current evaluations rely on arti cial queries extracted from released code which begs the question do such evaluations re ect real life usages?
to answer this question we capture ne grained development histories from real ide interactions.
we use them as a ground truth and extract real queries for a speci c method call recommender system.
we compare the results of such real queries with di erent arti cial evaluation strategies and check several assumptions that are repeatedly used in research but never empirically evaluated.
we nd that an evolving context that is often observed in practice has a major e ect on the prediction quality of recommender systems but is not commonly re ected in arti cial evaluations.
ccs concepts general and reference evaluation information systems recommender systems software and its engineering software notations and tools humancentered computing design and evaluation methods keywords empirical study arti cial evaluation ide interaction data .
introduction research in the area of recommendation systems for software engineering rsse regularly produces exciting ideas on how to automatically support developers with their daily coding and maintenance tasks.
examples include recommending the next syntactic token links to related code snippets and which windows to close .
to provide quantitative evidence that such tools are accurate and valuable extensive evaluations are needed.conducting such evaluations is often challenging.
many evaluations of rsse involve humans for realistic evaluations e.g.
.
unfortunately conducting such controlled experiments is often infeasible due to the high cost in terms of both time and resources .
other issues include the replicability of the experiment or privacy constraints when analyzing developer behavior.
in addition controlled experiments are based on selected use cases which limits the generalizability of the results.
to overcome these challenges many researchers have resorted to arti cial evaluation strategies that generate evaluation queries from released code.
these strategies can overcome the drawbacks of controlled experiments with real developers.
they are easy to conduct and scale well allowing them to cover di erent scenarios.
however using them raises the following critical question how realistic are they?
in other words do arti cial evaluations actually re ect real life usages?
given that the method for creating queries from released code greatly di ers between the evaluation strategies we surveyed it is important to understand how di erent decisions may in uence evaluations.
to the best of our knowledge these questions have not been systematically and empirically investigated.
in order to compare arti cial approaches to real approaches it is necessary to have the appropriate ground truth.
we use an existing ide interaction tracker to capture a dataset of ne grained history of source code directly in the development environment of developers.
this dataset provides the necessary means for a realistic evaluation of source based rsse.
in our experiments we feed a method call recommender pbn with queries extracted from the captured code changes and compare this realistic evaluation strategy with arti cial approaches to answer two research questions rq1 do arti cial queries have an e ect on the measured prediction quality of a recommender?
rq2 do real queries have properties that are not re ected in arti cial queries?
our results show that arti cial evaluations can be misleading often suggesting a higher prediction quality than what would be achieved in practice.
we show that the di erences result from ignoring evolving context that is not captured in arti cial queries.
our results help toolsmiths make informed decisions about the evaluation strategy best suited for their goals and understand implications of these decisions.
in summary this paper makes the following contributions we present a survey of related work to identify common evaluation strategies and classify them based on the design decisions they make.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
using a tool that captures developers interactions we collect a set of real code changes from professional software developers researchers and students.
we use this dataset to conduct an extensive experiment to evaluate the quality of arti cial evaluation strategies and tradeo s in their design space.
we publish the tools used in our experiments on the artifact page of this paper .
we also publish all parts of the dataset for which we have the required permission.
this dataset can be used as a benchmark for evaluating future recommenders.
.
current ev aluation techniques we survey source based rsse to understand how they are currently evaluated.
we consulted a rsse survey the recent rsse book and several recent publications in premier software engineering conferences to identify related evaluation techniques.
because we use a source based method call recommender in our experiments we focus the survey on the evaluations of related rsse.
while the most related evaluations would be those of other method call recommenders we relax the survey criteria and also consider other source based rsse to get a general overview of how evaluations are typically done.
we include approaches that consider the structural and context information found in code as opposed to techniques that treat code as text .
based on this survey we summarize existing evaluation strategies and motivate our problem statement accordingly.
.
existing evaluation strategies many rsse are being evaluated through controlled experiments that involve human subjects.
to quantify the performance of the system researchers analyze if participants successfully complete a task e.g.
or measure how long subjects take for completion e.g.
.
in other more qualitative evaluations experts judge the usefulness of the proposals e.g.
or the subjects rate their experience with the tool e.g.
.
all these are valid evaluations and they often create additional qualitative insights.
however controlled experiments also have their downsides.
their nature limits their scope and makes it hard to generalize the results.
many tools also work with external services e.g.
q a websites which hinders replication or require very task speci c data like navigation information e.g.
which makes it hard to design appropriate tasks.
most importantly designing a controlled experiment involving humans takes a lot of time.
in addition the risk of a failed experiment is also quite high because a study cannot be simply replayed when for example a bug is discovered after the fact.
as a result it is much more common to nd arti cial evaluations of rsse in the literature which motivated the research questions of our work.
in the following we will discuss several representatives of arti cial evaluations that we found in the literature.
the goal is not to present an exhaustive list of prior evaluations but to introduce several high level ideas by example.
heinemann et al.
present a method call recommender .
in addition to considering the structure of a program the approach considers identi ers such as variable names.
the recommender tokenizes source code into an event stream and learns a model of this stream.
to evaluate the approach they iterate over this event stream predict every method invocation that they encounter and measure the quality of the proposals.
they assume linearity and do not include information that is found after the query point.zhang et al.
propose a recommender that predicts parameters for method calls .
for evaluation they use published source code and query the recommender at each observed parameter.
queries contain all observed information from the code with the exception of the parameter that is to be predicted.
they also conducted a user study that analyzes the perceived usefulness and opinions of the participants to get qualitative feedback for their tool.
however they do not present quantitative data about the performance of the recommender or the correctness of the proposals.
bruch et al.
propose a method call recommender based on the best matching neighbor bmn algorithm.
queries are automatically generated from api usages observed from code repositories.
a query consists of a subset of the observed method calls in the api usage and the evaluation measures how well the recommender predicts the removed calls.
the authors use two strategies to generate such queries a no calls included strategy that mimics the situation where a developer triggers code completion when she starts to implement a method and a rst half strategy that keeps only the rst half of the method calls to mimic the situation where a developer triggers code completion after she wrote parts of a method.
follow up work replaces the bmn algorithm by a patternbased bayesian network pbn as the recommender engine.
the evaluation also uses both the no calls included strategy and the rst half strategy.
in addition queries are generated with a random half strategy that randomly selects which half of the method calls is kept to mimic that developers may not write code in a linear fashion.
the random selection is repeated and the results are averaged.
grapacc recommends code snippets that are related to the current context.
patterns are mined from the source code of some java projects to create the recommender which is then evaluated on several other projects.
for the evaluation all method calls are extracted from the method bodies in the validation projects.
these sets of methods are divided into two parts the rst part is used as a query for the recommender the second part as the expectation.
the evaluation measures the fraction of proposed method calls that are contained in the second part and the proposal.
this evaluation technique is similar to the rst half evaluation followed by bruch et al.
.
mapo is a miner and recommender for api usage patterns.
the miner identi es api patterns in a large pool of released source code.
in the ide the current context is matched against these patterns to retrieve related code snippets.
the authors use code snippets selected from a tutorial book in the evaluation which are considered correct and complete.
in their queries they use all context information and the rst method call.
given such a query the recommender suggests related code snippets which are manually matched with the expectation.
prospector is a recommender that is queried with a tuple of two api types an input type from the current context and a target type that the developer wants to obtain an instance of.
prospector returns a sequence of method calls that would return an object instance with the respective type.
for the evaluation the authors manually picked example programming problems that they deem realistic and that prospector is applicable to.
for the queries they always assume that the developer knows both types.
112table classi cation of existing arti cial evaluation strategies.
columns show the identi ed query scenarios while rows show the selection strategies.
divides.alt0m n divides.alt0m m divides.alt0m linear random guervo et al.
present insynth a tool to synthesize type correct expressions.
as such expressions can be complex structures that contain nested sub expressions the approach e ectively recommends code snippets.
for the evaluation they manually create query expectation pairs as benchmarks taken from several open source projects.
a query is a program snippet in which a single expression is removed for the benchmark.
the evaluation measures whether insynth can synthesize that expression again.
since the target expression is selected arbitrarily the evaluation approach resembles the random removal approach used for pbn .
unlike the pbn evaluation the benchmark set is manually created rather than automatically generated.
raychem et al.
develop a recommender that suggests multiple missing method calls in a piece of code.
they traverse the syntax tree and reduce it to a sequence of method calls.
missing method calls are holes in this sequence.
the recommender calculates the most likely sequences of method calls that ll the holes.
three kinds of queries are used for evaluation a single hole at the end of the program multiple holes that are manually introduced and one or more random holes that are automatically introduced.
the rst query strategy assumes linearity in code development the other two assume non linearity.
the authors do not describe whether there is a limit on how many holes are introduced our understanding is that only a small percentage of the method calls are removed for querying.
note that the three last papers are somewhat di erent from the others since they also include queries manually created by the tool smith.
this is done to create realistic meaningful queries that a real developer might trigger.
however since the creation of such queries is very subjective and not based on any input from actual developers we consider these evaluations as arti cial.
some approaches conduct controlled experiments and capture all interactions of the subjects for later experiments.
however existing approaches either capture data that is not appropriate to build or evaluate source based rsse e.g.
or they do not perform automated evaluations and evaluate results manually e.g.
.
to the best of our knowledge there have been no attempts to create a benchmark of developer interactions for an automated evaluation of rsse before.
.
problem statement our survey of evaluation approaches suggests that arti cial evaluations are more popular than real evaluations.
this is not surprising since real evaluations tend to be too expensive and time consuming to be practical .
they usually also involve factors like privacy considerations and convincing developers to use a research prototype.
while arti cial evaluation strategies do not have the issues mentioned above it is important to understand how close they are to real evaluations.
to investigate this the goal of this paper is to answer the two research questions posed in the introduction.
speci cally we evaluate di erent arti cial evaluation tech niques that employ di erent query generation strategies and compare them to a realistic evaluation.
our survey identi ed two factors that di erentiate the arti cial query generation strategies the query scenario and the selection strategy.
query scenario.
any piece of code has context information .
this includes structural context information such as the surrounding method or class as well as information such as which methods have been called and in which order.
the query scenario describes how much context from the nal code is kept in the query.
most query generation approaches focus on the target information they want to predict.
for example a method call recommender focuses on the methods called in a particular context while a parameter recommender focuses on the parameters of a particular method call.
given a nal code snapshot of mitems of target context information the query generation approaches range from removing all of this context divides.alt0m to leave one out m divides.alt0m with shades in between where the target context is partly preserved n divides.alt0m .
divides.alt0min this case all target context information is removed from the nal state of the code resulting in a minimal query.
if the approach depends on speci c information e.g.
the type of the variable on which code completion was triggered this information is preserved.
this strategy mimics the situation where developers are just starting to write code and may not know where to start.
creating such queries is straightforward since there are no ambiguities in what goes into a query.
n divides.alt0min this case parts of the existing code is preserved.
this mimics the typical development scenario where developers implement some parts of a method but potentially miss details for which they need the recommender s help.
m divides.alt0min this case only one piece of information is removed.
this mimics the case of developers who already implemented most functionality but only miss one part.
selection strategy.
the selection strategy is the second differentiating factor which determines how partial information is selected from the nal context.
it answers the question given a complete piece of code from a repository which parts of it should be removed for querying?
several approaches assume a linear development of source code and remove the later parts of a method.
other approaches perform a random selection of the context or even repeat the random selection multiple times to cover di erent parts of the existing code.
linear assuming code is developed in a linear fashion greatly simpli es the evaluation and makes its implementation straightforward.
however there is no empirical evidence that developers actually code in this fashion and so it is unclear how realistic this assumption is.
random for a thorough evaluation several random subselections are made for a complete usage.
the results are averaged to get one representative prediction quality measure.
the averaging adds an extra layer to the implementation.
heuristics may be needed to limit the number of selected queries since the number of sub selections can be quite large.
randomly selecting parts of the existing code to remove might also hide corner cases in which a recommender performs particularly well or badly.
table classi es the related work we presented in section .
along the two identi ed dimensions.
given the variability of arti cial evaluation strategies it is important to understand the impact of di erent choices how they re ect real life developer usage and how they compare to a real evaluation 113repositoriesreference modelsmicro commitsrecommender developersreal queries??
?artificial queriespredictionquality real predictionquality artificial generating queriesestablishing ground truthlearning modelsfigure overview of evaluation comparison strategy strategy.
such knowledge is valuable in two ways first it helps to judge the validity of evaluation results reported in the literature.
second it helps researchers make informed decisions about their evaluation strategies and how to interpret their evaluation results.
.
overview of ev aluation setup figure outlines the overall setup we use to compare the identi ed evaluation strategies.
the idea is composed of two parts.
first we establish a ground truth from which we generate di erent types of queries for the identi ed evaluation strategies.
second we provide the di erent queries to a recommender and compare the quality of its proposals as a means to compare the di erent strategies.
to ensure fairness and comparability we use the same recommender system for all evaluation strategies.
the choice of the particular subject recommender system is less important because the e ect of any weakness or strength of the recommender will be equal across all evaluation strategies.
we use the pattern based bayesian network pbn method call recommender because it provided us with a complete open source evaluation pipeline.
the core data structure used in pbn is an object usage which contains information about the context in which the usage of a particular api type was observed e.g.
the enclosing method or the way the object is initialized as well as the set of methods that have been invoked on that instance of the type.
to build the recommender we create reference models for various api types the learning models step in figure .
we used an open source dataset of c projects as input training data to create the reference models .
the dataset was created from open source github repositories and contains usage data for more than unique apis each of which includes various types.
we lter the dataset to the types that appear in our ground truth and use the collected usages to build reference models for these types.
since establishing the ground truth and generating the queries are the main steps in setting up our evaluation comparison experiments we respectively dedicate section and section to the details of each of these steps.
.
establishing the ground truth to ensure a realistic and fair comparison we rst establish the ground truth about what queries developers actually perform and what the code looks like at that point.
thus we are interested in capturing a development history of how method calls are added to the code under edit over time.
this allows us to replay the development using the actual state of the code at query time i.e.
to simulate a controlled experiment with real queries.
it also allows a comparison of the results for di erent recommenders.the commit history obtained from a project s source control repository has been commonly used to obtain such a development history e.g.
hassan and holt .
however it has also been shown that on average commits are created on every third day with a high variance between users and the type of changes .
it has also been shown that version control commits shadow many of the intermediate code changes .
this means that the version history found in public repositories of open source software is too coarse grained for our purposes.
instead we choose to capture more ne grained code changes directly from the developer s integrated development environment ide similar to the idea previously proposed by robbes and lanza .
while both hassan and holt and robbes and lanza used some form of development history to improve recommenders our work is di erent in that we use this development history tocompare di erent evaluation strategies rather than use it to improve the recommender itself.
.
creating the tooling to get more ne grained code changes we want to capture snapshots of the code under edit every time a change occurs.
such information is best captured directly from within the developers ide.
additionally we want to capture interactions of the developers with the ide s code completion tool and store which method was selected from the list of proposals if any.
we combine the code snapshot and the timestamp as well as the optional selection of a method proposal if available.
we call the collection of this information an enriched micro commit .
we can use these micro commits to create real queries and to replay the recorded development history including code completion after the fact.
to collect such information we extended feedbag an open source instrumentation of visual studio that collects interactions of c developers with their ide .
we extended the instrumentation of the code completion and added a static analysis that extracts context information from the code under edit.
each time code completion is triggered by the developer or when it pops up automatically we create a snapshot of the source code under edit.
snapshots are stored in the form of simpli ed syntax trees a lightweight format that also includes typing information and which supports markers for code completion trigger points.
.
gathering the data feedbag s sources are publicly available and the tool can be installed from within visual studio.
once a user has the tool installed their interactions and micro commits are automatically captured.
users can then upload this captured data to our servers at any time through a provided dialog.
we rst deployed our modi ed version of the tracking tool 114table contributed events per developer id type days queries researcher .
student .
student .
hobby programmer .
student .
student .
... unknown .
... professional .
total .
with company x cannot be named for privacy reasons that develops tax and accounting related software as well as in house software for years.
it employs more than developers out of which more than write programs in c .
development projects span from small training examples to core business applications.
in addition we advertised the project in several social media channels and during various conferences to widen our user base.
even before our active recruitment e orts several open source developers independently installed our tool after seeing it in visual studio s public plug in repository.
we also had several students installfeedbag while they were developing di erent systems including a game web applications in asp.net or their own masters thesis project.
finally one of the authors of this paper along with student assistants also had feedbag installed while they working on the tools used in this paper.
our nal data set therefore contains queries from a variety of users and projects including industrial developers opensource developers researchers and students.
the individual participant contributions are listed in table .
note that the table is cropped for brevity but the complete list is available on the artifact page.
the asterisk in the table marks the contributions of the involved author of this paper.
in total we received submissions of captured data from users.
out of these were industrial developers that provided of our queries.
the remaining users with percentage contributions shown in parenthesis were students .
researchers .
and hobby programmers .
.
the remaining users .
decided not to ll our optional pro le information.
the submissions cover days and span over a period of months but not all users participated the whole time.
.
post processing the data there is still a gap between our collected ground truth and the input data required for pbn.
the collected micro commits are le oriented snapshots whose contents re ect a complete type declaration in a le e.g.
a class with all its methods and the corresponding method bodies while the input data for pbn are object usages.
to bridge this gap we rst sort the micro commits by time and declared type.
as a result we get the development history of a le.
after this we extract object usages for all types used in each micro commit.
since a micro commit represents a whole class we extract object usages for several types in this step.
finally we merge the resulting usages from all micro commits of the same user group them by type and by enclosing method and preserve the order to create a complete usage history.
mc2a1a2b1 a3t0t1t2file xusagehistoriesmc1figure from micro commits to usage histories figure illustrates an example.
the le icons depict micro commits while the squares represent object usages.
the character in each square shows the type of that speci c object usage the index is only added for easier reference.
assume that we have created two micro commits for le x that were captured at times t0andt1.
for the micro commit att1 we captured the information that a speci c method was selected from the code completion proposals depicted by the .
assume that at time t0 the code only contained usages of type a while at time t1 usages of type b were also added.
our usage extraction therefore nds a single object usage for type a in the rst micro commit and two object usages for the types a and b in the second one.
in order to use all information that is contained in the usage history we developed two strategies that transform implicit knowledge into explicit states in the usage history.
given all extracted usages we can identify types that are used in a speci c context but for which we were not able to extract usages for all micro commits.
in the example we can derive that b was not yet used in the rst micro commit but added to the second one.
to make sure that we include such usages that are created from scratch i.e.
usages where we only know the object type and surrounding context but do not have any called methods yet in our evaluation we add an empty usage to b s history at time t0.
another corner case that needs to be handled is if the last micro commit in a usage history contains a selection.
in this case we will not actually see the e ect of the selection in subsequent micro commits because none exist.
in order to preserve the selection for our evaluation we create an arti cial usage in which we merge the usage on which code completion was triggered with the selection result.
referring to the example in figure a method selection took place for the micro commit captured at time t1.
assuming the completion trigger would have taken place on the object usage of type a and that t 1is the last micro commit in the history we would create an object usage a 3at time t which contains the selected method too.
after the extraction our usage history may contain subsequent occurrences of the same object usage.
this duplication may be due to several reasons.
for example consider the situation in which a developer invokes code completion but then cancels it.
this would result in two equal usage snapshots in the history.
another example is that the developer uses the types a and b in the same context.
she adds multiple method calls to the object of type b but leaves the object of type a untouched for a while.
this would add several unchanged usage snapshots to a s history.
we post process the histories and remove such duplicates.
after applying the described transformations our nal ground truth set that is used in our experiments consists of usage histories.
on average each usage history contains .
snapshots but a few outliers exist with a length 115of more than usages.
we manually inspected these cases and all inspected cases were examples in which the developer spent time in a speci c context implementing an algorithm and working with the same type over and over again.
frequent additions and removals of the same methods bloat up the histories for these usages e.g.
adding and removing a log statement.
however outliers with more than steps represent less than of our collected data so we did not introduce special handling for these cases.
our collected usage histories cover types used in di erent method contexts.
the dataset we used to train the recommender contains a total of object usages for these types.
note that pbn models do not contain ordering information.
while object usages only contain a setof method invocations our extraction implementation guarantees that the order in which invocations are entered re ects the order in the source code.
while this is irrelevant for object usages used in pbn models it is important for generating order dependent queries for some of the evaluation strategies we compare.
.
generating queries in this section we discuss how we use our collected groundtruth data set to generate queries that can be used to compare di erent evaluation strategies.
as discussed in section arti cial evaluations are based on released code found in a repository.
this version is treated as the nal state of the code and considered correct and complete.
arti cial evaluations apply heuristics to approximate past states of this version which are then used to generate queries.
the nal state serves as expectations to judge the quality of an rsse.
the evaluation thereby measures the recommender s ability to lead the developer towards that nal state.
since arti cial evaluation strategies are much cheaper than real evaluations we expect that toolsmiths and researchers will continue to use them.
the goal of our evaluation is to analyze the di erent heuristics that are used to approximate queries.
we do this by comparing an evaluation based on these heuristics to an evaluation based on real queries which we obtain from our object usage histories.
this evaluation comparison uncovers qualities and problems of the arti cial strategies and we identify guidelines for future evaluations.
.
obtaining real queries the usage histories from our dataset mimic the real development history and re ect changes to the les under edit in a very ne grained manner.
in terms of evaluating a recommender a query has an input state and an expected output state.
we assume that the last snapshot of a history represents the outcome of a development task.
we therefore use this nal snapshot to formulate our expectation on the evaluated recommender s proposals similar to how arti cial evaluations use the code from a repository.
however the di erence between an arti cial query and a real query lies in which code state is used for the query input.
we extract queries from our usage histories by combining pairs from each snapshot in the history with the nal state.
after ltering pure removals i.e.
queries in which calls were removed but no calls were added and queries that contained equal start and end states we ended up with real queries for the evaluation.start 1public void m t t new t t.m1 t.mx t.my end 1public void m t t t. create t.m2 t.m1 strategy de nition site calls in query linear t.create m2 random t.create m1 and m2in multiple queries real t.create m1 real new t m1 mx my figure example of a 1query case labeled as divides.alt02 and the queries created for each strategy yellow is change red is removal and green is addition .
.
generating pbn queries at this point we have the set of real queries obtained from the usage history.
to compare the results of a real evaluation to an arti cial approach it is necessary to emulate the heuristics that build an arti cial start state from the real end state.
recall from section .
that there are two dimensions used to automatically generate arti cial queries query scenario divides.alt0m n divides.alt0m and m divides.alt0m and selection strategy linear random and real .
to create the arti cial queries we rst identify the query scenario for each real query and then apply the di erent selection strategies on that query to create an arti cial one.
query scenario.
we rst categorize the collected queries by the type of performed change.
we assign each query a label that re ects the number of calls added or removed.
a label n r a means that the query contained ncalls in the input and that rcalls were removed while acalls were added to come to the nal state.
consider the query in figure as an example.
the start state contains three calls m1 mx and my .
for the nal state mxand mywere removed while m2 was added.
thus this is an example of a query.
since pbn can only suggest method call additions and not removals we needed to adapt query labels accordingly.
we do so by dropping the removals from the labels used for categorization.
for example for the query in figure even though the removals mxandmyare used in the real query we do not include them in the nal categorization label.
instead we label the query as a divides.alt02change to indicate that the query already contained one out of the two nal calls.
we assign the queries to the three query scenarios based on this label.
selection strategy.
once a query is assigned to a query scenario we next generate the actual queries for each selection strategy as follows.
we use the query in figure to explain the di erence between strategies.
linear in this case the query is taken only from the end state.
the method calls to be included in the query are selected top down from the list of existing method calls in the end state.
for our example query which was classi ed as a1 divides.alt02query according to the above query scenario classi cation we could technically generate both divides.alt02and1 divides.alt02 queries.
yet we only generate a divides.alt02query to have the real query to compare it to.
the linear approach would select m2for the query because it is the rst method call that exists in the end state code.
however the order of appear116ance of calls in source code at the end state might not be the order in which they were added during development as can be seen from the example.
random the random strategy also selects the information to include in the query from the end state but instead of selecting method calls linearly it selects them randomly.
in the end state of figure there are only two methods.
to generate the divides.alt02query using the random strategy we randomly pick one call and use it in the query.
to make sure all scenarios are covered the approach repeats this random selection until all possible method calls are covered and the results are averaged.
in our example two possible queries will be generated where the rst includes only m1 as input and expects m2and the second includes only m2as input and expects m1.
an average of the prediction quality of both results is then taken to re ect the prediction quality of this whole query.
real based on the unique opportunity of having detailed development information available in our collected usage histories we introduce the real selection strategy to reproduce what would actually happen during development or during a controlled experiment with subject developers.
we only use the information that would be available to a recommender in a real life scenario where the query was placed during development.
this means that only the start state is used to query the recommender.
the ne grained history re ects the evolving context over time and includes all information that do not show in the end state because they were missing changed or got removed in the usage history.
for example the query does not only include method m1 but also the removed methods mxand my as well as the original de nition site new t that was changed during development to the static call t.create .
real without noise real to understand the e ect of the evolving context in real queries we add a fourth strategy that we call real .
the only di erence between real and real is that the latter would not include any evolving context in the query.
to create the query real uses the context of the end state and selects all methods from the start state that have not been removed during development.
in the example the divides.alt02query would include only m1 because it existed before.
in addition t.create would be selected as the de nition site.
we consider real to be an arti cial approach because the selection of the methods for the query and the inclusion of the correct de nition site can only happen after the fact .
the input of each generated query is used to request proposals from the recommender which is built from the reference models.
we measure the prediction quality by comparing the set of proposals with the expected outcome which is the set of methods that are missing in the query input but that exist in the end state.
the similarity is calculated through the f1 measure i.e.
the combination of recall and precision .
a detailed overview of the number of queries we captured in each query scenario is shown in table .
.
do artificial queries work?
in this section we empirically compare the di erent selection strategies described in the previous section.
we follow the evaluation comparison strategy that has been outlined in figure and explained in section in order to answer the two research questions posed in the introduction.table available queries for n divides.alt0m scenarios n divides.alt0m .
.
.
.
.
.
.
do artificial queries affect the measured prediction quality of a recommender?
to compare the evaluation strategies we feed our reference recommender with the generated queries.
the quality is measured by comparing the proposals to the expected additions available in the end state of the query case.
table shows the quality obtained for each selection strategy and corresponding query scenario.
we explain these results by going through each query scenario columns and comparing the selection strategies rows .
we present the results of each query strategy and provide an interpretation of the ndings.
divides.alt0m.queries in this category contain no method calls as part of their input.
this means that none of the method calls in the expected state appear in the input state.
this is the most common case and .
of our data falls into this category.
we factored out new as a special subset of divides.alt0m that re ects the case in which the developer did not write any code so far.
in this case no information is available in the code context apart from the enclosing method and the type of the usage.
another special category is divides.alt01queries which can be assigned to both divides.alt0mandm divides.alt0m.
we decided to assign it to the divides.alt0mcategory but show it as a separate column in the table for better examination of the results.
the results in table show that for such queries no di erence exists between the arti cial strategies.
this is not surprising because all arti cial strategies end up with the same query created from the same end state.
one observation is that it seems that the more missing calls exist the harder it is for the recommender to nd them.
another observation is a notable di erence in the results of real queries.
while the di erence is already noticeable for new queries it gets worse for0 divides.alt01and the recommender seems to be unable to process divides.alt02 queries .
.
the only di erence here between real and the arti cial strategies is missing or changing context information.
while the de nition site might change for all divides.alt0mqueries divides.alt01and0 divides.alt02 might additionally contain calls that are about to be removed.
n divides.alt0m.such queries contain some existing calls and re ect the use case in which the developer has already started to write some code and then asks for help.
this is the case for .
of the queries in our dataset.
we factored out divides.alt02 queries for the same reasons as the divides.alt01 queries.
we nd that the f1 values for this query scenario di er greatly for divides.alt02queries.
the random approach reports the highest quality for the recommender .
.
the other approaches result in a much lower quality .
for linear and .
for real .
the real evaluation which includes evolving context in the query yields the lowest quality with .
.
the results for n divides.alt03 di er slightly between the di erent selection strategies and are around .
the only approach that sticks out is random which reports a 117table di erent query scenarios divides.alt0m n divides.alt0m new divides.alt01 divides.alt02 divides.alt02 n divides.alt03 m divides.alt0m linear .
.
.
.
.
.
random .
.
.
.
.
.
real .
.
.
.
.
.
real .
.
.
.
.
.
queries .
.
.
.
.
.
much higher quality of .
overall the random results are consistently higher in the n divides.alt0mquery scenario than the other selection strategies.
the real evaluation again reports worse quality than the remaining arti cial strategies.
m divides.alt0m.this is the extreme case of n divides.alt0m only the last call in the method is missing while the remaining methods are given as part of the input.
only .
of the queries in our dataset fall into this category.
the results show that the quality of the real and the real strategy are comparable.
however the reported quality of random is much higher .
and even exceeded by the linear strategy .
.
the selection of methods in the query is the only di erence between the three arti cial approaches yet we see di erent results.
the linear selection strategy reports the highest quality .
while real is close to the real result and reports .
.
the random approach mixes the di erent extremal values and reports a quality in between .
.
it seems that some missing methods are harder to predict than others and that developers select these methods last.
interpretation of results.
for the divides.alt0mqueries no di erence can be seen between the arti cial approaches.
on the other hand the real queries perform worse because the definition site is unknown.
the more calls that need to be predicted the more problematic this seems to be.
in addition a direct comparison between real and real shows that evolving context in real queries reduces the reported quality.
this is true for all query scenarios.
we look at this more closely in section .
where we examine the e ect of evolving context information.
when compared to the random selection strategy the linear strategy seems to be better in some cases e.g.
m divides.alt0m while worse in others e.g.
n divides.alt0m .
the randomized generation of multiple queries and averaging of the results seems to cause a smoothing e ect that creates more robust results.
we averaged the results over all queries not shown in the table to see if this is a general e ect.
however we found that all arti cial selection strategies achieve results that are comparable to each other linear .
random .
and real .
.
although all arti cial strategies create queries from the same set of methods found in the end state of a micro commit they select the methods to include di erently.
this seems to have a real e ect on prediction quality as can be seen for both n divides.alt0mandm divides.alt0mquery scenarios.
such a di erence suggests that some methods are harder to predict than others.
a possible explanation is that these calls might be used very rarely and the recommender favors common method calls.
to explain this intuition let us go back to the example in figure and assume that m2is not a very typical method which makes it harder to predict.
in the linear case thetable e ects of evolving context new m d m d real .
.
.
.
.
real .
.
.
.
.
queries .
.
.
.
.
recommender is lucky because it gets m2as input and has to predict m1.
in the random case two queries are provided to the recommender.
it does really well in the easy query i.e.
when the input is m2 and really bad in the other one i.e.
when the input is m1 .
since the results of both queries are averaged the extreme e ect is smoothed out a little.
however for the real strategy the recommender gets m1 and has to predict the hard method m2.
it thus follows that it would have a lower prediction quality.
as opposed to arti cial strategies queries in the real strategy only include the methods that were actually included by the developer rst.
since the prediction quality of the real query is even lower this also suggests that developers add such hard to predict methods later to their code rather than earlier.
thus it seems that for such corner cases arti cial approaches tend to give a higher prediction quality than it would actually be the case in a real setting.
.
do real queries have properties not reflected in artificial queries?
released code is a static picture of the development activities.
any intermediate changes or removals cannot be identi ed by arti cial evaluations because they are part of the development process and do not show up in the released code.
our data set provides a unique opportunity to explore the impact of intermediate code changes and code removals on recommender evaluations.
to explore this we compare therealand real strategies in more detail.
the input to real queries might miss context information or might contain context information that is changed in the end state.
for pbn this includes both method calls that have been removed during development and changes to the context of an object usage.
the context is de ned by the enclosing method and the de nition site of an object.
by construction the enclosing method is xed for all query generation strategies because object usages are always bound to a speci c method.
if the enclosing method is changed this would count as a di erent object usage.
on the other hand the de nition site which describes how an object instance was created may change during development e.g.
see the de nition site in figure .
if there are no removals or changes in the de nition site the query will not contain any artifacts of anevolving context i.e.
changed or removed information.
arti cial selection strategies are unaware of such changes and are thus not sidetracked by them.
this results in a higher prediction quality than for a real query.
we examine the e ect of evolving context in more detail in table .
in addition to queries in an unchanged context the context may evolve in four di erent ways that are applicable here.
moving method calls to other enclosing methods is a fth kind.
however distinguishing these moves from removals is outside the scope of our work and not considered here.
newly created usage new the most represented category in our dataset is usages that are added from scratch.
the values are equal to table and only included for easier 118comparison.
the di erence between real and real here can be explained by a changed de nition site.
unchanged context we categorize queries into this category that do no include any changes in the surrounding context.
by design no di erence exists for real and real queries in this category.
retired method calls m this category refers to the case in which the query has calls in the input that are removed during development and no longer contained in the end state.
for example this can happen when the developer chooses a more appropriate method call to use.
we nd that this is the case for .
of the queries in our dataset.
our results suggest that these extra methods that appear in the input seem to confuse the recommender.
the quality decreases from for real to .
for real.
changed de nition d a small part of our dataset .
includes queries in which the de nition site changes between the input and end state.
this particular context change has a big impact on real evaluations.
table shows a quality drop from .
for real to .
for real.
combination m d the second largest category in our dataset are queries that combine both a change in de nition site as well as the removal of method calls.
it prevents any meaningful proposal in our experiment for the real approach and the quality drops to .
.
we nd that real evaluations are sensitive to context changes.
while retired method calls have a minor impact on the result of a real evaluation the change or absence of the de nition site leads to a large di erence in the result.
such an impact is not covered in the arti cial evaluation strategies.
while the speci c context information we discussed in this section removed calls and changed de nition sites may be speci c to pbn other recommender systems that use context information will su er from the same problem when this information evolves e.g.
changing the implemented interfaces removing method calls on related objects re ordering call sequences etc.
.
the general problem is that context information can change during development.
arti cial evaluations usually do not mimic context evolution which results in unrealistic quality reports.
.
discussion our results indicate that the focus of an evaluation is an important factor when determining which query scenario and selection strategy to use.
in the following we discuss broader implication of our results and implications for future work.
.
implications of results choice of query scenario.
depending on the type of users the recommender is meant to support researchers can decide on which query scenario they use in their evaluation.
while divides.alt0mrepresents green eld projects or novices that request help from the recommender before actually starting to write code n divides.alt0mre ects advanced programmers that write something before triggering code completion or maintenance tasks in which the programmer typically starts to edit existing methods.
the m divides.alt0mquery scenario can be used to demonstrate support for corner cases or that even experts that just miss the last bit can get valuable support.
choice of selection strategy.
our ground truth re ects developers typical code completion usage.
in our dataset divides.alt0mqueries are the common case for which no di erence existsfor the di erent selection strategies.
on the other hand we nd great di erences between the selection strategies for the uncommon cases that include the n divides.alt0mandm divides.alt0mquery scenarios.
we observed for these query scenarios that some methods seem to be harder to predict than others.
depending on the methods that are contained in the query the reported quality may vary.
the random selection strategy leads to a smoothing e ect that can overcome this e ect to some extent but it leads to a reported quality that is generally higher than for the other arti cial approaches.
overall by comparing the average results over all queries we could measure only minor di erences between the di erent arti cial selection strategies.
we could not nd a single arti cial evaluation that is more realistic than the others.
when compared to the result of real queries we found that all arti cial evaluations report a higher quality.
considering the effect of an evolving context.
missing or changing context information a ected many query cases.
we found that in real queries the evolving context has a negative e ect on the recommendation quality.
the more changed context features the recommender engine takes into consideration the bigger this negative e ect is because more confusing information is passed to the recommender.
researchers should be aware of this e ect in their evaluations.
while this does not necessarily invalidate the results of articial evaluations it results in more positive results than what would actually be measured from real developers.
existing arti cial evaluations remove only the target information i.e.
the method calls .
however to make an arti cial evaluation more realistic toolsmiths have to check their assumptions about the context information used in the query.
they have to identify what context information may change in reality and they should mimic that in their automatically generated queries.
for example they should remove additional context information e.g.
de nition sites in our case from the input of a speci c fraction of the queries or set it to a random but valid value e.g.
set an an arbitrary de nition site stored in the model for a fraction of the queries .
.
implications for future work further comparisons.
we designed our experiments to use a single recommender to compare arti cial and real evaluation techniques.
future experiments should compare articial and real evaluation techniques using the same dataset across multiple recommenders.
this allows investigating which recommenders are more resilient to evolving context for example.
improving recommenders.
we found that .
of the snapshots in our dataset were pure removals of method calls.
while it could be the case that these are artifacts of maintenance tasks we hypothesize that this could also be caused by going through a learning process on how to use a given api to solve a task at hand.
these removals are not considered in any rsse so far probably because they cannot be observed by statically analyzing code repositories.
future work should investigate these removals.
it seems that they contain information that could be leveraged to further improve existing approaches or to create a new kind of rsse that points the developer to methods that should be removed.
evaluation style.
our dataset consists of a series of source code snapshots as illustrated in figure .
the evaluation design we used in this paper follows what we refer to as a goal 119stepwisegoal orientedtimefigure identifying expected query results oriented style in which the validation queries are created from the intermediate states and the proposals are validated on the end state.
this follows the intuition that this nal state is the desired outcome of a development task and that the recommender should lead the developer towards this outcome.
it might be that a recommender that suggests such an end state increases productivity because it points the developer to methods she must use in the end.
however it might also be that the proposal hinders the learning process because it might be unexpected at that exact point in time.
we observed in our dataset that the path towards the nal state is rarely straight.
therefore a viable alternative could be to use a step wise evaluation style instead in which each subsequent change would be evaluated individually.
for example each snapshot could be used as a query and the subsequent snapshot formulates the respective expectations.
such an approach follows the intuition that an rsse is expected to propose what the developer thought was right at the time of query.
such a tool might support the learning process which could be bene cial especially for novice developers.
however experienced developers that know the api in question might be disturbed by incorrect proposals.
it is not clear which of these should recommenders be designed to do and accordingly which of the evaluation strategies is better or more realistic.
to investigate this we conducted a preliminary experiment that used each usage snapshot as a query and compared the proposals to both the next snapshot and to the nal snapshots.
we calculated the f1 measure for both scenarios and over all queries the stepwise approach resulted in an average of .
and the goal oriented style in an average of .
.
a mann whitney u test shows that the di erence between these two evaluation approaches is statistically signi cant p value .
therefore future work should investigate assumptions on the expectations used to evaluate recommender systems.
in other words we need to analyze whether rsses should propose thecorrect answer or the expected answer .
.
threats to v alidity this section discusses the potential threats to the validity of our work as well as our mitigation strategies.
construct validity.
the prediction quality range reported by pbn for this dataset is lower than prediction qualities usually obtained in the literature including our previous experience with pbn on java swt .
we plan to investigate this more qualitatively to nd out if this is a factor of language di erences c vs. java or a factor of the api types included in the ground truth data set.
however our goal is not to promote a particular recommender but rather to compare di erent evaluation strategies.
thus we do not believe that this impacts the validity of our results since any di erence between evaluation strategies would still be observed.
additionally to avoid confounding factors that may a ect our comparison we ensure that we have enough object usages to build reference models for each api type we are interested in.
this ensures that all api types have a fair chance of getting good predictions.
that said evenif the model for one api type does not have enough data the e ect will be the same across all evaluation strategies keeping our comparisons fair.
internal validity.
to decrease the possibility of implementation bugs as much possible we used an existing recommender system with its existing evaluation pipeline.
we implemented only those parts necessary for the new experiments and thoroughly tested them.
our code is publicly available on our artifact page.
our results also depend on the quality of the static analysis of the c code we use to extract the micro commits.
c is a real world programming language and the analyzed programs contain very complex expressions.
while we excessively tested the analysis in an extensive test suite it is possible that we may have missed corner cases.
external validity.
our results are based on the comparison of the di erent evaluation strategies for only one recommendation system for single object patterns pbn .
our results might not generalize to other recommenders that have a different notion of context that deal with multi object patterns or that propose complete code snippets.
since we use the same recommender across all evaluation strategies our results are valid and ensure non biased comparisons.
since this is a limitation of the recommender rather than our ground truth data set the same comparison can be repeated with additional recommenders.
however this might entail some engineering e ort to adapt each recommender to use the ground truth data set.
we cover various kinds of developers by collecting data from diverse groups.
we have data from professional developers open source developers researchers and students.
however we do not currently have information about the project types these developers worked on.
in the future we will capture more information about the type of the project e.g.
green eld or maintenance and the role of the developer e.g.
developer tester or integrator to get a better picture about the task the developer was working on.
.
conclusion in this paper we surveyed related work to identify the current state of the art of evaluation strategies.
we found that many existing approaches are based on arti cial evaluations.
our goal was to analyze whether these evaluations re ect real life usages.
we presented a concept for the comparison of di erent evaluation strategies and collected a ground truth data set that allowed us to conduct the comparison.
we analyzed how the results of a real evaluation relate to the arti cial results.
we showed that arti cial approaches report a misleading quality for the evaluation if they do not consider evolving context information and therefore provide more information in the query than what would be available in a real scenario.
we believe that arti cial evaluations can still work if context evolution is carefully emulated in the evaluation.
.