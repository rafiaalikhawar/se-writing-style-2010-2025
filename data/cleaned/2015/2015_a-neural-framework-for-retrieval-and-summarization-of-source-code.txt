a neural framework for retrieval and summarization of source code qingying chen minghui zhou qychen pku.edu.cn zhmh pku.edu.cn school of electronics engineering and computer science peking university key laboratory of high confidence software technologies ministry of education china abstract coderetrievalandsummarizationaretwotasksoftenemployedby softwaredeveloperstoreusecodethatspreadsoveronlinerepositories.
in this paper we present a neural framework that allows bidirectional mapping between source code andnatural language to improve these two tasks.
our framework bvae is designed to have two variational autoencoders vaes to model bimodal data c vae for source code and l vae for natural language.
both vaes are trained jointly to reconstruct their input as much as possiblewithregularizationthatcapturestheclosenessbetweenthe latentvariablesofcodeanddescription.bvaecouldlearnsemantic vector representations for both code and description and generate completelynewdescriptionsforarbitrarycodesnippets.wedesign twoinstancemodelsofbvaeforretrievalandsummarizationtasks respectivelyandevaluatetheirperformanceonabenchmarkwhich involvestwoprogramminglanguages c andsql.experiments demonstrate bvae s potential on the two tasks.
ccs concepts software and its engineering search based software engineering keywords code retrieval code summarization neural framework acm reference format qingyingchenandminghuizhou.
.aneuralframeworkforretrieval andsummarizationofsourcecode.in proceedingsofthe201833rdacm ieee international conference on automated software engineering ase september montpellier france.
acm new york ny usa 6pages.
introduction developingandmaintainingsoftwareisacostlyandcomplicated activity.
in particular the modern software engineering strongly corresponding author permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september montpellier france association for computing machinery.
acm isbn ... .
be familiar with.
developers tend to frequently resort to documentation and online resources to reuse code written by others and thereforecodereusebecomesacommonpractice.nonetheless this process can still be challenging.
on the one hand it is not easy for developers to locate quickly what they are looking for in the documentation on the other hand the documentation of a library may be deficient and online resources may not be well documented.
for example although billions of lines of code reside in onlinerepositories only a small fraction of them are paired with highqualitycomments orsummaries whichmakes suchcodedifficult to understand let alone to reuse.
this challenge motivates two tasks code retrieval givenonenaturallanguagequery retrieve themostrelevantcodesnippet sothatdeveloperscanquickly find code snippet that meets their needs.
code summarization given one code snippet generate a natural language description automatically to help developers comprehend the snippet.
for code retrieval there are two main challenges we want to address informationretrieval ir basedmethodsfailtomatch semantically relevant snippets in particular many snippets are not pairedwith naturallanguage descriptions.how tobridgethe gap between the high level intent of queries and low level implementa tionofsourcecode?
existingapproachessuchascode nn requiresequentialscanoverallcandidatecodesnippets whichis inefficient when retrieving on large scale datasets.
toaddressthechallenges wepresentaneuralframeworkthat can project natural language and source code into a common semanticspace.theframework termedasbimodalvariationalautoencoder bvae iscomposedoftwovariationalautoencoders vae .
vae assumes a latent variable that can be interpreted as a high level semantic feature of the observed data .
by building the connection between the latent variables of natural language and source code bvae bridges their semantic gap and allows bidirectional mapping between them.
specifically bvae can learn semantic vector representations for bothcodesnippetsandqueriesandretrievesnippetsthatarenot paired with descriptions.
owing to this feature vector representations of snippets can be first computed and indexed offline.
at the onlineretrievalstage afterthevectorrepresentationofthequery iscomputed similarity searchalgorithms suchasbest bin first va file and ffva can be applied to retrieve the most relevant snippet without sequential scan.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france qingying chen and minghui zhou asvaehasprovedtobeeffectiveatgeneratingnaturallanguage bvae can also be applied to code summarization and generate completely new descriptions for arbitrary code snippets.
wedesigntwoinstancemodelsofbvaeforcoderetrievaland code summarization respectively and evaluate their performance on a recently introduced benchmark which involves two programming languages c and sql.
compared with previous best approach bvae leads to a significant improvement for retrieval taskandachievescomparableperformanceforsummarizationtask.
theseresults demonstratebvae s potentialon retrievalandsummarization of source code.
related work code retrieval is an active research topic in both natural language processing and software engineering communities.
some work focusedonretrievingcodesnippetsgivencodetokens which is not applicable if developers have no idea which libraries or apis cansatisfytheirneeds.themostrelevantstudiestooursarethat ofallamanisetal.
andiyeretal.
whichfocusonretrieving snippetsgivennaturallanguagequery.allamanisetal.
proposed amodelthatrepresents sourcecodeasatre e.however becausethis modelconditionssourcecodeonnaturallanguage itcannotbeusedforcodesummarization.iyeretal.
proposedanattention based sequence to sequencemodel termedascode nn whichperforms muchbetterthanthemodelofallamanisetal.andsupportsboth coderetrievalandcodesummarization.code nnconsumessource code tokens and generates words step by step.
however at the onlineretrievalstage code nn aswellasthemodelofallamanis et al.
requires sequential scan over all candidate code snippets given one query due to its architectural design and therefore is inefficient for largescale retrieval.
our approach proposed inthis paper addresses this problem.
there has been substantial effort devoted to code summarization and other similar tasks that predict natural language basedonsourcecode.someworktriedtogeneratesummariesforjava classes such as the template based approach the topic and ngramsmodels .giriprasadetal.
presentedanapproachthat generatessummariesforjavamethodsautomaticallyusingcontent selection and template based phrase generation and mcburney et al.
followed this line of work.
sridhara et al.
presented an automaticrule basedpipelineforidentifyingcodefragmentsthat implement high level abstractions of actions within java methods and expressing them as a natural language description.
koutrika et al.
presentedatemplatebasedapproachthatcantranslatesql queries to textual explanations.
ngonga ngomo et al.
focused ontranslatingsparqlqueriesintonaturallanguageusingrulesto processqueries.thesestudieseitherfocusonaspecificprogramminglanguageorrequirecostlymanualeffort.differentfromthese studies we attempt to generate completely new descriptions forarbitrary code snippets using machine learning techniques.
the mostrelevantworkiscode nn whichisaneuralmodelas described earlier.
bythesubmissionduedateofthecamera readyversion there is some recent work on the two tasks.
gu et al.
used two neural modelstojointlyembedcodesnippetsanddescriptionsintoavector spaceforretrievaltask.forsummarizationtask huetal.
focusedonrepresentingsourcecodebetterbeforefeedingitinto neural models which is orthogonal to our work.
the proposed framework bvae bvae is a variant of vae .
in this section we first review vae and then present bvae.
after that we introduce how the tasksofcoderetrievalandcodesummarizationareaccomplished based on bvae.
.
vae vae is a generative model and can be considered as a regularized versionofstandardautoencoder ae .anaealwaysconsistsoftwo parts the encoder and the decoder.
the encoder maps the input xto a hidden code z while the decoder takes zas its input and reconstructs xasmuchaspossible.insteadofusingadeterministic function as the encoder in ae vae imposes a prior distribution p z over the latent variable zand learns an approximate posterior distribution q z x over zconditioned on the observed variable x where arevariationalparameters.intuitively theencoderofvae maps xto a region rather than a single point in the latent space which makes vaemore effective at extracting for global semantic features than the standard ae .
from the perspective of vae the latent variable zis a high level representation of the observed data x. usually thestandardgaussian equation andthediagonal gaussian equation are used as the priordistribution p z and the posterior distribution q z x respectively p z n z q z x n z x diag 2 x where and 2are the mean and variance vector.
the goal of training vae is to maximize the lower bound on the true log likelihood of x or to minimize its opposite loss function l x eq z x kl q z x p z r x kl q z x p z where aretheparametersofgenerativemodel i.e.decoder and kl stands for kullback leibler divergence1.
the loss function in equation can be considered as the reconstruction loss rplus a regularization term.
in the experiments presentedinthispaper weestimatethereconstructionlossusinga single sample from q z x with reparameterization trick and use the standard gaussian and the diagonal gaussian for the prior andposteriordistributionof z suchthatthekldivergencetermin equation can be computed in closed form.
.
bvae bvae is composed of two vaes c vae for source code and l vae for natural language.
figure 1depicts the bvae framework.
all components in the framework are used at training time.
at test time the components inside blue dashed shape are used for code retrieval and those inside orange dashed shape for code summarization.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a neural framework for retrieval and summarization of source code ase september montpellier france encoder of c vae decoder of c vae encoder of l vae decoder of l vae figure illustration of bvae framework.
thecoreideaofbvaeisthatgivenapairofsourcecodesnippet andnaturallanguagedescription theirlatentvariablesshouldbe closetoeachother.toachievethat bvaeremovesthekldivergencebetweentheposteriorandpriordistributionforbothvaes and instead adds another regularization to capture the closeness betweenthetwolatentvariables.specifically weproposetwokinds of closeness regularization posterior regularization preg foreachofthetwovaes we use the kl divergence of the posterior distribution from that of the other vae.
the loss function of bvae denoted by lb takes the following form lb c c l l xc xl lc c c l l xc xl ll c c l l xc xl lc c c l l xc xl c rc c c xc c kl q c zc xc q l zl xl ll c c l l xc xl l rl l l xl l kl q l zl xl q c zc xc where c c l l and the subscript candlstand for c vae and l vae respectively.
mean posterior regularization mpreg different from preg mpreg forces the posterior distributions of both vaes to approach the mean posterior qm zm xc xl i.e.
lc c c l l xc xl c rc c c xc c kl q c zc xc qm zm xc xl ll c c l l xc xl l rl l l xl l kl q l zl xl qm zm xc xl qm zm xc xl n zm c xc l xl diag 2c xc 2 l xl .
byapplyingoneofthetworegularizationtechniques theconnection can be built between the originally isolated c vae and l vae.
specifically given a pair of code snippet and natural language description c vae taking code as input and l vae taking description as input would obtain close latent variables.
.
bvae for code retrieval candidate networks for the encoder and decoder of bvae include multilayerperceptron mlp convolutionalneuralnetwork cnn recurrent neural network rnn etc.
as a proof of concept im plementation we use the simplest i.e.
mlp as the encoder anddecoderfor retrievaltask since morepowerful modelsareharder to train when vae is applied to text .
weusethebag of wordsassumptionforbothsourcecodeand natural language.
we design the same architectures for c vae and l vae.
given a sequence of tokens code or natural language embeddingsoftokensarefirstaveraged followedbya tanhactivation.
then severalfully connectedlayersareappliedtopredicttheposterior distribution q z x .
formally the encoder takes the following form h tanh tt summationdisplay.
t 1et h e fcle h w h e b diag exp w h e b wheretis the number of tokens in the given code snippet cor naturallanguage query l etistheembedding ofthe tthtokenwt fcrepresentsthefully connectedlayerwith tanhactivationand le is the number of fclayers.
it means identity function when leis .
the decoder generates each token independently conditioned onz.
formally it takes the following form z s n z diag 2 h d fcld z s h o softmax w o h d b o whereldisthenumberoffclayersinthedecoder and h o r v v is the number of unique tokens contains the probability of each token conditioned on the latent variable z. the reconstruction loss is r x t summationdisplay.
t logp wt z wherep wt z is gathered from h o .
in order to force c vae and l vae to encode more important information in the latent variables we find it beneficial to use the inverse document frequency idf as weight in the reconstruction loss i.e.
r x t summationdisplay.
t idf wt logp wt z summationdisplay.
w w tf w idf w logp w z wherewis the set of unique tokens in given corl andtfdenotestermfrequency tf .idfofeachtokeniscomputedbasedon training set.
after the training process of bvae finishes the mean cfor the posterior distribution q c zc xc of the snippet can be computedoff lineforarbitrarycodesnippet c.givenanaturallanguage queryl itsmean lfortheposteriordistribution q l zl xl should be first calculated and then similarity search algorithms in highdimensionalspacescanbeappliedtoretrievethehighlyrelevant code snippets.
otherthan sequentialscan manysimilarity search algorithms suchasbest bin first va file andffva havebeen authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france qingying chen and minghui zhou figure the decoder of l vae for code summarization at training time.
dashed lines are used to indicate that the in put dropout technique is applied here.
proposed andcanbeappliedheretocoderetrieval.clearly compared with models that require sequential scan bvae is more suitableforefficientretrievalonlargescaledatasets.further bvae supportsretrievingcodesnippetsthatarenotpairedwithnatural language descriptions.
this feature allows leveraging vast amount of code available in open source repositories on github.
while conducting the experiments for simplicity we use cosine similaritybetween land c vectorrepresentationofqueryand snippet to rank candidate code snippets.
.
bvae for code summarization for code summarization task we use the same architecture as that forcoderetrieval exceptthatthedecoderofl vaeisanrnn in order to generate a sequence of words.
figure2depicts the architecture of rnn decoder of l vae at training time.
specifically we use gated recurrent unit gru as the rnn cell.
the latent variable is used as the initial state of rnnafteranfctransformation.theinputofthedecoderiswords from golden text.
in order to force the decoder to rely more on the latent variable we adopt the input dropout technique which randomlyreplacessomefractionofinputswiththeunknowntoken unk .
the target words are that of original text shifted left by one word.
the reconstruction loss is computed by rl l l xl t summationdisplay.
t logp l wt zl w1 w2 wt givenonespecificcodesnippet c its cofq c zc xc shouldbe firstcomputedandthenfed intothedecoderofl vae whichcan generate new text from latent space.
the output of l vae is taken asthesummaryofthegivencodesnippet c.intuitively thedecoder ofl vaegeneratesthesummaryfromthelatentvariableofcode which is inferred by the encoder of c vae.
different from training process at test time the input of one rnn step is the word generated in its previous step.
beam search2 is employed here to help generate a better summary.
.
implementation details .
.
source code tokenization.
we use modified versions of an antlr parser for c and python sqlparse for sql to tokenize sourcecode.inthetokenizationprocess commentsareremoved names of tables and columns are replaced with numbered placeholder tokens while preserving any dependencies in the query for example sql query select max score from student score is turned into select max col0 from tab0 .
for c sql and natural language tokens occurring less than times in training set are replaced with their corresponding unk token.
.
.
hyperparameters.
wesetalltheembeddingsizesofc sqlandnaturallanguageto256andlatentsizeto128.aftertryingsomeexplorativeexperiments wefindthatmpregperforms slightly better than preg on code summarization task for sql and on code retrieval for both c and sql while preg can achieve better performance on code summarization for c than mpreg.
therefore weusempregforallexperimentsexceptthatofcode summarization for c .
idf is used to weigh the reconstruction lossofc vaeandl vaeonlyforsqldataset sincewefindthereisno significant improvement for c dataset after idf is used.
c c l and lare set to .
.
.
and .
respectively.
for code retrieval both leandldaresetto1 andhiddensizeinfclayeris .
for code summarization leandldare set to gru state size is256 thekeepprobabilityofinputdropoutis0.
thebeamsearch size is and the maximum summary length when decoding is .
alltrainable parametersareinitialized withxavierinitialization .
we use adam optimizer to train the models with batch size of start with a learning rate of .
and decay it by a factor of .
after epochs if the loss on training set does not decrease in allrecent epochs.
when the model performance on dev set does not increase in a certain number of epochs we save the model and evaluate its performance on eval set.
experiment setup .
dataset thedatasetforexperimentswascollectedfromstackoverflow so by iyer et al.
.
this dataset contains pairs of c code naturallanguagedescription fortraining 6601pairsforvalidation and pairs for test.
for sql these numbers are and3340.these pairswereextractedfrompostsofso wherethe title of a post was taken as the natural language description and the code snippet in some accepted answer was considered as an implementation of the corresponding title.
to get a high quality dataset iyer et al.
trained a semi supervised classifier to filter out bad posts that have no relation to the corresponding code snippet.
for code summarization task a given code snippet can be described in many different ways with little overlapping content between them which may make automatic evaluation less reliable.
to mitigate this issue iyer et al.
provided two additional human annotated titles for each of code snippets randomly chosenfromthetestset.halfofthe200codesnippetswasusedfor model tuning dev set and the rest for evaluation eval set .
.
code retrieval .
.
baseline models.
ret ir candidate code snippets are rankedaccordingtocosinesimilaritybetweenthequeryandnaturallanguagedescriptionsthatareassociatedwithcodesnippets.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a neural framework for retrieval and summarization of source code ase september montpellier france herein tf idfweightsareusedasfeaturestocomputecosinesimilarity.thismodelcanonlyretrievecodesnippetsthatarepaired with descriptions.
msae msae is a multi modal retrieval model based on stackedautoencoders ae .itsoverallframeworkissimilartobvae butitconsistsofaesandhasnolatentvariables.msaeseeksto ensure the closeness euclidean squared distance between the hidden states of two aes.
we design the same encoders and decoders of msae as those of bvae see section .
.
code nn prior to our study code nn achieves thestate of the art performance on the same dataset for both tasks.
it employs an attention based sequence to sequence architecture to modeltheprobabilityofnaturallanguageconditionedonsource code.
source code is tokenized to a sequence and then fed into a long short term memory lstm encoder.
the decoder also an lstm network computes the distribution of word step by step guided by a global attention mechanism .
.
.
evaluation method.
for code snippet ciin dev or eval one of the two human annotated titles is taken as the query while thecollecteddescriptionassociatedwith ciisusedforcalculation of cosine similarity in ret ir and not used in both code nn and our model.
forty nine snippets randomly chosen from the test set areusedasdistractors and alongwiththegoldenone ci comprise the retrieval set.
all the snippets are ranked by a specific model and the rank of the golden snippet ci denoted by ri i su s e dt o computethemeanreciprocalrank mrr .werepeat20timesthe procedure for different random distractor snippets and compute the mean with .
confidence interval of the mrr.
.
code summarization .
.
baseline models.
ir given snippet c retrieve csmost similar to cin terms of token levenshtein distance and take the description of csas the summary of c. moses a phrase based machine translation system.
sum nn aneuralattentionmodelforabstractivesentence summarizationwhichemploysanattention basedencoder decoder architecturethatgeneratesnextwordconditionedonafixedwindow of previously generated words.
msae msae focuses only on retrieval task originally.
we extend it to summarization task in the same way we extend bvae.
code nn anattention basedsequence to sequencemodel asdescribedinsection .
.
.code nnalsousesbeamsearchto generate summary with the beam size of and the maximum summary length of .
we use the same value for the two hyperparameters in our model.
.
.
evaluation method.
there are three reference texts for each code snippet in dev and eval.
we evaluate the generated textbycomputingbleu 4score andmeteorscore against the reference texts.
both metrics are commonly used for automatic evaluation of machine translation.
bleu measures the average n gram precision with length penalty for short texts.
meteor computes a matching score using a combination of unigram precision unigram recall and a measure of fragmentation that is designed to directly capture how well orderedthematchedwordsinthegeneratedtextsareinrelation results for code retrieval task.
results on dev set are in parentheses.
model mrrc ret ir .
.
.
.
msae .
.
.
.
code nn .
.
.
.
bvae .
.
.
.
sqlret ir .
.
.
.
msae .
.
.
.
code nn .
.
.
.
bvae .
.
.
.
table results for code summarization task.
results on dev set are in parentheses.
model bleu meteor c ir .
.
.
.
moses .
.
.
.
sum nn .
.
.
.
msae .
.
.
.
code nn .
.
.
.
bvae .
.
.
.
sqlir .
.
.
.
moses .
.
.
.
sum nn .
.
.
.
msae .
.
.
.
code nn .
.
.
.
bvae .
.
.
.
to thereference texts.
inthe experiments weuse meteor asthe mainmetric i.e.reportbleu 4scorewhenthemodelachievesbest meteor score on dev set.
results table1shows the results for code retrieval task.
for c our model outperforms all baselines by a large margin.
for sql our model is somewhatworsethancode nnondevset butismuchbetter on eval set.
table2showstheresultsforcodesummarizationtask.forc ourmodeloutperformsallbaselinesonbothmetrics exceptcodenn on meteor score on dev set.
for sql our model achievesbetter performance other than code nn but its bleu scores arelowerthanours.throughmanualinspection wefindthatthe summariesgeneratedbycode nnareusuallylongerthanthose generated by bvae.
bvae performs much better than msae.
we can conclude that incorporating latent variables benefits both tasks.
to sum up bvae leads to a significant improvement for retrieval task.
for summarization task bvae achieves comparableperformance to previous best model code nn although with out significant improvement more powerful structure for latent variables diagonal gaussian now along with better encoders and decoderscanbedesignedinthefuturetoimprovethetask since authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france qingying chen and minghui zhou bvae is a framework and current results have demonstrated its potential.
threats to validity internal validity the dataset for experiments were extracted from posts of stackoverflow therefore its generation process wasnotexactlythesameasthewaydevelopersretrieveorsummarize snippets in the real world.
this threat was mitigated by two efforts.first asemi supervisedclassifierwastrainedtofilterout bad posts that have no relation to the corresponding code snippet.
second two additional human annotated descriptions were provided for each test snippet.
forretrievaltask theretrievalsetconsistsofonegoldensnippet and forty nine randomly chosen distractors.
although this setting differs from the real world it is effective to demonstrate if a model outperforms another one.
for summarization task we use bleu and meteor scores to evaluate the quality of summaries automatically.
although the two metricsarecommonlyusedinthefieldofmachinetranslation it would be better to perform another human evaluation process.
external validity could the results be generalized to other programming languages?
theoretically bvae can be applied to anyprogramminglanguage.weevaluatebvaeonc ageneralpurposeimperativelanguage andsql adeclarativelanguagefor queryingdatabases.theexperimentresultsdemonstratebvae s potential on both languages.
conclusion and future work in this paper we present a neural framework termed as bvae for retrieval and summarization of source code.
we design two instancesofbvaeforretrievalandsummarizationrespectively and evaluate their performance on a benchmark involving c and sql.
comparedwithpreviousbestapproach bvaeleadstoasignificant improvementforretrievalandachievescomparableperformance for summarization task.
the results demonstrate bvae s potential on the two tasks.
in future work we plan to employ morepowerful structure for latentvariablesanddesignbetterencodersanddecodersforbothcvae and l vae.
besides since bvae allows bidirectional mapping between source code and natural language we will attempt to extend it to code generation task.