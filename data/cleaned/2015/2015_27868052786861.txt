systematic testing of asynchronous reactive systems ankush desai university of california berkeley usa.shaz qadeer microsoft research redmond usa.sanjit seshia university of california berkeley usa.
abstract we introduce the concept of a delaying explorer with the goal of performing prioritized exploration of the behaviors of an asynchronous reactive program.
a delaying explorer strati es the search space using a custom strategy and a delay operation that allows deviation from that strategy.
we show that prioritized search with a delaying explorer performs signi cantly better than existing prioritization techniques.
we also demonstrate empirically the need for writing di erent delaying explorers for scalable systematic testing and hence present a exible delaying explorer interface.
we introduce two new techniques to improve the scalability of search based on delaying explorers.
first we present an algorithm for strati ed exhaustive search and use e cient state caching to avoid redundant exploration of schedules.
we provide soundness and termination guarantees for our algorithm.
second for the cases where the state of the system cannot be captured or there are resource constraints we present an algorithm to randomly sample any execution from the strati ed search space.
this algorithm guarantees that any such execution that requires ddelay operations is sampled with probability at least ld wherelis the maximum number of program steps.
we have implemented our algorithms and evaluated them on a collection of real world fault tolerant distributed protocols.
categories and subject descriptors d. .
software program veri cation d. .
testing and debugging general terms algorithms reliability veri cation keywords systematic testing model checking asynchronous programs distributed systems random sampling1.
introduction asynchronous reactive systems are ubiquitous across domains like distributed systems device drivers web applications and operating systems.
processes in these systems communicate by exchanging messages asynchronously and react to environment input continuously.
the system reliability depends critically on correct handling of asynchrony and reactivity using stateful protocols.
testing and debugging of these systems is notoriously di cult due to the nondeterministic nature of their computation an error could result from a combination of some choice of inputs and some interleaving of event handlers.
this paper is concerned with the problem of systematic testing of such such systems by automatically enumerating all sources of nondeterminism both from environment input and from scheduling of concurrent processes.
the main challenge in scaling systematic testing to realworld programs is the large number of behaviors that explode exponentially with the number of steps in the program.
techniques such as state caching and partialorder reduction have been developed to combat this explosion yet their worst case complexity remains exponential.
in practice the search often takes too long and has to be terminated because of a time bound thereby giving no information to the programmer.
therefore researchers have been motivated to investigate prioritized search techniques both deterministic and randomized to provide partial coverage information.
however all of these techniques have been developed for shared memory multithreaded programs.
in asynchronous reactive programs the primary mechanism for communication among concurrent processes is messagepassing rather than shared memory.
we have discovered empirically section that prioritization techniques developed for multithreaded programs are not e ective when applied to message passing programs.
in this paper we introduce a new technique for systematic testing of asynchronous reactive programs.
our technique is inspired by the notion of a delaying scheduler for multithreaded programs.
a delaying scheduler is a deterministic thread scheduler equipped with a delay operation whose invocation changes the default scheduling strategy.
for asynchronous reactive programs we generalize this notion to a delaying explorer of allnondeterministic choices section both from input and from the interleaving of event handlers.
the key observation that makes a delaying explorer suitable for systematic testing is that every execution can be permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august september bergamo italy c acm.
... .
db 1db 2db db 4db db db d1 d2figure strati cation using delaying explorers produced by introducing a nite number of delays in the default deterministic execution prescribed by the explorer.
we show that appropriately designed delaying explorers are signi cantly better than existing prioritization techniques in searching for errors in executions of asynchronous messagepassing systems.
a delaying explorer induces strati cation in the search space of all executions.
a stratum is the set of executions that require the same number of delays.
figure represents the strati cation pictorially db is the set of executions with one delay db is the set of executions with two delays and so on.
a delaying explorer speci es a prioritized search that explores these strata in order.
since the number of possible executions increases exponentially with the delay budget exploration for high budget values becomes prohibitively expensive.
therefore a delaying explorer is effective only if bugs are uncovered at low values of the delay budget.
figure shows the strati cation induced by two di erent delaying explorers.
the explorer d2is more e ective thand1at discovering a particular bug if that bug lies in a lower stratum for d2than ford1.
the di erence in strati cation induced by di erent delaying explorers has practical consequences.
we have observed empirically that there is considerable variance in the speed of detecting errors across di erent delaying explorers for di erent test problems1.
motivated by this observation we have designed a general delaying explorer interface that helps programmers quickly write custom search strategies in a small amount of code typically less than loc.
delaying explorers also provides developers and testers with a simple and elegant mechanism to express domain speci c knowledge regarding parts of the search space to prioritize.
we have written several delaying explorers using our framework and used them to nd bugs in implementations of distributed protocols that could not be discovered using any other method.
we describe a particular case study in section .
given a delaying explorer we need techniques for e ectively exploring the strata induced by the explorer.
in this paper we also present two algorithms strati ed exhaustive search ses and strati ed sampling ss for solving this problem.
ses performs strati ed search by iteratively incrementing the delay budget and exhaustively enumerating all schedules that can be explored with a given delay budget.
1a test problem is the combination of a program and a speci cation.inspired by model checking techniques we incorporate state caching to avoid redundant exploration of schedules.
by caching the states visited along an execution we can prune the search if an execution generated subsequently leads to a state in the cache.
incorporating state caching in delaying exploration is nontrivial because search is performed over executions of the composition of the program and the delaying explorer both reading and updating their private state in each step of the execution.
the naive strategy of caching the product of the program and the explorer state does not work because the delaying explorer can be an arbitrary program with a huge state space of its own.
instead our algorithm caches only the program state yet guarantees that in the limit of increasing delay budgets all executions of the program are covered.
our evaluation shows that ses nds bugs orders of magnitude faster than prior prioritization techniques on our benchmarks section .
even though state caching is an important optimization it is not a panacea to the explosion inherent in systematic testing.
the complexity of the algorithm mentioned in the previous paragraph still grows exponentially with the number of allowed delays.
consequently if a delaying explorer is unable to nd a bug quickly within a few delays the search must be stopped because of the external time bound.
to further scale search to large delay budgets we present the ssalgorithm which performs strati ed sampling of the search space with probabilistic guarantees.
our algorithm guarantees that any execution that is visited with db delays is sampled with probability at least ldb wherel is the maximum number of program steps.
ssis useful because it allows even distribution of the limited time resource over the entire search space.
furthermore since each sample is generated independently of every other sample random exploration can be easily and e ciently parallelized or distributed.
finally for some systems state caching may not be possible because of the di culty of taking a snapshot of the entire system state.
in this situation search based on random sampling could be very useful.
we empirically show section that on our benchmarks sscan nd bugs faster often by an order of magnitude compared to the prior best technique for random sampling of executions of multithreaded programs.
we have implemented our framework and algorithms for systematic testing of applications written in p a domainspeci c language for asynchronous event driven programming currently used for developing device drivers and distributed services.
the pcompiler generates both executable ccode and zing code which can be explored with the zing model checker.
the generated ccode is used for executing the application either locally on a single computing node or distributed across a collection of nodes.
we have implemented in pa fault tolerant transaction management system tms that internally comprises many protocols such as two phase commit multi paxos and chain replication .
using our test framework we found many bugs caused by protocol level race conditions in our implementation of tms.
the suite of protocols in tms form the benchmark set for evaluating our algorithms.
we note that our techniques are not limited to the planguage.
they generalize to any programming system with 74two properties ability to create executable models of the execution environment of a program and control over all sources of nondeterminism in program semantics.
we conclude this section by summarizing our contributions .
we introduce delaying explorers as a foundation for systematic testing of asynchronous reactive programs.
we empirically demonstrate that for the domain of messagepassing programs delaying explorers are better often by an order of magnitude than existing prioritization techniques.
.
we observe that the e cacy of a delaying explorer depends on the test problem.
to enable programmers to easily write custom explorers we have created a exible interface for specifying explorers.
we have written four delaying explorers each in less than loc using our interface.
.
we present the ses algorithm that uses state caching for e ciency while prioritizing search using a delaying explorer.
the algorithm guarantees soundness even without caching the state of the delaying explorer.
.
we present the ssalgorithm to e ciently sample executions with a xed number of delays.
our algorithm guarantees that if a buggy execution exists with dbdelays for a given delaying explorer then each sample triggers the bug with probability at least ldbwherelis the maximum number of steps in the program.
.
delaying explorers in this section we provide intuition for delaying explorers and their use in systematic testing of asynchronous reactive systems.
we begin by formally stating our model of programs and explorers.
a programpis a tuple s cid t s .sis the set of states of p. .cidis a nite set of nondeterministic choices that pcan make during execution.
this set includes both choices due to scheduling of concurrent processes in pand choices due to nondeterministic input received by each process.
.t2cid s s is the transition function of p. if s0 t c s we say that s s0 is a transition of p. we de ne choices s fcj9s0 t c s s0g.
.s0is the initial state of p. a sequence of states s0 s1 s2 s nis an execution ofpif si si is a transition of pfor alli2 n .
a states2sis reachable if it is the nal state of some execution.
an in nite sequence of states s0 s1 s2 is an in nite execution ofpif si si is a transition of pfor alli .
we assume that p isterminating i.e.
it does not have any in nite executions.
the formalization of the nondeterministic transition graph of an asynchronous reactive program is standard in the literature it is depicted pictorially in figure .
the exploration algorithms popularized by model checking tools e.g.
spin view the transitions coming out of a state as unordered the order in which those transitions are explored is considered an implementation level detail.
a delaying s0 s2 s1 s3 s5 s4 s6s7 s8 s9 s10s11 s12figure a concurent program next delay next delay next delays1 d1 0s0 d0 s0 d2 s4 d3 s1 d4 s2 d5 0s0 d6 next s3 d11 0next delay s5 d7 s1 d8 2next delay s7 d9 s2 d10 figure a concurrent program composed with a delaying explorer explorer formalized below instead considers the order of transitions an important concern for e cient exploration.
it provides a general interface for specifying this order based on the entire history of the program execution.
a delaying explorer dis a tuple d next step delay d0 .dis the set of states of d. the state of the explorer typically includes a data structure e.g.
stack or queue to maintain an ordering among the choices available to the program.
.next2d!cidis a total function.
given a explorer stated the choice next d is prescribed by the explorer to be taken next.
.step2s d!dis a total function.
suppose we have a program state sand a explorer state d and we execute the choice next d ats.
then step s d yields the explorer state corresponding to the program state t next d s .
the step function enables building explorers which change their state in response to speci c events that occur during execution of the program such as sending or receiving of messages creation of new processes etc.
.delay2d!dis a total function.
given a explorer stated the application delay d yields a new explorer state.
the delay function provides a mechanism to change the next choice to be explored.
.d0is the initial state of d. consider a delaying explorer that attemts to order the outgoing transitions of each state left to right for the program in figure .
the unfolding of the nondeterminism in this program as controlled by such a delaying explorer is shown in figure .
we formalize and explain the intuition behind this gure below.
let p d denote the composition of a program pand a delaying explorerd.
a state of p d is a triple s d n where 75sis the state ofp dis the state ofd andnis the number of consecutive delay operations applied in state s. a nite sequence s0 d0 n0 x0 !
s1 d1 n1 x1 !
s2 d2 n2 x2 !
is an execution of p d if for alli either xi next ni t next di si si and step si di di or xi delay ni ni ni jchoices s j si si and delay di di .
in this execution a transitionnext !is anext transition anddelay !is adelay transition.
in figure each state has exactly these two outgoing transitions.
a triple s d n is a reachable state of p d if it occurs on an execution.
a db delay execution of p d is one in which the number of delay transitions is db.
thus a delaying explorer dinduces a strati cation of the executions of a programpsuch that the i th stratum contains exactly the set ofi delay executions.
in order to ensure that all behaviors are covered the delaying explorer must ensure that all nondeterministic choices from a state are generated by successive applications of delay .
to formalize this requirement we de ne delayk fork inductively as delay0 d d delayk d delay delayk d and nextk fork inductively as next0 d fg nextk d nextk d fnext delayk d g a delaying explorer dissound with respect to a program p ifchoices s nextjchoices s j d for every reachable state s d of p d .
this property states that all nondeterministic choices in a state are covered through iterative application of the delay operation composed with next .
in figure all successors s1throughs3 of states0are reachable via at most two invocations of delay .
this property guarantees theorem that reachability analysis on p d is equivalent to reachability analysis on p. theorem .consider a program pand a delaying explorerdthat is sound with respect to p. a statesis reachable inpi s d is reachable in p d for somed.
example let us consider a simple program in which the only source of nondeterminism is the scheduling of concurrent processes.
an example of a delaying explorer for this program is a round robin process scheduler.
the state dof this scheduler is a queue of process ids initialized to contain the id of the initial process.
next returns the process id at the head of the queue.
step instruments the program s execution so that the id of a new process is added to the tail the id of a terminated process is removed and the id of a blocked process is moved to the tail.
delay moves the process id at the head to the tail.
this explorer maintains the invariant that the ids of all enabled processes are present in the queue.
by applying the delay operation at most n times where nis the size of the queue any enabled process can be moved to the head and be returned by a subsequent call to next .
therefore this explorer is sound with respect to the program.
l l l dbexecution with delay executions with delay executions with delaysfigure strati ed exhaustive search .
stratified exhaustive search figure shows a pictorial representation of strati ed exhaustive search of a program with respect to a delaying explorer.
in this picture lis the maximum number of steps in the program.
in contrast to the graphs in figures and where a node represents the program state each node in figure is a complete execution of the program.
the root node is the execution with no delays.
this execution presents at mostlpositions to insert a delay operation each yielding another complete execution with a single delay operation.
these executions are indicated by the nodes at the end of the edges coming out of the root node.
this process can be continued until all executions have been generated.
it is clear that there can be at most ldbexecutions with no more than dbdelays.
thus for small values of db it is feasible to enumerate all executions even for large values of l. this observation suggests our strati ed exhaustive search algorithm ses which generates executions level by level exploring all executions in a level before moving to the next level.
a delaying explorer induces a strati cation of the executions of a program in general di erent delaying explorers induce di erent strati cation for the same program.
thus a delaying explorer is a mechanism to bias the search performed by ourses algorithm to di erent parts of the execution space.
the algorithm in figure takes as input a program p a delaying explorer d and a parameter .
it uses three global variables.
the integer db initialized to and iteratively incremented by contains the current delay bound.
during the search a frontier of pending executions that go beyond the current delay bound is maintained in the dictionary frontier .
for each state sin the frontier frontier contains a pair d i wheredis the explorer state just prior to the the execution of i th transition from state s. the mapping from sto d i is put into the frontier because execution of the i th transition would require more delays than the current bound.
finally we optimize the search by using a cache of hashes of visited states maintained in the set cache .
the workhorse of our algorithm is delayboundeddfs a procedure with four parameters program state s explorer stated transition count i and delay count n. the goal of delayboundeddfs is to continue exploration from state s. the transition count iis the number of transitions already explored from s. the delay count nis the number of delays required starting from the initial state to execute the next transition out of s.delayboundeddfs iterates through the transitions from sby repeatedly invoking the next operation of the delaying explorer to nd out which transition to execute and incrementing ito indicate the execution of another transition.
for each discovered state s0 ifs0is not present in 76vardb n varfrontier dictionaryhs d n i varcache sethsi delayboundeddfs s s d d i n n n f vars0 s while i jchoices s j f s0 i t next d s i if s062cache f cache add s0 delayboundeddfs s0 step s d n g if n db i jchoices s j f frontier s d i break g d n delay s d n g g ses f vardb0 n varfrontier0 dictionaryhs d n i db frontier cache cache add s0 delayboundeddfs s0 d0 while frontier6 f frontier0 frontier frontier db0 db db db foreach s d i 2frontier0 delayboundeddfs s delay s d i db0 g g figure ses algorithm strati ed exhaustive search cache then it is added to cache and delayboundeddfs is called recursively on s0.
to move to the next transition the delay operation of the delaying explorer needs to invoked.
if the current delay count nhas already reached the current delay bound dband there is at least one more transition to be executed then exploration cannot continue from sand work for the remainder of exploration from sis added to the frontier.
otherwise the delay operation is used to update dand the delay count nis incremented.
the top level procedure of our algorithm is ses.
this procedure initializes dbto and frontier and cache to .
it then executes two nested loops.
the outer loop iterates over the value of dbincrementing it by each time around.
the goal of each iteration of this loop is to restart each pending exploration in the current frontier.
to do this task a copy of frontier is made in frontier0and frontier is reset to .
the inner loop then picks each work item in frontier0 and invokes delayboundeddfs with it.
the execution of the inner loop re lls frontier which is again emptied in the next iteration of the outer loop.
theorem formalizes the correctness of the ses algorithm.
theorem .consider a program pand a delaying explorerdthat is sound with respect to p. the ses algorithm figure terminates and visits a state s0i s0is reachable froms0.
neither the termination nor the safety argument for our algorithm depends on cache .
the only role of cache is to optimize the search by avoiding redundant executions.
there fore there is considerable exibility in how much memory is devoted to the storage for cache .
the two extreme cases are when cache is not used at all and when all visited states are put into cache .
but it is possible and our implementation supports imposing a bound on the memory consumption for cache beyond which states are either not added to cache or added with replacement.
an important consideration in our use of cache is that we store only the program state in it and avoid storing the explorer state.
this design has the advantage that we get the maximum pruning out of the use of state caching.
if a state sis rst visited with explorer state dand later with explorer stated0 the second visit is ignored even if it happened with fewer delays compared to the rst visit.
as a result we can avoid re exploration for the second visit.
however it may be possible that a state is discovered with a higher delay than the minimum delay required to visit it.
we believe that this trade o is good because the primary goal of a delaying explorer is to bias the search rather than enforce strict priority.
finally we note that it is enough to store only a hash of a state in cache .
but it is important to store the full state both when it is passed as a parameter to delayboundeddfs or when it is stored in frontier since the program needs to be executed from it.
for the latter uses a state could either be cloned or reconstructed by re executing the program from the beginning.
.
stratified sampling in the previous section we described the ses algorithm to perform strati ed exhaustive search over the executions of an asynchronous reactive program.
in this section we describe a complementary algorithm that enables strati ed exploration via near uniform random sampling of executions from the strata induced by a delaying explorer we call this algorithm the strati ed sampling algorithm ss .
to motivate why random sampling is bene cial we note that the complexity of the ses algorithm grows exponentially with the upper bound on the number of allowed delays.
consequently if a delaying explorer is unable to nd a bug quickly within a few delays the search often takes more time than the programmer is willing to wait for.
to deal with this common problem a time bound is usually supplied in addition to the number of delays.
when an external time bound could stop the search before the delay limit has been reached random sampling has certain advantages over exhaustive deterministic exploration.
first unlike deterministic exploration random sampling can sampleevery execution with a non zero probability making it possible to distribute the limited time resource over the entire search space.
second since each sample is generated independently of every other sample random exploration can be easily and e ciently parallelized an important advantage in an era where parallelism is abundantly available via multicore and cloud computing.
figure shows how our algorithm samples an execution with two delay operations.
first the executepath function de ned later in figure executes the program using a custom strategy de ned by the delaying scheduler with77s0 sn0 sn0 sl0 0sn1 1s0 sn1 sl1 1sl2 2s0 s1 2next nextnextnext nextnextdelay nextdelayfigure a run of ssalgorithm out introducing any delays.
the executepath function returns the length of the execution l0from the start state to the terminal state.
using choose l0 we uniformly pick a valuen0in the range l0 to insert the rst delay.
when executepath is invoked again it introduces a delay at n0 deterministically executes the program upto termination and returnsl1 the length of the path since the last delay.
usingchoose l1 we uniformly pick a value n1in the range l1 to insert the second delay.
finally the execution s0 !
sn0 !s0 !
sn1 !s0 !
sl2 2represents a random execution with two delays.
given a program p a delaying explorer d and a delay bound db an invocation of delayboundedsample figure produces a terminating execution of pwith no more than dbdelays.
the random exploration performed by our algorithm is very di erent in spirit from the classical random walk algorithm on a state transition graph figure which starts from the initial state and executes the program by randomly selecting a transition out of the current state.
this naive random walk although it guarantees a non zero probability for sampling any execution su ers from the problem that the probability of sampling long executions decreases exponentially with the execution length.
instead our algorithm performs a random walk not on the state transition graph but on a di erent graph figure induced by the delaying explorer d. in this graph each node is a complete terminating execution as opposed to a state and an edge is a position in the execution for inserting a delay as opposed to transition .
we show later that the probability of sampling any execution requiring dbdelays is at least1 ldb.
unlike the naive random walk the probability of sampling an execution is exponential in the number of required delays rather than the number of steps.
a long execution has just as much chance to be produced as a short execution with the same number of delays thereby eliminating the bias towards short executions.
the algorithm in figure uses a single global variable path a sequence of natural numbers.
this sequence represents a path as follows.
for each istarting from and up to path length executepforpath steps followed by a delay.
finally execute puntil it terminates.
the procedure executepath performs the execution encoded by path and returns the number of steps performed after the last delay.
the procedure delayboundedsample invokes the procedure executepath repeatedly to randomly sample an executionvarpath sequencehni executepath nf vari j n vars s vard d s d i s0 d0 while i path length f j while j path f s d j t next d s step s d j g d i delay s d i g j while jchoices s j f s d j t next d s step s d j g returnj g delayboundedsample f vari l n if jchoices s0 j return path l executepath i while i db invariant l f path append choose l l executepath i i g g ss f vari n db while true f i while i numsamples db f delayboundedsample i i g db db g g figure ssalgorithm near uniform random sampling with dbdelays.
if the initial state s0does not have any transitions there is nothing to do.
otherwise it sets path to the empty sequence and calls executepath which executes pwithout any delays.
the algorithm chooses a step at random from the number of steps returned by executepath as the position to execute a delay operation.
it extends path with it and invokes executepath again to create a new execution.
it continues to do so iteratively until the number of delays in the execution has reached db.
a single invocation of delayboundedsample samples a single execution with dbdelays.
to calculate this sample it must re execute the program dbtimes and perform dbrandom choices.
theorem .consider a program pand a delaying explorerdthat is sound with respect to p. letlbe the maximum number of steps along any execution of p. for any integer db 0and any execution of p d with db delays thessalgorithm figure generates with probability at least1 ldb.
78figure also shows a procedure ssthat repeatedly invokes delayboundedsample to implement a strati ed sampling algorithm.
this procedure has an timeout terminated and innite outer loop that repeatedly increases the delay bound db.
the inner loop samples numsamples db executions from the set of executions with exactly dbdelays by invokingdelayboundedsample repeatedly.
our algorithm is parameterized by a function numsamples that speci es the number of executions to be sampled for each delay bound.
as we have explained before the number of executions increases exponentially with the number of available delays.
therefore we believe that a practical numsamples function should also have an exponential dependency on the delay bound.
for our evaluation section we chose c1 cdb to be the shape for numsamples db through trial and error we found that c1 and c2 work well for the benchmarks we studied in this paper.
.
implementation in this section we provide an overview of our framework for the evaluation of delaying explorers in systematic testing of reactive asynchronous programs.
pprograms we wrote our programs in the pprogramming language a domain speci c language for implementing asynchronous event driven systems.
a pprogram is a collection of state machines each with an input message queue communicating with each other by sending and receiving messages.
the pcompiler generates from the input program both ccode and zing code.
the generated ccode is used for executing the application either locally on a single computing node or distributed across a collection of nodes thepruntime supports both local and distributed execution.
the generated zing code is provided as input to the zing explorer for systematic testing.
the contribution of this paper exploiting delaying explorers to search executions of asynchronous programs depends on two properties of the pprogramming and testing framework.
first pallows the programmer to write concurrency unit tests by composing a program with an executable model of its execution environment also written inp.
environment models are erased during compilation to ccode and replaced with hand written ccode.
second p provides control over all sources of nondeterminism in the program execution to enable systematic exploration of these nondeterministic choices.
the techniques described in this paper are applicable to any programming system with these two properties.
there are two sources of nondeterminism in the semantics of pprograms.
first phas interleaving nondeterminism because the language provides a primitive for dynamic machine creation.
as a result multiple machines can be executing concurrently.
in each step one machine can be chosen nondeterministically to execute and it can either compute on local state or dequeue a message or send a message to another machine.
this nondeterminism implicitly creates nondeterminism in the order in which messages are delivered to a machine.
the code of a machine has to be programmed robustly and tested so that it continues to perform safely regardless of the reordering.
second a pprogram may also make an explicit nondeterministic choice by using the spe interface izingdelayingscheduler f next is called to get the next process to be executed intnext delay is called to cycle through scheduling choices void delay start is called when a new process is created void start intprocessid finish is called when a process is terminated void finish intprocessid step is called to communicate information about execution e.g.
change priority blocked process etc.
void step params object p g figure delaying explorer interface cial expression whose evaluation results in a nondeterministic boolean choice.
this feature is extremely useful for modeling the environment of reactive systems like nondeterministic component failure or message loss.
to nd bugs quickly and debug them it is essential to control both these sources of non determinism.
implementing a delaying explorer we have implemented the algorithms in sections and using the infrastructure in the zing model checker.
the component of zing most pertinent to our implementation is state caching and the explorer that orchestrates the depth rst search of the statetransition graph of the input zing program.
we modi ed the explorer to query an external object implementing the izingdelayingscheduler interface.
the explorer invokes the method next to determine the process whose transition it should explore and the method delay to inform the scheduler of its decision to delay the next process.
the methods start finish and step together implement the capability formalized by the step function described in section these methods inform the delaying scheduler of important events occurring during the execution.
the method start is invoked whenever a new process is created and the method finish whenever a process terminates.
the method step is used to implement a general mechanism for instrumenting the program s execution for updating the scheduler state.
controlling non determinism the general approach of controlling schedules in systematic testing frameworks is to instrument the program at every synchronization points.
in the context of asynchronous message passing programs like p the only synchronization points are at enqueue of a message blocking at dequeue and creation of a new machine more details in .
the pcompiler automatically instruments the program at these three points and passes the information to the delaying explorer using thestep function.
in addition to prioritizing interleaving nondeterminism a delaying explorer must also prioritize explicit nondeterministic choice.
we simply adopt the convention that false is ordered before true.
for a language that provides nondeterministic choice over types other than boolean the choices may be controlled by expanding the izingdelayingscheduler interface.
.
evaluation our evaluation was directed towards the following goals xevaluate the performance of ses andssin comparison with the best known approaches preemption bounding and probabilistic concurrency testing respectively section .
.
xevaluate the performance of di erent delaying explorers in nding bugs and demonstrate the need for exible delaying explorer interface section .
.
xdemonstrate the bene t of writing custom explorer with a case study of chain replication protocol section .
.
experimental setup all the experiments are performed on intel xeon e5 .40ghz cores threads 160gb machine running bit windows server os.
the zing model checker can exploit multiple cores during exploration as its iterative depth rst search algorithm is parallel .
we do not report the time taken to nd bugs as it is dependent on the degree of parallelism and the parallel explorer implementation but instead we report the number of distinct states explored in the case of ses and number of schedules explored in the case of ss before nding the bug.
time taken to nd the bug is directly proportional to these parameters.
the numbers reported for the evaluation of strati ed sampling algorithm in table are a median over runs of the experiment.
benchmarks we have used p to implement a fault tolerant transaction management system tms and a windows driver communicating with an osr device.
we used p because its compiler provides a translation both to ccode for execution on the microsoft azure cluster and to zing code for systematic testing.
our implementations are not abstract models they are detailed enough to be deployed as a distributed service.
tms uses various protocols like twophase commit protocol for atomicity of transactions chain replication protocol for fault tolerant replication of state machines multi paxos protocol for consistent log replication and consensus.
the buggy programs used for evaluation in this paper were collected during the development of this protocol suite.
each row in table represents a di erent bug.
we only consider hard to nd bugs that led to unhandled event exceptions system crash and violation of global safety speci cations written as monitors .
.
evaluation of sesand ss evaluating ses we applied the iterative ses algorithm with di erent delaying explorers to the set of buggy programs incrementing the value of dbby after each iteration .
for evaluating the performance of ses we implemented iterative preemption bounding pb with statecaching in zing .
table shows the number of distinct states explored before nding the bug by both the approaches.
it can be seen that pbfails to nd the bug in most of the cases and in cases where pbsucceeds ses with some delaying explorer is able to nd the bug orders of magnitude faster except for tms and chainrep .
also there is a lot of variance in the performance of ses when combined with di erent delaying explorers which motivates the need for a exible interface to write custom delaying explorers.evaluating ss we implemented random scheduler rs as the baseline for comparison.
random scheduler fails to nd most of the bugs as the probability of nding a bug decreases exponentially with length of buggy execution.
we found that iterative random scheduler irs that combines random scheduling with iterative depth bounding performs better than simple random scheduling.
strati cation in irs is obtained by iteratively incrementing the maximum depth bound.
we incremented the depth bound by after each iteration and sampled 3iexecutions from each stratum whereiis the iteration number .
we compared the iterative ssalgorithm described in section with the pct algorithm which is considered as state of the art in probabilistic concurrency testing.
pct provides probabilistic guarantees of nding a bug with bugdepthd by randomly inserting dpriority inversions.
most of the concurrency bugs using pct were found with bug depth of less than in .
the pct algorithm makes an assumption about the maximum length of program execution k which is hard to compute statically in the case of asynchronous reactive programs.
we use k and d for our experiments.
table shows that pct fails to nd most of the bugs con rming that the bugs in asynchronous programs generally have a larger bug depth .
in the cases where pct succeeds in nding the bug sswith some delaying explorer is orders of magnitude faster.
similar to the behavior of ses forssalso we see variance in performance of di erent delaying explorers across di erent problems.
comparison between sesandss we have extensively used both ses andssfor nding bugs in our implementations.
in our experience the ses algorithm is able to nd bugs faster than ssin most of the cases as it uses statecaching to prune redundant explorations.
furthermore ses can nd low probability bugs that occur at smaller values of delay budget faster than ss.
in the case of chainrep and paxos there was a low probability bug at small delay budget ssfails to nd it whereas ses nds it.
as the delay bound increases search space explodes exponentially.
if there is a bug that requires large delay budget for a given strati cation strategy then ses may fail to nd it due to running out of memory.
we came across scenarios tms and tms in table where ses ran out of memory but after running ssfor a long time we uncovered a bug.
sscan be kept running for a long time without any memory constraints.
since it performs sampling with probabilistic guarantees it may nd a bug at larger delay budget where ses fails.
we can fruitfully combine both approaches as follows.
performses rst to nd all shallow few delays bugs quickly and get strong coverage guarantees.
once ses has uncovered all shallow bugs and has almost consumed the memory budget perform ssfrom the frontier states and get probabilistic guarantees.
we leave the evaluation of this combination for future work.
.
experience with delaying explorers we have implemented three di erent delaying explorers.
in this section we explain the construction of each explorer and the reasons for the variance in their performance.
the 80table evaluation results for ssandsesusing various delaying explorers strati ed sampling strati ed exhaustive search programsno.
of schedules explored before nding bug no.
of states explored before nding bug rs irs pctss delaying explorerpbses delaying explorer rr rtc prr rr rtc prr 2pc1 2pc2 2pc3 2pc4 chainrep chainrep chainrep chainrep chainrep chainrep chainrep chainrep chainrep chainrep multipaxos multipaxos multipaxos paxos paxos paxos tms tms tms tms osr osr !the search ran out of memory budget of 60gbor exceeded the time budget of hours.
!the search exceeded the time budget of hours running for longer duration .
source code for these explorers is available at the following website .
run to completion explorer rtc the run to completion explorer was introduced in prior work for testing device drivers written in p. the default strategy in rtc is to follow the causal sequence of events giving priority to the receiver of the most recently sent event.
when a delay is applied the highest priority process is moved to the lowest priority position.
even for small values of delay bound this explorer is able to explore long paths in the program since it follows the chain of generated events.
in our experience this explorer is able to nd bugs that are at large depth better than any other explorer.
for example bugs in chainrep and tms were found were found by rtc at depth greater than and delay budget less than while other explorers could not nd these bugs.
round robin explorer rr the round robin delaying explorer explained earlier in section cycles through the processes in process creation order.
it moves to the next task in the list only on a delay or when the current task is completed.
round robin explorer has been used in the past to test multithreaded programs.
in our experience in most of the cases other delaying explorers perform better than rr.rrcan be used for nding bugs that manifest through a small number of preemptions or interleaving between processes.
our evaluation shows that most bugs in asynchronous programs do not fall in that category.
probabilistic round robin explorer prr a probabilistic delaying explorer is one in which the step operation is allowed to make random choices.
while a determinis tic delaying explorer induces a xed strati cation over the executions of a program a probabilistic delaying explorer induces a probability space over strati cation.
we have experimented with a cannibalistic version of the round robin explorer prr .
we believe that the culprit behind the poor performance of the round robin explorer is its default process scheduling order which is based on the order of process creation.
the simplest way to change this default order is to randomize it.
instead of inserting a freshly created process at the tail of the queue insert it at a random position in the queue everything else carries over from the round robin explorer.
the probabilistic round robin explorer is still sound since the de nitions of next and delay do not change.
table indicates that prr typically performs better than rr.
.
writing a custom delaying explorer after testing the chain replication protocol using the three delaying explorers explained earlier we tested it for more speci c scenarios.
one such scenario is testing the system against random node failures.
we provide a brief description of the chain replication protocol.
next we show how we wrote a custom explorer to test for the node failure scenario and found a previously unknown bug in our implementation.
the chain replication protocol is a distributed faulttolerant protocol for replicating state machines.
consider an instance of a chain replication system with machines instances of server machine s1 s connected in a chain instance of master machine m and instance offault machine f .s1 s 4communicate with each other to implement replication.
mperiodically monitors the health of s1 s 4to detect if any of them has failed.
if it detects a fault in si it tells the neighbors of sito re81con gure.
fis a machine that models fault injection.
it maintains a set of numbers initialized to f1 4g.frepeatedly and nondeterministically removes a number ifrom this set and sends a failure message to siuntil the size of the set becomes .
the chain replication protocol is expected to behave correctly for nservers as long as at most n fail.
when a distributed system starts up there is an initialization phase involving exchange of messages between nodes for setting up the network topology and other system conguration.
bugs during the initialization phase are straight forward infrequent and get discovered quickly.
subtle bugs are generally encountered after the system is initialized and has reached an interesting global state.
since we want to test our system against a speci c scenario of failure occurring after the system has stabilized the new delaying explorer should not spend a lot of time injecting failures or monitoring the system during the initialization phase.
we need strati cation that gives less priority to certain interleaving in the the initial phase.
to capture this intuition with a delaying explorer we wrote a customized delaying explorer custexplorer .
the explorer maintains an ordering of all dynamically created machine and cycles through them based on the ordering.
the program can change the ordering by invoking changeorder callbacks implemented using step .
using changeorder callback in the initialization phase the machines s1 s are ordered before machines mandf.
after the initialization phase the machines mandfare moved ahead in the ordering as compared to machines s1 s .
thus custexplorer helps in stratifying the search by giving less priority to interleaving the failure and monitor machines until the system has stabilized.
using custexplorer we were able to nd a previously unknown bug in chain replication which occurred when the failure was injected simultaneously at two neighboring nodes after the initialization phase.
custexplorer was able to nd the bug with ses by exploring states and with ssby exploring schedules.
we applied the same strategy to chainrep as it had similar bug related to node failure and we were able to nd the bug in states which is nearly times faster than the next best.
.
related work model checking is a classic technique applied to prove temporal properties on programs whose semantics is an arbitrary state transition graph.
our use of state caching to prune search is inspired by model checking.
partial order reduction is another technique to prune search.
combining partial order reduction with schedule prioritization techniques is known to be a challenging problem .
coons et al.
have proposed a technique to combine preemptionbounding with partial order reduction.
in future work we would like to investigate the feasibility of combining delayed exploration with partial order reduction.
there is prior work on random sampling of concurrent executions.
sen provides an algorithm for sampling partiallyordered multithreded executions.
similar to our work the pct algorithm also exploits prioritization techniques to e ectively sample multithreaded executions.
the pct al gorithm characterizes a concurrency bug according to its depth and guarantees that the probability of nding a bug with depth din a program with lsteps andnthreads is at least nld .
the mathematical techniques underlying pct and our sampling algorithm are di erent.
pct provides a custom algorithm for a particular notion of bug depth whose de nition has a deep connection with the proof for the probability bound.
on the other hand our algorithm does not depend on a characterization of bugs.
rather it is parameterized by a delaying explorer a mechanism used by the programmer to stratify the search space.
consequently the proof for our probability bound is a straightforward combinatorial argument on a bounded tree in terms of its branching factor and depth.
predictive testing follows the basic recipe of executing the program collecting information from the execution constructing a model of the program from the collected information and then re executing the program based on new predicted interleavings likely to reveal errors.
the various techniques di er in the information collected and the targeted class of errors.
the search performed by predictive techniques is goal driven but typically does not provide coverage guarantees.
on the other hand our search technique is not goal driven but provides coverage guarantees.
concurrit proposes a domain speci c language for writing debugging scripts that help the tester specify thread schedules for reproducing concurrency bugs.
the search is guided by the script without any prioritization.
in contrast our work is focused on nding rather than reproducing bugs.
instead of a debugging script a tester writes a domain speci c scheduler with appropriate uses of sealing iterative deepening with delays automatically prioritizes the search with respect to the given scheduler.
.
conclusion we have demonstrated how delaying explorers help in systematic testing of asynchronous reactive programs.
we also showed that using delay bounding with a single default scheduler is not scalable for nding bugs.
di erent delaying explorers induce di erent strati cation and hence writing custom delaying explorers as unit test strategies can make testing complex asynchronous protocols scalable.
we also presented and evaluated two algorithms ses for exhaustive search with strong coverage guarantees and showed how state caching can be used e ciently for pruning ssfor sampling executions with probabilistic guarantees.
we evaluated both these algorithms on real implementation of distributed protocols and showed that our techniques perform orders of magnitude better than state of art search prioritization techniques like preemption bounding and pct.
.