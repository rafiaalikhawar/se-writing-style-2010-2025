a framework for checking regression test selection tools chenguang zhu1 owolabi legunsen2 august shi2 and milos gligoric1 1the university of texas at austin 2university of illinois at urbana champaign email cgzhu utexas.edu legunse2 illinois.edu awshi2 illinois.edu gligoric utexas.edu abstract regression test selection rts reduces regression testing costs by re running only tests that can change behavior due to code changes.
researchers and large software organiza tions recently developed and adopted several rts tools to dealwith the rapidly growing costs of regression testing.
as rts toolsgain adoption it becomes critical to check that they are correctand efficient.
unfortunately checking rts tools currently reliessolely on limited tests that rts tool developers manually write.
we present rtsc heck the first framework for checking rts tools.
rtsc heck feeds evolving programs i.e.
sequences of program revisions to an rts tool and checks the output againstrules inspired by existing rts test suites.
violations of these rulesare likely due to deviations from expected rts tool behavior andindicative of bugs in the tool.
rtsc heck uses three components to obtain evolving programs autoep automatically generatesevolving programs and corresponding tests defectsep usesbuggy and fixed program revisions from bug databases and evoep uses sequences of program revisions from actualopen source projects histories.
we used rtsc heck to check three recently developed rts tools for java clover ekstazi andstarts.
rtsc heck discovered bugs in these three tools.
i. i ntroduction regression testing is an important widely used but costly approach for checking that code changes do not break previously working functionality.
regression testing costs arisefrom running tests on each program revision such costs havebeen growing quadratically with the growth in the number oftests and with increasing frequency of code changes .
regression test selection rts reduces regression testing costs by selecting to re run on each new programrevision only a subset of tests that can change behavior dueto code changes i.e.
affected tests.
a typical rts technique collects dependencies e.g.
methods classes for each test andselects to re run only tests whose dependencies changed.
anrts technique is safe if it does not miss to select any affected test and precise if it selects only affected tests.
to deal with the rapidly growing costs of regression testing several rts tools were recently developed and adoptedby both industry and researchers.
industry examples includemicrosoft s test impact analysis for .net which shipswith visual studio to millions of developers and clover s testoptimization for java which was recently open sourcedto increase adoption.
researchers also developed several rtstools in the last five years alone some of these toolshave been adopted by large software organizations .
as rts tools gain adoption and become more mainstream it becomes critical and timely to check their correctness andefficiency.
we say an rts tool is correct if it is safe andprecise subject to the implemented rts technique.
rts toolefficiency is measured by comparing its end to end time i.e.
test selection time plus execution time for selected tests withretestall i.e.
running all tests at each revision.
unfortunately there is no systematic approach for checking correctness and efficiency of rts tools.
checking rts toolscurrently depends solely on the limited sets of tests that eachrts tool developer manually writes.
prior research establisheda framework for analytically evaluating rts techniques and several researchers semi formally proved safety and com putational complexity of rts techniques .
however these proofs and analyses may not carry over to the rts tools that implement those techniques implementing a technique ina tool requires engineering and like any other software rtstools may contain bugs.
a framework for checking rts toolswill enable researchers to compare existing rts tools andcheck future rts tools and provide greater confidence todevelopers who are considering to adopt rts tools.
we present rtsc heck a novel framework for checking rts tools.
rtsc heck feeds evolving programs i.e.
sequences of program revisions to an rts tool and checks theoutput against rules that specify likely violations of expected behavior.
rtsc heck currently uses seven hand crafted rules inspired by developer written tests for rts tools users canextend the set of rules.
rtsc heck detects violations in three categories.
rtsc heck detects a likely safety violation if an rts tool does not select expected tests e.g.
not selectingto run newly failed tests that fail in retestall.
rtsc heck detects a likely precision violation if an rts tool selects unnecessary tests e.g.
running alltests the second time when run twice on the same program revision.
rtsc heck detects agenerality violation if an rts tool does not integrate well with the program leading to unexpected behavior e.g.
failingmore tests than retestall due to incorrect instrumentation.
rtsc heck also generates an efficiency report which shows if an rts tool takes longer to run on average than retestall.
rtsc heck uses the common assumptions in rts research that tests are not flaky and there is no test orderdependency .
all violations rtsc heck detects are due to implementation issues orlimitations of the underlying rts technique that were unknown a priori currently we map violations to these two root causes manually.
rtsc heck has three components for obtaining the evolving programs i.e.
code and tests for testing rts tools.
first ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
rts tools components configurationrtscheck autoep defectsep evoepcomponentsobtain evolving programs retestall rts tool rts tool nrun programs check rulessafety violations precision violations generality violations efficiency report fig.
an overview of the rtsc heck framework the autoep component automatically generates a code revision randomly generates tests for that revision and systematically modifies the old revision with a set of evolution operators to obtain subsequent revisions of code and tests.
second the defectsep component obtains evolving programs with two revisions from bug databases using the fixed version as the first revision and the buggy version as the second revision.
finally the evoep component obtains evolving programs by extracting program revisions from software repositories.
our current rtsc heck implementation supports checking and comparing rts tools for java.
we used rtsc heck to check and compare three recent rts tools clover ekstazi and starts .
clover is developed by industry while ekstazi and starts are developed by researchers.
we used rtsc heck to obtain a total of 31k evolving programs which have a total of 4m tests.
rtsc heck reported 24k violations we inspected a subset of these violations and mapped them to implementation issues seven limitations of the underlying rts techniques and two false positives.
we reported all bugs to the developers of these rts tools four of which were already known to the developers.
the developers already confirmed of the previously unknown bugs.
each rtsc heck component contributed to discovering several unique bugs and we found at least six bugs in each rts tool that we checked.
this paper makes the following contributions framework we present rtsc heck the first framework for systematically checking rts tools for likely violations of expected behavior and for reporting efficiency.
we are the first to apply automatic generation of evolving programs and existing bug databases for checking rts tools.
implementation we implement rtsc heck to check rts tools for java.
rtsc heck can be extended to check new rts tools support new components for obtaining evolving programs or use different rules.
rtsc heck is available at evaluation we deployed rtsc heck to check three rts tools clover ekstazi starts.
rtsc heck discovered bugs four of which were known.
the rts tool developers so far confirmed of the remaining bugs.
ii.
t hertsc heck framework figure shows an overview of rtsc heck .
rtsc heck takes two inputs a configuration file for setting up the components that obtain evolving programs and the rtstable i rules for detecting violations from running an evolving program with retestall and at least one rts tool id violation description type r1 in some revision the number of newly failed tests when run with the tool is lower than with retestallsafety r2 in some revision the tool selects zero tests but all other tools select all tests r3 in all revisions the tool selects all tests precisionr4 in some revision the tool selects all tests but all other tools select zero tests r5 the first two revisions are the same and the tool selects one or more tests in the second revision r6 in the first revision the tool selects a different number of tests than retestall generalityr7 in some revision the number of failed tests when run with the tool is greater than with retestall tools to check.
based on these inputs rtsc heck obtains evolving programs feeds these programs one at a time to the rts tools and checks for likely violations violations for short .
we first describe the rules that rtsc heck uses to detect violations and then we describe each component for obtaining evolving programs.
a. rules for detecting violations we define an evolving program as a sequence of nprogram revisions p0 p1 ... p n .
each program revision is a tuple of code under test and a test suite.
for a given evolving program rtsc heck runs each program revision with the rts tools being checked the results of test execution and tool specific intermediate data are stored as metadata.
the metadata is available when running the next program revision and enables rts tools to perform selection.
our rules for detecting violations apply to one evolving program at a time the rules are defined over the metadata available after executing the evolving program.
table i shows our rules for detecting violations.
we assume that rtsc heck executes the given evolving program with retestall and at least one rts tool although rules r2 and r4 apply only if more than one rts tool is provided as the input.
for each rule we show a unique id a short description of when the rule is violated and the type of the violation that the rule detects there are three types of violations safety violations precision violations and generality violations.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
safety violations rules r1 and r2 detect safety violations.
a safety violation occurs when the rts tool may be selecting fewer newly failed tests than should be selected a newly failed test passed in the old revision but fails in the new revision.
violating rule r1 indicates a bug we assume no flaky tests and no test order dependencies .
not selecting a newly failed test means that an rts tool could cause users to miss a fault in the code.
violating rule r2 may not indicate a true bug because it compares an rts tool against other tools.
however in such cases the difference in selection selecting no tests versus all other tools selecting all tests is extreme so it is highly likely there is a bug in the tool.
precision violations rules r3 r4 and r5 detect precision violations.
a precision violation occurs when the rts tool may be selecting too many tests more than are necessary.
of these rules there is no guarantee that violating rules r3 and r4 indicate actual bugs in the rts tool it may be the correct behavior for the rts tool to be selecting all the tests as they may all be affected tests.
however again such scenarios are rather extreme and likely indicative of a bug where all other tools select no tests or if on every revision a tool selects all tests.
violating rule r5 indicates a true bug if there is no change there should be no affected tests.
generality violations rules r6 and r7 detect generality violations.
a generality violation occurs when using the rts tool with the code and tests leads to different behavior than what is expected such as crashing or failing more tests than with retestall.
violating either of the two rules indicates a true bug in the rts tool.
for rule r6 an rts tool on a fresh first revision should always select all the tests as retestall does.
not selecting all the tests indicates the tool is not working properly with the code tests.
while violating r6 means running fewer tests than should be run it is not a safety violation as there is no change that leads to any affected tests.
for rule r7 if an rts tool results in more failed tests than retestall the extra failed tests must be due to bad integration with the tool e.g the tool caused tests to behave differently or the tool is crashing for some tests.
our rules are inspired by the assertions from existing manually written tests for rts tools capturing common expected behaviors of rts tools.
note that while assertions from manually written tests helped design the rules rts tool developers usually check these assertions on very few if any evolving programs.
starting with some of the base assertions from existing tests we modified them to only test extreme cases.
for example instead of having a rule that is violated when an rts tool selects fewer tests than other rts tools our rule r2 is a more extreme version of this rule.
intuitively having more extreme rules leads to fewer false positives.
the rules we use here do not necessarily find all bugs in rts tools.
however rtsc heck has a modular design and provides a way to extend the set of rules that can help detect more violations that lead to finding more bugs.
we plan to study various extensions in the future.b.
the autoep component autoep obtains evolving programs via automated code generation test generation and code evolution.
our key idea is to apply bounded exhaustive testing with randomly generated tests and state comparison for checking rts tools.
additionally we develop a novel set of program evolution operators.
autoep works in three main steps generate the first revision in an evolving program generate tests for the firstrevision programs and evolve those first revision programs to obtain corresponding second revision programs.
a program may evolve in multiple ways so autoep obtains multiple evolving programs from a first revision program.
program generator autoep uses jdolly to generate the first revision programs.
jdolly systematically generates java programs up to specified bounds and was originally developed for testing java refactoring engines .
we choose jdolly because it exhaustively generates programs with complex relations among code elements e.g.
class inheritance .
table ii shows the user specifiable constraints we use to tune program generation for jdolly.
the table shows a unique identifier for each constraint id and a short summary of each constraint summary see .
more details about the constraints are available elsewhere .
table ii program generation constraints used in autoep id summary see jd1 noconstraints jd2 clzwithmethodandsuperclz jd3 clzwithmethodandsubclz jd4 clzwithfieldandsuperclz jd5 clzwithmethodandfield jd6 someinheritance jd7 somemethod jd8 somefield jd9 somecaller jd10 somefieldsomefieldaccessnote that programs generated by jdolly may not compile.
therefore autoep contains a postprocessing step to remove programs that do not compile.
test generator autoep uses the randoop tool to generate tests.
each test method generated by randoop is a sequence of method calls.
the method sequence length and the number of test methods to generate can be specified as autoep inputs we evaluate with maximum sequence lengths of and up to tests per length.
the other input to randoop is the set of classes for which to generate tests.
the list of classes we provide to randoop are the first revision classes generated by jdolly.
randoop starts with an empty set of sequences and randomly chooses in each step a method to invoke from one of the first revision classes.
the call sequence is extended until the specified limit is reached or invoking the current sequence causes an exception so that further extension of the sequence is not beneficial.
the arguments for each method call in the sequence are selected either from a predefined pool for each type e.g.
null for reference types or from the results of prior method calls in the same method call sequence.
oracles in randoop tests are limited so we additionally capture program state as new oracles for the tests.
specifically the program state contains objects of all classes in the generated program including values of all primitive inherited fields and information about their type i.e.
the class hierarchy .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
we capture the program state at the end of a test run by a lightweight heap traversal.
we treat the state captured at the end of a test run in the first revision program as the expected value of the state.
essentially one can manually create an assertion that fails if the state captured at the end of a test run does not match this expected value of the state.
if running the test later e.g.
after evolution the state does not match this expected state then this assertion fails.
dynamic instrumentation that we use to capture program state is lightweight it only adds a single line to each constructor.
we confirmed on a large number of examples that our instrumentation does not conflict with that used by the dynamic rts tools in our study nor do they impact dependencies collected by the rts tools.
program evolver automatically checking rts tools requires at least two revisions of an evolving program which is unlike prior work on program and test generation for testing compilers refactoring engines etc.
which generate many single revision programs.
our approach for evolving the first revision of an evolving program into the second revision uses mutations similar in spirit to how the emi approach creates program variants for compiler testing.
the differences with emi are the set of program evolution operators used for evolving the initial revision and the goal of generating programs.
the goal of many operators in autoep is to change the behavior of the code under test in ways that affect test outcomes.
to generate second revision programs autoep mutates first revision programs using a set of program evolution operators that we define based on the literature on developing safe rts techniques for object oriented programming languages .
table iii program evolution operators available in autoep id description e1 add extends e2 copy field e3 copy field and replace e4 copy method e5 copy method and replace e6 evolve to next program e7 increase constants e8 remove extends e9 remove methodtable iii shows the program evolution operators supported in autoep .
the first column shows id for each operator that will be used later in this document and the second column briefly describes each operator.
add extends e1 adds an extends keyword to a class and systematically chooses a superclass.
copy field e2 copies a field from one class to another.
copy field and replace e3 copies a field from one class to another and changes its initial value if primitive .
copy method e4 copies a method from one class to another.
copy method and replace e5 copies a method from one class to another and changes a constant in its body.
evolve to next program e6 evolves a program to a subsequent program considering the order in which the programs were generated.
increase constant e7 replaces a constant with a larger value.
remove extends e8 removes an extends keyword.
remove method e9 removes a method.
operators e1 e2 e3 e4 e5 e8 and e9 impact class relationships.
e6 corresponds to a random program evolution.finally e7 modifies various code elements where constants can appear e.g.
field initialization or return statement.
each operator may be applied to several locations in a first revision program resulting in many evolving programs.
autoep s operators are related to operators in mutation testing which are used to evaluate test suite quality.
for example increase constant is available in most mutation testing tools.
however other autoep operators have no equivalent previously proposed mutation operator.
c. the defectsep component the defectsep component obtains evolving programs by extracting fixed and buggy revisions from bug databases.
our goal is to use a real bug introducing change and a failing test for checking rts tools.
several bug databases follow a similar structure there are two program revisions for each bug one revision corresponding to the buggy program revision and the other corresponding to the fixed program revision.
these programs are usually large open source projects and the bugs are actual bugs fixed by developers of those projects.
there is usually at least one test that fails in the buggy revision i.e.
the bug revealing test and passes in the fixed revision.
the main idea behind defectsep is to reverse the order of the buggy and fixed revisions to simulate a program change that leads to failing tests.
if an rts tool is integrated in such an evolving program the tool should always select the failing test s .
although our original motivation is to check for safety violations we still check all applicable rules.
for the case of rule r5 because there cannot be two revisions that are the same otherwise there cannot be a buggy and a fixed revision we also run the fixed revision twice to have the first two revisions be the same and see if r5 is violated.
in our current version of rtsc heck the defectsep uses the defects4j bug database .
defects4j includes a large number of bugs and it has been used for many software engineering research tasks .
we are the first to use defects4j for evaluating rts tools.
d. the evoep component the evoep component obtains an evolving program by extracting program revisions from existing software repositories.
furthermore like with defectsep we run the first revision twice to ensure the first two revisions are the same which helps us check if r5 gets violated.
the main motivation for having evoep is to evaluate rts tools with evolving programs with more than two revisions the previous two components use only two revisions .
thus evoep may potentially discover bugs that require more than two revisions to expose.
moreover checking an rts tool with evoep can be seen as integration testing.
evoep extracts evolving programs from projects with complex setups so it also has the potential to discover bugs that manifest only with specific program configurations.
additionally the best way to evaluate efficiency is likely by observing execution time on longerrunning evolving programs.
finally is is important to check rts tools on actual program changes over a period of time.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iv number of generated programs base and number of generated evolving programs g total and c compilable for v arious modes using autoep basee1 e2 e3 e4 e5 e6 e7 e8 e9 g c g c g c g c g c g c g c g c g c jd1 jd2 jd3 jd4 jd5 jd6 jd7 jd8 jd9 jd10 iii.
e xperiment setup in this section we describe the rts tools used in the evaluation and present evolving programs obtained by rtsc heck .
we make a replication package for our evaluation publicly available on our website .
a. the rts tools under evaluation we use three rts tools in our evaluation clover ekstazi and starts .
all the tools work for java.
further ekstazi and starts are developed by researchers.
clover was developed in industry and started life as a proprietary product but is now open source.
clover .
in clover the test optimization feature performs rts.
clover performs source code instrumentation prior to compilation.
then at runtime it records in a database a mapping from each method under test to the set of test methods that use the method under test.
after a change clover first finds the methods under tests that changed then it queries its database to find which tests used the changed methods.
clover re instruments the files that contain changed methods and keeps the same instrumentation for unchanged files.
clover re runs tests that it found to use changed method s from the previous revision plus any test s not already in its database e.g.
newly added tests.
finally after running the tests clover updates the method to tests mapping in its database with information from the current run in preparation for a future run.
we use clover .
.
and the default configuration.
ekstazi .
ekstazi uses dynamic binary instrumentation to track the class dependencies of test classes.
more specifically ekstazi tracks as test dependencies the classes i.e.
underlying compiled .class files that are used while executing each test class.
ekstazi computes and stores a checksum for each test dependency in a revision.
then after a code change ekstazi recomputes the checksum of all the test dependencies to see which ones have changed.
the affected tests computed by ekstazi are all test classes for which at least one dependency has a different checksum in the previous and current revision plus any newly added test s .
finally while running the affected tests in the current revision ekstazi uses its instrumentation to track and update the dependencies of the affected tests in preparation for a future run.
we use ekstazi .
.
and the default configuration.starts .
starts statically computes the dependencies of each test class and does not require any instrumentation.
first starts uses jdeps to extract the dependencies of each class in the application.
starts computes as test dependencies the reflexive and transitive closure for each node that represents a test class in the dependency graph.
note that starts can be imprecise because the dependencies found by jdeps are only potentially used classes and are not necessarily runtime dependencies the constant pool in a.class file contains the list of fully qualified names of all classes that are used in the source file and starts can miss dependencies when the relationship between classes happens only via reflection.
starts computes changes and computes stores checksums in the same way as ekstazi.
we use starts .
and the default configuration.
b. evolving programs autoep table iv shows the number of generated evolving programs using different autoep modes i.e.
a combination of a program generation constraint and a program evolution operator.
each row of the table shows the constraints used in program generation and each column shows one way to evolve those programs.
we configured autoep to generate programs i.e.
first revisions of evolving programs for each mode we limit the number of programs to make the experiments feasible.
table iv shows in the base column the number of those programs out of that successfully compile.
autoep evolves only the programs that can be compiled.
for each program evolution operator we show the number of generated evolving programs g and the number of those that can be successfully compiled in the second revision c the following sections use only compilable evolving programs.
as we expected some operators are better than others at generating compilable evolving programs.
for example increasing a constant e7 or copying a field e2 does not introduce any compilation error.
on the other hand removing a method e9 frequently leads to a compilation error because those methods are invoked from at least one of the tests.
finally as expected copying a field or increasing a constant does not create any evolving program when no field is present in the original program e.g.
mode jd2 e2 or all fields are