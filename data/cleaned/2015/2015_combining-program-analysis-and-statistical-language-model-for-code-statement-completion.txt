combining program analysis and statistical language model for code statement completion son nguyen and tien n. nguyen computer science department the university of texas at dallas usa email sonnguyen tien.n.nguyen utdallas.eduyi li and shaohua wang department of informatics new jersey institute of technology usa email yl622 davidsw njit.edu abstract automatic code completion helps improve developers productivity in their programming tasks.
a program contains instructions expressed via code statements which are considered as the basic units of program execution.
in this paper we introduce a uto sc which combines program analysis and the principle of software naturalness to fill in a partially completed statement.
a uto sc benefits from the strengths of both directions in which the completed code statement is both frequent and valid.
a uto sc is first trained on a large code corpus to derive the templates of candidate statements.
then it uses program analysis to validate and concretize the templates into syntactically and type valid candidate statements.
finally these candidates are ranked by using a language model trained on the lexical form of the source code in the code corpus.
our empirical evaluation on the large datasets of real world projects shows that auto sc achieves .
.
top accuracy and .
.
top accuracy in statement completion.
it also outperforms a state of the art approach from 9x 69x in top accuracy.
keywords code completion statement completion statistical language model program analysis i. i ntroduction code completion tool helps improve developers productivity by filling in the code during their editing.
a program contains instructions in source code to perform certain tasks.
the procedure to achieve a task is expressed via program statements each of which is considered as the basic unit of execution in a program.
a statement can declare a variable define an expression perform a simple action by calling a method control the execution flow of other statements create an object or assign a value to a variable attribute or field .
thus in this work we aim to support automated code completion to help developers fill in their current statements.
during writing the body of a method if a developer finishes one or more tokens of the current statement the tool as requested will fill in the remaining tokens of that statement.
if s he finishes a statement the tool will suggest the entire next statement nextstatement completion .
let us call it a statement completion sc tool.
sc encompasses next statement completion.
to build an effective and efficient sc tool one would face the following key challenges.
first the tool must predict the statement that a developer intends to type next to perform the programming task at hand.
second the resulting code after completion must conform to the syntactic and semantic constraints defined by the programming language in use.to address the first challenge one can rely on the principle of software naturalness .
source code is naturally written with certain regularity i.e.
it is repetitive and does not occur randomly.
the code elements appear together because they are intended by developers to achieve a programming task s .
hindle et al.
showed that such regularity in source code can be captured by statistical language models lms e.g.
ngram model can be leveraged to support code completion for the next token.
thus one could train an lm with a large code corpus and use it to predict each token at a time until a complete statement is suggested.
however the frequent code fragments learned from different contexts might make the code after completion syntactically or semantically incorrect.
for example after i if the most frequent variable in a corpus is i the resulting code is i i which is invalid.
a naive solution that uses program analysis pa to enforce the constraints in such output with multiple tokens would face combinatorial explosion.
for example assume that at each step a model predicts and maintains nmost likely valid tokens the number of statements with mcode tokens is nm.
to address the second challenge an sc tool can apply pa with program constraints on the candidate statements to eliminate the invalid ones the number of the remaining valid candidates is still large.
the accuracy of such a naive solution is very low due to the confounding effect of the accuracy of a prediction model for each token see section viii .
another solution to this issue is to search for an entire code statement.
however it is ineffective since statements are project specific and do not repeat often across different methods or projects as reported in pcc .
in fact learning to suggest entire statements is less effective than an sc tool that is capable of filling the remaining token s of the current statement.
this paper proposes a uto sc which combines program analysis and statistical lm in the process of statement completion.
we aim to benefit from the strengths of both directions in which lm produces natural code sequences and pa enforces syntactic and type constraints.
a uto sc works in three phases.
first it uses the n gram lm on an abstraction level higher than lexical code to learn to derive the most likely candidate templates for the current statement.
a candidate statement is modeled by a sequence of special annotations called extended code tokens excode for short .
an excode for a token is an annotation representing the token type and or data type 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
if available.
the token type encodes whether the token is a variable a field access a method call a type class etc.
such information in a template helps a lm predict better the next token e.g.
a variable cannot be next to another.
data types help distinguish the code fragments having the same meaning but with different variables names e.g.
int len s.length and int l str.length have the same meaning of retrieving the length of a string and assign it to an int variable .
with the types instead of the variables names the two fragments have the same template.
thus a uto sc can learn templates from one place to suggest for the other.
data types also help distinguish the cases in which two fragments with the same lexical tokens having different meaning.
for example x.next means a variable xof a scanner accessing the field next while in another place it means a variable xof a linkedlist calling the method next .
data types thus help determine the accessible method calls or field accesses for a variable.
at the second step the candidate templates are syntactically and type validated.
then the valid templates are concretized into code sequences.
finally all valid suggested code sequences are ranked based on their occurrence likelihoods given the partial code.
to do this we train another n gram model on the lexical form of the source code in a code corpus.
we conducted several experiments to evaluate a uto sc in statement completion on a dataset used in the existing approaches with 460k statements with a total of 1m suggestion points.
our results show that a uto sc is very effective with top accuracy of and top accuracy of .
on average.
that is in out of cases when a user requests to complete his her currently written statement s he can find the remaining of the desired statement in the top of the suggestion list.
importantly a uto sc significantly improves over the baseline model using only n gram on lexical code up to 142x in top accuracy and the model using lexical n gram pa up to 117x in top accuracy .
it also improves over the state of the art tool pcc with 69x higher in top accuracy.
in brief our contributions include .
a model with pa lm to complete the current statement .
an empirical evaluation showing our model s effectiveness and much better accuracy than the state of the art tool.
ii.
m otiv a ting example figure partially shows a method in apache ant .
assume that the cursor is at line right after the sign.
if a user requests a statement completion sc tool it will complete the current statement i.e.
the assignment to the variable len.a sc tool would predict the intention of the user and complete that assignment with the method call children.getlength .
the tool suggests a ranked list of candidate statements such as in figure .
if the cursor is at the beginning of a statement e.g.
at the beginning of the line the sc tool would suggest the entire assignment statement e.g.
int len children.getlength next statement suggestion .
that is sc includes the functionality of next statement suggestion e.g.
pcc .
note that the sc tool is automatically invoked as the user finishes typing a token in the middle of a statement e.g.
after int len etc.1nodelist listchildnodes node parent nodefilter f 2nodelistimpl matches new nodelistimpl 3nodelist children parent.getchildnodes 4if children !
null int len expected children.getlength candidate children.getlength candidate parent.numchildren candidate figure a partial method in class domutil of apache ant to support statement completion a model needs to consider the nature of source code.
source code is strictly defined by the syntax and semantics of the programming language.
source code is also repetitive .
thus the methods for sc can be realized in the following information retrieval ir and pattern mining program analysis and statistical language model .
for ir and pattern mining a model suggests to complete the current statement by searching for the same similar statement s that have been seen in a corpus.
when the retrieved statements have occurred frequently they can be viewed as code patterns.
such a pattern or a retrieved statement can be used as the candidate for completion.
however the tokens need to be filled for the current statement might not be a pattern leading to ineffectiveness of such approach.
moreover while as single tokens code is repetitive as entire statements they are quite unique for specific projects.
this phenomenon was reported by yang et al.
.
indeed in our experiment section ix the portion of repeated statements in our dataset is .
.
that is out of cases on average cannot be correctly suggested by searching for the same statements in the corpus of the previously seen statements.
as an example the statement int len children.getlength is not used in any other project in our dataset.
as an implication to suggest or complete a statement a model cannot rely solely on searching for the repeated statements as a whole.
for the program analysis pa direction although the number of valid candidates for the next token is limited the number of possible valid complete statements at the suggestion point might be combinatorially explosive or even infinite.
for the right side of the assignment at line the valid next token candidates include the appropriate prefix operators e.g.
and the open parenthesis field access method call and local variable e.g.
children f etc.
.
however there is an infinite number of valid statements at that point.
in brief program analysis direction could produce a large number of candidates with equal occurrence likelihoods despite that the candidates are syntactically or semantically valid.
the statistical language models lm leverage the fact that code is highly repetitive and predictable .
the next tokens to be filled are based on the frequent sequences of tokens and the partial code.
solely relying on those to fill in a statement a model could face the following issues.
the first issue is caused by the fact that the code in different places with the same lexical code sequence have different meaning .
for example in one place x.next means the variable xof a scanner in authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
jdk accessing the field next while in another place it means the variable xof a linkedlist calling the method next .
in this case a lm can mistakenly use one to suggest for another e.g.
it could recommend after x.next for the field access of a scanner which results in a semantic error.
second in contrast to represent the same meaning in different places in the same or different projects one could use different names of the variables .
for example the statement at line is a code fragment that performs the task of retrieving the size length of a list of nodes .
in other places we might see int size children.getlength .
those two code fragments might be deemed as not performing the same task if only the lexical tokens are considered.
thus an lm cannot learn from one place to complete the statement in the other place.
third the names of method calls and field accesses might not appear in the training data leading to the out of vocabulary oov issue.
this also applies to local variables due to their method specific nature.
in natural language human can understand a sentence even an oov word is missing.
however oov could cause the code un compilable.
finally even oov does not occur the completed code by a lm could violate syntactic and semantic constraints.
at line the most likely next sequences of tokens include i o r which are frequent in a corpus.
that would induce an undeclared variable error .
from the above discussion it is natural to combine pa and lm to benefit from the strengths of both directions in completing the current statement .
for example pa can be used to derive select the syntactically and type valid candidate statements from the list produced by statistical lm while the latter can apply the principle of code repetition to rank the valid and most likely statements higher in the candidate list.
a naive lm pa solution would use a statistical lm to predict the next token one by one and then use pa to filter out the invalid ones and rank the remaining ones according to their occurrence likelihoods.
however so the number of valid statements is still large.
our experiment section ix showed that among those valid ones the correct one is rarely in the top most likely candidates top accuracy is .
.
iii.
k eyideas and approach overview we develop a uto sc which combines program syntax and type constraints and the naturalness principle of source code in the process of code statement completion.
first we use an lm on an abstraction level higher than lexical source code to learn to derive the most likely candidate templates for the current statement.
second the candidate templates are syntactically and type validated and concretized into one or more code sequence candidates.
after all we rank the candidate code statements accordingly to their occurrence likelihoods by another lm trained on lexical source code.
to overcome the issues of a lm on oov and capturing high level abstraction of source code we design a template asa sequence of extended annotation code tokens excode for short .
an excode for a token is an annotation representing the token type and or data type if available details in section iv .
for an identifier its excode captures its token type i.e.
avariable a field access a method call a type class etc.
token types in a template helps a lm predict better the next token e.g.
an is needed after a method call.
excode also captures the data type if available.
for example children is of the type nodelist at line .
the data type facilitates a model to restrict possible method calls or field accesses.
however the variable names are not kept in an excode because we want to capture the code pattern at a higher level.
in contrast excode keeps the name of the class that is declared e.g.
nodelist innodelist children the method that is called e.g.
getlength the field that is accessed e.g.
next .
the rationale is that those elements are designed to be re used in different classes methods in the same or different projects e.g.
libraries frameworks .
such reused names would be useful for a model to learn to apply in different places.
the literals are not kept because they tend to be project specific except if they are special literals such asnull or0.
the other kinds of tokens are kept intact.
these treatments help a uto sc learn better the candidate templates.
at line the template has the left hand side of type int var int op assign and the right hand side of var nodelist op acc call nodelist getlength int lp rp.
by raising the abstraction from the code we aim to increase the regularity repetition to help a lm learn from other places to better find the statement templates.
for example while the fragment len children.getlength has never appeared in the project the above template occurs times.
our process of learning templates and concretizing into code helps our model overcome oov and the nature of locally used variable names.
the templates at higher level are learned from one place and applied to another and pa is used to concretize them with concrete accessible variables at the new place.
the step of learning at template level helps a uto sc cover more candidates improving recall while the use of pa helps retain more valid ones improving precision .
to enforce syntax and type constraints we train an lm with the sequences of excode to learn the statement templates and use that lm to suggest each excode byexcode to form the candidate templates.
during that syntactical and type rules are applied to those candidate templates to enforce their validity.
the second lm on lexical source code at the last step helps select the variable names when there still exist multiple candidates of code sequences.
when several valid variables are valid the lexical lm selects the names that come naturally and frequently at the place.
for example the tokens children node parent etc.
often go together.
thus at line the variable name children likely occur than student network etc.
iv .
e xtended code annota tion a. design strategies we present extended code annotation excode a code representation designed for sc.
let us explain what information needs to be encoded.
we first aim to encode token type of a code token.
that is we need to encode whether a code token is a keyword separator operator method call field access variable etc.
this enables a uto sc to learn program syntaxes on the validity of a next code token e.g.
a left parenthesis authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i excode annotation rules for code tokens token role construction rule example code excode keyword to corresponding reserved token if if for for operator o op name o .
op acc op assign separator to corresponding reserved token lp rp data type t type t int type int string type string v ariable v v ar type v len int var int parent unknown var unk literal l lit littype l hello lit string lit int method call mcall typ e m name m argcount m rt m substring call string substring string lp lit int rp field access ffield typ e f name f type f node.name var node op acc field node parent string special literal to corresponding reserved token null null zero empty must appear after the method call next not after the field next .
additionally for the validation of the type constraints thedata type of code tokens especially of method call field access and variable also needs to be encoded.
for example the rhs expression of the assignment at line must be of the type intorinteger because the lhs variable is of the type int.
because local variables are used locally their names and meaning might be different in different methods.
thus they can not be learned by a lm in a method to apply to the local variables but with different variable names in other methods.
thus the names of local variables should be abstracted in the representation to better capture code regularity.
meanwhile the names of data types methods and fields are kept since those elements are designed to be reused in other places.
thus those names can be learned from one place and be applied to others.
b. extended code tokens annotation and concretization definition token type .the token types in a program with regard to a programming language include keyword operator separator data type method call field variable and literal.
for children.getlength the token types of children and getlength are variable and method call respectively while .
access is an operator and and are separators lpand rp.
definition excode token .anexcode token is an annotation corresponding to a code token that represents its syntactic and type information including its token type and data type .
table i shows the rules to construct excode tokens for popular kinds of code tokens.
for children inchildren.getlength which has the role of a variable its corresponding excode token consists of the annotations var its data type nodelist and .
for method calls and field accesses the information including the enclosing type name return type and the arguments are additionally incorporated in the excode tokens.
for example the excode ofgetlength inchildren.getlength iscall nodelist getlength int .
definition excode annotation function .the annotation function c on a code sequence c c1c2...cn defines the corresponding excode sequence e e1e2...en such that eiis the corresponding excode token ofcidefined in table i. sincecis the current partial code to realize we perform partial program analysis using ppa to get token types and data types in a best effort fashion.definition excode token concretization function .the concretization function e v on an excode tokeneand the set vof the accessible variables and fields of the current class of the method defines the set of code tokens as follows e v braceleftbigg v v v type v type e ifeis a variable c otherwise where cis the respective non variable token listed in table i. in figure primevar nodelist prime v children wherev contains the set of accessible global local variables of the method listchildnodes .
note that literals will not be concretized except if they are special literals such as null or0.
definition excode sequence concretization function .the sequence concretization function e v on an excode sequence of length n en e1e2...en in a method and the set of the method s accessible variables and fields v defines a set of code sequences of length n in which each code sequence cn c1c2...cn ci ei v for i .
definition excode expression .in a method having the set of accessible variables v a n excode expression expr is an excode sequence with one or more excode tokens such that there is at least one code sequence cin expr v that is a valid code expression according to the programming language.
in our example the excode sequence var nodelist op notequals null is an excode expression since there exists a concretization to obtain a valid expression children !
null .
definition excode statement .in a method having the set of accessible variables v a n excode statement stm is an excode sequence with one or more excodes such that there is at least one code sequence cin stm v that is a valid statement.
we use excodes to represent a statement template.
for example type int var int op assign var nodelist op acc call nodelist getlength int lp rp is a template.
v. i dentifying candida te templa tes given the partial code p a uto sc first parses pto build the excode sequence e e1e2...en.
it uses the n gram model that is trained on the excode sequences built from a code corpus to predict each excode one by one that most likely followse.
it also uses rules for program constraints to derive the valid candidates of excode tokens and sequences.
the resulting excode sequences represent templates.
let us detail it.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm identifying candidate templates function identify templa tes partialcode project e partialcode def.
genseqs expandexcodeseq e project templs extractremainingparts genseqs e returntempls function expand excode seq exseq proj ifisended exseq reachmaxlen exseq then return exseq c getvalidnexttoken exseq proj def.
ifc then return topcands rank c exseq excode k form.
exsequences for allcand topcands do newseq concat exseq cand newtempls expandexcodeseq newseq proj exsequences.addsall newtempls returnexsequences a. training n gram lm with excodes to predict next excode to predict the next excode any statistical lm is applicable .
without loss of generality we use ngram lm .
the model is trained on the excode sequences built from a corpus.
for prediction given eand an excode candidate epsilon1 the likelihood that epsilon1is the next excode token followinge is estimated using the trained n gram lm excode p epsilon1 e excode e1e2...en epsilon1 b. deriving the next excode sequence for statement template next using excode a uto sc identifies the most likely valid excode one at a time and then composes them to obtain the candidates for statement template.
specifically algorithm shows how a uto sc identifies candidate templates.
in this algorithm the partial code is first parsed into the corresponding excode sequence line .
the next sequences are suggested by expanding the excode sequence token by token until encountering the end statement token or the length of the expanded sequence reaches the pre defined maximum length of code statements lines .
for each expansion step a uto sc applies the syntax rules and accessibility rules will be explained later to enforce program constraints.
the set of valid candidates of the next excode token is stored in c. then it selects the top k predefined value most likely tokens line .
these excodes are concatenated with eto form new candidates that are recursively expanded lines .
c. enforcing syntax rules and accessibility rules to decide the candidates for the next excode token a vocabulary vis a set of all distinct excode tokens.
since code has strict syntax and semantics for excode sequence e the valid next excode token following eis restricted by program constraints rules syntax rules and accessibility rules .definition program syntax rule .given the excode sequencee e1e2...en the vocabulary vof all excode tokens a program syntax rule rsyntax when applying on ewill return a set s ofexcode tokens in the vocabulary such that the resulting excode sequence e prime e1e2...en epsilon1does not violate a syntax rule of a programming language.
mathematically a program syntax rule rsyntax is a relation r v 2v rsyntax e s where s vis the set of tokens such that epsilon1 s e prime e1e2...en epsilon1does not violate a syntax rule.
for example the code int len has the excode sequence of type int var int op assign .
the excode tokens op assign and op acc are excluded from rsyntax e because an or .
cannot occur after the sign.
in this example rsyntax e includes literal variable method call field access data type prefix operators or open parenthesis.
note that to check for epsilon1 instead of checking all syntax rules one prime e1e2...en epsilon1at each expansion step for efficiency we could check the validity of epsilon1based on the last token en and finally check on the syntactic validity of entire sequence at the last step when the end of statement is reached.
definition accessibility rule .given the excode sequence e e1e2...en the vocabulary vof all excode tokens an accessibility rule raccess when applying on ewill return a set a vof the excode tokens in the vocabulary that are accessible at the current state of e. that is raccess is a relation raccess v 2v raccess e asuch that aincludes the excode tokens which correspond to the following cases all declared local variables within the current scope are valid.
in figure accessible local variables are var node var nodefilter var nodelistimpl and var nodelist .
all the accesses to the fields and the calls to the methods in the enclosing class are accessible.
the accessible field accesses and method calls of a variable.
for example for a sequence eending with var nodelist op acc all accessible field accesses and method calls in nodelist are included in raccess e .
all data types and literals are valid.
all keywords separators and operators are valid.
definition v alid next excode token .for a sequence a excode token is considered as valid if it satisfies all syntax rules and accessibility rules .
that is given an excode sequence e e1e2...en the set of valid candidates cisrsyntax e raccess e for all syntax rules and accessibility rules.
vi.
v alida ting candida te templa tes fully semantic checking with respect to the current programming language e.g.
java is always desired.
however it is impossible to do so for the candidate templates which are expressed as the sequences of excode tokens and do not contain concrete lexemes of variables.
because our design is to have excodes contain data type information we focus on performing type checking.
with type checking we can eliminate a large number of templates with incorrect and inconsistent types.
in general one could use a type checker for the current programming language e.g.
java type checker.
however we are authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii key type check rules for excode sequences syntax type type check e t excode seqs ofe literal e lit t t e.g.
lit string string v ariable e var t t e.g.
var int int orvar unk unk assignment e1 t1 e2 t2 e e1op e2 t1 t2 t1if t1!
unk and t2!
unk t1 else if t1!
unk t2 else if t2!
unk unk ift1 t2 unk prefix op e t e op op e bool if op not and t bool o r t unk num otherwise t num o r t unk num types include char short int etc.
postfix op e t e eop op num if t num o r t unk num types include char short int etc.
comparison e1 t1 e2 t2 e bool if t1 t2 o r t2 t1 e1op op e2 or t1 unk o r t2 unk infix e1 t1 e2 t2 e t2 if t1 t2 !
unk e1op op e2 t1 else if t2 t1 !
unk t1 else if t1!
unk t2 else if t2!
unk unk else if both are unk method call e t e1 t1 ... en tn e eop acc decl rt t m t1 p1 t2 p2 ... tn pn call t m n rt rt if ti ti o r ti unk for i ..n lpe1 ... enrp constructor call e t e1 t1 ... en tn e decl t t t t1 p1 t2 p2 ... tn pn ccall t t n t t if ti ti o r ti unk for i ..n lpe1 ... enrp field access e t e eop acc ft field t f ft statement v ariable decl e t e var t t if t t o r t unk forstmt s void i1 ti1 ... in tin for i1 ... in e e t t bool ort unk u1 ... um s u1 tu ... um tu m s1 t1 s while e s1 void e t t bool ort unk s1 t1 s if e s1 e t t bool ort unk s1 t1 s2 t2 exprstmt s e t e t block s s1 .
sn void e1 t1 ... en tn return s return e t e t decl rt t m t1 p1 t2 p2 ... tn pn t rt dealing with partially complete code and there are potentially program entities whose types cannot be resolved by ppa .
in those cases the variables without type information are annotated with unknown type.
thus we build a type checker forexcode with the accommodation of the unknown type.
auto sc performs type inference at the same time as type checking on excode statements and expressions using the rules in table ii.
the process of type checking is similar to type checking for the source code in java.
however there are two key differences.
first it works at the excode statements expressions corresponding to the statements expressions at the source code level note variables names are not there .
second due to unresolvable types a uto sc has to consider unknowntype in a flexible manner e.g.
that type does not violate any subtype constraint.
let us explain the key type checking rules.
.
literal.
when seeing the excode lit t that represents a literal with a type t we consider tas its type.
.
variable.
when seeing a var if the type of excode is available we use it.
otherwise the resulting type is unknown .
.
assignment.
the lhs and rhs expressions are typechecked first.
if both types are known the type of rhs must be a subtype or equal to the type of lhs.
if either of them are unknown we consider the assignment as valid with the known type.
if both are unknown the resulting type is unknown .
.
prefix.
if the operator is a negation and if the type of eis available it must be boolean otherwise it must be convertible to a numeric type char short int etc.
the resulting type is boolean or a numeric type accordingly.
if the type of eis unknown the result depends only on the operator table ii .
.
postfix.
the type of emust be convertible to a numeric type or it is unavailable.
the resulting type is numeric.
.
comparison.
the type of one side must be a sub type or equal to the type of the other side or the type of at least one of them must be unknown .
the resulting type is boolean .
.
infix.
both expressions on two sides need to be typechecked.
if both types are not unknown the type of one side must be a subtype or equal to the other and the expression is assigned with the super type.
if the type of one of the two sides is unknown the expression is assigned of the type of the known one.
otherwise the type of the expression is unknown .
.
method call.
the expressions for the receiver and the arguments need to be type checked first.
the type of each argument if available must be a subtype or equal to the type of the corresponding formal parameter in the declaration of the method.
the return type is used as the type of the call.
.
constructor call.
a constructor call is handled similarly as a method call except that the declared type is used and the method name is the same as the class name.
.
field access.
the receiver needs to be type checked.
the class of the field must be the same as the respective type stored in the excode .
.
variable declaration.
the rhs expression if any needs to be type checked and its type if available must be a subtype or equal to the type stored in the excode var t .
.
for while if statement.
the components in the excode of such a statement need to be type checked.
the conditional control statement must be of the type boolean orunknown .
.
expression block statement.
each statement in each of those compound statements needs to be type checked.
.
return statement.
the expression needs to be typechecked and its type must be a subtype or equal to the return type of the enclosing method.
definition type correct candidate template .given an excode sequence erepresenting the current partial code the template t as an excode sequence is considered as a typecorrect candidate template if the sequence concatenated by e andtis type checked by our rules.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
in figure both candidates 0and var nodelist op acc call nodelist getlength int lp rp are type correct.
vii.
c oncretizing sta tement templa tes and ranking code candida tes this section describes how the type correct candidate templates as excode sequences are converted to code candidate sequences with the accessible variables in the current scope.
the most likely code sequences are ranked based on their occurrence likelihoods computed by an lm.
let us detail it.
algorithm concretizing candidate template function concretize templ v codecands concretizenext templ v returncodecands function concretize next templ v currcands i ifi l e n templ then returncurrcands codecands codetokens templ v def.
ifcurrcands then for allt codetokens do newcand concat empty seq t codecands.adds newcand else for allt codetokens do for allcand currcands do newcand concat cand t codecands.adds newcand returnconcretizenext templ v codecands i concretization.
algorithm shows our procedure.
each excode token is converted into code tokens using function def.
.
these tokens are used to initiate a set of code sequences lines or concatenated with the current concretized code sequences to create the new ones lines .
the process recursively continues until the end of the template.
training an lm on lexical code and ranking candidate statements.
to rank the candidate code statements we train ann gram model lexemes on the lexical forms of the source code in a corpus.
for training all source files are tokenized based on naming conventions camelcase and hungarian and the obtained tokens are normalized to lowercase.
the trained lm is used to estimate the occurrence likelihoods of the code sequence that is concatenated from the current code and the candidate statement.
that is given the current codec the likelihood of the candidate statement is lexemes concat clexemes lexemes where clexemes and lexemes are the lexical forms of cand respectively.
viii.
e mpirical methodology we have conducted several experiments to empirically evaluate a uto sc in statement completion.
for that we seek to answer the following research questions table iii large corpus project files statements unique tokensa vg tokens in a statement ant .
batik .
cassandra .
log4j .
lucene .
maven2 .
maven3 .
xalan j .
xerces .
table iv small corpus training data test data files methods statements unique tokens a vg tokens in a statement .
.
rq1 accuracy and comparison .
how accurate is a uto sc incurrent statement completion and next statement suggestion ?
how is it compared with the state of the art tool pcc ?
rq2 intrinsic accuracy .
how accurate is a uto sc in completing code statement on various factors including code sequences lengths and code token types?
rq3 sensitivity analysis .
how do various factors affect our model e.g.
completion position thresholds and data s sizes?
rq4 time complexity .
what is our training testing time?
a. subject systems in this study we collected the same data set of java projects used in the existing studies in code completion table iii large corpus .
in the dataset the average number of code tokens in a statement is .
whereas more than of the code statements contain less than code tokens.
for comparison on next statement ns suggestion we also used the same dataset as in pcc small corpus in table iv .
in the training data of statements contain less than tokens.
the test dataset contains only individual files without their projects.
both training and test data are much smaller than our large corpus.
in our experiments to balance between the completion effectiveness and efficiency we set the maximum number of tokens in a statement of .
b. evaluation setup procedure and metrics we used the same setting with data across projects as in existing work .
that is we divided the source files of a project into equal folds.
we performed fold cross validation each fold was chosen for testing while the remaining folds and other projects were used for training.
accuracy on statement completion is measured as follows.
for a method in a source file in the test data our evaluation tool traverses its code sequentially from the beginning.
at a positioniin a method with a code sequence mn c1c2...cn a tool computes the top kmost likely code sequences s1 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
s2 ... sk for the remaining of the current statement based on the previous code sequence from the start of the method to the position i c1c2...ci .
if the actual code sequence fromito the end of the current statement siat the position t is among the above ksuggested sequences we count this as ahit.
the top kaccuracy is the ratio of the total hits over the number of tokens.
top kaccuracy for a project is computed on all positions of its methods in cross validation.
note that for the compound statements including if then if then else switch for while and do while statements we run a model to complete suggest their control expressions.
moreover at a local variable declaration statement a uto sc suggests a placeholder and consider it matching with the actual name because any new name can be used at that point.
to compare a uto sc with pcc in statement completion sc on large corpus we use the following sc setting that works for pcc which is aimed to suggest the next statement only section x .
at a position i the previous code sequence is divided into c1 c1c2...ctandc2 ct 1ct ...ci wheretis the ending position of the nearest completed statement.
c1is used as the input of pcc to suggest the next statement.
we collected into the list of the resulting suggestions the top kmost likely code statements from pcc that begin with c2.
among the list if there exists a statement that is the actual code sequence we count this as a hit.
to compare a uto sc with pcc in next statement ns suggestion on small corpus we uses the same setting as in pcc .
that is instead of traversing source code sequentially token by token we ran a uto sc and computed the top kaccuracy only at the beginning position of every code statement.
however in small corpus the test data includes individual files without containing the corresponding projects files.
meanwhile a uto sc is designed using program analysis on the code of currently developing projects.
therefore we created dummy projects for each of the testing files.
ix.
e mpirical results a. accuracy comparison rq1 comparative results of sc on large corpus we compared a uto sc with pcc which applies a statementleveln gram lm and searches for similar statements for next statement completion will be detailed in section x .
we also compared a uto sc against two baseline approaches n gram lm and n gram pa. for n gram lm we trained a n gram lm and used it to predict the next token by token and rank the candidates for the code sequence according their occurrence likelihoods.
the n gram pa model works similarly to n gram lm except that pa is additionally applied to filter out the invalid candidate sequences.
then the valid ones is ranked.
we used the grams for both n gram lm and n gram pa. in a uto sc sn gram lms excode and lexemes n .w e did not compare with a model that solely uses pa since it generates a huge number of equally ranked candidates.
as seen in table v the top accuracy for a uto sc is .
.
.
that is up to out of requests users could find their expected next code sequence for the current statement at thetable v code statement completion accuracy project top k auto scn gram lmn gram l m p apcc anttop top .
.
.
.
.
.
.
.
batiktop top .
.
.
.
.
.
.
.
cassandratop top .
.
.
.
.
.
.
.
log4jtop top .
.
.
.
.
.
.
.
lucenetop top .
.
.
.
.
.
.
.
maven 2top top .
.
.
.
.
.
.
.
maven 3top top .
.
.
.
.
.
.
.
xalantop top .
.
.
.
.
.
.
.
xercestop top .
.
.
.
.
.
.
.
top of our ranked list.
for pcc the top accuracy is from .
.
that is 9x 69x lower than a uto sc s .
meanwhile lexicaln gram achieves only from .
.
.
even when we used pa to filter out invalid suggestions the top accuracy is still very low .
.
that is more than 100x lower than a uto sc s top accuracy.
for top accuracy a uto sc also achieves up to .
which is 14x and more than 50x higher than pcc and both n gram lm and n gram pa. there are two key reasons for their low accuracy.
first code statement as its entirety is relatively project specific .
indeed on average the portion of the code statements in a project that can be found in others is only .
.
that leads to the low accuracy of pcc which relies on the repetition of entire code statements.
second for n gram baselines because the next sequence is suggested by predicting next token one at a time the accuracy of next sequence suggestion is affected by the confounding effect of the accuracy of a single nexttoken suggestion.
the highest top accuracy of an n gram lm for next code token suggestion is about .
.
therefore for predicting a next code sequence containing tokens on average the maximum top accuracy is .
.
.
note that in this experiment we used large corpus and the statement completion sc setting that are different from small corpus and the next statement ns setting used in pcc .
thus this leads to a different accuracy for pcc than the one reported in its paper .
comparative results of next statement ns suggestion on small corpus as seen in table vi a uto sc does not perform next statement suggestion as good as pcc.
the main reason is that the test set contains individual java files without the project specific files and information such as the fields and methods of the classes.
thus the components of auto sc relevant to program analysis e.g.
identifying the valid candidates for the next token and type checking cannot be performed as expected.
analysis we analyzed the correct results and found that a uto sc s high accuracy can be attributed to the fol717 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table vi next statement suggestion accuracy top top top top auto sc .
.
.
.
pcc .
.
.
.
1list string cfnames new arraylist string 2set stores getvalidcolumnfamilies columnfamilies 3for columnfamilystore cfstore stores 4cfnames.add cfstore.getcolumnfamilyname figure a partial method in cassandra lowing.
first it uses excode to derive the template at a higher abstraction level .
this helps a uto sc learn code patterns from other locations and avoid missing the potential candidates for the next sequence for the given partial code.
for example given a partial statement which starts with cfnames.add at line of figure where cfnames is a list string its expected next sequence is cfstore.getcolumnfamilyname where cfstore is a local variable.
the partial statement and the expected sequence both have never appeared in the training data.
this makes the baseline models which work on lexical tokens fail.
meanwhile the excode sequence corresponding to cfnames.add and that excode sequence for cfstore.getcolumnfamilyname cooccur several times.
thus var columnfamilystore op acc call getcolumnfamilyname ...lp rp rp corresponding to the code sequence cfstore.getcolumnfamilyname is listed in the set of about candidate templates.
additionally the application of pa to filter out the typeincorrect templates help auto sc achieve high accuracy .i n the above example the set of valid candidates among candidates that are learned from other places are adapted to fit with the current context using pa. for example auto sc concretized var columnfamilystore op acc ...lp rp rp by using the accessible variable cfstore for excode token var columnfamilystore instead of cfs orfilter as in other models.
this adaptation ability to the current method with pa is the third reason for auto sc s high accuracy .
another reason for our high accuracy is that inauto sc oov is addressed by enforcing accessibility rules to avoid missing the valid file specific or project specific tokens when producing the candidate templates.
finally a uto sc leverages the naturalness of source code in the lexical form to effectively rank the candidate code sequences.
in figure given a partial statement starting with reports.addall where the type of reports islist the expected sequence is getreportexecutions .
in fact method getreportexecutions is declared inside the current class and has never been seen in the training data.
this accessible call is still used to produce template.
since the type restriction for the argument of method addall there are a few type valid candidates such as getreportexecutions ornull .
then the candidate getreportexecutions is ranked on the top by the1list mojoexecution reports new arraylist 2reports.addall getreportexecutions figure a partial method in apache maven figure accuracy on length of remaining code sequences lexeme based lm leximes because the tokens reports and report ingetreportexecutions frequently go together such as reports.contains report and reports.add reportmojo .
we further studied the cases in which a uto sc did not suggest well.
we found that the majority of them are the cases whose the completion position is near the beginning of the current statement especially the cases of suggesting entire statement will be explained in section ix c1 .
since the nexttoken prediction accuracy is not the more next tokens predicted the lower next sequence completion accuracy.
b. intrinsic evaluation results rq2 we further studied the complexity and diversity and a utosc s effectiveness on different kinds of code tokens and different lengths of the statements completed by a uto sc.
we randomly sampled results from 460k total results.
first we classified the sampled results into categories corresponding to the size tokens of the remaining code sequence of the currently completed statement the maximum number of tokens to be completed is set to .
figure shows the number of correct results over the total number of results for each category.
as expected the longer the remaining sequence the more number of tokens to be completed the less number of correct results.
as seen a uto sc correctly handles complex completed statements with various lengths.
also through the similar shapes of two types of columns from left to right we see that the proportions of correct results over the total ones for all categories are quite uniform.
thus our model is effective for various lengths of the remaining sequences even for long sequences .
a correct example in maven is as follows.
the given partial code is the fragment activ activ new activ .
the correct suggestion is activ.setactivebydefault settingsactivation.isactivebydefault which has a total of tokens after the cursor.
second to study the results by a uto sc with respect to different kinds of tokens we classified all the tokens in authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
figure accuracy on various token types table vii impact of completion points on accuracy location top top top top top 1st quartile .
.
.
.
.
2nd point .
.
.
.
.
3rd quartile .
.
.
.
.
the sampled results into several categories corresponding to different syntactical token types.
as seen in figure theproportions of the correct results over the total ones for allcategories are relatively similar.
thus a uto sc is equally effective for diverse kinds of tokens.
a correct example is aconditional expression null file.isdirectory containing a null literal infix operators identifiers and separators.
for a partial condition of an if then statement if jarfile .
c. sensitivity results rq3 completion position because a uto sc is based on the given code sequence a completion point in the code sequenceof a method m n c1c2...cnhas impact on accuracy.
thus we conducted an experiment to measure that.
we first chosea random project lucence .
for each method we chose a completion point at three locations the first quartile pointl n the middle point l2 n and the third quartile point l3 3n .
as seen in table vii accuracy slightly increases if we move the point to a later part of a method from 1stto3rdquartile point.
this is expected as a uto sc has more information.
we also computed the accuracy as the completion points at the beginning of a new statement i.e.
next statement suggestion .
the percentage of the cases in which the nextstatement being correctly ranked on the top of the suggestionlist is .
top accuracy not shown .
this is expectedbecause any type of statement is valid at those beginning points.
however a uto sc s accuracy is still .9x better than top accuracy in next statement suggestion of pcc .
threshold k auto sc also relies on the pre defined numberkof most likely tokens for next excode to identify templates.
figure shows the accuracy and running time per completion request when we varied k. as seen when kis small the accuracy are very low because the correct next excode token might be dropped out of top k. the figure impact of threshold kon accuracy and running time table viii impact of ninn gram lms on accuracy n top .
.
.
.
.
running time 1095ms 2287ms 3169ms 4388ms 5447ms accuracy increases when we keep a larger number of topcandidates.
however with a large k k the number of the predicted code sequences is very large.
this lead to aslower increasing trend as kis larger.
regarding the running time since the number of the predicted code sequencesexponentially increases when we increase k the running time for each request also grows exponentially when kis larger.
v alue of nin then gram lms we also measured the impact of the size nin then gram lms excode and lexemes on a uto sc s accuracy.
we varied nfor both excode and lexemes from and computed top accuracy when we ran a uto sc on a randomly selected project.
as seen in figure viii the accuracy grows from .
to .
for n .
the reason is that the n gram lms with largernis able to capture more precisely the current context and rank better the correct next excode tokens for excode and the correct next code sequences for lexemes .
meanwhile the running time for each completion request increases linearlyfrom 095ms to 447ms because longer sequences need tobe computed as nis increased from to .
training data s size for training data s size we randomly selected a project ant and divided its source files into folds.
we used one fold for testing and increased the sizesof the training data by adding into a dataset of other projectsone fold at a time until remaining folds are added.
top 1accuracy increases from .
to .
when we increasetraining data table ix .
as expected with larger training datasets the model has observed more and performs better.
d. time complexity rq4 all experiments were run on a windows with intel xeon .7ghz 32gb ram.
a uto sc took minutes for training.
the average running time for a request is .5s.
on average in the results in which the remaining code sequence is in top ofthe ranked list the average number of tokens in the remainingcode sequences is .
tokens.
this equals the typing speed of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ix impact of training data s size on accuracy folds top .
.
.
.
.
top .
.
.
.
.
about .
tokens per sec which is slightly slower than the average human typing speed .
tokens sec .
e. ineffective cases for incorrect cases we classified them into the categories based on their number of code tokens that are in the remaining of the expected sequences.
we found that the portion of the cases which contain redundant tokens is up to .
for the percentages of the cases of and missed tokens are and respectively.
meanwhile the portion of the cases of more than missed tokens is only .
for example the correct one is commits.get readfrom the suggested one is commits.get writet o for the given partial code commits .
thus these results show that even for the ineffective cases auto sc s suggestion lists are still reasonable .
f .
threats to v alidity our selected projects are not representative and different from pcc s dataset.
however we chose a high number of projects with large numbers of statements.
for pcc we used its default setting for the comparison.
our simulated code suggestion procedure is not true code editing.
inaccuracy is from the fact that a uto sc cannot correctly resolve types roles sometimes due to incomplete code.
x. r ela ted work auto sc is related to pcc by yang et al.
.
in comparison there are fundamental differences between a uto sc and pcc.
first pcc focuses on suggesting the next statement when a user finishes the previous statement while a uto sc supports both filling a partially typed statement sc and generating a next statement ns .
pcc can be used to support statement completion when the partially typed statement is matched against the suggested statement s and the remaining tokens of swill be recommended for users.
second while pcc is based solely on statistical lm a uto sc combines pa and lm.
third the way pcc used an lm is also different.
pcc combines all lexical tokens belonging to a statement into a pseudo token called ir for the statement.
in training it converts source code into sequences of irs and trains a n gram model to learn to recommend an entire statement.
because the entire statements do not repeat often pcc has to consider similar irs as the same causing inaccuracy.
a uto sc uses lm pa to predict token by token and compose them.
we showed that a uto sc outperforms pcc in both sc and ns.
there exists a rich literature of approaches on cc.
the approaches can be broadly classified into the following categories.
the first category relies on program analysis.
ides support the completion of method calls field accesses.
eclipse and intellij idea also support template based completion for common constructs and apis for while iterator .the second category uses code pattern mining .
grapacc uses api patterns to match them against the current code.
bruch et al.
suggest a call based on frequent methods co occurrent calls and best matching and their calling structures.
the third category relies on statistical lms .
hindle et al.
usen gram on lexical tokens to predict the next token.
later tu et al.
improve n gram model with caching for recently seen tokens.
raychev et al.
use n gram to predict api call.
slamc associates code tokens with sememes including token roles and data types.
in comparison there are key differences.
first excode is designed for template statements while sememes are abstractions over source code to predict the next token.
second a uto sc has a type checker forexcode with unknown type while sememes do not have it.
third n gram topic model is used in sememes to provide the context for prediction while a uto sc uses pa lm.
finally slamc suggests only the next token.
gralan is a graphbased lm that captures usage patterns to suggest api calls.
recent advances in deep learning have been used in next token suggestion.
white et al.
use recurrent neural network rnn to learn the context to predict the next token while dam et al.
rely on lstm.
dnn4c incorporates syntactic information for better prediction using dnn lm .
despite the success of using statistical lms those existing approaches are still limited to support only next token.
they do not combine lm with pa as in a uto sc.
xi.
c onclusion we introduce a uto sc which combines pa and the principle of software naturalness complete partial statements.
we aim to benefit from the strengths of both directions.
auto sc is trained on a code corpus to learn the candidate templates.
then it uses pa to validate and concretize the templates into valid code statements.
finally they are ranked by using a lm trained on the lexical form of the source code.
we conducted several experiments to evaluate a uto sc in statement completion and next statement suggestion on datasets with 460k statements with a total of 1m suggestion points.
our results show that a uto sc is very effective with top accuracy of and top accuracy of .
on average.
that is in out of cases when a user requests to complete his her currently written statement s he can find the remaining of the desired statement in the top of the suggestion list.
importantly a uto sc significantly improves over the baseline model using only n gram on lexical code up to 142x in top accuracy and the model using lexical n gram pa up to 117x in top accuracy .
it also improves over the state ofthe art tool pcc with 69x higher in top accuracy.
acknowledgment this work was supported in part by the us national science foundation nsf grants ccf ccf twc1723198 ccf and cns .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.