what developers want and need from program analysis an empirical study maria christakis christian bird microsoft research redmond usa mchri cbird microsoft.com abstract program analysis has been a rich and fruitful eld of research for many decades and countless high quality program analysis tools have been produced by academia.
though there are some well known examples of tools that have found their way into routine use by practitioners a common challenge faced by researchers is knowing how to achieve broad and lasting adoption of their tools.
in an e ort to understand what makes a program analyzer most attractive to developers we mounted a multi method investigation at microsoft.
through interviews and surveys of developers as well as analysis of defect data we provide insight and answers to four high level research questions that can help researchers design program analyzers meeting the needs of software developers.
first we explore what barriers hinder the adoption of program analyzers like poorly expressed warning messages.
second we shed light on what functionality developers want from analyzers including the types of code issues that developers care about.
next we answer what non functional characteristics an analyzer should have to be widely used how the analyzer should t into the development process and how its results should be reported.
finally we investigate defects in one of microsoft s agship software services to understand what types of code issues are most important to minimize potentially through program analysis.
ccs concepts general and reference !empirical studies software and its engineering !software defect analysis keywords program analysis code defects .
introduction large software companies have recently started building program analysis ecosystems like google s tricorder permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
isbn .
.
.
.
microsoft s cloudbuild .
these ecosystems allow for distributively running several analyzers each with its own attributes like speed of the analysis type of detected code issues or number of true or false positives.
designers of such ecosystems need to decide which analyzers should run and when e.g.
in the editor as part of the build or during code review.
but how should the decisions be made?
which kinds of program analyzers are valuable to software engineers rather than a waste of time?
how do they t in the development process?
how should their results be reported?
so far much research and many studies on program analysis tools have focused on the completeness of these tools do they report spurious warnings?
their soundness do they miss bugs?
automation performance annotation overhead and modularity.
however as companies integrate program analyzers as part of their development process more investigation is needed into how these tools are used in practice and if practitioners needs are being met.
we posit that for research in this area to be impactful our community must understand the practices and needs of software developers with regard to program analysis.
in an e ort to improve this understanding our paper contains an empirical investigation at microsoft to answer the following high level research questions.
.what barriers hinder the adoption of program analyzers by practitioners?
.what functionality do practitioners want from program analyzers?
.what non functional characteristics should a program analyzer have to be widely used?
.what code issues occur most in practice that program analyzers should try to detect?
for our purposes we de ne program analysis as the process of automatically analyzing the behavior of a program without running it that is we are only considering static program analysis.
program analysis detects potential issues in the code and gives feedback.
feedback is in the form of warnings that are either true or false positives.
true positives ag real issues in the code whereas false positives warn about code issues that do not occur in practice.
we do not consider the compiler to be a program analyzer to only focus on tools whose primary functionality is program analysis and that are not by default part of the software development process.
our study comprises a number of investigative techniques.
we interviewed and surveyed developers from a diverse group of products to understand their needs and how program permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
analyzers can or do t into their process.
we also examined many corrected defects to understand what types of issues occur most and least often.
we expect the empirical results that we present here to shed light on many aspects of program analysis speci cally on what tools should be integrated in the development process where tool designers should focus their e orts what developers like and dislike in analyzers what types of code issues are most often encountered and what project managers should expect from di erent bug nding techniques.
we assert that by understanding the above the program analysis research community can focus on analyzers that are most amenable to real world use.
for researchers our ndings also provide a view into today s industrial realities with respect to program analysis.
.
survey in an e ort to understand developers perspectives on program analyzers we deployed a broad survey across microsoft.
surveys are bene cial because they allow researchers to elicit answers to the same set of questions from a large sample of some population.
in our case we are interested in industrial software developers.
our goal is to obtain a large enough sample such that responses are representative of the population and that quantitative analysis can nd results with statistical signi cance if indeed there are signals in the responses .
surveys have been used in empirical software engineering investigations many times in the past to provide insight .
.
data and methodology we used kitchenham and p eeger s guidelines for personal opinion surveys in software engineering research when designing and deploying our survey .
we followed a pilot and beta protocol when developing the survey.
we started by identifying the high level goals for our investigation uncover any obstacles in the adoption of program analyzers by developers.
understand how practitioners use program analyzers today and what functionality they nd desirable.
identify the non functional characteristics that developers want in a program analyzer.
determine how program analyzers should t into developers current practices.
from these goals we derived an initial set of survey questions.
to pilot our questions we scheduled interviews with ve developers across microsoft and administered our survey questions in person.
this allowed us to gauge if each question was clear enough or should be altered if the terms we used were familiar to developers and if the questions we asked were actually eliciting answers that helped us achieve our goals for the survey.
after updating the questions following these interviews we created a beta of our survey that we deployed to developers randomly selected across the company.
this initial survey included additional questions at the end asking participants if they found any portion of the survey di cult to understand or answer and asking if they had any other relevant information to share about the topic.
we received20 responses to this survey.
these responses were solely used to improve the survey itself and were not included in subsequent data analysis presented in this paper.
we then made improvements to the survey based on responses to the beta.
an example of such changes included de ning terms such as aliasing and purity more clearly.
in another case we had a question with check boxes that asked developers which types of code issues they would like program analyzers to detect.
this question was changed so that developers had to create a ranking of the types of issues in the beta some developers checked almost all of the boxes making the answers less informative.
a few of our questions were open ended for example why did you stop using program analyzers?
and the responses to the beta showed that there were clear categories in the answers.
for these we changed the question to a multiple choice format that included each of the categories and we added a write in option if the respondents answer did not t into one of these categories.
such changes allow analysis to scale with large numbers of responses.
we also made changes to the survey to ensure that it did not take too long to answer as long surveys may deter participation.
our goal was for the survey to take a respondent approximately minutes to complete.
after nalizing the questions we sent invitations to answer the survey to developers selected at random across all of microsoft.
the survey was anonymous as this increases response rates and leads to more candid responses.
as incentives have been shown to increase participation respondents could enter themselves into a ra e to win four amazon gift cards.
we received responses to the nal survey yielding a response rate.
other online surveys in software engineering have reported response rates from to .
the median time to complete the survey was and a half minutes quite close to our goal of minutes.
we report the median rather than average because there were some outliers that skew the average one person had a completion time of just under ve days!
.
the range of years of development experience was from zero to with a median of nine mean of .
.
the survey questions responses and analysis scripts can be accessed at .
.
results we break our results down into three categories.
first we look at the barriers to using program analyzers and the reasons why developers stop using them.
second we examine the functionality that the developers answers indicate they want in program analyzers.
this functionality includes the types of issues that program analyzers catch the types of programming languages they can analyze whether the analyzer examines a whole program or changes and if the developer can direct the program analyzer toward parts of the code.
third we look at the non functional characteristics that a program analyzer should have.
this includes attributes such as the time required for analysis how many false positives it should yield when it should run where the output should be and what form it should take and where the analysis should t into the development process.
in addition for most questions we break down our answers by attributes of the respondents.
from our interviews and based on anecdotal evidence we believe that developers who have at least a basic understanding of program analysis 333may have di erent views about the topic than those who are not familiar with it.
for the context of this paper we label these developers experts .
of respondents were at least familiar with program analysis.
in addition security issues are especially important to software companies and security is often given high priority by development teams.
in the research community security is a signi cant subarea in program analysis that receives a large amount of attention.
we refer to developers who indicate that security is a top concern to them as security developers .
of respondents indicated that they are security developers.
for many questions we examine the answers provided by developers who are familiar with program analysis and also by those who indicate that security is a top concern for them.
we report cases where there is a statistically signi cant di erence between these groups and the answers of the rest of the sample.
in cases where there are only two alternatives e.g.
using program analysis versus not using it we use a fisher s exact test .
when there are more than two choices such as the frequency of running program analysis we use a 2test to assess the di erence in distributions between these groups.
some of the questions on our survey asked developers to select and rank items from a list.
for example we asked developers to rank the pain points they encountered using program analysis as well as the code issues that they would like program analyzers to detect.
to analyze the answers for each option o we compute the sum of the reciprocals of the rank given to that option for each developer dthat responded d2d weight o x d2d1 rank d o ranks start at one the option with the greatest importance and go up from there.
if an option is not added to the ranked list by a developer the option is given a weight of zero for that developer.
in section we also give an overview of the program analyzers that the survey respondents use the most.
.
.
what makes program analyzers difficult to use?
in our beta survey we asked developers what pain points obstacles and challenges they encountered when using program analyzers.
we then examined their responses to create a closed response list of options.
in the nal survey we asked developers to select and rank up to ve of the options from the list.
figure shows their responses and gives insight into what developers care about most when using program analyzers.
many of our ndings such as the fact that false positives and poor warning messages are large factors are similar to those of johnson et al.
their work investigates why software engineers do not use static analysis tools to nd bugs through a series of interviews see section .
the largest pain point is that the default rules or checks that are enabled in a program analyzer do not match what the developer wants.
developers mentioned that some default program analysis rules such as enforcing a speci c convention for instance hungarian notation to name variables or detecting spelling mistakes in the code or comments are not useful and on the contrary they are actually quite annoying.
mitigations to this problem may include identifying a small key set of rules that should be enabled rather than having all rules enabled which is often the case or making the not cross platformmisses too many issuesno support for custom rulescan t handle all language featurescomplex user interfacecan t selectively turn off analysisno ranking of warningsno suppression of warningsbad visualization of warningsdifficult to fit into workflowno suggested fixestoo slowtoo many false positivesbad warning messageswrong checks are on by default 80pain points using program analyzersfigure pain points reported by developers when using program analyzers.
process of selecting the rules and checks that are enabled easy for developers.
just as helpful is knowing the pain points at the bottom of the list.
developers care much more about too many false positives than about too many false negatives misses too many issues .
one developer wrote of their team s program analyzer so many people ignore it because it can have a lot of false positives .
also the ability to write custom rules does not appear important to many unlike in the investigation by johnson et al.
.
we also asked developers if they had used program analysis but stopped at some point.
only of respondents indicated that they fell into this category.
when asked why they stopped there were three main reasons.
indicated that the reason was because the team policy regarding program analysis changed so that it was no longer required.
similarly indicated that they moved from a company or team that used program analysis to one that did not.
another reported that they could not nd a program analyzer that t their needs about half said this was due to the programming language they were using.
this highlights one aspect of adoption of program analyzers that we also observed in discussions with developers often their use of analyzers or lack thereof is related to decisions and policies of the team they are on.
program analysis should not have all rules on by default.
high false positive rates lead to disuse.
team policy is often the driving factor behind use of program analyzers.
.
.
what functionality should analyzers have?
one of the primary reasons why a program analyzer may or may not be used by a developer is whether the analyzer supports the programming language or languages that the developer uses.
we therefore asked developers what languages they use in their work.
because the list was quite long we aggregated responses into programming language categories as shown in figure .
the primary languages in the 334functionalassemblydynamic scriptingjavascriptlegacy compiledoop 100percent of developers using each languagefigure languages used by developers.
object oriented programming oop category include c and java.
legacy compiled languages comprise c c and objective c. dynamic scripting languages include python ruby and perl.
we break out javascript and variants such as typescript because it is unique and one of the fastest growing languages.
our categorization is based on our perceptions and experiences as well as observations of microsoft development.
as such it is one of many reasonable categorizations and is somewhat subjective e.g.
c is technically an oop language but it has been around much longer than java or c and is used for more low level functionality .
next we examine the types of code issues that developers consider to be important and that they would like program analyzers to detect.
in our initial beta survey we allowed developers to write in any type of issue that they deemed important.
we then grouped the responses from the beta leading to the types shown in figure and asked about the importance of each type of issue in the nal survey.
here and throughout the paper we only present analysis and data from the nal survey.
note that these types of issues can be overlapping and their de nitions exible.
for instance many security issues could also be reliability issues but the way to interpret figure for this particular example is that developers care much more about security issues than general reliability issues.
in other words this question was answered based on the cognitive knowledge developers have about each of these types.
the results indicate that security issues are the most important followed by violations of best practices.
interestingly in an era where a non trivial amount of software runs on mobile devices developers do not consider it important to have program analysis detect power consumption issues.
related to the types of issues that developers consider to be important are the potential sources of unsoundness in program analysis that can a ect the detection of such issues.
we listed the most common sources of unsoundness from program analysis research and asked developers to rank up to ve of them.
during our initial interviews and the beta survey we found that some developers were unfamiliar with the terminology used though most were aware of the concepts .
we therefore provided a brief explanation of each source of unsoundness in the survey.
figure shows the results.
as can be seen exceptional control ow and aliasing top the list while purity and dealing with oating point numbers are not considered critical.
exceptions add a large number of controlow transitions that complicate program analysis.
to avoid losing e ciency and precision due to these transitions many program analyzers choose to ignore exceptional control ow.
consequently users who would like analyzers to soundly check exceptional power consumptionportabilitystylecompliancereliabilitydependenciesmaintainabilitymemory corruptionmemory consumptionperformanceconcurrencybest practicessecuritycode issues developers would like detected 120figure ranking of types of code issues developers would like program analyzers to detect.
control ow should be willing to sacri ce speed and false positive rates of the analysis.
ignoring certain side e ects due to aliasing avoids the performance overhead of precise heap analysis so developers who do not want aliasing to be overlooked should be willing to wait longer for the analysis.
in practice developers may not want program analysis to always examine all of the code in an application.
when asked if developers would like the ability to direct the program analyzer toward certain parts of the code indicated that they have that functionality and are using it.
another indicated that they do not use an analyzer that can do that but it would be important to them that a program analyzer could be directed in such a way.
interestingly of developers that are using a program analyzer with the ability to be directed to particular parts of the code both experts and security developers use this functionality more than other developers to a statistically signi cant degree.
when asked to what level of granularity developers would like to be able to direct a program analyzer the overwhelming majority said the method level or the le level .
a related functionality is the ability to analyze a changelist purityfloating point numbersnon null args to mainobject invariantsreflectionmultiple loop iterationsiteratorsstatic initializationarithmetic overflowaliasingexceptional control flow 120sources of unsoundness that should not be overlooked figure ranking of the sources of unsoundness in program analysis that developers indicated should not be overlooked i.e.
considering them during analysis would be most helpful to developers .
also known as a commit rather than the entire codebase.
this type of functionality can help developers assess the quality and impact of a change before it is checked into the source code repository.
of developers indicated that they have and use this functionality in the program analyzer they use.
another said that they do not have this functionality but it would be an important factor in adopting a program analyzer.
in sum this of developers use or would like this ability.
when looking at experts this value jumps to no change for security developers .
other functionality is less attractive.
many analyzers provide the ability for developers to write their own program analysis rules.
however sometimes the learning curve can be steep or the background required may be deep in order to write custom analysis rules.
when asked about the ability to write custom rules said that they have the functionality and use it and said it is an important factor while the rest said they either do not use it or do not care about having it.
of experts and of security developers also indicated that they do not use or care about this functionality.
we postulated that the reason why developers are not interested in the ability to write custom program analysis rules is because they want to be able to select an analyzer and start using it without much e ort.
in fact this is not the case.
we asked developers whether they would be willing to add assertions pre postconditions and or invariants to their code if this would improve the analysis results.
fully of developers said they would add at least one of these types of speci cations to their code and indicated that they would be willing to write all of them.
this provides evidence that developers may be willing to provide additional information to program analyzers in return for better results e.g.
better precision .
when asked about the form that such code speci cations should take an overwhelming majority of developers said that they would be more willing to annotate their code with speci cations if these were part of the language for example taking the form of nun nullable reference types or an assert keyword.
one feature of program analyzers that developers use heavily is the ability to suppress warnings.
of developers indicated that they use some mechanism to suppress warnings.
the primary methods are through a global con guration le source code annotations i.e.
not in comments annotations in source code comments an external suppression le and by comparing the code to a previous baseline version of it .
when asked which of these methods they like and dislike of those that use source code annotations like them followed by using a global con guration le and providing annotations in code comments .
program analyzers should prioritize security and best practices and deal with exceptional control ow and aliasing.
developers want the ability to guide program analyzers to particular parts of the code and analyze changelists.
while most are not interested in writing custom rules developers are willing to add speci cations in their code to help program analyzers.
suppressing warnings is important preferably through code annotations.
.
.
what should the non functional characteristics of program analyzers be?
in the previous section we focused on the functionality that developers indicate they want in program analyzers.
when examining characteristics we investigate non functional aspects of program analyzers such as how long they should take to perform the analysis how often their warnings should be correct and how they should t into the development process.
in many cases there is a trade o between characteristics e.g.
an analysis that has fewer false positives may include more complex techniques such as alias analysis which would require longer to complete .
in these trade o situations we asked developers to indicate what characteristic they would sacri ce in order to improve another.
the time taken by a program analyzer is an important characteristic to developers because it can a ect how often and where the analyzer can be run which directly in uences the utility of the analyzer to the developer.
when asked how long a developer would be willing to wait for results from a program analyzer of developers said that it should run on the order of seconds and said they would be willing to wait multiple minutes.
thus long running analyzers that exceed a few minutes would not be considered by nearly three quarters of developers.
the time required for an analysis dictates where it ts into the development process.
when asked where in their development process they would like to use program analyzers of developers said every time they compile said once their change was complete but before sending out a code review request said during the nightly builds said every time unit tests were run and said they would like to run it at every stage of development.
related to how a program analyzer should t into the development process is how the results of the analyzer should be shown to the developer.
the top four answers from developers are shown in figure .
the preferred location by a wide margin is in the code editor followed by the build output.
this is in line with the ndings of the interviews by johnson et al.
all participants in their interviews wanted to be noti ed of issues in their code either in the ide or at build compile time.
moreover one of the main lessons learned from the findbugs experiences at google was that developers pay attention to warnings only if they appear seamlessly within the work ow.
warnings that are false positives are cumbersome and time consuming.
we asked developers the largest false positive rate that they would tolerate.
we show the results as a reverse cumulative distribution curve in figure with the acceptable false positive rate on the x axis and the percent of developers that nd that rate acceptable on the y axis.
from the graph of developers are willing to accept up in a browserin the code reviewin the build outputin my editor 250where should analysis be shown?
figure where developers would like to have the output of program analyzers.
100acceptable false positive rate percent of results that can be false positivespercent of developersfigure the largest false positive rate of analyzers that developers would tolerate.
to a false positive rate of developers are willing to accept a false positive rate up to and only percent of developers can handle a false positive rate as high as .
the designers of coverity con rm this aim for below of false positives for stable checkers.
when forced to choose between more bugs or fewer false positives they typically choose the latter.
we also asked developers when they are most willing to deal with false positives and gave them two extremes when it is easy to determine if a warning is a false positive and when analyzing critical code.
in the latter case nding issues is so important that developers are willing to deal with false positives in exchange for making sure no issues are missed.
of developers preferred to get false positives when they are easy to sift through.
interestingly developers are willing to give up more time if the quality of the results is higher.
said that they would prefer an analyzer that was slow but found more intricate code issues to an analyzer that was fast but was only able to identify super cial issues.
similarly also said they would prefer a slower analysis that yielded fewer false positives to a faster approach that was not as accurate.
reported that they would accept a slower analyzer if it captured more issues fewer false negatives .
while these all show that the majority of developers are willing to give up some speed for improved results note that the majority is slight.
still are willing to deal with more false positives more false negatives or super cial issues if meant the analysis was faster.
these numbers do not change signi cantly when looking at just experts or security developers.
much of the feedback from developers discussed the idea of having two kinds of analyzers one fast and running in the editor and another slow and running overnight.
one developer put it quite succinctly give me what you can give fast and accurate no false positives .
give me the slow stu later in an hour it is too good and cheap to not have it .
no reasonable change is going to be checked in less than half a day but i do want that style check for that one line x right away.
another developer made a comparison of this to unit versus integration testing.
there is even less agreement when we compare the tradeo of reporting false positives versus missing real issues false negatives .
.
developers would prefer fewer false positives even if it meant some real issues were missed and .
felt that nding more real issues was worth the cost of dealing with false positives.
some program analyzers can provide lists of possible xesfor the warnings that they identify.
we asked developers if they would prefer to sift through lists of potential xes to identify the correct x or if they would rather spend that time coming up with a x on their own.
indicated they would be willing to sift through up to potential xes while felt that that time would be better spent designing a x themselves.
program analysis should take a two stage approach with one analysis stage running in real time providing fast easy feedback in the editor and another overnight nding more intricate issues.
program analysis designers should aim for a false positive rate no higher than .
developers are willing to trade analysis time for higherquality results fewer false positives fewer false negatives more intricate issues .
.
.
additional developer feedback in our survey we asked developers if they would like to share any additional opinions thoughts or feedback that they felt were not covered by our questions.
developers answered this question and we inspected and organized their responses.
a number of key themes emerged from this analysis and we share those that are useful to program analysis researchers here.
developers indicated that determinism of the program analysis is important.
fxcop described in section is not deterministic because it uses heuristics about which parts of the code to analyze and the code contracts analyzer is not because it uses timeouts.
if a program analyzer outputs di erent results each time it is run it can be di cult to tell if an issue has been xed.
the coverity designers also stress that randomization is forbidden timeouts are also bad and sometimes used as a last resort but never encouraged .
when a developer makes a change to x a program analysis warning he or she would like an easy and quick way to check whether the warning is indeed xed.
a developer does not want to re build and re analyze everything for each warning.
supporting quickly re checking whether a speci c analysis error is xed would signi cantly help the test x test cycle.
many developers indicated that regardless of the analyzer they run they would like a way to see and track their warnings e.g.
in sonarqube .
sonarqube is a web based application that leverages its database to show and combine metrics as well as to mix them with historical measures.
having a standard way of program analysis and a standard format of warnings across the organization is important as it can lessen the learning curve and decrease heterogeneity of tools and processes between teams.
it would be bene cial if analysis could help engineers understand how to properly use a programming language.
some engineers learn just enough about a language to do the work and having an analyzer that teaches them which idioms libraries or best practices to use would be helpful.
.
.
implications in this section we highlight the main implications of our survey ndings for the program analysis community.
expertise.
when asked how frequently developers run 337program analyzers daily weekly monthly yearly never said they run them daily and another run them on a weekly basis while indicated they do not use program analyzers at all.
there is a strong relationship between both familiarity with program analysis and a focus on security with frequency of use.
of security developers and of experts use program analyzers daily weekly for both groups .
2tests showed that the di erences in frequency of use of program analyzers between experts and non experts and between security developers and non security developers are both statistically signi cant p .
this relationship may imply that if a developer has a deeper understanding of program analysis then he or she will be more likely to use it.
however it may also be the case that in the process of using program analysis frequently a developer develops an understanding of program analysis.
we also asked our survey participants which of the types of code issues that they encounter they estimate could have been caught by a program analyzer.
by and large for every type of code issue the majority believe that the issue could not have been caught.
however for reliability errors and maintainability issues experts have more faith in program analysis to a statistically signi cant degree.
of experts think maintainability issues would be caught and think reliability errors would be caught whereas non experts have even lower percentages.
therefore developers who have a better understanding of program analysis also have more trust in its bug nding capabilities.
more expertise in program analysis could also help in setting expectations with users.
as an example consider that analyzers typically ignore exceptional control ow to improve e ciency and precision i.e.
reduce the number of false positives .
however the responses to our survey did not indicate that developers who would like exceptional control ow to be checked by program analyzers are also willing to tolerate a large number of false positives.
speed vs. quality.
as we previously discussed there are two camps of developers those who are willing to wait longer if the quality of the analysis results is higher and those who are willing to deal with more false positives more false negatives or super cial issues if it makes the analysis faster.
this indicates that neither kind of program analysis is out of business there is clear demand for both.
however there is de nitely a correlation between the kind of the analysis and where it ts in the development process.
developers who want to run an analyzer at every state of development are more likely to prefer a fast and super cial analysis.
those who want the results after every compile slightly lean toward slow and deeper analyses.
finally developers who want the analysis results after a nightly build or right before a code review de nitely prefer a slower analysis that detects more intricate code issues.
these ndings are all statistically signi cant p .
annotations.
our ndings indicate that developers may provide additional information to program analyzers in the form of speci cations or annotations in return for better results e.g.
fewer false positives suppressed warnings .
still of the respondents are not willing to write any assertions preconditions postconditions and invariants or do not know what these are.
developers who like to suppress warnings with source code annotations are also more likely to provide speci cations to a statistically signi cant degree 2test p .
all this suggests that program analysis should betunable through annotations but without requiring them.
trust.
to build trust in their analyzers tool designers should keep in mind that developers care much more about too many false positives than too many false negatives.
moreover the largest pain point in using program analyzers is found to be that by default enabled rules or checks do not match what developers want.
.
live site incidents our survey allowed us to understand what developers want and are most interested in with regard to program analyzers.
we also sought to uncover the distribution of issues that occur in practice.
for this we examined live site incidents from a set of microsoft hosted services.
we chose services because software development is increasingly focused on services.
alive site incident refers to an issue that a ects the functionality of a service and requires the on call engineer to intervene for its resolution.
in other words a live site incident is a high severity bug that demands immediate attention by a software engineer so that the health of the a ected company service is not compromised further.
here we categorize live site incidents into the types of code issues that we considered in the survey see section .
we categorized a total of live site incidents which occurred and were xed during the rst two months of .
note that these incidents a ected di erent microsoft services.
we achieved this categorization by personally interviewing software engineers who were assigned the resolution of these live site incidents exchanging emails with them when an interview was not possible attending live site reviews that is meetings providing an overview of any live site incidents of the previous week and by carefully reading the root cause analysis of each live site incident on the company s tracking website for these incidents.
for incidents where our only source of information was the root cause analysis provided by the designated engineers on the tracking website the categorization comes from one of the authors of this paper rather than the software engineer that handled the incident.
this can introduce a threat to validity since the categorization may be subjective.
to alleviate this threat a random sample of of these incidents were coded by another researcher independently and inter rater reliability was calculated using fleiss on the results .
in our case there was perfect agreement with indicating that the threat of subjectivity in categorization is quite low.
figure shows the categorization of all live site incidents using the methodology described above.
note that most live site incidents are categorized as reliability errors followed by performance and dependency issues.
as figure shows no live site incidents are categorized as memory and power consumption issues or as style inconsistencies.
this makes sense since such code issues are unlikely to cause high severity bugs.
as part of the survey that we described in the previous section we asked participants which types of code issues they would like program analyzers to detect.
in accordance with figure the respondents are not very interested in detecting memory and power consumption issues or style inconsistencies which are ranked fth thirteenth and eleventh respectively in the survey.
surprisingly reliability errors are ranked ninth performance issues fourth and dependency issues eighth.
in other words high severity code issues that 338memory consumptionpower consumptionstylecompliancememory corruptionportabilitysecuritybest practicesmaintainabilityconcurrencydependenciesperformancereliability 40live site incidents per categoryfigure categorization of live site incidents for di erent company products.
require immediate intervention of a software engineer see figure are ranked low in the preferences of software engineers.
this mismatch between developers stated desires and the data regarding actual defects is surprising.
an explanation for this result is that how much bugs matter depends on your point of view.
that is they matter very much to researchers and designers of program analyzers but how much they matter to the users of analyzers can be unexpected.
during the live site reviews that we attended we witnessed quotes like oh it ll crash and we ll get a call or if developers don t feel pain they often don t care which were also recorded by bessey et al.
in .
this attitude toward bugs could also explain why the respondents of our survey would not like program analyzers to go after the most painful code issues presented in figure .
in the survey we asked participants which types of code issues that they encounter they estimate could have been caught by a program analyzer.
the most popular answers were best practices violations and style inconsistencies which are both super cial code issues.
on the other hand intricate code issues like reliability concurrency and security errors were selected by signi cantly fewer survey respondents and respectively .
figure about live site incidents indicates a strong need for program analyzers to detect reliability errors.
moreover security and concurrency errors are the rst and third most popular answers respectively to the previous survey question about which types of code issues software engineers would like program analyzers to detect .
this suggests that developers do not trust program analyzers to nd intricate code issues.
we see two possible explanations for this lack of trust which are also supported by related work.
first due to the various pain points obstacles or annoyances that users encounter when using program analyzers they might not derive the full potential of these tools.
in the end some users might abandon program analysis altogether incorrectly thinking that it lacks value .
second it could be the case that users have little understanding of how program analysis works in a particular tool as well as little interest in learning more.
as a consequence they might end up classifying any detected code issue thatis even slightly confusing to them as false .
the vast majority of costly bugs in software services are related to reliability.
developers rank reliability errors low in the types of issues they want program analyzers to detect.
developers do not seem to trust analyzers to nd intricate issues although they want them to detect such issues.
.
threats to validity as with any empirical study there may be limits to our methods and ndings .
because one of our primary instruments was a survey we were concerned that the right questions were included and presented in the right way .
to address construct validity we began by examining the landscape of the use of program analysis in microsoft and interviewing developers.
these interviews led to the creation of a beta survey that we deployed and which provided another round of feedback to ne tune the questions in the nal survey.
thus we have high con dence that the questions are asked in a clear and understandable manner and cover the important aspects of program analyzers.
we are also con dent that the options provided for answers to the questions capture the majority of developer responses.
with regard to external validity our analysis comes wholly from one software organization.
this makes it unlikely that our results are completely representative of the views of software developers in general.
however because microsoft employs tens of thousands of software engineers works on diverse products in many domains and uses many tools and processes we believe that our approach of randomly sampling improves generalizability signi cantly.
in an e ort to increase external validity we have provided a reference to our survey instrument so that others can deploy it in di erent organizations and contexts.
not all categorizations of live site incidents came directly from the software engineer that handled the incident.
in cases where we could not contact them we had to categorize them ourselves based on information in the incident tracker which may introduce subjectivity.
as mentioned previously we mitigated this by having multiple researchers categorize the incidents independently and checking the inter rater reliability which resulted in perfect agreement.
.
program analyzers in industry in this section we give an overview of program analyzers that are currently being used in three large software companies namely microsoft google and facebook.
we derived a list of rst party program analyzers at microsoft by interviewing leads and members of the managed and unmanaged static analysis teams in the company.
we then asked the participants of our survey see section which analyzers they have run the most.
we focus on the six most popular rst and third party program analyzers for microsoft.
for google and facebook we rely on recent research publications to determine which analyzers are currently being used.
first party program analyzers at microsoft.
the order in which we present the tools is arbitrary and all rst party tools were selected by at least of the respondents.
339binskim is a binary scanner that validates compiler and linker settings.
speci cally it validates that code has been built using the compiler and linker protections that are required by the microsoft security development lifecycle sdl e.g.
that a binary has opted into the address space layout randomization aslr or the hardware data execution prevention dep features.
fxcop analyzes managed assemblies using a set of pre de ned or custom rules and reports possible design localization performance and security improvements.
many of the detected code issues concern best practices violations.
policheck is an internal tool by microsoft s geopolitical product strategy team that scans text code code comments and content including web pages for anything that might be politically or geopolitically incorrect.
for example a person s name should not be written in red letters as in some context or culture it may signify that the person is dead.
prefast performs intraprocedural analysis to identify defects in c and c source code.
there are multiple prefast plug ins for detecting di erent kinds of code issues like badvalues for security errors cppcorecheck for reliability security and compliance errors driversdll for errors in kernel mode drivers etc.
pre x is one of the few analyzers in industry that performs cross binary data ow analysis.
pre x detects security reliability performance and memory consumption issues but without the user providing extensive annotations it is almost impossible to nd any memory or resource leaks.
stylecop was selected by .
of the respondents and is by far the most popular rst party tool.
it analyzes c code to enforce a set of style and consistency rules which is con gurable and may be augmented with custom rules.
third party program analyzers at microsoft.
here we present the six most popular third party analyzers that are being or have been used at microsoft.
resharper was selected by .
of the respondents and is the tool that has been run the most out of all rst and third party analyzers.
resharper is a productivity tool with a code analysis component which nds compiler errors runtime errors redundancies code smells and possible improvements right while the user is typing.
note that .
of our respondents have heard of resharper out of those .
are currently using resharper and out of those rank its code analysis as one of their top three favorite features said it was the most important feature .
just like resharper coderush is a productivity tool which supports easy code investigation automation of common code creation tasks easy search and navigation to a required code location etc.
among its features coderush also provides a code analysis tool that suggests slight code improvements and detects dead or duplicate code useless code e.g.
containing an unimplemented class member invalid code e.g.
containing a call to an undeclared method as well as unreadable code.
fortify is a tool for identifying security vulnerabilities in source code with support for programming languages.
its analysis is based on a set of security coding rules and its detected vulnerabilities are categorized and prioritized based on how much risk they involve and whether they provide an accurate action plan.
the microsoft investigation into fortify performed on two very large codebases revealed that its rules are thorough at the expense of being very noisy.
checkmarx analyzes source code also written in avery wide breadth of programming languages by virtually compiling it and performing queries against it for a set of pre de ned and custom security rules.
during an evaluation of the tool at microsoft checkmarx was found to be as accurate as fortify but easier to con gure.
fortify however was found to achieve deeper coverage.
coverity is considered one of the best commercial static analyzers for detecting security and reliability errors in c c c and java.
in general the code issues it reports have a very high x rate and a very low false positive rate.
coverity performs whole program analysis and is known to have detected serious bugs involving multiple functions or methods.
it is primarily o ered as a cloud based service.
cppcheck is a rule based analyzer for c and c .
it mainly detects reliability errors like null pointer dereferences use of uninitialized variables or unsafe functions etc.
the goal of cppcheck is to report no false positives therefore it is rarely wrong but as a consequence it misses many bugs.
program analyzers at google.
so far google has made several attempts to integrate program analyzers into the development process of software engineers.
the most prominent example of such an analyzer is findbugs which cheaply detects defects in java code including bad practices performance and correctness problems.
findbugs aims at identifying the low hanging fruit of code issues instead of nding all possible errors in a particular category.
other analysis tools that have at times been used by google include coverity klocwork and fault prediction .
however all of these analyzers have gradually been abandoned due to their false positive rates scalability issues and work ow integration problems .
google built tricorder a program analysis ecosystem for detecting a variety of code issues in a wide breadth of programming languages.
tricorder smoothly integrates into the development work ow scales and allows even nonanalysis experts to write custom analyzers.
moreover any integrated analyzer that is annoying to developers degrades the performance of tricorder or whose reported code issues are never xed by developers is banned from the ecosystem.
the overview of tricorder describes some of the tools that have been integrated in the google analysis ecosystem.
these include analysis frameworks like errorprone and clangtidy which nd bug patterns based on ast matching in java and c respectively and the linter analyzer which detects style issues and contains more than individual linters such as con gured versions of the checkstyle java linter and the pylint python linter .
lastly there are various domain speci c analyzers like androidlint for detecting issues in android applications and tools that analyze metadata associated with a changelist like how much code is transitively a ected by a particular changelist.
program analyzers at facebook.
infer is facebook s most well known analyzer in the current literature it is based on academic research in program analysis and there are many publications on its internals and its largescale deployment.
facebook uses infer to nd resource leaks and null pointer exceptions in android and ios applications.
the tool may report both false positives and false negatives.
infer s analysis is incremental which means that when analyzing a changelist it uses a cache of veri cation results so that only functions a ected by the changelist are analyzed.
discussion.
in this section we observe that although there are a few exceptions to this rule advanced program 340analysis techniques are generally underdeveloped in industry.
most of the program analyzers that we have presented are productivity tools linters or rule based scanners.
we are de nitely not claiming that simplistic program analyzers lack value we are however wondering why many innovative and bright research ideas do not seem to have substantial practical impact.
this trend has been observed before in an e ort to provide a few reasons for this gap between scienti c literature and industry.
here we would like to support these suggestions from the literature with data driven results from our survey.
for instance calcagno et al.
suggest that part of the problem is that research has focused too much on wholeprogram or specify rst analyses.
indeed the importance of compositional and incremental analyses is stressed by the fact that of the survey respondents do not currently have the functionality of analyzing only a changelist instead of an entire codebase but this functionality would be important to them.
furthermore nd the granularity of functions or methods more suitable for directing an analyzer toward the more critical parts of their code.
concerning program annotations of the respondents are not willing to write any speci cations or do not know what speci cations are.
calcagno et al.
also de ne the social challenge which has been described in other related work too .
engineers accumulate trust in an analyzer and start reacting to the bugs it reports when certain features are there full automation and integration into the development work ow scalability precision and fast reporting.
in fact the top six pain points obstacles or annoyances that our survey respondents encountered when using program analyzers are from most to least annoying irrelevant checks are turned on by default bad phrasing of warnings too many false positives too slow no suggested xes and di cult to integrate in the work ow see figure .
moreover the top six minimum requirements that an analyzer must satisfy for our respondents to start using it are from most to least minimal detect issues that are important to me easy to integrate in the work ow fast few false positives with suppression of warnings good phrasing of warnings.
lastly in terms of fast reporting of the respondents are only willing to wait seconds for a program analyzer to check a changelist and minutes.
.
other related work empirical studies.
there are relatively few empirical studies that analyze the usage and adoption of program analysis tools in industry especially from the point of view of software engineers.
so far many studies have analyzed the functionality of program analyzers mostly from the point of view of tool designers .
the work most closely related to ours investigates why software engineers do not use static analysis tools to nd bugs .
the results are collected from interviews with engineers and focus on the interviewees perception of tools including interacting with their interfaces and on what could have caused these perceptions.
although their interviewees felt that using static analysis is bene cial there are certain barriers in their use like false positives poorly presented warnings lack of or weak support from the team inability to suppress warnings poor environment integration long running times of the tools etc.
in section we discuss that our survey respondents have also identi ed the same painpoints.
in terms of support from the team many of our respondents that have stopped running program analyzers said it was because of a change in the team policy.
ayewah and pugh also conducted a survey and a controlled study on how software engineers use the findbugs tool .
although related our work is not concerned about a particular program analyzer we are rather focusing on what the general characteristics of any program analysis should be such that it is industrially relevant.
lastly our work is analogous to a recent empirical analysis of programming language adoption instead of programming languages it focuses on the adoption of program analyzers in a large software organization.
unsoundness in program analysis.
around the year unsoundness in program analysis was controversial in the academic community .
by now researchers have realized that soundness is commonly eschewed in practice .
in fact there have even emerged approaches for measuring the unsoundness in a static analysis and evaluating its practical impact .
in industry as it also becomes evident in section it is a well established design decision to build program analyzers to be unsound in order to increase automation improve performance achieve modularity and reduce the number of false positives or the annotation overhead.
the full range of program analysis techniques in industry from heuristics to more advanced methodologies like in coverity pre x and infer becomes more precise and e cient in detecting software bugs at the cost of ignoring or making unsound assumptions about certain program properties.
in our survey we presented engineers with common sources of unsoundness in program analyzers and asked them which of these should not be overlooked by tool designers see section .
we hope that our ndings will help designers in nding good trade o s when building practical analyses.
other companies.
ebay has suggested techniques for objectively evaluating the cost e ectiveness of di erent program analyzers and comparing them against each other .
ibm has experimented with an online portal that addresses common barriers to the wide usage of static analysis tools and promotes their adoption .
other evaluations.
other evaluations of usage of static analyzers include understanding how to improve their user interfaces and how to use suitable benchmarks for systematically evaluating and comparing these tools .
.
conclusion we have presented a multi method empirical investigation that we deployed at microsoft.
our ndings shed light on what practitioners want from program analyzers how program analysis researchers can achieve broad and lasting adoption of their tools which types of defects are most important to minimize through program analysis and which tools are currently being used at three large software companies.
we believe that our data driven answers to these questions are the rst step toward narrowing the gap between scienti c literature and industry with regard to program analysis.
.