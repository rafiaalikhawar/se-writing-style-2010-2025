multi modal attention network learning for semantic source code retrieval yao wan sharp jingdong shu yulei sui guandong xu zhou zhao jian wu and philip s. yu diamondmath college of computer science and technology zhejiang university hangzhou china school of computer science university of technology sydney australia department of computer science university of illinois at chicago illinois usa diamondmathinstitute for data science tsinghua university beijing china sharpstate key laboratory of cognitive intelligence iflytek hefei china wanyao jdshu zhaozhou wujian2000 zju.edu.cn yulei.sui guandong.xu uts.edu.au psyu uic.edu abstract code retrieval techniques and tools have been playing a key role in facilitating software developers to retrieve existing code fragments from available open source repositories given a user query e.g.
a short natural language text describing the functionality for retrieving a particular code snippet .
despite the existing efforts in improving the effectiveness of code retrieval there are still two main issues hindering them from being used to accurately retrieve satisfiable code fragments from largescale repositories when answering complicated queries.
first the existing approaches only consider shallow features of source code such as method names and code tokens but ignoring structured features such as abstract syntax trees asts and control flow graphs cfgs of source code which contains rich and welldefined semantics of source code.
second although the deep learning based approach performs well on the representation of source code it lacks the explainability making it hard to interpret the retrieval results and almost impossible to understand which features of source code contribute more to the final results.
to tackle the two aforementioned issues this paper proposes mman a novel m ulti m odal a ttention n etwork for semantic source code retrieval.
a comprehensive multi modal representation is developed for representing unstructured and structured features of source code with one lstm for the sequential tokens of code a tree lstm for the ast of code and a ggnn gated graph neural network for the cfg of code.
furthermore a multi modal attention fusion layer is applied to assign weights to different parts of each modality of source code and then integrate them into a single hybrid representation.
comprehensive experiments and analysis on a large scale real world dataset show that our proposed model can accurately retrieve code snippets and outperforms the state of the art methods.
index terms code retrieval multi modal network attention mechanism deep learning.
i. i ntroduction with the advent of immense source code repositories such as github and stackoverflow it is gradually becoming a key software development activity for programmers to search existing code with the same functionality and reuse as much of that code as possible .
the goal of code retrieval is to retrieve a particular code fragment from available opensource repositories given a user specification e.g.
a short text describing the functionality of the code fragment .
the key challenges of implementing such a code retrieval system lie equal contribution.
!
!
.
.
figure a motivating example to better illustrate our motivation.
a a code snippet and its corresponding description.
b the ast of the code snippet.
c the control flow graph of the code snippet.
in two folds a a deep semantic understanding of the source code and b measuring the similarity of cross modalities i.e.
input natural language and source code .
existing efforts and limitations.
many existing efforts have been made towards searching the huge amount of available code resources for a natural language query ranging from keyword matching to semantic retrieval .
lu et al.
expanded a query with synonyms obtained from wordnet and then performed keyword matching of method signatures.
lv et al.
expanded the query with the apis and considered the impact of both text similarity and potential apis on code search.
reiss et al.
developed a code retrieval system 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
named sourcerer which learned the semantic representation of source code through a probabilistic topic model.
inspired by the success of deep learning in computer vision and natural language processing tasks deep learning has been applied to better represent source code for tasks such as clone detection and code summarization .
to the best of our knowledge gu et.
al.
is the first who applied deep learning network to the task of code retrieval which captures the correlation between the semantic source code and natural language query in an intermediate semantic space.
however the approach still suffers from two major limitations a deep structured features of source code are often ignored .
the approach captures the shallow source code information including method name code tokens and api sequence missing the opportunity to capture the rich structure semantics of the code.
b lack of explainability .
the final results from a deep neural network is often hard to interpret since its internal working is always transparent to input data and different applications.
this is also a common issue when applying deep learning models.
for example in the code and its natural language descriptions are projected into an intermediate semantic space and constrained by a ranking loss function.
although the semantic representation of code is learned it is hard to infer which parts contribute more to the final result.
insights.
these aforementioned limitations motivate us to design a model which learns a more comprehensive representation on source code as well as with the ability of explainability.
from one hand for limitation a apart from the tokens of code we also extract more features of code from its multiple views such asabstract syntax tree ast and control flow graph cfg .
the ast and cfg are two types of intermediate code one of which represents the hierarchical syntactic structure of a program and the other represents the computation and control flow of a program .
in this paper we argue that aggregating complementary information from multiple views of source code can enrich its representation.
in this paper we use the term view and modality interchangeably.
we call the approach of learning code representation from its multiple views modalities asmulti modal learning .
to address the limitation b since different modalities reflect different features of the source code.
therefore each modality may not contribute equally to the final code representation.
for a given modality it consists of many elements tokens nodes in ast cfg weights are assigned to different elements via representation learning.
therefore we can infer which part contributes more to the final result from the final representation making explainability possible.
in this paper we design an attention mechanism to integrate the multi modal features into a single hybrid code representation.
a motivating example.
we give an example in figure to better illustrate our ideas.
figure a shows a simple c code example which aims to verify whether an array of integers contains an even number.
figures b and c represent 1the tree structure can also be seen as a special instance of graph with no circles and with each node having at most one parent node.the corresponding ast and inter procedural cfg of code in figure a respectively.
from figure a we can see that the semantics of the highlighted three words verify array even can be precisely captured by different code representations e.g.
plain text for check type augmented ast for binaryoperator and cfg for while .
these representations pay attention to different structure information of the code at different views e.g.
each node on ast represents a token and each node on cfg represents a statement.
this shows the necessity of considering various modalities to better represent the source code.
it is necessary to represent a code from multiple views especially from the structured information since the orders of tokens and statements on the two views can be different depending on different code representations.
for example based on plain text the token after while in figure a is and then followed by head .
differently on ast there will be two possible tokens following compound i.e.
branch test if binaryoperator as shown in figure b .
similarly after the token in the last statement at line there will be no token left based on plain text.
however based on cfg the next token is while a tt h e beginning of loop function based on cfg.
from figure we can also observe that there exists an alignment relationship among the code snippet and it is description.
for example the keyword verify should be closely connected to the word check in code.
that means on code retrieval we can infer which part of the retrieved code contributes most to the input query words.
this is very important to the model explainability.
our solution and contributions.
to tackle the two aforementioned issues in this paper we propose a novel model called multi modal attention network mman for semantic source code retrieval.
we not only consider the sequential features which have been studied in previous works i.e.
method name and tokens but also the structure features i.e.
ast and cfg extracted from code .
we explore a novel multi modal neural network to effectively capture these multi modal features simultaneously.
in particular we employ a lstm to represent the sequential tokens of code snippet a tree lstm network to represent the abstract syntax tree ast and a gated graph neural network ggnn to represent the cfg.
to overcome the explainability issue we design an attention mechanism to assign different weights to different parts of each modality of source code with the ability of explanation.
to summarize the main contributions of this paper are as follows.
we propose a more comprehensive multi modal representation method for source code with one lstm for the sequential content of source code a tree lstm for the ast of source code and a ggnn for the cfg of source code.
furthermore a multi modal fusion layer is applied to integrate these three representations.
to the best of our knowledge it is the first time that we propose an attention network to assign different weights to different parts of each modality of source code providing an explainability of our deep multi modal neural network for representation.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
to verify the effectiveness of our proposed model we validate our proposed model on a real world dataset crawled from github which consists of 527c code snippets.
comprehensive experiments and analysis show the effectiveness of our proposed model when compared with some state of the art methods.
organization.
the remainder of this paper is organized as follows.
section ii highlights some works related to this paper.
in section iii we provide some background knowledge on multi modal learning and attention mechanism.
in section iv we first give an overview our proposed framework and then present each module of our proposed framework in detail.
section v describes the dataset used in our experiment and shows the experimental results and analysis.
section vi presents a discussion on our proposed model including the strength as well as some threats to validity and limitations existing in our model.
finally we conclude this paper and give some future research directions in section vii.
ii.
r elated work in this section we briefly review the related studies from three perspectives namely deep code representation multimodal learning and attention mechanism.
a. deep code representation with the successful development of deep learning it has also become more and more prevalent for representing source code in the domain of software engineering research.
in mou et al.
learn distributed vector representations using tree structured convolutional neural network tree cnn to represent snippets of code for program classification.
similarly wan et al.
apply the tree structured recurrent neural network tree lstm to represent the ast of source code for the task of code summarization.
piech et al.
and parisotto et al.
learn distributed representations of source code input output pairs and use them to guide program synthesis from examples.
in li et al.
represent heap state as a graph and proposed a gated graph neural network to directly learn its representation to mathematically describe the shape of the heap.
maddison and tarlow and other neural language models e.g.
lstms in dam et al.
describe context distributed representations while sequentially generating code.
ling et al.
and allamanis et al.
combine the code context distributed representation with distributed representations of other modalities e.g.
natural language to synthesize code.
one limitation of the above mentioned approaches is that these approaches ignore cfg of source code which also conveys rich semantic information.
furthermore no unified network is proposed to effectively fuse these multiple modalities.
to mitigate this issue this papers resort to propose a multimodal network to learn a more comprehensive representation of source code.
b. multi modal learning one prevalent direction in multi modal learning is on joint representation which has been applied in many applicationssuch as image captioning summarization visual questioning answering and dialog system .
in chen et al.
propose an attentional hierarchical neural network to summarize a text document and its accompanying images simultaneously.
in zhang et al.
propose a multi modal i.e.
image and long description of product generative adversarial network for product title refinement in mobile e commerce.
in kim et al.
propose a dual attention network to capture a high level abstraction of the full video content by learning the latent variables of the video input i.e.
frames and captions.
similarly in hori et al.
answer questions about images using learned audio features image features and video description for the audio visual scene aware dialog.
another direction in multi modal learning is cross modal representation learning for information retrieval which is similar to our task.
cross modal representation learning aims to learn representation of each modality via project them into an intermediate semantic space with a constraint.
in carvalho et al.
propose a cross modal retrieval model aligning visual and textual data like pictures of dishes and their recipes in a shared representation space for receipt retrieval.
in ma et al.
propose a neural architecture for cross modal retrieval which combines one cnn for image representation and one cnn for calculating word level phrase level and sentence level matching scores between an image and a sentence.
the authors learn hash functions that map images and text in the original space into a hamming space of binary codes such that the similarity between the objects in the original space is preserved in the hamming space.
in this paper we draw the insights from multi modal learning but not limit to it.
we not only design a multi modal neural network to represent the code but also apply an attention mechanism to learn which part of code contributes more to the final semantic representation.
c. attention mechanism attention mechanism has shown remarkable success in many artificial intelligence domains such as neural machine translation image captioning image classification and visual question answering .
attention mechanisms allow models to focus on necessary parts of visual or textual inputs at each step of a task.
visual attention models selectively pay attention to small regions in an image to extract core features as well as reduce the amount of information to process.
a number of methods have recently adopted visual attention to benefit image classification image generation image captioning visual question answering etc.
on the other hand textual attention mechanisms generally aim to find semantic or syntactic input output alignments under an encoder decoder framework which is especially effective in handling long term dependency.
this approach has been successfully applied to various tasks including machine translation text generation sentence summarization and question answering .
in lu et al.
propose a co attention learning framework to alternately learn the image attention and the question attention for visual question authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
answering.
in nam et al.
propose a multi stage coattention learning framework to refine the attentions based on memory of previous attentions.
in paulus et al.
combine the inter and intra attention mechanism in a deep reinforcement learng setting to improve the performance abstractive text summarization.
in zhang et al.
introduce a self attention mechanism into convolutional generative adversarial networks.
to the best of our knowledge no study has attempted to learn multimodal attention models for the task of code retrieval.
iii.
p reliminaries in this section we first mathematically formalize the code retrieval problem using some basic notations and terminologies.
we then present some background knowledge of multi modal learning and attention mechanism.
a. problem f ormulation to start with we introduce some basic notations.
suppose that we have a set dofncode snippets with corresponding descriptions i.e.
d x d1 x d1 ... xn dn .
each code snippet and description can be seen as a sequence of tokens.
let xi xi1 xi2 ... x i xi be a sequence of source code snippet di di1 dd2 ... d i di be a sequence of a description where denotes the length of a sequence.
as we declared before we represent the source code from three modalities i.e.
tokens ast and cfg .
we denote the semantic representation of code snippet xias xi xtok i xast i xcfg i where xtok i xast i xcfg idenote the representation for the three modalities respectively.
since the code snippet and its description are heterogeneous the goal of this paper is to train a model to learn their representation in an intermediate semantic space simultaneously.
then in the testing phase the model can return a similarity vector of each candidate code snippet for a given query.
b. multi modal learning multi modal learning aims to build models that can process and aggregate information from multiple modalities .
one important task of multi modal learning is multi modal representation learning which is roughly categorized as two classes joint and coordinated .
joint representations combine the unimodal signals into the same representation space while coordinated representations process unimodal signals separately but enforcing certain similarity constraints on them to bring them to what we term an intermediate semantic space.
we introduce these two kinds of techniques in our problem setting.
figure illustrates the difference and connection between of joint and coordinated representations.
for code snippet x we extract its multiple modalities such asxtok xast xcfg.
since these modalities are complementary representation of a same code we can apply the joint representation which is formularized as follows x f parenleftbig xtok xast xcfg parenrightbig where the multimodal representation xis computed using functionf e.g.
a deep neural network that relies on unimodal representations xtok xast xcfg.
xtokxastxcfgd x f parenleftbig xtok xast xcfg parenrightbig g1 g2 figure the difference and connection between joint and coordinated representations adapted from .
while considering the code snippet xand description d since these two modalities are from different sources it is desirable for us to apply coordinated representation for them which is defined as follows g1 x g2 d where each modality has a corresponding projection function g1andg2above that projects it into an intermediate semantic space with a similarity constraint coordination on them.
examples of such coordination include minimizing cosine distance or maximizing correlation.
in this paper the cosine similarity function is adopted.
c. attention mechanism attention networks learn functions that provide a weighting over inputs or internal features to steer the information visible to other parts of a network.
to some extent it is biologically motivated by the fact that our retina pays visual attention to different regions of an image or correlate words in one sentence.
to date many variants of attention mechanism have been evolved.
from another perspective the attention mechanism can be seen as the process of soft addressing in a memory unit.
the source composed of key kand value v can be seen as the content of memory.
given an input query the goal of attention mechanism is to return an attention value.
formally we can define the attention value among query key and value as follows.
q k s o f t m a x g q k where qis the query and kis the key gis the attention score function which measures the similarity between query and key.
usually the ghas many options such as multi layer perceptron dot product and scaled dot product .
we call this kind of attention as inter attention.
however there exists a condition that the query is the key itself.
in this condition we call it intra attention also well known as selfattention which exhibits a better balance between ability to model long range dependencies and computational and statistical efficiency.
after obtaining the attention score the final attended vector can be represented as the weighted sum of each value in the memory v summationdisplay i i q k vi authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
where viis thei th value in the memory.
in this paper we adopt the inter attention.
furthermore the key is the hidden state of token or node in ast cfg and the value is the corresponding context vector cf.
sec.
iv c .
iv .
m ulti modal attention network a. an overview !
!
figure the workflow of mman.
in this section we firstly give an overall workflow of how to get a trained model for code retrieval.
then we present an overview of the network architecture of our proposed mman model.
figure shows the overall workflow of how to get a trained model which consists of an offline training stage and an online retrieval stage.
in the training stage we prepare a large scale corpus of annotated code description pairs.
the annotated pairs are then fed into our proposed mman model for training.
after training we can get a trained retrieval network.
then given a natural language query related source code snippets can be retrieved by the trained network.
figure is an overview of the network architecture of our proposed mman model.
we split the framework into three submodules.
a multi modal code representation cf.
sec.
iv b .
this module is used to represent the source code into a hidden space.
b multi modal attention fusion cf.
sec.
iv c .
this attention module is designed to assign different weight on different parts for each modality and then fuse the attended vector into a single vector.
c model learning cf.
sec.
iv e .
this attention module is designed to learn the comment description representation and code representation in a common space through a ranking loss function.
we will elaborate each component in this framework in the following sections.
b. multi modal code representation different from previous methods that just utilize sequential tokens to represent code we also consider the structure information of source code in this section we present a hybrid embedding approach for code representation.
we apply a lstm to represent the tokens of code and a tree lstm to represent the ast of code a ggnn to represent the cfg of code.
lexical level tokens the key insight into lexical level representation of source code is that the comments are always extracted from the lexical of code such as the method name variable name and so on.
in this paper we apply lstm network to represent the sequential tokens.
htok i l s t m parenleftbig htok i w xi parenrightbig wherei ... x wis the word embedding layer to embed each word into a vector.
the final hidden state htok x of the last token of code is the token modality representation of x. syntactic level ast we represent the syntactic level of source code from the aspect of ast embedding.
similar to a traditional lstm unit we propose tree lstm where the lstm unit also contains an input gate a memory cell and an output gate.
however different from a standard lstm unit which only has one forget gate for its previous unit a tree lstm unit contains multiple forget gates.
in particular considering a node nwith the value xiin its one hot encoding representation and it has two children nlandnr which are its left child and right child respectively.
the tree lstm recursively computes the embedding for nfrom the bottom up.
assume that the left child and the right child maintain the lstm state hl cl and hr cr respectively.
then the lstm state h c ofnis computed as hast i cast i l s t m parenleftbig parenleftbig bracketleftbig hast il hast ir bracketrightbig bracketleftbig cast il cast ir bracketrightbig parenrightbig w xi parenrightbig wherei ... x and denotes the concatenation of two vectors.
note that a node may lack one or both of its children.
in this case the encoder sets the lstm state of the missing child to zero.
in this paper we adopt the hidden state of root node as the ast modality representation.
it s worth mentioning that when the tree is simply a chain namely n the tree lstm reduces to the vallina lstm.
figure shows the structure of lstm and tree lstm.
syntactic level cfg as the cfg is a directed graph we apply a gated graph neural network ggnn to represent the cfg which is a neural network architecture developed for graph.
we first define a graph as g v e wherevis a set of vertices v lscriptv andeis a set of edges vi vj lscripte .
lscriptv and lscripteare labels of vertex and edge respectively.
in our code retrieval scenario each vertex is the node of cfg and each edge represents the control flow of code which has multiple types.
ggnn learns the graph representation directly through the following procedures first we initialize the hidden state for each vertex v v ashcfg v w lscriptv wherewis the one hot embedding function.
then for each round t each vertex v v receives the vector mv t which is the message aggregated from its neighbours.
the vector mv t 1can be formulated as follows mv t summationdisplay v prime n v w lscriptehv prime t wheren v are the neighbours of vertex v. for round t message from each neighbour is mapped into a shared space viaw lscripte.
for each vertex v v the ggnn update its hidden state with a forget gate.
in this paper we adopt the gated recurrent unit gru to update the hidden state of each vertex which can be formulated as follows.
hcfg v t g r u hcfg v t mv t .
finally with trounds of iterations we aggregate the hidden states of all vertices via summation to obtain the embedded authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
!
.
.
.
.
.
.
.
.
!
.
.
!
!
!
!
!
!
.
.
.
.
.
.
figure the network architecture of our proposed mman model.
we first extract the code description pairs from training corpus.
we then parse the code snippets into tree modalities i.e.
tokens ast cfg.
then the training samples are fed into the network as input.
a multi modal code representation.
we first learn the representation of each modality via lstm tree lstm and ggnn respectively.
b multi modal attention fusion.
we design an attention layer to assign different weight on different parts for each modality and then fuse the attended vector into a single vector.
c model learning.
we map the comment description representation and code representation into an intermediate semantic common space and design a ranking loss function to learn their similarities.
figure an illustratation of tree lstm and lstm.
figure an illustration of ggnn.
representation of the cfg.
figure illustrates the structure of ggnn.
c. multi modal attention fusion after we obtain the semantic representation of each modality we need to fuse them into a single representation.
as we declare before for a unimodal since it is composed of many elements it is desirable to assign different weights to each element.token attention.
for tokens not all tokens contribute equally to the final semantic representation of code snippet.
therefore we introduce the attention mechanism on tokens to extract the ones that are more important to the representation of a sequence of code tokens.
the attention score for tokens tok is calculated as follows tok i exp gtok ftok htok i utok summationtext jexp gtok ftok htok j utok where htok irepresents the hidden state of i th token in code.
ftokdenotes a linear layer and gtokis the dot product operator.
utokis the context vector of token modality which can be seen as a high level representation of sequential tokens of code.
the word context vector utokis randomly initialized and jointly learned during the training process.
ast attention.
for the ast not all nodes contribute equally to the final semantic representation of code snippet indicating that different construct occurring in the source code e.g.
if condition then should also be considered distinctly.
similar to token attention the attention score for ast nodes astis calculated as follows ast i exp gast fast hast i uast summationtext jexp gast fast hast j uast where hast irepresents the hidden state of i th node in the ast.
fastdenotes a linear layer and gastis the dot product operator.
uastis the context vector of ast modality which can be seen as a high level representation of ast nodes of code.
cfg attention.
for the cfg different statement in the source code should also be assigned different weight for the final authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
representation.
therefore we assign each cfg nodes with the weight cfgas cfg i sigmoid gcfg fcfg hcfg i ucfg where hcfg irepresents the hidden state of i th node in the cfg.
fcfgdenotes a linear layer and gcfgis the dot product operator.
ucfgis the context vector of cfg modality which can be seen as a high level representation of cfg nodes of code.
it s worth mentioning that cfg attention weighted by sigmoid function achieves better performance than that by softmax function from the experimental results.
multi modal fusion.
we then integrate the multi modal representation into a single representation via their corresponding attention score.
we first concatenate them and then feed them into a one layer liner network which can be formularized as follows.
x w bracketleftbigg summationdisplay i tok ihtok i summationdisplay i ast ihast i summationdisplay i cfg ihcfg i bracketrightbigg where xis the final semantic representation of code snippet x is the concatenation operation and wis the attention weight for each modality.
d. description representation in the training phase the descriptions are extracted from the code comments while in the testing phase the description are regarded as the input queries.
in this paper we apply a vallina lstm to represent the description.
hdes i l s t m parenleftbig hdes i w di parenrightbig wherei ... d andwis the word embedding layer to embed each word into a vector.
the hidden state of last step hdes d can be used as a vector representation of d. e. model learning now we present how to train the mman model to embed both code and descriptions into an intermediate semantic space with a similarity coordination.
the basic assumption of this joint representation is that if a code snippet and a description have similar semantics their embedded vectors should be close to each other.
in other words given an arbitrary code snippet xand an arbitrary description d we want it to predict a high similarity if dis a correct description of x and a small similarity otherwise.
in training phase we construct each training instance as a triple x d d for each code snippet x there is a positive description d a correct description of x as well as a negative description an incorrect description of x d randomly chosen from the pool of all d s. when trained on the set of x d d triples the mman predicts the cosine similarities of both x d and x d pairs and minimizes the hinge range loss l summationdisplay x d d dmax sim x d sim x d where denotes the model parameters ddenotes the training dataset sim denotes the similarity score between code anddescription is a small constant margin.
x d andd are the embedded vectors of x d andd respectively.
in our experiments we adopt the cosin similarity function cf.
iv f and set the fixed value to .
.
intuitively the ranking loss encourages the similarity between a code snippet and its correct description to go up and the similarities between a code snippet and incorrect descriptions to go down.
f .
code retrieval after the model is trained we can deploy it online for service.
given a code base x for a given query q the target is to rank all these code snippets by their similarities with query q.w e first feed the code snippet xinto the multi model representation module and feed the query qas description into the lstm module to obtain their corresponding representations denoted asxandq.
then we calculate the ranking score as follows sim x q cos x q xtq bardblx bardbl bardblq bardbl where xandqare the vectors of code and a query respectively.
the higher the similarity the more related the code is to the given query.
v. e xperiments and analysis to evaluate our proposed approach in this section we conduct experiments to answer the following questions rq1.
does our proposed approach improve the performance of code retrieval when compared with some stateof the art approaches?
rq2.
what is the effectiveness and the contribution of each modality of source code e.g.
sequential tokens ast cfg of source code for the final retrieval performance and what about their combinations?
rq3.
what is the performance of our proposed model when varying the code length code length code ast node number code cfg node number and comment length?
rq4.
what is the performance of our proposed attention mechanism?
what is the explainability of attention visualization?
we ask rq1 to evaluate our deep learning based model compared to some state of the art baselines which will be described in the following subsection.
we ask rq2 in order to evaluate the performance of each modality extracted from source code.
we ask rq3 to analyze the sensitivity of our proposed model when varying the code length code ast node number code cfg node number and comment length.
we ask rq4 to verify the explainability of our proposed attention mechanism.
in the following subsections we first describe the dataset some evaluation metrics and the training details.
then we introduce the baseline for rq1.
finally we report our results and analysis for four research questions.
a. dataset collection as described in section iv our proposed model requires a large scale training corpus that contains code snippets and their corresponding descriptions.
following but different from authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
cfg size050100150200250300350400count a cfg node number.
comment length0100200300400500count b comment length.
figure the histogram of the dataset in our experiments.
a cfg node number distribution.
b comment length distribution.
we evaluate the performance of our proposed model on a corpus of c code snippets collected from github a popular open source projects hosting platform .
actually we have considered the dataset released by while this dataset only contains the cleaned java snippets without the raw data unable to generate the cfg.
therefore we resort to build a more complicated language c dataset which may also provide more challenges and opportunities for our further research.
to construct the codebase we crawl all the c language repositories by its api2.
we then exclude the repositories whose stars number is smaller than .
we select only the methods that have documentation comments from the crawled projects.
finally we obtain a c corpus consisting of commented c methods.
figure shows the length distributions of code and comment on testing data.
from figure 7a we can find that the lengths of most code snippets are located between to .
this was also observed in the quote in functions should hardly ever be lines long .
from figure 7b we can notice that the lengths of nearly all the comments are smaller than .
this also confirms the challenge for capturing the correlation between short text with its corresponding code snippet.
having collected the corpus of commented code snippets we extract the multi modal code features and it s corresponding description i.e.
method name tokens ast cfg description as follows method name extraction .for each c method we extract its name and parse the name into a sequence of tokens according to camel case and if it contains we then tokenize it via .
token extraction .to collect tokens from a c method we tokenize the code by .
!
space .
after we tokenize function body function name we limit their max length as and respectively.
ast extraction .to construct the tree structure of code we parse c code into abstract syntax trees via an open source tool named clang .
for simplification we transform the generated asts to binary trees by the following two steps which have been adopted in a split nodes with more than children generate a new 2we crawled the github in oct. so the repositories in our database are created from august to oct. .right child together with the old left child as its children and then put all children except the leftmost as the children of this new node.
repeat this operation in a top down way until only nodes with children left b combine nodes with child with its child.
cfg extraction .to construct the cfg of code we first parse c function into cfg via an open source tool named svf which has been widely used in value flow analysis .
we then remove nodes with same statement or no statement.
for nodes with same statement we retain the nodes which occur in the output of svf first and remove their child nodes and link children of their child nodes to them.
for nodes without statement we delete them and link their child nodes to their parent nodes.
we set maximum size of cfg nodes as .
description extraction .to extract the documentation comment we extract description via the regular expression .
we check the last sentence before every function and if it meets the condition that we have defined via regular expression then we extract the description from the sentence.
we shuffle the dataset and split it into two parts namely samples for training and samples for evaluation.
it s worth mentioning a difference between our data processing and the one in .
in the proposed approach is verified on another isolated dataset to avoid the bias.
since the evaluation dataset doesn t have the ground truth they manually labeled the searched results.
we argue that this approach may introduce the human bias.
therefore in our paper we resort to the automatic evaluation.
b. evaluation metrics for automatic evaluation we adopt two common metrics to measure the effectiveness of code retrieval i.e.
successrate kand mean reciprocal rank mrr both of which have been widely used in the area of information retrieval.
to measure the relevance of our search results we use the success rate at rank k. the successrate kmeasures the percentage of queries for which more than one correct result could exist in the top kranked results which is calculated as follows successrate k parenleftbigg q q summationdisplay q 1 frank q k parenrightbigg whereqis a set of queries is a function which returns if the input is true and returns 0otherwise.
successrate k is important because a better code search engine should allow developers to discover the needed code by inspecting fewer returned results.
the higher the successrate value is the better the code search performance is.
we also use mean reciprocal rank mrr to evaluate the ranking of our search results.
the mrr is the average of the reciprocal ranks of results of a set of queries q. the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
reciprocal rank of a query is the inverse of the rank of the first hit result .
the mrr is defined as mrr q q summationdisplay q frank q where q is the size of query set.
the higher the mrr value is the better the code retrieval performance is.
c. implementation details to train our proposed model we first randomize the training data and set the mini batch size to .
we build three separate vocabulary for code comment and ast leaf node tokens with size and respectively.
for each batch the code is padded with a special token pad to the maximum length.
all tokens in our dataset are converted to lower case.
we set the word embedding size to .
for lstm and treelstm unit we set the hidden size to be .
for ggnn we set the hidden size to and set rounds of iteration for ggnn.
the margin is set to .
.
we update the parameters via adam optimizer with the learning rate .
.
to prevent over fitting we use dropout with .
.
all the models in this paper are trained for epochs taking about hours.
all the experiments are implemented using the pytorch .
framework with python .
and the experiments were conducted on a computer with a nvidia quadro rtx gpu with gb memory running ubuntu .
.
d. q1 comparison with baselines we compare our model with the following baseline methods codehow is a state of the art code search engine proposed recently.
it is an information retrieval based code search tool that incorporates an extended boolean model and api matching.
deepcs is the state of the art deep neural network based approach for code retrieval.
it learns a unified vector representation of both source code and natural language queries.
mman tok ast cfg w o.att.
represents our proposed model on three modalities without attention component for each modality.
we also derive its variants on composition of these modalities.
mman tok ast cfg w.att.
represents our proposed model on three modalities with attention component for each modality.
we also derive its variants on combinations of these modalities.
automatic evaluation.
we evaluate mman on the evaluation dataset consisting of descriptions.
in this automatic evaluation we consider each description as an input query and its corresponding code snippet as the ground truth.
table i shows the overall performance of the three approaches measured in terms of successrate kandmrr .
the columnsr r 5andr show the results of the average successrate kover all queries when kis1 5and respectively.
the column mrr shows the mrr values of the three approaches.
from this table we can draw thetable i comparison of the overall performance between our model and baselines on automatic evaluation metrics.
best scores are in boldface.
method r r r mrr codehow .
.
.
.
deepcs .
.
.
.
mman tok ast cfg w o.att.
.
.
.
.
mman tok ast cfg w.att.
.
.
.
.
table ii effect of each modality.
best scores are in boldface.
method r r r mrr mman tok w o.att.
.
.
.
.
mman ast w o.att.
.
.
.
.
mman cfg w o.att.
.
.
.
.
mman tok ast cfg w o.att.
.
.
.
.
mman tok w.att.
.
.
.
.
mman ast w.att.
.
.
.
.
mman cfg w.att.
.
.
.
.
mman tok ast cfg w.att.
.
.
.
.
following conclusions a under all experimental settings our mman tok ast cfg method obtains higher performance in terms of both metrics consistently which indicates better code retrieval performance.
for the r k the improvements to deepcs are .
.
and .
respectively.
for the mrr the improvement to deepcs is .
.
b comparing the performance of two variants of our proposed model mman tok ast cfg w. or w o.att.
we can observe that our designed attention mechanism indeed has a positive effect.
e. q2 effect of each modality to demonstrate the effectiveness of fusing multiple modalities we have conducted experiments over different modality combinations to validate the effectiveness of fusing multiple modalities.
table ii presents the performance of mman over various source combinations with and without attention.
from this table we can observe that incorporating more modalities will achieve a better performance which shows that there is a complementary rather than conflicting relationship among these modalities.
in a sense this is consensus with the old saying two heads are better than one .
comparing the performance of each modality comparison with or without attention we also obtain that our designed attention mechanism has a positive effect on fusing these modalities together.
f .
q3 sensitivity analysis to analyze the robustness of our proposed model we study four parameters i.e.
code length code ast node number code cfg node number and comment length which may have an effect on the code and comment representation.
figure 8a shows the performance of our proposed method based on different evaluation metrics with varying code and comment lengths.
from figures 8a a b c we can see that in most cases our proposed model has a stable performance even though the code length or node number increases dramatically.
we take this effect into account by attributing it to the hybrid representation we adopt in our model.
from figure 8a d we can see that the performance of our proposed model decreases authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
code length0.
.
.
.
.
.0scoremrr r 1r r a code length.
ast size0.
.
.
.
.
.0scoremrr r 1r r b ast node number.
cfg size0.
.
.
.
.
.0scoremrr r 1r r c cfg node number.
comment length0.
.
.
.
.
.0scoremrr r 1r r d comment length.
figure experimental results of our proposed method on different metrics w.r.t.
varying code cfg node number and comment length.
on four metrics when the lengths of comments increase.
this shows that increasing the length of a comment will increase the difficulty of comment understanding.
overall it further verifies the robustness of multi modal code representation.
g. q4 qualitative analysis and visualization in the previous sections we have shown the effectiveness of our proposed attention mechanism from the evaluation metrics.
to gain some insights on the superiority of multimodal representation and how the attention works we show some cases retrieved by our model with different modality combinations see figure and a visualization on the attention of each modality to interpret which part of code contributes more to the final result see figure .
figure shows the first retrieved result of our proposed mman model with different modality combination and the deepcs for query print any message in the axel structure .
we can see that our proposed model can accurately retrieve the ground truth code snippets when compared with the stateof the art deepcs model as shown in figure 9b .
when comparing figure 9a with figure 9c and figure 9d we can also clearly see the superiority of multi modal approach for accurate code representation.
figure visualizes the attention weights assigned to each part of code w.r.t.
different modalities for the sake of interpreting which part of code contributs more to the final result.
from figure 10a we can see that the attention on tokens can really extract the important parts such as the function name print message .
on the other hand from figure 10b we can see the attention on ast assigns more weight on the leaf node e.g.
axel as well as some operation nodes e.g.
binaryoperator .
furthermore from figure 10c we .
!
!
.
.
.
.
!
.
.
!
!
!
.
!
!
!
!
!
!
.
!
.
!
.
!
.
!
!
.
!
.
.
!
!
!
.
!
!
.
!
!
.
!
!
!
!
!
!
!
!
!
.
!
.
!
.
.
!
figure the retrieved first result of our proposed mman model with different modality combination and the deepcs for query print any message in the axel structure observe that the attention on cfg assigns more weight on the invoked function node e.g.
prinf andfree .
this can be illustrated by the fact that cfg can describe the control flow of a source code snippet.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
figure attention visualization to interpret the contribution of each part of code w.r.t.
different modalities.
vi.
d iscussion a. strength of mman we have identified three advantages of mman that may explain its effectiveness in code retrieval a a more compresensive representation of source code from its multiple modalities.
the mman represents the source code snippet from its multiple modalities i.e.
tokens ast and cfg which contains complementary information for code representation.
b an attention mechanism to assign different weights on different parts for each modality.
the mman contains an attention mechanism which can infer the contribution of each part of code to the final result and through visualization we can obtain a explainability for our deep learning based model and c a unified framework to learn the heterogeneous representation of source code and description in an intermediate semantic space.
the mman is an end to end neural network model with a unified architecture to learn the representation of source code and description in an intermediate semantic space.
b. threats to v alidity and limitations our proposed mman may suffer from two threats to validity and limitations.
one threat to validity is on the evaluation metrics.
we evaluate our approach using only two metrics i.e.
successrate randmrr which are both standard evaluation metrics in information retrieval.
we do not use precision at some cutoff precision k since the relevant results need to be labelled manually.
we argue that this kind of approach will introduce the human bias.
however a humanevaluation is also needed for the sake of fair comparison with deepcs.
another threat to validity lies in the extensibility of our proposed approach.
our model needs to be trained on a large scale of corpus which is collected from online platforms such as github.
since the writing style of different programmers may differ greatly lots of efforts will be put into this step.
in this paper we have defined many regular expressions to extract the samples that meets our condition at the same time many samples are filtered.
furthermore the cfg can only be extracted from a whole program.
therefore it s difficult to extend our multi modal code representation model to some contexts where the cfg are unable to be extracted such as many code snippets from stackoverflow.
vii.
c onclusion and future work in this paper we have proposed a novel multi modal neural network with attention machanism named mman for the task of code retrieval.
apart from considering the sequential features of source code such as tokens and api sequences mman also considers the structure features of code such as ast and cfg to learn a more comprehensive semantic representation for code understanding.
furthermore we put forward an attention mechanism to interpret the contribution of each part of code.
in addition we proposed a unified framework to learn the representation of code representation and natural language query simultaneously.
our experimental study has shown that the proposed approach is effective and outperforms the state of the art approaches.
in our future work we plan to conduct comprehensive experiments on other dataset of different language such as java or python as well as human evaluation to further verify the effectiveness of our proposed approach.
furthermore we believe that it s promising to explore the potentiality of multimodal code representation on some other software engineering tasks such as code summarization and code clone detection.
acknowledgment this paper is partially supported by the subject of the major commissioned project research on china s image in the big data of zhejiang province s social science planning advantage discipline evaluation and research on the present situation of china s image no.
16ysxk01zd2yb the zhejiang university education foundation under grants no.
k18 no.
k17 and no.
k17 the major scientific project of zhejiang lab under grant no.
2018dg0zx01 the national natural science foundation of china under grant no.
the key laboratory of medical neurobiology of zhejiang province and the foundation of state key laboratory of cognitive intelligence grant no.
cogosc iflytek p.r.
china.
this work is also supported in part by nsf under grants iii iii iii satc cns1626432 australian research council linkage project under lp170100891 and australian research grant de170101081.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.