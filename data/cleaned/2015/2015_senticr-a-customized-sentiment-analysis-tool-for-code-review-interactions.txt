senticr a customized sentiment analysis tool for code review interactions toufique ahmed amiangshu bosu anindya iqbal shahram rahimi department of computer science engineering bangladesh university of engineering and technology dhaka bangladesh department of computer science southern illinois university carbondale il usa toufiqueahmed anindya cse.buet.ac.bd abosu rahimi cs.siu.edu abstract sentiment analysis tools developed for analyzing social media text or product reviews work poorly on a software engineering se dataset.
since prior studies have found developers expressing sentiments during various se activities there is a need for a customized sentiment analysis tool for the sedomain.
on this goal we manually labeled review commentsto build a training dataset and used our dataset to evaluateseven popular sentiment analysis tools.
the poor performancesof the existing sentiment analysis tools motivated us to build senticr a sentiment analysis tool especially designed for code review comments.
we evaluated senticr using one hundred fold cross validations of eight supervised learning algorithms.
we found a model trained using the gradient boosting tree gbt algorithm providing the highest mean accuracy the highest mean precision .
and the highest mean recall .
in identifying negative review comments.
i. i ntroduction a person s sentiment i.e.
positive or negative attitude towards another person entity or event significantly influences his her decision making process such as forming relationships choosing candidates in a local election selecting commercialproducts reviewing movies or predicting financial conditionof a stock market .
sentiments not only influence thequality of relationship between two persons but also have highimpacts on productivity task quality task synchronization and job satisfaction of collaborative activities such assoftware development .
due to the limited availability offace to face communications oss developers primarily usevarious text based tools such as mailing lists forums sourcecode repositories code reviews and issue tracking tools tomanage their collaborations .
prior software engineer ing se studies found developers expressing sentiments incommit messages issue tracking systems project artifacts and mailing lists .
yet thelack of a reliable sentiment analysis sa tool forthe se domain has hindered evaluating the impacts of thosesentiments.
most of the prior research on sa techniques have focused on analyzing social media posts product reviews or moviereviews.
although existing sa tools work well on socialmedia posts or product reviews those perform poorly ona se dataset .
the text of se communicationsoften differ from articles books or even spoken languageas those often include technical jargons word contractions emoticons urls and code snippets.
since sa tools need tobe customized for each domain the poor performancesof existing sa tools on a se dataset is not surprising.
to validate the need for a customized sa tool for the se domain we built a sentiment oracle by manually labeling 2000randomly selected review comments from popular oss projects.
using our oracle we evaluated the performances of seven commonly used sa tools.
the poor performances ofexisting sa tools on our dataset motivated us to implementsenticr a supervised learning based sa tool for code reviews.we evaluated eight commonly used supervised learning algo rithms based on hundred fold cross validations.
we found a model trained using the gradient boosting tree gbt algorithm providing the highest mean accuracy thehighest mean precision .
and the highest mean recall .
in identifying review comments expressing negativesentiments.
in summary the primary contributions of this study are an empirically built sentiment oracle for the se domain.
senticr a supervised sentiment analysis tool for codereview comments.
both senticr and our sentiment oracleare publicly available at a comparative analysis of existing sentiment analysistools on a se dataset.
the remainder of the paper is organized as follows.
section ii provides background about code review and sentiment analysis.
section iii evaluates existing sentiment analysis tools.section v describes the threats to validity of our findings.
finally section vi provides some directions for future workand concludes the paper.
ii.
b ackground this section presents a brief background on two topics relevant to this study peer code review and sentiment analysis.
a. code review code review is the practice where a developer submits his her code change to a peer to judge its eligibility to be included into the main project code base .
compared withthe traditional heavy weight inspection process peer codereview is more informal tool based and used regularly in .
c circlecopyrt2017 ieeease urbana champaign il usa t echnical research new ideas106 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
practice .
because the peer code review process has been established to be effective many oss projects have adoptedit into their development process .
b. sentiment analysis sentiment analysis is a natural language processing technique that analyzes the attitude of a speaker or an author of a body of text towards entities such as products services organizations individuals issues or events .
sentimentanalysis techniques aim to identify polarity i.e.
positive negative or neutral in a sentence or a paragraph.
researchers have primarily used two types of sentiment analysis techniques.
supervised learning based techniques which are able to adapt and create trained models for specific purposes and contexts can be used in conjunction with any of the existing supervised learning methods e.g.
na ve bayes and svm .
instead of using standard supervisedtechniques researchers have also proposed several customtechniques specifically for sentiment classification tasks thatcan take into account the contexts of words expressing senti ments .
however supervised learning techniques requirea labeled training dataset which might be costly or evenprohibitive.
on the other hand lexicon based analyzers which do not require a training dataset identify the sentiment for a docu ment from the semantic orientations of words or phrases inthe document .
although lexical methods do not rely on alabeled dataset it is hard to create a unique lexicon based dictionary suitable for different contexts and often require customizations of the dictionary for each domain .
iii.
e v aluation of sentiment analysis tools most of the prior se studies used either sentistrength a lexicon based analyzer or nltk a supervised learning based classifier trained on moviereviews .
however a recent study observednot only poor accuracies but also significant disagreementsbetween these two tools which could potentially lead tocontradictory conclusions.
to use in our study an evaluationof the existing tools on a se dataset is necessary.
sinceno labeled sentiment dataset exists in the se domain webuilt a sentiment dataset using randomly selected code reviewcomments.
following sections describe our dataset generationprocess and an evaluation of seven popular sentiment analysistools.
a. training dataset generation we adopted following eight step approach to build a labeled sentiment dataset from code review comments.
we used our gerrit miner tool to mine the code review repositories of popular oss projects.our project selection was based on two criteria i an open sourceproject actively using gerrit and ii each project hasperformed at least code reviews.
a manual inspection of the comments posted by some accounts e.g.
qt sanity bot or buildbot suggested fig.
.
web app to manually label the review comments that those accounts were automated bots rather thanhumans.
these accounts typically contain one of thefollowing keywords bot auto ci jenkins inte gration build hook recheck travis or verifier .because we wanted only code review comments fromactual reviewers we excluded these bot accounts after amanual inspection had confirmed that the comments wereautomatically generated.
we randomly selected total review comments each having at least characters from the selected twentyprojects comments from each project .
the random ness of our selection process ensures that all types ofreview comments are included in our dataset.
we did not adopt a proportionate selection as some of the large projects e.g.
android chromium os and openstack had significantly higher number of code reviews than theothers.
a proportionate selection would have biased ourdataset in favor of the vocabulary of those large projects.
we developed a web app figure to manually label the selected review comments.
three of the authors independently labeled each of the review comment as positive negative or neutral based on what s he would personally perceive if s he was the recipient of areview comment.
to eliminate potential biases the raters could not view the ratings provided by others.
the three raters had consensus on comments .
in their initial independent ratings.
we used fleiss kappa useful for more than two raters to measure the level of agreement among the threeraters.
.
p .
indicates a moderate agreement for our initial ratings.
the perception ofsentiment from a text varies across different persons .while it was possible to achieve higher consensus amongour initial ratings with a prior discussed rating strategy such a rubrics would have prevented us from capturing areal world sentiment perception.
for the comments where one of the raters disagreed we had discussion sessions to determine the final ratings.
the final distribution of the labeled comments were .
positive .
negative and .
neutral .to improve the performances of sentiment classifiers weconverted our three class dataset into a two class datasetby relabeling both positive and neutral comments as non negative .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i performances of seven sentiment analysis tools precision recall f measure accurac y afinn .
.
.
.
nltk hu and liu .
.
.
.
sentistrength .
.
.
.
textblog .
.
.
.
usent .
.
.
.
nltk vader .
.
.
.
vivekn .
.
.
.
since the labeled dataset is highly imbalanced we used undersampling i.e.
randomly excluding a subset of the majority class to exclude neutral comments.we use this dataset of labeled comments as an oracle for all the subsequent analyses in this study.
ourdataset satisfies thewall s recommendation of minimum1000 labeled text for sentiment training .
b. evaluation results we evaluated seven popular sentiment analysis tools five lexicon based and two supervisedlearning based using our oracle.
we measure theperformances of the tools based on four measures precision recall f measure and accuracy.
table i shows the perfor mances of the seven tools.
each of the tools performed poorlyin identifying negative review comments with the highestprecision of only nltk vader .
although vivekn hadthe highest recall .
it had the lowest precision .
.none of the tools achieved a f measure above .
validatingthe need for a customized sentiment analysis tool for codereview comments.
since these tools are not trained using a sedataset their inaccuracies are not surprising.
a reliable senti ment analyzer for the se domain requires either a supervisedmodel trained on a se dataset or a customized lexicon basedse dictionary.
iv .
s enti c r as entiment analysis tool for code reviews the poor performances of existing sentiment analysis tools motivated us to implement senticr a supervised learning based sentiment analysis tool for code review comments.we wrote senticr in python using nltk for languagepreprocessing and scikit learn for supervised learningalgorithms.
in the following subsections we describe the datapreprocessing steps and evaluation of senticr.
a. data preprocessing the text of a review comment differs from articles books or even spoken language.
for example review comments often contain word contractions emoticons urls and code snippets.
therefore we implemented an eight step data pre processing as follows.
expansion of contractions contractions which are shortened form of one or two words are widely used ininformal written communications.
some commonly used con tractions and their expanded forms include i m ia m doesn t does not and don t do not.
by creating two different lexicons of the same term contractions increasethe number of unique lexicons and misrepresent the realcharacteristics of a dataset.
we replaced the commonly used124 contractions each with its expanded version.
url removal a review comment may include url