on learning meaningful code changes via neural machine translation michele tufano jevgenija pantiuchina cody watson gabriele bavota denys poshyvanyk college of william and mary williamsburg virginia usa email mtufano cawatson denys cs.wm.edu universit della svizzera italiana usi lugano switzerland email gabriele.bavota jevgenija.pantiuchina usi.ch abstract recent years have seen the rise of deep learning dl techniques applied to source code.
researchers have exploited dl to automate several development and maintenance tasks such as writing commit messages generating comments and detecting vulnerabilities among others.
one of the long lasting dreams of applying dl to source code is the possibility to automate non trivial coding activities.
while some steps in this direction have been taken e.g.
learning how to fix bugs there is still a glaring lack of empirical evidence on the types of code changes that can be learned and automatically applied by dl.
our goal is to make this first important step by quantitatively and qualitatively investigating the ability of a neural machine translation nmt model to learn how to automatically apply code changes implemented by developers during pull requests.
we train and experiment with the nmt model on a set of 236k pairs of code components before and after the implementation of the changes provided in the pull requests.
we show that when applied in a narrow enough context i.e.
small mediumsized pairs of methods before after the pull request changes nmt can automatically replicate the changes implemented by developers during pull requests in up to of the cases.
moreover our qualitative analysis shows that the model is capable of learning and replicating a wide variety of meaningful code changes especially refactorings and bug fixing activities.
our results pave the way for novel research in the area of dl on code such as the automatic learning and applications of refactoring.
index t erms neural machine translation empirical study i. i ntroduction several works recently focused on the use of advanced machine learning techniques on source code with the goal of semi automating several non trivial tasks including code completion generation of commit messages method names code comments defect prediction bug localization and fixing clone detection code search and learning api templates .
the rise of this research thread in the software engineering se community is due to a combination of factors.
the first is the vast availability of data specifically source code and its surrounding artifacts in open source repositories.
for instance at the time of writing this paper github alone hosted 100m repositories with over 200m merged pull requests prs and 2b commits.
second dl has become a useful tool due to its ability to learn categorization of data through the hidden layer architecture making it especially proficient in feature detection .
specifically neural machine translation nmt has become a premier method for the translation of differentlanguages surpassing that of human interpretation .
a similar principle applies to translating one piece of source code into another.
here the ambiguity of translating makes this method extremely versatile one can learn to translate buggy code into fixed code english into spanish java into c etc.
the third is the availability of relatively cheap hardware able to efficiently run dl infrastructures.
despite all the work only a few approaches have been proposed to automate non trivial coding activities.
tufano et al.
showed that dl can be used to automate bug fixing activities.
however there is still a lack of empirical evidence about the types of code changes that can actually be learned and automatically applied by using dl.
while most of the works applying dl in se focus on quantitatively evaluating the performance of the devised technique e.g.
how many bugs is our approach able to fix?
little qualitative analysis has been done to deeply investigate the meaningfulness of the output produced by dl based approaches.
in this paper we make the first empirical step towards extensively investigating the ability of an nmt model to learn how to automatically apply code changes just as developers do in prs.
we harness nmt to automatically translate a code component from its state before the implementation of the pr and after the pr has been merged thereby emulating the combination of code changes that would be implemented by developers in prs.
we mine three large gerrit code review repositories namely android google source and ovirt .
in total these repositories host code reviews related to subprojects.
we collected from these projects merged prs that underwent code review.
we only considered merged and reviewed prs for three reasons.
first we wanted to ensure that an nmt model is learning meaningful changes thus justifying the choice of mining reviewed prs as opposed to any change committed in the versioning system.
second given the deep qualitative focus of our study we wanted to analyze the discussions carried out in the code review process to better understand the types of changes learned by our approach.
indeed while for commits we would only have commit notes accompanying them with a reviewed pr we can count on a rich qualitative data explaining the rationale behind the implemented changes.
third we only focus on merged prs since the code before and after i.e.
merged the pr is available.
this is not the case for abandoned prs.
we extract method level ast ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
edit operations from these prs using fine grained source code differencing .
this resulted in method pairs each of them representing the method before pr not submitted and after pr merged the pr process.
an encoder decoder recurrent neural network rnn is then used to learn the code transformations performed by developers during pr activities.
we demonstrate a quantitative and qualitative evaluation of the nmt model.
for the quantitative analysis we assessed its ability in modifying the project s code exactly as done by developers during real prs.
this means that we compare for the same code components the output of the manually implemented changes and of the output of the nmt model.
the qualitative analysis aims instead at distilling a taxonomy of meaningful code transformations that the model was able to automatically learn from the training data see fig.
.
the achieved results indicate that in its best configuration the nmt model is able to inject the same code transformations that are implemented by developers in prs in of cases depending on the number of possible solutions that it is required to produce using beam search .
moreover the extracted taxonomy shows that the model is able to learn a rich variety of meaningful code transformations automatically fixing bugs and refactoring code as humans would do.
as explained in section iii these results have been achieved in a quite narrow context i.e.
we only considered pairs of small medium methods before after the implementation of the changes carried by the pr and this is also one of the reasons why our infrastructure mostly learned bug fixing and refactoring activities as opposed to the implementation of new features .
however we believe that our results clearly show the potential of nmt for learning and automating non trivial code changes and therefore can pave the way to more research targeting the automation of code changes e.g.
approaches designed to learn and apply refactorings .
to foster research in this direction we make publicly available the complete datasets source code tools and raw data used in our experiments .
ii.
a pproach our approach starts with mining prs from three large gerrit repositories sec.
ii a .
we extract the source code before and after the prs are merged.
we pair pre pr and post pr methods where each pair serves as an example of a meaningful change sec.
ii b .
method pairs are then abstracted filtered and organized in datasets sec.
ii c .
we train our model to translate the version of the code before the pr into the one after the pr to emulate the code change sec.
ii d .
finally nmt s output model is concretized into real code sec.
ii e .
a. code reviews mining we built a gerrit crawler to collect the pr data needed to train the nmt model.
given a gerrit server the crawler extracts the list of projects hosted on it.
then for each project the crawler retrieves the list of all prs submitted for review and having merged as the final status.
we then process each merged pr pusing the following steps.
first let us define the set of java files submitted in pasfs f1 f2 ... f n .we ignore non java files since our nmt model only supports java.
for each file in fs we use the gerrit api to retrieve their version before the changes implemented in the pr.
the crawler discards new files created in the pr i.e.
not existing before the pr since we cannot learn any code transformation from them we need the code before after the pr to learn changes implemented by developers .
then gerrit api is used to retrieve the merged file versions impacted by the pr.
the two before after file sets might not be exactly the same due to files created deleted during the review process.
the output of the crawler is for each pr the version of the files impacted before pre pr and after post pr merged the pr.
at the end of the mining process we obtain three datasets of prs pr ovirt pr android and pr google .
b. code extraction each mined pr is represented as pr f1 ... f n f prime ... f prime m where f1 ... f n are the source code files before the pr and f prime ... f prime mare code files after the pr.
as previously explained the two sets may or may not be the same size since files could have been added or removed during the pr process.
in the first step we rely on gumtreediff to establish the file to file mapping performed using semantic anchors between pre and post pr files and disregarding any file added removed during the code review process.
after this step each pr is stored in the format pr f1 ... f k f prime ... f prime k where fiis the file before and f prime ithe corresponding version of the file after the pr.
next each pair of files fi f prime i is again analyzed using gumtreediff which establishes method to method mapping and identifies ast operations performed between two versions of the same method.
we select only pairs of methods for which the code after the pr has been changed with respect to the code before the pr.
then each pr is represented as a list of paired methods pr mb ma ... mb ma n where each pair mb ma icontains the method before the pr mb and the method after the pr ma .
these are examples of changes used to train an nmt model to translate mbinma.
we use the method level granularity for several reasons i methods implement a single functionality and provide enough context for a meaningful code transformation ii file level code changes are still possible by composing multiple methodlevel code transformations iii files represent large corpus of text with potentially many lines of untouched code during the pr which would hinder our goal to train a nmt model.
in this paper we only study code changes which modify existing methods disregarding code changes that involve the creation or deletion of entire methods files see section v .
c. code abstraction filtering nmt models generate sequences of tokens by computing probability distributions over words.
they can become very slow or imprecise when dealing with a large vocabulary comprised of many possible output tokens.
this problem has been addressed by artificially limiting the vocabulary size considering only most common words assigning special tokens authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i vocabularies dataset v ocabulary abstracted v ocabulary google android ovirt all e.g.
unk to rare words or by learning subword units and splitting the words into constituent tokens .
the problem of large vocabularies a.k.a.
open vocabulary is well known in the natural language processing nlp field where languages such as english or chinese can have hundreds of thousands of words.
this problem is even more pronounced for source code.
as a matter of fact developers are not limited to a finite dictionary of words to represent source code rather they can generate a potentially infinite amount of novel identifiers and literals.
table i shows the number of unique tokens identified in the source code of the three datasets.
the vocabulary of the datasets ranges between 42k and 267k while the combined vocabulary of the three datasets exceeds 370k unique tokens.
in comparison the oxford english dictionary contains entries for words .
in order to allow the training of an nmt model we need a way to reduce the vocabulary while still retaining semantic information of the source code.
we employ an abstraction process which relies on the following observations regarding code changes i several chunks of code might remain untouched ii developers tend to reuse identifiers and literals already present in the code iii frequent identifiers i.e.
common api calls and variable names and literals e.g.
foo are likely to be introduced in code changes.
we start by computing the top most frequent identifiers i.e.
type method and variable names and literals i.e.
int double char string values used in the source code for each of the three datasets.
this set contains frequent types api calls variable names and common literal values e.g.
n that we want to keep in our vocabulary.
subsequently we abstract the source code of the method pairs by means of a process that replaces identifiers and literals with reusable ids.
the source code of a method is fed to a lexer built on top of antlr which tokenizes the raw code into a stream of tokens.
this stream of tokens is then fed into a java parser which discerns the role of each identifier i.e.
whether it represents a variable method or type name and the type of a literal.
each unique identifier and literal is mapped to an id having the form of category where category represents the type of identifier or literal i.e.
type method var int float char string and is a numerical id generated sequentially for each unique type of instance within that category e.g.
the first method will receive method 0 the third integer value int 2 etc.
.
these ids are used in place of identifiers and literals in the abstracted code while the mapping between ids and actual identifier literal values is saved in a map m which allows us to map back the ids in the code concretization phase section ii e .
during the abstraction process we replace all identifiers literals with ids except for the list of most frequent identifiers and literals for which we keep the original token value in the corpus.
given a method pair mb ma the method mbis abstracted first.
then using the same mapping m generated during the abstraction of mb the method mais abstracted in such a way that identifiers literals already available in mwill use the same id while new identifiers literals introduced in ma and not available in mb will receive a new id.
at the end of this process from the original method pair mb ma we obtain the abstracted method pair amb a m a .
we allow ids to be reused across different method pairs e.g.
the first method name will always receive the id method 0 therefore leading to an overall reduction of the vocabulary size.
the third column of table i reports the vocabulary size after the abstraction process which shows a significant reduction in the number of unique tokens in the corpus.
in particular after the abstraction process the vocabulary contains i java keywords ii top identifiers literals iii reusable ids.
it is worth noting that the last row in table i i.e.
all does not represent the cumulative sum but rather the count of unique tokens when the three dataset corpora are merged.
having a relatively small vocabulary allows the nmt model to focus on learning patterns of code transformations that are common in different contexts.
moreover the use of frequent identifiers and literals allows the nmt model to learn typical changes e.g.
if i toif i and introduce api calls based on other api calls already available in the code.
after the abstraction process we filter out method pairs from which the nmt model would not be able to learn code transformations that will result in actual source code.
to understand the reasoning behind this filtering it is important to understand the real use case scenarios.
when the nmt model receives the source code of the method amb it can only perform code transformations that involve i java keywords ii frequent identifiers literals iii identifiers and literals already available in mb.
therefore we disregard method pairs where macontains tokens not listed in the three aforementioned categories since the model would have to synthesize new identifies or literals not previously seen.
in the future we plan to increase the number of frequent identifiers and literals used in the vocabulary with the aim of learning code transformations from as many method pairs as possible.
we also filter out those method pairs such that amb ama meaning the abstracted code before and after the pr appear the same.
we remove these instances since the nmt model would not learn any code transformation.
next we partition the method pairs in small and medium pairs based on their size measured in the number of tokens.
in particular small method pairs are those no longer than tokens while we consider medium pairs those having a length between tokens.
in this stage we disregard longer method pairs.
we discuss this limitation in section v. table ii shows the number of method pairs after the abstraction and filtering process for each dataset and the combined one i.e.
all .
each of the four datasets is then randomly partitioned into training validation authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii da tasets dataset msmall mmedium google android ovirt all and test sets.
before so we make sure to remove any duplicate method pairs to ensure that none of the method pairs in the test set have been seen during the training phase.
d. learning code transformations in this section we describe the nmt models we use to learn code transformations.
in particular we train these models to translate the abstracted code ambinama effectively simulating the code change performed in the pr by developers.
rnn encoder decoder to build such models we rely on an rnn encoder decoder architecture with attention mechanism commonly adopted in nmt tasks .
as the name suggests this model consists of two major components an rnn encoder which encodes a sequence of tokens xinto a vector representation and an rnn decoder which decodes the representation into another sequence of tokens y. during training the model learns a conditional distribution over a output sequence conditioned on another input sequence of terms p y1 .. y m x1 .. x n where the lengths nandmmay differ.
in our setting given the sequence representing the abstract code before the pr x amb x1 .. x n and a corresponding target sequence representing the abstract code after the pr y ama y1 .. y m the model is trained to learn the conditional distribution p ama amb p y1 .. y m x1 .. x n where xiandyjare abstracted source tokens java keywords separators ids and frequent identifiers and literals.
the encoder takes as input a sequence x x1 .. x n and produces a sequence of states h h1 .. h n .
in particular we adopt a bi directional rnn encoder which is formed by a backward and a forward rnn.
the rnns process the sentence both from left to right and right to left and are able to create sentence representations taking into account both past and future inputs .
the rnn decoder predicts the probability of a target sequence y y1 .. y m given h. specifically the probability of each output token yiis computed based on i the recurrent state si in the decoder ii the previous i 1tokens y1 .. y i and iii a context vector ci.
this vector ci also called attention vector is computed as a weighted average of the states in h ci summationtextn t 1aithtwhere the weights aitallow the model to pay more attention to different parts of the input sequence when predicting the token yi.
encoder and decoder are trained jointly by minimizing the negative log likelihood of the target tokens using stochastic gradient descent.
beam search decoding for each method pair amb a m a the model is trained to translate ambsolely into the corresponding ama.
however during testing we would like to obtain multiple possible translations.
precisely given a piece of source code mas input to the model we would liketo obtain kpossible translations of m. to this aim we employ a decoding strategy called a beam search used in previous applications of dl .
the major intuition behind a beam search decoding is that rather than predicting at each time step the token with the best probability the decoding process keeps track of khypotheses with kbeing the beam size .
formally lethtbe the set of khypotheses decoded until time step t ht y1 ... y1 t y2 ... y2 t ... yk ... yk t at the next time step t for each hypothesis there will be v possible yt terms vbeing the vocabulary for a total ofk v possible hypotheses ct k uniondisplay i yi ... yi t v1 ... yi ... yi t v v from these candidate sets the decoding process keeps the k sequences with the highest probability.
the process continues until each hypothesis reaches the special token representing the end of a sequence.
we consider these kfinal sentences as candidate patches for the buggy code.
hyperparameter search we tested ten configurations of the encoder decoder architecture with different combinations of rnn cells lstm and gru number of layers and units for the encoder decoder and the embedding size .
bucketing and padding was used to deal with the variable length of the sequences.
we trained the models for a maximum of 60k epochs and selected the model s checkpoint before over fitting the training data.
to guide the selection of the best configuration we used the loss function computed on the validation set not on the test set while the results are computed on the test set.
e. code concretization in this final phase the abstracted code generated as output by the nmt model is concretized by mapping back all the identifiers and literal ids to their actual values.
the process simply replaces each id found in the abstracted code to the real identifier literal associated with the id and saved in the mapping m for each method pair.
the code is automatically indented and additional code style rules can be enforced during this stage.
while we do not deal with comments they could be reintroduced in this stage as well.
iii.
s tudy design the goal of this study is to empirically assess whether nmt can be used to learn a diverse and meaningful set of code changes.
the context consists of a dataset of prs and aims at answering two research questions rqs .
a. rq1 can neural machine translation be employed to learn meaningful code changes?
we aim to empirically assess whether nmt is a viable approach to learn transformations of the code as performed by developers in prs.
to this end we use the eight datasets of method pairs listed in table ii.
given a dataset we train different configurations of the encoder decoder models on authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the training set then use the validation set to select the best performing configuration of the model.
we then evaluate the validity of the model with the unseen instances of the test set.
in total we experiment with eight different models one for each dataset in table ii i.e.
one model trained configured and evaluated on the google dataset of small methods one on the google dataset of medium methods etc.
.
the evaluation is performed by the following methodology.
letm be a trained model and tbe the test set of dataset d we evaluate the model m for each amb a m a t. specifically we feed the pre pr abstract code ambto the model m performing inference with beam search decoding for a given beam size k. the model will generate kdifferent potential code transformations ct ct1 ... c tk .w es a y that the model successfully predicted a code transformation if there exists a cti ct such that cti ama i.e.
the abstract code generated by developers after the merging of the pr .
we report the raw count and percentage of successfully predicted code changes in the test set with k .
in other words given a source code method that the model has never seen before we evaluate the model s ability to correctly predict the code transformation that a developer performed by allowing the model to generate its best guess i.e.
k or the top and top best guesses.
it should be noted that while we count only perfect predictions there are many other slightly different transformations that can still be viable and useful for developers.
however we discount these less than perfect predictions since it is not possible to automatically categorize those as viable and non viable.
b. rq2 what types of meaningful code changes can be performed by the model?
in this rq we aim to qualitatively assess the types of code changes that the nmt model is able to generate.
to this goal we focus only on the successfully predicted code transformations generated by the model trained on the all dataset considering both small and medium sized methods.
one of the authors manually investigated all the successfully predicted code transformations and described the code changes.
subsequently a second author discussed and validated the described code changes.
finally the five authors together defined and iteratively refined a taxonomy of code transformations successfully performed by the nmt model.
iv .
s tudy results a. rq1 can neural machine translation be employed to learn meaningful code changes?
table iii reports the perfect predictions i.e.
successfully predicted code transformations by the nmt models in terms of raw numbers and percentages of the test sets.
when we allow the models to generate only a single translation i.e.
beam they are able to predict the same code transformation performed by the developers in up to of the cases.
it is worth noting how the model trained on the combined datasets i.e.
all is able to outperform all the other single dataset model achieving impressive results even with a single guess .
table iii perfect predictions dataset beam msmall mmedium google1 .
.
.
.
.
.
android1 .
.
.
.
.
.
ovirt1 .
.
.
.
.
.
all1 .
.
.
.
.
.
for small and .
for medium methods .
this result shows that nmt models are able to learn code transformations from a heterogeneous set of examples belonging to different datasets.
moreover this also provides preliminary evidence that transfer learning would be possible for such models.
on the other end of the spectrum the poor performance of the models trained on google s dataset could be explained by the limited amount of training data see table ii with respect to the other datasets.
when we allow the same models to generate multiple translations of the code i.e.
and we observe a significant increase in perfect predictions across all models.
on average out of code transformations can be generated and perfectly predicted by the nmt model trained on the combined dataset.
the model can generate transformations in less than one second on a consumer level gpu.
summary for rq .nmt models are able to learn meaningful code changes and perfectly predict code transformations in up to of the cases when only one translation is generated and up to when possible guesses are generated.
b. rq2 what types of meaningful code changes can be performed by the model?
here we focus on the perfect predictions generated by the model trained on the whole dataset i.e.
all with beam size equals .
these perfect predictions were the results of unique types of ast operations as detected by gumtreediff that the model was able to emulate.
the complete list is available in our replication package .
fig.
shows the taxonomy of code transformations that we derived by manually analyzing the perfect predictions.
note that a single perfect prediction can include multiple types of changes falling into different categories of our taxonomy e.g.
a refactoring and a bug fix implemented in the same code transformation .
for this reason the sum of the classified changes in fig.
is .
the taxonomy is composed of three sub trees grouping code transformations related to bug fixing refactoring and other types of changes.
the latter includes code transformations that the model correctly performed i.e.
those replicating what was actually done by developers during the reviewed prs but for which we were unable to understand the rationale behind the code transformation i.e.
w h yi tw a s authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
refactoring inheritance forbid overriding add final to methodmethods interaction add parameter remove variable from method bodyencapsulation broad method visibilitynarrow method visibilitynaming rename parameter rename method rename variablereadability add braces to if statementadd remove this quali fierreplace generic speci fication with diamond operatorremove redundant else keywordreplace anonymous class with lambda expressionmerge catch blocks capturing both exceptions in catch expression merge variable definition initializationremove redundant initialization use ?
in generics as return type simplify if conditioninvoke overridden method instead of inherited one abstract an existing method using the abstract keywordbug fix methods invocation change parameters order in method invocationchange parameter value of invoked methodexceptions add try blockmove existing statements in try blockmove existing statements out of try blockconditional statements add remove operand from conditionadd null check change comparison operator e.g.
in conditionmodify if condition change operands order in if conditionvalues change method return valuelock mechanism remove synchronized keyword from methodremove synchronized blockremove thrown exceptionchange exception type in catch clausereplace if statement with assert statementmove synchronized keyword from signature to code block or vice versa other type remove type casting in method bodytriggered by other changesinitialization forbid multiple assignments add final to variableremove type casting in method signaturemethod signature add remove parameterdelete code change quali fied name in response to a move class refactoringremove finally from try catchremove try catchremove if conditionremove statementchange parameter typechange return typechange type of a variable class is not static anymore.
add object instance to invoke its methodsclass becomes static.
delete object instance to invoke its methodschange method invocation as result of a move methodadd code add conditionadd parameter in method constructor invocationreplace code replace invoked method replace statement add statementadd invoked method remove parameter from the method invocationremove invoked method104 fig.
.
taxonomy of code transformations learned by the nmt model performed .
we preferred to adopt a conservative approach and categorize transformations into refactoring and bug fix sub trees only when we can confidently link to these types of activities.
also for transformations the authors did not agree on the type of code change and hence we excluded them from our taxonomy related to perfect predictions .
here we qualitatively discuss interesting examples indicated using the code forkicon of code transformations belonging to our taxonomy.
we do not report examples for all possible categories of changes learned by the model due to lack of space.
y et the complete set of perfect predictions and their classification is available in our replication package .
c. refactoring we grouped in the refactoring sub tree all code transformations that modify the internal structure of the system by improving one or more of its non functional attributes e.g.
readability without changing the system s external behavior.
we categorized transformations into five sub categories.
inheritance refactorings that impact how the inheritance mechanism is used in the code.
we found three types of refactorings related to inheritance i forbid method overriding by adding the final keyword to the method declaration ii invoke overriding method instead of overridden by removing thesuper keyword to the method invocation and iii making a method abstract through the abstract keyword and deleting the method body.
code forkexisting method declared as final .i nt h e directbytebuffer class of android the nmt model added to the signature of the getlong int method the final keyword.
as stated by the developer implementing the pr directbytebuffer cannot be final but we can declare most methods final to make it easier to reason about .
code forkremoved unnecessary super specifier .
ap ri nt h e ovirt core subsystem was performed to clean up the class randomutil that extends java class java .util .random .
the nextshort method implemented in the refactored class was invoking nextint of the base class through the use of the super java specifier.
however such a specifier was redundant because nextint was not overridden in randomutil .
thus it was removed by the developer using this modifier has no meaning in the context that was removed .
code forkexisting method converted to abstract .
float getfloatunchecked int index throw new unsupportedoperationexception abstract float getfloatunchecked int index the above code listing shows the code taken as input by the nmt model top part pre pr and produced as output bottom post pr .
the code transformation replicates the changes implemented by a developer in a pr converting thegetfloatunchecked method into an abstract method deleting its body.
the rationale for this change is explained authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
by the developer who implemented this change the method getfloatunchecked is overridden in all child classes of the abstract class implementing it and thus there is no need for the abstract base class to carry an implementation that throws unsupportedoperationexception .
the developer also mentions alternative solutions such as moving this and similar methods into an interface but concludes saying that the effort would be much higher.
this case is interesting for at least two reasons.
first our model was able to learn a combination of code transformations needed to replicate the pr implemented by the developer i.e.
add the abstract keyword and delete the method body .
second it shows the rich availability of information about the rationale for the implemented changes available in code review repositories.
this could be exploited in the future to not only learn the code transformation but also to justify it by automatically deriving the rationale from the developers discussion.
methods interaction these refactorings impact the way in which methods of the system interact and include i add parameter refactoring i.e.
a value previously computed in the method body is now passed as parameter to it and ii broadening the return type of a method by using the java wildcard ?
symbol.
code forkmethod returns a broader generic type .
i restmodifyview p i post p parent throws restmodifyview p ?
post p parent throws the code listing shows a change implemented in a pr done on the google gerrit repository and correctly replicated by the nmt model.
the post method declaration was refactored to return a broader type and improve the usage of generics.
as explained by the developer this also allows to avoid the unchecked warnings from the five implementations of the post method present in the system thus simplifying the code.
naming this category groups refactorings related to the renaming of methods parameters and variables.
this is usually done to improve the expressiveness of identifiers and to better adhere to the coding style guidelines.
indeed good identifiers improve readability understandability and maintainability of source code .
code forkrename method .
one example of correctly learned rename method is the one fixing a typo from the onsucess method in the ovirt system .
in this case the developer and the nmt model both suggested to rename the method inonsuccess .
code forkrename parameter .
a second example of renaming is the renamed parameter proposed for the endtrace jmethod type method in a pr impacting the abstracttracerbrush class in the android repository .
the developer here renamed several parameters for clarity and in this case renamed the type parameter into method to make it more descriptive and better reflect its aim.
encapsulation we found refactorings aimed at broadening and narrowing the visibility of methods see fig.
.
thiscan be done by modifying the access modifiers e.g.
changing a public method to a private one .
code forkbroadening and narrowing method visibility.
an example of a method for which our model recommended to broaden its visibility from private topublic is the of method from the key android class .
this change was done in a pr to allow the usage of the method from outside the class since the developer needed it to implement a new feature.
the visibility was instead narrowed from public to private in the context of a refactoring performed by a developer to make more methods private .
this change impacted the currentuser .getuser method from the google repository and the rationale for this change correctly replicated by the nmt model was that the getuser method was only used in one location in the system outside of its class.
however in that location the value of the user is already known thus do not really requiring the invocation of getuser .
readability readable code is easier to understand and maintain .
we found several types of code transformations learned by the model and targeting the improvement of code readability.
this includes i braces added to if statements with the only goal of clearly delimiting their scope ii the merging of two statements defining e.g.
string address and initializing e.g.
address getaddess a variable into a single statement both e.g.
string address getaddess iii the addition removal of the this qualifier to match the project s coding standards iv reducing the verbosity of a generic declaration by using the java diamond operator e.g.
map string list string mapping new hashmap string list string becomes map string list string mapping new hashmap v remove redundant else keywords from if statements i.e.
when the code delimited by the else statement would be executed in any case vi refactoring anonymous classes implementing one method to lambda expressions to make the code more readable vii simplifying boolean expressions e.g.
if x true becomes if x where x is a boolean variable and viii merging two catch blocks capturing different exceptions into one catch block capturing both exceptions using the or operator .
code forkanonymous class replaced with lambda expression .
public boolean isdiskexist return execute new java.util.concurrent.callable java.
lang.boolean java.lang.override public java.lang.boolean call try public boolean isdiskexist return execute try in the above code listing the nmt model automatically replaces an anonymous class top part pre pr with a lambda expression bottom part post pr replicating changes made by ovirt s developers during the transitions of the code through java .
the new syntax is more compact and readable.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
code forkmerging catch blocks capturing different exceptions .
public static integer getinteger string nm integer val try catch illegalargumentexception e catch nullpointerexception e public static integer getinteger string nm integer val try catch illegalargumentexception nullpointerexception e as part of a pr implementing several changes the two catch blocks of the getinteger method were merged by the developer into a single catch block see the code above .
the nmt model was able to replicate such a code transformation that is only meaningful when an exception is caught and the resulting code that is executed is the same for both instances of the exception as in this case .
this code change while simple from a developer s perspective is not trivial to learn due to the several transformations to implement i.e.
removal of the twocatch blocks and implementation of a new catch block using the or operator and to the pre condition to check i.e.
the same behavior implemented in the catch blocks .
d. bug fix changes in the bug fix subtree see fig.
include changes implemented with the goal of fixing a specific bug which has been introduced in the past.
the learned code transformations are organized here into five sub categories grouping changes related to bug fixes that deal with i exception handling ii the addition modification of conditional statements iii changes in the value returned by a method iv the handling of lock mechanisms and v wrong method invocations.
exception this category of changes is further specialized into several subcategories see fig.
including i the addition delation of thrown exceptions ii the addition of try catch finally blocks iii narrowing or broadening the scope of the try block by moving the existing statements inside outside the block iv changing the exception type in the catch clause to a narrower type e.g.
replacing throwable with runtimeexception .
code forkadd try catch block .
public void test getport throws ioexception datagramsocket thesocket new datagramsocket public void test getport throws ioexception try datagramsocket thesocket new datagramsocket the above code from the android repository shows the change implemented in a pr aimed at fixing resource leakages in tests .
the transformation performed by the nmt model wrapped the creation and usage of a datagramsocket object into a try with resources block.
this way thesocket .close will be automatically invoked or an exception will be thrown thus avoiding resource leakage.
code forknarrowed the scope of try block .
public void testget nullpointerexception try concurrenthashmap c new concurrenthashmap c.get null shouldthrow catch java.lang.nullpointerexception success public void testget nullpointerexception concurrenthashmap c new concurrenthashmap try c.get null shouldthrow catch java.lang.nullpointerexception success another change replicated by the nmt model and impacting the andorid test suite is the code transformation depicted above and moving the concurrenthashmap object instantiation outside of the try block.
the reason for this change is the following.
the involved test method is supposed to throw a nullpointerexception in case c.get null is invoked.
y et the test method would have also passed if the exception was thrown during the cinstantiation.
for this reason the developer moved the object creation out of the try block.
conditional statements several bugs can be fixed in conditional statements verifying that certain preconditions are met before specific actions are performed e.g.
verifying that an object is not null before invoking one of its methods .
code forkadded null check .
public void run mcallback.onconnectionstatechange bluetoothgatt.
this gatt failure bluetoothprofile.state disconnected public void run if mcallback !
null mcallback.onconnectionstatechange bluetoothgatt.
this gatt failure bluetoothprofile.state disconnected the code listing shows the changes implemented in an android pr to fix a nullpointerexception when accessing mcallback in bluetoothgatt .
the addition of the ifstatement implementing the null check allows the nmt model to fix the bug exactly as the developer did.
code forkchange comparison operand .
public void reset int i if i i mlen public void reset int i if i i mlen a second example of a bug successfully fixed by the nmt model working on the conditional statements impacted the api of the fieldpacker class.
as explained by the developer the pr contributed a fix to the fieldpacker .reset api which was not allowing the fieldpacker to ever point to the final entry in its buffer .
this was done by changing the operand to as shown in the code reported above.
v alues the only type of change we observed in this category is the change of methods return value to fix a bug.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
this includes simple cases in which a boolean return value was changes from false totrue see e.g.
as well as less obvious code transformations in which a constant return value was replaced with a field storing the current return value e.g.
return refs my config converted into return ref where ref is a variable initialized in the constructor .
lock mechanism these code changes are all related to the usage of the synchronized java keyword in different parts of the code.
these include its removal from a code block from a method signature and moving the keyword from the method signature to a code block or vice versa .
we do not discuss these transformations due to lack of space.
methods invocation these category groups code transformations fixing bugs by changing the order or value of parameters in method invocations.
code forkflipped parameters in assertequals .
public void testconvertmbtobytes org.junit.assert.assertequals bytes public void testconvertmbtobytes org.junit.assert.assertequals bytes in this example the developer fixed a bug in the test suite by flipping the order in which the parameters are passed to the assertequals method.
in particular while the assert method was expecting the pairs of parameters long expected long actual test was passing the actual value first thus invalidating the test.
the fix automatically applied by the nmt model swaps the arguments of the assertequals .
e. other as previously said we assigned to the other subtree those code transformations for which we were unable to clearly identify the motivation reason.
this subtree includes changes related to i the method signature added removed changed parameter or return type ii types removed type casting in method body or its signature changed variable type iii variable initialization iv replaced statement invoked method v added code condition statement invoked method parameter vi deleted code if condition finally block try catch block invoked method statement vii changes triggered by the other changes e.g.
static method call replaced with an instance method call or vice versa see fig.
.
note that while we did not assign a specific meaning to these changes due to a lack of domain knowledge of the involved systems these are still perfect predictions that the nmt model performed.
this means the code changes are identical to the ones implemented by developers in the pr.
summary for rq .our results show the great potential of nmt for learning meaningful code changes.
indeed the nmt model was able to learn and automatically apply a wide variety of code changes mostly related to refactoring and bugfixing activities.
the fact that we did not find other types of changes such as new feature implementation might be dueto the narrow context in which we applied our models i.e.
methods of limited size as well as to the fact that new features implemented in different classes and systems rarely exhibit recurring patterns i.e.
recurring types of code changes that the model can learn.
more research is needed to make this further step ahead.
v. t hrea ts tovalidity construct validity.
we collected code components before and after pull requests through a crawler relying on the gerrit api.
the crawler has been extensively tested and the manual analysis of the extracted pairs performed to define the taxonomy in fig.
confirmed the correctness of the collected data.
internal validity.
the performance of the nmt model might be influenced by the hyperparameter configuration we adopted.
to ensure replicability we explain in section ii how hyperparameter search has been performed.
we identified through the manual analysis the types of code transformations learned by the model.
to mitigate subjectivity bias in such a process the taxonomy definition has been done by one of the authors double checked by a second author and finally the resulting taxonomy has been discussed among all authors to spot possible issues.
moreover in case of doubts the code transformation was categorized in the other subtree in which we only observed the type of code change implemented without conjecturing about the goal of the transformation.
however as in any manual process errors are possible and we cannot exclude the presence of misclassified code transformations in our taxonomy.
external validity.
we experimented with the nmt model on data related to java programs only.
however the learning process is language independent and the whole infrastructure can be instantiated for different programming languages by replacing the lexer parser and ast differencing tools.
we only focused on methods having no more than tokens.
this is justified by the fact that we observe a higher density of method pairs with sizes less than tokens in our dataset.
the distribution also shows a long tail of large methods which could be problematic when training a nmt model.
distribution and data can be accessed in our replication package .
also we only focus on learning code transformations of existing methods rather than the creation of new methods since these latter are i complex code changes that involve a higher level of understanding of the software system in its entirety and ii not well suited for nmt models since the translation would go from to empty methods.
finally pull request data from three gerrit repositories were used.
while these repositories include hundreds of individual projects thus ensuring a good external validity of our findings our results might not generalize to other projects languages.
vi.
r ela ted work deep learning dl has recently become a useful tool to study different facets of software engineering.
the unique representations allow for features to be discovered by the model rather than manual derivation.
due to the power of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
these representations many works have applied these models to solve se problems .
however to the best of our knowledge this is the first work that uses dl techniques to learn and create a taxonomy from a variety of code transformations taken from developers prs.
white et al.
uses representation learning via a recursive autoencoder for the task of clone detection .
each piece of code is represented as a stream of identifiers and literals which they use as input to their dl model.
using a similar encoding tufano et al.
encodes methods into four different representations then the dl model evaluates how similar two pieces of code are based on their multiple representations .
another recent work by tufano et al.
applies nmt to bug fixing patches the wild .
this work applies a similar approach but rather than learning code transformations they attempt to learn bug fixing commits to generate patches.
these works are related to ours since we use a similar code representation as input to the dl model yet we apply this methodology to learn as many code transformations as possible.
white et al.
also compare dl models with natural language processing models for the task of code suggestion.
they show that dl models make code suggestions based upon contextual features learned by the model rather than the predictive power of the past ntokens .
further expanding upon the powerful predictive capabilities of these models dam et al.
presents deepsoft which is a dl based architecture used for modeling software code generation and software risk prediction .
dl has also been applied to the areas of bug triaging and localization.
lam et al.
makes use of dl models and information retrieval to localize buggy files after a bug report is submitted.
they use a revised v ector space model to create a representation the dl model can use to relate terms in a bug report to source code tokens .
likewise to reduce the effort of bug triaging lee et al.
applies a cnn to industrial software in order to properly triage bugs.
this approach uses word2vec to embed a summary and a description which the cnn then assigns to a developer .
related to software bugs wang et al.
uses a deep belief network dbn to learn semantic features from token vectors taken from a programs asts.
the network then predicts if the commit will be defective .
many dl usages aim to help developers with tasks outside of writing code.
choetkiertikul et al.
proposes a dl architecture of long short term memory and recurring highway network that aims to predict the effort estimation of a coding task .
another aid for developers is the ability to summarize a given segment of source code.
to this point allamanis et al.
uses an attentional neural network ann with a convoluation layer in order to summarize pieces of source code into short functional descriptions .
guo et al.
develops a dl approach using rnns and word embeddings to learn the sentence semantics of requirement artifacts which helps to create traceability links in software projects .
the last example of dl implementations that aid developers in the software development process is an approach developed by gu et al.
that helps to locate source code.
this implementation uses nns and natural language to embed code snippets with natural language descriptions intoa high dimensional vector space helping developers locate source code based on natural language queries .
dl based approaches have also been applied to more coding related tasks one such task is accurate method and class naming.
allamanis et al.
uses a log bilinear neural network to understand the context of a method or class and recommends a representative name that has not appeared in the training corpus .
also helping with correct coding practices gu et al.
uses an rnn encoder decoder model to generate a series of correct api usages in source code based upon natural language queries.
the learned semantics allow the model to associate natural language queries with a sequence of api usages .
recently we have seen dl infiltrate the mobile se realm.
moran et al.
uses a dl based approach to automatically generate guis for mobile apps.
in this approach a deep cnn is used to help classify gui components which can later be used to generate a mock gui for a specific app .
although dl approaches are prevalent in se this work is the first to apply dl to empirically evaluate the capability to learn code changes from developer prs.the previous work has shown that dl approaches can yield meaningful results given enough quality training data.
thus we specifically apply nmt to automatically learn a variety of code transformations from real pull requests and create a meaningful taxonomy.
vii.
c onclusion we investigated the ability of nmt models to learn how to automatically apply code transformations.
we first mine a dataset of complete and meaningful code changes performed by developers in merged pull requests extracted from three gerrit repositories.
then we train nmt models to translate pre pr code into post pr code effectively learning code transformations as performed by developers.
our empirical analysis shows that nmt models are capable to learn code changes and perfectly predict code transformations in up to of the cases when only a single translation is generated and up to when possible guesses are generated.
the results also highlight the ability of the models to learn from a heterogeneous set of prs belonging to different datasets indicating the possibility of transfer learning across projects and domains.
the performed qualitative analysis also highlighted the ability of the nmt models to learn a wide variety of code transformations paving the way to further research in this field targeting the automatic learning and application of non trivial code changes such as refactoring operations.
in that sense we hope that the public availability of the source code of our infrastructure and of the data and tools we used can help in fostering research in this field.
viii.
a cknowledgment this work is supported in part by the nsf ccf and ccf grants.
pantiuchina and bavota thank the swiss national science foundation for the financial support through snf project jitra no.
.
any opinions findings and conclusions expressed herein are the authors and do not necessarily reflect those of the sponsors.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.