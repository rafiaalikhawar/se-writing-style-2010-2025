turning programs against each other high coverage fuzz testing using binary code mutation and dynamic slicing ulf karg n nahid shahmehri department of computer and information science link ping university se link ping sweden ulf.kargen nahid.shahmehri liu.se abstract mutation based fuzzing is a popular and widely employed black box testing technique for finding security and robustness bugs in software.
it owes much of its success to its simplicity a well formed seed input is mutated e.g.
through random bit flipping to produce test inputs.
while reducing the need for human effort and enabling security testing even of closed source programs with undocumented input formats the simplicity of mutation based fuzzing comes at the cost of poor code coverage.
often millions of iterations are needed and the results are highly dependent on configuration parameters and the choice of seed inputs.
in this paper we propose a novel method for automated generation of high coverage test cases for robustness testing.
our method is based on the observation that even for closed source programs with proprietary input formats an implementation that can generate well formed inputs to the program is typically available.
by systematically mutating theprogram code of suchgenerating programs we leverage information about the input format encoded in the generatingprogramtoproducehigh coveragetestinputs capableof reaching deep states in the program under test.
our method works entirely at the machine code level enabling use cases similar to traditional black box fuzzing.
we have implemented the method in our tool mutagen and evaluated it on popular linux programs.
we found that formostprograms ourmethodimprovescodecoverage by one order of magnitude or more compared to two wellknown mutation based fuzzers.
we also found a total of unique bugs.
categories and subject descriptors d. .
testing and debugging testing toolsgeneral terms security keywords fuzz testing fuzzing black box dynamic slicing program mutation .
introduction memory safety errors such as buffer overflows are often the result of subtle programming mistakes but can expose users to highly critical security risks.
by supplying vulnerable software with specifically crafted inputs attackers can often leverage these kinds of bugs to corrupt important data structures and execute arbitrary code with the privileges of the vulnerable program.
such attacks may lead to a complete compromise of the system on which the program is running.
security bugs commonly stem from programs failing to appropriately handle rare corner cases during the processing of inputs.
traditional testing methods using manually written test cases such as unit testing are typically aimed at testing functional correctness of software under normal usage situations and have therefore often proven insufficient to uncover critical security bugs.
intuitively a security bug is often introduced as a consequence of a particularly unclear or complex part of the program specification.
if test cases are based on the same specification chances are high that the test writer has also failed to account for all obscure corner cases.
in that perspective automated test case generation not biased by a specification or the developers interpretation thereof is an attractive alternative for security testing.
in this paper we consider fuzz testing a practical and popular black box technique for automatic test case generation.
the term fuzz testing or simply fuzzing was first coined byb.p.millerin1990afternoticingthat whileworkingover a dial up connection to a mainframe random bit errors in program inputs introduced by interference from a thunderstorm would often cause common unix utilities to crash.
the subsequent study showed that supplying random inputs to programs was a surprisingly effective means to discover robustness errors.
more recently fuzzing has gained widespread popularity as a security testing method.
many large software vendors such as adobe google and permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
copyright is held by the owner author s .
publication rights licensed to acm.
esec fse august september bergamo italy acm.
... .
microsoft employ fuzzing as part of their quality assurance processes.
most fuzz testing tools can be broadly divided into using either a mutation based or ageneration based inputgenerationstrategy .
theformerentailsperformingsome random transformations to an initial well formed input calledaseed whilethegeneration basedapproachusesaformal specification of the input format e.g.
a grammar to generate inputs.
while simple and easy to get started mutationbased methods almost invariably yield lower code coverage than generation based approaches.
mutated inputs often deviate too much from the format expected by the program causing them to be rejected early in processing.
however as many input formats are highly complex creating a formal specification for a generation based fuzzer is often a very time consuming and error prone task.
furthermore it is difficult to create tools and input specification formalisms that can cater to all possible kinds of input formats.
for that reason security analysts are often forced to write their own target specific fuzzers.
another case where a security analyst may be confined to usingamutation basedapproachiswhentestingthird party closed source programs as it is not uncommon for such programs to use proprietary or undocumented input formats.
analyzingclosed sourceprogramsforsecurityflawsisacommon and profitable endeavor in practice as third party independent security testing has become an important part in many software vendors strategy to rid their programs of security bugs .
several large companies such as google and microsoft are e.g.
offering substantial monetary awards for security bugs found in their products.
in this paper we propose a novel method for completely automaticgenerationofhigh coveragetestinputsforrobustness testing.
our method works directly at the machinecode level and thus allows use cases where the analyst does nothaveaccesstosourcecodeordocumentationoftheinput format.
the method is based on the observation that even in cases of proprietary input formats the tester often has access to an implementation that can generate well formed input to the program under test.
consider for example a hypothetical spreadsheet editor.
even if the editor is closedsource and uses a proprietary format for spreadsheets it can most likely store spreadsheets in a format that it can read itself.
in a sense the specification of the input format isimplicitly encoded in the machine code of the program s output module.
on a high level our method works by systematically introducingmutations essentiallybugs intoa generating program and using the outputs from that mutated program as inputs to the program under test.
instead of simply mutating every instruction in the generating program which would be impossible due to the undecidability of static disassembly or using the naive approach of mutating every instruction observed to execute during one run we use dynamic slicing to decide which instructions to mutate.
this approach significantly reduces the number of necessary mutations by in our experiments with a corresponding reduction in time spent on executing mutants.
our main hypothesis is by performing mutations on the generating program s machine code instead of directly on a well formed input the resulting test inputs are closer to the format expected by the program under test and thus yield better code coverage.
dispatcher assessor fuzzer found bugs coverage data target program well formed input mutated inputs figure typical mutation based fuzzing pipeline.
we have implemented the proposed method in our prototype tool called mutagen .
we evaluated the method on several popular linux programs and found that for most programs it improved code coverage by one order of magnitude or more compared to two well known mutation based fuzzers.
in the programs used in our tests we found a total of unique bugs.
.
background and motivation typically a testing campaign using mutation based fuzzing proceeds as follows.
first a set of seed inputs of the appropriatetypeiscollected.
forexample iftheobjectiveisto testapdfreader asetofwell formedpdffilesiscompiled eithermanuallyore.g.usinganautomatedwebsearch.
each seed is then passed to the fuzzer which performs mutations on well formed inputs.
mutations can range from simply flipping bits at random to more complex transformations such as deleting moving or duplicating data elements.
the fuzzingdispatcher is responsible for launching the program under test and supplying it with mutated inputs from the fuzzer.
an assessor component is responsible for determiningiftheprogramhandledthemalformedinputinagraceful way or if it suffered a fault.
when finding security bugs is the objective a simple but effective approach is to simply observe whether the program under test crashes as dangerous memory corruption bugs typically manifest themselves as e.g.
segmentation faults during fuzz testing.
since several different mutated inputs often trigger the same bug during a fuzzing campaign the assessor commonly also has a means of estimating the uniqueness of found bugs.
the industrystandard approach used by most existing fuzzing tools is to calculate a hash digest of the stack trace at the point of failure.
the outcome of the fuzzing campaign is a set of crashers mutated inputs that were observed to trigger unique faults.
figure shows the testing pipeline for a mutationbased fuzzer.
as mutation based fuzzing usually yields poor code coverage the general view among security experts is that several million iterations are typically needed to find bugs in practice .
further while seemingly simple to set up and get started mutation based fuzzing is highly dependent on the quality of the seed inputs as well as various configuration parameters such as the fraction of input bits that are mutated .
the traditional wisdom has therefore been that a great deal of tweaking based on human intuition is needed to achieve good results with fuzz testing.
our method by contrast strives to eliminate the need for human effort as much as possible by leveraging information about the input format encoded in the generating program.
toillustratethepotentialmeritsofourapproach consider for example a highly structured format e.g.
one based on xml.
blindly mutating a well formed input is extremely 783likely to break the structure of the input causing it to be rejected early in processing by the program under test.
by contrast our method would cause the input mutation to be applied already on the internal binary representation of the data used by the generating program.
by performing mutations before the final output encoding is applied we hypothesize that our method is much better at maintaining the structure of the inputs and thus reaching deeper states in the program under test.
.
design and implementation figure shows a high level overview of the mutagen process.
the dynamic slicer first executes the generating program on a given input and computes a cumulative dynamic slice a set containing instructions that contributed to computing at least one byte of the generator s output.
that set is then fed to the mutator which again executes the generating program this time introducing mutations to the instructions contained in the slice.
for each execution one specific mutation from a finite set of mutation operators is introduced at one instruction until all applicable mutations have been applied to each instruction in the slice.
the resulting set of outputs from the generator one per execution is used as the set of test inputs to the program under test.
the final stage is similar to that of a classical mutation based fuzzer using a dispatcher and assessor to run the program under test and detect failures.
while our current implementation only considers input output from files it could also be extended to e.g.
handle communication over network sockets.
rather than finding all instructions that somehow influenced output the purpose of our dynamic slicing is to computeaconservativesubset ofoutput influencinginstructions.
more specifically we only consider instructions that the output is directly data dependent upon and not implicit dependencies from indirect memory accesses or control dependence.
the reasons for this are twofold.
the first reason is that in order to maintain output structure we want to limit mutations to the output generation of the program while keeping the program semantics largely the same.
therefore we want to avoid performing mutations that e.g.
significantly alter the control flow of generating programs.
the second reason is that considering implicit dependencies almost invariably leads to overly large slices.
previous work has e.g.
shown that considering implicit dependencies from indirect memory accesses in binary programs leads to many spurious transitive dependencies.
overly large slices not only lead to more time spent on input generation but can also be detrimental to the overall stability of the inputgeneration process.
if the selection of target instructions is too wide chances are high that the generating program will at some point corrupt its environment e.g.
by overwriting an important configuration file thus preventing future invocations of the program from running properly.
using a narrowerselectionoftarget instructions formutation largely eliminates the need for isolation between mutant executions e.g.
by snapshotting and restoring the system and is thus a key factor in reducing the runtime cost of our method.
the following subsections describe the mutagen components in more detail.
.
dynamic slicing we base our dynamic slicer on a system we developed in previous work .
in order to make the paper selfdispatcher assessor mutator dynamic slicer found bugs coverage data generating program target program generator input well formed input slice mutated inputs figure the mutagen pipeline for test generation and execution.
contained we briefly describe the relevant details of that system here.
the dynamic slicer is implemented using the pin dynamic binary instrumentation framework and works directly on linux x86 executable programs.
first the system records a dynamic data dependence graph dyddg representing the entire data flow during one execution.
that graph is then traversed backwards from a point of interest to compute a slice.
during execution every instruction instance is assigned a uniqueid derivedfromitsaddressandaper basic blockexecution count.
these instruction instances effectively form nodes in the dyddg.
in order to record the data flow of programs a shadow state is maintained during execution mirroring every register and memory location with a corresponding shadow register and shadow memory location.
the shadows are dynamically updated during runtime to always contain the id of the instruction instance i.e.
dyddg node that last defined the corresponding real register or memory location.
each time an instruction is executed the shadows of its input operands are inspected so that edges to the preceding data flow nodes can be recorded.
note that our system considers all executed instructions including those in e.g.
shared libraries.
in order to scale to realistic executions the system uses secondary storage to record dyddgs.
since dyddgs are often too large to fit in ram this approach is necessary but requirescarefuldesignofbothalgorithmsandthegraphrepresentation in order to maintain good performance despite high access time delays.
in addition to its core function we have modified the system to monitor all file related system calls during execution of the generating program.
whenever the program writes a byte from memory to a file the system makes a record of the node in the dyddg that the written data corresponds to.
this way after the generating program has finished we can compute the cumulative dynamic slice by simply finding all nodes that are reachable from any of the nodes corresponding to an output byte.
we perform a simple depthfirst search using the spanning tree algorithm to traverse the dyddg and compute the slice.
thus we don t need to re visit already traversed nodes ensuring good performance even for large output files.
one of the challenges with dynamic data flow analysis of binaries is the lack of type information.
when implementing similar analyses for high level programs one can use the program symbol table to allocate one shadow variable for each variable in the program ensuring accurate and precise data flow tracking.
in a binary program however the analysis engine only sees memory and register accesses.
the most conservative approach would be to track dependencies 784on the byte level i.e.
allocating one shadow byte for every byte of memory.
however this approach would be highly wasteful in terms of time and space overhead since the vast majority of memory accesses use the native word size of the cpu e.g.4bytesfortheia32 x86architecture .
therefore the slicer uses shadow memory with a minimum granularity of the native word size a common approximation used by e.g.
the popular dynamic analysis tool memcheck .
this approach may lead to lost data dependencies in case of sub word writes since an entry in shadow memory will always point to the instruction that last wrote any byte of the corresponding memory word.
while such problems are rare in general due to the scarcity of sub word writes the approximation can sometimes be problematic in the outputtracking stage of our analysis since many programs emit single bytes as part of the final output encoding.
to avoid having to track dependencies on the byte level which would effectively quadruple the time and space overhead of slicing we instead use the following method when a sub word write is observed during runtime a virtualdependence edge to the previous definition of the full word is recorded along with edges for the regular input operands.
this way no dependencies are ever lost at the cost of many false dependencies.
to compensate for false dependencies we also modify the dyddg traversal algorithm.
if a virtual dependence edge is encountered during traversal we only follow that edge if the corresponding defining instruction i.e.
the target of the edge performs a sub word memory write.
this way we prune the vast majority of spurious dependencies arising from re used memory on e.g.
the stack while still accurately accounting for e.g.
writes into byte arrays.
the remaining spurious dependencies may lead to slightly overapproximated slices but does not seem to negatively affect our test generation in practice.
.
program mutation programmutationhasbeenstudiedextensivelywithinthe context of mutation testing .
the objective of mutation testing is often to determine the adequacy of test suites.
to assess the test suite of a program the tests are run on mutated variants of the program referred to as mutants.
by counting the number of mutants that were killed by the test suite i.e.
failed at least one test the overall quality of the test suite can be quantified and missing test cases can be identified.
a mutant is created by applying a mutation operator to one or more statements in a program.
a mutation operator applies a small syntactic change to the affected statement.
we have implemented our mutation component using valgrind .
valgrind performs dynamic binary instrumentationbyfirstconvertingnativeinstructionsintotherisc like vex intermediate representation ir .
all instrumentation and analysis is performed on the vex ir which is transparently re compiled to native code and executed by valgrind using a just in time compiler.
we apply our mutations on the vex ir which avoids having to deal with the highly complex x86 instruction set comprising more than different opcodes.
the code translation units used by valgrind are singleentry multiple exit superblocks of vex code.
complex instructionsarebrokendownintoseveralprimitivevexstatements and intermediate results are stored in single staticassignment virtual registers called temporaries which are lo t0 get i32 t1 get i32 t2 shl32 t1 x2 i8 t3 add32 t0 t2 t4 ldle i32 t3 put t4 a original vex ir t0 get i32 t1 get i32 t5 shl32 t1 x2 i8 t2 sub32 t5 0x1 i32 t3 add32 t0 t2 t4 ldle i32 t3 put t4 b vex ir after applying mutation to statement figure example of vex ir for the x86 instruction mov eax cal to one superblock.
as an example we consider the x86 instruction mov eax which calculates an address from registers ecxandedx fetches a bit word from that address and writes the word to register eax.
the corresponding translation into vex is shown in figure 3a.
the first two vex statements fetch the contents of ecxand edx intotemporaries t0andt1 respectively.
thenexttwostatements perform the address calculation statement reads a word from memory and the final statement puts that value into eax.
since mutation testing often aims to assess a test suite s ability to catch errors in the program logic it is common to use mutation operators that replace arithmetic relational or logic operators or the absolute values of constants.
recall however that in our setting the objective is to mutate the output generation of programs while keeping program semantics mostly unchanged.
hence most of our mutation operators perform small arithmetic changes to computations.
we only mutate vex statements that produce an intermediate result stored in a temporary.
mutating other statements such as statement in figure 3a is redundant since such statements only serve to propagate the result of a computation to a register or memory.
our different mutation operators are summarized in table .
mutations perform simple arithmetic changes to the result of a computation while mutations and perform the same changes to respectively the first and second operands of a statement where applicable.
note that since temporaries are single static assignment we allocate a new temporary for the mutated input operand in these cases ensuring that the mutation only affects the target statement.
figure 3b shows an example of applying mutation subtract from destination to statement in the original vex code.
mutations and are special cases that are used instead of mutations when the destination temporary is later directly used as an address.
statement in figure 3a is an example of such a case.
the rationale for treating these statements differently is that a multiplicative change of an 785table mutation operators used by mutagen.
mutation description add to destination subtract from destination multiply destination by divide destination by add to destination subtract from destination add to first operand subtract from first operand multiply first operand by divide first operand by add to second operand subtract from second operand multiply second operand by divide second operand by switch and or switch add sub switch signedness addressisextremelyunlikelytoresultinanewvalidaddress the program would most likely only crash as a result of such amutation.
further itismuchmorelikelythatthemodified address will point to a valid data object e.g.
an adjacent element of an array if the native word size bytes in x86 is used in the addition subtraction instead of .
mutation switches the bitwise and or operators for one another where applicable and mutation equivalently switches the add subtract operators.
finally mutation attempts to emulate a common programmer error where a signeddatatypeisusedinsteadofanunsigned orviceversa.
for operations that exist in both signed and unsigned variants mutation changes the signedness of the operation.
we systematically apply all applicable mutation operators to each vex statement that is part of a translated x86 instruction in the dynamic slice.
using the terminology of mutation testing we only perform first order mutations i.e.
we only apply one mutation operator to one vex statement per execution.
special purpose instructions such as floating point or simd instructions are ignored in our prototype implementation.
one of the challenges of mutation testing is to avoid equivalent mutants i.e.
mutants that are syntactically different but semantically equivalent.
in this work we redefine the term to mean mutants that produce identical output given a specific input albeit being possibly semantically different.
equivalent mutants does not adversely affect the outcome of testing but introduce unnecessary time overhead in the test generation.
we perform basic data flow analysis within superblocks to prune obvious cases that would lead to equivalent mutants.
performing mutation on statement in figure 3a is e.g.
redundant if we know that t0is only used in statement where mutation is going to be applied anyway.
similarly applying mutation to statement is redundant if mutation has already been applied in an earlier execution.
these optimizations reduce the number of equivalent mutants by roughly .
during test generation we compute a hash of each output and keep only unique files.
.
test dispatching the test dispatcher runs the program under test on each mutated file.
to avoid missing crashes in cases where the program has registered handlers for certain signals we perform simple instrumentation of the program to catch interesting signals segmentation fault illegal instruction etc.
before they are sent to a signal handler.
when a crash is detected we make note of the input file that caused the crash and similarly to other fuzzing platforms compute a hash digest of the stack trace at the time of failure to distinguish between unique crashes.
.
evaluation similar to mutation based fuzzers our approach relies on simple syntactic mutations without explicitly incorporating semantic information as in generation based fuzzing.
therefore an important question is what benefit our approach offersovermutation basedfuzzing.
iftherearenobenefits we would have a hard time justifying the increased complexity of our method to developers and security analysts.
another important question is how effective our mutation operators are.
since to the best of our knowledge program mutation has not been applied in this context before investigating the effectiveness and adequacy of the mutation operators is important as it will help identify directions for future improvement of our method.
in the following section we address these two questions and present a preliminary evaluation of the overall performance and effectiveness of mutagen.
.
methodology in our evaluation we used different generating programs and different file types.
for each file type we generated two sets of mutated inputs which were used to test a total of popular linux programs.
all experiments were run on virtualbox virtual machines with one virtual cpu core per machine.
all virtual machines were running linux mint .
.
we used two intel xeon e3 workstations at .
ghz as hosts each host running four concurrent virtual machines one per host core .
all programs generators programs under test or both used in our experiments are listed in table .
for the opensource programs we used the latest versions supplied by the linux mint package manager.
the latest version of the closed source program nconvert was downloaded from the vendor s website.
for our fuzzing targets we have focused on programs that operate on popular document image or media types.
since such programs often take inputs originating from untrusted sources such as the internet they are popular targets for attackers.
fuzzing is therefore often applied as part of the quality assurance process for these kinds of programs.
table shows the selection of generating programs.
with the exception of avconv we used two distinct inputs for each test generation run.
for avconv we used one input to generate mp3 and aac files using both constant and variable bit rate encodings.
all test sets were created by mutating the conversion of a sample file column using the configurations shown in column .
the names of the resulting sets of test inputs are shown in column .
for each generator configuration we also performed one run without mutations to produce a well formed input.
we 786table programs used in the experiments.
program version description avconv .
6audio and video encoder decoder.
open source.
convert .
.
10commandline interface to the imagemagick image editor converter.
open source.
nconvert .17commandline interface to the xnview image editor converter.
closed source.
pdftocairo .
.5pdf conversion utility using poppler.
open source.
mudraw .
2pdf conversion utility based on mupdf.
open source.
pdftops .
.5pdf to postscript conversion utility using poppler.
open source.
ps2pdf .10postscript to pdf conversion utility using ghostscript.
open source.
inkscape .
.4svg vector graphics editor.
open source.
refer to that input here as the base case of each generated test set.
our selection of programs is limited in part by the fact that many popular linux programs share the same code base.
for example there exist a plethora of pdf utilities and viewers for linux but the vast majority are based on oneof xpdf poppler aforkof xpdf mupdf orghostscript .
two programs that use the same library or code base are likely to crash on the same malformed input.
to avoid potentially skewing the results of our testing we have striven to choose only one testing target from each family of programs.
table shows the combinations of programs under test and input data sets used in the test runs.
these combinations are henceforth referred to as test configurations .
we measured instruction coverage for each test execution using a simple tool implemented with pin.
since the coverage tool incurs a significant overhead we also performed all runs without any instrumentation to get accurate time measurements.
toevaluatehowourapproachcomparestoexistingfuzzing tools wecomparecodecoveragewithtwopopularmutationbased fuzzers zzuf and radamsa .zzufperforms simplerandombit flippingoninputs while radamsa usesseveral inputtransformationsknowntohavefoundbugsinthepast such as duplicating or moving data elements incrementing bytes removing bytes etc.
as seeds for the mutation based fuzzers we used the base cases of each test set.
default parameters are used for both fuzzers.
for each test configuration we ran the mutation based fuzzers for as many iterations as there were files in the corresponding mutagen test set.
note that this approach favors mutagen since the time taken for executing mutants is not accounted for.
therefore we also ran zzufand radamsa on all test configurations without coverage instrumentation for hours to allow a more fair comparison of the ability to find bugs.
.
results program mutation .
the results of our test generation runs are shown in table .
column and show the total number of runs and files generated respectively.
roughly of mutant executions results in an output.
col umn shows the number of unique files produced for each configuration.
with the exception of inkscape about of all mutant executions produce unique outputs.
the numberofmutationexecutionsfor inkscape issignificantlylarger than for the other programs yet the number of unique files is much lower.
the reason for this anomaly is unknown.
one possibility is that the slicing step fails to identify interesting instructions to mutate for inkscape e.g.
because the output generation code makes heavy use of implicit dependencies section .
comparing the number of runs with the dynamic slice size column we see that each instruction results in roughly mutations on average.
benefit of dynamic slicing .
column shows the total number of instructions executed at least once for each generator configuration.
the benefit of dynamic slicing is evident when comparing these figures with the slice sizes.
performing dynamic slicing reduces the number of instructions to mutate by between and yielding a corresponding reduction in test generation time.
the time required for computing slices was largely negligible in comparison to the time taken for executing mutants.
the longest slice computation inkscape sample .svg required .
seconds and the shortest nconvert sample .tiff .
seconds.
coverage and found bugs .
the outcome of our testing experiments is summarized in table .
we report all coverage metrics as the relative cumulative increase in instruction coverage over the base coverage i.e.
the coverage when running the program under test on the base case of the test set.
note that we present coverage relative to the baseline only as there is no clear definition of an absolute coverage ratio when testing binary programs that are dynamically linked.
with the exception of nconvert andinkscape the coverage increase of mutagen is typically one order of magnitude ormoreoverthatof zzufandradamsa.pdftops particularly stands out with a coverage increase of over .
inkscape showsamoremoderate2.5xincreaseoverthebestmutationbased fuzzer probably largely due to the low number of test inputs.
the results also show that the coverage increase of the mutation based fuzzers was much larger for nconvert than for the other programs.
upon closer examination we found that the first iteration achieved a coverage increase of about and that for the remaining iterations the increase was negligible.
we speculate that this may be due to some particularly complex error management code being run when certain malformed inputs are encountered.
column shows the time required to run tests without coverage instrumentation.
due to the small number of test cases no test configuration requires more than minutes to complete.
as mentioned in section .
measuring coverage of the mutation based fuzzers for only as many iterations as there arefilesinthecorrespondingmutagentestsetmaynotoffer a fair comparison.
figure shows the coverage of zzufand radamsa versus iteration for test configurations.
due to space constraints we only show one plot per program and file type but all configurations show similar results.
the x in the plots marks the last iteration when any coverage improvement was observed.
with the possible exception of inkscape all cases appear saturated already after a small number of iterations.
it is therefore unlikely that running the fuzzers for a longer time would produce significantly 787table mutation configurations and results.
generating programinput configurationtest set nametotal ins.slice size runs filesuniq.
filestime hours avconv sample .mp3mp3 mp3 cbr mp3 cbr .
mp3 mp3 vbr mp3 vbr .
mp3 aac cbr aac cbr .
mp3 aac vbr aac vbr .
convertsample .pngpng tifftiff .
sample .png tiff .
nconvertsample .tifftiff pngpng .
sample .tiff png .
pdftocairosample .pdfpdf pdfpdf .
sample .pdf pdf .
pdftopssample .pdfpdf psps .
sample .pdf ps .
inkscapesample .svgsvg svgsvg .
sample .svg svg .
table testing results.
program test setbasecov.
ins.
mutagen zzuf radamsa crashesunique bugstime s rel.
cov.
increaserel.
cov.
increaserel.
cov.
increase ps2pdfps .
.
.
.
ps .
.
.
.
pdftopspdf .
.
.
.
pdf .
.
.
.
mudrawpdf .
.
.
.
pdf .
.
.
.
inkscapesvg .
.
.
.
svg .
.
.
.
avconvmp3 cbr .
.
.
.
mp3 vbr .
.
.
.
aac cbr .
.
.
.
aac vbr .
.
.
.
convertpng .
.
.
.
png .
.
.
.
tiff .
.
.
.
tiff .
.
.
.
nconvertpng .
.
.
.
png .
.
.
.
tiff .
.
.
.
tiff .
.
.
.
higher coverage.
again inkscape stands out somewhat.
in order to be consistent with the methodology used for the other programs only about iterations were used for inkscape which is likely too few to achieve full saturation.
some of the plots show conspicuous similarities between the results for zzufandradamsa.
we suspect that this may be due to the naming scheme used for generated files.
we add the iteration number as a suffix to each file name which causes the length of the file name to change deterministically.
for avconc aac cbr we e.g.
see small jumps in coverage at the 10th 100th and 1000th iteration.
mutagen produces inputs that crash programs out of which were unique bugs according to the stack trace.
some of the bugs are simple correctness errors such as divisions by zero.
other bugs involve trying to free memory at invalid addresses which could have security implications.
neither the short instrumented runs nor the hour uninstrumented runs of zzuforradamsa yielded any bugs.
effectiveness of mutation operators .
table summarizes the fractions of mutants and test inputs contributedby each mutation operator for all test sets.
some mutation operators are significantly more common than others.
for example every vex statement that is candidate for mutation has an output operand which means that mutations are always applied while for example mutation switch and or is very uncommon.
note that the number of mutants for mutations are not exactly equal.
since we identify the candidate vex statements online during the test generation phase we may in some rare cases fail to apply a mutation when the preceding mutant crashed very early.
this results in a very small variation in the number of mutants for each family of mutation operators.
we do not expect this to affect our results in a significant way.
due to our pruning of equivalent mutants the add and subtract mutations and as well as and are rare in comparison to their multiply and divide counterparts section3.
.
wealsonotethatmutation16 switch add subtract is very common probably much due to the commonality of address generation expressions like the one in figure .
4500ps2pdf ps 7500pdftops pdf 7500mudraw pdf 900inkscape svg 3000avconv mp3 cbr .
.
4500avconv aac cbr 4500convert png 4500convert tiff 4500nconvert png 4500nconvert tiff 1figure instruction coverage versus iteration for the mutation based fuzzers.
the solid red line represents zzuf while the dashed blue line represents radamsa.
within the field of mutation testing it has previously been shown that out of a larger set of mutation operators a small number of operators can achieve results very close to the full set.
we wanted to investigate if the same was also true in our context.
to this end we investigated the instruction coverage achieved by the optimal set onof mutation operators of size n for each nbetween and .
since this is an instance of the minimum set cover problem which is known to be np complete we simply computed our results bymeansofexhaustiveenumerationofallcombinations.
for eachcombinationof nmutationoperators wecalculatedthe averageoftherelativecoverageincreaseoveralltestconfigurations and selected the combination with the highest average coverage.
we also counted how many of the bugs would be found with the optimal reduced operator set of each size.
at first it may seem appropriate to weight the coverage of each mutation operator by e.g.
the number of test cases or the number of mutants since table shows that some mutations are much more common than others.
however applying such linear weighting would unduly favor the rare mutations due to the very pronounced saturation effect observed in random testing.
consider for example two mutation operators miandmj yielding the respective sets of test inputs tiandtj ti tj .
these test sets in turn cover instructions iiandijwhen used as inputs to some program.
if we randomly sample tj inputs from ti to produce a smaller test set tr that test set will still yield a coveragethatissignificantlyhigherthan parenleftbig tj ti parenrightbig ii due to said saturation effect.
for this reason we do not apply any weighting of coverage scores when computing optimal set combinations.
the optimal operator sets of each size are presented in table .
using only mutation operators achieves .
of the mean coverage of all operators and finds of the bugs in our experiments.
this corresponds to a reduction of mutant executions cf.
table .
with operators we achieve .
of the mean coverage and find all bugs whileachieving a reduction of mutant executions.
the most effective operator is number which alone achieves of the total mean coverage.
operator is the least effective which is perhaps not surprising since it is also the least common one.
several interesting observations regarding the effectiveness of different types of mutation operators can also be made from the results subtraction is better than addition.
all the subtract operators perform better than the add operators .
this is likely because subtraction can cause larger semantic modifications to inputs by changing a small number to a very large number by means of a wrap around.
as boolean variables are commonly represented by the integers and subtractioncanalsochangetruthvaluesinbothdirections while addition can only change false to true .
operators largely subsume operators .
mutations on destination operands appear to largely subsume mutations on the first input operand.
this is not an unexpected result since many vex statements that have only one input operand simply serve to receive some intermediate result produced by an earlier statement.
note that we prune the trivial cases where the result of a computation is only used once within a superblock section .
.
mutating the second operand is effective.
mutations are significantly more effective than .
we speculate that this is because in expressions with constants the second operand often holds the constant value.
as constants are not first loaded into temporaries in the vex ir they are not subjected to mutation by operators .
mutating the second operand 789table contributions from each mutation operator.
mutation mutants test inputs crashes bugs .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table effectiveness of reduced mutation operator sets.
optimal mutation operator set cover bugs .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
could therefore be important for instructions that use constants.
mutating address generators is worthwhile.
since mutations and are only applied on statements that compute addresses we can implicitly draw the conclusion that mutating address generating instructions is effective both for improving coverage and for finding bugs.
one reason that addition beats subtraction here could be that pointers to arrays are common in programs.
adding to such a pointer will shift it to the second element of the array while subtracting from it will cause it to point outside the array.
.
discussion and future work the results in section .
clearly suggest that our main hypothesis holds mutating a correct implementation of a program that can generate well formed inputs achieves a considerably better coverage and finds more bugs comparedto traditional mutation based fuzzing.
in this section we further analyze the results and suggest directions for future work.
comparison to mutation based fuzzing.
while it is difficult to fit mutagen into one of the traditional fuzzing paradigms we believe its typical use cases to be similar to those of mutation based fuzzing.
for example mutagen could potentially serve as a drop in replacement for a mutation basedfuzzerwhene.g.craftingaformalinputspecification for a generation based fuzzer is not feasible.
compared to previous results from mutation based fuzzing our method has a relatively high crash density i.e.
ratioofcrashestotestcases andhasaveryhighbugdensity per crash .
that suggests that mutagen can produce a small set of high quality test inputs which can find bugs in a small number of test runs.
while test generation is fairly time consuming with our current implementation mutagen can generate a set of inputs that can be used to quickly test several programs that accept the same file type.
this is not possible with traditional mutation based fuzzing where the main cost lies in executing the program under test on a very large number of test inputs.
further while the results of our limited evaluation are not statistically significant the fact that mutagen found several bugs while zzufand radamsa found none may suggest that our approach is better at finding complex bugs that mutation based fuzzing fails to catch.
comparisontogeneration basedfuzzing.
ourmethod also bears some resemblance to generation based fuzzing most notably the ability to exploit information about input formats to generate semi valid inputs.
there are however several important differences.
first our method is potentially more versatile as it does not require a formal specification of an input format.
for example the well known generation based fuzzer spike uses a block based approach to define input formats.
while spike has proven very successful at testing e.g.
implementations of binary network protocols it would not work for inputs that are not easily fitted into a block based formalism.
secondly a key difference between mutation based and generationbasedmethodsisthelatter sabilitytotestlong runningprograms with state such as implementations of stateful network protocols by modeling the protocol state machine.
as our method relies entirely on implicit information about the structure of input such models cannot be incorporated into our approach.
an interesting direction for future work is to investigate if mutagen can sufficiently retain the semantics of the generating program so that its state machine is kept synchronized with the server when testing stateful protocols.
finally apart from a formal specification of the input format generation based fuzzers typically also need a set of heuristics for generating semi valid inputs.
such heuristics are often compiled from inputs or input patterns that have been known to trigger bugs in the past .
our method does not currently allow incorporating such heuristics when applying mutations.
thegeneralconsensusamongsecurityexpertsisthatmethods based on generation typically achieve better coverage than those based on mutation.
miller and peterson e.g.
report higher coverage for generation compared to mutation when fuzzing a png parsing library.
for this reason a comparison between our approach and generation based fuzzing would be interesting.
there are several practical complications however that make such a comparison diffi790cult.
for example to the best of our knowledge there are no publicly available robust generation based fuzzers available for many of the popular input formats that we use in our evaluation.
should we repeat our evaluation comparing against generation based fuzzers instead of mutation based we would be forced to write our own fuzzers for many of the input formats.
aside from the significant extra work effort this would introduce an obvious risk of bias.
a direct side by side comparison may also be difficult due to the aforementioned differences in use cases and application areas.
analogous to mutation based fuzzing the results of a comparisonaree.g.likelytobeverydependentonthechoice of seed inputs for the generating programs.
input space compression.
a traditional mutation based fuzzer such as zzuf has a search space of size 2kfor a seed of size kbits.
the obvious benefit of a generationbased fuzzer as well as our approach is that the search space can be reduced by applying information about the input format.
in order to cover a sufficient part of the input space amutation basedfuzzeralsoneedsalargeanddiverse selection of seed inputs.
the optimal selection of seeds is still an active area of research .
with our approach the equivalent problem is to identify appropriate generating programs as well asinputs and configurationparameters for these programs.
investigating the importance of the choice ofgeneratingprogramsisanimportanttopicforfuturework.
another interesting question is whether the code coverage of thegenerating program is a good estimate of the covered input space.
performance improvement.
one obvious direction of future work is to improve the performance of mutagen.
currently the most time consuming part of the mutagen pipeline is the execution of mutants to produce test cases.
the current mutation tool is implemented with valgrind which facilitates rapid development since it is not necessary todealdirectlywiththeintricaciesofthex86instructionset.
however valgrind does incur an overhead of at least 10x.
a more performance conscious choice would be to perform mutations by means of e.g.
binary patching which would allow mutants to execute at near native speed.
mutant reduction.
another way to reduce the cost of test generation is to reduce the number of mutants.
our investigation of the effectiveness of mutation operators shows that in our particular experiments we could reduce the number of mutants by about while still finding most bugs and retaining high coverage.
we intend to further study the potential for mutant reduction in future work.
we also intend to verify if the reason that mutations of the second operand perform so well is indeed due to arithmetic with constants.
if that is the case an alternative mutation strategy could be to only perform mutations along with direct manipulation of constants.
intermittent mutation.
finally another strategy for input generation is to only intermittently inject errors into the generating program rather than statically mutating instructions.
this could potentially retain the semantics of the generating program to a larger degree possibly enabling even higher coverage of the program under test.
similar to mutation based fuzzing however the number of generated inputs would be unbounded.
.
related work following the pioneering work by miller et al.
many of the early advances in fuzzing were made outside of academia.
recently woo et al.
studied scheduling algorithmsformutation basedfuzzing andrebertetal.
investigated the importance of seed selection.
while both of these works focus on traditional mutation based fuzzing mutagen could still benefit from their findings.
efficient scheduling algorithms could help mutagen prioritize which parts of a generating program to mutate first if there is a fixed time budget for executing mutants.
our approach could also benefit from the findings of rebert et al.
when selecting inputs to generating programs.
generation based fuzzers have proven particularly effective at finding bugs in compilers and interpreters.
yang et al.
use generation based fuzzing to find bugs in c compilers and holler et al.
use grammars in combination with code fragments known to have previously triggered bugs to test interpreters .
due to the highly stringent requirements on input structure and the general scarcity of inputgenerating programs we believe generation based fuzzing to be favorable over our method in this specific domain.
symbolic execution is an alternative to black box testing methods.
the sage and mayhem systems can for example generate high coverage test cases using concolic execution of program binaries.
while such methods have seen tremendous development during recent years their scalability is still limited.
wang et al.
propose a hybrid approach to improve the coverage of programs that perform integrity checks on their inputs.
a constraint solver is used to repair checksum fields in fuzzed files allowing them to pass integrity checks.
the authors note that the checksum repair stage is time consuming and that their method would fail to handle input formats that use cryptographically strong integrity checks.
our approach by contrast sidesteps that problem altogether by allowing mutations to be applied before checksum computations.
several previous works describe methods for automatically recovering input formats by dynamic analysis of program binaries .
the recovered input formats could later be used to drive a generation based fuzzer.
while similar in spirit to our work these methods are limited by the fact that input formats can only be recovered with respect to a pre determined formalism e.g.
context free grammars.
for that reason they face a similar problem as generationbased fuzzing systems there is no one formalism suitable for describing all input formats.
our method tackles the problem of input generation at a lower level and does not need to make assumptions about properties of input formats.
.
conclusion in this paper we have presented a novel method for automatic test case generation using dynamic slicing and program mutation.
we empirically evaluated the method on linux programs and found that it improved code coverage withatleastoneorderofmagnitudeinmostcases compared to two well known fuzzers.
we found a total of crashing inputs of which represent unique bugs.
we evaluated the effectiveness of our mutation operators and suggested ways to further improve the coverage and performance of our method.
.