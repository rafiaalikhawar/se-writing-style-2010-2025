crowd debugging fuxiang chen and sunghun kim department of computer science and engineering the hong kong university of science and technology hong kong china fchenaa hunkim cse.ust.hk abstract research shows that in general many people turn to qa sites to solicit answers to their problems.
we observe in stack over ow a huge number of recurring questions despite mechanisms having been put into place to prevent these recurring questions.
recurring questions imply developers are facing similar issues in their source code.
however limitations exist in the qa sites.
developers need to visit them frequently and or should be familiar with all the content to take advantage of the crowd s knowledge.
due to the large and rapid growth of qa data it is di cult if not impossible for developers to catch up.
to address these limitations we propose mining the qa site stack over ow to leverage the huge mass of crowd knowledge to help developers debug their code.
our approach reveals warnings and .
of them are con rmed by developers from eight high quality and well maintained projects.
developers appreciate these ndings because the crowd provides solutions and comprehensive explanations to the issues.
we compared the con rmed bugs with three popular static analysis tools findbugs jlint and pmd .
of the bugs identi ed by our approach only findbugs detected six of them whereas jlint andpmd detected none.
categories and subject descriptors d. .
debugging aids general terms human factors veri cation keywords crowd debugging crowd sourcing debugging .
introduction research shows that in general many people turn to question answer qa sites to solicit answers to theirproblems .
in this paper we are interested only in qa sites that discuss source code problems hence qa sites refer only to those sites that are source code related unless stated otherwise.
previous research work has shown that users ask recurring questions in qa sites such as yahoo!
answers and naver .
allamanis et al.
reported that there are recurring questions about code idioms in the developer s qa site stack over ow so .
we also observed many recurring questions appearing in so.
so is actively used to ask questions for various reasons such as fast response in receiving answers and having many experts in the community.
treude et al.
reported that within the rst two years of so s establishment there were already three million questions with answers .
we observe that many questions and answers contain code fragments where the former display the code in issue and the latter provide the recti ed version.
owing to this we use so in our study.
discovering questions that frequently recur strongly implies that many developers are facing similar if not the same issues in their source code.
however there are limitations in the qa sites.
developers need to visit them frequently and or should be familiar with all the content in order to take advantage of the crowd s knowledge.
due to the large volume and rapid growth of the qa data it is di cult if not impossible for developers to catch up.
regardless of the mechanisms that have already put into place to deter recurring questions in so it is still observed that there is a huge number of recurring questions.
therefore we are motivated to mine qa sites to leverage the huge mass of crowd knowledge to help developers detect defective code fragments in their source code.
with our technique we propose to nd defective code fragments by rst detecting code clones before making use of them to triangulate source code anomalies.
the defective code fragments with the crowd s explanation are then coupled with the crowd s suggested solution with the crowd s explanation as well and reported to developers for their concurrence.
although there are existing similar debugging tools such asfindbugs pmd and jlint they are based on manually identi ed rules and patterns rather than patterns mined from the crowd.
furthermore these tools are only detectors as they lack the solutions to the detected issues.
our approach has the advantage of leveraging the big data of crowd knowledge to check for source code issues and toprovide solutions and an explanation which are absent from existing static analysis tools.
our technique is able to generate a reasonable number of warnings in eight high quality and well maintained projects and produce a high percentage .
of con rmed bugs.
developers show appreciation of our ndings as we are able to provide them with solutions and an explanation from the crowd.
we also compared our results with existing static analysis tools findbugs jlint and pmd .
six of our con rmed bugs are detected by findbugs whereas jlint and pmd detected none.
despite being able to detect six of the same bugs these three popular static analysis tools missed .
bugs.
overall this paper makes the following contributions an empirical evaluation of recurring questions in qa sites we evaluate the number of recurring questions in so which serves as the main motivation for this research work.
a novel debugging technique that leverages the crowds knowledge we propose a novel technique in leveraging the crowds knowledge to detect defective code fragments in software projects and to provide the crowds suggestions and explanations to aid in debugging.
an empirical evaluation of crowd debugging which includes developers feedback and comparison with existing static analysis tools we evaluate our technique by reporting the warnings for developers concurrence and comparing our results with existing static analysis tools findbugs pmd and jlint .
the remainder of this paper is organized as follows.
section presents the motivation in our design of the crowd knowledge based debugging and section provides our crowd debugging approach using an illustrated factual example.
we describe our evaluation settings in section and portray the results in section .
related work is surveyed and shown in section while we further analyse the various limitations and threats in section .
we conclude with directions for future research in section .
.
recurring questions research has been conducted to show that questions are recurring in a myriad of qa sites such as yahoo!
answers and naver .
wang et al.
found recurring questions within a short span of four months february to june in yahoo!
answers .
jeon et al.
also found recurring qa pairs in a collection of questions in the computer novice category in naver .
in retrospect to software engineering previous research has studied the common questions asked by developers at the project and organizational levels as well as in mailing lists and discussion forums .
fritz et al.
and sillito et al.
identi ed developers questions that relate to a lack of support in projects and common developers questions that relate to software evolution tasks.
hen et al.
extracted recurring questions from mailing lists and discussion forums to provide software development related documents to help developers in the software implementation process.
in so the qa site tailored specially for developers recurring questions are however discouraged and mechanisms have been put into place to detect and deter recurring ques tions .
similar questions are also shown when a developer posts a new question.
recurring questions in so are generally categorized into three groups exact word by word copying and pasting partial use of keywords in original questions and having subtle semantic di erences to the original questions that do not belong to the previous two groups .
if there is a recurring question in so developers can vote to include it with comments and links to the original question.
this will then result in the question being modi ed to re ect it as recurring .
recurring questions can also be merged by notifying moderators to perform a merge operation on the recurring questions .
table number of recurring questions in so since its establishment.
questions are recurring despite mechanisms put into place to prevent them.
so users can also detect duplicates after posting their questions to further eliminate them.
therefore we believe this .
is a high number for recurring questions.
of of period questions recurring questions aug may .
we have observed recurring questions in our use of so on several occasions and this led us to investigate the number of recurring questions in so.
table shows the total number of questions in so from august till may the number of identi ed recurring questions and its ratio with respect to the total number of questions.
despite having mechanisms put into place to prevent recurring questions from taking place and so users can detect duplicates after questions are posted to further eliminate them we observed many recurring questions in so from our dataset.
although the ratio of recurring questions to the entire question set is around it is still a huge number .
we therefore hypothesize that programming issues faced by a developer may also be faced by others and questions asked in so can be leveraged to help other developers in similar situations.
this further motivates us to design crowd knowledge based debugging by tapping into the existing pool of knowledge from the qa database.
we have used all the question data from august till may and we do not limit ourselves to only the recurring question data so as to increase the chances of a higher detection rate.
.
approach this section describes our approach to leverage the crowd s qa knowledge to detect defective code fragments.
figure shows the overview of our approach.
so questionanswer code pairs have been populated in a database and we perform code clone detection between the target source code and the question code blocks in so to identify similar fragments in the target code.
the detected pairs of code clones are the shaded regions in figure .
the shaded regions in the rst gure depict two matched clone fragments m1 and m2 from the target source code while the shaded regions in the center gure show the matched clones qf1 and qf2 from the so question code blocks .m1 a target source code b so question blockqf1 c so answer blockclone detection1element and title matching2clone pair 1clone pair 2matched o mismatched x m2qf2...........................non essential statement filterssimilarity filtersimilarity filter3post filteringfigure overview of approach.
the target source le is compared against the so question code blocks for code clones.
the code like terms of the matched clone in the question are further compared with those in the answer code blocks.
a matched code like term denotes that the clone fragment in the target source le is potentially defective.
post lters are then applied to reduce the potential false positives.
the code like terms from the detected clones in the so question code blocks are then compared with the code like terms in the corresponding answer code blocks .
code like terms are sequences of characters that resemble code elements .
this step is needed as we observed questions and answers containing code fragments usually have some identical code terms.
the questions usually contain the problematic code while the answers provide the recti ed version.
if there is a matched code like term it will be marked as a potential defective code fragment.
we further applied post ltering process to identify and remove potential false positives and the output will be treated as a defective code fragment.
together with the so answer with explanation and its url these will be reported to the developers.
in figure m1 is a defective code fragment as there are matched code like terms between its matched clone qf1 and the answer code blocks .
m2 however is not a defective code fragment as there is no matched code like term between qf2 and the answer code blocks .
our approach can be seen in ve consecutive phases namely code pairs database building clone detection element and title matching post filtering and reporting defective code fragments .
the following subsections explain each phase with a concrete example from jfreechart.
.
code pairs database building our goal in this phase is to create and populate a database that contains question answer code pairs from so.
speci cally we are interested in questions that have the highest scored answers at least a up voted score and contain code elements within questions and answers.
if there are multiple answers with the same high score to a question we will choose the most recent answer even though it might not have been explicitly marked as accepted.
we observed that better answers can be posted after a previously given answer has been marked as accepted.
an answer to a question in so is voted upwards manually to give a score to the answer by the community of developers if the solution presented solves the issue highlighted in the question .
the highest scored answers would mean that the answers have a strong consensus among a larger group of developers.code elements are important in our approach as we need them for detecting faults in the target source code.
most code elements are formatted in code blocks inside the question and answer sections of a so post.
the code elements are either embedded inside the code tag for inline code or the pre code tags for blocks of code .
we de ne code blocks here as the entire content inside either of the previous mentioned encompassing tags.
the detected question answer pair is denoted as q a where qrefers to the code blocks from the so question body andarefers to the corresponding answer code blocks .
in this paper we consider only java questions.
we use the public so data from the stack exchange data dump website for convenience purposes as all the data we needed are structured in the xml form posts.xml for easy extraction.
in particular we harvest java tagged questions from posts.xml .
posts.xml contains all so questions and answers since its establishment from august to may gb in le size .
a total of code pairs are mined and used for detecting defective code fragments.
future addition of new so data can be harvested quickly by querying the stack exchange data explorer by the questions creation time.
however our approach is not limited to java.
this is because so contains questions for di erent types of programming languages.
extracting code fragments of another programming language can be replicated similarly by mining so questions with the tagged keyword of that programming language e.g.
c instead of java.
furthermore the clone detector ccfinderx used in the next phase section .
is able to identify code clones using textually similar tokens regardless of the type of programming languages.
.
clone detection our goal here is to compare the target source code from the software projects with qto detect similar code in the target.
we experimented several state of the art code detection techniques such as graph based grouminer tree based deckard and function based simcad .
these techniques depend on existing java language compilers such as the eclipse java compiler .
these compilers can handle some errors but cannot handle partial code fragments verywell .
many code fragments in so are structured concisely but incomplete thus making them non compilable .
despite wrapping with enclosing classes methods in mitigation to the code with no class name or method signature we were still faced with many compilation errors which were due to missing types.
for example simcad is unable to parse of the so code fragments.
existing stateof the art partial program analysis ppa which attempts to recover missing types also depends on java language compiler and has shown ine ective in processing code from so .
in light of these limitations we turned to tokenbased code detection approach which is independent of any language compiler.
to detect more precise and accurate code clones we restrict the code clone detection to only type i and ii clones.
type iii clones have several constraints such as producing many false positives not being representative of all subjects and require individual subject experiments in determining the lines of gap for omission .
we used ccfinderx a textual token based code clone technique to achieve this detection goal.
ccfinderx is chosen for several reasons.
besides being able to detect type i and ii clones e ciently it also has the ability to detect a wider range of code fragments in di erent languages.
as we do not want our detection to be so ne grained that it covers every single statement it may detect we limit the minimum number of detected tokens to default is in ccfinderx.
this number is chosen because it symbolises a concrete short method with a method name the return type and a single statement body.
we want our clone detector to identify as many useful clones as possible.
the use of a minimum numbered token means that several detected code clone pairs can come from the same pair of source target code pair.
as long as it meets the constrained minimum of tokens ccfinderx will extract the cloneset that covers the largest consecutive code lines that contain the tokens.
the detected code pair is denoted as qf n mm where qfnrefers to the nthdetected code fragment in qandmm refers to the mthmatched code fragment from the software projects.
as an example ccfinderx detected a code clone pair between axisentity.java in jfreechart and the so question id .
since there is only one detection between axisentity.java from jfreechart and the so question id the detected code pair is qf m1 where qf1represents the code fragment in b so question block figure while m1represents the code fragment in a target source code figure .
then qf m1 is traced back to the corresponding q a to form the clone fragment answer code pair qf a for the next phase processing.
.
element and title matching our goal in this phase is to identify the potential defective code fragments from the suite of mmidenti ed in the earlier phase section .
.
the collection of mmin section .
is the set of candidate code fragments where they might be defective.
we discard a detected code pair if there are no matching code like terms between the so title and qfn the so title andmm and mmanda.
we observed many of them are false positives and thus are not essential to our detection.the matching will involve stemming the natural words codelike terms using porter s stemming algorithm before comparing them.
also we observed qfncontaining too many or too few lines of code are false positives.
in addition so does not advocate pasting large chunk of code.
we experimented and found qfnwhose code lines are greater than or less than or equal to three are false positives.
we removed such warnings.
we also compare the code like terms for similarity between the qf n a code pairs from section .
to determine if the corresponding mmis defective.
if at least one code like term is found in both qfnanda then the corresponding code fragments mmfrom the software project are considered to be potentially defective.
it is because we observed that in many answers there are