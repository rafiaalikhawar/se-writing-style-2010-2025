predicting node failure in cloud service systems qingwei lin microsoft research beijing chinaken hsieh microsoft redmond usayingnong dang microsoft redmondusa hongyu zhang the university of newcastle nsw australiakaixin sui microsoft research beijing chinayong xu microsoft research beijing china jian guang lou microsoft research beijing chinachenggang li microsoft research beijing chinayoujiang wu microsoft redmond usa randolph yao microsoft redmond usamurali chintalapati microsoft redmond usadongmei zhang microsoft research beijing china abstract in recent years many traditional software systems have migrated to cloud computing platforms and are provided as online services.
the service quality matters because system failures could seriously affect business and user experience.
a cloud service system typically contains a large number of computing nodes.
in reality nodes may fail and affect service availability.
in this paper we propose a failure prediction technique which can predict the failure proneness of a node in a cloud service system based on historical data before node failure actually happens.
the ability to predict faulty nodes enables the allocation and migration of virtual machines to the healthy nodes therefore improving service availability.
predicting node failure in cloud service systems is challenging because a node failure could be caused by a variety of reasons and reflected by many temporal and spatial signals.
furthermore the failure data is highly imbalanced.
to tackle these challenges we propose ming a novel technique that combines a lstm model to incorporate the temporal data a random forest model to incorporate spatial data a ranking model that embeds the intermediate results of the two models as feature inputs and ranks the nodes by their failure proneness a cost sensitive function to identify the optimal threshold for selecting the faulty nodes.
we evaluate our approach using real world data collected from a cloud service system.
the results confirm the effectiveness of the proposed approach.
we have also successfully applied the proposed approach in real industrial practice.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa association for computing machinery.
acm isbn .
.
.
.
concepts software and its engineering software testing and debugging maintaining software keywords failure prediction service availability node failure cloud service systems maintenance.
acm reference format qingwei lin ken hsieh yingnong dang hongyu zhang kaixin sui yong xu jian guang lou chenggang li youjiang wu randolph yao murali chintalapati and dongmei zhang.
.
predicting node failure in cloud service systems .
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
introduction in recent years deploying applications and services on large scale cloud platforms such as microsoft azure google cloud and amazon aws has been widely accepted by software industry.
these cloud service systems need to provide a variety of services to millions of users from around the world every day therefore high service availability is essential as a small problem could cause serious consequences.
many service providers have made tremendous efforts to maintain high service availability.
for example amazon ebs claims to have five nines which represents the service availability of .
allowing at most seconds down time per month per vm.
microsoft azure also claims similar service availability.
although a lot of effort has been devoted to service quality assurance in reality cloud service systems still encounter many problems and fail to satisfy user requests.
these problems are often caused by failures of computing nodes in cloud service systems.
a cloud service system typically contains a large number of computing nodes which supply the processing network and storage resources that virtual machine instances need.
some empirical studies have dedicated to the analysis of node failures.
for esec fse november lake buena vista fl usa lin et al.
example vishwanath and nagappan classified server failures in a data center and found that of all servers had at least one hardware incident in a given year.
ford et al.
studied the availability of google distributed storage systems and characterized the sources of faults contributing to unavailability.
dinu and ng analyzed hadoop behavior when nodes fail and found that a single failure can result in unpredictable system performance.
in microsoft each day out of all the server nodes in its cloud system less than .
of the nodes encounter failures.
while the failure rate of .
may seem insignificant it has drastic impact on services that target at .
availability or beyond.
according to our experience in microsoft node failure was one of the top causes of service down time.
to improve service availability we propose to predict the failureproneness of a node in cloud service systems before the failure actually happens.
we apply machine learning techniques to learn the characteristics of historical failure data build a failure prediction model and then use the model to predict the likelihood of a node failing in the coming days.
the ability to predict faulty nodes enables cloud service systems to allocate vms virtual machines to the healthier nodes therefore reducing the occurrences and duration of vm down time caused by the node failures.
furthermore if a node is predicted as faulty the cloud service system can perform proactive live migration to migrate the virtual machines from the faulty node to a healthy node without disconnecting the service.
however building an accurate prediction model for node failure in cloud service systems is challenging.
we have identified three main reasons complicated failure causes due to the complexity of the large scale cloud system node failures could be caused by many different software or hardware issues.
examples of these issues include software bugs os crash disk failure service exception etc.
there is no simple rule metric that can predict all node failures in a straightforward manner.
complex failure indicating signals failures of a node could be indicated by many temporal signals produced by the node locally.
they could also be reflected by spatial properties that are shared by nodes that have explicit implicit dependency among them in different global views of the cloud.
we need to analyze both temporal signals and spatial properties to better capture the early failure indicating signals.
highly imbalanced data node fault data is highly imbalanced as most of the time the cloud service system has high service availability.
for example in our system the node ratio between failure and healthy classes is less than i.e.
less than .
nodes contain failures .
the highly imbalanced data poses great challenges to prediction.
to tackle the challenges we propose ming a novel technique for predicting node failure in cloud service systems.
ming includes a lstm based deep learning model to incorporate the temporal data a random forest model to incorporate the spatial data a learning to rank model that embeds the intermediate results of the two models as feature inputs and ranks the nodes by their failureproneness and a cost sensitive function to identify the optimal threshold in the ranked results for selecting the faulty nodes.
we evaluate our approach using real world data collected from a cloud based service system in production.
the results show thatming outperforms the baseline approaches that are implemented using conventional classification techniques.
furthermore we have successfully applied ming to the maintenance of a cloud service system x in microsoft since september .
in a typical day the top failure prone nodes in service x identified by ming capture above of the failures in the next day.
in an a b testing conducted by the service x team ming is able to intelligently allocate new vms to more healthier nodes and has achieved above reduction in these vms failure rate.
ming won the tech transfer of the year award in a research division of microsoft.
the main contributions of our work are as follows we propose ming which can improve service availability by predicting node failure in cloud service systems.
through failure prediction intelligent vm allocation and migration can be achieved.
to build an accurate failure prediction model we design a twophase training model which can well handle the temporal and spatial features and is less sensitive to highly imbalanced data.
we evaluate our approach using data collected from a cloudbased service system.
ming achieves the average recall of .
and precision of .
on three datasets and outperforms the baseline approaches built using conventional classifiers.
we have applied ming to the maintenance of a large scale cloud service system.
the results confirm the effectiveness of ming in industrial practice.
to the best of our knowledge this is the first time node failure prediction is applied in a production cloud service environment.
the rest of this paper is organized as follows in section we introduce the background and motivation of our work.
section covers the proposed framework and algorithms.
the evaluation is described in section .
we also discuss the results and presents the threats to validity.
in section we share our success stories and experience obtained from industrial practice.
the related work and conclusion are presented in section and section respectively.
improving service availability of cloud systems .
cloud service systems cloud computing has emerged as a new paradigm for delivery of computing as services via the internet.
it offers many service models such as infrastructure as a service iaas platform as a service paas and software as a service saas .
deploying applications and services on cloud computing platforms such as microsoft azure and amazon web services has been widely accepted by software organizations and developers.
a typical cloud service system contains a large number of physical servers or nodes .
for example amazon web services is likely to have .
million servers1.
the nodes are arranged into racks and a group of racks form a cluster.
virtualization is one of key technologies used in modern cloud computing which offers better scalability maintainability and reliability.
a physical node can host multiple virtual machines vms .
vms can be backed up scaled or predicting node failure in cloud service systems esec fse november lake buena vista fl usa duplicated making it easy to suit end users needs.
when a vm allocation request is sent out the cloud service system will determine the appropriate node to host the vm.
if a node fails all vms hosted on it will correspondinly fail.
cloud service systems also support live migration which refers to the process of moving a running vm between different nodes without disconnecting the client or application .
live migration is a powerful mechanism for managing cloud services as it enables rapid movement of workloads within clusters with low impact on running services.
.
service availability for large scale software systems especially cloud service systems such as microsoft azure amazon aws and google cloud high service availability is crucial.
service availability is a state of a service being accessible to the end user.
usually expressed as a percentage of uptime it represents the level of operational performance of a system or component for a given period of time.
as cloud systems provide services to hundreds of millions of users around the world service problems can often lead to great revenue loss and user dissatisfaction.
hence in today s practice the service providers have made every effort to maintain a high service availability such as five nines .
meaning less than seconds down time per month per vm allowed2.
although tremendous effort has been made to maintain high service availability in reality there are still many unexpected system problems caused by software or platform failures such as software crashes network outage misconfigurations memory leak hardware breakdown etc.
.
these problems become more severe with the ever increasing scale of the service systems.
for example in february aws experienced a massive outage of its s3 storage services causing a majority of websites which relied on aws s3 unresponsive.
of the internet s top retailers observed website performance slow by or more3.
it has been found that node failure is one of the most common problems that cause system unavailability .
our experience in microsoft also shows that node failure is the dominant cause of service down time.
if a node fails all the vms running on it will correspondingly fail which could affect service availability.
.
improving service availability by node failure prediction different nodes may fail at different times.
we propose to predict the failure proneness of a node based on the analysis of historical fault data before the node fails.
the ability to predict node failure can help improve service availability from the following two aspects vm allocation which is the process of allocating a vm virtual machine to a node.
to enable better vm allocation we can always allocate vms to a healthier node rather than to a faulty node.
live migration which is the process of moving a running vm between different nodes without disconnecting the client or application.
to enable more effective live migration of nodes we can proactively migrate vms from the predicted nodes to the healthy ones before node failure actually happens.
to achieve so we can build a prediction model based on historical failure data using machine learning techniques and then use the model to predict the likelihood of a node failing in the near future.
the prediction model should have the following abilities the prediction model should be able to rank all nodes by their failure proneness so that the service systems can allocate a vm to the healthiest node available.
the prediction model should be able to identify a set of faulty nodes from which the hosted vms should be migrated out under the constrains of cost and capacity.
there are several technical challenges in designing a failure prediction model for a large scale cloud .
.
complicated failure causes.
due to the complexity of a cloud service system node failures could be caused by many different software or hardware issues.
examples of these issues include os crash application bugs disk failure misconfigurations memory leak software incompatibility overheating service exception etc.
according to the estimation of domain experts in microsoft the number of root causes of node failure is over one hundred.
simple rule based or threshold based models are not able to locate the problem and achieve good prediction results.
to tackle this challenge in this paper we propose a machine learning based approach to node failure prediction in cloud systems.
.
.
complex failure indicating signals.
failures of a single node could be indicated by temporal signals coming from a variety of software or hardware sources of the node.
examples of the temporal signals are performance counters logs sensor data and os events.
they are continuously monitored and changing over time.
in a large scale cloud system failures of a node could also be reflected by spatial properties shared by the nodes that are explicitly implicitly dependent on each other.
we have identified the following dependencies between two nodes resource based dependency two nodes may compete for a computing resource e.g.
a router location based dependency two nodes may co exist in the same computing segment domain such as the same rack load balancing based dependency two nodes may be in the same group for load balancing.
the mutually dependent nodes tend to be infected by the same failure inducing cause.
for example if a certain portion of nodes fail other nodes in the same segment could fail in the near future too.
therefore the spatial properties that are shared among the mutually dependent nodes also have predictive power.
examples of the spatial properties include update domain shared router rack location resource family load balance group batch operation group etc.
we need to incorporate both temporal and spatial data in order to better capture the early failure indicating signals and build an accurate prediction model.
to tackle this challenge in this paper we construct two specific base learners to incorporate temporal and spatial data respectively.
we then ensemble the results to train a ranking model.
.
.
highly imbalanced data.
in a large scale cloud service system of microsoft every day only one in one thousand nodes could esec fse november lake buena vista fl usa lin et al.
become faulty.
the extreme in imbalanced ratio poses difficulties in training a classification model.
fed with such imbalanced data a naive classification model could attempt to judge all nodes to be healthy because in this way it has the lowest probability of making a wrong guess.
some approaches apply data re balancing techniques such as over sampling and under sampling techniques to address this challenge.
such approaches help raise the recall but at the same time could introduce a large number of false positives which dramatically decreases the precision.
to tackle this challenge in this paper we propose a ranking model to rank the nodes by their failure proneness.
unlike a conventional classification model whose objective is to find a best separation to distinguish all the positive and negative instances a ranking model focuses on optimizing the top kreturned results therefore it is more appropriate in our scenario.
the proposed approach .
overview in this paper we propose ming a novel technique for improving service availability by predicting node failure in a cloud service system.
figure shows an overall workflow of the proposed approach.
ming includes two phases of training.
in phase two base classification models are trained a lstm model for temporal data and a random forest model for spatial data.
in phase the intermediate results of the two base learners are embedded as features and fed as input to a ranking model.
the ranking model ranks the nodes by their failure proneness.
ming identifies the toprones that minimize the misclassification cost as the predicted faulty nodes.
we describe the major steps in this section.
figure the overview of ming .
phase training in this phase we first train base learners on the training data.
as stated in section to construct an effective node failure prediction model we collect heterogeneous data for each node from diverse sources and identify features from the data with the help of product teams.
table shows some examples of features.
these features can be categorized into the following two types temporal features which directly represent a node s local status in time such as performance counters io throughput resource usage sensor values response delays etc.
or can be aggregated as temporal data from the original sources such as log event counts error exception event counts system event counts etc.
.
we collect of these features in total.
spatial features which indicate explicit implicit dependency in global relationships among nodes.
examples of these features include deployment segment rack location load balance group policy group update domain etc.
we collect these features.
table some examples of features feature type description updatedomain spatial the domain where nodes share same update setting.
memoryusage temporal memory consumption.
disksectorerror temporal sector errors in a disk drive.
serviceerror temporal error counts from a deployed service.
racklocation spatial the location of the rack the node belongs to.
loadbalancegroup spatial the group where nodes load are balanced.
ioresponse temporal i o response time.
osbuildgroup spatial the group where nodes have the same os build.
it is known that different machine learning algorithms could work well only on some specific types of features while performs weakly on others.
to support specific type of features feature conversion needs to be performed e.g.
converting categorical features into numeric values which may incur much information loss.
to cater for different types of features in this phase we build a separate learner for each type of data temporal and spatial.
figure training lstm model for temporal features we apply lstm long short term memory which is a widely adopted deep neural network dnn model .
it can balance between retaining the previous state and memorizing new information.
lstm can better capture the patterns behind the time series data and has proven to be successful in solving tasks such as machine translation and speech recognition.
figure illustrates the training of the lstm model in our approach.
we use bi directional lstm for the time series data x1 x2 ... xn where xiis the input vector of all temporal features at time i. the lstm layer produces the representation sequence h1 h2 ... hn which is fed to a dense layer fully connected layer .
the output of the dense layer is a x vector vt. the vector is fed predicting node failure in cloud service systems esec fse november lake buena vista fl usa to asof tmax function which is the final layer of a dnn based classifier.
figure training random forest model for spatial features we apply the random forest learner which is one of the most widely used classification methods.
it builds a multitude of decision trees at training time and outputs the class of the voting result from the individual trees.
random forest splits the trees based on the information gain therefore it can better reflect the impact of discrete values.
figure illustrates the training of the random forest model in our approach.
given the spatial data an ensemble of trees total are trained.
the results h1 h2 ... hn are concatenated into a vector vsand fed to a majority voting module producing the final classification result.
.
phase training in phase training we construct a prediction model to predict the failure proneness of nodes in near future.
different nodes have different likelihood of failing.
in this step we formulate the prediction problem as a ranking problem.
that is instead of simply telling whether a node is faulty or not we rank the nodes by their probability of being faulty.
by giving relative order as opposed to a binary label the results of a ranking algorithm can distinguish between different degrees of failure proneness.
the ranking results better serve the goal of vm allocation allocating a vm to a healthier node and on demand live migration migrating a vm from a faulty node to a healthier nodes .
furthermore as the ranking approach is effective in optimizing the top kresults it could work better in the case of highly imbalanced data.
in this phase we take the intermediate output vectors produced by the base learners as the input vector to the ranking model.
more specifically from the lstm model we use the output vector vt of the dense layer.
from the random forest model we use the output vector vsproduced by the trees.
we then concatenate the two output vectors and form a x input vector vfor the ranking model.
to train a ranking model we obtain the historical failure data about the nodes and rank the nodes according to the frequency and duration of failures.
we adopt the concept of learning to rank which automatically learns an optimized ranking model to minimize the cost of disorder especially the cost for the top results similar to the optimization of the top results in a search engine .
specifically lambdamart is used here which is the boosted tree version of learning to rank algorithm.
it has proven to be avery successful algorithm and won the yahoo!
learning to rank challenge track .
.
cost sensitive thresholding to improve service availability we would like to intelligently allocate vms to the healthier nodes so that these vms are less likely to suffer from node failures in near future.
we also propose to proactively migrate live vms residing on high risk nodes to healthy nodes.
to achieve so we apply cost sensitive thresholding to identify the faulty nodes for live migration.
as most of the nodes are healthy and only a small percentage of nodes are faulty we select the top rnodes returned by the ranking model as the faulty ones.
the optimal top rnodes are selected with historical data to minimize the total misclassification cost r arg min r costratio fpr fnr where fprandfnrare the number of false positives and false negatives in the top rpredicted results respectively.
we denote by cost 1the cost of failing to identify a faulty node false negatives .
we denote by cost 2the cost of wrongly identifying a healthy node as faulty false positives which involves the cost of unnecessary live migration from the faulty node to a healthy node.
we define costratio as a ratio cost cost .
the value of costratio is estimated by experts in product teams.
in current practice due to the concerns about cost and capacity costratio is set to i.e.
precision is valued more than recall .
the optimum rvalue is determined by minimizing the total misclassification cost with historical data.
the toprnodes are predicted faulty nodes.
they are high risky nodes and the vms hosted on them should be migrated out.
evaluation .
research questions in this section we evaluate our approach using real world data.
we aim at answering the following research questions rq1 how effective is the proposed approach in predicting node failures in a cloud service system?
in this rq we evaluate the overall effectiveness of ming in predicting node failures in a cloud service system.
as classification methods are commonly used in fault prediction we also compare ming with the baseline approaches that are implemented using conventional classifiers including logistic regression random forest lstm and svm.
to perform the comparison before applying the conventional classifiers we convert features to the feature types that can be consumed by the target classifiers e.g.
converting the categorical data into numerical data .
we then construct classification models and compare the prediction results with those returned by the proposed approach.
rq2 are the temporal and spatial features useful for failure prediction ?
as described in section we collect both temporal and spatial data for node failure prediction.
in phase training we construct a lstm and a random forest based prediction model to incorporate the temporal and spatial features respectively.
in this rq we evaluate the usefulness of each type of the features.
to answer this rq we train the prediction mode using temporal and spatial data esec fse november lake buena vista fl usa lin et al.
separately and compare the results with those achieved by ming which uses both types of data .
to train the prediction model with temporal data alone only the lstm model described in section is needed.
to train the prediction model with spatial data alone only the random forest model is needed.
the rest of the settings remain the same.
rq3 is the proposed ranking method effective ?
as described in section we propose a ranking method to rank nodes by the fault probability section .
.
in this rq we evaluate if the proposed ranking method is effective by measuring the effectiveness of ming that replaces the ranking model with a conventional classification model denoted as ming c .
to answer the rq in ming c we replace the ranking model with svm logistic regression and random forest classifier separately.
the input to the classifiers are still the combination of output vectors produced by phase training.
the rest of the settings remain the same between ming and ming c. to enable comparison the output of the classifiers are ranked by the probability values.
in ming we use lambdamart as the ranking algorithm.
to show that ming is actually independent of a specific ranking algorithm we also experiment with a variant of ming which replaces lambdamart with fasttree ranker.
the fasttree algorithm is an implementation of fastrank.
it builds each regression tree which is a decision tree with scalar values in its leave in a step wise fashion.
we will compare the effectiveness of ming with the two different rankers.
.
evaluation setup .
.
dataset.
to evaluate the proposed approach we collect data from a production cloud service system.
for offline training we collect over three month datasets and each dataset covers one month period in .
the data are from part of the data centers containing over half a million of physical cloud computing nodes.
all faulty nodes over the period are selected as positive samples while healthy nodes negative samples are randomly selected with a sampling rate.
we use three datasets collected over three day periods after the training periods for testing.
note that for all nodes the feature data is collected at least hours before the class label data is collected.
the hour gap is intended to simulate real world usage predicting node failures using the signals collected before the failures actually happen .
table summarizes the datasets used in this experiment.
note that we did not apply the commonly used cross validation because cross validation may not reflect the real results of prediction and sometimes may overclaim the effectiveness.
more details will be discussed in section .
.
table the experimental data training period test period dataset dataset dataset .
.
tool implementation.
we implemented the proposed approach by leveraging the various components provided by azureml4 is a production environment for development and deployment of machine learning models.
the experimental evaluation is running on a windows server with intel cpu e5 4657l v2 .40ghz .
with gb memory .
.
.
evaluation metrics.
most of existing classification based prediction models use precision recall f1 measure to evaluate the effectiveness of the models.
in our experiments we also use these metrics as evaluation metrics although there are more metrics for evaluating a ranking based model .
precision measures the percentage of identified faulty nodes that are actually faulty.
recall measures the percentage of faulty nodes that are correctly identified over all the faulty nodes.
f1 measure is the harmonic mean of precision and recall which weights recall and precision equally.
ming also ranks the nodes according to their failure proneness.
to evaluate the ranking ability of ming we compute the precision k values which are the precision value for top knodes k and .
an ideal failure prediction model should be able to correctly identify the failure prone nodes in the topkreturned results.
thus the higher the metric value the better the prediction performance especially in actual practice when the cost of false positive is higher than false negative as discussed in section .
.
.
evaluation results rq1 how effective is the proposed approach in predicting node failures in a cloud service system?
table shows the evaluation results of ming in identifying faulty nodes the top rreturned nodes .
ming achieves good results on all datasets and outperforms the baseline methods.
the average precision recall and f1 measure values are .
.
and .
respectively.
considering the highly imbalanced nature of data and the complexity of the problem it is very challenging to achieve both high recall and high precision.
in our scenario precision is more important than recall therefore we tradeoff some recall to achieve higher precision.
our results show that ming outperforms the baseline approaches that are implemented in conventional classification algorithms svm logistic regression random forest and lstm in both precision and recall.
the average absolute improvement in f1 measure over logistic regression svm random forest and lstm is .
.
.
and .
respectively.
we also evaluate the ranking ability of ming by examining the topkreturned results.
figure shows the precision k values.
for the top and returned nodes the precision values achieved by ming on all datasets are close to .
when the top returned nodes are examined the precision values are still higher than .
on all datasets.
the results show that ming can effectively rank the faulty nodes and consistently achieve high precision.
we also compare the precision k results of ming with those of the classification algorithms svm logistic regression random forest and lstm .
to enable the comparison we rank the probability values returned by each classification algorithm.
the results are also shown in figure .
ming outperforms the classification algorithms consistently when precision k is concerned.
in ming the faulty nodes are identified by selecting the optimal set of top rnodes that minimize the total misclassification cost.
we also experimented with different thresholds the rvalues that predicting node failure in cloud service systems esec fse november lake buena vista fl usa table the effectiveness of ming ming logistic regression lr svm random forest lstm precision recall f1 precision recall f1 precision recall f1 precision recall f1 precision recall f1 dataset .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dataset .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dataset .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
avera e .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
figure the precision of the top kreturned nodes figure the roc curve of the comparative approaches classify faulty and healthy classes.
the results are shown in figure where the roc curve5plots tpr true positive rate versus fpr false positive rate with a varying threshold value.
the results show that ming outperforms the baseline approaches consistently under different fpr tpr ratios.
for example on dataset the auc area under curve value achieved by ming is .
while the value for logistic regression svm random forest and lstm are .
.
.
and .
respectively.
in summary the experimental results show that the proposed approach is effective in predicting node failures in a cloud service system and outperforms the baseline methods.
rq2 are the temporal and spatial features useful?
ming utilizes two base learners random forest and lstm to incorporate the temporal and spatial features respectively.
in this rq we evaluate the usefulness of each type of the features.
the results are shown in table .
both types of the features are used ming achieves an average f1 measure of .
.
if the temporal features are used alone only the lstm model is trained and the average f1 measure drops from .
to .
.
if the spatial features are used alone only the random forest model is trained and the average f1 measure drops from .
to .
.
we can see the ensemble model adopted by ming achieves the best overall results.
also the results achieved by spatial features are higher that those achieved by temporal features indicating that the spatial features are more predictive than the temporal features.
in summary the experimental results show that both types of features are useful for node failure prediction and the spatial features have more predictive power.
the results also confirm that each base learner is useful and the proposed ensemble model is more effective.
rq3 is the proposed ranking method effective ?
in this rq we evaluate if the propose ranking method is effective by comparing ming the proposed approach with the ranking model and ming c the variant of ming that replaces the ranking esec fse november lake buena vista fl usa lin et al.
table the effectiveness of the ensemble model temporal spatial ming temporal only lstm spatial only random forest precision recall f1 precision recall f1 precision recall f1 dataset .
.
.
.
.
.
.
.
.
dataset .
.
.
.
.
.
.
.
.
dataset .
.
.
.
.
.
.
.
.
avera e .
.
.
.
.
.
.
.
.
figure ranking vs. classification average precision k model with a conventional classifier such as random forest rf svm and logistic regression lr .
the results are shown in table .
clearly ming outperforms ming cwith all classifiers in both precision and recall.
we also evaluate the accuracy of the top k returned results.
figure shows the average precision k values on all the three datasets.
clearly ming outperforms ming cunder allkvalues.
furthermore ming r fasttreeranker the ming variant with the fasttree ranker also outperforms ming cwith all classifiers and achieves comparable performance with ming.
this indicates that the results achieved by ming is independent of a specific ranker that we have chosen.
it is the general design of a ranking model in our approach that helps improve the effectiveness of ming.
table the effectiveness of the ranking model precision recall f1 ming .
.
.
ming cwith random forest .
.
.
ming cwith svm .
.
.
ming cwith lr .
.
.
ming r fasttreeranker .
.
.
.
discussions .
.
why does ming work.
to build an effective prediction model we collected more than features from a wide variety of data sources.
the features identified from temporal and spatial data contain early signals of node failures.
the prediction model is trained using a large amount of real world data.
it is widely known that a traditional machine learning algorithm works better on a certain type of features while performs weakly on other types of features.
to work with all types of features feature conversion like converting categorical features into numerical features needs to beperformed before training a model.
however a lot of information could be lost during the conversion process thus decreasing the accuracy.
as shown by our experimental results the two base models in ming the lstm model and the random forest model can better capture the characteristics of temporal and spatial features therefore producing better results.
ming embeds the intermediate output of the two base models as the feature input for a ranking model and formulates the node failure prediction problem as a ranking problem.
by optimizing the order of failure proneness the faulty nodes with higher failureproneness are raised to the top resulting in the high accuracy of the predicted results.
the order can be utilized in vm allocation and live migration practice as vms can be moved to a much healthier node.
furthermore traditional classification algorithms are naturally sensitive to the distribution of classes as they optimize to split the data instances into classes while a ranking based method like ming focuses on the order of data instances and is therefore less sensitive to the imbalanced class problem.
.
.
evaluation metrics.
much research work show that in a large software system the distribution of faults is skewed that a small number of modules accounts for a large proportion of the faults.
in our work we find that the distribution of faulty nodes is also skewed.
the ratio between faulty and healthy nodes could be as high as .
the highly imbalanced data imposes challenges for failure prediction.
in general it is difficult for a machine learning technique to distinguish a small number of faulty modules from a large number of modules.
the highly imbalanced failure data also has implications on evaluation metrics.
each day out of all the computing nodes in the cloud service system we studied less than .
of the nodes encounter failures.
while the failure rate of .
may seem insignificant the absolute number of failed nodes is significant as the total number of nodes is very large.
therefore the .
failure rate has significant adverse impact on service availability.
in literature zhang also pointed out that prediction results may not be always satisfactory in the presence of imbalanced data distribution.
they found that high probability of detection pd i.e.
true positive rate and low probability of false alarm p f i.e.
falsepositive rate do not necessarily lead to high precision.
the reason is that the percentage of faulty modules could be very small.
the zhangs equation for precision is defined as follows precision tp tp fp f p t p n eg p f pos pd where neg is the number of negative instances and pos the number of positive instances.
from the equation we can see that even if pdis high and p fis low the precision would be low if the number predicting node failure in cloud service systems esec fse november lake buena vista fl usa of negative instances neg is much more than the number of positive instances pos .
therefore the metrics p f pd and the roc curves should be used with caution.
in our study we show recall precision values as well as the roc curves to confirm the effectiveness of the proposed approach.
.
.
oversampling.
before constructing a prediction model one could apply an imbalanced data handling approach such as smote to balance the data.
smote is a commonly used oversampling technique in which the minority class is over sampled by creating synthetic examples through finding k nearest neighbors along the minority class.
we tried to apply smote to our approach but did not get promising results.
this is because after balancing there are still many false positives due to the highly imbalanced data.
.
.
parameter settings.
in the experiments and comparative evaluations described in section .
we use the default settings of the machine learning algorithms.
it has been observed that there is a bias in the comparison between different algorithms with default parameter settings .
to evaluate the impact of parameter settings we have also experimented with different values of several important parameters a iterate the output vector size parameter used by lstm and random forest from to with a step of b iterate the number of boosting iterations parameter used by lambdamart from to with a step of c iterate the number of leaves parameter used by random forest from to with a step of .
with different parameter settings the resulting average f1 scores on three datasets are quite stable ranging from .
to .
with a delta of .
to .
compared to the average f1 measure of .
shown in table .
these results show that ming is insensitive to parameter settings.
.
.
threats to validity.
we have identified the following threats to validities subject systems in our experiments we only collect data from one cloud service system of one company.
therefore our results might not be generalizable to other systems.
however it is challenging to get access to data from many cloud systems.
the system we studied is a typical large scale cloud service system from which sufficient data can be collected.
furthermore we have applied our approach in the maintenance of an actual cloud service system.
in future we will reduce this threat by evaluating ming on more subject systems and report the evaluation results.
evaluation metrics we used the precision recall f measure metrics to evaluate the prediction performance.
these metrics have been widely used to evaluate the effectiveness of a prediction model.
prior work points out that a broader selection of metrics should be used in order to maximize external validity.
in our future work we will reduce this threat by experimenting with more evaluation measures such as the cumulative lift chart clc and the faultpercentile average fpa metrics used in .
success stories .
success stories we have successfully applied ming to the maintenance of service x which is a large scale cloud service system in microsoft.
service x allows developers and it professionals to build deploy and manageapplications.
the cloud service achieves global scale on a worldwide network of data centers across many regions.
figure the failures captured vs. the nodes examined ming is currently used by service x to preferentially select healthier nodes for vm allocation.
the current in production model is trained and scored using azureml.
the automation relies on the azureml batch web service feature.
we invoke daily jobs across the entire azure stack to refresh daily the failure proneness scores of all the nodes in service x. after deploying ming the product team computes the percentage of failures captured by the nodes that are ranked by their failureproneness score.
the results are shown in figure .
in a typical day the top most failure prone nodes predicted by ming capture above of the failures in the next day.
the product team also conducted an a b testing in a large scale cloud environment.
the results show that ming is able to intelligently allocate vms to more healthier nodes and has achieved above reduction in these new allocated vms failure rate.
the ability to predict node failure in cloud service systems also helped product teams diagnosis service problems.
we work with domain experts in product teams to identify the influential features using feature selection methods and help product teams analyze the root causes of the node failures based on the influential features.
for example we found that upgrading system software to a specific version caused a potential node failure and some configuration changes to the cluster also resulted in node failure.
we even found that the nodes on the upper layer rack have much higher failure probability than the nodes on the lower rack as the hot air rises up to the top rack .
.
lessons learned freshness of the training data extending the training period does not bring many benefits to the prediction accuracy.
currently we use one month data for training.
extending the length of the training period can only improve the results slightly.
the reason is that the cloud service systems are evolving rapidly and the fault patterns vary a lot over time.
also new applications new deployment and new versions happen frequently and could introduce new faults that are previously unseen.
learning from very old data will not have much gain in the prediction as some old fault patterns could have vanished.
therefore in practice we need to train the esec fse november lake buena vista fl usa lin et al.
model frequently e.g.
daily with new data in order to catch the new fault patterns emerged recently.
the time gap between data and label as we are dealing with a prediction problem when training the model we need to take care of the time gap between the collection of feature data and the collection of labels.
if we did not set the time gap between them the model could learn from some feature data coming with the failures.
however in the case of failure prediction we need features that are early indicators of failures.
the feature data collected at the time of failure may not have predictive power.
in practice to secure the time gap we collect feature data at least hours before the collection of labels.
in this way we always learn from early signals.
cross validation and online prediction cross validation is often used to evaluate machine learning models.
a k fold crossvalidation randomly divides a data set into kpartitions and uses k 1partitions to train the prediction model and the remaining partition to test the model.
therefore it is possible that knowledge that should not be known at the time of prediction is utilized in cross validation.
for example an incident may cause many nodes to fail around the same time.
in cross validation data about these new failures may be randomly selected for training which increases the prediction accuracy in testing for the nodes affected by the same incident around the same time.
therefore cross validation is not suitable for evaluating our model in practice even though it can lead to better results than online prediction.
in real world online prediction training and testing data are strictly split by time and the testing period is always after the training period.
therefore the characteristics of data appear in the future is not used for training.
the problem of cross validation in an online prediction scenario is also observed by others .
related work .
failure prediction in recent years we have witnessed a lot of interest in developing software defect prediction models .
it is widely believed that some internal properties of software e.g.
metrics have relationship with the external properties e.g.
quality .
software defect prediction refers to building a prediction model for new software modules using historical metric and fault data collected from existing projects.
for example menzies et al.
performed an extensive study on nasa datasets using three classification techniques with static code metrics.
nam et al.
proposed a heterogeneous defect prediction method that matches up different metrics in different projects.
jing et al.
proposed a heterogeneous defect prediction method based on unified metric representation and canonical correlation analysis.
kim et al.
addressed the data quality issue in software defect prediction and found that a small degree of data noise does not affect the prediction results significantly.
recently deep learning techniques have been applied to software defect prediction as well .
there are also related work on predicting disk failures and computing system failures .
for example pinheiro attempted to find variables that may be used to predict disk failures from observations of a large disk drive population in a production internet services deployment.
gaber et al.
used machine learning algorithms to extract compound features representing thebehavior of the drives and predict the failure of the drives.
xu et al.
utilized both disk level sensor data and system level signals for predicting disk errors in cloud systems.
our work is about node failure prediction for a cloud service system which has a larger scope and requires analyzing more heterogeneous features.
node failure can be triggered by any software or hardware issue or a mixture of both which brings new challenges compared to software disk failure prediction.
.
analysis of failures in cloud systems there have been some previous studies in the literature on failures of a data center.
for example ford et al.
studied data collected from google storage systems over a one year period and characterized the sources of faults contributing to unavailability.
their results indicate that cluster wide failure events should be paid more attention during the design of system components such as replication and recovery policies.
gill et al.
presented a large scale analysis of failures in a data center network.
they characterized failure events of network links and devices estimated their failure impact and analyzed the effectiveness of network redundancy in masking failures.
zhou et al.
performed an empirical study on the quality issues of a production big data platform used in microsoft.
they analyzed real service quality issues and investigated the common symptom causes and mitigation solutions.
their finding shows that .
of escalations are caused by hardware faults and .
are caused by system side defects.
several methods have also been proposed to detect node failures in a cloud data center based on the network structure.
besides there have been studies on detecting the gray failures which are component failures whose manifestations are fairly subtle and thus defy quick and definitive detection .
these approaches provide solutions to detect node failures and improve the service quality but they did not provide systematic methods for predicting node failure in cloud service systems.
conclusion to maintain and improve service availability of a cloud service systems we propose ming a node failure prediction approach.
using ming we can intelligently allocate migrate vms to the healthier nodes so that these vms are less likely to suffer from node failures.
we propose an ensemble of machine learning models to combine heterogeneous data from diverse sources.
to better handle the highly imbalanced data we rank the nodes according to their failure proneness and select the top rnodes that minimize the misclassification cost.
we have evaluated the proposed approach using real world data and have successfully applied ming to the maintenance of a production cloud service system.
we believe that given the importance of service availability failure predictors will play increasingly important roles in the design and maintenance of cloud service systems.
our proposed approach is an important step in this direction.
acknowledgement we specially thank our product team partners ervin peretz geoffrey goh david dion bertus greeff john miller girish bablani for the collaboration and suggestion and our intern students pu zhao yuchen sun wenchi zhang for the development and experiments.
predicting node failure in cloud service systems esec fse november lake buena vista fl usa