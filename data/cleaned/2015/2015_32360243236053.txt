optimizing test prioritization via test distribution analysis junjie chen yiling lou hcst peking university moe china chenjunjie louyiling pku.edu.cnlingming zhang department of computer science university of texas at dallas usa lingming.zhang utdallas.edujianyi zhou hcst peking university moe china zhoujianyi pku.edu.cn xiaoleng wang baidu online network technology beijing co. ltd china wangxiaoleng baidu.comdan hao lu zhang hcst peking university moe china haodan zhanglucs pku.edu.cn abstract test prioritization aims to detect regression faults faster via reordering test executions and a large number of test prioritization techniques have been proposed accordingly.
however test prioritization e ectiveness is usually measured in terms of the average percentage of faults detected concerned with the number of test executions rather than the actual regression testing time making it unclear which technique is optimal in actual regression testing time.
to answer this question this paper rst conducts an empirical study to investigate the actual regression testing time of various prioritization techniques.
the results reveal a number of practical guidelines.
in particular no prioritization technique can always perform optimal in practice.
to achieve the optimal prioritization e ectiveness for any given project in practice based on the ndings of this study we design learning based predictive test prioritization ptp .
ptp predicts the optimal prioritization technique for a given project based on the test distribution analysis i.e.
the distribution of test coverage testing time and coverage per unit time .
the results show that ptp correctly predicts the optimal prioritization technique for out of open source projects from github outperforming stateof the art techniques signi cantly in regression testing time e.g.
.
to .
improvement in detecting the rst regression fault.
furthermore ptp has been successfully integrated into the practical testing infrastructure of baidu a search service provider with over 600m monthly active users and received positive feedbacks from this work is partially supported by national key research and development program of china grant no.
2017yfb1001803 nsfc grant nos.
and it is also partially supported by nsf grant nos.
ccf ccf1763906 ut dallas start up fund google huawei and samsung.
corresponding author hcst is short for key lab of high con dence software technologies.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior speci c permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa association for computing machinery.
acm isbn .
.
.
.
testing team of this company e.g.
saving beyond 2x testing costs with negligible overheads.
ccs concepts software and its engineering software testing and debugging keywords regression testing test prioritization machine learning acm reference format junjie chen yiling lou lingming zhang jianyi zhou xiaoleng wang dan hao and lu zhang.
.
optimizing test prioritization via test distribution analysis.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa 12pages.
introduction test prioritization one of the most widely studied regression testing approaches aims to reorder test executions with the goal of detecting more faults earlier .
to date based on whether considering test execution costs various cost unaware techniques and cost aware techniques have been proposed.
these existing prioritization techniques are mostly evaluated based on average percentage of faults detected apfd which calculates the average rate of faults detected when executing di erent numbers of tests.
however apfd assumes that all the tests have the same execution time and treats them equivalently which is usually not true in practice.
for example for the project mapdb used in this paper the test with the longest running time spends .
105x more time than that with the shortest running time.
to address this measurement issue elbaum et al.
proposed a cost cognizant version of apfd apfd c which considers di erent test costs and fault severities.
since fault severities can be hard to determine in practice epitropakis et al.
further simpli ed this measurement by assuming all faults have the same severity.
however to date there lack extensive studies uniformly comparing both cost unaware and cost aware techniques in terms of both apfd and apfd c and thus it is yet unknown which test prioritization technique performs optimal in practice.
to answer this question we rst conduct an empirical study of widely used prioritization techniques in terms of both apfd and esec fse november lake buena vista fl usa j. chen y. lou l. zhang j. zhou x. wang d. hao l. zhang apfd con open source projects from github.
the studied techniques include four widely studied cost unaware techniques i.e.
the total technique the additional technique the search based technique and the adaptive random technique a stateof the art cost aware technique i.e.
the cost cognizant additional technique and also a baseline technique i.e.
the cost only technique that simply prioritizes tests based on the ascending order of test execution time .
our study reveals various practical guidelines.
in particular there is no type of test prioritization techniques that can always perform optimal in terms of the apfd cmeasurement.
for example the state of the art additional technique does not perform well in terms of apfd c while the minimalist cost only technique can perform rather well in some cases.
to achieve the optimal prioritization e ectiveness for any given project in practice we further propose a predictive test prioritization approach ptp which optimizes test prioritization by guiding the selection of prioritization techniques i.e.
cost unaware costaware and cost only techniques via machine learning.
there are two main challenges in ptp.
the rst one is which characteristics can impact the selection of prioritization techniques.
through quantitatively and qualitatively analyzing the study results we nd that the distribution of test coverage testing time and coverage per unit time can greatly in uence the e ectiveness of prioritization techniques.
this is also the key insight of ptp.
the test coverage testing time and coverage per unit time of each test are regarded as basic features and then how to utilize such basic features to predict the optimal prioritization technique is the second challenge.
based on the observations in the study the high level distribution patterns constructed from these basic features are more directly related to the optimal prioritization.
therefore in ptp we adopt the recently proposed xgboost short for extreme gradient boosting algorithm that can e ectively learn high level features .
ptp builds a predictive model via xgboost by collecting three groups of features i.e.
distribution of test coverage testing time and coverage per time unit on existing projects and labeling which prioritization technique performs optimal on the training data.
based on this model for a new project ptp can directly predict which prioritization technique performs optimal in practice.
to su ciently evaluate the e ectiveness of ptp we conduct a study on the same projects and additional open source projects from github projects in total .
from the results ptp correctly predicts the optimal technique for out of projects indicating extremely high accuracy of .
in fact ptp can also be viewed as a new prioritization technique which outperforms the existing costunaware cost aware and cost only techniques to a large extent e.g.
.
to .
faster in detecting the rst regression fault.
in addition ptp has been integrated into the practical testing infrastructure of baidu a famous search service provider with over 600m monthly active users.
in this paper we report all the results of ptp on the projects of baidu collected until feb. including industrial subjects with real regression faults.
the results show that ptp correctly predicts the optimal prioritization technique for of subjects a high accuracy of .
.
same to the ndings on open source projects ptp also outperforms the existing costunaware cost aware and cost only techniques signi cantly e.g.
.
to .
faster in detecting the rst real regression fault further demonstrating ptp s e ectiveness in practice.
in this industry application we use the model trained based on the data of the open source projects to predict the optimal prioritization technique for industrial subjects directly indicating that ptp provides a generally usable model for its practical usage.
in particular ptp received positive feedbacks from the testing team of this company e.g.
saving beyond 2x testing costs with negligible overheads .
this paper investigates which prioritization technique should be applied to a given project in practice through an empirical study and a learning based approach and has the following contributions empirical study.
an empirical study comparing various test prioritization techniques in terms of actual regression testing time which provides a series of practical guidelines to advance test prioritization.
practical approach.
a machine learning based approach that predicts which test prioritization technique is optimal in practice for a given project based on its distribution of test coverage testing time and coverage per unit time.
e ectiveness evaluation.
experimental results on opensource projects demonstrating that the proposed approach outperforms state of the art test prioritization techniques by up to .
in detecting the rst regression fault.
industry application.
integration of ptp in the industrial testing infrastructure achieving up to .
faster detection of the rst regression fault compared with state of the art techniques on industrial subjects with real regression faults.
study on optimal test prioritization to learn which technique performs optimal in terms of practical e ectiveness we perform an empirical study in this section.
.
study design .
.
subjects.
in this work we used open source projects from github totaling lines of source code and .
seconds of testing time.
note that we used projects in this study and used all the projects in the study evaluating our learning based approach ptp presented in section .
for the projects used in this study projects are randomly selected from prior prioritization work .
besides we collected another open source projects from github each of which has at least minutes testing time including camel core chukwa commons pool hbase mapdb opentripplanner and php which have been used in other testing debugging topics .
table 1presents the basic information of all the subjects.
as the execution time of a test may vary slightly in di erent executions we ran each test times and used the average execution time.
our study is conducted on a workstation with eight core intel xeon e5620 cpu .4ghz 24g memory and ubuntu .
.
.
.
.
faults.
as the existing studies have demonstrated mutation faults to be suitable for software testing experimentation same as prior work we used mutation faults in our study since it is quite challenging to nd a large number of real regression faults .
following prior work for each subject we rst generated all mutants i.e.
mutation faults and randomly selected mutation faults each of which can be detected by at least one test.
then we constructed mutation groups each of 657optimizing test prioritization via test distribution analysis esec fse november lake buena vista fl usa table open source subjects from github id subjects sloc tloc test time s assertj core .
asterisk java .
camel core .
chukwa .
commons pool .
gson re .
hbase .
jasmine maven plugin .
java apns .
lastcalc .
mapdb .
opentripplanner .
php .
vraptor archive .
blue ood .
cloudhopper smpp .
commons dbcp .
commons email .
commons math .
commons io .
cucumber reporting .
ddd cqrs sample .
dictomaton .
dotci .
ews java api .
exp4j .
gdx artemis .
geo .
geohash java .
hivemall .
http proxy servlet .
jackson core .
jackson datatype guava .
jopt simple .
joss .
jsprit .
la4j .
lambdaj .
languagetool .
metrics core .
raml java parser .
redline smalltalk .
roaringbitmap .
rome .
scribe java .
spring data solr .
spring retry .
stream lib .
tamper .
webbit .
total .
columns present the lines of source code lines of test code number of tests and the testing time in seconds i.e.
the time spent on executing the whole test suite .
which contains randomly selected mutation faults.
that is we created faulty versions for each subject each version contains mutation faults .
if the total number of mutation faults is less than the number of mutant groups is less than .
following prior work we used pit and mutgen to generate mutation faults for java and c projects respectively.
.
.
studied test prioritization techniques.
considering the exclusion inclusion of testing costs we classify the existing prioritization techniques into two types cost unaware and cost aware techniques.
besides we implemented a cost only technique which prioritizes tests only based on their costs.
that is we implemented three types of prioritization techniques abbreviated as unaware aware and only in the gures and tables in total.
as the mostly studied testing costs are time spent on running each test abbreviated as testing time in this paper we use testing time to represent testing costs.
cost unaware test prioritization refers to the prioritization techniques without balancing testing time and other factors e.g.
test coverage in prioritization including the following techniques.
total and additional prioritization are both greedy algorithms .
the total technique prioritizes tests based on the descending order of the number of program elements e.g.
statements covered by the tests whereas the additional technique prioritizes tests based on the number of program elements that are uncovered by already selected tests but covered by these unselected tests.
although conceptually simple the additional technique has been widely recognized as a state of the art technique .
search based prioritization regards all the permutations of a test suite as candidate solutions and uses some heuristics to guide the process of searching for a better execution ordering of tests.
as the genetic algorithm is evaluated to be e ective in test prioritization we use it as the representative in search based prioritization.
it rst randomly generates a set of permutations and then forms the next generation through crossover andmutation operations.
for crossover operation each pair of permutations in the population is regarded as parent permutations to generate two o spring permutations through crossover on a random position.
for mutation operation it randomly selects two tests and exchanges their positions for each o spring permutation.
same as prior work for the genetic algorithm we set the size of population to be the number of generations to be and the probabilities for crossover and mutation to be .
and .
.
adaptive random prioritization is proposed to prioritize tests based on their diversity .
it de nes a test set distance to determine which test is selected next during prioritization.
here we use thetest set distance that is de ned to select a test that has the largest minimum distance with the already selected tests as the representative since it is evaluated to be more e ective and e cient among all the proposed test set distances .
we abbreviate the four cost unaware techniques the total additional search based and adaptive random prioritization as tot add sea and arp in the gures and tables.
cost aware test prioritization balances testing time and other factors in test prioritization.
in this study we use cost cognizant additional prioritization which is a typical cost aware technique.
it is rstly proposed to leverage testing time and fault severities in test prioritization .
however as fault severities are rarely available in practice epitropakis et al.
adapted this technique by ignoring fault severities.
although epitropakis et al.
proposed another cost aware technique in our paper we used only the adapted cost cognizant additional technique because another technique requires the fault history which is hard to collect in practice and both techniques are shown to represent state of the art cost aware prioritization in terms of apfd c .
more speci cally this adapted cost cognizant additional technique prioritizes tests based on the number of program elements uncovered by already selected tests but covered by these unselected tests per unit time .
cost only test prioritization schedules the execution order of tests based on testing time alone ignoring other factors including test coverage .
more speci cally the cost only technique prioritizes tests based on the ascending order of the execution time of each test.
in this paper this technique is treated as the baseline.
658esec fse november lake buena vista fl usa j. chen y. lou l. zhang j. zhou x. wang d. hao l. zhang table test prioritization comparison in terms of apfd idstatement method unawareaware onlyunawareaware onlytot add sea arp tot add sea arp .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
each row presents the average apfd values of all mutation groups for each subject and the cells marked with the shading represent the highest apfd values among all the studied techniques on each subject.
.
.
independent and dependent variables.
in our study we used two widely used coverage criteria i.e.
statement coverage and method coverage in test prioritization.
we used clover and gcov to collect test coverage i.e.
statement method coverage for java and c projects respectively.
to measure the e ectiveness of prioritization techniques we used both apfd and apfd c. the formula apf d tf1 tf2 ... tfm n m 2n presents the apfd value calculation for a project with ntests and mfaults.
note that tfiis the ranking of the rst test in the prioritized test suite that detects the ith fault.
apfd cis extended from apfd by considering both fault severities and test execution cost .
as it is hard to acquire the fault severities for each project in practice following the prior work we simplify apfd cby regarding all the faults have the same severity shown in apf dc pm i pn j tfitj 2ttfi pn j 1tj m where tjrepresents the execution time of the jth test .
.
results and analysis .
.
antitative analysis.
in this section we quantitatively evaluated the studied test prioritization techniques in terms of both apfd and apfd c. table anova analysis and tukey s hsd test of test prioritization techniques in terms of apfd idstatement method unawareaware onlyunawareaware onlytot add sea arp tot add sea arp bacb c a d babb a c daa c b e c a ab b b d cabb b d cabb ab d c a ab b b d baaa a b caab b d caab ab d caa b c b d ab a a ab b c cabc b d cabc b d caab a d caab a d caab b d baaa a b ab a a b c d aaaa b c dacb c b d caba ab c baa c d e baa c c d ebda c f ebda c f caab a d c a ab b a d ameans the group with signi cantly best e ectiveness.
apfd results.
table 2shows that the comparison results of prioritization techniques in terms of apfd.
from this table cost unaware prioritization performs the best in terms of apfd for almost all of subjects and both coverage criteria.
moreover the additional technique outperforms the other three cost unaware techniques in most cases which is consistent with prior studies .
to investigate whether these techniques have signi cant differences in terms of apfd we performed one way anova analysis at the signi cance level of .
.
in addition we performed tukey hsd post hoc test to rank the e ectiveness of these techniques as di erent groups.
table 3shows the analysis results.
alltable test prioritization comparison in terms of apfdc idstatement method unawareaware onlyunawareaware onlytot add sea arp tot add sea arp .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table anova analysis and tukey s hsd test of test prioritization techniques in terms of apfdc idstatement method unawareaware onlyunawareaware onlytot add sea arp tot add sea arp bacb c a d babb a c dbc b a a fdec b a dbcb c a a cbbb a a dccb a a dcbb a a fdec b a ed b cc b a bc bc c b a a ccdc b a ebdc a a dbc c a a caab a d caab a c dccb a a dccb a a cb cb d a a bab c a a edbc b a fecd b a db b cc a a ebcd a a caca b d ebda c f d c ab e a bc cbbd a b p values in the one way anova analysis are much less than .
which means that these techniques have signi cant di erences on each subject.
on the whole at least one cost unaware technique signi cantly performs better than cost aware prioritization in most cases.
moreover cost unaware prioritization always signi cantly outperforms cost only prioritization.
that is cost unaware test prioritization is the most e ective in terms of apfd.
among the four cost unaware techniques consistent with prior work the total technique performs the worst in most cases while the additional technique performs the best in most cases.
apfd cresults.
we compared these techniques in terms of apfd c whose results are shown in table .
the distribution of shadings in this table is totally di erent from that in table .
from table each of the three types of test prioritization can perform the best in some cases even the minimalist cost only technique performs rather well on some subjects while other techniques do not perform well.
for example when using method coverage costunaware cost aware and cost only test prioritization performs the best on subjects respectively.
among the four cost unaware techniques the additional technique does not have obvious advantages compared with the others but the arp technique is competitive with the additional technique in terms of apfd c. we suspect the reason to be that all the techniques except arp tend to execute earlier the tests with higher coverage which usually cost more time leading to delayed fault detection in terms of apfd c. to con rm our ndings we also performed one way anova analysis andtukey hsd post hoc test whose results are shown in table .
all p values are much less than .
indicating these techniques also have signi cant di erences in terms of apfd con each subject.
however di erent from the apfd results in table there is not any type of test prioritization that clearly outperforms the others on the whole.
moreover among the four cost unaware techniques no technique dominantly performs better than the others.
659optimizing test prioritization via test distribution analysis esec fse november lake buena vista fl usa overall we get the following ndings from this study the cost cognizant apfd cmeasurement has di erent trends compared with the widely used apfd indicating that apfd cinstead of apfd should be used in test prioritization research.
no type of test prioritization always performs better than the others in terms of apfd c. surprisingly even the cost only technique can outperform other techniques in some cases.
.
.
alitative analysis.
in this section we qualitatively analyzed in depth reasons for prioritization e ectiveness.
since different types of techniques tend to have di erent performance for di erent subjects we suspect one major di erence among the three types of techniques the prioritization criteria i.e.
test coverage testing time and coverage per unit time may impact prioritization e ectiveness.
therefore we analyzed the distribution of test coverage testing time and coverage per unit time.
here we analyzed the test distribution for six representative subjects because the others have similar conclusions with at least one of these subjects shown by figure .
we used statement coverage as the representative.
the x axis represents the percentage of tests where we rank the tests based on the ascending order of the values of testing time test coverage and coverage per unit time respectively.
the y axis represents the logarithm of testing time test coverage or coverage per unit time.
we deal with the exponent by transforming them to be larger than one to make the logarithm values always positive.
based on table the cost only technique performs optimal for commons pool id is and lastcalc id is the cost unaware technique performs optimal for assertj core id is and php id is and the cost aware technique performs optimal for gson re id is and java apns id is .
from figure the subjects with the same conclusions tend to have similar distribution of test coverage testing time and coverage per unit time which con rms our hypothesis.
for example for assertj core andphp the testing time of most tests is quite close and the testing time of only a small number of tests is largely di erent from others and thus the cost only technique performs the worst.
for commons pool and lastcalc the distribution of testing time is more uneven than that of test coverage and coverage per unit time and thus the advantage of the cost only technique becomes more distinct.
that is these distribution patterns make some technique perform better or worse.
the distribution of test coverage testing time and coverage per unit time has important impacts on the determination of which prioritization technique the cost unaware cost aware and costonly techniques should be applied to a speci c project in practice.
predictive test prioritization based on the above ndings projects with similar distribution of test coverage testing time and coverage per unit time tend to have the same optimal prioritization technique.
that is based on the distribution of existing projects it is possible to build a predictive model to predict the optimal prioritization technique for a new project in order to achieve the fastest fault detection in practice.
based on this insight we propose the rst predictive test prioritization approach abbreviated as ptp which predicts theoptimal prioritization technique for a speci c project in advance via machine learning described in section .
and then evaluate the e ectiveness of ptp described in section .2and section .
.
.
ptp approach figure 2shows the overview of ptp.
in general ptp builds a predictive model to predict the optimal prioritization technique for any given project in practice based on various test distribution features such as the distribution of test coverage testing time and coverage per unit time.
in the training process ptp collects the test distribution features and label information e.g.
which prioritization technique performs optimal for each training project and performs feature normalization and over sampling to build the predictive model.
then given any new project ptp can predict its optimal test prioritization technique based on its test distribution features.
note that it is impossible to collect the test distribution features without running the project.
therefore following all the existing work in test prioritization ptp uses the test distribution information of the previous version instead.
that is ptp applies the predictive model on the speci c project by using the distribution information of a previous version.
we next describe the details for building the ptp predictive model .
.
feature collection.
since the qualitative analysis in section .
.
shows that the optimal prioritization technique for a speci c project is related to its distribution of test coverage testing time and coverage per unit time ptp regards these information as the features of a predictive model.
that is for each project with a test suite we extract the three group of features the number of program elements covered by each test the testing time of each test and the number of program elements covered by each test per unit time.
although coverage per unit time can be calculated based on test coverage and testing time we still use its distribution information as one type of features since it can facilitate the machine learning process and the three groups of features directly map to the three types of prioritization techniques.
for each group of features for an instance i.e.
a project with a test suite ptp ranks them based on the ascending order of their values.
figure 3shows an example of extracting features for an instance using a project with tests t1 t2 t3 .
we rst collect test coverage c testing time t and coverage per unit time c t for each test shown in figure a and then rank the values of c t and c tof all tests in ascending order respectively shown in figure b .
finally the features of this instance are shown in figure c .
since di erent projects tend to have di erent number of tests that cause the unaligned issue of features ptp applies zero padding for test coverage testing time and coverage per unit time respectively to obtain the same number of features.
another bene t of zero padding is that such features also consider the impacts of the number of tests on test prioritization since the number of tests i.e.
test suite size may also be an important factor for test distributions.
.
.
instance labeling.
the label for each training instance of ptp is which prioritization technique performs optimal for a project.
we use the apfd cas the measurement of prioritization techniques since it is a more practical measurement than apfd and compare three types of techniques including the cost unaware cost aware and cost only techniques.
here we use the additional technique as the representative of cost unaware techniques due to the following 660esec fse november lake buena vista fl usa j. chen y. lou l. zhang j. zhou x. wang d. hao l. zhang .
.
.
.
.00commons poollog time .
.
.
.
.00lastcalclog time .
.
.
.
.00assertj corelog time .
.
.
.
.00phplog time .
.
.
.
.00gson firelog time .
.
.
.
.00java apnslog time a testing time .
.
.
.
.00commons poollog cov .
.
.
.
.00lastcalclog cov .
.
.
.
.00assertj corelog cov .
.
.
.
.00phplog cov .
.
.
.
.
.
.
.
.
.
.00gson firelog cov .
.
.
.
.00java apnslog cov b test coverage .
.
.
.
.00commons poollog covpertime .
.
.
.
.00lastcalclog covpertime .
.
.
.
.00assertj corelog covpertime .
.
.
.
.
.
.
.
.
.00phplog covpertime .
.
.
.
.00gson firelog covpertime .
.
.
.
.00java apnslog covpertime c coverage per unit time figure distribution of testing time test coverage and coverage per unit time training datafeaturecollectionnormalizationinstance labeling training set model trainingtesting datafeature collectionoptimal testprioritization normalization over samplingfigure overview of ptp t1t2t3ctc t f1f2f3f4f5f6f7f8f9 a collected data b ranking c features of an instancefigure an example of feature collection two reasons the cost aware technique studied in this paper is adapted from the traditional additional technique by epitropakis et al.
and when cost unaware test prioritization outperforms cost aware test prioritization and cost only test prioritization at least the additional technique will outperform them based on the apfd cresults in section .
.
.
therefore when collecting the label for an instance ptp compares the average apfd cvalues on all mutation groups for the three techniques and treats the technique with the highest average apfd cas the label.
.
.
predictive model training.
since di erent training instances tend to have di erent value ranges for test coverage testing time and coverage per unit time ptp normalizes the three types of feature values for each training instance into the range using min max normalization respectively.
that is ptp adjusts values measured on di erent scales into a common scale.
for example supposed the set of values for the coverage feature extracted from a training instance is denoted as x x x ... x n and the the normalized instance is denoted as x i.e.
value of x i after normalization is denoted as x i where i n. formula 1shows the min max normalization on x i .
besides ptp uses the oversampling strategy i.e.
resampling the minority class to deal with the imbalanced data problem following the existing work .
x i x i min x k k n max x k k n min x k k n based on the set of processed training data ptp trains a predictive model via machine learning which is used to predict which technique performs optimal for a speci c project in practice.
the test coverage testing time and coverage per unit time of each test are regarded as features but such basic features are hard to predict the optimal prioritization technique.
based on the observations in the study the high level distribution patterns constructed from these basic features are more directly related to the optimal prioritization.
the recently proposed xgboost algorithm is an optimized distributed gradient boosting learning algorithm that can e ectively learn high level features constructed from basic features using tree ensemble models which is suitable to our problem.
therefore in ptp we adopt it to learn the high level patterns.
also xgboost supports to automatically do parallel computation to improve e ciency.
note that we also investigate the impact of di erent machine learning algorithms in section .
.
.
ptp evaluation design we conducted an extensive study to evaluate ptp.
in particular this study shares the similar experimental setup as section .
and we present the di erences between them as follows.
.
.
subjects.
to make results more generalizable in this study we scale up our subjects to open source projects from github shown in table .
despite that we just have original test suites for the subjects the number of prioritization runs is also which is small in number for machine learning.
to possess enough instances for ptp we constructed test suites for each subject by randomly selecting a subset of tests from its original test suite.
each constructed test suite is used as an instance where test prioritization applies.
in this way we constructed such instances in total.
besides each original test suite can also be used as an instance and thus we have instances in total.
as each subject has about mutant groups for each instance we calculated the average apfd cvalues of all the mutant groups as the measurement of the corresponding technique and labeled the prioritization technique with the largest average apfd cvalues.
following the existing work on machine learning we used the leave one out crossvalidation to evaluate ptp.
that is for each subject we used all the instances from remaining subjects as the training data to predict 661optimizing test prioritization via test distribution analysis esec fse november lake buena vista fl usa the optimal prioritization technique for the target subject with its original test suite .
.
.
measurements.
from our industry partners they tend to care di erent metrics based on di erent requirements.
for example they care the time spent on detecting the rst regression fault for starting debugging and releasing resources earlier.
to su ciently evaluate the practical e ectiveness of ptp besides apfd c we used other three practical time based measurements in milliseconds whose importance is con rmed by our industry partners ft the time spent on detecting the rst regression fault .
lt the time spent on detecting the last regression fault.
at the average time spent on detecting all regression faults.
even though both at and apfd cmeasure the e ectiveness of test prioritization by considering all detected faults they have di erent computations and can have di erent results.
.
.
implementation.
we used the xgboost approach implemented by the xgboost python package to build the predictive model choosing the softmax objective due to the multiclass classi cation problem.
based on a preliminary study that we conducted on a small dataset we set eta .
max depth silent and num round and other parameters to be default values.
we investigate the impact of parameters in section .
.
in this study we used statement coverage in ptp since it is usually more e ective than others which is also con rmed by our study in section .
.
results and analysis overall e ectiveness.
based on experimental results ptp correctly predicts the optimal technique for of subjects except assertj core asterisk jave geohash java and spring data solr indicating extremely high accuracy i.e.
.
table 6shows the comparison results of ptp with the other three techniques.
row avg presents the average e ectiveness of all subjects and row imp presents the average improvement of ptp compared with the other techniques.
from the last two rows ptp outperforms all the three techniques in terms of all timebased measurements.
surprisingly the improved rate of ptp for ft ranges from .
to .
compared with all existing state ofthe art techniques.
in practice ft is usually the most important measurement in industry because developers tend to start debugging immediately after the rst test failure.
that further con rms the practical value of ptp.
moreover the improved rates of ptp for at and lt are also high ranging from .
to .
and .
to .
respectively demonstrating the practical e ectiveness of ptp from various angles.
the improved rate of ptp for apfd cis smaller than other measurements.
the reason is that apfd chas the value range of making apfd cvalues of di erent techniques tend to be close.
parameter evaluation.
we investigate the impact of main parameters in ptp i.e.
max depth andnum round for xgboost .
the former represents the maximum depth of a tree and the latter represents the number of rounds for boosting.
figure 4shows the number of subjects whose optimal prioritization technique is correctly predicted by ptp when changing each parameter alone.
from this gure regardless of parameter values the number of subjects predicted correctly is always close to the total number of subjects i.e.
demonstrating the stable e ectiveness of ptp.
345678number of correct prediction a max depth 100200300number of correct prediction b num round figure impact of parameters on ptp learning algorithm impacts.
we investigate the impacts of machine learning algorithms.
besides xgboost we use one widelyused baseline algorithm i.e.
random guess rg and another ten typical classi cation algorithms including multivariate bernoulli naive bayes bnb multi layer perceptron mlp support vector machine svm logistic regression lr ridge regression rr k nearest neighbors knn random forest rf extra trees et linear discriminant analysis lda and quadratic discriminant analysis qda .
in particular we used their implementations provided by scikit learn package in python with their default settings.
figure 5shows the comparison results using di erent machine learning algorithms in terms of e ectiveness i.e.
the number of subjects predicted correctly.
from this gure all the machine learning algorithms perform signi cantly better than the baseline rg indicating the promising direction of using machine learning to predict the optimal test prioritization technique.
besides xgboost performs better than all other machine learning algorithms demonstrating the power of xgboost to predict the optimal test prioritization technique in ptp.
the three tree based algorithms xgboost rf and et rank at top because these tree based algorithms are ensemble learning algorithms that integrate multiple decision trees.
that indicates our features can be modeled well by tree structures.
xgboostbnbetknnldalrmlpqdarfrrsvmrgnumber of correct predictionfigure e ectiveness of machine learning algorithms table 7shows the comparison results using di erent learning algorithms in terms of e ciency milliseconds including the average time spent on o line training a predictive model and the average time spent on online prediction for each subject.
we do not show the cost of random guess because this algorithm has almost no cost.
from this table the o ine training time of xgboost is larger than many other learning algorithms but it is still acceptable i.e.
about .
minutes.
moreover the training process is conducted o ine and thus its cost can be ignored.
on the other hand for all the machine learning algorithms their online predicting time is negligible i.e.
in milliseconds demonstrating the practical feasibility of ptp.
table e ciency of machine learning algorithms ms stage xgboost bnb et knn lda lr mlp qda rf rr svm train predict overall learning algorithms indeed have impacts on ptp mostly demonstrated by its e ectiveness.
the tree based learning algorithms especially xgboost perform better in terms of e ectiveness.
regarding to practical usage xgboost is a better choice for ptp.
662esec fse november lake buena vista fl usa j. chen y. lou l. zhang j. zhou x. wang d. hao l. zhang table e ectiveness of ptp compared with the other three techniques on open source subjects idapfd c ft ms lt ms at ms ptp unaware aware only ptp unaware aware only ptp unaware aware only ptp unaware aware only .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
avg .
.
.
.
imp .
.
.
.
.
.
.
.
.
.
.
.
in summary ptp e ectively predicts the optimal test prioritization technique advancing the practical usage of test prioritization to a large extent.
with this approach practitioners can always have close to optimal prioritization results signi cantly improving regression testing e cacy in practice.
industry application of ptp recently ptp has been integrated into the practical testing infrastructure for baidu a famous search service provider with over 600m monthly active users.
projects in baidu have considerable regression testing costs due to the following reasons.
first the projects and tests in baidu are large scale.
second the regression testing process is conducted once a change is submitted according to the continuous integration policy of baidu and there is high change frequency.
for example one project in baidu has about commits per day.
moreover even though this company has an abundanceof resources the resources are still limited relative to such considerable costs.
therefore more testing optimizations are in demand.
our ptp approach aims to achieve optimal test prioritization to detect faults earlier which admirably serves their needs of quicker test feedback and less computing resource consumption and thus has been integrated into baidu s practical testing infrastructure for faster fault detection.
ptp does achieve great e ectiveness in the practical usage in baidu.
here we report all the results of ptp on the projects of baidu collected until feb. including industrial subjects totaling over million of lines of source code nearly 30k tests and over hours of testing time.
for each subject we have a set of real regression faults on real faulty versions to evaluate the e ectiveness of ptp.
in particular for these industrial subjects we collected the real regression faults during practical testing from dec. to feb. i.e.
faulty versions with real regression faults in total.
table 8shows the basic information of these industrial 663optimizing test prioritization via test distribution analysis esec fse november lake buena vista fl usa table industrial subjects from baidu id sloc test time ms fv faults i1 200k i2 200k i3 200k i4 500k i5 500k i6 500k i7 500k i8 500k i9 20k i10 20k i11 20k the last two columns present the number of faulty versions and the number of real regression faults.
due to the policy of baidu we hide project names and report the rough scale of sloc.
subjects and faults.
in particular for each industrial subject we used the predictive model trained based on the data from the open source projects used in our work to predict the optimal prioritization technique and then determined an execution order of tests through the predicted optimal test prioritization.
this is a practical application scenario of learning based techniques i.e.
leveraging su cient open source data to facilitate industrial usage.
based on the results on industrial subjects ptp correctly predicts the optimal prioritization technique for of subjects demonstrating the accuracy of .
.
table 9shows the practical e ectiveness of ptp on industrial subjects.
similar to ndings on open source subjects ptp outperforms all the three techniques in terms of all time based measurements on industrial subjects.
the improved rates of ptp for ft range from .
to .
compared with the three state of the art techniques demonstrating its practical value in industry.
also the improved rates of ptp for at and lt range from .
to .
and from .
to .
.
besides ptp has improvements in terms of apfd c ranging from .
to .
.
even though ptp also achieves great improvements on industrial subjects we nd that the improvements are smaller than those on open source subjects.
one possible reason is that we used mutation faults in open source subjects while used real faults in industrial subjects.
that also re ects that test prioritization techniques perform a bit di erent on industrial subjects with real faults and open source subjects with mutation faults indicating the necessity of using both kinds of subjects to evaluate test prioritization techniques.
such results indicate that ptp not only achieves signi cant effectiveness on open source subjects but also performs great on industrial subjects with real regression faults.
in particular after applying ptp to baidu we received positive feedbacks from the testing team of this company according to the practical usage .
.
.ptp saves beyond 2x testing costs for the anonymous projects with negligible overheads.
...ptp has been successfully integrated into the practical testing of the anonymous projects ... threats to validity the threats to external validity mainly lie in the subjects and faults.
although these subjects may not su ciently represent other subjects we have already used the relatively large number of subjects open source subjects from github among existing prioritization studies.
regarding to faults used in the studies we used mutation faults for open source subjects because they are evaluated to be suitable for software testing experimentation and are widely used in test prioritization research .
prior work discussed the threat from mutations in the future we will reduce this threat accordingly.
in this work to reduce these threats we also report the results of ptp on industrial subjects with real regression faults i.e.
industrial subjects with real regression faults.
here following prior work we regard each failure as each fault in industrial subjects since it is hard to distinguish whether di erent failures are caused by the same fault and many faulty versions in the study have only one failure.
the threats to construct validity mainly lie in the regression scenario instance collection and measurement.
in the studies on open source subjects we regard the version without faults as the former version and the version with faults as the current version following test prioritization literature .
to reduce this threat our industry application of ptp uses the real regression scenario.
we do not consider test evolutions here and in the future we will consider it to further reduce this threat.
in particular the ptp trained model is based on the traditional general test prioritization which are designed to work for a series of subsequent versions.
two independent recent studies both demonstrated that software changes do not impact the e ectiveness of general test prioritization much.
in the evaluation of ptp to possess enough instances we randomly constructed test suites besides using the original test suites.
however even based on the set of randomly constructed test suites and original test suites our study demonstrates that ptp can predict the optimal technique for each project with the original test suite.
considering the di erence between constructed test suites and original test suites we will repeat the experiment by using more real instances.
in the evaluation of ptp we used timebased measurements for test prioritization because ptp can also be viewed as a type of prioritization technique.
more discussion on their di erence is given by section .
however strictly speaking ptp is a prediction technique and thus we will further measure its e ectiveness by using more measurements in machine learning.
discussion practical implications.
time plays a non trivial role in practical test prioritization demonstrated from at least two aspects prioritization algorithms and measurements.
regarding to prioritization algorithms our study shows that the cost only technique performs surprisingly well on some subjects e.g.
mapdb .
that is considering the cost of coverage collection the cost only technique may be a good candidate prioritization technique in practice.
regarding to measurements our study shows that comparison results on apfd are much di erent from apfd c indicating the importance of practical factors e.g.
testing time in test prioritization.
new perspective from ptp.
our work provides a new perspective for test prioritization problem.
in the past the existing work solves this problem mainly through proposing new prioritization techniques hoping they always outperform the others.
however due to its inherent di culties it is quite hard to gure out an outstanding technique which always produces the best prioritization results.
instead our work admits the advantages of existing prioritization techniques and aims at generating the optimal prioritization results for each project through the selection of prioritization techniques.
that is ptp opens an entire new dimension di erent from the current research direction of test prioritization.
664esec fse november lake buena vista fl usa j. chen y. lou l. zhang j. zhou x. wang d. hao l. zhang table e ectiveness of ptp compared with the other three techniques on industrial subjects idapfd c ft ms lt ms at ms ptp unaware aware only ptp unaware aware only ptp unaware aware only ptp unaware aware only i1 .
.
.
.
i2 .
.
.
.
i3 .
.
.
.
i4 .
.
.
.
i5 .
.
.
.
i6 .
.
.
.
i7 .
.
.
.
i8 .
.
.
.
i9 .
.
.
.
i10 .
.
.
.
i11 .
.
.
.
avg .
.
.
.
imp .
.
.
.
.
.
.
.
.
.
.
.
practical usage of ptp.
in the evaluation of ptp on open source subjects we used the widely used leave one out cross validation described in section .
.
and our results demonstrate the great e ectiveness of ptp.
di erently in the industry application of ptp we used the model trained based on the data of the opensource projects to predict the optimal prioritization technique for industrial subjects directly and the results further demonstrate the practical e ectiveness of ptp.
that indicates that ptp can provide generally usable models for its later practical usage demonstrating the stability and applicability of ptp.
future extensions of ptp.
for predictive test prioritization there are a lot of future extensions.
first based on figure the problem can be transformed to a pattern recognition problem i.e.
image recognition since the similar distribution images tend to have the same prediction conclusions.
in the future we will collect the distribution images from github and improve ptp by applying learning or even deep learning techniques to these images.
second besides the used distributions some other factors may also in uence prioritization technique selection e.g.
the degree of coverage overlapping.
in the future we will leverage such information to improve ptp.
third ptp uses apfd cvalues to label instances but other time based measurements like ft lt and at can also be used as labels to guide the selection depending on developers speci c requirements.
in the future we will extend ptp using various measurements to label instances and evaluate their e ectiveness.
related work as our work investigates the selection of test prioritization techniques through an empirical study and a novel machine learning based approach we summarize the existing work into two parts test prioritization techniques and empirical studies.
prioritization techniques.
most of the existing prioritization techniques belong to the cost unaware techniques e.g.
the four cost unaware techniques studied in this paper.
besides time aware test prioritization also belongs to this category.
slightly di erent from the preceding cost unaware techniques timeaware techniques focus on prioritizing tests satisfying the time constraint.
however these techniques do not deal with the balance between testing costs and other factors in test prioritization and thus do not belong to cost aware techniques.
for cost aware techniques epitropakis et al.
proposed a cost aware multi objective prioritization technique and compared its performance with the cost cognizant additional greedy technique .
chen et al.
proposed a learning based approach to prioritizing tests for compilers based on the predicted bug revealing probability per unit time for each test.
busjaeger et al.
proposeda learning based prioritization technique which prioritizes tests by assigning each test an aggregated score through machine learning.
di erent from them our work does not present a prioritization algorithm but aims at selecting the best performance technique among existing ones for a speci c project.
empirical studies.
most of existing empirical studies compared the performance among cost unaware techniques and they usually used the apfd measurement .
for example hao et al.
conducted an empirical study to compare existing coverage based prioritization techniques in terms of apfd and found that the additional technique even outperforms the optimal coverage based prioritization technique which performs the best in terms of coverage i.e.
apxc rather than fault detection i.e.
apfd .
besides several empirical studies investigated the performance of cost aware techniques.
in summary the existing empirical studies investigated only either the performance of costunaware techniques or the performance of cost aware techniques.
that is to our best knowledge none of the existing work compares the performance of cost unaware and cost aware techniques together in terms of practical e ectiveness and this paper is the rst one to compare both cost unaware and cost aware techniques in terms of actual regression testing time.
conclusion despite the large body of research on test prioritization the optimal test prioritization technique in practice still remains unknown.
to answer it this paper performs the rst study to compare various prioritization techniques in terms of both apfd and apfd c. the study results reveal a number of practical guidelines and show that actually no existing technique can always perform the best.
our quantitative and qualitative analyses further show that the distribution of test coverage testing time and coverage per unit time can serve as the guidelines for selecting the optimal prioritization technique for di erent cases.
based on the ndings we design a predictive test prioritization approach ptp which can predict the optimal prioritization technique for a given project based on its test distribution in prior versions.
the experimental results show that ptp outperforms the studied techniques signi cantly e.g.
by .
to .
in detecting the rst regression fault.
furthermore ptp has been integrated into the testing infrastructure of baidu demonstrating .
to .
improvement in detecting the rst regression fault compared with state of the art techniques on real regression faults .
the data in this work except the data on industrial projects due to the policy of the company are available at our project website .
665optimizing test prioritization via test distribution analysis esec fse november lake buena vista fl usa