local based active classification of test report to assist crowdsourced testing junjie wang1 song wang4 qiang cui1 qing wang1 1laboratory for internet software technologies 2state key laboratory of computer science institute of software chinese academy of sciences beijing china 3university of chinese academy of sciences beijing china 4electrical and computer engineering university of waterloo canada wangjunjie cuiqiang wq itechs.iscas.ac.cn song.wang uwaterloo.ca abstract in crowdsourced testing an important task is to identify the test reports that actually reveal fault true fault from the large number of test reports submitted by crowd workers.
most existing approaches towards this problem utilized supervised machine learning techniques which often require users to manually label a large amount of training data.
such process is time consuming and labor intensive.
thus reducing the onerous burden of manual labeling while still being able to achieve good performance is crucial.
active learning is one potential technique to address this challenge which aims at training a good classi er with as few labeled data as possible.
nevertheless our observation on real industrial data reveals that existing active learning approaches generate poor and unstable performances on crowdsourced testing data.
we analyze the deep reason and nd that the dataset has signi cant local biases.
to address the above problems we propose local based active classi fication loaf to classify true fault from crowdsourced test reports.
loaf recommends a small portion of instances which are most informative within local neighborhood and asks user their labels then learns classiers based on local neighborhood.
our evaluation on test reports of commercial projects from one of the chinese largest crowdsourced testing platforms shows that our proposed loaf can generate promising results.
in addition its performance is even better than existing supervised learning approaches which built on large amounts of labelled historical data.
moreover we also implement our approach and evaluate its usefulness using real world case studies.
the feedbacks from testers demonstrate its practical value.
ccs concepts software and its engineering !software testing and debugging corresponding author.keywords crowdsourced testing test report classi cation active learning .
introduction crowdsourced testing is an emerging trend in both the software engineering community and industrial practice.
in crowdsourced testing crowd workers are required to submit test reports after performing testing tasks in crowdsourced platform .
a typical test report contains description screenshots and an assessment as to whether the worker believed that the software behaved correctly i.e.
passed or behaved incorrectly i.e.
failed .
in order to attract workers testing tasks are often nancially compensated especially for these failed reports.
under this context workers may submit thousands of test reports.
however these test reports often have many false positives i.e.
a test report marked as failed that actually involves correct behavior or behavior that was considered outside of the studied software system.
project managers or testers need to manually verify whether a submitted test report reveals fault true fault as demonstrated in table .
however such process is tedious and cumbersome given the high volume workload.
our observation on one of the chinese largest crowdsourced testing platforms shows that approximately projects are delivered per month from this platform and more than test reports are submitted per day on average.
inspecting reports usually takes almost half a working week for a tester.
thus automatically classifying true fault from the large amounts of test reports would signi cantly facilitate this process.
several existing researches have been proposed to classify issue reports of open source projects using supervised machine learning algorithms .
unfortunately these approaches often require users to manually label a large number of training data which is both time consuming and labor intensive in practice.
therefore it is crucial to reduce the onerous burden of manual labeling while still being able to achieve good performance.
in this paper we try to adopt active learning to mitigate this challenge.
active learning aims to achieve high accuracy with as few labeled instances as possible.
it recommends a small portion of instances which are most informative and asks user their labels.
these labeled reports are then used to train a model to classify the remaining reports.
however our experiments reveal that existing active learning techniques generate poor and unstable performances on permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
these crowdsourced testing data details are in section .
.
we further analyze the deep reason and nd that previous active learning techniques assumed data are identically distributed .
however crowdsourced test reports often have signi cant local bias i.e.
the contained technical terms may be shared by only a subset of reports rather than all the reports details are in section .
.
to address the local bias and more e ectively classify the crowdsourced reports we proposed local based active classification approach loaf .
the idea behind is to query the most informative instance within local neighborhood then learn classi ers based on local neighborhood details are in section .
.
we experimentally investigate the e ectiveness and advantages of loaf on test reports of commercial projects from one of the chinese largest crowdsourced testing platforms.
results show that loaf can achieve .
accuracy with the e ort of labeling reports on median.
it outperforms both existing active learning techniques and supervised learning techniques which built on large amounts of historical labeled data.
in addition we implement our approach1 conduct a case study and a survey to further evaluate the usefulness of loaf.
feedback shows that testers agree with the usefulness of loaf and would like to use it in real practice of crowdsourced testing.
these results imply that when historical labeled data are not available our approach can still facilitate the report classi cation with high accuracy and little e ort.
this can reduce the e ort for manually inspecting the reports or collecting large number of high quality labeled data.
this paper makes the following contributions we propose local based active classi fication loaf to address the two challenges in automating crowdsourced test reports classi cation i.e.
local bias problem and lacking of labeled data.
to the best of our knowledge this is the rst work to address these two problems of automating test report classi cation in real industrial crowdsourced testing practice.
we evaluate our approach on test reports of projects from one of the chinese largest crowdsourced testing platforms and results are promising.
we implement our approach and evaluate its usefulness using real world case studies.
the rest of this paper are organized as follows.
section describes the basic background of this study.
section discusses the design of our proposed approach.
section shows the setup of our experimental evaluation.
section presents the results of our research questions.
section discloses the threats to the validity of this work.
section surveys related work.
finally we summarize this paper in section .
.
background .
crowdsourced testing in this section we describe the background of crowdsourced testing to help better understand the challenges we meet in real industrial crowdsourced testing practice.
1the implementation of loaf are available at ac.cn cn membershomepage wangjunjie wangjunjie loaf.zip.
figure the procedure of crowdsourced testing table an example of crowdsourced test report attribute description example environment phone type samsung sn9009 operating system android .
.
rom information kot49h.n9009 network environment wifi crowd workerid location beijing haidian district testing task id name incognito mode input and operation stepsinput sina.com.cn in the browser then click the rst news.
select setting and then set incognito mode .
click the second news in the website.
select setting and then select history .
result description incognito mode does not work as expected.
the rst news which should be recorded does not appear in history .
screenshot assessment passed or failed given by crowd worker failed our experiment is conducted with baidu crowdsourced testing platform2.
the general procedure of such crowdsourced testing platform is shown in figure .
in general testers in baidu prepare packages for crowdsourced testing software under test and testing tasks and distribute them online using their crowdsourced testing platform.
then crowd workers could sign in to conduct the tasks and are required to submit crowdsourced test reports3.
table demonstrates the attributes of a typical crowdsourced report4.
the platform can automatically record the crowd worker s information and environment information on which the test is carried on.
a worker is required to submit the testing task s he carried on and descriptions about the task including input operation steps results description and screenshots.
the report is also accompanied with an assessment as to whether the worker believes that the software behaved correctly i.e.
passed or incorrectly i.e.
failed .
in order to attract more workers testing tasks are often nancially compensated.
workers may then submit thousands of test reports due to nancial incentive and other motivations.
usually this platform delivers approximately projects per month and receives more than test reports per day on average.
among those reports more than are reported as failed .
nevertheless they have many 2baidu baidu.com is the largest chinese search service provider.
its crowdsourcing test platform test.baidu.com is also the largest ones in china.
3we will simplify crowdsourced test report as crowdsourced report or report potentially avoiding the confusion with test set in machine learning techniques.
4reports are written in chinese in our projects.
we translate them into english to facilitate understanding.
191false positives i.e.
a test report marked as failed that actually involves correct behavior or behavior outside of the studied software system.
this is due to the nancial compensation mechanism which usually favors failed reports.
currently in this platform testers need to manually inspect these failed test reports to judge whether they actually reveal fault true fault .
however inspecting reports manually could take almost half a working week for a tester.
besides only less than of them are nally determined as true fault.
obviously such process is time consuming tedious and low e cient.
in consequence this motivates us to e ciently automate the classi cation of crowdsourced test reports.
.
active learning active learning is a sub eld of machine learning.
the key hypothesis is that if the learning algorithm is allowed to choose the data from which it learns it will perform better with less training data .
consider that for any supervised learning system to perform well it must often be trained on hundreds even thousands of labeled instances.
for most of the learning tasks labeled instances are very di cult time consuming or expensive to obtain.
active learning attempts to overcome the labeling bottleneck by selecting unlabeled instances and asking user their labels simpli ed as select a query or query in the following paper .
in this way the active learner aims to achieve high accuracy using as few labeled instances as possible thereby reducing the cost of obtaining labeled data .
all active learning techniques involve measuring the informativeness of unlabeled instances.
informativeness represents the ability of an instance in reducing the uncertainty of the classi cation.
for example the chosen instance is the one which the classi er is least certain how to label .
.
local bias local bias refers to the phenomenon that data are heterogeneous within dataset and their distributions are often di erent among di erent parts of the dataset .
crowdsourced reports naturally have local biases where the contained technical terms may be shared by only a subset of reports rather than all the reports.
the main reason is as follows.
software often has several technical aspects e.g.
display location navigation etc.
in crowdsourced testing each report usually focuses on specific technical aspects of the project and describes the software behavior with speci c technical terms.
when classifying a particular report reports which share more technical terms might be helpful to build classi ers while reports which share less or none terms might contribute less to the classi cation.
we present an illustrative example to illustrate the local bias of crowdsourced reports and its in uence.
we rst randomly select an experimental project p1 a map app and use k means to group the reports into four clusters based on their technical terms details are in section .
.
each cluster contains a set of reports that share similar technical terms which should be the testing outcomes for same technical aspect or similar aspects.
we then generate the term cloud for each cluster in figure .
the size and color demonstrate the frequency of technical terms with larger and darker ones denoting the terms of higher frequency.
figure illustrative example of local bias we can easily observe that the technical terms exert great di erences among clusters.
the top left cluster might concern with the technical aspect of speech recognition with terms like recognition speech and voice .
the top right cluster contains such terms as location gps and deviation which might relate to the technical aspect of location.
taken in this sense models built on the reports from recognition cluster may fail to classify the reports in location cluster.
note that we have tried the number of clusters ranging from to all exposing the local bias problem.
here we only show the results with four clusters for better visualization.
local bias has been widely investigated in e ort estimation and defect prediction studies .
the most common technique to overcome the local bias is relevancy ltering e.g.
nearest neighbor similarity .
to be more speci c it allows the use of only certain train instances which are closer to the test instances for model construction.
however there are scarcely any studies focusing on dealing with the local bias in active learning methods.
this paper borrows the idea of relevancy ltering and designs a novel approach to mitigate the local bias in active learning.
.
approach figure demonstrates the overview of our approach.
we will rst illustrate the feature extraction followed with the details of our local based active classi fication loaf .
.
extracting features the goal of feature extraction is to obtain features from crowdsourced reports which can be used as input to train machine learning classi ers.
we extract these features from the text descriptions of crowdsourced reports.
we rst collect di erent sources of text description together input and operation steps result description .
then we conduct word segmentation as the crowdsourced reports in our experiment are written in chinese.
we adopt ictclas5for word segmentation and segment descriptions into words.
we then remove stopwords i.e.
am on the etc.
to reduce noise.
note that workers often use di erent words to express the same concept so we introduce the synonym replacement technique to mitigate this problem.
synonym library of ltp6is adopted.
each of the remaining term corresponds to a feature.
for each feature we take the frequency it occurs in the description as its value.
we use the tf term frequency instead of 5ictclas is widely used chinese nlp platform.
6ltp is considered as one of the best cloud based chinese nlp platforms.
192figure overview of our approach tf idf because the use of the inverse document frequency idf penalizes terms appearing in many reports.
in our work we are not interested in penalizing such terms e.g.
break problem that actually appear in many reports because they can act as discriminative features that guide machine learning techniques in classifying reports.
we organize these features into a feature vector .
.
local based active classification loaf the primary focus of active learning techniques is measuring the informativeness of unlabeled data details are in section .
in order to query the most informative instance to help build e ective classi er.
the design of existing techniques is based on such assumption that data are identically distributed .
however crowdsourced reports usually have signi cant local bias details are in section .
so existing techniques would generate poor and unstable performance on these data details are in section .
.
to address such problem we propose the local based active classi fication loaf .
the main idea is to query the most informative instance within local neighborhood then learn classi ers based on local neighborhood.
local neighborhood is de ned as one or several nearby with smallest distance labeled instances.
to cope with the local bias in the investigated dataset the informativeness in our approach is measured within local neighborhood denoted as local informativeness .
in details we rst obtain the local neighborhood for each unlabeled instance and measure its local informativeness.
we then select the most local informative unlabeled instance and query its label.
algorithm and figure summarize the process of loaf.
loaf rst chooses the initial instance and ask user its label.
after that loaf would iteratively select one instance ask user its label and learn classi ers from the labeled instances.
loaf then leverages the up to date labeled information to choose which instance to select next.
the process will continue until satisfying one of the termination criteria.
we will illustrate the process in more detail in the following subsections.
.
.
initializing this step is to choose the initial test report and ask user its label.
because of the existence of local bias randomly choosing the initial instance in existing active learning techniques cannot work well .
hence our approach proposes a new initial selection method to choose the initial test report for labeling.
this can mitigate the in uence of initial instance and obtain stable good performance.
loaf rst computes the distance between each pair of report then obtains the nearest distance for each report.
second loaf selects the report whose nearest distance isalgorithm local based active classi cation loaf input unlabeled report set u labeled report set l null tag for termination tm true parameter prlocal andprstop output classi cation results c initialize choose the initial report ur iaccording to eq.
and query its label l l ur i u u ur i while tm do select a query select unlabeled report ur ifrom uaccording to eq.
and query its label l l ur i u u ur i learn classi ers foreach unlabeled report ur iinudo choose prlocal labeled report lr jaccording to eq.
learn classi er and obtain the classi cation result cifor report ur i. end for whether to terminate judge whether can terminate based on prstop if yes set tm as false.
end while the largest among all reports.
this is to ensure the most sparse local region can also build e ective classi ers details are in section .
.
and .
.
.
in detail the chosen initial instance is computed as follows arg max i2ufmin j2u j6 i distance ufi uf j g where urepresents the initial unlabeled dataset ufiand ufjdenote the feature vector of ithandjthtest report inu respectively.
we apply cosine similarity between two feature vectors to measure their distance.
this is because prior study showed that it performs better for highdimensional text documents than other distance measures e.g.
euclidean distance manhattan distance .
.
.
selecting a query this step aims at selecting the most informative test reports and asking user their labels.
in each iteration given a set of labeled reports and unlabeled reports loaf will select an unlabeled report based on its local informativeness.
in detail rstly loaf obtains the local neighborhood for each unlabeled report and measures its local informativeness.
this is done through computing the distance between the unlabeled report and every labeled report.
the labeled report with nearest distance is treated as the local neighborhood for each unlabeled report and the local informativeness is measured using the nearest distance.
secondly loaf selects the unlabeled report whose local informativeness is the largest which can better serve the local based classi cation in section .
.
.
this is done by selecting the 193unlabeled report whose nearest distance is the largest among all unlabeled reports.
the rationale behind is as follows to serve as the localbased classi cation in the next step it should be guaranteed that each unlabeled instance has nearby reports which can be used to classify itself.
hence if one instance s nearest neighbor has already been labeled the local based classi er would have the higher probability of correctly classifying it.
on the other hand if the nearest labeled instance turns out to be far away this might imply big uncertainty in model building and classi cation.
from this point of view the farthest of the nearest labeled instance turns out to be the most informative one for local based classi cation.
in detail the selected instance is computed as follows arg max i2ufmin j2l distance ufi lfj g where uandlrepresent the unlabeled dataset and labeled dataset in this iteration respectively.
ufiandlfjdenote the feature vector of ithunlabeled report and feature vector ofjthlabeled report respectively.
similarly we apply cosine similarity to measure the distance of two feature vectors.
.
.
learning classifiers this step aims at learning classi ers to classify unlabeled reports using labeled reports.
since our investigated dataset has local bias loaf uses the local based classi cation i.e.
utilizing the reports in one s local neighborhood to conduct the classi cation.
in detail for each unlabeled report loaf uses the prlocal most nearest labeled test reports to build classi er.
theprlocal labeled reports are chosen as follows argprlocalfmin j2l distance ufi lfj g where ufidenotes the feature vector of ithunlabeled report which needs to be classi ed.
lfjrepresents the feature vector of jthlabeled report and lrepresents the labeled dataset in this iteration.
note that if the size of lis smaller thanprlocal then prlocal is set as the size of l. similarly we apply cosine similarity to calculate the distance of the two feature vectors.
.
.
whether to terminate this step is to judge whether the labeling process could be terminated.
our approach considers two di erent scenarios of termination.
the rst one is that user can input the maximum e ort e.g.
number of labeling instances they can afford.
when the e ort is reached the labeling process will be terminated.
the second one is that the approach will decide whether to terminate according to the classi cation results.
if the classi cation for each unlabeled report remains unchanged in the successive prstop iterations loaf would suggest termination.
for both scenarios the performance in the last iteration is treated as the nal classi cation results.
note that as the classi cation performance in the rst scenario is unguaranteed this paper only evaluates the effectiveness of loaf under the second scenario.
.
experiment setup .
research questions we evaluate our approach through three dimensions effectiveness advantage and usefulness.
speci cally our evaluation addresses the following research questions table projects under investigation fa tf tf fa tf tf p1 .
p2 .
p3 .
p4 .
p5 .
p6 .
p7 .
p8 .
p9 .
p10 .
p11 .
p12 .
p13 .
p14 .
p15 .
p16 .
p17 .
p18 .
p19 .
p20 .
p21 .
p22 .
p23 .
p24 .
p25 .
p26 .
p27 .
p28 .
p29 .
p30 .
p31 .
p32 .
p33 .
p34 .
summary fa tf tf .
projects for case study c1 .
c2 .
c3 .
rq1 e ectiveness how e ective is loaf in classifying crowdsourced reports?
we rst investigate the performance of loaf in classifying crowdsourced report for each experimental project.
then we study the in uence of parameter prlocal and prstop on model performance as well as suggest the optimal prlocal andprstop.
finally we explore the in uence of initial instance on model performance and further demonstrate the e ectiveness of our initial selection method.
rq2 advantage can loaf outperform existing techniques in classifying crowdsourced reports?
to demonstrate the advantages of our approach we compare the performance of loaf with both active learning techniques and supervised learning technique details in section .
.
rq3 usefulness is loaf useful for software testers?
we conduct a case study and a questionnaire survey in baidu crowdsourced testing group to further evaluate the usefulness of the proposed loaf.
.
data collection our experiment is based on crowdsourced reports from the repositories of baidu crowdsourced testing platform.
we collect all crowdsourced testing projects closed between oct. 20th and oct. 30th .
there are totally projects.
table provides more details with total number of crowdsourced reports submitted number of failed reports fa number and ratio of reports assessed as true fault tf tf .
due to commercial consideration we replace detailed project names with serial numbers.
additionally we randomly collect three other projects closed in mar.
10th to conduct the case study in section .
.
note that our classi cation is conducted on failed reports not the complete set.
we exclude the passed reports because of the following reason.
as we mentioned failed reports can usually involve both correct behaviors and true faults.
however through talking with testers in the company we nd that almost none of the passed reports involve true fault.
194we use the assessment attribute of each report as the groundtruth label of classi cation.
to verify the validity of these stored labels we additionally conduct the random sampling and relabeling.
in detail we randomly select projects and sample of crowdsourced reports from each selected project.
a tester from the company is asked to relabel the data without knowing the stored labels.
we then compare the di erence between the stored labels and the new labels.
the percentage of di erent labels for each project is all below .
.
experimental setup and baselines to demonstrate the advantages of loaf we rst compare our approach to three active learning techniques which are commonly used or the state of the art techniques margin sampling randomly choose the initial test report to label.
use all current labeled reports to build classi er and query a report for which the classi er has the smallest di erence in con dence for each classi cation type true fault or not .
label it and repeat the process.
least con dence randomly choose the initial test report to label.
use all current labeled reports to build classi er and query a report that the classi er is least con dent about.
label it and repeat the process.
informative and representative randomly choose the initial test report to label.
use all current labeled reports to build classi er.
then query a report which can both reduce the uncertainty of classi er and represent the overall input patterns of unlabeled data.
label it and repeat the process.
this is the state of the art technique.
we use the package quire7for experiments.
to alleviate the in uence of initial report on model performance for each method we conduct experiments with randomly chosen initial reports.
as there might be some historical data which can be utilized for model building we also compare our approach with supervised learning technique .
inspired by the crossproject prediction in defect prediction and e ort estimation our experimental design is as follows for each project under testing we choose the most similar project from all available crowdsourced testing projects then build a classi er based on the reports of that selected project.
the similarity between two projects is measured using the method in which is based on the marginal distribution of training set and test set.
both our approach and these baselines all involve utilizing machine learning classi cation algorithm to build classi er.
we have experimented with support vector machine svm decision tree naive bayes and logistic regression .
among them svm can achieve good and stable performance.
hence we only present the results of svm in this paper due to space limit.
.
evaluation metrics as the main consideration for active learning is coste cient we utilize accuracy ande ort to evaluate the classi cation performance.
the f measure of classifying true fault after termination which is the harmonic mean of precision and recall is used to measure the accuracy .
the reason we do not use quire.ashxprecision and recall separately is because most of the fmeasure in our experiments is .
with precision and recall.
we record the percentage of labeled reports among the whole set of reports after termination to measure the e ort for the classi cation.
.
results .
answering rq1 effectiveness rq1.
what is the performance of loaf in classifying crowdsourced reports?
table demonstrates the accuracy and e ort of loaf for each experimental project under the optimal prlocal and prstop as well as the statistics.
results reveal that in projects loaf can achieve the accuracy of .
denoting precision and recall.
in projects the accuracy is above .
.
furthermore the minimum accuracy attained by loaf is .
with precision and recall both above .
.
for e ort loaf merely requires to label of all test reports on median for building e ective classi er.
beside for projects loaf only needs to label less than of all reports which can support classifying all the remaining test reports e ectively.
the above analysis indicates that loaf can facilitate the report classi cation with high accuracy and little e ort.
rq1.
what is the impact of parameter prlocal on model performance and what is the optimal prlocal for loaf?
we use prlocal to control the local neighborhood when conducting local based classi cation section .
.
.
to answer this question we vary prlocal from to with prstop as and compare the classi cation performance.
we nd that for all experimental projects there are two patterns of performance trend under changing prlocal.
due to space limitation we only present the performance trend of one random chosen project for each pattern in figure .
we can observe that prlocal indeed could in uence the model performance which indicates the need for nding the optimal prlocal.
in the rst pattern the performance would remain best from prlocal is thus the optimal prlocal ranges from to .
in the second pattern only when prlocal is between and the accuracy is highest while the e ort is lowest thus the optimal prlocal is in a narrower range from to .
the reason why performance keeps unchanged in the rst pattern is that the actual labeled instances are fewer than prlocal so that the actual prlocal used in such scenario is the number of labeled instances not the demonstrated prlocal.
generally speaking too small and too large prlocal can both result in low accuracy and high e ort.
this may be because small prlocal will result in the over tting of classi ers while large prlocal can easily bring noise to the model.
to determine the optimal prlocal in our approach we rst obtain the value of optimal prlocal for each experimental project.
then we count the occurrence of each value and consider the value with most frequent occurrence as optimal prlocal which is and in our context.
we simply use in the following experiments for saving computing cost.
195table comparison of classi cation performance with di erent active learning techniques rq1.
rq2.
loaf margin samp.
least conf.
infor.
repre.
acc.
e .
acc.
e .
acc.
e .
acc.
e .
p1 .
.
.
.
.
.
.
p2 .
.
.
.
.
.
.
p3 .
.
.
.
.
.
.
p4 .
.
.
.
.
.
.
p5 .
.
.
.
.
.
.
p6 .
.
.
.
.
.
.
p7 .
.
.
.
.
.
.
p8 .
.
.
.
.
.
.
p9 .
.
.
.
.
.
.
p10 .
.
.
.
.
.
.
p11 .
.
.
.
.
.
.
p12 .
.
.
.
.
.
.
p13 .
.
.
.
.
.
.
p14 .
.
.
.
.
.
.
p15 .
.
.
.
.
.
.
p16 .
.
.
.
.
.
.
p17 .
.
.
.
.
.
.
p18 .
.
.
.
.
.
.
p19 .
.
.
.
.
.
.
p20 .
.
.
.
.
.
.
p21 .
.
.
.
.
.
.
p22 .
.
.
.
.
.
.
p23 .
.
.
.
.
.
.
p24 .
.
.
.
.
.
.
p25 .
.
.
.
.
.
.
p26 .
.
.
.
.
.
.
p27 .
.
.
.
.
.
.
p28 .
.
.
.
.
.
.
p29 .
.
.
.
.
.
.
p30 .
.
.
.
.
.
.
p31 .
.
.
.
.
.
.
p32 .
.
.
.
.
.
.
p33 .
.
.
.
.
.
.
p34 .
.
.
.
.
.
.
min .
.
.
.
.
.
.
max .
.
.
.
.
.
.
med.
.
.
.
.
.
.
.
avg.
.
.
.
.
.
.
.
note the two numbers in one cell represent minimum and maximum value of random experiments.
a project b project figure in uence of pr local on performance rq1.
rq1.
what is the impact of parameter prstopon model performance and what is the optimal prstop for loaf?
in our work we use prstop to decide whether to terminate section .
.
.
to answer this question we use the optimal prlocal obtained earlier which is and vary prstop from to for experiments.
through examining the performance trend for all experimental projects we nd that there are also two patterns of performance trend under changing prstop.
we only present the performance trend of one random chosen project for each pattern in figure because of space limitation.
similarly we can observe that prstop indeed could inuence the model performance thus suggesting the optimal prstop would be helpful.
it is easily understood that with a project b project figure in uence of pr stop on performance rq1.
the increase of prstop the e ort would increase correspondingly.
from the rst pattern we can observe that accuracy might occasionally decrease with the increase of prstop but can recover when prstop is .
the second pattern shows that the accuracy can reach the highest and remain unchanged from prstop is .
to determine the optimal prstop we rst obtain the value ofprstop when the accuracy reach the highest and keep stable for each project and for the two demonstrated projects .
we then consider the maximum among these values as optimal prstop which is to ensure all the projects can reach the highest accuracy.
in our experimental context the optimal prstop is .
rq1.
what is the impact of initial instance on model performance and how e ective of our initial selection method?
to answer this question we experiment with randomchosen report acting as the initial instance and repeat n times n is set as half of project size .
figure demonstrates the min average and max value of model performance for random selection of initial report.
nearly projects would undergo quite low accuracy min value is less than .
when randomly choosing initial report.
for e ort nearly projects would involve quite high e ort max value is more than when choosing some random report for initial labeling.
this reveals that random selection of initial instance can usually fall into low performance.
the results further indicate the great necessity to design a method for the selection of initial instance to ensure a stable performance.
we then compare the performance of our initial selection method details in section .
.
with the statistics of random selection.
for accuracy in projects our method of initial selection can achieve the maximal value which the random selection can ever achieve.
for other projects the di erence between our method and the maximal value of random selection is almost negligible ranging from .
to .
.
these ndings reveal that our initial selection method can achieve high and stable accuracy even compared with the best accuracy which random selection can ever reach.
for e ort in projects our method of initial selection requires less e ort than the maximal e ort which random selection would require.
in projects our method requires less e ort than the average e ort required by random selection.
in of remaining projects the di erence between the e ort of our method 196figure in uence of initial instance on model performance rq1.
and the average e ort of random selection is smaller than .
these illustrations further reveal that our initial selection method is superior to random selection considering its stable results high accuracy and little e ort.
.
answering rq2 advantage rq2.
how does loaf compare to existing active learning techniques in classifying crowdsourced reports?
table illustrates the performance of loaf and three active learning baselines.
as we mentioned in section .
we conduct experiments for each active learning method because their random initialization can e ect the nal performance.
we present the minimum and maximum performance of the random experiments for these methods.
loaf has well designed initial selection step section .
.
so its performance is unique.
the value with dark background denotes the best accuracy or e ort for each project.
at rst glance we can nd that all the three baseline methods can occasionally fall into quite low accuracy and more e ort for most of the projects.
on the contrary as we have well designed method to choose the initial report loaf can achieve high and stable performance.
we also noticed that even the state of the art method infor.
repre.
method in table can not perform well for crowdsourced testing reports.
this may stem from the fact that it does not consider the local nature of the dataset.
we then compare the performance of loaf with the best performance which the baselines can ever achieve highest accuracy or least e ort .
we can observe that all statistics min max median and average of accuracy for loaf is higher than the best performance of baselines denoting our approach can achieve a more accurate classi cation performance the median value is .
.
in addition these statistics for e ort of our approach are all less than the baselines vs. for median value .
we then shift our focus to the performance of each project.
for more than of all projects our approach can attain both the highest accuracy and least e ort even compared with the best performance of baselines.
besides for one of the remaining projects our approach can achieve much higher accuracy with more e ort at worst p3 than the best baseline approach.
for other four projects our approach has slight decline in accuracy but with least e ort.
we also noticed that the decline is quite small with the maximum being .
.
vs. .
and all the accuracy of our approach is above .
.
when it comes to the time and space cost of the mentioned techniques it takes less than .
seconds and less figure comparison of classi cation performance with supervised learning technique rq2.
then .0mb memory for each step.
due to space limit we would not present the details.
rq2.
how does loaf compare to existing supervised learning technique in classifying crowdsourced reports?
figure illustrates the accuracy of loaf and supervised prediction technique.
we do not present the e ort because supervised learning relies on historical data for classi cation thus the e ort is labeling all the training data.
we can easily observe for all the projects the accuracy of loaf is higher than the accuracy obtained by supervised learning.
the median accuracy of supervised learning for all projects is .
which is much smaller than that of loaf .
.
this indicates that loaf can perform even better than supervised classi cation which built on the large amount of historical labelled data.
the reason might be originated from the fact that existing techniques cannot well deal with the local bias of crowdsourced testing data.
.
answering rq3 usefulness rq3 is loaf useful for software testers?
to further assess the usefulness of loaf we use the implementation of our proposed loaf to conduct a case study and a questionnaire survey in baidu.
we randomly select three projects for our case study details are in table .
we collect these test reports as soon as it was closed without any labeling information.
six testers from the crowdsourced testing group are involved.
we divide them into two groups according to their experience with details summarized in table .
the goal of this case study is to evaluate the usefulness of loaf in classifying the true fault from the crowdsourced test reports.
firstly each practitioner in group a is asked 197table participant of case study rq3 group a group b a1 years experience in testing b1 a2 years experience in testing b2 a3 years experience in testing b3 to do the classi cation using loaf.
as a comparison practitioners in group b are asked to do it manually.
to build the ground truth we gather all the classi cation outcomes from the practitioners.
follow up interviews are conducted to discuss the di erences among them.
common consensus is reached on all the di erence and a nal edition of classication is used as the ground truth details in table .
besides the accuracy and e ort evaluation metrics in prior experiments we also record the time taken for the classi cation.
table presents the detailed results.
table results of case study rq3 results from group a results from group b project c1 c2 c3 c1 c2 c3 accuracy .
.
.
.
.
.
.
.
.
.
.
.
e ort time min note the two numbers in one cell represent minimum and maximum value from the three practitioners.
the classi cation performance of loaf group a is much better than the performance by manual group b with higher accuracy lower e ort and less time.
in particular with the increase of project size the consumed e ort and time of manual classi cation can dramatically increase while its accuracy would also drop obviously.
although testers can assign wrong labels to the queried reports when using loaf the nal accuracy is less a ected by these mistakes.
this further denotes the e ectiveness and usefulness of loaf in real practice.
in addition we design a questionnaire and conduct a survey to consult testers about the usefulness of loaf.
the questionnaire rst demonstrates a short description about loaf a visualized classi cation process and a summarized evaluation result on projects.
then it asks three questions shown in table .
we provide ve options for the rst two questions and allow respondents freely express their opinion for the third question.
we send invitation emails to the testers who are involved in the report classi cation in baidu crowdsourced testing group.
we totally received responses out of requests.
as indicated in table of all respondents of them agree that loaf is useful for report classi cation and they would like to use it.
this means testers agree the usefulness of loaf in general.
only hold conservation options and disagreed.
when it comes to the reason of disagreement they mainly worry about its exibility and performance on new project as well as the recall of faults.
this paves the direction for further research.
in addition the project manager shows great interest in loaf and is arranging to deploy loaf on their platform to assist the classi cation process.
.
threats to v alidity the external threats concern the generality of this study.
first our experiment data consist of projects collected from one of the chinese largest crowdsourced testing platforms.
we can not assume a priori that the results of ourstudy could generalize beyond this environment in which it was conducted.
however the various categories of projects and size of data relatively reduce this risk.
second all crowdsourced reports investigated in this study are written in chinese and we cannot assure that similar results can be observed on crowdsourced projects in other languages.
but this is alleviated as we did not conduct semantic comprehension but rather simply tokenize sentence and use word as token for learning.
regarding internal threats we only utilize textual features to build classi ers without including other features.
besides we only use cosine similarity for measuring the distance between crowdsourced reports without trying other measures.
the experiment outcomes have proved the e ectiveness of our method.
anyhow we will try more features e.g.
the attributes of report and other metrics for distance to further investigate their in uence on model performance.
construct validity of this study mainly questions the data processing method.
we rely on the assessment attribute of crowdsourced reports stored in repository to construct the ground truth.
however this is addressed to some extent due to the fact that testers in the company have no knowledge that this study will be performed for them to arti cially modify their labeling.
besides we have veri ed its validity through random sampling and relabeling.
.
related work .
crowdsourced testing crowdsoucing is the activity of taking a job traditionally performed by a designated agent usually an employee and outsourcing it to an unde ned generally large group of people in the form of an open call .
chen and kim applied crowdsourced testing to test case generation.
they investigated object mutation and constraint solving issues underlying existing test generation tools and presented a puzzle based automatic testing environment.
musson et al.
proposed an approach in which the crowd was used to measure real world performance of software products.
the work was presented with a case study of the lync communication tool at microsoft.
gomide et al.
proposed an approach that employed a deterministic automata to help usability testing.
adams et al.
proposed motif to detect and reproduce context related crashes in mobile apps after their deployment in the wild.
all the studies above use crowdsourced testing to solve some problems in traditional software testing activities.
however our approach is to solve the new encountered problem in crowdsourced testing.
feng et al.
proposed test report prioritization methods for use in crowdsourced testing.
they designed strategies to dynamically select the most risky and diversi ed test report for inspection in each iteration.
their method was evaluated only on projects with students acting as crowd workers while our evaluation is conducted on projects with case study in one of the chinese largest crowdsourced testing platform.
besides our approach requires much less e ort than theirs.
our previous work proposed a cluster based classi cation approach to e ectively classify crowdsourced reports when facing with plenty of training data.
however training data is often not available.
this work can reduce the e ort for collecting training data while still being able to achieve good performance.
198table results of survey rq3 questions strongly disagreedisagree neither agree strongly agreetotal q1.
do you think loaf is useful to classify true fault from crowdsourced test report?
q2.
would you like to use loaf to help with the report classi cation task?
if disagree for either of the question please give the reason.
worry about its accuracy on other projects flexibility on new projects might miss crucial fault .
automatic classification in software engineering issue reports are valuable resources during software maintenance activities.
automated support for issue report classi cation can facilitate understanding resource allocation and planning.
menzies and marcus proposed an automated severity assessment method by text mining and machine learning techniques.
tian et al.
proposed drone a multi factor analysis technique to classify the priority of bug reports.
wang et al.
proposed a technique combining natural language and execution information to detect duplicate failure reports.
zanetti et al.
proposed a method to classify valid bug reports based on nine measures quantifying the social embeddedness of bug reporters in the collaboration network.
zhou et al.
proposed a hybrid approach by combining both text mining and data mining techniques of bug report data to automate the classi cation process.
wang et el.
proposed fixercache an unsupervised approach for bug triage by caching developers based on their activeness in components of products.
mao et al.
proposed content based developer recommendation techniques for crowdsourcing tasks.
our work is to classify test report in crowdsourced testing which is di erent from the aforementioned studies in two ways.
firstly crowdsourced reports are more noise than issue reports because they are submitted by non specialized crowd workers and often under nancial incentives.
in this sense classifying them is more valuable yet possesses more challenges.
secondly previous studies utilized supervised learning techniques to conduct the classi cation which often requires users to manually label plenty of training data.
our proposed approach reduces the burden of manual labeling while still being able to achieve good performance.
there were researches to classify app reviews as bug reports feature requests etc.
which can help deal with the large amounts of reviews.
app reviews are often considered as issue reports by users who behave unprofessionally as crowd workers in our context.
but these related methods need large number of labeled data to learn the supervised model which is time consuming.
moreover the performances of our approach surpass theirs f measure is .
to .
in a lot.
some other studies focus on di erentiating between malicious behaviors and benign behaviors of app based on data ow or context information .
this motivates us to utilize new sources of information to conduct the report classi cation in future work.
.
active learning in software engineering there is a wealth of active learning studies in machine learning literature such as .
several studies have utilized active learning techniques for software engineering tasks.
bowring et al.
proposed an automatic approach for classifying program behavior by leveraging markov model and active learning.
lucia et al.
proposed an approach to actively incorporate user feedback in ranking cloneanomaly reports.
wang et al.
proposed a technique that can re ne the results from a code search engine by actively incorporating incremental user feedback.
thung et al.
proposed an active semi supervised defect prediction approach to classify defects into odc defect type.
ma et al.
proposed an active approach for detecting malicious apps.
these aforementioned studies merely utilized existing active learning techniques to solve the software engineering tasks without considering the speciality of these tasks.
kocaguneli et al.
proposed quick which is an active learning method that assists in nding the essential content of software e ort estimation data so as to simplify the complex estimation methods.
nam et al.
proposed novel approaches to conduct defect prediction on unlabeled datasets in an automated manner.
these two researches have something in common with our work they are new designed active learning methods for specialized task.
but their methods can not be directly used for the classi cation of crowdsourced reports and could not handle the local bias in dataset.
.
conclusion this paper proposes local based active classi fication loaf to address the two challenges in automating crowdsourced test reports classi cation i.e.
local bias problem and lacking of historical labeled data.
we evaluate loaf from the standpoints of e ectiveness advantage and usefulness in one of the chinese largest crowdsourced testing platforms and results are promising.
active learning techniques are promising when facing such situation that data are abundant but labels are scarce or expensive to obtain.
this is becoming quite common in software engineering practice for the last decade.
hence our approach can well motivate the various classi cation problems which have required plenty of labeled data e.g.
report classi cation app review classi cation .
it should be pointed out however that the presented material is just the starting point of the work in progress.
we are closely collaborating with baidu crowdsourced platform and planning to deploy the approach online.
returned results will further validate the e ectiveness as well as guide us in improving our approach.
future work will also include exploring other features and techniques to further improve the model performance and stability.
.