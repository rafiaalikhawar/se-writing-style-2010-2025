do android taint analysis tools keep their promises?
felix pauck university paderborn paderborn germany fpauck mail.uni paderborn.deeric bodden university paderborn paderborn germany eric.bodden uni paderborn.deheike wehrheim university paderborn paderborn germany wehrheim uni paderborn.de abstract in recent years researchers have developed a number of tools to conduct taint analysis of android applications.
while all the respective papers aim at providing a thorough empirical evaluation comparability is hindered by varying or unclear evaluation targets.
sometimes the apps used for evaluation are not precisely described.
in other cases authors use an established benchmark but cover it only partially.
in yet other cases the evaluations differ in terms of the data leaks searched for or lack a ground truth to compare against.
all those limitations make it impossible to truly compare the tools based on those published evaluations.
we thus present reprodroid a framework allowing the accurate comparison of android taint analysis tools.
reprodroid supports researchers in inferring the ground truth for data leaks in apps in automatically applying tools to benchmarks and in evaluating the obtained results.
we use reprodroid to comparatively evaluate on equal grounds the six prominent taint analysis tools amandroid dialdroid didfail droidsafe flowdroid andiccta .
the results are largely positive although four tools violate some promises concerning features and accuracy.
finally we contribute to the area of unbiased benchmarking with a new and improved version of the open test suite droidbench .
ccs concepts software and its engineering empirical software validation software testing and debugging keywords android taint analysis tools benchmarks empirical studies reproducibility.
acm reference format felix pauck eric bodden and heike wehrheim.
.
do android taint analysis tools keep their promises?.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
introduction with smartphones becoming a part of everyone s daily life users frequently downloading new apps to their phones and even performing security critical applications like banking or the coordination of medical treatments the probability of an undesired leak of private data increases steadily.
such data leaks may arise form coding mistakes but also may be the result of malicious attacks.
according to gartner currently of all mobile phones use android as operating system.
in the past researchers have proposed a number of tools to detect data leaks in android applications.
the tools employ various analysis techniques ranging from static over dynamic and hybrid analyses to methods built on logical reasoning .
static analysis tools often employ a form of taint analysis sensitive data i.e.
data from specific private sources is tainted and then statically tracked through the application s data flow and sometimes control flow.
whenever tainted data reaches a pre defined public sink the taint analysis tool reports a privacy leak.
since several decades static analysis is known to be undecidable .
undecidability forces static taint analysis tools to approximate the data flows they compute.
in case of an over approximation the taint analysis might report spurious warnings so called false positives while in the case of under approximations it may miss actual data flows resulting in false negatives .
while all static taint analysis tools naturally share the idea of tracking taints each tool has its own strengths and weaknesses.
for instance some but not all tools have good support for handling component lifecycles callbacks or inter component communication icc .
further the tools provide different levels of precision by supporting or not an object field or context sensitive analysis .
due to these various features both researchers and practitioners wonder which tool is the optimal choice in which application context a question that can only be answered through a comparative evaluation.
many papers proposing taint analysis tools evaluate those tools using benchmarks such as the open test suites droidbench or icc bench .
these so called micro benchmarks consist of artifical mini apps developed for benchmarking purposes only.
each app or a predefined combination of multiple apps represents one benchmark case.
benchmark cases contain intentionally encoded data leaks e.g.
flows of data from a statement accessing the device s serial number getdeviceid to a statement sending sms sendtextmessage ... .
while the usage of such common benchmark suites seems to provide a solid basis for an unbiased and systematic comparison it suffers from some fundamental drawbacks for instance the micro benchmarks mostly lack information about the exact data leaks contained in each test case i.e.
the socalled ground truth .
instead comparisons take place on the grounds of just counting the number of data leaks found and comparing it against the one given for the benchmark case.
this hides incorrectarxiv .02903v2 jul 2019esec fse november lake buena vista fl usa felix pauck eric bodden and heike wehrheim leak detections in cases where these numbers match up coincidentally.
as our experiments confirm this problem exists not just in theory but has impaired past evaluations.
the comparison gets even less systematic when moving to realworld apps i.e.
apps that can be downloaded from an app market such as google s play store.1the inclusion of such apps in experiments is indispensable when it comes to evaluating the tools for scalability.
yet in most recent works reporting the evaluation of android taint analysis tools it is quite unclear which apps exactly have been used in the respective evaluation.
to give one example of many li et al.
state we randomly selected apps from our googleplay set for our study.
without naming them .
an additional related problem is that it is unclear which exact data leaks are to be found and have been found.
since for real world apps not even the number of actual data leaks is known let alone their exact data flows the evaluations simply report the number of leaks found without being able to assess which fraction of the tools warnings might be false and how many leaks are missed.
in fact this way of measuring success rewards rather than penalizes false positives as higher numbers are frequently seen as better results.
in summary tool benchmarks frequently lack confirmability and reproducibility.
the goal of this work is to remedy this unsatisfactory situation by enabling a reproducible fair and unbiased comparison.
specifically we present reprodroid a framework allowing for the accurate and systematic benchmarking of android taint analysis tools.
the approach comprises the following original contributions.
first we introduce the android app analysis query language aql which is used to precisely define analysis questions and answers.
in questioning the usage of aql allows us to run all examined tools on the same target i.e.
inspect the existence of the same flow of data.
on the answer side aql acts as a standardized language for describing the flows found.
second the associated aql system delegates analysis questions to appropriate tools by matching the question subject against tool capabilities and converts the produced answers into the aql format.
through those unique features the aql system supports the completely automatic benchmarking of tools.
third we present the benchmark refinement and execution wizard brew which helps one to refine execute and evaluate precisely formulated benchmarks.
brew allows us and others to more easily determine the ground truth of data leaks for test apps.
as another major contribution of this work we use reprodroid to carry out a reproducible comparison of some of the most prominent taint analysis tools for android apps amandroid dialdroid didfail droidsafe flowdroid andiccta on apps micro benchmarks and real world apps .
we find that the experiments reproduce most but not all results of previously published evaluations.
to further contribute to the area of systematic benchmarking we provide all benchmarks now precisely defined within a new version of the open test suite droidbench .
to summarize this paper presents the following contributions the android app analysis query language aql a mechanism to precisely define taint analysis queries and responses theaql system which dispatches aql queries to tools and consolidates their responses 1google play store refinement and execution wizard brew a tool to refine execute and precisely evaluate formulated benchmarks and a large scale comparative empirical evaluation of amandroid dialdroid didfail droidsafe flowdroid and iccta ondialdroid bench droidbench icc bench and newly developed test apps.
the remainder of this paper is structured as follows.
section introduces some basic concepts and a running example.
section details reprodroid s three major components.
section and present our large scale comparative evaluation and its results for the six mentioned taint analysis tools.
we discuss related work in section and conclude in section .
background in this section we start with introducing basic terminology and concepts in particular explain taint analysis and the current form of benchmark suites.
.
taint analysis the purpose of taint analysis is to track the flow of sensitive data within programs.
for smart phone apps a data leak occurs when private data phone numbers device identifiers contact data flows from sensitive sources to public sinks internet sms transmission .
in this case sensitive data is leaked.
taint analyses are most frequently used to detect such leaks it taints sensitive data at its source and propagates the taint information through the application or even a combination of apps issuing a warning if tainted data reaches a sink.
taint tracking can be performed statically on program code or dynamically by executing the app and monitoring tainted data.
android provides an open communication structure between apps.
moreover when android apps include third party libraries those execute with the same access rights as the app itself.
those features make android apps particularly vulnerable to attacks targeting private data.
taint analysis tools can cope with these special features to various extents.
the following programming language features and analysis functionalities are supported by some but not all tools aliasing the same memory location object may be referenced by different variables.
in this situation one variable is an alias of another and a taint related to one alias must be carried over to all others.
static fields static fields are declared on a type not their instance.
in particular their values can be accessed without requiring access to any object reference.
static taint analyses must thus treat static fields differently from instance fields.
lifecycle and callbacks each android component has its own lifecycle defining a sequence of invocations to callback functions that the android framework issues at appropriate lifecycle events.
user interface interactions map to callbacks as well.
to model all possible execution sequences of an app the analysis must take all appropriate callbacks into account and it can do so with various levels of precision.
inter component communication a leak may originate in one class and end in another.
additionally android allowsdo android taint analysis tools keep their promises?
esec fse november lake buena vista fl usa found a flow to sink virtualinvoke r4.
android.telephony.
smsmanager void sendtextmessage java.lang.string java.lang.string java.lang.string android.app.
pendingintent android.app.pendingintent null r5 null null from the following sources r5 virtualinvoke r3.
android.telephony.
telephonymanager java.lang.string getdeviceid in de.ecspride.
mainactivity void oncreate android.os.bundle listing excerpt of flowdroid s result directleak1.apk for inter component or inter app communication icc iac via the instantiation of so called intents andintent filters .
for example to access the device s pre installed camera app it is sufficient to dispatch a certain intent.
intents may propagate tainted data from one app or component to another.
analysis abstraction and algorithmics depending on the exact analysis abstraction and algorithmics the taint analysis may or may not support different sensitivities such as flow context path field object and or thread sensitivity .
while generally the support for more such sensitivities may increase precision reducing the amount of false positives the positive effects differ.
for instance while some level of object sensitivity is known to be important for the precise analysis of java and android applications threadsensitivity may well be less important in the case of android.
reflection java s reflection mechanism allows one to invoke methods or access fields through dynamically generated strings.
an analysis must resolve these strings to reliably detect taint flows through such invocations.
the fact that each android taint analysis tool supports those features to a different extent makes it important to evaluate them comparatively as without such comparison it is impossible to tell which features actually matter.
conducting a fair and automatic comparison of tools however is complicated by differences in pursued target flows and output formats.
a necessary part of every taint analysis is the identification of sources i.e.
statements that extract sensitive information from a certain resource and sinks i.e.
statements that exfiltrate information out of the app.
there exist different definitions of sources and sinks as well as various approaches to determine which statements belong to which set .
for instance some analysis tools consider logging statements as sinks others do not.
whereas one analysis approach defines that each source and sink has to be protected by at least one permission another might not consider such constraints.
the second difference concerns the representation of flows between sources and sinks for which every tool uses its own usually textual individually structured format.
this renders it impossible to automatically compare results produced by different tools without additional effort.
for example the result generated by flowdroid see listing from a structural point of view has little in common with didfail s result see listing .
yet both listings show the same finding a flow from getdeviceid source to sendtextmessage ... sink .
sink android.telephony.smsmanager void sendtextmessage java.lang.string java.lang.string java.lang.string android.app.pendingintent android.
app.pendingintent src android.telephony.telephonymanager java.lang.string getdeviceid listing excerpt of didfail s result directleak1.apk import android.telephony.telephonymanager testcase name directleak1 version .
... description easy testcase the value of a source is directly sent to a sink dataflow source sink number of leaks challenges public class mainactivity extends activity listing excerpt of droidbench s app source code directleak1.apk .
benchmarks most android app analysis tools are evaluated by means of benchmarks.
in this context a benchmarks is a collection of apps that have certain features.
for example the most frequently used microbenchmark droidbench comprises apps.
while most android taint analysis tools are evaluated by applying them to droidbench such evaluations are of limited value because droidbench currently only imprecisely specifies the ground truth in each benchmark case i.e.
the correct result of flow analyses.
listing gives an excerpt of the source code of one of droidbench s apps namely directleak1 the one the prior analysis results referred to .
between the imports and the source code of the main activity of this app a comment is placed.
this comment including the number of leaks is the only description of the expected analysis results there is a source directly connected to a sink.
neither do we learn which source and sink this might be nor how the flow propagates through the app s .
moreover the information is not given in a machine readable format.
looking it up in the source code reveals that the source is a getdeviceid function call which is used as a parameter of a sendtextmessage ... statement.
in this case a manual inspection is easy and can be done quickly.
however for more challenging benchmark cases or hundreds of cases this task becomes exhausting and error prone.
for example the machine readable information number of leaks is sometimes simply wrong e.g.
for droidbench sstrongupdate1 it says but the description and app execution prove that there is no leak .
still the authors of icc bench adopted the same method to specify their expected results.
hence automatically comparing actual results against expected ones is neither possible for droidbench nor foricc bench .
droidbench andicc bench are micro benchmarks.
evaluations on real world apps usually lack a ground truth altogether.
such evaluations usually focus on the to most downloaded or most popular apps that can be downloaded from google s playstore.
dialdroid bench is a benchmark suite comprising real worldesec fse november lake buena vista fl usa felix pauck eric bodden and heike wehrheim apps.
its apps however are given without source code only in the form of .apk files and comprise no description of the data leaks they contain.
in summary while there seems to be a general agreement of evaluating tools on the grounds of public benchmarks the evaluation itself is imprecise the ground truth needed to determine the tools precision and recall is often simply not known.
to evaluate tools automatically one further misses a standardized machine readable format for expected as well as detected data flows.
in the end this impedes a reproducible and unbiased comparison of analysis tools.
we next explain how reprodroid overcomes these limitations.
approach thereprodroid framework supports tool evaluation and comparison by the following three concepts the design of aql aquery language for precisely formulating questions about app properties such as flows the design and implementation of a query execution system being able to interface to diverse tools and the design and implementation of a query wizard for determining the ground truth in apps and for executing thus specified benchmarks and rating their outcome.
together they provide an automatic benchmarking system for appanalysis tools.
in the following we describe all three parts in turn.
.
app analysis language the android app analysis query language aql consists of two main parts namely aql queries and aql answers.
aql queries enable us to ask for android specific analysis subjects in a general tool independent way.
the grammar defining aql queries currently allows to ask for analysis subjects such as flows intents intent filters and permissions.
considering our running example we can enumerate all taint flows within the directleak1.apk app by composing the following query flows in app path to directleak1.apk ?
or instead explicitly check the taint flows we expect flows from statement getdeviceid method oncreate ... class mainactivity app path to directleak1.apk to statement sendtextmessage ... method oncreate ... class mainactivity app path to directleak1.apk ?
furthermore the aql offers several options to merge and filter queries as well as methods to match intents and intent filters.
similarly aql answers are used to represent analysis results in a standardized form.
the syntax of aql answers is defined via an xml schema definition.
considering the running example again we might get an answer that represents the flow from getdeviceid to the sendtextmessage ... statement cf.
listing .
in this each statement comes with a precise description of where it can be found by naming the method class and app containing the statement.
aql supports additional syntax to uniquely identify answer flows flow reference type from statement ... getdeviceid ... statement method ... oncreate ... ... method classname ... mainactivity classname app file ... directleak1.apk file hashes ... hashes app reference reference type to ... sendtextmessage ... ... reference flow flows answer listing shortened aql answer directleak1.apk statements and apps for example using function call parameters full jimple syntax3statements or .apk file hashes.
.
query execution system theaql system is our approach to process aql queries and to determine aql answers.
in the scope of this paper it is sufficient to observe the aql system as a blackbox which accepts analysis questions encoded in aql queries as input executes appropriate analysis tools and converts their output into aql answers.
to do so it requires a configuration in form of an .xml file that describes a which tools are avaliable in a certain instance of the aql system and how to execute these b which queries can be answered by which tool and c how to convert a tool s result into an aql answer.
for instance an aql system can be configured to execute flowdroid in case of intra app flow questions and iccta in case of inter app questions since flowdroid does not support icc iac.
considering the running example such an aql system recognizes thatflowdroid is available and able to answer the query regarding flows inside one app only.
consequently flowdroid is launched by executing the command or script specified in the configuration.
once its computation is finished a tool specific converter translates the tool s result into an aql answer.
we currently have converters covering in particular the six tools of our experiments in section .
with this we can orchestrate the execution of tools and convert their results into the standardized aql format.
.
benchmark refinement and execution reprodroid s final component is the benchmark refinement and execution wizard brew .
it is an assistant that can be used to do what the name suggests first refine and then execute a benchmark.
for this it offers a graphical user interface gui simplifying the handling of different sets of apps and benchmarks and the identification of sources and sinks.
3jimple stands for java but simple .
it is the primary intermediate language supported by soot .do android taint analysis tools keep their promises?
esec fse november lake buena vista fl usa the process of refining an existing benchmarks suite i.e.
completing it and bringing it into standardized format consists of three steps case identification we load the apps of the test suite into the wizard.
after loading each app resembles one benchmark case.
we can then restructure the cases by deactivating certain cases or combining cases e.g.
if we are interested in a flow from a source in one app to a sink in another app .
source and sink identification for source and sink identification the wizard displays all statements that possibly represent a source or a sink inside all apps that have been loaded during the first step.
since the wizard is just an assistant it cannot decide on its own which of these statements are real sources and sinks.
however brew can preselect all sources and sinks according to a predefined list of sources and sinks as produced by susi or pscout .
it is up to the user to perform the remaining task of deselecting all unwanted sources and sinks from the preselected ones.
it is also possible to combine certain sources and sinks.
this might be necessary to unify differently defined sources and sinks.
for example whereas one analysis considers the function call getlastknownlocation ... which returns an location object as source another analysis only considers the call of getlongitude orgetlatitude called upon such a location object as source.
however any of these calls refers to the same resources and hence all calls can be interpreted as a single source.
ground truth identification finally we have to decide which cases are positive and which cases are negative benchmark cases.
more precisely there may exist cases where we define a flow that should not be found by an analysis.
determining positive and negative cases remains a manual task which requires inspection of the case.
considering the running example only the directleak1.apk may be loaded as first step.
then if we choose to automatically mark all sources and sinks according to susi the getdeviceid sendtextmessage ... but no further statement get marked as source and sink.
in the last step this results in one benchmark case which is correctly and initially always marked as positive case by brew .
once the refinement steps have been completed the benchmark can be executed and evaluated.
to do so brew determines one aqlquery and one expected aql answer per benchmark case.
hence to automatically execute and evaluate a benchmark brew sends a query to an aql system for each benchmark case and checks whether the actual result determined that way matches the expected one.
for this purpose it is checked whether one flow of the expected result matches one flow of the actual result.
for example one analysis may detect a flow from getlastknownlocation ... tosendtextmessage ... whereas another analysis finds a flow from getlongitude to the same sink.
in both cases the expected flow regarding the accessed resources has been found.
consequently one matching flow per benchmark case is sufficient.
to evaluate the outcome of a benchmark execution brew counts the number of successful and failed benchmark cases.
a case is successful if a certain flow that was expected to be found has been benchmarks .
.brew .aql system config tools resultsquery aql query result aql answer figure sketch of the reprodroid toolchain found true positive or if a flow that was explicitly not expected to be found has not been found true negative .
in contrast a case fails if an expected flow was not found false negative or if a not expected flow has falsely been detected false positive .
based on this information brew computes the commonly used metrics precision recall and f measure.
overall brew helps and guides its users while refining a benchmark.
refining in this context refers to the process of adding missing information to a benchmark case.
thereby insufficiently described benchmark cases cf.
listing become usable i.e.
become available in machine readable format which allows one to execute and evaluate the benchmark cases automatically.
.
the reprodroid toolchain the composition of the three previously described components form our complete framework reprodroid see figure brew uses theaql system which again uses the aql .
in figure two parts are visualized inside clouds.
these symbolize the resources that exist in the community namely analysis tools and benchmarks.
each gray rectangle represents one of reprodroid s components.
to execute and evaluate a benchmark we first have to choose one and provide it to brew as input.
two methods are available to do so.
either an already refined benchmark can be loaded or a new unrefined set of apps can be refined interactively.
after setting up or loading the desired benchmark brew will issue one aql query per benchmark case.
then one query after another arrives at an aql system configured to use a certain set of tools.
the aql system executes the tools required to answer each query .
and produces oneaql answer per query as output.
this answer in turn is returned to brew which then decides if this actual answer matches the expected one .
.brew s gui provides possibilities to compare single results on a textual or graphical level.
the latter one depicts all sources and sinks as nodes and connections between them as edges in a graph.
all known intermediate statements between each source and sink are also depicted.
brew further allows one to export complete results to an sql database e.g.
to make them viewable online.
in summary we input a benchmark and one or more analysis tools and receive as output one benchmark result which includes the expected and actual aql answers of each benchmark case as well as the calculated values for precision recall and f measure.
considering the running example we could for instance input thedirectleak1.apk as a benchmark consisting of only one app and setup an aql system to use flowdroid .
the refinement step performed by means of brew allows us to identify the expected leak which is consequently available as ground truth or expected aql answer see listing .
along with that an aql query is composed to ask whether the expected leak actually exists.
the consideredesec fse november lake buena vista fl usa felix pauck eric bodden and heike wehrheim aql system takes this query and thereupon executes flowdroid .
once flowdroid finishes its computation the aql system converts flowdroid s result see listing into an aql answer see listing which is returned to brew .
since the expected and actual result coincide also see listing the benchmark case is evaluated as successful.
considering this tiny benchmark precision recall and f measure would be at its optimal value .
.
this toolchain is applicable to different scenarios.
for instance a specialist could refine a benchmark based on his expertise.
this refined benchmark can then be executed and used in an evaluation by any type of user such as an analysis tool developer.
the toolchain also allows us to refine and execute a benchmark in a distributed and iterative way.
multiple people can iteratively refine a single benchmark.
a benchmark can be executed extended by some additional cases and executed again without rerunning the already executed cases.
this can split the burden of refining and executing a benchmark among multiple persons.
experimental setup the design and implementation of reprodroid for the first time facilitates an accurate comparison of tools.
in the following we reevaluate six taint analysis tools for android apps and their promises on a number of micro benchmarks and real world apps.
.
tool selection we start with a description of our tool selection.
the tools selected for our evaluation all implement taint analyses.
this is the most frequently used technique for finding data leaks and thus provides us with a number of tools to be studied.
furthermore for the purpose of this study we only consider static taint analysis tools.
we further consider only approaches that are at least flow sensitive or context sensitive such as to assure that all tools are competitors within the same league.
table list of tools tool version amandroid november .
.
dialdroid september didfail march droidsafe june final flowdroid april nightly iccta february has been updated since then.table lists the tools employed in our evaluation along with their release dates and versions if available .
for all tools we generally used the most recent version.
just for flowdroid it holds that it is so frequently updated that we had to fix a version for our experiments.
all six tools we consider have at least icc and at best iac capabilities except for flowdroid which computes flows within single components only.
yet it is an important tool to consider because it is the most widely used static analysis tool for android.
considering iac is interesting because prior evaluations typically showed a deteriorating precision when iac benchmark cases were involved.
we thus seek to specifically measure how well tools perform on more accurate iac benchmarks.
we briefly describe some characteristics of the tools analysis engine all tools except amandroid are based on soot and operate on jimple as intermediate language.
4instructions are available on github and sink identification the sources and sinks considered by dialdroid didfail flowdroid andiccta are specified by susi a machine learning approach for source and sink detection.
droidsafe employs its own source and sink identification which is claimed to be even more precise than susi s .
the list of considered sources and sinks used by amandroid seems similar although shorter its origin remains unclear.
for our micro benchmarks we made sure that the sources and sinks needed for finding flows are identified by all tools.
icc and iac capabilities didfail dialdroid anddroidsafe are the only tools that are shipped with built in iac capabilities.
the other tools have icc capabilities only and require a tool called apkcombiner to lift their analysis to iac level.
apkcombiner has been developed along with iccta for precisely this purpose.
it takes multiple .apk files as input and merges them into a single .apk file.droidsafe s built in iac capabilities did not show any effect in our experiments no iac involving flows were found.
hence we decided to use droidsafe in combination with apkcombiner as well.
note furthermore that dialdroid is only able to detect inter component or inter app taint flows any intra component flows are ignored.
a number of other tools e.g.
would fit into our scope.
we shortly comment on and provide reasons why we omitted them in related work see section .
.
benchmarks our experiments are based on three benchmark suites droidbench icc bench and dialdroid bench .
the first two are well known micro benchmark suites which have been used in various evaluations before usually version .
or .
of droidbench and version .
of icc bench which we use as well .
the third suite dialdroid bench is a collection of partially malicious real world apps downloaded from google s playstore and gathered by bosu et al.
.
in addition we have developed apps comprising positive and negative feature checking benchmark cases.
a featurechecking benchmark case exploits only one specific feature at a time and can thus be used to explicitly check the handling of a dedicated feature in a tool.
this is in contrast to similar cases of droidbench which often combines two or more features in one case.
the feature checking benchmark cases cover all features listed in section .
.
since we are particularly interested in icc and iac we have developed three apps to specifically evaluate the precision of intentmatching algorithms.
such algorithms play an essential role when inter component or inter app flows are analyzed.
the analysis has to detect whether a certain intent can be received by a component.
if so the action category and data attributes of an intent have to match those of a component s intent filter.
to this end the three apps comprise positive and negative cases considering matching action category and data attributes respectively.
together these newly developed apps represent our droidbench extension.
droidbench together with this extension is refined by means of brew .
as a result our benchmark suite nowdo android taint analysis tools keep their promises?
esec fse november lake buena vista fl usa contains a collection of apps with a their source code and b the ground truth for data leaks in the form of aql answers.
we made this extended and refined benchmark suite as well as refined versions of the other suites publicly available for other researchers to perform similar experiments.
.
research questions and experiments our evaluation addresses the following research questions rq1 do android app analysis tools keep their promises?
rq2 how do the tools compare to each other with respect to accuracy?
rq3 which tools support large scale analyses of real world apps?
we designed specific experiments for every research question.
rq1 .
in order to address rq1 we first need to determine what we consider to be a promise of a tool.
looking at the articles introducing tools two properties of tools play a common role the supported features and the tool s accuracy i.e.
its precision recall and f measure as shown in experiments.
runtime appears to play a minor role most articles only give vague runtime information.
to evaluate the tools with respect to these sorts of promises we ran the following experiments.
first we prepared a benchmark set consisting of the micro benchmarks from droidbench and icc bench plus our feature checking benchmark cases all refined and executed with brew .
to evaluate the six different tools brew is launched six times each time with the respectively configured underlying aql system.
each configuration makes the aql system use only one tool.
the setup of a tool is untouched only required launch parameters are given and the usable memory is specified.
all other options are set to default.
rq2 .
as a number of researchers have already carried out a comparison of their own tool with some existing tools we wanted to see what the outcome of a more refined comparison is.
for the comparison we chose f measure as our means for evaluating accuracy.
for each catergory and tool the average value is computed.
basically for comparison the rule the larger the better can be applied on the achieved value.
for evaluation we again used our refined version of droidbench .icc bench is not used since we do not want to intermix benchmark cases.
rq3 .
regarding scalability we seek to evaluate whether the tools are able to deal with large apps in terms of code size a large numbers of apps icc and iac and newer android versions.
for we used dialdroid bench a benchmark suite containing large real world apps.
again we employed brew to help us determine the ground truth for these apps.
due to the size of apps this is anything but straightforward as a simple manual inspection of all potential flows is not feasible.
we used the following procedure to nonetheless achieve a systematic derivation of a ground truth.
first we created one positive benchmark case for every pair of sources and sinks.
this resulted in potential positive benchmark cases.
second we ran all six tools on these cases ending with a report of candidates for privacy leaks.
of these potential leaks had been found by two tools.
no leak was found by more than three tools and there were six benchmark cases for which this occurred.
for all leaks that had been detected multiple times we manually investigated the source code of the associated apps.
we used the jadx decompiler to extract thetable feature promisesaliasing static callbacks lifecycle inter procedural inter class iac icc explicit icc implicit flowcontextfieldobjectpaththreadawareness reflection tool sensitivity amandroid dialdroid didfail droidsafe flowdroid iccta supported confirmed partially confirmed not confirmed aborted apps source code.
to inspect the decompiled code we then used android studio.
the manual inspection led to the confirmation of positive cases.
four other positive cases had to be rejected because there exists no data flow between the source accessing device s location and the sink logging e.g.
a username .
by means of brew these confirmed and rejected benchmark cases have been stored respectively as positive and negative cases in another refined version ofdialdroid bench .
we are aware that the determined ground truth is most likely incomplete and does not involve all apps of dialdroid bench .
however to our knowledge it represents the first available precise in terms of flows ground truth description for a set of real world apps.
in cooperation with others we plan to extend it in future.
for inspecting the tools abilities to handle a high number of apps icc and iac we used our own intent matching benchmarks.
for checking their applicability to newer android apis we ran the tools on different versions of our feature checking test apps compiled and developed with android studio as well as on the droidbench apps developed with eclipse.
.
execution environment all experiments were executed on a debian jessie virtual machine which has java .
.0 161 installed.
it was set up to use two cores of an intel xeon cpu e5 v3 .30ghz and gb memory whereof gb were assigned to the analysis tool as heap space of the respective java virtual machine.
experimental results .
rq1 do android app analysis tools keep their promises?
feature promises.
each tool promises to support a certain set of features see original paper and summary in surveys .
on the basis of our feature checking benchmark cases we verified which features are supported.
table summarizes the results.
each row stands for one tool each column represents one feature.
the entries describe the promises and their degree of fulfillment the symbol stands for fullsupport forpartial support and for all feature checking benchmark cases having been failed.
furthermore if the symbol is circled the corresponding feature was promised to be supported.
consequently a promise violation is denoted as a circled minus symbol .
in the table five promise violations appear.
we shortly describe the reasons for these.
amandroid failed to detect the correct order of statements causing it to falsely determine a taint flow.esec fse november lake buena vista fl usa felix pauck eric bodden and heike wehrheim dialdroid struggled in dealing with implicit intents for icc and iac.
in case of the test app for implicit icc its execution was aborted at the end due to a database error.
droidsafe failed to handle callbacks correctly.
this is surprising as the paper claims that due to its flow insensitivity it can handle callbacks more easily.
in addition droidsafe seems to over approximate in case of the context sensitivity check violating a second promise.
however its development has stopped in and the last supported android version is .
.
.
hence it is plausible that some features are no longer supported.
fordidfail we could not check whether it keeps its promises since it cannot handle newer apps see table nor apps that do not name its targeted android api version in its manifest.
to do so has become optional along with the establishment of android studio as dedicated development platform in .
in summary according to our feature checking benchmarks all promises except five are kept and most of the promised features are fully supported.
accuracy promises.
accuracy is typically evaluated by the metrics precision recall and f measure the harmonic mean of the first two.
for the sake of clarity and because it best represents the overall accuracy we only report the f measure here.
figure depicts the fmeasures for all six tools on different sets of micro benchmark apps.
we used different sets because the promises themselves typically refer to different benchmark suites.
the dark bar always represents thepromised value the brighter one the actual value determined in our experiments.
droidbench version .
figure ashows the f measure values for the current version .
of droidbench .
since no paper promised anything for the complete .
set there are no dark promised bars shown.
all tools have an accuracy of about apart from dialdroid anddidfail which have less.
does not sound to be an inspiring confidence but a lot of distinct features are exploited in droidbench .
specifically such features designed to challenge existing tools.
thus it was to be expected that each tool makes mistakes at some point.
furthermore we find that dialdroid has a very low value in figure aas well as in most of the others.
this is because dialdroid is designed for icc and iac cases only tracking no intra component flows and consequently fails in all other cases.
didfail only reached an f measure of .
fordroidbench .
.
this is because didfail is the oldest and fewest updated tool considered.
droidbench version .
most tools were proposed when droidbench .
did not exist hence an older version of droidbench was used.
the bar chart in figure bshows the promised and actual values for droidbench before version .
.
on this no tool achieved its promised value.
with a relative deviation of flowdroid is closest to its promise.
the other tools are at about to below the promised value.
all tools nevertheless achieved better values for this set than for the .
set.
droidbench version .
subset the results improve if we only take a certain subset of droidbench .
into account see figure c .
this subset has been used in the papers proposing flowdroid andamandroid .
however only flowdroid is able to keep its promise for this subset.
.
amandroid dialdroid didfail droidsafe flowdroid iccta promised actual .
.
.
.
.
.667f measurea droidbench .
.
amandroid dialdroid didfail droidsafe flowdroid iccta promised .
.
.
.
actual .
.
.
.
.
.735f measureb droidbench version .
.
amandroid dialdroid didfail droidsafe flowdroid iccta promised .
.
.
.
actual .
.
.
.
.889f measurec droidbench .
flowdroid amandroid subset .
amandroid dialdroid didfail droidsafe flowdroid iccta promised .
actual .
.
.
.
0f measured droidbench .
iac only .
amandroid dialdroid didfail droidsafe flowdroid iccta promised actual .
.
.
.
.
.533f measuree droidbench .
iac icc only .
amandroid dialdroid didfail droidsafe flowdroid iccta promised .
actual .
.
.19f measuref icc bench .
figure accuracy promises droidbench .
iac only bosu et al.
evaluated dialdroid for all iac cases of droidbench .
and claimed to achieve an fmeasure of .
see figure d .
in our experiments we could only reproduce an f measure of .625for the same subset.
nonetheless this is the best value for this subset.
all other tools could not even reach which makes them inappropriate.
as expected flowdroid was not able to detect any inter app flows.
unfortunately iccta was also unable to handle those although it should be as claimed in the associated paper and determined by the featurechecking benchmark.
a closer look at the individual results reveals that iccta could not resolve flows between setresult .. andonactivityresult .. statements.
the intra app parts of thedo android taint analysis tools keep their promises?
esec fse november lake buena vista fl usa table f measure scores id category dialdroid didfail droidsafe iccta flowdroid amandroid 1fieldandobjectsensitivity .
.
.
.
.
.
.
2callbacks .
.
.
.
.
.
.
3unreachablecode .
.
.
.
.
.
.
4androidspecific .
.
.
.
.
.
.
5generaljava .
.
.
.
.
.
.
6emulatordetection .
.
.
.
.
.
.
7lifecycle .
.
.
.
.
.
.
8intercomponentcommunication .
.
.
.
.
.
.
9threading .
.
.
.
.
.
.
arraysandlists .
.
.
.
.
.
.
aliasing .
.
.
.
.
.
.
interappcommunication .
.
.
.
.
.
.
reflection .
.
.
.
.
.
.
dynamicloading .
.
.
.
.
.
.
native .
.
.
.
.
.
.
implicitflows .
.
.
.
.
.
.
reflection icc .
.
.
.
.
.
.
selfmodification .
.
.
.
.
.
.
.
.
.
.
.
.
leak however could be found as well as the connection between the two apps involved.
it seems to be a tiny but decisive bug in iccta s implementation.
droidbench .
iac and icc only once we add all icc cases ofdroidbench all tools except dialdroid become more accurate on average see figure e .iccta improves the most and achieves an f measure of .
.
the best value is achieved by amandroid which overtook dialdroid for this extended subset.
flowdroid s value is not 0because some of droidbench s icc cases communicate values through static fields or use statements receiving or sending intents as only sources or sinks.
flowdroid can handle both.
icc bench finally we inspected icc bench see figure f .
for this micro benchmark set we have only three promises.
whereas amandroid almost keeps its promise dialdroid lacks .
iccta underperforms the most because of the reason discussed above.
to conclude we could not reproduce the accuracy that was claimed in the proposing papers apart from one promise made by arzt et al.
for flowdroid considering a small set of benchmark cases see figure c .
in consequence the answer to rq1 is thus that the tools in general keep only parts of their promises.
.
rq2 how do the tools compare to each other with respect to accuracy?
for the comparison of tools wrt.
accuracy we used droidbench .
and its specific categories .
table shows all categories of droidbench .
in its second column.
the following six columns show the f measure of each tool for all benchmark cases in the associated category.
the categories supported best are placed at the top of the table.
additionally a color scheme has been added to emphasize each tool s performance the darker the background of a cell is the higher the f measure.
we find that the first of categories are handled properly by most tools.
the f measure values achieved in category namely interappcommunication are inadequate.
according to the promises made the tools should be able to analyze inter app cases.
in particular dialdroid should achieve a top value here but it does not excel at an f measure of .
.
the remaining six categories are insufficiently handled by all tools apart from sometable dialdroid bench results toolnumber of successfully analysis time number of true analyzed apps per app minutes false positives amandroid dialdroid didfail droidsafe flowdroid iccta special cases.
however this matches our expectations since no tool claimed to be able to handle such cases.
before we conclude this section a few remarks regarding the runtime while most tools needed on average seconds to analyze one app droidsafe required more than and flowdroid less than seconds on average.
in addition droidsafe timed out in cases by exceeding a maximal execution time of minutes.
this also happened in case of didfail andiccta but only once.
in summary we see that there is no single best tool.
every tool has at least one other tool performing better in at least one category.
overall see sums in the final row amandroid .
andflowdroid .
score best wrt.
accuracy.
.
rq3 which tools support large scale analyses of real world apps?
table second column shows how many real world apps each tool was able to analyze without exceeding the maximal execution time of minutes.
the third column shows the average execution time of each tool the last column how many of expected results could be matched by each tool.
there is no tool that was able to analyze all apps and apart from flowdroid every tool on average required more than two minutes to analyze an app.
the results for the newly defined ground truth last column reveal that flowdroid currently seems to be the best choice to reliably deal with large apps since all other tools missed some confirmed leaks or falsely detected rejected ones.
in addition in case of amandroid dialdroid droidsafe andiccta most of the confirmed leaks remain undetected.
.
.
.
amandroid dialdroid didfail droidsafe flowdroid iccta precision .
.
.
.
.
.
recall .
.
.
.
.
.
f measure .
.
.
.
.
.
figure intent matching precision recall f measure table up to date status tooleclipse androidstudio api api api amandroid dialdroid didfail droidsafe flowdroid iccta apkcombiner supported fails crashesthe intent matching benchmark results further support this conclusion see figure .
apart from amandroid no tool is able to accurately match the action category or data field.
finally we investigated whether the tools areesec fse november lake buena vista fl usa felix pauck eric bodden and heike wehrheim able to handle newer android versions.
table shows the outcomes.
amandroid fails to perform a proper analysis however without crashing.
droidsafe iccta andapkcombiner all crash while analyzing apps built for an api above which is supported by the majority .
of android devices.5a common cause is a tool dependency on the apktool .
old versions of it fail to decompile newer android apps.
the same happens to the apkcombiner .
thereby amandroid droidsafe flowdroid andiccta lose their ability to analyze inter app scenarios.
as discussed before didfail fails to analyze newer apps developed with android studio.
in summary the answer to rq3 is each tool in our scope still has shortcomings when it comes to analyzing real world apps.
.
threats to validity the main threat to the validity of our experiments arises from the manual though tool assisted definition of the expected results.
however currently we see no way around it because the tools that could potentially be used to derive the ground truth are at the same time the tools we want to evaluate.
thus we cannot rely on a single tool to generate the ground truth.
moreover reprodroid allows us to refine the expected result definitions multiple times and thereby to achieve precise results.
similarly also other experts can use reprodroid to define and refine their own benchmarks.
in all our experiments the tools have been executed in their default configuration.
only the available memory has been changed according to our system s setup.
some tools may produce different results when executed with specific launch parameters.
these results may be more accurate or less computed faster or slower and might thus change the outcome of our experiments.
for example flowdroid has an option to activate the tracking of implicit flows.
consequently its f measure value would have been greater than in category see table .
we restricted our experiments to the default configuration nonetheless because this is the one which a non expert software developer is likely going to use.
another threat to validity are the metrics precision recall and fmeasure that we and others frequently used to measure for accuracy.
for some feature restricted parts of e.g.
droidbench these metrics are misleading.
the aliasing subset for example comprises four benchmark cases three of which are negative cases.
a tool capable of correctly answering these three negative cases will still have a precision recall and f measure value of if it just fails the single positive case.
finally our implementation of the aql system may contain bugs.
in particular the converter used to translate tool specific answers into aql answers must work as intended in order to produce correct and meaningful results.
we extensively tested the converter and fixed all errors.
due to the imprecise format of some tools results sources or sinks are sometimes more specifically fordidfail not uniquely identifiable while converting it into the aql format.
therefore the converter over approximates i.e.
all candidates are taken into account as source or sink respectively.
considering our experiments on real world apps brew similarly over approximated during the identification of sources and sinks in case of amandroid didfail anddroidsafe .
thus method calls are matched by method names without considering the parameters given as input.
such aspects have already been taken into consideration.
related work providing an overview of analysis tools for android apps is the topic of three recent surveys .
these works collect and summarize tools and their functionality as outlined in research papers.
they provide no systematic experimentation to assess and compare tools in particular not with respect to their promises.
a thorough comparison of android analysis tools has so far been difficult due to the lack of precisely defined benchmarks.
this situation is different in other analysis contexts.
competitions like the annual competition on software verification sv comp the sat solving competition6or the hardware model checking competition hmcc provide well defined benchmarks in different categories with precisely fixed outcomes.
often they do not just require participating tools to give yes no answers but in addition to provide witnesses or proofs of their results.
with the aql we already have a format for witnesses of taint flows available.
finally there are more tools which potentially fit in our scope.
apposcopy wechecker and separ should fit perfectly however are not publicly available.
scandroid is publicly available and fits into our scope as well nonetheless it is largely outdated and cannot produce results for any considered micro benchmark.
droidinfer employs an interesting type based approach.
however in this particular case it requires a lot of effort to build a converter to extract the determined taint flows because of its uncommon result structure.
additionally the tool seemed not ready for competitive comparison since its execution fails for most micro benchmark cases.
thus we decided to omit the tool.
other available tools do not fit into our scope due to their result representation even though they inspect privacy leaks.
for example horndroid determines sinks and provably shows that these can be reached by taint flows.
it fails however to name sources which is why we cannot determine which specific flow is found.
conclusion in this paper we reported on the results of a reproducibility study on static taint analysis tools for android apps.
to support our own as well as similar studies we developed a framework for inferring data leaks in test apps and for automatically running tools on benchmark sets.
with the help of this framework we assembled precise benchmark suites and re evaluated six existing tools on them.
in the evaluation we in particular studied the handling of specific features the accuracy of tools and their relation to the promised values.
the results indicate that studies and benchmarks like ours are indeed needed to provide a solid ground for a fair and unbiased comparison of tools.