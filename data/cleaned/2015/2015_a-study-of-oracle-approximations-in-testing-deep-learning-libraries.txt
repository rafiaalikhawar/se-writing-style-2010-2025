a study of oracle approximations in testing deep learning libraries mahdi nejadgholi jinqiu yang department of computer science and software engineering concordia university mnejadg jinqiuy encs.concordia.ca abstract due to the increasing popularity of deep learning dl applications testing dl libraries is becoming more and more important.
different from testing general software for which output is often asserted definitely e.g.
an output is compared with an oracle for equality testing deep learning libraries often requires to perform oracle approximations i.e.
the output is allowed to be within a restricted range of the oracle.
however oracle approximation practices have not been studied in prior empirical work that focuses on traditional testing practices.
the prevalence common practices maintenance and evolution challenges of oracle approximations remain unknown in literature.
in this work we study oracle approximation assertions implemented to test four popular dl libraries.
our study shows that there exists a non negligible portion of assertions that leverage oracle approximation in testing dl libraries.
also we identify the common sources of oracles on which oracle approximations are being performed through a comprehensive manual study.
moreover we find that developers frequently modify code related to oracle approximations i.e.
using a different approximation api modifying the oracle or the output from the code under test and using a different approximation threshold.
last we performed an in depth study to understand the reasons behind the evolution of oracle approximation assertions.
our findings reveal important maintenance challenges that developers may face when maintaining oracle approximation practices as code evolves in dl libraries.
index t erms software quality assurance software testing testing deep learning libraries test oracle i. i ntroduction deep learning dl techniques are widely applied to solve important real world problems such as image and voice recognition and autonomous driving cars.
due to the complexity and wide application of deep learning techniques practitioners build dl libraries to make dl techniques more accessible to application developers.
modern dl applications heavily depend on popular dl libraries such as tensorflow pytorch theano and keras .
the quality assurance practice of dl libraries is critical as it affects millions of applications that are built on top of them.
however there has been little attention to study the quality assurance practice in dl libraries i.e.
the test cases written by the developers of dl libraries .
such test cases guarantee the correctness of the implementations in dl libraries.
insufficient or low quality test cases affect the quality of dl libraries.
furthermore the generated models from dl libraries can be negatively impacted as well .
dl libraries especially thecore algorithm module heavily encode mathematical formulas and arithmetic operations.
the corresponding test cases in dl libraries have unique properties which are not studied by prior empirical studies on test cases .
testing dl libraries often requires oracle approximations oa for short instead of definitely asserting the equality between the output from code under test cut and the oracle.
oracle approximations are needed in dl libraries for various reasons.
the output of the implementations in dl libraries may slightly vary in each test run e.g.
due to randomness .
for the cases where test oracles are floating numbers it may be hard for testers to precisely define the oracle in code.
moreover the output from cut is allowed to be slightly different from a computed oracle such as when oracle is other dl implementations i.e.
differential testing .
oracle approximation is an essential testing practice adopted by dl libraries in implementing assertions.
below is an example of oa assertion from keras.
in the code snippet pis the defined oracle line .
the oa assertion line compares whether the mean of a sample from a binomial distribution i.e.
rand a n output from a keras function is close enough to the defined oracle p. the value that defines the accepted range is named as approximation threshold i.e.
.
in the code snippet below.
1def test random binomial self p .
... rand gets the output from cut which is a keras function that generates a binomial distribution assert np.
abs np.mean rand p .
oracle approximation opens new challenges to developers.
first developers need to select proper sources of oracles for oa assertions.
improper test oracles may introduce unstable test results i.e.
flaky tests.
during code evolution some sources of test oracles may be more fragile than the others and require more frequent updates i.e.
modifying test oracles or the accepted range of oracles .
second oracle approximations require developers to properly decide the range of accepted oracles.
if the range is too loose the test case would fail to check the correctness of the implementation.
if the range is too tight the test case would constantly fail and introduce false warnings of failing tests to developers.
last developers may encounter challenges in the maintenance and evolution of oa assertions.
for instance when testing the recurrent neural network rnn component in tensorflow developers use the output from keras as an oracle and implement the comparison using oracle approximation.
however in accommodating a 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
recent keras update tensorflow developers had to modify the approximation threshold i.e.
loosening the range to avoid constant test failures.
hence to better address the challenges it is important to understand the current oa practices in dl libraries.
in this work we take an important first step to study oa practices in dl libraries.
in particular we study the practice and evolution of oracle approximations in four popular dl libraries.
our study provides both quantitative and qualitative findings to conclude common practices and highlight maintenance challenges that can inspire future research to provide better tooling support to developers in adopting and maintaining oa practices and better utilize oracle approximations to guarantee the quality of dl libraries.
we answer the following four research questions rqs by studying four popular dl libraries i.e.
tensorflow pytorch theano and keras .
rq1 how many oracle approximation oa assertions are implemented in testing deep learning libraries?
we studied the prevalence of oa assertions in testing dl libraries.
we investigated the commonly used apis that developers use when implementing oa assertions.
we find that there exists a non negligible portion of oa assertions i.e.
to in the four studied dl libraries.
rq2 what are the common types of test oracles and thresholds used in oa assertions?
we performed a manual study to identify the common types of test oracles used in oa assertions.
we derived a systematic labeling system to guide this categorization process.
our study shows that a diverse set of oracle types are used in oaassertions.
developers often use computation based oracles such as output from other dl libraries or output from similar functions in the same dl library.
moreover through analyzing the thresholds used we find that developers often need to specify thresholds used instead of using the default ones provided by oa assertion apis.
rq3 what are code changes developers perform on oa assertions?
our study shows that developers frequently modify oa assertions because of code evolution.
on average of the modifications are about using a different oa assertion api .
are about tightening or loosening the thresholds and .
are about modifying cut or test oracles.
rq4 why developers perform changes on oa assertions?
we performed an in depth analysis on all the oa related commits of one test module in tensorflow i.e.
under kernel test directory.
in total we analyzed commits on oa assertions in depth including analyzing the code changes in the commits analyzing other relevant code reading the commit messages and executing test cases before and after the commits.
our findings reveal maintenance challenges developers may face when managing oa assertions.
paper organization.
section ii discusses the background of oracle approximations.
section iii describes the approaches we follow to answer the four rqs.
section iv presents the answers to the four rqs.
section v lists both internal andexternal threats.
section vi surveys the related work.
finally section vii concludes the paper.
ii.
b ackground on oracle approxima tions in this section we describe the background on oracle approximations especially the reasons that oa assertions are necessary in dl libraries.
there exists a non negligible difference between the true value of a mathematical function and the value that is calculated by the code implementation of the mathematical function.
we call such non negligible difference a numerical error .
the implementation of dl algorithms consists of encoding mathematical formulas and arithmetic operations in programming languages.
therefore dl libraries inevitably contain such numerical errors.
such numerical errors will be represented by oracle approximations in test cases.
atkinson and han elementary numerical analysis categorize five common numerical errors in arithmetic computations.
below we describe the five types of numerical errors in detail and their relevance to dl libraries the first three types of errors contribute to the analyzed oracle approximations in this work and the last two types do not.
machine representation error.
when representing floating numbers in computer there exists a difference between the true value and the value represented by computers.
due to the limited representation and use of rounding techniques this type of error widely exists in software that contains floating number computation.
dl libraries are no exception.
some of the oa assertions studied in this paper are related to this type of error.
mathematical approximation error.
errors occur because computers use estimation and approximation algorithms in the process of calculating certain functions such as differential equations and trigonometric functions.
such estimations introduce numerical errors.
an example of such approximation function in dl libraries is gradient function over tensors.
our study includes the oa assertions that are caused by this type of error.
defects in implementations.
this source of error is related to developer error in the process of calculation.
developers may introduce implementation defects in dl libraries which may lead to severe numerical errors in any generated dl models.
the oa assertions in our study could be related to this type of error.
modeling error.
this type of error occurs when there are uncertainties in the modeled mathematical relation of a phenomenon.
for instance in dl libraries there might be some unspecified implementation details in the distribution definition of the learning algorithms.
this type of error does not contribute to our studied oa assertions.
physical measurement error.
there might exist a difference between the physical reality of a phenomenon and the measured value.
such errors are typically introduced in data collection process and do not exist in dl libraries.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
therefore the analyzed oracle approximations in this work are not related to this type of error.
iii.
m ethodology in this section we provide details on the studied four dl libraries and describe the methodology we used to conduct the study on oracle approximations to answer the following four research questions rqs rq1 how many oracle approximation oa assertions are implemented in testing deep learning libraries?
rq2 what are the common types of test oracles and thresholds used in oa assertions?
rq3 what are code changes developers perform on oa assertions?
rq4 why developers perform changes on oa assertions?
a. studied systems table i sta tistics on the studied deep learning libraries .k eras is a python library .there is code of other languages e.g.
c in tensor flow theano and pytorch .
system version release date kloc studied python dev.
period tensorflow .
.
theano .
.
pytorch .
.
keras .
.
in total we include four deep learning libraries in the study i.e.
tensorflow theano pytorch and keras .
the selected four systems are commonly cited as the most popular dl libraries e.g.
the most starred and forked dl projects on github and most cited framework on arxiv and medium.1table i shows the statistics of the studied dl libraries.
for rq1 and rq2 we analyzed a recent stable version of each deep learning library see v ersion in table i .
for rq3 and rq4 i.e.
two rqs about code evolution we analyzed a development period for each library see studied dev.
period in table i .
the analyzed development history starts from the first commit of each studied dl library.
among the four analyzed systems keras is the only project that is in pure python because there is no computational core in keras structure.
however tensorflow pytorch and theano have a c c core and python api as their default interface2.
in these three frameworks there is a non negligible portion of non python core code i.e.
kloc c code in tensorflow and kloc c code in pytorch and kloc c code in theano.
our study focuses on python test cases i.e.
test cases written in python and does not include c c test cases.
however our study actually includes a comprehensive set of test cases that test non python code.
1for more information on metrics and results of deep learning framework popularity analysis please read 2for tensorflow there are other api in other languages such as java and go but we do not cover them in this study.based on tensorflow3and pytorch documentation4 python has been mentioned as the preferable api for development and testing.
in tensorflow and pytorch it remains as a common practice to use python wrappers to wrap c code e.g.
the computation core in tensorflow and further test the python wrappers using python test cases.
note that wrapping c c code in python there may or may not be new functionalities e.g.
computation data processing being added in the python code.
in short although we focus on test cases written in python our study actually includes test code for testing both python and c code in tensorflow and pytorch since their c code is tested by python test cases.
in the future we plan to expand the study of oracle approximations to other programming languages e.g.
java and c by analyzing systems such as microsoft cntk deeplearning4j and caffe.
b. identifying oa assertions rq1 and rq2 first we identify the assertion methods that developers use in the studied libraries to express oa assertions.
then we extract oa assertions by finding the usages of the identified oaassertion methods.
in particular we parse every test method in python into an abstract syntax tree ast iterate the ast and identify the assertion statements that use one of the identified oa assertion methods.
in python assertions are expressed in two ways assert keyword which is then followed by a boolean expression and using customized assertion apis e.g.
internally defined by each python project the python unittest built in functions and assertion apis provided by numpy i.e.
a commonlyused library in python that supports computation on arrays and matrices.
inherited from the above mentioned two ways oracle approximation assertions can be expressed using assert keyword which is followed by a boolean expression that performs relational operations particularly and and assertion functions that allow the expression of oracle approximations i.e.
oaassertion apis.
note that for we do not include notations such as and because we find that these greater notations are often about about inequality instead of approximations.
differently we find that and are often used to express oracle approximations such as absolute oracle output .
.
our procedure of identifying oa assertion apis for each studied library combines reading the official documentation of the libraries i.e.
tensorflow pytorch theano and keras to initiate regular expression queries such as assert close assert almost searching the initiated queries in each codebase and expanding the list of oa assertion apis by examining the search results.
the steps in our procedure is performed 3in documentation of tensorflow it has been mentioned that for testing new customized operation we usually do this in python for convenience .
therefore testing the computational core which is in c is actually implemented in python test cases.
4in pytorch s readme it has been explicitly mentioned that pytorch is not a python binding into a monolithic c framework.
it is built to be deeply integrated into python .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
iteratively until we are not able to find any new oa assertion apis in one studied library.
the two authors of this work confirmed that the resulting list of apis indeed expresses the oracle approximations.
our manual analysis in rq2 confirmed that the approach and heuristics we adapt to identify oa assertions are accurate more than of the identified oa assertions are true oa assertions.
c. identifying oracle type in oracle approximation assertions rq2 test oracles are defined differently e.g.
a defined oracle or output from an external library using differential testing .
understanding the commonly used oracle types in oracle approximation assertions would inspire future research to design better tooling support when oracle approximations are needed in assertions.
for example differential testing may use oracle approximations to allow a slight difference between one function from tensorflow and a similar function from keras.
developers would also need to specify the maximum value of such slight difference.
if the maximum allowance is set too large then the test case is weak in detecting regressions.
if the maximum allowance is set too small then it may require frequent changes as code evolves.
we manually examined the oracle type for every oracle approximation assertion in a statistically significant sample from each of the studied system.
in particular we analyzed oa assertions from tensorflow from pytorch from theano and from keras.
all the samples are taken with a confidence level of and a confidence interval of .
to derive a systematic labeling two of the authors independently perform analysis on a set of oa assertions from tensorflow and take notes about oracle types.
the two authors then discussed and agreed on a systematic labeling that consists of five main categories manually defined d differential testing dt a naive implementation ni a relevant internal function ri and the same code under test scut .
examples of the five categories will be provided when we present the results in section iv b. we use the following approach to perform the systematic labeling process.
given one oracle approximation assertion the following steps are used to label the oracle type identify the argument that represents a test oracle.
for all the approximation apis in table ii except for the category of assert and relational operations developers are expected to place the output from the code under test cut as the first argument and the test oracle as the second argument.
however in practice developers may switch the placement of the two arguments.
therefore we need to decide which argument is representing the test oracle by identifying the argument represents the output from cut by understanding the test code.
decide if the oracle is defined or through computation.
if the oracle is defined or assigned with numerical values e.g.
np.abs np.mean rand .
.
where .
is the defined oracle we labeled the oracle type as adefined oracle d .
developers may perform computation on the defined numerical values e.g.
calculating the mean of a data set and use the result as a test oracle instead of using the raw values.
for such cases we still labeled them as doracle type.
given an computation oracle decide whether the computation code is internal or external.
if the oracle is output from an external library e.g.
using numpy s result when testing tensorflow we labeled the oracle type as differential testing dt .
classify the oracle type into one of the three types i.e.
ni ri scut when the computation code is internal.
developers may provide a naive implementation i.e.
often in the test code and use the result from the naive implementation as a test oracle.
the naive implementation ni is similar to the code under test in terms of functionality but may lack of complexity and provide limited accuracy.
oracles of relevant internal code ri refer to the oracles that are computed by a relevant function in the same codebase e.g.
comparing the output from two methods in tensorflow .
we also find assertions in which developers use the output of the same code under test scut as the test oracle.
among thescut cases when used as test oracles the same code under test may or may not be invoked with the exactly same inputs and configurations.
we also label scut cases with a sub category label based on this distinction.
the systematic labeling is comprehensive as it covers whether the oracle is defined d or through computation ni dt ri andscut .
among the oracles from computation the labeling covers all the cases where the computation is from internal ni ri scut and external dt .
upon agreeing on the systematic labeling the two authors independently label the oracle types for the randomly sampled oa assertions and resolved the disagreements through discussions.
the two authors reached a cohen s kappa of .
in this systematic labeling process which is a substantial level of agreement.
d. categorizing the modifications on oracle approximation assertions rq3 in rq3 we investigate what are the changes developers perform on oa assertions and the distribution of such code changes.
we developed an automated solution to answer rq3.
the tool firstly extracts all the changes of oa from commit history.
each oa modification consists a pair of deleted and added lines of code both of which are on oa assertions.
after extracting the modifications the tool determines the category of each modification.
to extract oa modifications i.e.
a pair of added and deleted lines given one commit we utilized the abstract syntax trees asts that are generated from statements that differ in pre and post commit versions.
tokens in oa assertions are aligned for equality comparison and the top matched oa statements are extracted as one oa modification.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
in the second step the tool decides the category of oa modification by first deciding whether cosmetic or noncosmetic modifications then further dividing the non cosmetic modifications into three categories i.e.
api modification oracle cut modification and threshold modification.
cosmetic modifications are changes that only contain tabs spaces and newlines.
we identify cosmetic changes by asserting whether one added and one deleted lines are identical after removing spaces tabs and newlines.
for non cosmetic modifications the tool uses the syntactic position of the modified token in ast to decide whether the modification is to change api oracle cut or threshold.
for api modification the arguments of the added and deleted lines are the same but the assertion api has been changed.
for oracle cut modification the first two arguments of an oa assertion are modified.
due to the fact that developers might misplace oracle and cut we are not able to precisely distinguish the oracle change from cut change automatically.
for threshold modification developers may change the threshold used to control the comparison between the oracle and the output from cut i.e.
tightening the comparison by using a smaller threshold or loosening the comparison by using a larger threshold.
e. understanding the reasons behind oa assertions modifications through manual analysis rq4 understanding the context and reasons that developers perform modifications on oa assertions will provide insights to provide better tooling support to developers in maintenance activities.
to answer this rq we performed an in depth analysis on all the commits on one computation component in tensorflow i.e.
the kernel tests directory .
in particular we categorized the commits based on the effects of code changes and reasons behind.
the kernel tests directory in tensorflow contains all the test cases that test tensor operation functions5.
focusing on one directory allows us to conduct an in depth analysis since such analysis requires extensive understanding and knowledge on the test suites and source code in the study.
we leveraged the labels of rq3 in the first manual study of rq4.
the tool we developed for answering rq3 can label a commit as a cosmetic or a non cosmetic change.
we examined the cosmetic changes one by one to make sure all are indeed cosmetic changes.
for non cosmetic changes different from rq3 labels we further analyze the effect and possible reasons behind the change.
we combined reading commit messages source code and test file changes to understand the nature and root causes of the commits.
furthermore we reproduced some commits of each category i.e.
executing test cases in the versions of preand post commit to better understand the rationale of code changes performed by developers.
iv .
r esults this section presents the answers to the four research questions.
5more information about operations in tensorflow can be found at docs python tf operation.a.
rq1 how many oracle approximation oa assertions are implemented in testing deep learning libraries?
we followed the procedure that is described in section iii b to identify the oracle approximation assertion apis used in each of the studied dl library see table ii .
then we expanded the api list to include all the assertion apis used in each dl library.
last we counted the number of all assertions and the number of oa assertions for each dl library.
table ii presents the oa assertion apis used in each studied dl library the number of each api usage and also the source of the api i.e.
in which library the api is defined .
we categorize the oa assertion apis into four main categories.
the apis of absolute and relative tolerance use both absolute and relative thresholds to express the accepted range of oracles.
whether the assertions using these apis fail or not depends on the result of assert abs cut oracle atol rtol abs oracle in which cut is the output from code under test atol is absolute tolerance and rtol is relative tolerance.
the second category of oa apis uses absolute tolerance only for which the following condition is used to determine the result assert abs cut oracle atol .
the third category of oa assertion apis use rounding to assert the closeness between cut and oracle based on assert abs oracle cut .
decimal .
the argument decimal is used to specify the number of significant digits in cut and test oracle.
the last category of apis i.e.
error bounding do not directly compare cut and oracle but first calculates the difference between the two and then asserts whether the difference i.e.
error is smaller than a threshold such as assert error threshold .
our study shows that the oa assertion apis from numpy are commonly used across the studied dl libraries.
moreover in each dl library developers may define library specific apis to express oa assertions e.g.
of all the oa assertions in tensorflow .
both tensorflow and pytorch use the python built in assertion apis that are defined in unittest.testcase in tensorflow and in pytorch .
table iii shows the results of rq1.
in particular we show the number of oa assertions the number of all assertions the number of test cases with at least one oracle approximation assertion and the number of all test cases.
in summary there exists a non negligible portion of oa assertions in each studied dl library ranging from to .
if considering the test cases to of all the test cases contain at least one oracle approximation assertion.
we find that there exists a non negligible portion of oa assertions in dl libraries.
assertion apis from numpy are commonly used in tensorflow pytorch theano and keras to express oa assertions.
developers may heavily use customized functions to express oracle approximation assertion i.e.
in tensorflow .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii api s tha t express oracle approxima tions in each of the studied dl libraries .
category source methods tensorflow pytorch theano keras absolute and relative tolerencenumpy assert allclose actual desired rtol atol numpy assert allclose actual desired rtol atol numpy asserttrue isclose actual desired rtol atol keras assert list pairwise a atol tensorflow assertallclose actual desired rtol atol tensorflow assertallcloseaccordingtotype actual desired rtol atol absolute tolerencetensorflow assertnear actual desired atol tensorflow assertarraynear actual desired atol tensorflow assertndarraynear actual desired atol rounding toleranceunittest assertalmostequal actual desired decimal unittest assertalmostequals actual desired decimal numpy assert array almost equal actual desired decimal numpy assert almost equal actual desired decimal error boundingpython assert error threshold unittest asserttrue a b unittest assertless a b unittest assertlessequal a b numpy assert array less a b tensorflow assertalllessequal a b table iii sta tistics of oa assertions in the studied dl libraries .
subject assertions oa assertions perc.
test cases test cases w oa perc.
tensorflow pytorch theano keras b. rq2 what are the common types of test oracles and thresholds used in oa assertions?
we manually analyzed oa assertions from the studied dl libraries i.e.
tensorflow pytroch theano and keras.
the instances consist of random samples from all the oa assertions in the four studied libraries.
each random sample from each library achieves a confidence level of and a confidence interval of .
we labeled the oracle type in each oracle approximation assertion following the systematic labeling that is described in section iii c. also we recorded whether threshold that controls the approximation assertion is either default which means developers do not specify a certain threshold and use the default value in the assertion api or customized o r depend which means the exact value used as the threshold will be decided during runtime i.e.
depending on data types or hardware specifications .
table iv shows the results from our manual labeling on oracle types and approximation thresholds.
the systematic labeling contains five main categories.
defined oracle d .
developers often define the oracle e.g.
a floating point number and assign the defined to a variable that is then used in assertions.
in some cases developers may perform simple calculations on the defined oracles and suchcalculations are irrelevant to the code under test.
for such cases we also label them as defined oracle type.
we show an example of defined oracle from tensorflow below.
in the example below pis assigned with an array i.e.
using numpy line and is compared with the output from code under test in line .
our study shows that of oa assertions use defined test oracles.
test util.run in graph and eager modes 2def testlogits self p np.array dtype np.
float32 ... self .assertallclose p self .evaluate new p rtol 1e atol .
naive implementation ni .
the oracles are computed by a naive implementation which is often defined in test code.
the naive implementation provides a similar functionality with the code under test.
what differentiates ni from dis that defined oracles may involve computation but the computation is irrelevant to the code under test.
in the code snippet below the test oracle klexpected is computed through the simple implementation line to line .
1kl kullback leibler.kl divergence a b 2kl val sess.run kl provide a naive implementation to get kl expected 5a logits np.random.randn batch size categories 6b logits np.random.randn batch size categories 7prob a np softmax a logits 8prob b np softmax b logits 9kl expected np.
sum prob a np.log prob a np.log prob b axis 11self .assertallclose kl val kl expected differential testing dt .dt is commonly used to test systems when there exist a variety of implementations such authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iv oracle types and thresholds used in the oa assertions of the studied dl libraries .
subject defined naive differential relevant internal same code under test scut threshold oracle d impl.
ni testing dt function ri same input different inputs default customized depend tensorflow pytorch theano keras as compilers .
in the dt cases of testing dl libraries the oracles are computed from similar code in other dl libraries.
the code snippet below shows an example of dt that compares the output from theano s tensordot function line with numpy.tensordot line .
1c tensordot avec bmat axes 2f2 inplace func c 3aval rand 4bval rand 5utt.assert allclose np.tensordot aval bval axes f2 aval bval we find that of oa assertions utilize differential testing i.e.
using outputs of other libraries as test oracles.
particularly in theano there exists a significant portion ofoa assertions that use differential testing oracles and the percentage is much higher than the other dl libraries.
the relevancies inferred from differential testing practices can be used to improve test cases for both libraries invovled.
relevant internal function ri .
different from dt rirefers to the cases where the oracles are produced by relevant functions in the same codebase under test.
the relevant functions when invoked with certain arguments can be used to generate the oracles that are then compared with the code under test.
below is an example of ri category from tensorflow.
in particular y1 line and y2 line are computed from two relevant functions in tensorflow and then be compared for similarity line .
y1 nn ops.atrous conv2d y1 f rate padding padding y2 nn ops.conv2d y2 f strides padding padding y2 array ops.batch to space y2 crops pad block size rate self .assertallclose y1.
eval y2.
eval rtol 1e atol 1e in three of the four studied dl libraries ridoes not take a significant portion i.e.
of all oa assertions.
differently in theano riis a much larger portion i.e.
.
same code under test scut .
in some cases test oracles may be produced by invoking the same code under test with either the same inputs or different inputs.
in dl libraries the possible reasons include metamorphic testing i.e.
two different inputs may yield the same result from the same code under test or testing the randomness i.e.
two runs of the same under test with the same input may produce slightly different results .
thus we derive two sub categories under the category of scut based on whether the two sets of inputs i.e.
arguments are the same or not.
in total we labeled of the oa assertions with the oracle type of scut.
among them use the same input to generate the oraclesand use different inputs to generate the oracles.
we have found that in the scut cases with the same input the purpose of the developer is to assert whether an additional operation on cut can cause non tolerable error or not.
differently in scut cases with different inputs the concept is analogous to metamorphic testing.
such information potentially can be leveraged to improve automated test generation techniques in the future e.g.
using the inferred metamorphic relations.
1def test statefulness gru self model keras.models.sequential ... right padded input is being modified out6 model.predict left padded input ... left padded input is being modified out7 model.predict right padded input np.testing.assert allclose out7 out6 atol 1e table iv also shows how many of the analyzed cases use default runtime and customized thresholds to restrict the approximations.
our study shows that developers often need to specify a proper threshold when using oracle approximations i.e.
ranging from to in the studied systems.
for to studied cases developers need to use different thresholds in the same oracle approximation assertion based on runtime behaviors such as different data types e.g.
float16 float32 etc.
and whether cpu or gpu is configured in the test environment.
our study shows that there exists a diverse set of different test oracles and thresholds used in oa assertions.
our study also shows that there exists a significantly portion of test oracles are through computation.
this indicates that applying automated non oracle testing techniques e.g.
metamorphic testing differential testing should carefully consider and utilize oracle approximation practices.
c. rq3 what are code changes developers perform on oa assertions?
we developed an automated solution to answer this rq in section iii d. in particular the approach firstly determines whether a commit is to add oa assertions to delete the oa assertions or to modify the oa assertions.
among the modifications the approach classifies whether modifications is on oa assertion api the parameters cut oracle or the threshold.
table v shows the distribution of oracle approximation assertion commits on the above mentioned categories.
of all the commits involve changes at least one oracle approximation assertion.
the remaining rows show the number authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table v breakdown of code changes on oa assertions tensorflow pytorch theano keras total of commits total of commit oa the numbers below show the numbers of changes in each category.
total of changes oa of oa additions of oa deletions of cosmetic modi.
of non cosmetic modi.
... of api modi.
.
.
.
o f para.
modi.
... of thres.
modi.
of changes in each category.
one commit may consist of multiple changes.
among all the changes on oa assertions are adding new oa assertions e.g.
increasing the test coverage or modifying non oa assertions to oa assertions.
are deleting existing oa assertions.
the remaining changes are oa modifications.
among the oa modifications after removing cosmetic modifications i.e.
the non cosmetic modifications are further divided into three categories api modification on all non cosmetic changes parameter cut oracle modifications and threshold modifications .
figure shows three examples of the three detailed modification categories.
the first code snippet is an example of api modification from pytorch.
in this example developers modified the oa assertion api i.e.
using assertless instead of assertlessequal while keeping the other arguments unchanged.
the second code snippet is from keras in which both of the cut and oracle are modified.
in this case developers changed used a specific property of cut and oracle to compare for faster test execution.
the last code snippet is from tensorflow and the thresholds i.e.
both the absolute tolerance and relative tolerance are modified in order to reduce the flakiness of this assertion.
with the smaller thresholds i.e.
before this change the assertion is more likely to fail.
self .assertlessequal fn .data initial value self .assertless fn .data initial value an example of api modification from pytorch assert allclose out out2 atol 1e assert allclose np.squeeze out np.squeeze out2 atol 1e an example of cut oracle modification from keras self .assertallclose sample mean analytic mean atol .
rtol .
self .assertallclose sample mean analytic mean atol .
rtol .
an example of threshold modification from tensorflow fig.
.
examples of the three modification categories api cut oracle and threshold .
6according to the commit message the assertion is flaky and developers decided to loosen the thresholds to avoid any failures.
we find that developers frequently change oa assertions i.e.
of all the commits.
among the code changes onoa assertions developers often perform non cosmetic modifications on oa assertions i.e.
up to of all changes.
developers can benefit from future tooling support to help them better manage code evolution on oa assertions.
d. rq4 why developers perform changes on oa assertions?
in rq3 we show that many code changes on oa assertions are to modify oracle or threshold.
in rq4 we analyzed all the commits on oracle approximations in one computation module in tensorflow to understand the reasons behind those changes.
table vi shows the results of our manual study on oarelated commits in kernel tests directory in tensorflow.
below we describe detailed analysis of each category and provide some examples.
table vi the reasons behind code changes of test cases in kernel tests directory of tensor flow reasons of modifications commits changes refactoring increasing test coverage threshold loosening cases threshold tightening cases data type related modifications hardware related modifications rollbacks unknown refactoring.
developers may perform refactoring to improve test cases e.g.
reducing the test execution time without sacrificing coverage.
we call such cases refactoring as they do not change the structure of test cases significantly.
below is a refactoring example from tensorflow.
the assertion line is modified following the change on a variable assignment line .
in this case developers change the size of the cut variable i.e.
from to to reduce the cost of running this test.
this refactoring change does not affect the threshold.
therefore although the oracle is changed the threshold remains unmodified.
v .
array ops.zeros x v .
array ops.zeros x ... yval is computed from v self .assertallclose i i yval rtol 1e self .assertallclose i i yval rtol 1e increasing test coverage.
in the development of dl libraries developers may add new code to provide new functionalities or improve the current features.
correspondingly test cases should be updated to test the new code.
developers may modify existing test cases to test new code by increasing the test coverage instead of adding brand new test cases.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
threshold modification.
we describe the categories under threshold modifications in detail.
we find that there are four reasons that developers need to perform threshold modifications.
hardware related modifications.
the output from cut may produce different results depending on whether gpu or cpu is configured.
often developers implement the two test scenarios in one test method and use different thresholds to differentiate the comparison with the same oracle.
for example as code evolves developers may add gpu support to certain functions the corresponding test cases are subject to change as well.
in total we find that in commits developers change threshold to accommodate testing in gpu environment.
below is an example of hardware related threshold modification.
the commit is from tensorflow and the commit message is implement gpu version of tf.determinant .
developers modified the configuration of the test case to include testing gpu line .
correspondingly the invoked test method comparedeterminantbase line would need to modify the threshold to accommodate such change.
in this example developers loosened the threshold i.e.
from the default value 1e to 5e .
1message implement gpu version of tf.determinant.
2def comparedeterminantbase ... self .assertallclose np ans out self .assertallclose np ans out atol 5e 6def comparedeterminant ... with self .test session with self .test session use gpu true self .
comparedeterminantbase matrix x linalg ops.matrix determinant matrix x data representation related modifications.
computation on floating numbers suffers from the imprecision of representing floating numbers in machines.
therefore the output from cut in dl libraries may return slightly different values in the same test method because of different data representations e.g.
float16 float32 etc.
in the code example below developers changed the assertallclose toassertallcloseaccordingtotype in test unsortedsegmentsumtest .
the latter oa assertion api has different default threshold values for different data types.
so developers use this oa assertion api to express different levels of approximation for different data types.
self .assertallclose np ans tf ans self .assertallcloseaccordingtotype np ans tf ans we reproduced this commit logged the value of tf ans executed the test case times and concluded that unsortedsegmentsum indeed generates different values for different data types.
loosening the threshold.
in total we reproduced and analyzed commits in this category.
when one oaassertion fails developers may choose to relax the threshold.
the reason was reflected in many commit messages that contains words such as flaky test or flakiness .
for instance the commit message of the code change belowis increase tolerance in flaky multinomial test but there is a problem with multinomial distribution test and their solution is to make the function less sensitive to system error .
to void such flaky tests developers may loosen the threshold i.e.
atol is changed from to .
in the code snippet below.
self .assertallclose sample mean analytic mean atol .
rtol .
... self .assertallclose sample mean analytic mean atol .
rtol .
below we show another example in the category of loosening threshold.
developers modified totally invocations of assertallclose and used loosened thresholds to eliminate the flakiness of the tests.
the commit message is reduce flakiness of tf.distributions tests by tweaking the tolerances.
.
self .assertallclose sample mean analytic mean atol .
rtol .
self .assertallclose sample cov analytic cov atol .
rtol .
self .assertallclose sample var analytic var atol .
rtol .
self .assertallclose sample stddev analytic stddev atol .
rtol .
self .assertallclose sample mean analytic mean atol .
rtol .
self .assertallclose sample cov analytic cov atol .
rtol .
self .assertallclose sample var analytic var atol .
rtol .
self .assertallclose sample stddev analytic stddev atol .
rtol .
we reproduced the commit above and logged relevant values to show the flakiness before the commit and also how the flakiness is reduced after the commit.
there are four oa assertions in the code example above.
the oa assertion api assertallclose uses both relative and absolute tolerances to formulate the accepted difference between an output of cut and an oracle.
to visualize how close the oa assertions are to test failures we formulated safe margin function as safe margin oa atol rtol oracle abs cut oracle where atol andrtol are respectively absolute and relative tolerance and cut and oracle represent the output from the cut and test oracle.
if the safe margin value becomes smaller than zero then there will be an unacceptable difference between cut and oracle and therefore the assertion will fail.
figure visualizes the histogram of the safe margin values in runs of the four invocations of assertallclose .
the figure includes two sets of data one for the pre commit version red and one for the post commit version green .
the unsafe margins are highlighted in gray which means that if any of the red or green lines falls into the gray area the corresponding assertion would fail.
figure shows that after the commit i.e.
loosening the threshold two properties of the safe margin authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
the safe margins of the four oa assertions in two versions before red and after green the threshold modification commit.
the gray area is where the test would fail if any sage margin value falls into it.
distributions are changed.
first none of the post commit values falls into the range of test failures i.e.
the gray block on the left .
second the variance of post commit values is lower i.e.
less scattered in the figure because the developers removed the use of relative tolerance in the commit.
before the commit of loosening the threshold many safe margin values fall into the test failure range i.e.
the gray area in the figure .
the assertions are flaky and may fail in some runs.
hence developers manually loosened the threshold to avoid the test failures.
after this commit i.e.
the green lines the assertions become less sensitive to variance and thus less flaky.
we find that oa assertions are often modified as code evolves.
developers modify oa assertions for various reasons such as to increase test cover age to accommodate different hardware specifications to support different data representations or to avoid test failures.
our study indicates challenges on maintaining oa assertions as there is no systematic support to help developers manage oa assertions e.g.
detecting improper threshold values and suggesting changes on oa assertions when code evolves.
v. t hrea ts to validity internal validity.
studying oracle approximation practice requires a list of oa assertion apis that developers use in each studied library.
in this work we use an iterative approach that combines reading documentation and code search.
it is possible that we might miss some oa assertion apis that are rarely used in the codebases as we do not use an automated method.
however we believe that it should minimal impacts on the findings due to the rarity.
in rq2 the manual categorization on oracle types is partially subjective in some of the computation oracles i.e.
relevant internal implementation and native implementation.
to reduce the subjective bias in this process we have two people concluding the labels independently and discuss to reach an agreement.
in addition when detecting code changes onoa assertions we rely on the fact that the modifications i.e.
lines are performed directly on the statments of oaassertions apis.
hence we are not able to detect changes if the changes are performed on the variables and the variables are used in oa assertions.
external validity.
first we focus on studying dl libraries in python or the python part in a multi language library in this work.
our findings may not be generalizable to dl libraries of other programming languages.
in the future we plan to extend the study to include other languages such as c and java.
second our findings are based on the four studied dl libraries.
in the future we plan to include more dl implementations including the ones in different languages.
third when analyzing the reasons behind the code changes onoa assertions because such analysis would require much manual effort and understanding on the relevant code base we limit the study to one test module in tensorflow.
in the future we plan to expand the scope of this rq significantly to reveal different maintenance challenges developers might have when testing different components in dl libraries.
vi.
r ela ted work studies on testing scientific software.
there exist some efforts in literature that try to bridge the gap between software engineering and scientific software.
particularly researchers from both communities start to realize the importance of systematic testing on the developed scientific programs.
software defects cause inaccurate results instead of models and algorithms and may lead to publication retraction .
hannay et al.
studied the development process of scientific software and revealed that despite a consensus on the importance of testing only a small number of scientists have sufficient knowledge on testing.
carver et al.
perform case studies on the development of five scientific software and find that developers often find it challenging to perform validation and verification due to difficulty of writing good test cases.
however it remains unknown how developers actually conduct testing in scientific software i.e.
detailed analysis on test cases.
hence our work takes an important step to understand the testing practice in open source dl libraries which have many similarities with traditional scientific software.
moreover although testing scientific software often requires oracle approximation there is no prior work that focuses on oracle approximation practices in scientific software.
researchers have proposed various testing techniques to alleviate the oracle problem in testing scientific software such as metamorphic testing and property testing .
interesting our study highlights that when using non oracle testing to test dl libraries such as metamorphic testing and differential testing oracle approximations are commonly used.
our findings indicate that future adaption of automated nonoracle testing techniques in dl libraries should consider the prevalence of oracle approximations to avoid flaky test or overrestricted oracle comparison.
studies on software testing.
researchers perform empirical studies on practices of test cases to better understand the challenges of software testing.
v ahabzadeh et al.
study authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the prevalence and reasons of bugs in test code.
barr et al.
survey oracle problems in the literature.
zaidman et al.
study the co evolution of source code classes and test cases.
pinto et al.
work on understanding common myths on test suite evolution.
their study find that although most test case changes are about refactoring deletion and addition of test cases there are some changes that complex and are hard to automate.
beller et al.
work on understanding why developers do not perform test in ides.
luo et al.
conduct the study to understand why flaky tests occur.
beller et al.
empirically study why ci tests fail by analyzing travis ci and github.
prior studies on software testing focus on general software and do not consider the peculiarity of dl libraries.
in this work we present the first important step to understand how developers perform oracle approximations in dl libraries which is not studied by prior work.
testing deep learning models and deep learning libraries.
in recent years there have been much research effort paid to improve testing dl models.
new coverage metrics targeting at dl models are proposed.
automated testing techniques are proposed to improve the testing of dl models.
pei et al.
present the first white box testing of dl models and utilize a new coverage metric to guide the white box fuzzing.
tian et al.
propose to utilize fuzz testing i.e.
fuzzing images to generate more test data for autonomous driving cars.
more recently more advanced coverage metrics are proposed to improve testing dl models.
moreover ma et al.
propose to use mutation testing to evaluate the effectiveness of test cases on dl models.
differently our work focuses on studying testing practice in dl libraries and the quality will affect the accuracy of the generated models.
some recent efforts are spent to improve the quality of dl libraries and applications.
zhang et al.
performed an empirical study on open source dl applications on github and categorized totally bugs.
hung et al.
propose a novel technique to detect defects in dl libraries based on inconsistencies among different dl libraries.
their work leverages the differences in the generated dl models which are high level outputs while our work examines tests at all levels from the perspective of oracle approximation practice.
it remains as future work to examine how low level oracle approximations if not done properly may affect the accuracy of high level models.
vii.
c onclusions in this paper we present an empirical study on oracle approximations in testing dl libraries.
our study is an important first step to understand the current practices of using oracle approximations in compute intensive software such as dl libraries.
our work answers four research questions.
first we study the prevalence of oracle approximations in the test cases of dl libraries.
we find that up to of all the assertions use oracle approximations.
second we study and conclude the diversity of test oracles and thresholds used in oracle approximations.
in many cases oracles used in oracle approximations are obtained through computation.
third westudy the common code changes that developers perform on oracle approximation assertions.
last we conclude the reasons behind the code changes on oracle approximations.
our findings reveal maintenances challenges developers may be faced with in oracle approximations and may inspire future research to provide better tooling support for developers to better manage oracle approximation practices.