deepgauge multi granularity testing criteria for deep learning systems lei ma1 felix juefei xu2 fuyuan zhang3 jiyuan sun4 minhui xue3 b ol i5 chunyang chen6 ting su3 l il i6 yang liu3 jianjun zhao4 and yadong wang1 1harbininstituteoftechnology china2carnegiemellonuniversity usa3nanyangtechnologicaluniversity singapore 4kyushu university japan5university of illinois at urbana champaign usa6monash university australia abstract deeplearning dl definesanewdata drivenprogrammingparadigmthatconstructstheinternalsystemlogicofacraftedneuron networkthroughasetoftrainingdata.wehaveseenwideadoptionofdlinmanysafety criticalscenarios.however aplethora ofstudieshaveshownthatthestate of the artdlsystemssuffer fromvariousvulnerabilitieswhichcanleadtosevereconsequences when applied to real world applications.
currently the testing adequacy of a dl system is usually measured by the accuracy of test data.
considering the limitation of accessible high quality test data goodaccuracyperformanceontestdatacanhardlyprovide confidencetothetestingadequacyandgeneralityofdlsystems.
unlike traditional software systems that have clear and controllable logic and functionality the lack of interpretability in a dl system makes system analysis and defect detection difficult which couldpotentiallyhinderitsreal worlddeployment.inthispaper wepropose deepgauge asetofmulti granularitytestingcriteria for dl systems which aims at rendering a multi faceted portrayal of the testbed.
the in depth evaluation of our proposed testing criteriaisdemonstratedontwowell knowndatasets fivedlsystems andwithfourstate of the artadversarialattacktechniques against dl.the potential usefulness of deepgauge sheds lighton the construction of more generic and robust dl systems.
ccs concepts software and its engineering software testing and debugging theory of computation adversarial learning keywords deep learning software testing deep neural networks testing criteria acm reference format leima felixjuefei xu fuyuanzhang jiyuansun minhuixue boli chunyang chen ting su li li yang liu jianjun zhao and yadong wang.
.
deepgauge multi granularity testing criteria for deep learning systems.
lei ma is the corresponding author.
email malei hit.edu.cn.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september montpellier france association for computing machinery.
acm isbn ... .
of the 33rd acm ieee international conference on automated software engineering ase september montpellier france.acm newyork ny usa 12pages.
introduction deeplearning dl systemshavegainedgreatpopularityinvarious applications e.g.
speechprocessing medical diagnostics image processing and robotics .
a deep neural network dnn as a type of deep learning systems is the key driving force behind recent success.
however dnn based software systems such as autonomous driving often exhibit erroneous behaviors that lead to fatal consequences.
for example several accidents havebeenreportedduetoautonomousvehicle sfailuretohandle unexpected corner case driving conditions.
one of the trending research areas is to investigate the cause of vulnerabilityindlsystemsbymeansofgeneratingadversarialtest examplesforimage andvideo baseddlsystems.suchcarefully learned pixel level perturbations imperceptible to human eyes can cause the dl based classification system to output completely wrongdecisionswithhighconfidence .eversincetheinception of adversarial attacks on the dl systems more and more research hasbeendedicatedtobuildingupstrongattackers .as a consequence betterdefense mechanisms in dl systemsagainst adversarialattacksareindireneed.varioustechniquestonullify adversarialattacksandtotrainamorerobustdlsystemareemerginginrecentstudies .together research inbothrealmsformsavirtuouscircleandblazesatrailforbetter understandingofhowtobuildmoregenericandrobustdlsystems.
however whatisstilllackingisasystematicwayofgaugingthe testingadequacyofgivendlsystems.currentstudiesfocusonly on pursuing high accuracy of dl systems as a testing criterion for which we show several caveats as follows.
first measuring the software quality from dl output alone is superficial in the sense thatfundamentalunderstandingofthedlinternalneuronactivitiesand network behaviors is not touched upon.
we agree that it could be an indicator of dl system quality and generality but it is far fromcomplete andoftentimesunreliable.
second acriterionsolely basedondloutputwillrelyheavilyonhowrepresentativethetest dataare.havingachievedhigh performancedloutputdoesnot necessarily mean that the system is utmost generic and achieving low performancedoesnotindicatetheoppositeeither.adlmodelcanbeimmunetomanyknowntypesofadversarialattacks butmayfailfromunseenattacks.thisisbecausesuchacriterionbasedonly on dl outputs is far from being comprehensive and it leaves high risks for currently cocooned dl systems to be deployed in the realworld environment where newly evolved adversarial attacks are authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france ma xu zhang sun xue li chen su li liu zhao and wang inescapable.
third anydlsystemthatpassessystematictesting should be able to withstand all types of adversarial attacks to some extent.
such generality upon various attacks is of vital importance for dl systems to be deployed.
but apparently this is not the case unless we stick to a set of more comprehensive gauging criteria.
we understand that even the most comprehensive gauging criteria wouldnotbeabletoentirelyeliminaterisksfromadversarialattacks.
nevertheless byenforcingasuitablesetoftestingcriteria wehope thatadlsystemcouldbebettertestedtofacilitatetheconstruction of a more generic and robust deep learning system.
towardsaddressingtheaforementionedlimitations asetoftesting criteria is needed as opposed to the sole criterion based ondl decision output.
in addition to being scalable the proposed criteriawillhavetomonitorandgaugetheneuronactivitiesand intrinsicnetwork connectivityat variousgranularitylevels sothat a multi faceted in depth portrayal of the dl system and testing quality measures become desirable.
in this work we are probing this problem from a software engineeringandsoftwaretestingperspective.atahighlevel erroneous behaviorsappearedindnnsareanalogoustologicbugsintraditional software.
however these two types of software are fundamentally different in their designs.
traditional software represents itslogicascontrolflowscraftedbyhumanknowledge whileadnn characterizes its behaviors by the weights of neuron edges and the nonlinear activation functions determined by the training data .
therefore detectingerroneousbehaviorsindnnsisdifferentfrom detectingthoseintraditionalsoftwareinnature whichnecessitates novel test generation approaches.
to achieve this goal the very first step is to precisely define a set of suitable coverage criteria which can guide test design and evaluate test quality.despite a number of criteria existingfor traditional software e.g.
statement branch data flow coverage they completely lose effect in testing dnns.
to the best of our knowledge thedesignoftestingcoveragecriteriafordnnsisstillattheearlystage .withoutacomprehensivesetofcriteria designing tests to cover different learned logics and rules of dnns isdifficulttoachieve.consequently erroneousbehaviorsmaybe missed evaluatingtestqualityisbiased andtheconfidenceof obtained testing results may be overestimated.
in this paper we proposedeepgauge asetoftestingcriteriabasedonmulti level and granularitycoveragefortestingdnnsandmeasurethetesting quality.
our contributions are summarized as follows our proposed criteria facilitate the understanding of dnns as well as the test data quality from different levels and angles.
ingeneral wefinddefectscouldpotentiallydistributeonboth major function regions as well as the corner case regions of dnns.
given a set of inputs our criteria could measure to what extentitcoversthemainfunctionalityandthecornercasesofthe neurons wheredldefectscouldincur.ourevaluationresults revealthattheexistingtestdataofagivendlingeneralskew more towards testing the major function region with relatively few cases covering the corner case region.
in line with existing test data of dnns we evaluate the usefulnessofourcoveragecriteriaasindicatorstoquantifydefect detection ability of test data on dnns through generating newadversarial test data using well known adversarial data generation algorithms i.e.
fast gradient sign method fgsm basic iterative method bim jacobian based saliency map attack jsma and carlini wagner attack cw .
the extensive evaluation shows that our criteria can effectively capturethedifferencebetweentheoriginaltestdataandadversarial examples where dnns could and could not correctly recognize respectively demonstratingthatahighercoverageofourcriteriapotentiallyindicatesahigherchancetodetectthednn sdefects.
the various criteria proposed behave differently on dnns w.r.t.
networkcomplexityanddatasetunderanalysis.altogether thesecriteriacanpotentiallyhelpusgaininsightsoftestingdnns.byprovidingtheseinsights wehopethatbothsoftwareengineering andmachinelearningcommunitiescanbenefitfromapplying new criteria for gauging the testing quality of the dnns to gain confidence towards constructinggeneric androbust dlsystems.
to the best of our knowledge this is among the earliest studies to propose multi granularity testing criteria for dl systems which are mirrored by the test coverage in traditional software testing.
preliminaries in this section we first introduce traditional software and then deeplearningsystemsofwhichthearchitecturalfeaturesappearing to be a step above current traditional software.
we will see thatdl fundamentally changes the software development paradigm.
precisely wetrytoanalogizethattheprogramminglanguagelogic execution to traditional software is what the connectivity strength weights to a dnn.
as we will see below we are attempting to connectthesetwocounterpartsaswellasdiscussingthedifferences.
.
coverage criteria in traditional software testing weregardtraditionalsoftwareasanyprogramwritteninhigh level programminglanguages e.g.
c c java python .specially each statement in traditional program performs some certain operation that eithertransforms theoutputs fromthe previousstatement to the next one or changes the program states e.g.
assign new values tovariables .
softwaredefects bugs canbeintroducedbydevelopers due to incorrect implementation which may cause unexpected outputs or even fail stop errors e.g.
program crashes .
to detect defects software testing is one of the most widely adoptedsoftwarevalidationtechniquesinsoftwareindustry given asetoftestdata itfeedsthesetestdataasinputstoprogramand validates the correctness of the program s run time behavior by comparingtheactualoutputswithexpectedones testoracles and measurestestadequacybyusing coveragecriteria theimportant practical measures to quantify the degree to which the software is tested .
the program with higher test coverage often suggests that it has a lower chance of containing defects.
many software testingstandardsrequireasoftwareproducttobethoroughlytested withhightestcoveragebeforeshipment whichisusedasanindicator and confidence of the software quality.
on some safety critical systems the requirement of some form of test coverage is even .
for example ecss e st 40c standards demand statementcoverageofthesoftwareundertestfortwocriticallevels.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deepgauge multi granularity testing criteria for deep learning systems ase september montpellier france for traditional software a number of coverage criteria have alreadybeendefinedatdifferentlevels toanalyzethesoftwareruntimebehaviorfromdifferentperspectives i.e.
codelevel e.g.
statement branch data flowcoverageandmutationtesting ormodel level e.g.
stateandtransitioncoverage tocater for different testing methods and granularities.
some commonly used test coverage criteria are listed as follows statementcoveragemeasureswhethereachinstructionhasbeen executed and branch coverage focuses on whether each branch of control structure e.g.
in iforswitch case statements has been covered both of which are control flow based criteria.
data flow coverage enforces the coverageof each variable definition and its uses to detect data flow anomalies.
model basedcoveragecriteria aimtocovermoreprogram behaviors via abstracted behavior models.
other comprehensive variants of test coverage could be referred to .
however none of these criteria can be directly applied to test dnns due to its unique architecture as explained below.
.
deep neural network architecture inourpaper weregardadlsystemasanysoftwaresystemthat includes one or more dnns.1unlike traditional software programmedwithdeterministicalgorithmsbydevelopers dnnsare programmedbythetrainingdata selectedfeatures andnetwork structures e.g.
number of layers .
specially a dnn consists ofmultiple interconnected neurons organized on layers the input layer the outputlayer and one or multiple hiddenlayers.
each neuronisacomputingunitthatcomputesitsoutputbyapplyingan activationfunction toitsinput.inclassicdnns eachneuronisfullyconnected with all neurons on the next layer and each edge has a weight which indicates the strength of the connections betweenneurons.
overall a dnn could be considered as a function that transformsagiveninputtotheoutput andthisfunctionisdecided by the aggregated effects from its computation units i.e.
neurons each of which contributes to the whole computation procedure.
figure1 a shows an example of a three layer dnn.
toaccomplishatask e.g.
predictionontheautonomousvehicles steeringanglebymonitoredimages dnnsaretrainedand programmedthroughalargesetoflabelledtrainingdata.however similartotraditionalsoftware dnnsmayalsocontaindefects e.g.
givewrongsteeringangles duetoincorrect incompletetraining data or even the wrongly stipulated run time programming i.e.
training procedure.forexample humananalystmayincludeerroneous and noisy data when collecting training data.
in such a case a given input data might be wrongly handled e.g.
classi fied predicted causing losses and even severe tragedies if theflawed dnns are deployed to safety critical systems e.g.
the recent tesla autonomous driving accident .
for the complex and high dimensional real world inputs it is almost impossible for human toensure all possible even corner case dataare included.
to systematicallytestanduncoverhiddendefectsofdnns itiscrucial 1in particular a dl system may either be entirely composed of dnns or have dnns as its core with extra software encapsulation.
in this paper we mostly focus on dnns since it is the core of a dl system and our methods could be extended to support general dl systems.
although the trainingprogramofthecurrentstate of the artdnnsarestillwrittenastraditionalsoftware the obtained dnn from the training program is fundamentally different in how the logic is encoded.
layer input layer layer hidden layer layer output layer n1 n2 n3n4 n5 n6 n7n8 n9 a all behaviors main behaviors k multisection coverage corner case behaviors boundary coverage erroneous behaviors b figure1 a anexampleofafullyconnecteddnn.
b behaviorsofdnnsandrelationsbetweendefinedcoveragecriteria the red points denote erroneous behaviors therein .
todefineasetofsuitablecoveragecriteriaforevaluatingthetest adequacyaswellasgaugingtheinternalcoveredstatesofdnns to gain confidence on the testing results of dnns.
coverage criteria for testing dl systems fortraditionalsoftwaretesting developersdesignandseekasetof representativetestdatafromthewholelargeinputspace hoping thattheselectedtestdatacoulddetectthesoftwaredefectsunder limited computational resources.
testingcoveragecriteriaisproposedtoshatterandapproximate the software internal states.
it partitions the input space and establishes the relation of an input subspace and an approximatedsoftware internal state.
in this way compared with the test datafrom a single input subspace the same number of test data fromdifferent input sub spaces would have a higher chance to covermore diverse software states resulting in a higher possibility to detectmorediversesoftwaredefects.overthepastdecades asetof well designedcoveragecriteria e.g.
statementcoverage branch coverage havedemonstratedtheirpracticalvalue andarewidely adopted in software industry to systematically guide the testing processtounveilthesoftwaredefectsatdifferentlevels e.g.
unit level testingsmallsnippetsoffunctions.
integrationlevel testing multiplesub modulesorfunctionstochecktheirinteractions.
system level testing the software system as a whole.
thecurrentstate of the practicednntesting however isstillat itsearlystageandmainlyreliesonthepredictionaccuracy similar to black box system level testing that only observes inputs andits corresponding outputs lacking systematic testing coverage criteria for defect detection.
furthermore traditional software and dnns have obvious differences so existing coverage criteria for traditional software could not be directly applied to dnns.
in this section we design a set of dnn testing coverage criteria frommultiplelevels aimingtogaugethetestingadequacyofdnns and facilitate the detection of those erroneous behaviors from multipleportrayals.tobeusefultowardsindustrylevelapplications webelievethatthetestcriteriashouldbesimple scalableaswell general enough to be applied to a large range of dnns without confining onspecific dnn structureor activation functions.conceptually similar to traditional software the behaviors of dnns canbedividedintotwocategories i.e.
majorfunctionbehaviors 3theinputspaceofasoftwarecouldbesolargethatitisoftenimpossibletoenumerateandtestall the possibilities given limited computation resource.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france ma xu zhang sun xue li chen su li liu zhao and wang andcorner casebehaviors bothofwhichmaycontainerroneous behaviors seefigure b andourevaluationresultsinsection .
wehavetakenthesefactorsintoconsiderationduringthedesign of coverage criteria.
letn n1 n2 ... be a set of neurons of a dnn.
let t x1 x2 ... be aset oftestinputs.
we use x n todenote afunctionthatreturnstheoutputofaneuron n nunderagiventest inputx t.4let the dnn have llayers and lidenote the set of neurons on the i th layer i l .
.
neuron level coverage criteria at the neuron level we use the output values of neuron ndetermined from the training to characterize its behaviors.
since the internal logic of a dnn is mostly programmed by training data intuitively thefunctionality i.e.
neuronoutput foreachneuronof adnnshouldfollowsomestatisticaldistributionthatislargelydetermined by the training data.
the output distribution of a neuron obtained from training data analysis would allow to approximately characterizethemajorfunctionregionswhoseoutputvaluesare oftentriggeredbyinputdatawithasimilarstatisticaldistributionto the training data and the corner cases whose output values rarely occur.however forapractical sizeddnn obtaininganaccurate outputdistributionfor each neuronwouldbecomputationallyintensive.
with the similar spirit while being scalable we leverage the neuron output value boundaries obtained from training data to approximate the major function region and corner case region.
specially for a neuron n lethighnandlownbe its upper and lower boundary output values respectively on the value range of itsactivationfunction where highnandlownarederivedfromthe training dataset analysis.
we refer to as the major function region of a neuron n. definition .
.
for a test input x t we say that a dnn is locatedinits majorfunctionregion givenxiff n n x n .
toexhaustivelycoverthemajorfunctionregions wepartition intoksections andrequireeachofthemtobecoveredbythetestinputs.wenamethiscoverageas k multisection neuron coverage.
i k multisection neuron coverage .
given a neuron n thekmultisection neuron coverage measures how thoroughly the given set of test inputs tcovers the range .
to quantify this we divide the range intokequal sections i.e.
k multisections for k .
we write sn ito denote the set of values in thei th section for i k. if x n sn i we say the i th section is covered by the test inputx.therefore foragiven setof testinputs tandthe neuron n itsk multisection neuron coverage is defined as the ratio of the number of sections covered by tand the total number of sections i.e.
kin our definition.
we define the k multisection coverage of a neuronnas sn i x t x n sn i k. 4thispaperfocusesonfeedforwardneuralnetworks.forrecurrentneuralnetworks rnns wecan unrollacertaindepthoflayersofanrnnandadapt x n bysetting xtobeaninputsequence.we further define the k multisection neuron coverage of a dnn as kmncov t k summationtext.
n n sn i x t x n sn i k n .
however for a neuron n there are also cases where x n may locate out of i.e.
x n lown or x n highn .wereferto lown highn asthecornercase region of a neuron n. definition .
.
for a test input x t we say that a dnn is located in its corner case region givenxiff n n x n lown highn .
note that the profiled outputs of a neuron obtained from the training data would not locate into the corner case region.in other words if test inputs follow a similar statistical distribution with the training data a neuron output would rarely locate in cornercase region as well.
nevertheless it does not mean that testing the corner cases of a neuron is not important because defects of dnns could also locate in the corner case regions see section .
.
to cover these corner case regions of dnns we define two coveragecriteria i.e.
neuronboundarycoverageandstrongneuronactivationcoverage.givenatestinput x if x n belongsto lown or highn we say the corresponding corner case regioniscovered.toquantifythis wefirstdefinethenumberof covered corner case regions as follows uppercornerneuron n n x t x n highn lowercornerneuron n n x t x n lown .
ii neuron boundary coverage .
neuron boundary coverage measureshowmanycorner caseregions w.r.t.bothoftheupper boundary and the lower boundary values have been covered by thegiventestinputset t.itisdefinedastheratioofthenumberof coveredcornercasesandthetotalnumberofcornercases n nbcov t uppercornerneuron lowercornerneuron n .
somerecentresearchondnnsinterpretabilityempiricallyshows thatthehyperactiveneuronsmightpotentiallydeliverusefullearning patterns within dnns .
based on this intuition the proposed coverage criteria in the rest of this section focus more on the hyperactive neuron cases e.g.
top kneuron coverage in the next subsection .
similar to neuron boundary coverage we further define strong neuron activation coverage to measure the coverage status of upper corner cases.
iii strongneuronactivationcoverage .strongneuronactivation coverage measures how many corner cases w.r.t.the upper boundary value highn have been covered by the given test inputs t. it is defined as the ratio of the number of covered upper corner cases and the total number of corner cases n snacov t uppercornerneuron n .
.
layer level coverage criteria atlayer level weusethetophyperactiveneuronsandtheircombinations or the sequences to characterize the behaviorsof a dnn.
foragiventestinput xandneurons n1andn2onthesamelayer we sayn1is more active than n2givenxif x n1 x n2 .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deepgauge multi granularity testing criteria for deep learning systems ase september montpellier france for thei th layer we use topk x i to denote the neurons that have the largest koutputs on that layer given x. for example in figure1 a assume x n1 and x n3 are larger than x n2 the top neurons on layer are n1andn3 depicted in green .
i top kneuroncoverage .thetop kneuroncoveragemeasures howmanyneuronshaveoncebeenthemostactive kneuronson each layer.
it is defined as the ratio of the total number of top k neurons on each layer and the total number of neurons in a dnn tkncov t k uniontext.
x t uniontext.
i ltopk x i n .
the neurons from the same layer of a dnn often play similar roles and the top active neurons from different layers are important indicators to characterize the major functionality of a dnn.intuitively to more thoroughly test a dnn a test dataset should uncover more top active neurons.
ii top kneuronpatterns .givenatestinput x thesequenceof thetop kneuronsoneachlayeralsoformsapattern.infigure a assumetheneuronsingreenarethetop 2neuronsoneachlayer thepatterncanberepresentedas n1 n3 n5 n6 n8 n9 .more formally a pattern is an element of 2l1 2l2 2ll where 2liisthesetofsubsetsoftheneuronson i thlayer for1 i l. given the test input set t the number of top kneuron patterns for tis defined as tknpat t k topk x ... topk x l x t .
intuitively the top kneuron patterns denote different kinds of activated scenarios from the top hyperactive neurons of each layer.
experiments weimplement deepgauge onkeras2.
.
withtensorflow1.
.
backend andapplytheproposedtestingcriteriatodnnsfor evaluation in this section.
.
evaluation subjects datasetsanddnnmodels.
weselecttwopopularpublicly available datasets i.e.
mnist and imagenet see table for evaluation.
mnist is for handwritten digits recognition containing input data in total of which are training data and10 are test data.
on mnist we use three pre trained lenet family models lenet lenet and lenet for analysis.
to further demonstrate the usefulness of our criteria towards largerscalereal worlddlsystems wealsoselectimagenet alargesetofgeneralimagedataset i.e.
ilsvrc forclassification containing more than .
million training data and test data from categories.
the dnns we used for imagenet are pre trained vgg and resnet models both of which are relatively large in size and obtain competitive records in the ilsvrc competition containing more than and neurons and and layers respectively.
as a dnn testing criteriontowardsfutureindustrylevelapplication webelievethe scalability up to imagenet like or even larger data size and model size is almost indispensable.
adversarial test input generation.
besidesusingoriginaltest data accompanied in the corresponding dataset for coverage evaluation wefurtherexplorefourstate of the artadversarialtestinput generationtechniques i.e.
fgsm bim jsma andcw for comparative study.
each of the adversarial techniques generatesteststodetectdnn spotentialdefectsthroughtheminor perturbations on a given input described as follows fgsm crafts adversarial examples usingloss function j x y with respect to the input feature vector where denotes the modelparameters xistheinput and yistheoutputlabelof x the adversarialexampleisgeneratedas x x sign xj x y .
bim applies adversarial noise many times iteratively with a smallparameter ratherthanone withone atatime which gives a recursive formula x xandx i clipx x i sign x i 1j x i y whereclipx denotes a clipping of the values of the adversarial sample such that they are within an neighborhood of the original input x. jsma isproposed for targetedmisclassification.
for aninput x and a neural network f the output of class jis denoted as fj x .
to achieve a target misclassification class t ft x is increased while the probabilities fj x of all other classes j nequaltdecrease untilt argmaxjfj x .
carlini wagner cw carlini and wagner recently proposed new optimization based attack technique which is arguably the most effective in terms of the adversarial success rates achieved with minimal perturbation .
in principle the cw attack is to approximate the solution to the following optimization problem argmin x l x x j x y wherelisalossfunctiontomeasurethedistancebetweenthe predictionandthegroundtruth andtheconstant istobalance the two loss contributions.
in this paper we adopt the cw where each pixel is allowed to be changed by up to a limit.
figure2shows examples of the generated tests of the four adversarialtechniquesonthesampleddatafrommnisttestset.in this example we could see that compared with fgsm and bim jsma and cw perturb fewer pixels on the sampled test input.
furthermore given the same input data but different dnns the same technique would often generate different adversarial test results.
for example given the input image jsma generates different results on dnns i.e.
lenet lenet and lenet .
in other words the studied adversarial techniques are often dnn dependent.
.
evaluation setup mnist.on mnist dataset each image is single channel of size .
before the evaluation starts we first obtain the dnn s neuron output statistical information through runtime profiling eachofthestudieddnns i.e.
lenet lenet andlenet using the60 000trainingdata.whentestingevaluationstarts foreach dnn under analysis we run the test data on the model toobtainthecorrespondingcoverage.foreachstudieddnn we furthergenerateanotherfoursetsofadversarialtestdatawhichcan exploredefectsofdl 5throughfgsm bim jsma and cw .
we show that deepgauge is general and easy to be testedonthestate of the artadversarialtestgenerationtechniques.
aftergeneratingthefouradversarialdatasets weaggregateeach ofthemwiththeoriginalmnisttestdataset withatotalsize20 foreach whichenablesustoperformthecomparativestudyon 5each generated adversarial dataset is of the same size as the original test set.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france ma xu zhang sun xue li chen su li liu zhao and wang test org.fgsm bim jsma cw fgsm bim jsma cw fgsm bim jsma cw lenet lenet lenet figure examples of original sampled test data from mnist in comparison to the ones generated by each adversarial technique on the corresponding studied dnn models.
table breakdowns of datasets and dnn models.
datasetdatasetdnn model neuron layertest data source description for eval.
mnist digit recog.lenet test org.
lenet 8fgsm bim jsma cwlenet imagenetgeneral image vgg test org.
with classes resnet fgsm bim cw how the adversarial test data enhances the defect detection ability fromourcoveragecriteriameasurement.sincethestudiedadversar ialtestgeneration techniquesaremodel dependent theadversarial datasets generated by the same adversarial techniques are actually different for each model though the same number i.e.
five datasetsareusedtoevaluateoneachmodel.foreachadversarial technique we actually use it to generate three adversarial datasets one for each of lenet lenet and lenet respectively.
the detailed parameter configurations for each criterion are shown in table whereuandlare the output upper bound maximal value and lower bound minimal value obtained for each neuronduringprofiling respectively isthestandarddeviationof theoutputsofaneuronduringprofiling.althoughthedefinitionof neuron boundary coverage and strong neuron activation coverage arebasedon uandlalone i.e.
neuronoutputupperbound ub and lowerbound lb it would also be interesting to see what results could be obtained if we further tighten corner case regions i.e.
increase upper bound and decrease lower bound .
therefore be sides setting ub and lb to uandlas defined in section .
w e also evaluate another two configurations by increasing ub resp.
decreasinglb by0 .
and forneuronboundarycoverageand strongneuronactivationcoverage seetable .intotal wehave models datasets criterion settings evaluation configurations for mnist.imagenet.
imagenet ilsvrc ismorechallengingfor evaluation due to its large data size more than .
million training data as well as large image size for processing.
moreover the dnns that achieve high accuracy are often complex.comparedwithlenetfamilymodels thestudiedvgg 19and resnet 50aremuchmorecomplexintermsofbothneuronsand layers.duetothecomputationalcomplexityofadversarialtestgen erationonimagenetforanalysis werandomlysampleimagesfromtable the parameter configurations for evaluation.
dl coverage criteria parameter configuration k multisection neuron cov.
kmnc k k n.a.
neuron boundary cov.
nbc lb llb l .
lb l ub uub u .
ub u strong neuron activation cov.
snac ub uub u .
ub u top kneuron cov.
tknc k k k top kneuron patterns tknp k k k each of itslabeledcategories in the original imagenettest dataset withatotalnumberof5 000imagesasthetestdataforourevaluation.wealsotrytousefgsm bim jsma andcwtogenerate adversarialtestsforeachofthestudieddnns.however weareun abletosetupjsmatorunsuccessfullyoneitherofthetwodnns.
overall we have a total of models datasets criterion settings experimental configurations for imagenet.
to support such large scale evaluation we run the experiments onacomputercluster.eachclusternoderunsagnu linuxsystem withlinuxkernel3.
.0ona18 core2.3ghzxeon64 bitcpuwith gb of ram and also an nvidia tesla m40 gpu with 24g.
.
experimental results in our experiments we have seen useful testing feedbacks frommultiple perspectives with each testing criterion showing some uniqueportrayaloftheruntimebehaviorofdnns.wefirstdescribe some obtained results and then summarize our findings.
.
.
mnist and imagenet.
mnist.as shown in table the coverageofdifferentcriteriaobtainedbytheadversarialtechniques generally increase compared with the original mnist test dataset.
forinstance asforlenet thejsmaincreasesthecoverageofthe originaltestsfrom .
to52.
by31.
in multisection neuroncoverage from9.
to16.
by78 inneuronboundary coverage from .
to .
by in strong neuron activation coverage from62.2to66.2by6.
intop 1neuroncoverage and from to by .
in top neuron patterns.
the increase of coverage infers that the adversarial test data overallexplorenewdnns internalstates someofwhicharenot 6thiscouldbepotentiallycausedbythemassivedatasizeandthecomplexityofvgg 19andresnet50.
the similar issue on jsma was also reported in a previous work .
7duetothepagelimit weputmoredetailedexperimentalresultdiscussion aswellasdataplotonthe paper s accompanying website authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deepgauge multi granularity testing criteria for deep learning systems ase september montpellier france table coverage results of deepgauge on mnist letnet models and generated tests by adversarial techniques.
testingdnn eval.
config.
test org.test org.
test org.
test org.
test org.
criteria fgsm bim jsma cw ln 1k .
.
.
.
.
k .
.
.
.
.
kmncln 4k .
.
.
.
.
k .
.
.
.
.
ln 5k .
.
.
.
.
k .
.
.
.
.
ln 1lb l ub u .
.
.
.
.
l .
u .
17. .
.
.
.
l u .
.
.
.
.
ln 4lb l ub u .
.
.
.
.
nbc l .
u .
0. .
.
.
.
l u .
.
.
.
.
ln 5lb l ub u .
.
.
.
.
l .
u .
1. .
.
.
.
l u .
.
.
.
.
ln 1ub u .
.
.
.
.
ub u .
23. .
.
.
.
ub u .
.
.
.
.
ln 4ub u .
.
.
.
.
snac ub u .
1. .
.
.
.
ub u .
.
.
.
.
ln 5ub u .
.
.
.
.
ub u .
3. .
.
.
.
ub u .
.
.
.
.
ln 1k .
.
.
.
.
k .
.
.
.
.
k .
.
.
.
.
ln 4k .
.
.
.
.
tknc k .
.
.
.
.
k .
.
.
.
.
ln 5k .
.
.
.
.
k .
.
.
.
.
k .
.
.
.
.
ln 1k k k ln 4k tknp k k ln 5k k k coveredbytheoriginaltests.assuchadversarialtestdatareveal defectsofstudiedlenetmodels itindicatesthatgeneratingtests towards improving the coverage of the proposed criteria might potentially trigger more states of a dnn incurring higher chances of defectdetection whichisconsistentwiththeusageoftestcoverage in traditional software testing.
imagenet.
the testing coverage on the imagenet shares some similarity with mnist data while showing some differences.
vgg and resnet models are much larger in size and complexity potentiallycausing the obtainedcoverage lower thanthat oflenetinmanycases.considerthe10 multisectionneuron coverage fgsmachieves48.
onlenet butonly18.
onvgg19.
at first glance it is tempting to draw the conclusion that a dnnwithhighercomplexityintermsofnumberofneuronsand layersismoredifficulttobecoveredbytests.ourresultsshowthat this might not be generally applicable.
for example the originaltests achieves .
kmnc k on resnet but only obtains .
on vgg although resnet has a larger number ofneuronsandlayers.comparedwithmnist thegeneratedadversarial tests on imagenet incur even higher increase on the neuron boundary coverage nbc and strong neuron activation coverage snac .
for example on resnet50 under lb land ub u configuration bim increases these two criteria by from .
to .
and from .
to .
respectively.table4 coverageresultsonimagenet vgg 19andresnet50 and generated tests by adversarial techniques.
testingdnn eval.
config.
test org.test org.
test org.
test org.
criteria fgsm bim cw vgg 19k .
.
.
.
kmnc k .
.
.
.
resnet 50k .
.
.
.
k .
.
.
.
vgg 19lb l ub u .
.
.
.
l .
u .
1. .
.
.
nbc l u .
.
.
.
resnet 50lb l ub u .
.
.
.
l .
u .
1. .
.
.
l u .
.
.
.
vgg 19ub u .
.
.
.
ub u .
3. .
.
.
snac ub u .
.
.
.
resnet 50ub u .
.
.
.
ub u .
2. .
.
.
ub u .
.
.
.
vgg 19k .
.
.
.
k .
.
.
.
tknc k .
.
.
.
resnet 50k .
.
.
.5k .
.
.
.3k .
.
.
.
vgg 19k k tknp k resnet 50k k k the top neuron coverage obtained by both mnist and imagenet seetable tknc andtable tknc showthatmany neurons of a dnn have been triggered into a top k i.e.
and inourevaluatedcases hyperactivestates.forexample onvgg the sampled original tests of imagenet achieve .
top neuron coverage and .
top neuron coverage.
although the top k coverage improvement is not that obvious compared with other criteria the adversarial data still trigger more neurons as top k activated neurons in many cases which detects the hidden defects.
fordifferenttestinputdatasets itisoftenthecasethatonlya fixed subset of neurons of each layer would function as top hyperactivatedneurons.thiswouldbeahintthatthetophyperactivated neurons of each layer might describe the high level major function skeletonofaneuronnetwork.incomparisonwiththetop kneuron patterns see table tknp and table tknp albeit most of the top hyperactive neurons are relatively stable for each layer theircombinationstillcapturesthestructuraldifferenceofinput data.8these two layer level criteria altogether provide us with the information on which neurons matter the most within each layer andthetop kneuronpatternswouldmostlybeabletodifferentiate theinputdatawhen kisproperlyselectedgivenatargetdnn.the findings indicate that generating tests to cover more top kneuron patterns would have a higher chance to find defects of a dnn.
.
.
findings and remarks.
theoverallexperimentalresultsdemonstrate the usefulness of our proposed testing criteria for dnns and arealsohelpfultoexplainthedifferenceofthestate of the practice adversarial techniques form multiple perspectives the original test data of mnist and imagenet cover both the dnns major function region see kmnc as well as corner case region seenbcandsnac intables 3and4.thisalsohappens 8our in depth investigation on the generated top kneuron patterns for imagenet show that the relatively large pattern coverage improvement even for the top 1case is relevant to the large of neurons and layers in vgg and resnet .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france ma xu zhang sun xue li chen su li liu zhao and wang tothegeneratedadversarialdatasets showingthatadefectof dnncanoccureitherinamajorfunctionregionoracorner case region both of which should be extensively tested.
thetestdatageneratedbyfourstudiedadversarialtechniques combinedwithoriginaltestdata generallyboostthecoverageof ourcriteria.sinceanadversarialtestdatacouldpotentiallyreveal defectsofadlsystem itmeansthatboostingthecoverageofour testing criteria could to some extent enhance the fault detection ability which is consistent with the practical purpose of testing criteria widely adopted in traditional software testing.
it alsoshows that our test criteria metrics could capture the dnns internalbehavioraldifferenceofbenignandadversarialtestdata.
wenotethatincreasingthetestcoveragedoesnotnecessarily imply that new defects could be detected in traditional software testing.
the same conclusion applies to our coverage criteria for dnns as well though better defined coverage criteria would be much more pronounced in finding defects.
test data including the generated test data by adversarial evaluated on both mnist and imagenet mostly obtain a higher k multisection neuron coverage than the neuron boundary coverage and strong neuron activation coverage revealing that the testdatacovermoreofthemajorfunctionregionthanthecorner caseregionofadnn.thedesignoffuturedltestingtechniques should also take account of covering corner case regions.
formostoftheevaluatedconfigurations wefindthatahigher strong neuron activation coverage is more achieved than its correspondingneuronboundarycoverage.thismightbecausedbytheuniquecharacteristicsofthoseactivationfunctionsinourstudieddnns whichmakesthelowerregion smallvalue more difficult to be covered than the upper region of the statistical profilingdistribution.9thisobservationisconsistentwiththe models we studied aslenet family vgg and resnet all usereluas activation functions which could make the lower regions of a neuron much smaller than the upper regions.
remark .
in general for neuron boundary coverage and strong neuron activation coverage the higher resp.
lower theneuron s upper resp.
lower bound the lessincrementoncoverageweobserve fortopkneuroncoverage thelargerthe valueofk thelessincrementoncoverage fortop kneuron patterns the larger the value of k the more increment on patterns.
remark .
for neuron boundary coverage and strong neuron activation coverage the adversarial techniques havesufficient diversity on the performance which is similar to traditionaltestgenerationwhichaimtocoverdifferentpotential defects.
specifically we observe that the adversarial tests generated by cw are harder to be distinguished by the testcoverage since the cw perturbation concentrates more onthe objects with smaller magnitude which may trigger less internal behavior changes of dnns.
9lenet 1istheonlyexceptionalcase whichmightbecausedbytheover simplicityofitsnetwork.
10in particular relufunction propagates the positive output of a neuron to the next layer while blocking the negative output by setting it to zero which stops influencing its following layers.test orig.
test orig.
fgsm test orig.
bim test orig.
jsma test orig.
cw .
.
.
.
.
.
lenet lenet lenet .
.
.
.
.
threshold .
.
.
.
.
.
lenet lenet lenet .
.
.
.
.
.
.
.
.
.
.
.
.
.
threshold .
.
.
.
.
.
.
lenet lenet lenet .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
threshold .
.
.
.
.
.
.
lenet lenet lenet .
.
.
.
.
.
.
.
.
.
.
.
.
.
threshold .
figure3 thedeepxploreneuroncoverageresultsonmnist dataset under different threshold configurations.
.
comparison with deepxplore s neuron coverage dnc peietal.
proposeakindofneuronactivationcoverageasthe measurementfortestingdatadiversityofadnnandarguethatthe higher the activation coverage the more states of a dnn could be explored withahigherchancefordefectsdetection.akeyparame terofdncisauser specifiedthreshold andifanoutputofaneuron islargerthanthethreshold theneuroniscountedascovered.to demonstrate the difference between our set of criteria and dnc wesetupthedncevaluationwiththesamedataset model aswell as adversarialdata generationsettings asdescribedin section .
.
forthethresholdparameter wefirstsetthresholdstobe0and0.
as used in to make an even more comprehensive comparison we also use two other settings i.e.
.
and .
.
figures 3and4 show that the results of dnc obtained on the original test dataset and the dataset generated by adversarial techniques for mnistand imagenet are almost the same for all experimental settings indicating that dnc is unable to differentiate the original test data fromadversariallygeneratedones whichtriggerthecorrectand incorrect behaviors ofa dnn respectively.this meansthat dnc could hardly capture the difference between original test data and corresponding test data generated by adversarial techniques.
however todetectthedefectsofdnnsinamorefine grainedlevel itisnecessarythatthecoveragecriteriacapturesuchminordifferences where the defects adversarially triggered states also lie in.
ourfurtherin depthinvestigationondncrevealsthat thiscoverage criterion imposes several limitations dnc uses the same thresholdastheactivationevaluationforalltheneurons.however we find that the output statistical distribution of different neurons are quite diverse.
given a test suite for analysis the outputs of someneuronsmayexhibitquiteasmallvariancewithalargemean value whileothersmighthavealargevariancewithalowmean value.
therefore using the same threshold for all neurons withoutconsideringthedisparityinneuron sfunctionaldistributions would greatly diminish the accuracy.
for example given a neuronwithverysmallmeanandstandarddeviation evenaslightly larger user specified threshold would generally determine that this neuron cannot be covered.
dnc normalizes the dynamic range authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deepgauge multi granularity testing criteria for deep learning systems ase september montpellier france vgg19 resnet50 .
.
.
.
.
.
.
.
threshold vgg19 resnet50 .
.
.
.
.
.
.
threshold .2test orig.
test orig.
fgsm test orig.
bim test orig.
cw vgg19 resnet50 .
.
.
.
.
.
.
threshold .
vgg19 resnet50 .
.
.
.
.
.
.
.
threshold .
figure the deepxplore neuron activation coverage results on imagenet dataset for different threshold settings.
of neuron outputs according to maxandminoutput of neurons on the corresponding layer for each input image under analysis.
this raises an issue that the same normalized activation value e.g.
.
means differently for different input data as the maxandmin outputofeachlayermightchangeforeachinput whichrenders thenotionof largerthanagiventhreshold inconsistentamong different inputs.
among different inputs which is a very important property to support the findings of neuron activation coverage.
we instead specifytheupperandlowerboundsforeachneuronobtainedfrom the analysis on the training dataset as opposed to each individual input.
in other words our method relies on the statistics of the trainingsetwhichisusedtodeterminethemainfunctionalityof the dnn system.
.
threats to validity and discussion the selection of evaluation subjects i.e.
dataset and dnn models could be a threat to validity.
we try to counter this by using the commonly studied mnist dataset and the practical large scale dataset imagenet for each studied dataset we use the well known pre trained models of different sizes and complexity ranging from neurons up to more than neurons.
even though someof our results might not generalize to other datasets and dnn models.
another threat could be caused by the configurable hyperparametersinthecoveragecriteriadefinition.asacountermeasure whileconsideringthelimitedcomputationalresources weevaluate each criterion with different settings and analyze the influence of the parameters on criteria accuracy.
even though it might still not cover the best parameter use cases.
for example our evaluation studiedk andk fork multisection neuron coverage.weleavetheoptimizedhyper parameterselectioninourfuture work.furtherthreatcouldbecausedbythequalityoftrainingdata used for distribution i.e.
the interval range analysis of neuron output.inthispaper weconsiderpubliclyavailablewell pretrained dnn models accompanied by training data with good quality.
foradversarialtestgeneration weselectfourpopularstate ofthe practicetechniquestosimulatedefectsfromdifferentsources andgranularity.weeitherfollowtheauthors suggestedsettingsor usetheirdefaultsettings.moreover tomakecomprehensivecomparisonswithdeepxplore sneuroncoverage dnc weevaluate dnc with multiple threshold settings.
related work in this section we attempt to review the most relevant work in three aspects testing verification and security of dl systems.
.
testing of dl systems traditionalpracticesinmeasuringmachinelearningsystemsmainly rely on probing their accuracy on test inputs which are randomly drawn from manually labeled datasets and ad hocsimulations .
however suchblack boxtestingmethodologymaynotbeableto find various kinds of corner case behaviors that may induce un expectederrors .wicker etal.
recentlyproposedascale invariantfeature transformfeature guidedblack box testingand showeditscompetitivenesswithcwandjsmaalongthisdirection.
peietal.
proposedawhite boxdifferentialtestingalgorithm for systematically finding inputs that can trigger inconsistenciesbetween multiple dnns.
they introduced neuron coverage formeasuring how much of the internal logic of a dnn has been tested.however it still exhibits several caveats as discussed in section4.
.deeptest investigatesabasicsetofimagetransformations e.g.
scaling shearing androtation fromopencvandshows thattheyareusefultodetectdefectsindnn drivenautonomous cars.alongthisdirection deeproad usesinputimagescene transformation and shows its potentiality with two scenes i.e.
snowy and rainy for autonomous driving testing.
the scene transformation is obtained through training a generative adversarial network gan withapairofcollectedtrainingdatathatcoverthe statistical features of the two target scenes.
comparedwithtraditionalsoftware thedimensionandpotential testingspaceofadnnisoftenquitelarge.deepct adaptsthe concept of combinatorial testing and proposes a set of coverage basedontheneuroninputinteractionforeachlayerofdnns to guide test generation towards achieving reasonable defect detec tion ability with a relatively small number of tests.
inspired by the mc dc test criteria in traditional software sunet al.
proposed a set of adapted mc dc test criteria for dnns and show thatgeneratingtestsguidedbytheproposedcriteriaonsmallscale neuralnetworks consistingof denselayerswithnomorethan5 hidden layers and neurons exhibits higher defect detection ability than random testing.
however whether mc dc criteria scaletoreal world sizeddlsystemsstillneedsfurtherinvestigation.
instead of observing the runtime internal behaviors of dnns deepmutation proposes tomutate dnns i.e.
injectingfaults authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france ma xu zhang sun xue li chen su li liu zhao and wang eitherfromthesourcelevelormodellevel toevaluatethetestdata quality whichcouldpotentiallybeusefulfortestdataprioritization in respect of robustness on a given dnn.
ourworkofproposingmulti granularitytestingcoveragefordl systems is mostly orthogonal to the existing work.
compared with the extensive study on traditional software testing testing dl is still at an early stage.
most existing work on dl testing lacks somesuitablecriteriatounderstandandguidethetestgenerationprocess.
since test generation guided by coverage criteria e.g.
statement coverage branch coverage towards the exploration of diverse softwarestatesfordefectdetectionhasbecomethe defactostandard in traditional software testing the study to design suitabletestingcriteriafordlisdesperatelydemanding.thispapermakesanearlyattempttowardsthisdirectionbyproposingasetof testing criteria.
our criteria not only can differentiate state of theartadversarialtestgenerationtechniques butalsopotentiallybe useful for the measurement of test suite diversity by analyzing the dnns internalstatesfrommultipleportrayals.webelievethatour proposed criteria set up an important cornerstone and bring a newopportunity to design more effective automated testing techniques guided by testing criteria for dl systems.
.
verification of dl systems formalmethodscanprovideformalguaranteesaboutsafetyand robustness of verified dl systems .
the main concernofformalmethodsaretheirscalabilityforreal world sized e.g.
neurons or even more dl systems.
theearlyworkin providedanabstraction refinementapproach to checking safety properties of multi layer perceptrons.
theirapproachhasbeenappliedtoverifyanetworkwithonly6 neurons.
dlv can verify local robustness of dl systems w.r.t.
a set of user specified manipulations.
reluplex is a sound and completesmt basedapproachtoverifyingsafetyandrobustnessof dl systems with reluactivation functions.
the networks verified by reluplex in have layers and relunodes.
deepsafe usesreluplexasitsverificationengineandhasthesamescalabilityproblemasreluplex.ai2 isasoundanalyzerbasedon abstract interpretation that can reason about safety and robustness ofdlsystems.ittradesprecisionforscalabilityandscalesbetter thanreluplex.theprecisionofai2dependsonabstractdomains used in the verification and it might fail to prove a property when itactuallyholds.verivis canverifysafetypropertiesofdl systems when attackers are constrained to modify the inputs only throughgiventransformationfunctions.however real worldtransformations can be much more complex than the transformation functions considered in the paper.
.
attacks and defenses of dl systems aplethoraofresearchhasshownthatdeeplearningsystemscan be fooled by applying carefully crafted adversarial perturbation added to the original input many of which are basedongradientoroptimizationtechniques.however itstilllacks extensivestudyonhowtheseadversarialtechniquesdifferentiate intermsofdnns internalstates.inthisstudy wemakeanearly attempt towards such a direction based on our proposed criteria.with the rapid development of adversarial attack techniques extensivestudieshavebeenperformedtocircumventadversarial attacks.
galloway et al.
recently observe that low precision dnns exhibit improved robustness against some adversarial attacks.
this is primarily due to the stochastic quantization in neural networkweights.ensembleadversarialtraining ganbased approaches random resizing and random padding gametheory anddifferentiablecertificate methodsareall investigatedtodefendagainstadversarialexamples.byapplying image transformations such as total variance minimization and imagequilting veryeffectivedefensescanbeachievedwhenthe networkistrainedontheaforementionedtransformedimages .
formoreextensivediscussiononcurrentstate of the artdefense techniques we refer readers to .
ourproposedtestingcriteriaenablethequantitativemeasurementofdifferentadversarialattacktechniquesfromthesoftware engineeringperspective.thiscouldbepotentiallyhelpfulforunderstanding and interpreting dnns behaviors based on which moreeffectivednndefensetechniquecouldbedesigned.infuture work it would be also interesting to examine how to integrate the proposedtestingcriteriaintothedldevelopmentlifecycletowards building high quality dl systems.
conclusion and future work thewideadoptionofdlsystems especiallyinmanysafety critical areas has posed a severe threat to its quality and generalization property.toeffectivelymeasurethetestingadequacyandlaydown thefoundationtodesigneffectivedltestingtechniques wepropose a set of testing criteria for dnns.
our experiments on two well known datasets five dnns with diverse complexity and four state of the artadversarialtestingtechniquesshowthatthetests generated by the adversarial techniques incur obvious increases ofthecoverageintermsofthemetricsdefinedinthepaper.this demonstrates that deepgauge could be a useful indicator for evaluating testing adequacy of dnns.
to the best of our knowledge our work is among the early studiestoproposetestingcriteriafordlsystems.weexpectthat the proposed testing criteriacould be particularly amenable to dl testing in the wild.
in the next step we will continue to explorealternative testing criteria for dnns such as the combination ofboth hyperactive and hypoactive neurons.
we also plan to studythe proposed testing criteria guided automated test generation techniquesfordnns.wehopethatourstudynotonlyprovidesan avenue to illuminate the nature and mechanism of dnns but also lays the foundation towards understanding and building generic and robust dl systems.