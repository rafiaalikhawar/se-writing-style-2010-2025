is the cure worse than the disease?
overfitting in automated program repair edward k. smith earl t. barr?
claire le goues yuriy brun university of massachusetts?university college london carnegie mellon university amherst ma usa london uk pittsburgh pa usa tedks brun cs.umass.edu e.barr ucl.ac.uk clegoues cs.cmu.edu abstract automated program repair has shown promise for reducing the significant manual effort debugging requires.
this paper addresses a deficit of earlier evaluations of automated repair techniques caused by repairing programs and evaluating generated patches correctness using the same set of tests.
since tests are an imperfect metric of program correctness evaluations of this type do not discriminate between correct patches and patches that overfit the available tests and break untested but desired functionality.
this paper evaluates two well studied repair tools genprog and trpautorepair on a publicly available benchmark of bugs each with a human written patch.
by evaluating patches using tests independent from those used during repair we find that the tools are unlikely to improve the proportion of independent tests passed and that the quality of the patches is proportional to the coverage of the test suite used during repair.
for programs that pass most tests the tools are as likely to break tests as to fix them .
however novice developers also overfit and automated repair performs no worse than these developers.
in addition to overfitting we measure the effects of test suite coverage test suite provenance and starting program quality as well as the difference in quality between novice developer written and tool generated patches when quality is assessed with a test suite independent from the one used for patch generation.
categories and subject descriptors d. .
program modification d. .
testing tools general terms experimentation keywords automated program repair empirical evaluation independent evaluation genprog trpautorepair i ntro class .
introduction automated program repair holds great potential to reduce debugging costs and improve software quality.
for example genprog quickly and cheaply generated patches for out of c bugs while parshowed comparable results on java bugs .
while some techniques validate patch correctness with respect to user provided or inferred contracts a larger proportion use test cases.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august september bergamo italy.
copyright is held by the owner author s .
publication rights licensed to acm.
acm .
.
.
.
.
most common prior evaluations of automatic repair provide evidence of techniques feasibility with respect to this test casebased definition of patch correctness e.g.
.
however in practice test suites are rarely exhaustive and repair techniques must avoid breaking undertested functionality.
when evaluations of repair techniques use the same test cases to both construct patches and validate their correctness they fail to measure if or to what degree those repair techniques break undertested functionality.
in our review of the literature most of the prior evaluations of automated repair techniques that relied on test cases or workloads to validate candidate patches failed to evaluate those patches independently of patch construction.
more recent work e.g.
has begun to consider independent quality measures though less extensively than we do here.
and while some evaluations have used humans to independently measure repair acceptability and maintainability unlike our work they neither directly nor objectively evaluate functional patch correctness.
meanwhile our recent evaluation of searchrepair uses the same methodology as the evaluation we propose here .
this paper focuses on repair techniques that use test cases or workloads to validate patch construction or generate and validate g v techniques.
as section describes g v techniques are worth our investigation because they have broad applicability to mature deployed legacy software.
our evaluation does not use mature legacy software but is motivated by the techniques applicability to such software.
investigating other approaches such as synthesis based repair techniques is also of great value but is outside the scope of this paper.
our contribution is a controlled investigation of genprog and trpautorepair both test case guided search based automatic program repair tools with freely available implementations that scale to large programs.
the evaluation identifies the circumstances under which these techniques succeed as well as those under which they break undertested functionality despite producing patches that pass all test cases used for patch construction.
the evaluation uses two test suites per program one suite to construct the patch and another to evaluate it.
to borrow from machine learning vocabulary one test suite is training data used to construct a patch and the other is evaluation orheld out data used to evaluate the quality of the patch.
patches that are overly specific to the training tests and fail to generalize to the held out tests overfit to the training tests.
techniques that produce overfitting patches tend to fix certain program behavior while breaking other behavior.
the goals of our study are to evaluate the quality of automatically generated patches independently of their construction and measure the effects of properties of the input program and test suite on patch quality.
this requires a large corpus of bugs in programs that each has at least two independent exhaustive with permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
copyright is held by the owner author s .
publication rights licensed to acm.
esec fse august september bergamo italy acm.
... .
respect to some specification representation test suites.
we produce theintro class dataset for our evaluation by collecting student written programs with defects submitted as homework in a freshman programming class and all with student written bugfixing patches.
each program is accompanied by two independent test suites a black box test suite written by the course instructor to the natural language specification and a white box test suite constructed using the symbolic execution engine klee on a reference solution.
this dataset is publicly available http repairbenchmarks.cs.umass.edu .
our study admittedly uses small programs written by novice programmers which threatens the generalizability of our results.
we make this tradeoff because we need many programs with multiple exhaustive test suites to evaluate the repair properties we are interested in.
understanding repair techniques at this scale increases understanding of repair techniques in general.
our study increases the understanding of how why and under what circumstances search based repair succeeds and fails.
to the best of our knowledge this is the first systematic effort to evaluate the correctness of automated repair with respect to objective independent measures.
we measure overfitting and characterize repair quality along several previously unexplored dimensions including test suite coverage quality and provenance.
we also explicitly compare automatically generated and novice developerwritten patches with respect to functionality as opposed to human judgments.
we find that genprog and trpautorepair are less likely to repair programs that fail more training tests.
patches overfit to the training test suite often breaking undertested functionality.
patch minimization does not reduce this effect.
higher coverage test suites lead to higher quality patches.
patches generated with lower coverage test suites overfit more.
trpautorepair is more likely to break undertested functionality when patching programs that pass more tests to start with such that the patched program is often worse than the un patched program.
genprog patches overfit less for programs that pass more tests but do not substantially improve test suite performance on the held out set.
using human generated requirements based test suites to guide automated repair leads to higher quality patches than using automatically generated test suites.
novice developers also overfit to provided test suites when fixing their own programs.
however neither tool overfits significantly less than novice developers.
genprog and trpautorepair can often generate multiple patches for the same bug.
combining multiple patches can slightly decrease overfitting but the practical effect is quite small.
the rest of this paper is structured as follows.
section summarizes automated repair.
section describes our dataset.
section discusses the results of a series of experiments measuring how the quality of inputs to automatic program repair affects the output patches.
section presents a case study demonstrating overfitting.
finally section acknowledges threats to the validity of our results section places our work in the context of related research and section summarizes our contributions.
.
automated program repair automatic repair techniques can be classified broadly into two classes generate and validate g v techniques create candidate patches often via search based software engineering and then validate them typically through testing e.g.
.
synthesis based techniques use constraints to build correct by construction patches via formal verification or inferred or programmer provided contracts or specifications e.g.
.
this paper focuses on g v techniques for two reasons first such a focus is necessary because while both synthesisbased and g v techniques share high level goals they work best in different settings and have different limitations and challenges.
for example the performance of synthesis based repair relates strongly to the power of the underlying proof system which is typically irrelevant to g v repair.
second g v is particularly promising for deployed legacy software because it typically does not require that the program be written in a novel language or include special annotations or specifications.
as examples clearview genprog par and debroy and wong have successfully fixed bugs in legacy software.
although new projects appear to be increasingly adopting contracts their penetration into existing systems and languages remains limited.
few maintained contract implementations exist for widely used languages such as c. as an example as of march in the debian main repository only packages depended onzope.interfaces by far the most popular python contractspecific library in debian out of a total of python related packages.
for ubuntu out of python related packages depended on zope.interfaces .
synthesis based techniques show great promise for new or safety critical systems written in suitable languages and adequately enriched with specifications.
however the significance of defects in existing software demands that research attention be paid at least in part to techniques that address software quality in existing systems written in legacy languages.
since legacy codebases often are often idiosyncratic to the point of not adhering to the specifications of their host language it might not be possible even to add contracts to such projects.
g v repair works by generating multiple candidate patches that might address a particular bug and then validating the candidates to determine if they constitute a repair.
in practice the most common form of validation is testing.
a g v approach s input is therefore a program and a set of test cases.
the passing tests validate the correct required behavior and the failing tests identify the buggy behavior to be repaired.
g v approaches differ in how they choose which locations to modify which modifications are permitted and how the candidates are evaluated.
of existing g v techniques to the best of our knowledge genprog trpautorepair and ae are the only publicly available repair tools that both repair programs written in c and target general purpose bugs as opposed to focusing on one domain of bugs such as concurrency or integer overflow .
therefore in this paper we use genprog and trpautorepair as exemplars of g v program repair.
unlike genprog and trpautorepair ae is deterministic and so much of our experimental methodology does not apply.
however we do find that ae similarly overfits to input tests section .
.
genprog uses a genetic programming heuristic to search the space of candidate repairs.
given a buggy program and a set of tests genprog generates a population of random patches by using statistical fault localization to identify which program elements to change those that execute only on failing test cases or on both failing and passing text cases and selecting elements from elsewhere in the program to use as candidate patch code.
the fitness of each patch is computed by applying it to the input program and running the result on the input test cases a weighted sum of the count of passed tests informs a random selection of a subset of the 533population to propagate into the next iteration.
these patch candidates are recombined and mutated to form new candidates until either a candidate causes the input program to pass all tests or a preset time or resource limit is reached.
because genetic programming is a random search technique genprog is typically run multiple times on different random seeds to repair a bug.
trpautorepair uses random search instead of genetic programming to traverse the search space of candidate solutions.
instead of running an entire test suite for every patch trpautorepair uses heuristics to select the most informative test cases first and stops running the suite once a test fails.
trpautorepair limits its patches to a single edit.
it is more efficient than genprog in terms of time and test case evaluations .
trpautorepair was also published under the name rsrepair in the strength of random search on automated program repair by yuhua qi xiaoguang mao yan lei ziying dai and chengsong wang in the international conference on software engineering we refer to the original name in this paper.
there are three key hurdles that g v must overcome to find patches .
first there are many places in the buggy program that may be changed.
the set of program locations that may be changed and the probability than any one of them is changed at a given time describes the fault space of a particular program repair problem.
genprog and trpautorepair tackle this challenge by using existing fault localization techniques to identify good repair candidates.
second there are many ways to change potentially faulty code in an attempt to fix it.
this describes the fix space of a particular program repair problem.
genprog and trpautorepair tackle this challenge using the observation that programs are often repetitive and logic implemented with a bug in one place is likely to be implemented correctly elsewhere in the same program.
genprog and trpautorepair therefore limit the code changes to deleting constructs and copying constructs from elsewhere in the same program.
finally as a challenge that applies to genprog in particular genetic programming is known to lead to bloat in which solutions contain more code than necessary to maximize fitness .
genprog minimizes code bloat post facto prior work has claimed that minimization reduces patches overfitting to the training tests .
trpautorepair only attempts single edit patches and thus does not further minimize successful patches.
genprog and trpautorepair share sufficient common features to allow consistent empirical and theoretical comparisons.
for example our experiments use the same fault localization strategy and fix space weighting schemes for both.
this allows us to focus on particular experimental concerns and mitigates the threat that unrelated differences between the algorithms confound the results.
however the algorithms vary both in the way they traverse the search space and in the way they evaluate candidate patches and thus we expect our findings to generalize to other g v techniques especially in light of recent successes in modeling and characterizing the similarities in g v approaches .
.
the dataset this section describes the intro class dataset of bugs in versions of six small c programs together with two types of tests and human written bug fixes.
this dataset is available at .
.
the subject programs our dataset is drawn from an introductory c programming class at uc davis with an enrollment of about students.
the use of this anonymized dataset for research was approved by the uc davis irb.
to prevent identity recovery students names in the datasetprogram loctests buggy versionscomputationbb wb bb wb checksum checksum of a string digits digits of a number grade grade from score median median of numbers smallest min of numbers syllables count syllables total ?
?
?
of the black box and white box buggy versions are unique.
figure the instructor written implementations of the six subject programs vary in size from to loc.
the blackbox bb tests are instructor written to cover the specification.
the white box wb tests are automatically generated for complete coverage of a reference implementation.
the programs revision histories contain versions that pass at least one and fail at least one bb test and versions that pass at least one and fail at least one wb test with a total of unique buggy versions.
were securely hashed and all code comments were removed.
the dataset includes six programming assignments figure .
each assignment requires students to individually write a program that satisfies a provided set of requirements.
the requirements were of relatively high quality and a good deal of effort was spent to make them as clear as possible given their role in a beginning programming class.
further the students were taught to first understand the requirements then design then code and finally test their submissions.
students working on their assignments submit their code by pushing to a personal git repository.
the students may submit as many times as they desire without penalty until the deadline.
on every submission a system called gradebot runs the student program against a set of black box test cases described next comparing the output against an instructor written reference implementation.
the students learn how many tests run and how many pass but no other information.
the grade is proportional to the number of tests the latest submission before the deadline passes.
students do notknow the test cases used by the gradebot so when a submission fails a test the student has to carefully reconsider the program requirements.
theg v techniques evaluated in this paper rely on a pool of candidate source code elsewhere in the program.
we were initially concerned that the programs small size will impede patch construction.
however as section .
shows automated repair was often able to produce patches.
further we found that increasing the pool of candidate source code lines showed neither an increase in repair rate nor a decrease in overfitting behavior.
.
test suites and measure of patch quality each program has two test suites a black box test suite and a white box test suite.
the instructor written black box test suite is based solely on the program specification.
the instructor separated the input space into equivalence partitions and selected an input from each partition.
while the instructor paid special attention to writing high quality test suites the instructor is human and not infallible.
the white box test suite achieves edge coverage also called 534branch and structural coverage on the instructor written reference implementation.
we created the white box test suite using klee a symbolic execution tool that automatically generates tests that achieve high coverage .
when klee failed to find a covering test suite we manually added tests to achieve full edge coverage.
the black box and white box test suites were developed independently and independently describe desired program behavior.
because students can query how well their submissions do on the black box tests without learning the tests themselves they can use the results of these tests to guide their development.
a repair tool can analogously use the black box tests to guide automated repair.
we use the two test suites to measure functional patch quality.
when a human or a tool uses black box tests to construct a patch we evaluate how well the patch performs on the held out white box test suite.
if the patch passes all black box tests provided as input to the repair tool but fails some white box tests then the patch overfits to the black box tests and fails to generalize to the held out tests.
we can similarly measure overfitting to white box tests.
several experiments described in section use this method for measuring patch quality in terms of overfitting and generalizability the inverse of overfitting .
.
buggy program versions because the homework is submitted to a git repository student submissions to gradebot provide a detailed history of student efforts to solve each problem.
inevitably some submissions contain bugs in that they do not satisfy all of the requirements for the assignment.
we can approximate if a submission is buggy by evaluating its performance on the two test suites.
many though not all of the final submitted versions are correct.
to identify a specific buggy version of a program pick a test suite e.g.
black box and find all versions that pass at least one and fail at least one test in that suite.
overall we identified buggy versions using the black box suites and buggy versions using the white box suites figure the union of these sets constitutes unique buggy programs.
for each of the versions we ran each test and observed the version s behavior on that test.
we observed failures.
the overwhelming majority of errors were caused by incorrect output this accounted for cases.
segmentation faults accounted for test failures other errors detected by program exit status codes accounted for errors.
the remaining errors were due to timeouts likely caused by infinite loops.
.
empirical ev aluation we evaluate g v repair via a series of controlled experiments using the dataset from section .
section .
outlines our experimental procedure and reports baseline results for successful patching.
section .
examines overfitting in g v repair and measures how various factors affect overfitting.
section .
compares g v repair to novice developers in terms of overfitting.
finally section .
tests previously proposed approaches to combat overfitting.
.
evaluation methodology this section outlines the methodology we use to evaluate genprog and trpautorepair and presents baseline results.
we use each tool to attempt to repair each of the program versions that fail at least one specification based black box test providing the black box test suite as the training suite to both tools.
for each buggy version we compute the black box tests it passes and fails and then sample randomly those tests to produce and subsets of the training suite of the same pass fail ratio rounding up to the nearest test .
these test suite subsets represent test suites of varying levels of coverage.
we use the term scenariotool runs scenarios buggy programs genprog16104 trpautorepair19326 a before repair training suite passing raterepair success rate genprog trpautorepair b figure a genprog and trpautorepair patch creation rates.
b genprog s and trpautorepair s scenario patch creation rates producing at least one patch that passes all the blackbox tests in attempts on different seeds improve as the number of passing before repair training suite tests increased.
this relationship is significant for genprog p and for trpautorepair p .
to refer to the pair consisting of the buggy program version and a coverage measure.
thus for black box tests there are 112scenarios.
we attempt to repair each scenario times providing a new randomly generated seed each time for a total of3 240attempted repairs repair attempts is sufficient to achieve statistically significant results.
as is standard for genprog and trpautorepair when repairing a buggy version the tool uses the code in that version to construct potential patches.
when a tool exits successfully after generating a patch that passes of the training suite we run the klee generated white box held out evaluation suite over the patch to measure its quality.
figure a summarizes the fraction of the time each run each scenario and each buggy version was fixed by each of the two tools.
while fewer genprog runs find patches .
vs. .
genprog is able to patch more scenarios .
vs. .
and more distinct buggy program versions .
vs. .
than trpautorepair.
figure b shows the relationship between the number of blackbox tests the un patched buggy program fails and patch success.
genprog is slightly more likely to patch buggy versions that fail fewer tests a linear regression confirms a slight positive trend with significance p .
the similar trend detected for trpautorepair also statistically significant p .
based on these results we conclude that genprog and trpautorepair generate patches sufficiently often to enable further empirical experiments.
all relationships reported in the following sections are evaluated via linear regression unless otherwise specified.
while we give significance when appropriate none of the detected relationships had large effects measured by r2 and we do not conclude that any of the relationships are strongly linear.
535genprog trpautorepair available training suite coverageevaluation suite passing ratefigure the coverage of the test suite genprog and trpautorepair use to repair the buggy program strongly correlates p with the portion of the white box tests the patched program passes.
.
overfitting research question how often do the patches produced byg v techniques overfit to the training suite and fail to generalize to the held out evaluation suite and thus ultimately to the program specification?
having shown that the repair techniques often find patches that cause a program to pass all of the training test suite we next evaluate the quality of those patches.
specifically we are interested in learning if g v techniques produce patches that overfit to the training test suite.
we find that the median genprog patch which passes of the specification based black box training suite by definition passes only .
of the klee generated white box evaluation suite mean .
.
the median trpautorepair patch passes .
of the evaluation suite mean .
.
we conclude that tool generated patches often overfit to the training suite used in constructing the patch.
for programs that pass most tests before repair both genprog and trpautorepair are more likely to decrease the correctness of the program as measured by the number of independent tests it passes under repair than to increase it.
research question how does training suite coverage affect patch overfitting?
in practice test suites are typically incomplete.
to measure how g v techniques perform when given incomplete test suites we use subsets of the specification based black box test suites as the training suites and measure the relationship between the coverage of the training suite and the patch s overfitting.
for each buggy program we use the test suite sampling procedure from section .
to produce and sized 1for completeness we also evaluated if ae another publicly available g v tool overfits.
ae produces patches for .
of buggy programs and the median patch passes .
of the evaluation suite mean .
.
because ae is deterministic and only produces one patch per buggy program version our other experiments that rely on using multiple random seeds do not apply.
before repair training passing rateafter repair evaluation passing rate genprog trpautorepair before repair training passing ratechange in evaluation test passing rate genprog trpautorepairfigure top the fraction of evaluation tests the patched program passes is significantly positively correlated with the fraction of training tests the un patched version passes p for both tools .
bottom however for un patched programs that pass more of the training tests to start with both tools are more likely to break functionality than fix it.
the correlation between before repair training suite pass rate and the evaluation tests fixed is significantly negative for trpautorepair.
genprog exhibits a positive trend but breaks more tests than fixes for buggy programs that pass less than of the training suite p for both trends .
test suites that keep consistent the pass fail ratio of every buggy version but vary the test suite coverage.
as before for each tool we repeat this process times each time resampling the test suites and using a different random seed.
figure shows the relationship between training suite coverage and overfitting the fraction of the held out white box tests the patched program passes .
for both genprog and trpautorepair higher coverage training suites improve the quality reduce the overfitting of a patch the patch passes more white box tests on average.
a linear regression confirms both positive trends with significance p .
we conclude that genprog and trpautorepair benefit from highcoverage test suites in repairing bugs.
using low coverage test suites which are unfortunately common in practice poses a risk of automated patches that overfit to that test suite.
research question how does the number of tests that a buggy program fails affect the degree to which the generated patches overfit?
536section .
showed that the number of training tests the buggy version fails is related to a technique s ability to produce a patch.
now we explore if it is also related to overfitting.
figure relates the quality of the generated patch as measured by its performance on the held out klee generated white box tests to the number of training black box tests the original program passes.
the top of figure shows that programs that pass more training tests before repair are more likely to pass the evaluation tests postrepair.
linear regression confirms the positive trend for both tools with significance p .
however the bottom of figure shows that both genprog and trpautorepair are also more likely to break the held out test cases than fix them when repairing programs that initially pass most of the black box tests.
a linear regression confirms a negative trend for trpautorepair with significance p .
genprog exhibits a slight positive trend with significance p but still tends to break tests rather than fix them for most programs.
the trendline intercepts with zero at x and advances very little above this point.
we conclude that g v repair presents a danger when fixing highquality programs that pass most of their test suites.
the patches are likely to overfit to the tests breaking other previously correct functionality.
for low quality programs that fail many tests genprog and trpautorepair repair more functionality than they break on average.
research question how does the training test suite s provenance automatically generated vs. human written influence the patches overfitting?
we have shown that using low coverage test suites to fix bugs can lead to lower quality patches.
this suggests that automatic test generation might be used to improve test suite coverage prior to a repair attempt such an approach would require an oracle to generate expected test outputs although perhaps in some situations the expected behavior could be defined by the un patched program behavior.
here we evaluate if automatically generated tests generated with klee as described in section .
are as effective for use by g v repair as human written tests.
we refer to the method by which the tests are created as test provenance .
figures a and c summarize the relationship between test suite provenance and genprog patch overfitting.
when genprog repaired buggy programs using all of the black box tests as the training suite figure a its patches did relatively well on the white box evaluation tests.
however the same was not true when genprog constructed patches using white box tests as the training suite with the black box tests as the held out suite figure c .
in the latter case genprog overfit significantly to the white box tests.
figure e directly compares the two provenance methods.
a two sample test supports our conclusion that the black box patches pass more of the white box tests than the white box patches do the black box tests with significance p .
cliff s delta test reports a large magnitude effect magnitude .
similarly figures b and d summarize the effect of test suite provenance on trpautorepair patch quality and figure f directly compares the two provenance methods.
the effect is nearly identical to genprog although trpautorepair has slightly worse performance even with black box tests.
the two sample test similarly supports this conclusion p and a cliff s delta test reports a similar large magnitude effect.
we conclude that test suite provenance plays an important role in genprog generated patch quality.
some methods for generating test suites may be better suited for automated repair than others.genprog trpautorepair a white box passing rate of genprog patches generated with black box tests.
b white box passing rate of trpautorepair patches generated with black box tests.
c black box passing rate of genprog patches generated with white box tests.
d black box passing rate of trpautorepair patches generated with white box tests.
black box white box of held out tests passed e direct comparison of genprog patches trained with blackbox and white box suites.
black box white box of held out tests passed f direct comparison of trpautorepair patches trained with black box and white box suites.
figure a and b when black box tests guided repair search the resulting patches did well on the evaluation whitebox tests.
c and d however the same was not true when using white box tests to guide the search for patches.
e and f the direct comparisons show that patches generated using the black box suite generalize to evaluation tests much better than patches generated using the white box suite.
the line shows the median and the dot the mean.
for both tools wilcoxon signed rank tests detected a significant difference p with a large cliff s delta in both cases.
.
do tools outperform novice developers?
one of the advantages of our dataset is that every buggy program has an associated human fix corresponding to that program s student author s final submission.
the students who produced the programs in our dataset are faced with a challenge similar to that presented a white box passing rate of novice developer written patches using black box tests.
genproghuman trpautorepair of white box tests passed b direct genprog novice developer and trpautorepair comparison.
figure a novice developer written patches also overfit to the black box tests used during development.
b the median shown as the line human written patch overfits slightly less than those generated by genprog and trpautorepair but the mean shown as the dot genprog generated patches overfit slightly less than the others in part because the student written patches show higher variance than both automatic techniques.
however this is not statistically significant.
to our repair tools they write and submit code gain information about how many tests their code passes and fails make changes and resubmit.
those who have taught introductory programming courses know that students follow a number of search strategies while constructing repairs ranging from structured reasoning to random search.
this section compares the patches produced by the automated repair tools to the results of novice developers repair attempts.
as before repair tools and now humans have the specification based black box test suite available during repair to serve as a training suite and the klee generated white box tests are held out and can be used to evaluate the quality of the repair.
research question do tool generated patches overfit more than novice developer written patches?
figure a shows that student solutions do in fact overfit to the provided test suites and often fail to generalize to held out tests.
figure compares the quality of genprog novice developer and trpautorepair patches.
both genprog and trpautorepair patches have both a lower mean and median passing rate for held out tests than the studentwritten patches.
while there is a visual difference in figure b between student written and tool generated patches the wilcoxon signed rank test reports no significant difference between the samples.
patches generated by either tool demonstrate significantly less variability in quality than student written patches.
comparing automatically generated patches to novice developerwritten patches might seem unfair since repair tools can only access tests that represent a partial specification while humans can reason abstractly about the program specification.
however while humans can reason about program faults abstractly above the level of a repair tool they are also subject to a large array of cognitive biases that can hamper their debugging effort.
repair tools have no such biases and will mechanically explore the solution space as guided by an objective function without becoming irrationally fixated on particular solutions.
.
mitigating overfitting research question does genprog s patch minimization reduce overfitting?
genprog uses patch minimization via delta debugging to reduce code bloat.
trpautorepair does not perform minimization because the produced patch is only ever a single edit.
intuitively a small change to a program is less likely to encode special behavior that handles just the training tests in a separate way .
thus far all results we have described for genprog have used genprog s builtin patch minimization procedure.
we now investigate if disabling this feature increases overfitting.
we compared unminimized patches produced by genprog to their minimized versions in terms of the number of black box and whitebox tests the patched versions passed.
in all experiments regardless of the tests used paired wilcoxon tests show that the test passing rates of the minimized and unminimized patches were drawn from the same distribution and fail to reject the null hypothesis p in all cases after benjamini hochberg correction for false discovery rates .
this indicates that minimization does not reduce the degree to which genprog overfits.
research question can overfitting be averaged out by exploiting randomness in the repair process?
do different random seeds overfit in different ways?
some repair tools including genprog and trpautorepair can generate multiple patches for the same defect such as when run on multiple different random seeds .
this affords a unique opportunity even if patches do overfit to their test suites it is possible that a group of patches better represents the desired program behavior than an individual patch.
specifically even if each patch overfits on some subset of desired behavior if each patch in a group encodes most of that behavior a group vote on the behavior may outperform each individual patch.
n version patches may therefore provide an avenue to mitigate overfitting.
human written code typically lacks sufficient diversity to enable true n version programming but randomized g v repair may not.
we created the n version program pnin the following way for each buggy version test suite subset pair pb run genprog on pb times.
if fewer than three of the runs result in a patch we exclude this pair from this experiment.
we call these n patched versions p1p pnp.
next we create a new program pn that on input i runs each of p1p pnponi and returns the output most frequently returned by output by those program.
if two or more return values tie pnreturns one of those values at random.
figure shows that n version patches constructed from genprog s output do not perform statistically significantly better than either individual genprog generated patches or novice developer written patches.
the only case in which n version programs outperformed individual patches was when trpautorepair constructed patches using klee generated white box suites for training not shown in figure .
recall that training on white box suites produced poorquality patches research question .
while n version trpautorepair patches are statistically significantly better than individual trpautorepair patches p the cliff s delta is negligible and n version trpautorepair patches do not significantly outperform those written by novice developers.
we conclude that when tools can produce quality patches using high quality test suites there is insufficient diversity in the patches genprog genprog n versionhuman trpautorepair trpautorepair n versionwhite box passing ratefigure tool generated patches and n version programs made up of those patches perform worse than humans written patches on average.
n version genprog programs underperform even the individual genprog patches and n version trpautorepair programs perform negligibly worse than individual trpautorepair patches while not statistically differing from human written patches.
to further improve quality.
however when repair tools produce poor quality patches diversity sometimes provides a modest benefit.
n version programming may indeed provide an avenue to mitigate the worst cases of overfitting.
.
case study section .
showed that test suite provenance has the largest effect on the quality of automatically generated patches.
this section describes a case study of a buggy student program and two patches that genprog produced for it using the white box test suite to highlight the ways that some test suites can lead to increased overfitting.
themedian homework assignment asks students to produce a c function that takes as input three integers and outputs their median.
figure shows the black and white box test suites for the median program.
one of the student s buggy non final submissions to the homework was 1int med int n1 int n2 int n3 if n1 n2 n1 n3 n2 n1 n1 n3 n3 n1 n1 n2 return n1 if n2 n3 n1 n2 n2 n3 n3 n2 n2 n1 return n2 if n1 n3 n3 n2 return n3 this submission is close to correct.
despite its incorrect logic e.g.
the equality checks on lines and it passes five of the six white box and six of the seven the black box tests.
the execution fails to reach a return statement for the fifth black box and for the second white box tests for which n3is the median and n1 n2 .
given this program and the white box suite genprog generated several patches of varying quality.
one such low quality genprogpatched program is 1int med int n1 int n2 int n3 if n1 n2 n1 n3 n3 n1 n1 n2 return n1 black box tests white box tests med med med med med med med med med med med med med figure white and black box suites for median .
if n2 n1 return n3 if n2 n3 n1 n2 n2 n3 n3 n2 n2 n1 return n2 if n1 n3 n3 n2 return n3 one of the conditions in the check on line has been removed and this program returns n1as the median if it is coincidentally equal to either n2orn3 or if it is actually the median and n3 n2 .
ifn1is not the median but n2 n1 the check moved to line this code will possibly but not necessarily incorrectly return n3.
the rest of the logic is unaffected.
this patch addresses the original problem in the student s code at least with respect to the white box suite.
this code is correct when n1is the median and n3 n2 n2is the median and n2 n1 or n3is the median and n2 n1.
although this code passes all of the white box tests improving on the original student submission it passes fewer black box tests than the original failing tests and in figure .
this patch is an excellent example of overfitting the fitness function and highlights weaknesses in the white box test suite many of the inputs have repeated elements.
as a result the student s otherwise logically incorrect equality checks on lines and of the original submission mask the larger problems in the low quality patch.
running genprog with the same white box test suite but a different random seeds can lead to different patches for the same bug.
for example for this buggy program genprog also produced the following patched program 1int med int n1 int n2 int n3 if n1 n2 n1 n3 n2 n1 n1 n3 n3 n1 n1 n2 return n1 if n2 n3 n1 n2 n2 n3 n3 n2 n2 n1 return n2 if n1 n3 n3 n2 return n3 else return n3 the incorrect equality checks on lines and remain.
this patch inserted return n3 into the else block of the last set of conditions that seek to determine if n3is the median.
ignoring the equality checks this is actually a reasonable solution because by that point the only remaining option should be that n3is the median.
for this buggy program the student rewrote the logic considerably eliminating the equality checks on lines and and properly handling the last set of conditionals 5391int med int n1 int n2 int n3 if n2 n1 n1 n3 n3 n1 n1 n2 return n1 if n1 n2 n2 n3 n3 n2 n2 n1 return n2 if n1 n3 n3 n2 n2 n3 n3 n1 return n3 in this example genprog solutions overfit to the test suite while the student written patch is more general.
this example highlights weaknesses in the white box test suite which fails to encode key behavior.
this raises interesting questions about the potential of automatic test case generation to augment the input given to g v repair techniques more work is required to improve the quality of the output of such techniques before the two approaches can be usefully integrated.
.
threats to v alidity our experiments may not generalize.
we only experiment with genprog and trpautorepair two of several g v repair techniques and our results may not extend to other automatic program repair mechanisms.
however recent work has started to unify the theory underlying g v repair suggesting that results from two different techniques may extend to others.
our subjects are small student written programs with fairly small test suites.
therefore our results may not generalize to large real world programs.
however this is a necessary tradeoff as the goals of our study require programs that can be tested exhaustively with respect to multiple specification representations.
understanding repair techniques at the scale of our experiments increases understanding of the repair techniques in general.
additionally while our subjects size allows for a very large dataset for conducting controlled trials it may also affect the ability to find diverse patches.
we ran seeds per repair effort a relatively small number by the standards of metaheuristic search algorithms but comparable to previous program repair evaluations.
more attempts may have revealed more solutions.
finally we used the recommended genprog parameters defined in previous work a full parameter sweep is outside the scope of this investigation.
our intro class dataset umass.edu includes all the buggy versions student written solutions and test suites.
this makes our experiments repeatable.
however parts of the creation of the dataset were manual.
while the white box suites were generated automatically to the extent possible and black box suites were generated by a rigorous manual analysis of the requirements at least the latter is subject to human interpretation.
thus a replication of our experiments on different programs or with different test suites on our programs may be affected by human subjectivity and may produce different results.
genprog trpautorepair and many other related repair techniques rely on randomized algorithms.
evaluating systems that involve randomized algorithms is particularly difficult and requires paying special attention to the sample sizes statistical tests crossvalidation and uses of bootstrapping.
our work is consistent with the guidelines for evaluating randomized algorithms to enhance the credibility of our findings.
specifically we used a large sample of buggy student programs controlled for a variety of potential influencers in our experiments and used fixed effects regression models and two sample tests along with false discovery rate correction to lend statistical support to our findings.
.
related work most prior evaluations of g v repair techniques demonstrate by construction that the technique is feasible and reasonably efficient in practice .
some show that the resulting patches withstand red team attacks some illustrate with a small number of examples that g v generated patches for security vulnerabilities protect against exploits and fuzzed variants of those exploits on typical user workloads and some consider the fraction of a set of bugs their technique can repair .
these evaluations have demonstrated that g v techniques can repair a moderate number of bugs in medium sized programs as well as evaluated the monetary and time costs of automatic repair the relationship between operator choices and test execution parameters and success and human rated patch acceptability and maintainability .
however these evaluations have generally not used an objective metric of correctness independent of patch construction.
our evaluation measures patch correctness independently of patch construction.
we empirically examine how test suite coverage and provenance number of test failures and patch minimization affect repair effectiveness defined by both success and functional correctness.
we perform these experiments using a much larger number of bugs than ever before designed to permit controlled evaluations that isolate particular features of the inputs such that we can examine their effects on automatic repair in a statistically significant way.
concurrent research is starting to evaluate repair techniques in terms of overfitting .
evaluating the degree to which relifixand genprog introduce regression errors is a step toward the independent correctness evaluation we advocate here where we use independent test suites to measure patch quality.
by contrast those experiments use the subset of the original test suite that does not execute any of the lines associated with the bug under repair ignoring specifically regressions a patch is most likely to introduce.
poor quality test suites result in patches that overfit to those suites .
our evaluation goes further demonstrating that high quality high coverage test suites still lead to overfitting and identifying other relationships between test suite properties and patch quality.
finally prior and concurrent human evaluations of automatically generated patches have measured acceptability and maintainability .
while the human judgment is a criterion not used by the repair tools for patch construction it is fundamentally different from the correctness criterion we use in our evaluation as it is often difficult for humans to spot bugs even when told exactly where to look for them .
meanwhile our recent evaluation of searchrepair uses the same methodology as the evaluation we present here .
our work evaluates automated repair so that it can be improved.
empirical studies of fixes of real bugs in open source projects can also improve repair by helping designers select change operators and search strategies .
understanding how automated repair handles particular classes of errors such as security vulnerabilities can guide tool design.
for this reason some automated repair techniques focus on a particular defect class such as buffer overruns unsafe integer use in c programs single variable atomicity violations deadlock and livelock defects concurrency errors and data input errors .
other techniques tackle generic bugs.
for example the armor tool replaces buggy library calls with different calls that achieve the same behavior and relifix uses a set of templates mined from regression fixes to automatically patch generic regression bugs.
our evaluation has focused on tools that fix generic bugs but our methodology can be applied to focused repair as well.
540user provided code contracts or other forms of invariants can help to synthesize correct by construction patches e.g.
via autofixe for eiffel code and semfix for c .
directfix aims to synthesize minimal patches to be less prone to overfitting but only works for programs using a subset of c language features and has only been tested on small programs.
synthesis techniques provide the benefit of provable correctness for patches but require contracts so they are unsuitable for legacy systems.
synthesis techniques can also construct new features from examples rather than address existing bugs.
our work has focused on g v approaches and investigating overfitting and patch quality in synthesisbased techniques is a complementary and worthwhile pursuit.
the techniques evaluated in this paper genprog and trpautorepair are representative of g v approaches.
our work does not create a new bug fixing technique but rather evaluates existing techniques in a new way to expose previously hidden limitations to g v program repair.
our findings may extend to other search based or test suite guided repair techniques e.g.
.
monperrus has recently discussed the challenges of experimentally comparing program repair techniques.
for example the selection of test subjects defects can introduce evaluation bias .
our evaluation focuses precisely on the limits and potential of repair techniques on a large dataset of defects and controls for a variety of potential influencers addressing some of monperrus concerns .
genetic programming tends to produce extraneous code that does not contribute to the fitness of the solution .
genprog attempts to mitigate this through solution minimization which may reduce the chances of breaking undertested functionality.
overfitting is also a well studied problem in machine learning .
our experiments suggest that minimization and overfitting are unrelated which is consistent with prior results in machine learning .
to the best of our knowledge ours is the first consideration of this relationship in the program repair domain.
g v approaches fall in the space of search based software engineering which adapts search methods such as genetic programming to software engineering tasks.
search based software engineering has been used for developing test suites finding safety violations refactoring and project management and effort estimation .
good fitness functions are critical to searchbased software engineering.
our findings indicate that using test cases alone as the fitness function leads to patches that may not generalize to the program requirements and more sophisticated fitness functions may be required for search based program repair.
n version programming combines multiple different programs trying to solve the same problem in the interest of achieving resiliency and correctness through redundancy.
n version programming works poorly with human written systems because the errors humans make do not appear to be independent .
our evaluations have shown that n versions of automatically generated patches has a minor positive effect but failed to fully generalize to the desired behavior.
.
conclusions and implications g v automated repair shows promise for reducing the manual bug fixing burden and improving software quality.
however if these techniques are to gain practical traction we must augment feasibility demonstrations with qualitative evaluations that address the quality and applicability.
in this paper we systematically evaluated the factors affecting the output quality of genprog and trpautorepair two representative g v techniques through a controlled evaluation on a large set of programs written by novice developers with naturally occurring bugs and human written patches.
based on our findings the open research challenges include repair techniques must go beyond testing on the training data to characterize functional correctness.
genprog and trpautorepair produced patches for more than half of the bugs in our dataset .
and .
respectively .
the ability to produce a patch was correlated with input program quality as measured by the test suites.
however those patches tended to overfit to the test suite used to generate the patch.
when using requirements based black box tests genprog and trpautorepair overfit significantly less than when using generated white box tests.
interestingly the novice programmers students also overfit to the provided test cases.
these results highlight both the significant promise of automatic repair and the fact that more work is needed to improve repair output quality.
we advocate that future evaluations of g v repair tools withhold some portion of tests from the repair tool at least some of which share code under test with the tests exposing the buggy behavior.
this is similar to the machine learning evaluational technique of cross validation and provides a higher level of confidence that a repair technique is able to repair isolated defects without introducing regressions.
automatic repair should be used in appropriate contexts.
both test suite coverage and input program quality appear related to the quality of the automatically generated patches.
higher coverage test suites were more likely to lead to more general patches.
patches produced for higher quality programs were at best unlikely to improve functionality and at worst likely to break existing functionality.
this suggests that automatic repair techniques might be best applied early in the development lifecycle though unfortunately this is the time when the program quality itself is likely low reducing the likelihood of repair success and the test suite is least likely to be comprehensive.
different repair techniques are likely to be useful at different times and more study is needed to explore this space.
the quality of repair test suites should be measured and improved appropriately.
the provenance of the test suites automatically generated or human written had a striking relationship with the resulting patch quality.
automatic test input generation techniques should fit naturally into a toolchain for automatic repair particularly when user provided test cases fail to fully cover the program functionality or when critical functionality should be independently tested post repair to ensure that overfitting has not occurred.
our results suggest that more work is needed to fully understand and characterize test suite quality beyond coverage metrics alone.
patch diversity might improve repair quality.
low quality patches especially those generated using automatically generated tests demonstrated sufficient functional diversity to improve on the patched programs via plurality voting.
plurality voting may thus mitigate the risks of low quality test suites in the appropriate settings.
while g v techniques have not yet become a silver bullet of program repair in some cases and settings they already outperform beginner developers.
our results suggest that if several shortcomings are addressed there is significant promise that automated repair techniques can be impactful and helpful parts of the software development process.
.