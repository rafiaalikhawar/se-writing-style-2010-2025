automatically generating commit messages from diffs using neural machine translation siyuan jiang ameer armaly and collin mcmillan department of computer science and engineering university of notre dame notre dame in usa sjiang1 aarmaly cmc nd.edu abstract commit messages are a valuable resource in comprehension of software evolution since they provide a record of changes such as feature additions and bug repairs.
unfortunately programmers often neglect to write good commit messages.
different techniques have been proposed to help programmersby automatically writing these messages.
these techniques are effective at describing what changed but are often verbose and lack context for understanding the rationale behind a change.
incontrast humans write messages that are short and summarize the high level rationale.
in this paper we adapt neural machine translation nmt to automatically translate diffs into commit messages.
we trained an nmt algorithm using a corpus of diffs and human written commit messages from the top 1k githubprojects.
we designed a filter to help ensure that we only trained the algorithm on higher quality commit messages.
our evaluation uncovered a pattern in which the messages we generate tend tobe either very high or very low quality.
therefore we created a quality assurance filter to detect cases in which we are unable to produce good messages and return a warning instead.
i. i ntroduction commit messages are natural language descriptions of changes in source code.
when a programmer updates code a typical procedure is to upload the change to a version control system with a short commit message to describe the purpose of the change e.g.
adds support for inch tablet screen size.
the repository stores the message alongside a diff that represents the difference between the current and previous version of the affected files.
the practice is extremely common for this paper alone we obtained over 2m diffs and messages from just 1k projects on github.
commit messages are useful because they help programmers to understand the high level rationale for a change without read ing the low level implementation details.
they serve a valuable purpose in comprehension of software evolution and act as a record of feature additions and bug repairs .
unfortunately programmers sometimes neglect commit messages likely due to the same time and market pressures that have been reported to affect many types of documentation .
in short programmers use commit messages but often avoid writing them themselves.
automated generation of commit messages has been proposed as an alternative to manual efforts by programmers.
for example buse et al.
describe deltadoc a tool that summarizes what changed in the control flow of a program between code versions.
likewise cortes coy et al.
built changescribe which summarizes changes such as methodadditions.
these and other existing techniques see section ii have been shown to be effective in answering questions about what changed and where from one code version to another.
what is missing from existing approaches is a short high level description of the purpose behind commits.
currentapproaches are effective at summarizing what changed and where but do not answer the question why .
questions of why traditionally require human insight since they involve synthesis of different complex data sources and context.
however as mockus et al.
observed many commit messages are similar and can be broadly categorized as related to bug repair feature additions etc.
plus they follow similar grammatical patterns such as verb direct object structure e.g.
adds support for... .
this observation leads us to believe that the text of commit messages can be learned and predicted if there is sufficient data.
our view is in line with the hypothesis of naturalness of software that software artifacts follow patterns that can be learned from sufficiently large datasets.
in this paper we adapt a neural machine translation nmt algorithm to the problem of commit message generation.several nmt algorithms have been designed to translatebetween natural languages by training a neural network onpairs of sentences that humans have already translated.
the datasets required are enormous by typical software engineering research standards involving up to tens of millions of pairs of sentences .
we trained an nmt algorithm using pairs of diff s and commit messages from 1k popular projects on github.
while we were able to obtain quite large datasets over 2m commits we encountered many commit messages that were gibberish or very low quality a problem others haveobserved which if left in the training data could be reflected in the nmt s output.
therefore we designed a filter to ensure that we only trained the algorithm using messages with a verb direct object pattern.
we investigate and report the effectiveness of the predictions from the process.
we found promising results as well as key constraints on the accuracy of the predictions.
in short thenmt process performed quite well under select conditions but poorly in others.
we report these results and promising and poor conditions as a guide to other researchers and platform for advancement in this research area.
to further promoteadvancement of the area we make our implementation and data freely available in an online replication package.
.
c circlecopyrt2017 ieeease urbana champaign il usa t echnical research135 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
our approach has two key advantages that make it a supplement to rather than a competitor of existing automatic commit message generation techniques.
first we produce short summary messages rather than exhaustive descriptions of code changes.
and second our approach produces messages forchanges to many types of software artifact in the repository not solely source code.
a. the problem in this paper we target the problem of automatically generating commit messages.
commit messages are useful in the long term for program comprehension and maintainability but cost significant time and effort in the short term.
these short term pressures lead programmers to neglect writing commit messages like other types of documentation .
buse et al.
point out that programmers use commit messages for two reasons to summarize what changed and to briefly explain why the change was necessary.
to date research into commit message generation has exclusively focused on the question what.
in this paper we seek to begin answering why.
existing commit message generation techniques produce relatively long messages that include details such as the methods that were added or the number of files changes what information .
while useful these techniques are a complementto rather than a replacement for high level whyinformation that humans write such as adds support for inch tablet screens.
normally this high level information requires human judgment.
but we hypothesize that there are patterns of commits and that these patterns can be detected and used to generate messages for similar commits later.
given a large number of pairs of diff s and messages we believe we can train an algorithm to write new messages for new commits based on the new commits similarity to older ones.
please note that we do not claim to generate new insights for completely new types of commits that task is likely to remain in the hands of human experts.
however we do aim to write messages that reflect knowledge that can be learned from records of previous commits.
in the long run we hope that this technology will help reduce manual effort by programmers in reading and understanding code changes in repositories.
b. paper overview figure depicts an overview of our paper.
we have divided the work into three segments in part a section iv we present our approach to filtering for verb direct object v do commit message patterns and training an nmt algorithm to produce messages with this pattern.
the v do filter was introduced because a large percentage of the messages in the repositories we downloaded were very low quality and we needed to ensure that we trained the nmt algorithm only with examples matching an acceptable pattern.
we then trained an nmt algorithm on the pairs of diffs and commit messages where the messages followed the v do pattern.
in part b sections v and vi we evaluate the quality of the commit messages produced by the algorithm with anautomated method and a human study with ph.d. students and professional programmers.
we observe that while there are a significant number of positive results there are also asignificant number of negative results.
therefore in part c sections vii we design a quality assurance qa filter to detect cases in which the nmt algorithm is likely to produce a negative result.
we then modify our approach to produce a warning message instead of a commit message in those cases and update our evaluation to show the effects of our modification.
in short we reduce the number of poor predicted messages by at a cost of also mistakenly reducing high quality predictions by .
ii.
r elated work we split the related work into three categories the work that generates commit messages the work that summarizes source code and the work that applies deep learning algorithms in software engineering.
a. commit message generation techniques we categorize the commit message generation techniques into three groups based on the inputs of the techniques.
the first group uses code changes of a commit as an input and summarizes the changes to generate the commit message.
for example buse et al.
have built deltadoc which extracts path predicates of changed statements and follows a set of predefined rules to generate a summary .
similarly linares v squez et al.
have built changescribe which extracts changes between two abstract syntax trees and summarizes the changes based on predefined rules .
supplementing the first group the second group is based on related software documents.
for example le et al.
have built rclinker which links a bug report to the corresponding commit message .
rastkar and murphy have proposed to summarize multiple related documents for commits .
integrating the ideas of the first and the second groups morenoet al.
have built arena which summarizes changes and finds related issues to generate release notes .
the third group is our technique using diff s generated by git diff as inputs.
our technique is to translate a diff updated evaluation with qa filterpairs of diffs commit messages1 filter for v do message patterns nmt training procedure2 evaluation of predicted messages3part a part b qa filter for likely poor predictions4 5part c fig.
the overview of our paper.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
to a natural language sentence.
our technique supplements the first group in two ways.
first the techniques in the first group often generate multi line summaries that contain pseudocode and template text.
in contrast our technique generates onesentence descriptions which can be used as a headline of the multi line summaries.
second our technique summarizes both code and non code changes in diff s. b. source code summarization source code summarization techniques generate descriptions of source code pieces .
the algorithms of the techniques can be adapted to generate summaries for changes in commits.
code summarization can be categorized into two groups extractive and abstractive.
extractive summarization extracts relevant parts of source code and uses the relevant parts as a summary .
abstractive summarization includes information that is not explicitly in the source code.
for example sridhara et al.
has designed a natural language generation nlg system to create summaries of java methods .
first the nlg system finds important statements of a java method.
second the system uses a text generation algorithm to transform a statement to a natural language description.
this algorithm has predefined text templates for different statement types such as return statements and assignment statements.
both deltadoc and changescribe discussed in section ii a follow the similar nlg design.
besides the nlg approach to generate abstractive summaries iyer et al.
have built code nn which uses an neural machine translation nmt algorithm to summarize code snippets .
this work is similar to our technique because our technique also uses an nmt algorithm.
there are two key differences between our technique and code nn.
first the goal of code nn is summarizing code snippets and the goal of our technique is summarizing changes.
second code nn parses code snippets and removed all the comments.
in contrast our technique s input is an entire diff with code comments and diff marks e.g.
denoting insertion .
c. deep learning in software engineering deep learning algorithms are becoming more prevalent in software engineering research.
deep learning algorithms as applied to software automatically learn representations of software artifacts.
for example to detect code clones traditional approaches predefine the representations of code fragments some techniques use token sequences to represent code others use graphs .
in contrast the deep learning approach introduced by white et al.
learns the representations of code automatically.
similarly deep learning algorithms are introduced in bug localization software traceability and code suggestions .
our technique is similar to the work done by gu et al.
because both our and their techniques use neural machine translation nmt .
gu et al.
use nmt to translate natural language queries to api method sequences .
similarly several code generation techniques use nmt to translate naturallanguage to programming language .
in contrast our technique translates diff s to natural language.
our technique is also similar to the work by alexandru et al.
which investigates the suitability of nmt for program comprehension.
alexandru et al.
use nmt for source code tokenization and token annotation.
while alexandru et al.
target on lower level source code understanding token level we target on understanding higher level of mixtures of code and text diff level .
iii.
b ackground we split the background section into three subsections.
the first subsection is about the empirical studies on commit messages which motivate us to generate short descriptionsof commits.
the second subsection describes rnn encoderdecoder a popular neural network translation model which is an important background for the third subsection.
the third subsection describes attentional rnn encoder decoder which is used in our work.
a. commit messages our work is motivated by the findings of the studies by buse et al.
and by jiang and mcmillan .
the results of the two studies indicate three things.
first commit messages are pervasive and desired.
buse et al.
examined 1k commits from five mature software projects and found that .
of the commits have non empty messages.
jiang and mcmillan collected over 2m commit messages from 1k projects.
second human written commit messages are short.
in buse et al.
s study the average size of the non empty commit messages is .
lines.
similarly the study of jiang and mcmillan shows that of the commit messages have only one sentence.
third commit messages contain various types of information not solely summaries of code changes.
buse et al.
manually analyzed commit messages and found that the messages are not only about what the changes are but also about why the changes are made.
supported by the three findings ourtechnique aims to generate one sentence commit messages which mimic the human written commit messages.
b. rnn encoder decoder model neural machine translation nmt is neural networks that model the translation process from a source languagesequence x x1 ... xn to a target language sequence y y1 ... yn with the conditional probability p y x .
cho et al.
introduced rnn encoder decoder as an nmt model which is commonly used and can produce state of the art translation performance .
as a promising deep learning model rnn encoder decoder has been used in addressing other software engineering tasks .
rnn encoder decoder has two recurrent neural networks rnns .
one rnn is used to transform source language sequences into vector representations.
this rnn is calledthe encoder.
the other rnn is used to transform the vector representations to the target language sequences which is called the decoder.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
x1 h1 x2 h2 ... xt htht ...... fig.
the architecture of the encoder in rnn encoderdecoder start h y1 h ... yt h t ......ht yt 1h t fig.
the architecture of the decoder in rnn encoderdecoder encoder the input of the encoder is a variable length sequence x x1 ... xt .
the encoder takes one symbol at a time as shown in figure .
as an rnn the encoder has a hidden state h which is a fixed length vector.
at a time step t the encoder computes the hidden state htby ht f ht xt wherefis a non linear function.
two common options for fare long short term memory lstm and the gated recurrent unit gru due to space limit we do not describe these two unit types in detail here .
for example bahdanau et al.
use gru and sutskever et al.
use lstm .
the last symbol of xshould be an end of sequence eos symbol which notifies the encoder to stop and output the final hidden stateht which is used as a vector representation of x. decoder figure shows the rnn of the decoder.
the output of the decoder is the target sequence y y1 ... yt prime .
one input of the decoder is a start symbol denoting the beginning of the target sequence.
at a time step t the decoder computes the hidden state h prime tand the conditional distribution of the next symbol ytby h prime t f h primet yt ht p yt yt ... y ht g h prime t yt ht whereht generated by the encoder is called the context vector fandgare non linear functions.
function fhere and fin equation are often the same.
function gmust produce valid probabilities.
for example softmax can be used as g. the decoder finishes when it predicts an eos symbol.
training goal the encoder and the decoder are jointly trained to maximize the conditional log likelihood max 1 nn summationdisplay i 1logp yi xi where is the set of the model parameters nis the size of the training set and each xi yi is a pair of a source sequence and a target sequence in the training set.
c. attentional rnn encoder decoder and nematus bahdanau et al.
introduced the attentional rnn encoderdecoder in which attention mechanism is introduced to deal with long source sequences .
we use this mechanism in ourwork because our source sequences diff s are much longer than natural language sentences.
the attention mechanism includes several modifications in both the encoder and the decoder which we describe in the following subsections.
encoder the encoder in the attentional model is a bidirectional rnn which has two rnns forward and backward.
the two rnns have the same architecture.
the forward rnn is the same as the rnn in the original rnn encoder decoder model figure which reads the source sequence xas it is ordered from x1toxt.
the forward rnn generates a sequence of the hidden states h1 ... ht .
in contrast the backward rnn reads xin the reversed order and generates a sequence of the hidden states ht ... h1 .
in the end for each symbol xiinx the encoder outputs hi which is a concatenation of hiand hi.
decoder the decoder computes the hidden state h prime tand the conditional distribution of the next symbol ytby h prime t f h primet yt ct p yt yt ... y ct g h primet yt ct wherefandgare non linear functions like fandgin equations and .
ctis the distinct context vector for yt and can be computed by ct t summationdisplay i 1 tihi wheretis the length of the input sequence the weight ti can be trained jointly with the other components in the model andhiis generated by the encoder.
since ctis designed to introduce the context s impact to yt attentional rnn encoderdecoder works better on long source sequences.
therefore we use this nmt model in this paper rather than the original one.
iv .
a pproach this section describes our approach including the data set preparation and the nmt training procedure.
this section corresponds to part a in the paper overview figure and is detailed in figure .
a. preparing a data set for nmt we used the commit data set provided by jiang and mcmillan which contains 2m commits.
the data set includes commits from top 1k java projects ordered by the number of stars in github.
we describe how we prepared the data set for nmt algorithms as follows.
preprocessing the data set first we extracted the first sentences from the commit messages.
we used the first sentences as the target sequences because the first sentences often are the summaries of the entire commit messages.
similarly gu et al.
used the first sentences of the api comments as their target sequences .
second we removed issue ids from the extracted sentences and removed commit ids from the diff s because issue ids and commit ids are unique ids and authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
increase the vocabularies of the source and the target languages dramatically which in turn cause large memory use of nmt.
third we removed merge and rollback commits the same practice done by jiang and mcmillan .
merges and rollbacks are removed because the diff s of merges and rollbacks are often more than thousands of lines which nmt is not suitable to translate.
for the same reason we also removed anydiff that is larger than 1mb.
after the above steps we have .8m commits remaining.
finally we tokenized the extracted sentences and the diff sb y white spaces and punctuations.
we did not split camelcase so that identifiers e.g.
class names or method names are treated as individual words in this study.
setting maximum sequence lengths for nmt training a maximum sequence length for both source and target sequences need to be set for an rnn encoder decoder .
since nmt is for translating natural language sentences maximum sequence lengths for both source and target sequences are often set between to .
because the lengths of our source and target sequences are very different we set the maximum sequence lengths separately.
for our target sequences we set the maximum length at tokens including words and punctuations because the first sentences from the commit messages tend to be short.
in our data set of the first sentences have less than tokens.
for our source sequences we set the maximum length at tokens because is the largest maximum length used by nmt in natural language translation.
many configurations are possible and optimizing the maximum diff length for generating commit messages is an area of future work.
in pilot training evaluationmodel training trained model2 filtering and preprocessing1diffstraining and validation sets preprocessed commitmessagespreprocessed diff filescommit messagesdiff files diffstest set preprocessed commit messagespreprocessed diff files generated commit messages evaluation3diffsdiff s vocabulary diffsmessages vocabularydata processing legend data process modelv do filter fig.
the detailed process in part a figure .
there are three main steps filtering and preprocessing the data training a neural machine translation model evaluating the model which is part b figure .studies a maximum length of outperformed lengths of and .
after applying the maximum lengths for source and target sequences and we have 75k commits remaining.
v do filter we introduced v erb direct object v do filter because we found that the existing messages have different writing styles and some of the messages are poorly written which may affect the performance of nmt.
to obtain a set of commit messages that are in a similar format we filtered the messages for verb direct object pattern.
we chose this pattern because a previous study shows that of commit messages follow this pattern .
to find the pattern we used a natural language processing nlp tool stanford corenlp to annotate the sentences with grammar dependencies.
grammar dependencies are a set of dependencies between parts of a sentences.
considering a phrase program a game this phrase has a dependency which is called dobj in stanford corenlp where the governor is program and the dependent is game .
for v do filter we look for dobj dependencies which represent the verb direct object pattern.
for each sentence we checked whether the sentence is begun with a dobj dependency.
if the sentence is begun with a dobj we mark the sentence as a dobj sentence.
in the end we have 32k commit messages that are dobj sentences.
generating training v alidation test sets we randomly selected 3k commits for testing 3k commits for validation and the rest 26k commits for training.
selecting v ocabularies nmt needs predefined vocabularies for commit messages and diff s. in the training set the commit messages have 16k distinct tokens words orpunctuations and the diff s have 65k distinct tokens.
we selected all the 16k tokens in the commit messages to be the vocabulary of commit messages.
we used the most frequent 50k tokens in the diff s to be the vocabulary of diff s. all the tokens that are not in the diff vocabulary only occur once in the training set.
additionally the vocabulary size of 50k is often used by other nmt models .
b. nmt training and testing in this section we describe how we trained and tested an nmt model for generating commit messages.
model we used nematus in our work because it is robust easy to use and produced best constrained systems for seven translation directions e.g.
english to german etc.
in wmt shared news translation task .
nematus is based on theano and implements the attentional rnn encoderdecoder see section iii c with several implementation differences .
training setting we borrowed the training setting that sennrich et al.
used to produce the best translation systems in wmt .
the training goal is cross entropy minimization .
the learning algorithm is stochastic gradient descent sgd with adadelta which automatically adapts the learning rate.
the size of minibatches is the size of word embeddings is the size of hidden layers is .
for each epoch the training set is reshuffled.
the model is authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
validated every 10k minibatches by bleu which is a commonly used similarity metric for machine translation.
the maximum number of epochs is 5k the maximum number of minibatches is 10m and early stopping is used .
during the training the model is saved every 30k minibatches.
so after the training a list of models are saved and the ensemble results of the last four models are used for evaluation.
one key difference between our and sennrich et al.
s training processes is that sennrich et al.
used maximum sentence length of for all the languages we used for commit messages and for diff s as explained in section iv a2.
training details we trained on the training set of 26k pairs of commit messages and diff s with a validation set of 3k pairs.
we conducted the training on an nvidia geforce gtx with 8gb memory.
the learning algorithm stopped at 210k minibatches.
because a model is saved every 30k minibatches seven models are saved from this training.
the training process took hours.
testing details while we describe our evaluation in the next section certain technical details are relevant here.
we ran nematus with the last four saved models on the testing set and we obtained the ensemble result.
we used the same gpu as we used in training.
the testing process took .
minutes.
we note that we followed the standard evaluation procedure for nmt and used a test set of 3k .
v. e v aluation using anautomatic metric in this section we evaluate the generated messages from our approach that we described in the last section.
our objective isto assess the similarity between the generated messages and the reference messages in the test set.
this section corresponds to part b in the paper overview figure .
note that this evaluation is distinct from the experiment with human evaluators that we describe in section vi which is also a component of part b. in this section we ask rq1 compared to the messages generated by a baseline are the messages generated by the nmt model more or less similar to the reference messages?
rq2 are the messages generated by the nmt model more or less similar to the reference messages when v do filter is enabled or disabled?
we ask rq1 to evaluate the nmt model compared to a baseline which we describe in the following subsection.
we ask rq2 in order to evaluate the impact of v do filter.
in the following subsections we first introduce the baseline for rq1.
then we introduce the metric for measuring similarity between two messages.
finally we report our results for the research questions.
a. baseline moses we used moses as the baseline in rq1.
moses is a popular statistical machine translation software which is often used as a baseline in evaluating machine translation systems .
for example iyer et al.
used moses as a baseline when they evaluated code nn .
to run moses for translating diff s to commit messages we trained a gramlanguage model using kenlm which is the same procedure in the study of iyer et al.
.
we did not use code nn as a baseline because in our pilot study of runningcode nn to generate commit messages code nn did not generate comparable results.
a possible reason is that code nnneeds parsing source sequences and diff s are not suitable for parsing.
b. similarity metric bleu bleu is widely used to measure the similarity between two sentences in evaluation of machine translation systems .
additionally bleu is recommended for assessing an entire test set instead of a sentence .
the calculation of bleu needs the modified n gram precisions.
for any n the modified n gram precision is calculated by pn summationtext gen ref test summationtext ngram gencntclip ngram summationtext gen ref test summationtext ngram gencntgen ngram cntclip ngram min cntgen ngram cntref ngram wheretest is the set of pairs of the generated and the reference messages in the test set gen is the set of distinct n grams in a generated message cntclipis defined in equation cntgen is the number of occurrences of an n gram in a generated message similarly cntrefis the number of the occurrences of an n gram in a reference message.
then bleu is bleu bp exp n summationdisplay n nlog pn bp braceleftbigg ifc r e r c ifc r wherenis the maximum number of grams pnis defined in equation bp is defined in equation ris the sum of the lengths of all the reference messages cis the sum of the lengths of the generated messages.
bleu scores range from to in percent .
the default value of nis which is used in our evaluation and is commonly used in other evaluations .
c. rq1 compared to the baseline the first two rows in table i list the bleu scores of moses and the nmt model we trained in section iv b which we refer to as nmt1.
the bleu score of our model is .
while the bleu score of moses is .
so according to the bleu metric the messages generated by the nmt model are more similar to the reference messages than the messages generated by the baseline.
one key reason that the attentional nmt model outperforms moses is that moses does not handle well very long source sequences with short target sequences.
particularly moses depends on giza for word alignments between source and target sequences and giza becomes very inefficient when a source sequence is authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i bleu s cores ofmoses and ourmodels on the testset model bleu len gen len ref p1 p2 p3 p4 moses .
.
.
.
.
nmt1 .
.
.
.
.
nmt232.
.
.
.
.
.
.
.
.
.
moses is the baseline model.
nmt1 is the nmt model with v do filter described in section iv b .
nmt2 is a model trained without v do filter described in section v d. len genis the total length of the generated messages cin equation .
len refis the total length of the reference messages rin equation .
the modified n gram precision pn where n is defined in equation .
this bleu score is calculated on a test set that is not v do filtered described in section v d. the other bleu scores are tested on a v do filtered test set described in section iv a4.
table ii bleu s cores ondiff so f different lengths diff length bleu len gen len ref p1 p2 p3 p4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
see table i for explanation of each column name.
the bleu scores are calculated based on the test results generated by model1 the nmt model with v do filter trained in section iv b. times longer than the target sequence or vice versa .
table i shows that the total length of the generated messages len gen in table i of moses is much longer than the total length of the reference messages which may cause the modified n gram precisions p p2 p3 andp4 of moses to be small.
to further examine the messages generated by our model we split the test set by the lengths of the diff s into four groups and calculated bleu scores separately for each group.
figure shows the distribution of the lengths of diff s in the test set and table ii shows the bleu scores for the diff s. this table shows that the diff s that have more than tokens have the highest bleu score.
one possible reason is that there are many more diff s that have more than tokens than the other smaller diff s. figure shows the distribution of the diff lengths in the training set.
this figure shows that the training set is populated by larger diff s which may cause the model to fit the larger diff s better.
in table ii the modified gram precision p4 is .
when diff lengths are between and and becomes .
when diff lengths are larger than .
this increase of p4means that the number of the grams that are shared by the generated and reference messages increase dramatically when the lengths of diff s increase to more than tokens.
in contrast p4changes much less .
to .
.
to .
in other cases.
d. rq2 impact of v do filter besides nmt1 the nmt model trained with v do filter in section iv we trained another model without v do filter which we refer to as nmt2.
in this subsection we compare nmt1 and nmt2 to see the impact of v do filter.
data set and training process for nmt2 without v do filter the data set has 75k commits.
first we extracted thetest set that is used by nmt1 so that we can compare the test results.
then from the remaining 72k commits we randomly selected 3k commits to be another test set which may contain messages that do not follow the v do pattern.
we refer to the first test set as test1 with v do filter and the second test set as test2 without v do filter .
then we randomly selected 3k for validation and used the rest 66k commits for training.
we note that the training set of nmt1 has only 26k commits so nmt2 has .
times more training data than nmt1.
the training set includes 45k distincttokens in commit messages and 110k distinct tokens in diff s. similar to the vocabulary setting we used in section iv a we used all the 45k tokens to be the vocabulary of commit messages.
we used the most frequent 100k tokens in diff s to be the vocabulary of diff s. all the tokens that are not included in the vocabulary only occur once in the training set.
we followed the same process described in section iv b .
the training process took hours.
the testing process for test1 took .
minutes and test2 took minutes.
results the third and fourth rows in table i show the bleu scores of nmt2 on test1 and test2 which are .
and .
respectively.
comparing the bleu scores of nmt1 and test1 the result shows that the messages generated bynmt2 are more similar to the reference messages in test1.
this finding indicates that although the training set without vdo filter has low quality messages there are valuable commits that do not follow the v do pattern but help the nmt model improve over test1 which follow the v do pattern.
however the bleu score of test2 is about percent lower than the bleu score of test1 which means that nmt2 does not perform well over the commits that do not follow the v dopattern.
for example a reference message in test2 is 7807cb6 diffs diffs diffs diffs the number of tokens in the diff s in the test setthe number of diff s fig.
the distribution of the lengths of diff s in the test set the number of tokens in the diff s in the training setthe number of diff s fig.
the distribution of the lengths of diff s in the training set authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ca7a229 which should be version numbers.
for such reference messages in test2 the nmt model cannot generate the same version numbers and is not meant to generate such numbers.
however similar messages in the training set cause the nmt model to try to generate such numbers for commit messages.
for example a generated message in test2 is dd38b1cc2 92007d1d7 while the reference message is run only on jdk7 for the moment .
vi.
h uman ev aluation in this section we ask human experts to evaluate the generated messages by the nmt model we described in section iv.
in section v we evaluated our model by theautomatic metric bleu.
our human study complementsthe evaluation that uses bleu in two ways.
first although bleu is a widely used metric that enables us to compare our model with others and to deliver reproducibility bleu is not recommended for evaluating individual sentences .
our human study can show how our model perform on individual messages.
second bleu calculates the textual similarity between the generated and the reference messages while the human study can evaluate the semantic similarity.
in this study we hired participants for minutes each to evaluate the similarity in a survey study.
two participants are computer science ph.d. students and participants areprofessional programmers with to years experience.
in the rest of this subsection we describe our survey design the process of conducting the survey and the survey results.
a. survey design we introduce our survey in the first page as this survey will ask you to compare two commit messages by their meaning.
you will be able to select a score between to where means there is no similarity and means that two messages are identical.
we permitted the participants to search the internet for unfamiliar concepts.
then we gave three scoring examples with recommended scores of and .
due to space limit we present only the first example in figure all the other examples are available in our online appendix section xi .
then in the remaining pages of the survey each page has one pair of the messages and we asked the participants to score the similarity by meaning.
note that the participants do not know who what generated the messages.
the order of the messages in every page is randomly decided.
in the end of the page there is an optional text box for the participants to enter their justifications.
a formal qualitative study about the participants comments will need to be performed in the future but is beyond the scope of this study.
figure shows one page of the survey.
b. survey procedure first the pairs of generated reference messages are randomly ordered in a list.
then for each participant a survey is generated with the messages in the list from a given starting point.
for example for the first three participants the surveys are generated with the messages starting from the first pair inexample of message added x to readme message edit readme recommended score explanation the two messages have only one shared word readme .
but the two messages are very similar in the meaning because added is a type of edit .
fig.
an scoring example we gave to the participants in the survey study.
below are two commit messages message added android sdk platform with api level to travis build file message remove redundant commands in travis config.
how similar are the two messages in terms of the meaning ?
no similarity whatsoever1 identical fig.
one of the pages that we ask the participants to score the similarity.
there is an optional text box for the participants to write their justifications in the end of the page.
this text box is omitted due to space limit.
the list.
in minutes the first participant was able to score pairs the second participant was able to score pairs the third participant was able to score pairs.
so the first pairs of messages were evaluated by three participants.
for the fourth participant we generated a survey starting from the62th pair and the participant stopped at 99th pair in minutes.
so after the first four participants we have pairs scored by three participants.
although it would be ideal if we obtain three scores for every pair we did not enforce all the pairsbeing scored by three participants because we want to have more pairs scored with the limited number of participants.
in the end pairs were scored by three participants pairs were scored by two participants and pairs were scored by one participant.
c. results figure shows the distribution of the median scores of the semantic similarity of the generated reference messages.
to be conservative we round down the median scores.
for example if a generated message has two scores and and the median score is .
we round down the median score to .
in total generated commit messages have scores made by the participants.
zero and seven are the two most frequent scores.
there are messages scored and messages scored which shows that the performance of our model tends to be either good or bad.
vii.
q uality assurance filter based on the results from our study with human evaluators section vi we propose a quality assurance filter qa filter to automatically detect the diff s for which the nmt model does not generate good commit messages.
by building this filter we investigate whether it is possible to automatically learn the cases where our nmt model does not perform well.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
the distribution of the median scores obtained in the human study.
there are scores in the figure.
each score is the median score of the scores made by one to three human experts for a generated message.
the scores range from to where denotes the generated message is not similar to the reference message at all and denotes the generated message is identical to the reference message.
the most frequent scores are and .
there are messages scored and234 messages scored .
for the rest of the scores the number of messages ranges from to .
in this section we describe the method of our filter how we evaluate the filter and the performance of the filter.
this section corresponds to part c in the paper overview figure .
a. qa filter our method of qa filter has three steps.
first we prepared the gold set.
we used the evaluated messages and the corresponding diff s in the human study as our gold set.
for each diff and the corresponding generated message there is a score we obtained in the human study figure that indicates whether the generated message for the diff is similar to the reference message i.e.
the actual human written message .
to be conservative we labeled the diff s that have scores of zero or one as bad and all the other diff s as not bad .
second we extracted the features of the diff s. we used term frequency inverse document frequency tf idf for every word in a diff as the features.
tf idf is widely used in machine learning for text processing which is computed based on the frequency of a word in a diff and whether the word is common in the other diff s. finally we used the data set of diff s and their labels to train a linear svm using stochastic gradient descent sgd as the learning algorithm.
after we trained the svm to predict whether the nmt model will generate a bad commit message tf idf linear svm with sgd trainingthe rest folds for trainingithfold for testing predict result for the ith foldtf idf trained modelfor ithfold i ... fig.
outline of our cross validation process.fig.
the predict results of the cross evaluation of qa filter.
qa filter reduced messages that are scored messages that are scored messages that are scored messages that are scored messages that are scored messages that are scored messages that are scored and messages that are scored .
we note that although we trained the qa filter with binary labels bad and not bad the evaluation result shows that qa filter is able to reduce more messages for lower scores.
for a diff we extract tf idfs from the diff and run the trained svm with the tf idfs.
b. cross v alidation evaluation figure illustrates our fold cross validation process.
we shuffled the gold set first and split the gold set into folds.
for each fold we trained a svm model on the other folds and tested the svm model on the one fold.
in the end we obtained the test results for every fold.
figure shows the predicts of all the folds.
in terms of detecting diff s for which the nmt model will generate bad messages qa filter has .
precision and .
recall.
furthermore if we label the messages with scores of or as good in this evaluation qa filter reduced of the bad messages at a cost of of the good messages.
viii.
e xample result table iii shows a representative example of a generated message that was rated highly by the human experts.
it includes the generated and reference messages three scores made by three participants and the corresponding diff .
in this example the reference message refers to a replacement of a call to a function called deactivate with a call to a function close .
to a human reader that is evident from the diff a call to deactivate is removed and a call to close is added.
the nmt algorithm also picked up on this change generating text close instead of mcursor.deactivate.
ix.
t hreats to validity one threat to validity is that our approach is experimented on only java projects in git repositories so they may not be representative of all the commits.
however java is a popular programming language which is used in a large number of projects.
in the future we will extend our approach to other programming languages.
another threat to validity is the quality of the commit messages.
we collected actual human written commit messages from github and used v do filter to obtain a set of relatively good quality commit messages.
but the human written messages may not contain all the useful information that should authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii e xample result diff a core ... cursortobulkcursoradaptor.java b core ... cursortobulkcursoradaptor.java public final class cursortobulkcursoradaptor ... public void close maybeunregisterobserverproxy mcursor.deactivate mcursor.close public int requery icontentobserver observer ... generated message cursortobulkcursoradapter .
close must call mcursor .
close instead of mcursor .
deactivate .
reference message call close instead of deactivate i ncursortobulkcursoradaptor .
close scores be in a commit message.
and programmers are often lazy and write poor messages .
however our objective in this paper is to generate commit messages that can be learned from the history of the repositories.
further improvement on human written messages falls outside the scope of this paper.
another threat to validity is about the human study because of the limited number of the participants.
we cannot guarantee that every final score for a generated commit message is fair.
we tried to mitigate this threat by hiring as many professional programmers as we can and having of the evaluated messages scored by three participants and of the evaluated messages scored by two participants.
additionally our use of stanford corenlp may not yield the best performance of stanford corenlp.
because stanford corenlp is trained on natural languages corenlp may not perform well on the commit messages which contain special identifiers such as variable names and software programming jargons.
several studies in sentiment analysis for software engineering have discussed the problem of using nlp tools without customization for software engineering.
in our future work we may work on this problem so that we can provide nmt with better training data.
x. d iscussion and conclusion the key advancement that this paper makes to the state ofthe art is a technique to generate short commit messages that summarize the high level rationale for a change to software.
as we note in section i a we do not claim to be able to provide new insights for completely novel changes to software that task is likely to remain in human hands forthe foreseeable future.
instead we learn from knowledgestored in a repository of changes that have already been described in commit messages.
several authors in the related literature have observed that many code changes follow similar patterns and have a similar high level rationale e.g.
.
traditionally programmers still need to manually writecommit messages from scratch even in cases where a commithas a rationale that has been described before.
what this paperdoes is automate writing commit messages based on knowledge in a repository of past changes.
our strategy was in a nutshell to collect a large repository of commits from large projects filter the commits to ensure relatively high quality commit messages and train a neural machine translation algorithm to translate from diff s to commit messages using the filtered repository.
we then evaluated the generated commit messages in two ways.
first we conducted an automated evaluation using accepted metrics and procedures from the relevant nmt literature section v .
second as a verification and for deeper analysis we also conducted an experiment with human evaluators section vi .
what we discovered is that the nmt algorithm succeeded in identifying cases where the commit had a similar rationale to others in the repository.
the evidence for this is the large bar for item in figure9 i tmeans that the human evaluatorsrated a large number of the generated messages as very closelymatching the reference messages.
however the algorithm alsogenerated substantial noise in the form of low quality messages note the large bar for item .
a likely explanation is that these include the cases that involve new insights which the nmt algorithm is unable to provide.
while creating these newinsights from the data is currently beyond the power of existing neural network based machine learning a problem observed across application domains at a minimum we would like to return a warning message to the programmer to indicate that we are unable to generate a message rather than return a low quality message.
therefore we created a quality assurance filter in section vii.
this filter helped reduce the number of low quality predictions as evident in the reduced bar for item in figure .
while we do view our work as meaningfully advancing the state of the art we by no means claim this work is definitive or completed.
we release our complete data set and implementation via an online appendix noted at the end of section iv.
our hope is that other researchers will use this data set and implementation for further research efforts.generally speaking future improvements are likely to lie in targeted training for certain types of commits combined with detection of change types.
it is probable that very high quality predictions are possible for some types of software changes but not others.
this work provides a foundation for those and other future developments.
xi.
r eproducibility our data sets scripts and results are accessible via