where is the bug and how is it fixed?
an experiment with practitioners marcel b hme national university of singapore singapore marcel.boehme acm.orgezekiel o. soremekun saarland university germany soremekun cs.uni saarland.desudipta chattopadhyay singapore university of technology and design singapore sudipta chattopadhyay sutd.edu.sg emamurho ugherughe sap berlin germany emamurho gmail.comandreas zeller saarland university germany zeller cs.uni saarland.de abstract research has produced many approaches to automatically locate explain and repair software bugs.
but do these approaches relate to the way practitioners actually locate understand and x bugs?
to help answer this question we have collected a dataset named d b the correct fault locations bug diagnoses and software patches of real errors in open source c projects that were consolidated from hundreds of debugging sessions of professional software engineers.
moreover we shed light on the entire debugging process from constructing a hypothesis to submitting a patch and how debugging time di culty and strategies vary across practitioners and types of errors.
most notably d b can serve as reality check for novel automated debugging and repair techniques.
ccs concepts software and its engineering software testing and debugging keywords debugging in practice user as tool benchmark evaluation user study acm reference format marcel b hme ezekiel o. soremekun sudipta chattopadhyay emamurho ugherughe and andreas zeller.
.
where is the bug and how is it fixed?
an experiment with practitioners.
in proceedings of 11th joint meeting of the european software engineering conference and the acm sigsoft symposium on the foundations of software engineering paderborn germany september esec fse pages.
introduction in the past decade research has produced a multitude of automated approaches for fault localization debugging and repair.
several benchmarks have become available for the empirical evaluation of such approaches.
for instance c reb and defects4j all authors conducted this work while a liated with saarland university germany.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior speci c permission and or a fee.
request permissions from permissions acm.org.
esec fse september paderborn germany copyright held by the owner author s .
publication rights licensed to association for computing machinery.
acm isbn .
.
.
.
a large number of real errors for c and java together with developer provided test suites and bug xes.
using such benchmarks researchers can make empirical claims about the e cacy of their tools and techniques.
for instance an e ective fault localization technique would rank very high a statement that was changed in the bug x .
the assumption is that practitioners would identify the same statement as thefault.
an e ective auto generated bug x would pass all test cases .
the assumption is that practitioners would accept such xes.
unfortunately debugging is not that simple particularly not for humans.
in this paper we provide another kind of benchmark one that allows reality checks .
given the complexity of the debugging process one might assume that it would be standard practice to evaluate novel techniques by means of user studies does the tool t into the process?
does it provide value?
how?
yet how humans actually debug is still not really well explored.
between and parnin and orso identi ed only a handful of articles that presented the results of a user study none of which involved actual practitioners and real errors.
since the end of we could identify only three papers that evaluated new debugging approaches with actual practitioners and real errors .
in this paper we do not attempt to evaluate a speci c approach.
instead we shed light on the entire debugging process .
speci cally we investigate how debugging time di culty and strategies vary across practitioners and types of errors.
for our benchmark we elicit which fault locations explanations and patches practitionersproduce.
we used real bugs from c reb which were systematically extracted from the most recent commits and the associated bug reports.
we asked software engineering professionals from countries to debug these software errors participants received for each error a small but succinct bug report the buggy source code and executable and a test case that fails because of this error.
weasked participants to point out the buggy statements fault localization to explain how the error comes about bug diagnosis and to develop a patch bug xing .
werecorded for each error their con dence in the correctness of their diagnosis patch thesteps taken the tools andstrategies used and thetime taken and di culty perceived in both tasks.
117esec fse september paderborn germany b hme soremekun cha opadhyay ugherughe and zeller a bug report and test case find mtime is broken behaves as mtime n lets say we created file each day in the last days mkdir tmp touch tmp a t date date yesterday y m d h m touch tmp b t date date days ago y m d h m touch tmp c t date date days ago y m d h m running a search for files younger than days we expect .
find tmp mtime tmp tmp a however with the current grep version i get .
find tmp mtime tmp b results are the same if i replace n with n or just n. b bug diagnosis and fault locations iffind is set to print les that are strictly younger than ndays mtime n it will instead print les that are exactly ndays old.
the function get comp type actually increments the argument pointer timearg parser.c .
so when the function is called the rst time parser.c timearg still points to .
however when it is called the second time parser.c timearg already points to n such that it is incorrectly classi ed as comp eq parser.c exactly ndays .
c examples of in correct patches example correct patches copy timearg and restore after rst call to get comp type.
pass a copy of timearg into rst call of get comp type.
pass a copy of timearg into call of get relative timestamp.
decrement timearg after the rst call to get comp type.
example an incorrect patch restore timearg only if classi ed as comp lt incomplete fix because it does not solve the problem for mtime n .
figure an excerpt of d b .
for the error find.66c536bb we show a the bug report and test case that a participant receives to reproduce the error b the bug diagnosis that we consolidated from those provided by participants including fault locations and c examples of ways how participants patched the error correctly or incorrectly.
weanalyzed this data and derived for each error important fault locations and a diagnosis evaluated the correctness of each submitted patch and provide new test cases that fail for incorrect patches.
findings .
to the best of our knowledge we nd the rst evidence that debugging canactually be automated and is no subjective endeavour.
in our experiment di erent practitioners provide essentially the same fault locations and the same bug diagnosis for thesame error.
if humans disagreed how could a machine ever produce the correct fault locations or the correct bug diagnosis?
moreover we nd that many of the participant submitted patches are actually incorrect while of all patches are plausible i.e.
pass the failing test case only are correct i.e.
pass our code review.
taking human error out of the equation provides opportunities for automated program repair .
we also nd that three in four incorrect patches introduce regression errors or do not x the error completely.
this provides opportunities for automated regression testing .
we also nd that practitioners are wary of debugging automation.
they might quickly adopt an auto repair tool for crashes but seem reluctant for functional bugs.
actual tools might prove such beliefs unwarranted.
benchmark .
since participants agree on essential bug features it is fair to treat their ndings as ground truth .
we have compiled our study data for all bugs into a benchmark named d b which is the second central contribution of this paper.
an excerpt of d b for a speci c bug is shown in figure .
d b can be used in cost e ective user studies to investigate how debugging time debugging di culty and patch correctness improve with the novel debugging repair aid.
d b can be used to evaluate without a user study how well novel automated tools perform against professional software developers in the tasks of fault localization debugging and repair.
study design the study design discusses our recruitment strategy the objects and infrastructure and the variables that we modi ed and observed in our experiment.
the goal of the study design is to ensure that the design is appropriate for the objectives of the study.
we follow the canonical design for controlled experiments in software engineering with human participants as recommended by ko et al.
.
.
research questions the main objective of the experiment is to construct a benchmark that allows to evaluate automated fault localization bug diagnosis and software repair techniques w.r.t.
the judgment of actual professional software developers.
we also study the various aspects of debugging in practice and opportunities to automate diagnosis and repair guided by the following research questions .
rq.
time and di culty.
given an error how much time do developers spend understanding and explaining the error and how much time patching it?
how di cult do they perceive the tasks of bug diagnosis and patch generation?
rq.
fault locations and patches.
which statements do developers localize as faulty?
how are the fault locations distributed across the program?
how many of the provided patches are plausible?
how many are correct?
rq.
diagnosis strategies.
which strategies do developers employ to understand the runtime actions leading to the error?
rq.
repair ingredients.
what are the pertinent building blocks of a correct repair?
how complex are the provided patches?
rq.
debugging automation.
is there a consensus among developers during fault localization and diagnosis?
hence can debugging be automated?
do they believe that diagnosis or repair for an error will ever be automated and why?
.
objects and infrastructure theobjects under study are real software errors systematically extracted from the bug reports and commit logs of two open source c projects nd grep .
the infrastructure is a lightweight docker container that can quickly be installed remotely on any host os .
the errors test cases bug reports source code and the complete docker infrastructure is available for download .
the objects originate from a larger error benchmark called c re b .
errors were systematically extracted from the most recent commits and the bug reports in four projects.
find and grep are well known well maintained and widely deployed opensource c programs with a codebase of 17k and 19k loc respectively.
for each error we provide a failing test case a simpli ed bug report and a large regression test suite see figure a .
we chose two subjects out of the four available to limit the time a participant spends in our study to a maximum of three working days and to help participants to get accustomed to at most two code bases.
118where is the bug and how is it fixed?
an experiment with practitioners esec fse september paderborn germany figure screenshot of the provided virtual environment.
to conduct the study remotely and in an unsupervised manner we developed a virtual environment based on docker .
the virtual environment is a lightweight docker image with an ubuntu .
guest os containing a folder for each buggy version of either grep or nd in total .
a script generates the participant id for the responses by a participant.
this ensures anonymity and prevents us from establishing the identity of any speci c participant.
at the same time we can anonymously attribute a response for a di erent error to the same participant to measure for instance how code familiarity increases over time.
the same script does some folder scrambling to randomize the order in which participants debug the provided errors the rst error for one participant might be the last error for another.
the scrambling controls for learning e ects.
for instance if every participant would start with the same error this error might incorrectly be classi ed as very di cult.
the docker image contains the most common development and debugging tools for c including gdb vim and eclipse.
participants were encouraged to install their own tools and copy the created folders onto their own machine.
a screenshot is shown in figure .
.
pilot studies researchers and students ko et al.
note that the design of a study with human participants is necessarily an iterative process.
therefore a critical step in preparing an experiment is to run pilot studies.
hence we rst evaluated our design and infrastructure in a sandbox pilot where we the researchers were the participants.
this allowed us to quickly catch the most obvious of problems at no extra cost.
thereupon we sought to recruit several graduate students from saarland university for the pilot study .
we advertised the study in lectures pasted posters on public bulletin boards and sent emails to potentially interested students.
from interested students we selected ve that consider their own level of skill in the programming with c asadvanced or experts .1we conducted the pilot study as supervised observational study in house in our computer lab.
after lling the consent form and answering demographic questions we introduced the errors and infrastructure in a small hands on tutorial that lasted about minutes.
then the students had eight hours including a one hour lunch break to debug as many errors as they could.
we recorded the screen of each student using a screen capturing tool.
1note that self assessment of level of skill should always be taken with a grain of salt cf.
dunning kruger e ect .independent of the outcome all participants received eur as monetary compensation.
while none of the data collected in the pilot studies was used for the nal results the pilot studies helped us to improve our study design in several ways no students.
for the main study we would use only software engineering professionals.
in seven hours our student participants submitted only a sum total of ve patches.
on average a student xed one error in eight hours while in the main study a professional xed errors in .
hours .
the feedback from students was that they under estimated the required e ort and over estimated their own level of skill.
no screen capturing.
the video of only a single participant would take several gigabyte of storage and it needs to be transferred online to a central storage.
this was deemed not viable.
good infrastructure.
the setup infrastructure and training material was deemed appropriate.
.
main study software professionals we make available the training material the virtual infrastructure the questionnaire that was provided for each error the collected data .
the experiment procedure speci es the concrete steps a participant follows from the beginning of the experiment to its end.
recruitment .
first participants would need to be recruited.
to select our candidates we designed an online questionnaire.
the questionnaire asks general questions about debugging in practice after which developers have the option to sign up for the experiment.
we sent the link to more than developers on github and posted the link to software development usergroups at meetup.com on six freelancer platforms including freelancer upwork and guru and on social as well as professional networks such as facebook and linkedin.
the job postings on the freelancer platforms were the most e ective recruitment strategy.
we started three advertisement campaigns in aug mar and july following which we had the highest response rate lasting for about one month each.
we received the rst response in aug and the most recent response more than one year later in oct .
in total we received responses out of which indicated an interest in participating in the experiment.
selection .
we selected and invited professional software engineers based on their experience with c programming.
however in the two years of recruitment only participants actually entered and completed the experiment.
there are several reasons for the high attrition rate.
interested candidates changed their mind in the time until we sent out the invitation when they understood the extent of the experiments working days or when they received the remote infrastructure and understood the di culty of the experiment 17k 19k unfamiliar lines of code .
demographics .
the nal participants were one researcher and eleven professional software engineers from six countries russia india slovenia spain canada and ukraine .
all professionals had pro les with upwork and at least success rate in previous jobs.
training .
before starting with the study we asked participants to set up the docker image and get familiar with the infrastructure.
we made available readme slides and tutorial videos .
minutes each that explain the goals of our study and provide details about subjects infrastructure and experimental procedure.
119esec fse september paderborn germany b hme soremekun cha opadhyay ugherughe and zeller how di cult was it to understand the runtime actions leading to the error?
not at all di cult slightly di cult moderately di cult very di cult extremely di cult which tools did you use to understand the runtime actions leading to the error?
how much time did you spend understanding the runtime actions leading to the error?
minute or less minutes minutes minutes .
.
.
minutes minutes or more enter to code regions needed to explain the root cause of the error.
what is the root cause of the error?
how does it come about?
how con dent are you about the correctness of your explanation?
not at all con dent slightly con dent moderately con dent very con dent extremely con dent if you could not explain the error what prevented you from so?
which concrete steps did you take to understand the runtime actions?
do you believe that the root cause of the error can be explained intuitively by the push of a button?yes in principle a tool might be able to explain this error.
no there will never be a tool that can explain this error.
why do you not believe so?
figure questions on the fault locations and bug diagnosis participants could watch the slides and the tutorial videos at their own pace.
the training materials are available .
moreover we informed them that they could contact us via email in case of problems.
we provided technical support whenever needed.
tasks .
after getting familiar with the infrastructure and the programs participants chose a folder containing the rst buggy version to debug.
this folder contains a link to the questionnaire that they are supposed to ll in relation with the current buggy version.
the text eld containing the participant s id is set automatically.
the questionnaire contains the technical questions is made available and is discussed in section .
in more details.
we asked each participant to spend approximately minutes per error in order to remain within a hour time frame.
from the pilot study we learned that incentive is important.
so we asked them to x at least of the errors in one project e.g.
grep before being able to proceed to the next project e.g.
nd .
debrie ng .
after the experiment participants were debriefed and compensated.
we explained how the data is used and why our research is important.
participants would ll a nal questionnaire to provide general feedback on experiment design and infrastructure.
for instance participants point out that sometimes it was di cult to properly distinguish time spent on diagnosis from time spent on xing.
overall the participants enjoyed the experiment and solving many small challenges in a limited time.
compensation .
it is always di cult to determine the appropriate amount for monetary compensation.
some guidelines recommend the average hourly rate for professionals the rationale being that professionals in that eld cannot or will not participate without pay for work time lost.
assuming working hours and an hourly rate of usd each participant received usd in compensation for their time and e orts.
the modalities were formally handled via upwork .
.
variables and measures the main objective of this study is to collect the fault locations bug diagnoses and software patches that each participant produced for each error.
to assess the reliability of their responses we use a triangulation question which asks for their con dence in the correctness of the produced artifacts.
in addition to these artifacts for each error we also measure the perceived di culty of each debugging task i.e.
bug diagnosis and bug xing the time spent with each how di cult was it to x the error?
not at all di cult slightly di cult moderately di cult very di cult extremely di cult how much time did you spend xing the error?
minute or less minutes minutes minutes .
.
.
minutes minutes or more important copy paste the generated patch here.
in a few words and on a high level what did you change to x for the error?
how con dent are you about the correctness of your x?
not at all con dent slightly con dent moderately con dent very con dent extremely con dent in a few words how did you make sure this is a good x?
if you could not x the bug what prevented you from so?
do you believe that this error can be xed reliably by the push of a button?yes in principle a tool could x this error reliably.
no there will never be a tool that can x this error reliably.
why do you not believe so?
figure questions on generating the software patch debugging task and their opinion on whether bug diagnosis or repair will ever be fully automated for the given error.
the questions that we ask for each error are shown in gures and .
to measure qualitative attributes we utilize the point likert scale .
a point likert scale allows to measure otherwise qualitative properties on a symmetric scale where each item takes a value from to and the distance between each item is assumed to be equal.2for instance for question in figure we can assign the value to not at all di cult up to the value for extremely di cult .
an average di culty of .
would indicate that most respondents feel that this particular error is very toextremely di cult to x while only few think it was not at all di cult .
we note that all data including time is self reported rather than recorded during observation.
participants ll questionnaires and provide the data on their own.
this allowed us to conduct the study fully remotely without supervision while they could work in their every day environment.
since freelancers are typically payed by the hour upwork provides mechanisms to ensure that working time is correctly reported.
while self reports might be subject to cognitive bias they also reduce observer expectancy bias and experimenter bias .
perry et al.
conducted an observational study with software developers in four software development departments and found that the time diaries which were created by the developers correspond su ciently with the time diaries that were created by observers.
in other words in the software development domain self reports correspond su ciently with independent researcher observations.
we checked the plausibility of the submitted patches by executing the complete test suite and the previously failing test case.
we checked the correctness of the submitted patches using internal code reviews.
two researchers spent about two days discussing and reviewing the patches together.
moreover we designate a patch as incorrect only if we can provide a rationale.
generally every qualitative analysis was conducted by at least one researcher and cross checked by at least one other researcher.
study results overall real errors in open source c programs were diagnosed and patched by participants who together spent working days .
2however the likert scale is robust to violations of the equal distance assumption even with larger distortions of perceived distances between items e.g.
slightly vs. moderately familiar likert performs close to where intervals are perceived equal .
120where is the bug and how is it fixed?
an experiment with practitioners esec fse september paderborn germany combinedbug diagnosispatching not at all difficultslightly difficultmoderately difficultvery difficultextremely difficult 010203040506070809001020304050600102030405060debugging fixing time in min type crashfunctionalinfinite loopresource leakfigure relationship between average time spent and di culty perceived for patching and diagnosing a bug.
each point is one of bugs the shape of which determines its bug type i.e.
crash functional error in nite loop or resource leak .
rq.
time and di culty our data on debugging time and di culty can be used in coste ective user studies that set out to show how a novel debugging aid improves the debugging process in practice.
we also elicit causes of di culties during the manual debugging process.
time and di culty .on average participants rated an error as moderately di cult to explain .
and slightly di cult to patch .
.
on average participants spent and minutes on diagnosing and patching an error respectively.
there is a linear and positive relationship between perceived di culty and the time spent debugging.
as we can see in figure participants perceived four errors to be very di cult to diagnose .
these are three functional errors and one crash.
participants spent about minutes diagnosing the error that was most di cult to diagnose.
however there are also nine errors perceived to be slightly di cult to diagnose with the main cluster situated between and minutes of diagnosis time.
participants perceived one functional error as very di cult to patch and spent about minutes patching it.
however there are also two bugs perceived to be not at all di cult to patch and took about minutes.
why are some errors very di cult ?
there are four errors rated as very di cult to diagnose .
in many cases missing documentation for certain functions ags or data structures were mentioned as reasons for such di culty.
other times developers start out with an incorrect hypothesis before moving on to the correct one.
for instance the crash in grep.3c3bdace is caused by a corrupted heap but the crash location is very distant from the location where the heap is corrupted.
the crash and another functional error are caused by a simple operator fault.
three of the four bugs which are very di cult to diagnose are actually xed in a single line .
for the only error that is both very di cult to diagnose and patch the developer provided patch is actually very complex involving added and deleted lines of code.
only one participant provided a correct patch.
rq.
fault locations and patches our data on those program locations which practitioners need to explain how an error comes about i.e.
fault locations can be used for a more realistic evaluation of automated fault localization and motivates the development of techniques that point out multiple pertinent fault locations.
our data on multiple patches for the same error can be used to evaluate auto repair techniques and motivates research in automated repair and its integration with automated regression test generation to circumvent the considerable human error.
regions per errorstatements per errorstatements per region 024681012141618202224262830locationsfigure boxplots showing the number of contiguous regions per bug diagnosis left the number of statements per diagnosis middle and the number of statements per contiguous region right .
for example le.c speci es six statements in two contiguous regions.
fault locations .the middle of consolidated bug diagnoses