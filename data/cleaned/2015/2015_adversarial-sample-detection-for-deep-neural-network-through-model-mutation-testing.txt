adversarial sample detection for deep neural network through model mutation testing 1stjingyi wang shenzhen university singapore u. of tech.
and design wangjyee gmail.com 4thxinyu wang zhejiang university wangxinyu zju.edu.cn2ndguoliang dong zhejiang university dgl prc zju.edu.cn 5thpeixin zhang zhejiang university pxzhang94 zju.edu.cn3rdjun sun singapore u. of tech.
and design sunjun sutd.edu.sg abstract deep neural networks dnn have been shown to be useful in a wide range of applications.
however they are also known to be vulnerable to adversarial samples.
by transforming a normal sample with some carefully crafted human imperceptible perturbations even highly accurate dnn make wrong decisions.
multiple defense mechanisms have been proposed which aim to hinder the generation of such adversarial samples.
however a recent work show that most of them are ineffective.
in this work we propose an alternative approach to detect adversarial samples at runtime.
our main observation is that adversarial samples are much more sensitive than normal samples if we impose random mutations on the dnn.
we thus first propose a measure of sensitivity and show empirically that normal samples and adversarial samples have distinguishable sensitivity.
we then integrate statistical hypothesis testing and model mutation testing to check whether an input sample is likely to be normal or adversarial at runtime by measuring its sensitivity.
we evaluated our approach on the mnist and cifar10 datasets.
the results show that our approach detects adversarial samples generated by state of the art attacking methods efficiently and accurately.
keywords adversarial sample detection deep neural network mutation testing sensitivity i. i ntroduction in recent years deep neural networks dnn have been shown to be useful in a wide range of applications including computer vision speech recognition and malware detection .
however recent research has shown that dnn can be easily fooled by adversarial samples i.e.
normal samples imposed with small human imperceptible changes a.k.a.
perturbations .
many dnn based systems like image classification and speech recognition are shown to be vulnerable to such adversarial samples.
this undermines using dnn in safety critical applications like self driving cars and malware detection .
to mitigate the threat of adversarial samples the machine learning community has proposed multiple approaches to improve the robustness of the dnn model.
for example an intuitive approach is data augmentation.
the basic idea is to include adversarial samples into the training data and retrain the dnn .
it has been shown that data augmentation improves the dnn to some extent.
however it does not help defend against unseen adversarial samples especially those obtained through different attacking methods.
alternative approaches include robust optimization and adversarial training which take adversarial perturbation into consideration and solve the robust optimization problem directly during model training.
however such approaches usually increase the training cost significantly.
meanwhile the software engineering community attempts to tackle the problem using techniques like software testing and verification.
in neuron coverage was first proposed to be a criteria for testing dnn.
subsequently multiple testing metrics based on the range coverage of neurons were proposed .
both white box testing black box testing and concolic testing strategies have been proposed to generate adversarial samples for adversarial training.
however testing alone does not help in improving the robustness of dnn nor does it provide guarantee that a well tested dnn is robust against new adversarial samples.
the alternative approach is to formally verify that a given dnn is robust or satisfies certain related properties using techniques like smt solving and abstract interpretation .
however these techniques usually have non negligible cost and only work for a limited class of dnn and properties .
in this work we provide a complementary perspective and propose an approach for detecting adversarial samples at runtime.
the idea is that given an arbitrary input sample to a dnn to decide at runtime whether it is likely to be an adversarial sample or not.
if it is we raise an alarm and report that the sample is suspicious with certain confidence.
once detected it can be rejected or checked depending on different applications.
our detection algorithm integrates mutation testing of dnn models and statistical hypothesis testing .
it is designed based on the observation that adversarial samples are much more sensitive to mutation on the dnn than normal samples i.e.
if we mutate the dnn slightly the mutated dnn is more likely to change the label on the adversarial sample than that on the normal one.
this is illustrated in fig.
.
the left figure shows a label change on a normal sample i.e.
given a normal sample which is classified as ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
cat mutationdnn mutated dnncat dogadversarial cat dog mutationdnn mutated dnndog cat fig.
label change of a normal sample and an adversarial sample against dnn mutation models.
a cat a label change occurs if the mutated dnn classifies the input as a dog.
the right figure shows a label change on an adversarial sample i.e.
given an adversarial sample which is mis classified as a dog a label change occurs if the mutated dnn classifies the input as a cat.
our empirical study confirms that the label change rate lcr of adversarial samples is significantly higher than that of normal samples against a set of dnn mutants.
we thus propose a measure of a sample s sensitivity against a set of dnn mutants in terms of lcr.
we further adopt statistical analysis methods like receiver operating characteristic roc to show that we can distinguish adversarial samples and normal samples with high accuracy based on lcr.
our algorithm then takes a dnn model as input generates a set of dnn mutants and applies statistical hypothesis testing to check whether the given input sample has a high lcr and thus is likely to be adversarial.
we implement our approach as a self contained toolkit called mmutant .
we apply our approach on the mnist and cifar10 dataset against the state of the art attacking methods for generating adversarial samples.
the results show that our approach detects adversarial samples efficiently with high accuracy.
all four dnn mutation operators we experimented with show promising results on detecting groups of adversarial samples e.g.
capable of detecting most of the adversarial samples within around dnn mutants.
in particular using dnn mutants generated by neuron activation inverse nai operator we manage to detect .
of the adversarial samples with .
mutations for mnist and .
of the adversarial samples with .
mutations for cifar10 on average.
ii.
b ackground in this section we review state of the art methods for generating adversarial samples for dnn and define our problem.
a. adversarial samples for deep neural networks in this work we focus on dnn classifiers which take a given sample and label the sample accordingly e.g.
as a certain object .
in the following we use xto denote an input sample for a dnn f.w eu s e cxto denote the ground truth label of x. given an input sample xand a dnn f w e can obtain the label of the input xunderfby performing forward propagation.
xis regarded as an adversarial sample with respect to the dnn fiff x negationslash cx.xis regarded as a normal sample with respect to the dnn fiff x cx.
notice that under our definition those samples in the training testing dataset wrongly labeled by fare also adversarial samples.since szegedy et al.
discoveried that neural networks are vulnerable to adversarial samples many attacking methods have been developed on how to generate adversarial samples efficiently e.g.
with minimal perturbation .
that is given a normal sample x an attacker aims to find a minimum perturbation xwhich satisfies f x x negationslash cx.
in the following we briefly introduce several state of the art attacking algorithms.
fgsm the fast gradient sign method fgsm is designed based on the intuition that we can change the label of an input sample by changing its softmax value to the largest extent which is represented by its gradient.
the implementation of fgsm is straightforward and efficient.
by simply adding up the sign of gradient of the cost function with respect to the input we could quickly obtain a potential adversarial counterpart of a normal sample by the follow formulation x x epsilon1sign j x cx wherejis the cost used to train the model epsilon1is the attacking step size and are the parameters.
notice that fgsm does not guarantee that the adversarial perturbation is minimal.
jsma jacobian based saliency map attack jsma is devised to attack a model with minimal perturbation which enables the adversarial sample to mislead the target model into classifying it with certain attacker desired label.
it is a greedy algorithm that changes one pixel during each iteration to increase the probability of having the target label.
the idea is to calculate a saliency map based on the jacobian matrix to model the impact that each pixel imposes on the target classification.
with the saliency map the algorithm picks the pixel which may have the most significant influence on the desired change and then increases it to the maximum value.
the process is repeated until it reaches one of the stopping criteria i.e.
the number of pixels modified has reached the bound or the target label has been achieved.
define ai ft x xi bi summationdisplay k negationslash t fk x xi then the saliency map at each iteration is defined as follow s x t i braceleftbigg ai bi if ai 0and bi otherwise authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
however it is too strict to select one pixel at a time because few pixels could meet that definition.
thus instead of picking one pixel at a time the authors proposed to pick two pixels to modify according to the follow objective arg max p1 p2 parenleftbigg ft x xp1 ft x xp2 parenrightbigg vextendsingle vextendsingle vextendsingle vextendsingle vextendsingle vextendsingle summationdisplay i p1 p2 summationdisplay k negationslash t fk x xi vextendsingle vextendsingle vextendsingle vextendsingle vextendsingle vextendsingle where p1 p2 is the candidate pair and tis the target class.
jsma is relatively time consuming and memory consuming since it needs to compute the jacobian matrix and pick out a pair from nearly parenleftbign parenrightbig candidate pairs at each iteration.
deepfool the idea of deepfool df is to make the normal samples cross the decision boundary with minimal perturbations .
the authors first deduced an iterative algorithm for binary classifiers with tayler s formula and then analytically derived the solution for multi class classifiers.
the exact derivation process is complicated and thus we refer the readers to for details.
c w carlini et al.
proposed a group of attacks based on three distance metrics.
the key idea is to solve an optimization problem which minimizes the perturbation imposed on the normal sample with certain distance metric and maximizes the probability of the target class label.
the objective function is as follow argmin x c f x t where xis defined according to some distance metric e.g l0 l2 l x x xis the clipped adversarial sample and tis its target label.
the idea is to devise a clip function for the adversarial sample such that the value of each pixel dose not exceed the legal range.
the clip function and the best loss function according to are shown as follows.
clip x .
tanh x loss f x t max max g x c c negationslash t g x t whereg x denotes the output vector of a model and tis the target class.
readers can refer to for details.
black box all the above mentioned attacks are white box attacks which means that the attackers require the full knowledge of the dnn model.
black box bb attack only needs to know the output of the dnn model given a certain input sample.
the idea is to train a substitute model to mimic the behaviors of the target model with data augmentation.
then it applies one of the existing attack algorithm e.g.
fgsm and jsma to generate adversarial samples for the substitute model.
the key assumption to its success is that the adversarial samples transfer between different model architectures .
b. problem definition observing that adversarial samples are relatively easy to craft a variety of defense mechanisms against adversarial samples have been proposed aswe have briefly introduced in section i. however athalye et al.
systematically evaluated the state of the art defense mechanisms recently and showed that most of them are ineffective.
alternative defense mechanisms are thus desirable.
in this work we take a complementary perspective and propose to detect adversarial samples at runtime using techniques from the software engineering community.
the problem is given an input sample xto a deployed dnn f how can we efficiently and accurately decide whether f x cx i.e.
a normal sample or not i.e.
an adversarial sample ?
if we know that xis likely an adversarial sample we could reject it or further check it to avoid bad decisions.
furthermore can we quantify some confidence on the drawn conclusion?
iii.
o ur approach our approach is based on the hypothesis that in most cases adversarial samples are more sensitive to mutations on the dnn model than normal samples.
that is if we generate a set of slightly mutated dnn models based on the given dnn model the mutated dnn models are more likely to label an adversarial sample with a label different from the label generated by the original dnn model as illustrated in fig.
.
in other words our approach is designed based on a measure of sensitivity for differentiating adversarial samples and normal samples.
in the literature multiple measures have been proposed to capture their differences e.g.
density estimate model uncertainty estimate and sensitivity to input perturbation .
our measure however allows us to detect adversarial samples at runtime efficiently through model mutation testing.
a. mutating deep neural networks in order to test our hypothesis and develop a practical algorithm we need a systematic way of generating mutants of a given dnn model.
we adopt the method developed in which is a proposal of applying mutation testing to dnn.
mutation testing is a well known technique to evaluate the quality of a test suiteand and thus is different from our work.
the idea is to generate multiple mutations of the program under test by applying a set of mutation operators in order to see how many of the mutants can be killed by the test suite.
the definition of the mutation operators is a core component of the technique.
given the difference between traditional software systems and dnn mutation operators designed for traditional programs cannot be directly applied to dnn.
in ma et al.
introduced a set of mutation operators for dnn based systems at different levels like source level e.g.
the training data and training programs and model level e.g.
the dnn model .
in this work we require a large group of slightly mutated models for runtime adversarial sample detection.
of all the mutation operators proposed in mutation operators defined at the source level are not considered.
the reason is that we would need to train the mutated models from scratch which is often time consuming.
we thus focus on the modellevel operators which modify the original model directly to authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i dnn model mutation operators mutation operator level description gaussian fuzzing gf weight fuzz weight by gaussian distribution weight shuffling ws neuron shuffle selected weights neuron switch ns neuron switch two neurons within a layer neuron activation inverse nai neuron change the activation status of a neuron obtain mutated models without training.
specifically we adopt four of the eight defined operators from shown in table i. for example nai means that we change the activation status of a certain number of neurons in the original model.
notice that the other four operators defined in are not applicable due to the specific architecture of the deep learning models we focus on in this work.
b. evaluating our hypothesis we first conduct experiments to measure the label change rate lcr of adversarial samples and normal samples when we feed them into a set of mutated dnn models.
given an input sample x either normal or adversarial and a dnn modelf we first adopt the model mutation operators shown in table i to obtain a set of mutated models.
note that some of the resultant mutated models may be of low quality i.e.
their classification accuracy on the test data drops significantly.
we discharge those low quality ones and only keep those accurate mutated models which retain an accuracy on the test data i.e.
at least of the accuracy of the original model to ensure that the decision boundary does not perturb too much.
once we obtain such a set of mutated models f we then obtain the label fi x of the input sample xon every mutated model fi f. we define lcr on a sample xas follows with respect tof .
x fi fi fa n df i x negationslash f x f where s is the number of elements in a set s. intuitively x measures how sensitive an input sample xis on the mutations of a dnn model.
table ii summarizes our empirical study on measuring x using two popular dataset i.e.
the mnist and cifar10 dataset and multiple state of the art attacking methods.
a total of mutated models are generated using nai operator which randomly selects some neurons and changes their activation status.
the first column shows the name of the dataset.
the second shows the mutation rate i.e.
the percentage of the neurons whose activation status are changed.
the third shows the average lcr with confidence interval of significance level of normal samples randomly selected from the testing set.
the remaining columns show the average lcr with confidence interval of significance level of adversarial samples which are generated using state of the art methods.
note that column wrongly labeled are samples from the testing set which are wrongly labeled by the original dnn model.
based on the results we can observe that at any mutation rate the values of the adversarial samples are significantlyoriginal mutation mutation positive sample negative sample adversarial sampledecision boundaries fig.
an explanatory model of the model mutation effect.
higher than those of the normal samples.
adv is significantly larger than nor.
further study on the lcr distance between normal and adversarial samples with respect to different model mutation operators is presented in section iv.
the results are consistent.
a practical implication of the observation is that given an input samplex we could potentially detect whether xis likely to be normal or adversarial by checking x .
c. explanatory model in the following we use a simple model to explain the above observation.
recall that adversarial samples are generated in a way which tries to minimize the modification to a normal sample while is still able to cross the decision boundary.
different kinds of attacks use different approaches to achieve this goal.
our hypothesis is that most adversarial samples generated by existing methods are near the decision boundary to minimize the modification .
as a result as we randomly mutate the model and perturb the decision boundary adversarial samples are more likely to cross the mutated decision boundaries i.e.
if we feed an adversarial sample to a mutated model the output label has a higher chance to change from its original label.
this is illustrated visually in fig.
.
d. the detection algorithm the results shown in table ii suggests that we can use lcr to distinguish adversarial samples and normal samples.
in the following we present an algorithm which is designed to detect adversarial samples at runtime based on measuring the lcr of a given sample.
the algorithm is based on the idea of statistical model checking .
the inputs of our algorithm are a dnn model f a sample xand a threshold hwhich is used to decide whether the input is adversarial.
we will discuss later on how to identify hsystematically.
the basic idea of our algorithm is to use hypothesis testing to decide the truthfulness of two mutual authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii average shown in percentage with confidence interval of significance level for normal samples and adversarial samples under nai mutated models.
dataset mutation rate normal samplesadversarial samples wrong labeled fgsm jsma c w black box deepfool mnist0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cifar100.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
exclusive hypothesis.
h0 x h h1 x h three standard additional parameters and are used to control the probability of making an error.
that is we would like to guarantee that the probability of a type i respectively a type ii error which rejects h0 respectively h1 whileh0 respectively h1 holds is less or equal to respectively .
the test needs to be relaxed with an indifferent region r r where neither hypothesis is rejected and the test continues to bound both types of errors .
in practice the parameters i.e.
and can often be decided by how much testing resources are available.
in general more resource is required for a smaller error bound.
our detection algorithm keeps generating accurate mutated models with an accuracy more than certain threshold on the testing data from the original model and evaluating x until a stopping condition is satisfied.
we remark that in practice we could generate a set of accurate mutated models before hand and simply use them at runtime to further save detection time.
there are two main methods to decide when the testing process can be stopped i.e.
we have sufficient confidence to reject a hypothesis.
one is the fixed size sampling test fsst which runs a predefined number of tests.
one difficulty of this approach is to find an appropriate number of tests to be performed such that the error bounds are valid.
the other approach is the sequential probability ratio test sprt .
sprt dynamically decides whether to reject or not a hypothesis every time after we update x which requires a variable number of mutated models.
sprt is usually faster than fsst as the testing process ends as soon as a conclusion is made.
in this work we use sprt for the detection.
the details of our sprt based algorithm is shown in algorithm .
the inputs of the detection algorithm include the input sample x the original dnn model f a mutation rate and a threshold of lcr h. besides the detection is error bounded by angbracketleft angbracketrightand relaxed with an indifference region .
to apply sprt we keep generating accurate mutated models at line .
the details of generating mutated models using the four operators in table i are shown in algorithm algorithm algorithm and algorithm respectively.
we then evaluate whetherfi x f x at line .
if we observe a label change ofxusing the mutated model fi we calculate and update thesprt probability ratio at line as pr pz p1 n z pz p0 n z withp1 h andp0 h .
the algorithm stops whenever a hypothesis is accepted either at line or line .
we remark that sprt is guaranteed to terminate with probability .
we briefly introduce the nai operator shown in algorithm as an example of the four mutation operators.
we first obtain the set of nunique neurons1at line .
then we randomly select n neurons is the mutation rate for activation status inverse at line .
afterwards we traverse the model f layer by layer at line and take those selected neurons at line .
we then inverse the activation status of the selected neurons by multiplying their weights with at line .
iv .
i mplementa tion and ev alua tion we have implemented our approach in a self contained toolkit which is available online .
it is implemented in python with about 5k lines of code.
in the following we evaluate the accuracy and efficiency of our approach through multiple experiments.
a. experiment settings a datasets and models we adopt two popular image datasets for our evaluation mnist and cifar10.
each dataset has images for training and images for testing.
the target models for mnist and cifar10 are lenet and googllenet respectively.
the accuracy of our trained models on training and testing dataset are .
.
for mnist and .
.
for cifar10 respectively which both achieve state of the art performance.
b mutated models generation we employ the four mutation operators shown in table i to generate mutated models.
in total we have neurons for the mnist model and7914 neurons for the cifar10 model.
for each mutation operator we generate three groups of mutation models from the original trained model using different mutation rate to see its effect.
the mutation rate we use for the mnist model is .
.
.
and .
.
.
for the cifar10 model since there are more neurons .
note 1for convolutional layer each slide of convolutional kernel is regarded as a neuron authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm sprt detect x f h 1letstop false 2letz be the number of mutated models fithat satisfyfi x negationslash f x 3letn be the total number of generated mutated models so far 4while !stop do apply a mutation operator to randomly generate an accurate mutation model fioffwith mutation rate 6n n iffi x negationslash f x then z z calculate the sprt probability ratio as pr ifpr then accept the hypothesis that x hand report the input as an adversarial sample with error bounded by return ifpr then accept the hypothesis that x hand report the input as a normal sample with error bounded by return algorithm nai f 1letnbe the set of unique neurons 2randomly select n unique neurons 3for every layer in fdo letqbe the set of selected neurons in this layer ifq negationslash then forq qdo q.weight q.weight that some mutation models may have significantly worse performance so not all mutated models are valid.
in our experiment we only keep those mutation models whose accuracy on the testing dataset is not lower than of that of its seed model.
for each mutation rate we generate such accurate mutated models for our experiments.
c adversarial samples generation we test our detection algorithm against four state of the art attacks in clverhans and deepfool detailed in section ii .
for each kind of attack we generate a set of adversarial samples for evaluation.
the parameters for each kind of attack to generate the adversarial samples are summarized as follows.
fgsm there is only one parameter to control the scale of perturbation.
we set it as .
for mnist and .
for ciafr10 according to the original paper.algorithm gf f 1letwbe the parameters of f 2extract the parameters of flayer by layer 3letnbe the total number of parameters of f 4randomly select n parameters to fuzz 5for every layer in fdo letw be the parameters of this layer find all the selected parameters pinw ifp negationslash then let avg w let std w for every parameter in pdo randomly assign the parameter according ton 2 algorithm ws f 1letnbe the set of unique neurons 2randomly select n unique neurons to shuffle 3for every layer in fdo letqbe the set of selected neurons in this layer ifq negationslash then forq qdo q.weight shuffle q.weight jsma there is only one parameter to control the maximum distortion.
we set it as for both datasets which is slightly smaller than the original paper.
c w there are three types of attacks proposed in l0 l2andl .
we adopt l2attack according to the author s recommendation.
we also set the scale coefficient to be .
for both datasets.
we set the iteration number to be for mnist and for cifar10 according to the original paper.
deepfool we set the maximum number of iterations to be and the termination criterion to prevent vanishing updates to be .
for both datasets which is a default setting in the original paper.
black box the key setting of the black box attack is to train a substitute model of the target model.
the substitute model for mnist is the first model defined in appedix a of .
for cifar10 we use the lenet as the surrogate model.
afterwards the attack algorithm we used for the surrogate model is fgsm.
for each attack we make attempts to generate adversarial samples.
notice that not all attempts are successful and as a result we manage to generate no more than adversarial samples for each attack.
further recall that according to our definition those samples in the testing dataset which are wrongly labeled by the trained dnn are also adversarial samples.
thus in addition to the adversarial samples generated authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm ns f 1for every layer in fdo letnbe the number of unique neurons in this layer randomly select n unique neurons letqbe the set of selected neurons randomly switch the weights of neurons in q table iii number of samples in each group.
dataset attack samples mnistnormal wrongly labeled fgsm jsma bb c w deepfool cifar10normal wrongly labeled fgsm jsma bb c w deepfool from the attacking methods we attempt to randomly select samples from the testing dataset which are wrongly classified by the target model as well.
table iii summarizes the number of normal samples and valid adversarial samples for each kind of attack used for the experiments.
b. evaluation metrics a distance of label change rate we use dlcr adv nor where adv and nor is the average lcr of adversarial samples and normal samples to measure the distance between the lcr of adversarial samples and normal samples.
the larger the value is the more significant is the difference.
b receiver characteristics operator since our detection algorithm works based on a threshold lcr h we first adopt receiver characteristic operator roc curve to see how good our proposed feature i.e.
lcr under model mutation is to distinguish adversarial and normal samples .
the roc curve plots the true positive rate tpr against false positive rate fpr for every possible threshold for the classification.
from the roc curve we could further calculate the area under the roc curve auroc to characterize how well the feature performs.
a perfect classifier when all the possible thresholds yield true positive rate and false positive rate for distinguishing normal and adversarial samples will have auroc .
the closer is auroc to the better is the feature.
c accuracy of detection the accuracy of the detection is defined in a standard way as follows.
given a set of images x labeled as normal or adversarial what is the percentage that our algorithm correctly classifies it as normal or adversarial?
notice that the accuracy of detecting adversarial samples isequivalent to tpr and the accuracy of detecting normal samples is equivalent to fpr .
the higher the accuracy the better is our detection algorithm.
c. research questions rq1 is there a significant difference between the lcr of adversarial samples and normal samples under different model mutations?
to answer the question we calculate the average lcr of the set of normal samples and the set of adversarial samples generated as described above with a set of mutated models using different mutation operators.
a set of mutants are generated for each mutation operator note that mutation rate .
is too low for ns to generate mutated models for cifar10 model and thus omitted .
according to the detailed results summarized in tabel ii and iv we have the following answer.
answer to rq1 adversarial samples have significantly higher lcr under model mutation than normal samples.
in addition we have the following observations.
adversarial samples generated from every kind of attack have significantly larger lcr than normal samples under a set of mutated models under any mutation rate and different kind of attack have different lcr.
we can see that the lcr of normal samples are very low i.e.
comparable to the testing error and that of adversarial samples are much higher.
fig.
shows the distance between lcr of adversarial samples and normal samples for different mutation operators.
we can see that the distance is mostly larger than and can be up to which well supports our answer to rq1.
we can also observe that adversarial samples generated by fgsm jsma deepfool black box have relatively higher lcr distance than those generated by cw and those wrong labeled samples in the original dataset.
in general our detection algorithm is able to detect attacks with larger distance faster and better.
as we increase the model mutation rate the lcr of both normal samples and adversarial samples increase as expected and the distance between them decreases.
we can observe from table iv that the lcr increases with an increasing model mutation rate in all cases.
from fig.
we see that a smaller model mutation rate like .
for mnist and .
for cifar10 have the largest lcr distance.
this is probably because as we increase the mutation rate normal samples are more sensitive in terms of the change of lcr since it is a much smaller number.
like adversarial samples generated by different attacking methods wrongly labeled samples also have significantly larger lcr than normal samples.
this suggests that wrongly labeled samples are also sensitive to the change of decision boundaries from model mutations as adversarial samples.
they are the same as the adversarial samples which are near to the decision boundary and thus can be potentially detected.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iv label change rate confidence interval with significance level for each group of samples under model mutation testing with different mutation operators nai result is shown previously in table ii .
the results are shown in percentage.
mutation operator dataset mutation rate normal samplesadv ersarial samples wrong labeled fgsm jsma c w black box deepfool nsmnist0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cifar100.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
wsmnist0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cifar100.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gfmnist0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cifar100.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
wrong labeledfgsm jsma cw bb dfnai operator label change rate distance mnist .
mnist .
mnist .
cifar10 .
cifar10 .
cifar10 .
wrong labeledfgsm jsma cw bb dfns operator label change rate distance mnist .
mnist .
mnist .
cifar10 .
cifar10 .
cifar10 .
wrong labeledfgsm jsma cw bb dfws operator label change rate distance mnist .
mnist .
mnist .
cifar10 .
cifar10 .
cifar10 .
wrong labeledfgsm jsma cw bb dfgf operator label change rate distance mnist .
mnist .
mnist .
cifar10 .
cifar10 .
cifar10 .
fig.
lcr distance between normal samples and adversarial samples using different mutation operators.
rq2 how good is the lcr under model mutation as an indicator for the detection of adversarial samples?
to answer the question we further investigate the roc curve using lcr as the indicator of classifying an input sample as normal or adversarial.
we compare our proposed feature i.e.
lcr under model mutations with two baseline approaches.
the first baseline referred as baseline is a combination of density estimate and model uncertainty estimate as joint features .
the second baseline referred as baseline is based on the label change rate of imposing random perturbations on the input sample .
table v presents the auroc results under different model mutation operators.
we compare our results with two baselines introduced above.
the best auroc results among the three approaches are in bold.
we could observe that our proposed feature beats both baselines in over half the cases excluding deepfool which we do not have any reported baseline results while baseline and baseline only win and cases respectively.
we could also observe that the auroc results are mostly very close to a perfect classifier i.e.
usually larger than .
which suggests that we could achieve high accuracy using the proposed feature to distinguish adversarial samples.
we thus have the following answer to rq2.
answer to rq2 lcr under model mutation could outperform current baselines to detect adversarial samples.table v auroc results.
bl means baseline .
dataset attack bl 1b l nai gf ns ws mnistfgsm .
.
.
.
.
.
jsma .
.
.
.
.
.
cw .
.
.
.
.
.
bb .
.
.
.
.
df .
.
.
.
wl .
.
.
.
.
cifar10fgsm .
.
.
.
.
.
jsma .
.
.
.
.
.
cw .
.
.
.
.
.
bb .
.
.
.
.
df .
.
.
.
wl .
.
.
.
.
rq3 how effective is our detection algorithm based on lcr under model mutation?
to answer the question we apply our detection algorithm algorithm on each set of adversarial samples generated using each attack and evaluate the accuracy of the detection in fig.
.
we also report the accuracy of our algorithm on a set of normal samples.
the results are based on the set of models generated using mutation rate .
for mnist and .
for cifar10 as they have good balance between detecting adversarial and normal samples.
we set the parameters of algorithm as follows.
since different kind of attacks have different lcr but the lcr of normal sample is relatively stable we choose to test against the lcr of normal samples.
specifically we set the threshold hto be nr where nris the upper bound of the confidence authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
.
.
.
.
.
wlfgsm jsma cwbb df avg normalmnist gf dectection accuracy .
wl fgsm jsma cw bb df avg normalmnist gf average number of mutations needed .
.
.
.
.
.
wl fgsm jsma cw bb df avg normalmnist nai detection accuracy .
wl fgsm jsma cw bb df avg normalmnist nai mutation number .
.
.
.
.
.
wl fgsm jsma cw bb df avg normalmnist ns detection accuracy .
wl fgsm jsma cw bb df avg normalmnist ns mutation number .
.
.
.
.
wl fgsm jsma cw bb df avg normalmnist ws detection accuracy .
wl fgsm jsma cw bb df avg normalmnist ws mutation number .
.
.
.
.
.
wl fgsm jsma cw df avg normalcifar10 gf detection accuracy wl fgsm jsma cw df avg normalcifar10 gf mutation number .
.
.
.
.
wl fgsm jsma cw df avg normalcifar10 nai detection accuracy wl fgsm jsma cw df avg normalcifar10 nai mutation number .
.
.
.
wl fgsm jsma cw df avg normalcifar10 ns detection accuracy wl fgsm jsma cw df avg normalcifar10 ns mutation number .
.
.
.
.
.
wl fgsm jsma cw df avg normalcifar10 ws detection accuracy wl fgsm jsma cw df avg normalcifar10 ws mutation number fig.
detection accuracy and number of mutated models needed.
interval of nor and is a hyper parameter to control the sensitivity of detecting adversarial samples in our algorithm.
the smaller is the more sensitive our algorithm is to detect adversarial samples.
the error bounds for sprt is set as .
.
.
the indifference region is set as .
nr.
fig.
shows the detection accuracy and average number of model mutants needed for the detection using the mutation operators for mnist and cifar10 dataset respectively.
we could observe that our detection algorithm achieves high accuracy on every kind of attack for every mutation operator.
on average the gf nai ns ws operators achieves accuracy of .
.
.
.
with .
.
.
.
mutated models for mnist with and .
.
.
.
with with .
.
.
mutated models for cifar10 on detecting the kinds of adversarial samples.
meanwhile we maintain high detection accuracy of normal samples as well i.e.
.
.
.
.
for mnist with and .
.
.
with for cifar10 for the above operators respectively.
notice that for cifar10 we could not train a good substitute model the accuracy is below using black box attack and thus have no result.
the results show that our detection algorithm is able to detect most of adversarial samples effectively.
in addition we observe that the more accurate is the original and as a result the mutated dnn model is e.g.
mnist the better is our algorithm.
besides we are able to achieve accuracy close to for jsmaand df.
we also recommend to use nai gf operators over ns ws operators as they have consistently better performance than the others.
we thus have the following answer to rq3.
answer to rq3 our detection algorithm based on statistical hypothesis testing could effectively detect adversarial samples.
effect of in this experiment we vary the hyper parameter to see its effect on the detection.
as shown in fig.
we set as .
for mnist and for cifar10.
we could observe that as we increase we have a lower accuracy on detecting adversarial samples but a higher accuracy on detecting normal samples.
the reason is that as we increase the threshold for the detection increases.
in this case our algorithm will be less sensitive to detect adversarial samples since the threshold is higher.
we could also observe that we would need more mutations with a higher threshold.
in summary the selection of could be application specific and our practical guide is to set a small if the application has a high safety requirement and vice versa.
rq4 what is the cost of our detection algorithm?
the cost of our algorithm mainly consists of two parts i.e.
generating mutated models denoted by cg and performing forward propagation denoted by cf to obtain the label of an input authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table vi cost analysis of our algorithm.
dataset cf operator cg n mnist0.
ms nai .
s .
.
ms ns .
s .
.
ms ws .
s .
.
ms gf .
s .
cifar100.
ms nai .
s .
.
ms ns .
s .
.
ms ws .
s .
.
ms gf .
s .
sample by a dnn model.
the total cost of detecting an input sample is thus c n cg cf wherenis the number of mutants needed to draw a conclusion based on algorithm .
we estimate cfby performing forward propagation for images on a mnist and cifar10 model respectively.
the detailed results are shown in tabel vi.
note that cgis the time used to generate an accurate model retaining at least accuracy of the original model and the cost to generate an arbitrary mutated model is much less.
in practice we could generate and cache a set of mutated models for the detection of a set of samples.
given a set of msamples the total cost for the detection is reduced to c m m n cf n cg.
in practice our algorithm could detect an input sample within .
second with cached models using a single machine.
we remark that our algorithm can be parallelized easily by evaluating a set of models at the same time which would reduce the cost significantly.
we thus have the following answer to rq4.
answer to rq4 our detection algorithm is lightweight and easy to parallel.
d. threats to v alidity firstly our experiment is based on a limited set of test subjects so far.
our experience is that the more accurate the original model and the mutated models are the more effective and more efficient our detection algorithm is.
the reason is that the lcr distance between adversarial samples and normal samples will be larger if the model is more accurate which is good for our detection.
in some applications however the accuracy of the original models may not be high.
secondly the detection algorithm will have some false positives.
since our detection algorithm is threshold based there will be some false alarms along with the detection.
meanwhile there is a tradeoff between avoiding false positives or false negatives as discussed above i.e.
in the selection of .
thirdly the detection of normal samples typically needs more mutations.
the reason is that we choose to test against nor since we do not know adv for an unknown attack.
since normal samples have lower lcr under mutated models in general they would need more mutations than adversarial samples to draw a conclusion.
v. r ela ted works this work is related to studies on adversarial sample generation detection and prevention.
there are several lines ofrelated work in addition to those discussed above.
a adversarial training the key idea of adversarial training is to augment training data with adversarial samples to improve the robustness of the trained dnn itself.
many attack strategies have been invented recently to effectively generate adversarial samples like deepfool fgsm c w jsma black box attacks and others .
however adversarial training in general may overfit to the specific kinds of attacks which generate the adversarial samples for training and thus can not guarantee robustness on new kinds of attacks.
b adversarial sample detection another direction is to automatically detect those adversarial samples that a dnn will mis classify.
one way is to train a detector subnetwork from normal samples and adversarial samples .
alternative detection algorithms are often based on the difference between how an adversarial sample and a normal sample would behave in the softmax output or under random perturbations .
c model robustness different metrics has been proposed in the machine learning community to measure and provide evidence on the robustness of a target dnn .
besides in and the following work neuron coverage and its extensions are argued to be the key indicators of the dnn robustness.
in bastani et al.
proposed adversarial frequency and adversarial severity as the robustness metrics and encode robustness as a linear program.
d testing and formal verification testing strategies including white box black box and mutation testing have been proposed to generate adversarial samples more efficiently for adversarial training.
however testing can not provide any safety guarantee in general.
there are also attempts to formally verify certain safety properties against the dnn to provide certain safety guarantees .
vi.
c onclusion in this work we propose an approach to detect adversarial samples for deep neural networks at runtime.
our approach is based on the evaluated hypothesis that most adversarial samples are much more sensitive to model mutation than normal samples in terms of label change rate.
we then propose to detect whether an input sample is likely to be normal or adversarial by statistically checking the label change rate of an input sample under model mutation.
we show that our algorithm is both accurate and efficient to detect adversarial samples by evaluating on mnist and cifar10 datasets.
acknowledgment xinyu wang is the corresponding author.
this research was supported by huawei grant rthw1801.
we are grateful to the discussions and feedbacks from the shield lab team of huawei research institute singapore.
this research was also partially supported by the national basic research program of china the program under grant 2015cb352201 and nsfc program no.
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.