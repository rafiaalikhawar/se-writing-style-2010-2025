clcdsa cross language code clone detection using syntactical features and api documentation kawser wazed nafi tonny shekha kar banani roy chanchal k. roy and kevin a. schneider department of computer science university of saskatchewan saskatoon saskatchewan canada email kawser.nafi tonny.kar banani.roy chanchal.roy kevin.schneider usask.ca abstract software clones are detrimental to software maintenance and evolution and as a result many clone detectors have been proposed.
these tools target clone detection in software applications written in a single programming language.
however a software application may be written in different languages for different platforms to improve the application s platform compatibility and adoption by users of different platforms.
cross language clones clcs introduce additional challenges when maintaining multi platform applications and would likely go undetected using existing tools.
in this paper we propose clcdsa a cross language clone detector which can detect clcs without extensive processing of the source code and without the need to generate an intermediate representation.
the proposed clcdsa model analyzes different syntactic features of source code across different programming languages to detect clcs.
to support large scale clone detection the clcdsa model uses an action filter based on cross language api call similarity to discard non potential clones.
the design methodology of clcdsa is twofold a it detects clcs on the fly by comparing the similarity of features and b it uses a deep neural network based feature vector learning model to learn the features and detect clcs.
early evaluation of the model observed an average precision recall and f measure score of .
.
and .
respectively for the first phase and .
.
and .
respectively for the second phase which indicates that clcdsa outperforms all available models in detecting cross language clones.
index t erms code clone api documentation word2vector source code syntax i. i ntroduction with the availability of a number of different widely used programming languages and platforms and to achieve compatibility and adoptability developers may be required to develop the same functionality in different programming languages for different versions of a software system.
a typical example of this is the development of mobile phone games where developers try to make a single game available on different platforms.
for example a very popular mobile phone game is temple run which is available on three popular mobile phone platforms windows1 ios2and android3.a s many differences exist among the different platforms it is required to develop the games according to their designated platforms such as in c c for a windows phone java for an android phone and objective c for an iphone .
although 1windows temple run 2ios temple run 3android temple runthere are tools available4that help to develop cross platform mobile games their execution still requires platform specific support.
a similar scenario also exists for desktop and web based applications such as microservices where the same functions need to be replicated across different programming languages such as antlr lucene and factual .
code fragments related to such functionality replication in different programming languages can referred to as cross language clones or clcs.
source code clones can be considered both harmful and useful based on their characteristics.
previous research showed that to of the source code of a software system is cloned .
duplicating code blocks in source code can add complexity and requires special attention to maintain and avoid unintended behaviour .
code clones are also responsible for introducing bugs and functionality divergence among codebases which decreases overall software system quality .
in addition to these once a code block is changed in the source code of a software system it is required to propagate that change to the other cloned code parts too which usually requires additional effort by the developers or maintainers.
however for most cases cross language clones are created intentionally by developers and are often unavoidable and cannot be removed .
so to help software developers and curators to manage cross language software systems in an easy time effective and cost effective way an automatic cross language code clone detection technique is required.
it is foremost in collaborative software system development where if a developer with expertise in one language changes or modifies their software version developers with expertise in other programming languages need to perform the same functional change to their respective versions as well.
this is more expensive and time consuming compared to changing a single language based software system as it requires prior knowledge regarding the system architecture as well as understanding the modification of the code performed by the first developer.
let us consider the example given in figure .
in scenario a we have code examples for three programming languages java python and c .
each code fragment performs one operation adding two integers.
now let us consider that the requirements change and it is needed to add two float4xamarin phonegap 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ing point numbers instead of integers scenario b .
at first java code is modified and updated by a developer marked with the green arrow labeled java .
now it is required to make the same change to the other versions of scenario a. sound knowledge of the application architecture and source code for the different platforms might guide developers to manually make the changes.
however such a task would demand considerably more effort during real life large scale software development.
to help developers in this situation a cross language clone detection technique would be useful for automatically detecting functionally similar code blocks of the java version in the c and python versions marked with blue arrows .
once the clones are located changes can easily be made to achieve scenario b marked with green arrows .
fortunately a good number of automated or semi automated techniques and models have been proposed for detecting code clones in single language based software systems such as ccfinder deckard nicad and sourcercc .
a very small number of models have also been proposed for cross language clone detection such as licca and clcminer .
among them clcminer only works with code revision histories diffs and detects clones by measuring source code token similarity.
a good number of software applications are available whose functionalities are supported by different platforms.
for example apache lucene can be applied for java c javascript and python.
apis of these tools are almost similar in name across different platforms.
at the same time their usage patterns are almost similar too in different programming languages.
authors of clcminer can track the changes or modifications of these api calls across different software applications without language dependencies.
but this scenario is totally different from the real world cross language clones given in figure .
and this model fails to detect or track these clones.
on the other hand licca detects code clones based on intermediate states represented by budimac et al.
s ssqsa architecture.
this architecture represents any source code in an enriched concrete syntax tree ecst .
licca analyzes the ecst and generates a matrix from it which was extended for cross language clone detection.
but this model has a number of limitations.
first it requires the source code length to be equal.
second code steps and flow of functionality of two code blocks needs to be the same.
these limitations have made this model not applicable in real world scenarios.
very recently perez et al.
proposed a cross language clone detection model where they try to learn an ast representation of some given source code with the help of the skip gram model .
although their approach is sound their model suffers from low average precision and f measure scores which makes their model difficult to operate in real world cross language clone detection scenarios.
in this paper we propose clcdsa a model for detecting cross language clones by analyzing source codes without the requirement of intermediate representations or restrictions.
this model can detect cross language clones by analyzing the similarity of syntactic source code features.
from a previous fig.
.
source code for adding two numbers in java c and python.
scenario a illustrates adding integers scenario b illustrates adding floats study we found that these features have almost similar values if two source code fragments from two different programming languages have similar functionality.
this model can detect cross language clones in two different ways.
first it can detect clones on the fly without having any predefined knowledge.
for this clcdsa measures cosine similarity between two matrices which are generated by extracting values of the features from source code fragments.
metrics are extracted by traversing the ast of related code fragments.
second clcdsa supports a reconfigured siamese architecture based deep neural network which can automatically learn the values of features from a good number of labeled data and can detect cross language clones with greater accuracy.
for providing support for large size datasets this model uses an action filter.
this action filter is designed based on api call similarity used in two different code fragments across different programming languages.
cross language api call similarity is learned with the help of the api documentation and mikolov s word2vec model.
this filter helps to discard non probable clone pairs from consideration before advancing to the main model which reduces computational complexity number of comparisons and improves scalability.
to the best of our knowledge we are the first to use effective source code syntactical features along with cross language api call similarity in detecting cross language clones semantic features .
for performing our evaluation and validating clcdsa we carefully collected a large size valid crosslanguage clone dataset.
from our evaluation we observed that for different threshold values clcdsa can detect clones on authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the fly with an average precision recall and f measure score of .
.
and .
respectively and for clcdsa deepnn the observed average precision recall and f measure are .
.
and .
respectively which outperforms all of the available models in terms of accuracy and scalability.
the dataset we used for validating our proposed model along with our evaluation results are available for further use .
the remainder of the paper is organized as follows section ii describes the design modules of the model section iii discusses the architectural design of the model section iv briefly discusses the evaluation and validation steps and results of clcdsa section v describes the probable threats to validity and how those were addressed section vi describes closely related work and section vii concludes the paper.
ii.
clcdsa c ross language clone detector in this section we describe our cross language clone detector clcdsa.
as with other clone detection models we process source code to extract the metric information needed for our work.
we use an action filter to filter out non probable clones from consideration to make our model more scalable.
a. feature selection table i shows the features saini et al.
proposed in their work as being useful for detecting code clones in java.
most of the features are derived from the software quality observatory for open source software sqo oss quality model proposed by samoladas et al.
.
although it has been shown that analyzing all these features is helpful in achieving great accuracy when detecting clones in a single programming language not all of these features can be used for cross language code clone detection.
in a previous study we determined which of the features in table i were most effective for cross language clone detection.
in that study we used a dataset that contained more than 300k functionally similar cross language clone code fragments written in three different languages java c and python.
we applied each of the features on functionally similar code clones to see the effect and performance of a feature in determining source code similarity.
the primary objective of our study was to see how similar the values were for each of the features for three different but functionally similar code fragements.
from our brief study and manual analysis we found that out of 24features 9features use almost the same metrics across the different programming languages shown in bold in table i .
by observing this we selected these features for detecting cross language clones in the belief that two code fragments will be functionally similar if they share similar metrics.
this study took more than hours of manual labor to effectively find the suitable features.
these features can also be useful in other cross language source code analysis based work and other research work such as code search rapid prototyping and code quality analysis .
b. preprocessing preprocessing of given code fragments for clcdsa is comprised of four steps.
first we remove all the commentstable i features to analyse source code name of the source code feature number of external methods called number of variables referenced number of variables declared number of statements total number of operators number of arguments number of expressions total number of operands method maximum depth of nesting number of loops for while number of local methods called halstead v ocabulary halstead effort to implement halstead difficulty to implement number of exceptions thrown number of exceptions referenced number of classes referenced mccabes cyclomatic complexity number of class casts number of boolean literals number of character literals number of string literals number of numerical literals number of null literals and string literals from the source code.
the reason behind this is to keep track of only the selected features and their values rather than keeping buggy information to analyze.
second we generate an abstract syntax tree ast for each of source code fragments in our repository.
for generating ast we used antlrv4 and its grammars.
the reason behind selecting antlrv4 is its acceptability and adaptability with various programming languages and platforms.
ast is generated by traversing the parse tree produced by antlrv4 related to the given code fragments.
third we apply regular expressions on the generated asts to extract a metric for each of the features that we selected for detecting cross language clones.
once we represented each of the source code fragments with an ast it became easier to track down all the features and their values with the help of regular expressions.
fourth we extract the api calls by traversing the ast with the help of a n ode visitor class.
these api calls are then passed to the next step action filter to filter out probable non clone pairs of source code to increase the scalability of clcdsa.
c. action filter api call similarity like other large scale clone detection techniques clcdsa also comprises a filter which helps to filter out the source code pairs which have almost zero probability of being a clone pair.
saini et al.
and sajnani et al.
used a number of tokens from source code and their semantic similarity as parameters to filter out non clone pairs before proceeding to the main clone detection model.
their approach is good enough to filter out non clone pairs for a single programming language but their approach will not work to filter out probable non clone pairs across two different programming languages.
in addition to these parameters saini et al.
proposed an additional way to filter out non clone pairs.
they tried to see how many similar api calls occurred in a authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
probable clone pair.
they believed that if api calls similarity is low between two code fragments their probability of beinga clone pair is also low.
this hypothesis is also true forcross language clone pairs.
but since api calls of differentprogramming languages are not semantically similar sainiet al.
s proposed way of detecting api calls similarity based on their semantics will not be applicable here.
a group of researchers recently found that api calls similarity across dif ferent programming languages could be learned and detectedby their documentation similarity .
programminglanguage developers and curators provide documentation ofeach of the apis they added to their regular libraries 5so that developers can understand the usage and scope of that api inthe source code.
the hypothesis of the two previous works is if two apis from two different programming languages performthe same task their usage documentation will be semanticallythe same.
we also adapted this hypothesis in designing theaction filter.
for large scale code clone detection this filteris very effective as regular code to code comparison fordetecting clones is expensive and time consuming increasesexponentially as the number of code fragments increases .
fig.
shows the example of two code blocks developed in java and c respectively which take two integer numbers asinput from users adds them and finally shows their sum onthe console.
in the java code example the api calls used are println and next int .
for the c code the api calls are w rite p arse and r ead line .
none of these api calls are textually similar to each other although they providesimilar functionality.
from fig.
we can also see the relateddocumentation for each of the api calls.
by analyzing theapi call documentation we see that the documentation for println and w rite have the same semantic meaning.
the same is true for other api calls.
by observing this wedecided to adapt this hypothesis in designing our action filter.with the help of the api call documentation we detect thelevel of similarity among api calls used in a probable crosslanguage clone pair.
if this similarity is less than the predefinedthreshold value then that clone pair is rejected and considereda non clone pair.
by this we reduce the number ofcomparisons before advancing to the main clone detectionmodel.
this helps the proposed model to execute swiftly aswell as makes it scalable for large sized clone data.
d. feature metrics extraction similarity detection metric based clone detection approaches are common and have been popular for detecting clones in single programming language based software applications .
for detectingcross language clones as they are functionally similar clones e.g.
type clones it is hard to see that metrics of thefeatures for two source code fragments of a cross languageclone pair are directly similar to each other.
for this weselected the features very carefully and finely tuned the featuremetrics.
in table i the name of the features we found feasible 5java python .
c fig.
.
examples of api call documentation.
api calls with the samefunctionality usually possess the same semantic description.
for detecting cross language clones have been identified in bold.
by traversing the generated ast of each source codefragment in the repository we extracted the value for each ofthe features listed in numeric form.
we recorded those metricsin a csv file.
the reason behind saving them in a csv fileis to have a table based record which is easy to handle.
for detecting cross language clones on the fly this model takes a matrix consisting of feature values as input.
it usedcosine similarity to detect a probable clone pair using thefollowing equation where aandbare two matrices cos a b bardbla bardbl bardbl b bardbl iii.
clcdsa m odel in this section we briefly discuss the design strategies of the model and the deep neural network it uses to detect crosslanguage clones along with the workflow.
a. dataset collection because of the absence of an openly available crosslanguage clone dataset for experimenting and evaluating our model we created a dataset with more than 78k solutions fromprogramming contests developed in developed in java c and python .
we collected the source code for all of thesesolutions from three open source programming contest sites atcoder google codejam7and coderbyte8.
the reason for selecting code from these websites is to ensure all the collectedcode written in different programming languages are func tional clones of each other.
according to the definition cross authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii description of database total number of code blocks total number of questions total number of java code blocks total number of python code blocks total number of c code blocks average lines of code java average lines of code python average lines of code c language clones mean two or more code blocks developed in different programming languages that are functionally similar to each other.
the open source contest sites accept solutions for a posted problem without any language restriction.
at the same time all the submitted code blocks are tested with the same input and expected output to validate them.
so we can easily say that accepted solutions for any programming language for a single problem statement are functional clones of each other which we can claim as the best validated cross language clone database in every respect.
the present status of our dataset is given in table ii.
at the time of collecting the dataset we visited each of the archived problems and their solutions.
for each of the problems we collected at least fully accepted solutions for each of the three programming languages under observation.
we visited around archived problem statements to construct our dataset.
in this way we built a complete cross language clone dataset of 78k solutions from programming contests developed in java python and c programming languages.
in this dataset accepted solutions for a single problem statement are validated cross language clones of each other whereas accepted solutions of two different problem statements are identified and validated as a cross language non clone pair.
among the entire dataset we used atcoder and google code jam for training and testing the deepnn model whereas source code collected from coderbyte was used for validating the model.
b. api learning and action filter model for learning api call similarity across different programming languages we first collected api documentation of the supported libraries of java c and python.
we developed a webcrawler tool to crawl through the api documentation provided by the language developers and curators.
for each of the api calls for every language we collected only the first sentence in the belief that it carries the information regarding the api call s functionality and scope.
we added the api calls documentation to an xml file.
for each of the programming languages we maintained a separate api calls documentation xml file.
once the xml files were created we traversed each of the xml files and read the collected api calls documentation manually.
the purpose of this step was to verify the quality of the collected api calls documentation.
if we found any non useful api call documentation we againsearched for that api call description and manually replaced the previous documentation with a proper description.
this process took us around hours of manual labor on average for the three programming languages.
second we adapted mikolov et al.
s proposed word2v ec model to generate a vector representation for each api call description.
we trained each api call documentation with the help of google s word vector pre trained model9.
the reason behind this is to train each word of an api call documentation with greater accuracy with the help of a reasonable sized pre trained word vector representation model.
third with the help of cosine similarity stated in equation we calculated the similarity between the vector representation of two api calls documentation selected from two different programming languages.
we map each api call of one language to api calls of the other languages in a one to many manner.
we also considered extracting information from the name of the api calls.
but as various programming languages use different naming conventions we found that this information is not feasible for our work.
so we only considered api documentation to learn and detect similarity among two api calls of two different programming languages.
we maintained a csv file to record all of the api call similarity scores along with the name of the api calls themselves.
we individually generated and maintained a csv file for each programming language pair we considered.
this is how we determined cross language api call similarity.
once the cross language api call similarity is learned the action filter is invoked which is composed of the following three steps.
first we selected a probable clone pair from two different programming languages retrieved their asts and extracted api calls.
second we selected two api calls from two source code fragments and queried them against the language pair specific csv file that was generated in the api learning step to obtain the similarity score.
each api call in a single source code fragment is paired with all the api calls in the other source code fragments and queried against the appropriate csv file to obtain the similarity score for that api pair.
the api pair with the highest similarity score is kept and the others are omitted.
at the same time that api pair was removed from consideration.
the same process was followed for the rest of the api calls until all the api calls of a source code fragment were paired up with those of the other source code fragments having the highest similarity score.
third we normalized the api call similarity score by taking the average of the remaining pairs similarity score.
let us consider aand bare representing lists of api calls used in two given source code fragments.
if a1 b1 a2 b2 .... ak bk are api call pairs depicting the highest similarity scores then the api call similarity for a probable clone pair is calculated using the following equation to normalize the api calls similarity score apicallssimilarity summationtextk i parenleftbig ak bk parenrightbig k let us consider the clone pair example given in fig.
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the java code api calls are println next int and c api calls are w rite r ead line p arse .
according to our action filter algorithm println was paired with each of w rite r ead line p arse and the println write pair survived with a .
similarity score.
after that next int was paired with r ead line p arse and the next int r ead line pair remained with a .
similarity score.
so the api call similarity score was .
.
.
.
.
.
the action filter will not consider a source code pair as a probable clone if theirapi call similarity score is less than .
.
c. feature v ector f ormulation section iii a describes the dataset we considered for evaluating our model.
for the clcdsa deep learning model we randomly divided our collected database for training testingand validation purposes.
first we started working with theatcoder dataset.
for each of the questions we selected ac cepted solutions for each of the three programming languages java c and python for training purpose accepted solutionsfor each language for testing the model and for validating themodel.
in this way accepted solutions for a single problemstatement for each of the programming languages were dividedinto training testing and validation sets.
next following thesame process we divided the google code jam dataset.
weconfigured the feature vector with metrics for each of thesource code fragments in a pair with an additional booleanmetric which indicated whether this code pair was a clone ornot for non clone and for clone .
so the feature vectordimension was .
in this way we generated a sample labeleddataset of 3b feature vectors where .5b feature vectorsrepresent clone pairs and the remaining .5b feature vectorsrepresent non clone pairs.
the whole sample was selectedrandomly without any preconditions or constraints.
thus wecan reasonably claim that the model that we generated forclcdsa represents the regular structure of source codefragments and can show the same performance with sourcecode fragments collected from other repositories for detectingcross language clones.
d. clcdsa deep learning model although various machine learning techniques exist for example oreo we exploited the deep learning methodology to detect cross language clones.
deep neural networks orplain neural networks usually use a good number of interme diate layers of neurons to automatically learn the features ofa model in a non linear transformation manner.
these modelsare popular because of their ability of learning features andproperties of universal approximation.
these models can easilybe adapted for large datasets.
some other areas successfullyaddressed by deep neural networks include computervision and natural language processing .
clcdsa s deep learning model was developed on the basis of the siamese architecture neural network whichworks efficiently when it is required to compare two objectsside by side to detect their similarity.
this model can also fig.
.
siamese architect for clcdsa.
the model comprises of a two identical subnetworks b comparator and c classification unit handle the symmetry of input vectors with great care .
this gives the advantage of the non requirement of main taining a predefined order of input vectors.
the source codeinput pairs c c2 and c2 c1 will be treated in the same manner.
siamese architecture also requires a reduced numberof parameters as the weight parameters are shared between thetwo identical sub neural networks.
fig.
shows the clcdsa deep learning model architecture.
we considered dimensional feature vector for eachsource code as input to the model for a pair it is .these two feature vectors were then fed to two identicalsubnetworks which performed the same transformation onboth of the feature vectors.
both of the subnetworks had thesame configuration and had fully connected hidden layersof units each.
once the output of this subnetwork modelwas generated two output vectors were concatenated and fedto the next comparator model.
the comparator model wascomprised of hidden layers of sizes with fullconnectivity among the neurons between two layers.
finally the output of the comparator was fed to the classification unit.the classification unit was comprised of a single neuron whichconsists of a logistic unit having the following calculationmechanism f parenleftbig summationdisplay i 1wixi parenrightbig epsilon1 summationtext10 i 1wixi hereirepresents the number of units at the last layer of the comparator unit which is for our experiments xiis the input value of that layer and wiis the estimated weight for that corresponding parameter.
although it is usually considered that the classification unit value greater than .
is a probableclone for any learning based clone detection model for our evaluation we also considered classification unit valuesof .
and .
as thresholds for accepting a code pair as clone.
we selected relu as the activation function and authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
clcdsa workflow for detecting cross language clones dropout for keeping the model free from over fit.
we also selected relative entropy as a loss function among the units of each layer to calculate the distance between the predicted label and the expected label.
we selected the stochastic gradient descent of learning rate .
with a reduction after every epoch.
we considered a batch size of for each epoch randomly initialized the values with the help of he normal and set the iteration value to to get the final trained model.
e. clcdsa workflow from fig.
we can observe the working strategy of our proposed cross language clone detection model clcdsa .
steps and are related to the generation of the training model for clcdsa and steps and are related to the on the fly clone detection.
in step source code from the repository is fetched into a language detection chamber.
it is required as the grammar for ast generation varies from one programming language to another.
once the language is detected in step the ast is generated and recorded.
this step is repeated for each source code fragment in our repository.
once the ast generation step is completed all the generated asts are fed to the feature extraction module.
in this step all the asts are traversed and numeric metrics for our selected features are calculated and recorded.
and these metrics are preserved as the name of the source code file.
once the metrics are generated in step with the help of cosine similarity clcdsa detects whether the selected source code pair is a clone or not.
before step or step all the source code pairs which are considered as probable clone pairs are required to pass through the action filter described in section ii c. during the training phase since we are already providing labelled data those feature vectors are not required to pass through the action filter.
in step with the help of the labelled data the neural net model has already been trained before deploying it to detect clones.in step clcdsa starts clone detection phase.
once the list of the probable clone pairs are given to the model as input with source code fragments developed in any programming language steps and are executed along with the action filter.
after that if on the fly clone detection is selected step will occur.
it will take the feature values of both the source code fragments as a matrix and with the help of cosine similarity will determine whether this pair is a clone or not based on the predefined threshold value to define a clone.
if the clone detection neural net model is preferred the feature vector for the source code pair will then be fed to the model and step will come in effect.
the output of the neural net will define whether the given source code pair of two different programming languages are clones or not based on the predefined threshold value.
iv .
e v aluation in this section we explain the analyses on the results that we obtained from clcdsa s evaluation and validation processes.
for analyzing the performance of the model we used three metrics which are popular and widely used in defining the performance of the clone detection models .
these are precision recall and f measure.
a. evaluation metrics precision stands for the relation of correctly predicted samples to the total number of positively evaluated samples.
in other words in clone related research precision refers to the percentage of accurately detected clones from a sample where both clones and non clones are placed together.
if np is the number of detected true positive clones and npis the number of detected false positive clones then the precision is precision p np np np recall shows to the comparative relation of correctly predicted samples to the total number of positive samples.
in authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
clone research the percentage of accurately detected clones from the total number of positive clones present at sample is called recall.
if npis the number of detected true positive clones and nnis the number of detected false negative clones then the recall is recall r np np nn f measure orf1shows the testing accuracy.
it is the harmonic average of the precision and recall scores of a model validation process.
if pis the calculated precision of a test system and ris the recall score of that system then fmeasure is f measure f1 p r p r b. research questions to evaluate validate and show the acceptability of our model we address the following research questions rq1 can clcdsa detect cross language clones on the fly without having any predefined knowledge and only by analyzing the given source code pair s syntactic features?
rq2 can clcdsa s deepnn model detect crosslanguage clones with high accuracy?
rq3 can clcdsa outperform all the available crosslanguage clone detection models and methodologies in terms of precision recall and f measure score?
rq4 does clcdsa support scalability in order to responsively process a large cross language clone dataset?
c. baseline methods to the best of our knowledge there exist only three tools and models which can detect cross language clones.
among them vislavski et al.
proposed licca which can detect similar source codes without being language specific.
their model fully depends on the ssqsa platform which generates a common intermediate representation of source code entities called ecst enriched concrete syntax tree .
next cheng et al.
proposed clcminer where they tried to find out source code tokens similarity to detect cross language clones.
as their tool was not available we replicated their model to show comparison with our work.
our reproduction results show almost the same accuracy that the authors stated in their paper.
finally perez in his master thesis dissertation showed that generating token vectors from traversing ast and learning vectors using the skipgram algorithm is helpful in detecting cross language clones.
their model considered source code pairs a clone if the estimated score is greater than .
.
d. rq1 clcdsa on the fly performance from table i we see the features for which we extracted metrics from each of the source code fragments in our repository.
we used cosine similarity as shown in eq.
for detecting similarity among the feature matrices related to thetable iii clcdsa sperformance detecting cross language clones without pre training and ont h e fly threshold threshold threshold language combinationprecision precall rf measure f1precision precall rf measure f1precision precall rf measure f1 java python0.
.
.
.
.
.
.
.
.
java c .
.
.
.
.
.
.
.
.
c python0.
.
.
.
.
.
.
.
.
table iv testing accuracy for clcdsa sdeepnn threshold threshold threshold language combinationprecision precall rf measure f1precision precall rf measure f1precision precall rf measure f1 java python0.
.
.
.
.
.
.
.
.
java c .
.
.
.
.
.
.
.
.
c python0.
.
.
.
.
.
.
.
.
source code of probable clone pairs.
from table iii we see the performance of clcdsa in detecting cross language clones on the fly.
with considering difference in feature values of two different source code fragments we see that clcdsa can detect clones among java python and c programming languages with an average precision of more than .
an average recall of around .
along with an average f measure score f1 of more than .
.
for this test case we actually restricted the feature metrics of the two source code fragments belonging to a probable clone pair to be very similar.
even in that scenario we saw that clcdsa could detect cross language clones with around accuracy.
when we allowed more difference around in matrix values of two source code fragments we observed that clcdsa can successfully detect cross language clones among our selected programming languages with average precision recall and f1 score of .
.
and .
respectively.
from this scenario we could see that with a small compromise with precision rate clcdsa would experience a high recall rate while maintaining the same accuracy in terms of an f measure score.
in crosslanguage clones based on developers choice and differences in programming language structures and features it is hard to get an exact match between two source code fragments from two different programming languages.
so accepting a good difference in matrix values of two probable clone candidates in a cross language environment observes a higher recall rate.
with the similarity threshold we observed an average precision recall and f measure score of .
.
and .
respectively which is still acceptable.
from our results we can say that clcdsa is capable of detecting cross language clones on the fly with great accuracy in terms of high precision recall and f measure scores.
e. rq2 clcdsa s deepnn performance we evaluated the deepnn model of clcdsa described in section iii d with the help of a testing and a validating dataset and performance metrics.
table iv shows the testing performance of the clcdsa model.
with the model authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table v clcdsa sdeepnn p erformance detecting cross language clones threshold threshold threshold language combinationprecision precall rf measure f1precision precall rf measure f1precision precall rf measure f1 java python0.
.
.
.
.
.
.
.
.
java c .
.
.
.
.
.
.
.
.
c python0.
.
.
.
.
.
.
.
.
configuration described in section iii d we see that with the threshold value of a classification unit of .
or greater the model can detect cross language clones among the three programming languages under observation i.e.
java c and python for the testing dataset with an average precision of above .
along with an average recall rate of .
and an average f measure score of above .
.
from this test case we see that with the training model that we configured we can detect cross language clones with a good accuracy even under a tight acceptance rate.
although the training model should perform more effectively in terms of recall rate as regular models do for cross language clones since the metrics for the features highly vary from language to language by accepting a low error rate in model prediction we observed a low recall rate.
with similarity threshold which means accepting a classification unit value of .
and above clcdsa could detect cross language clones with an average precision recall and f measure score of .
.
and .
respectively.
from this we can observe that despite compromising the precision a little the recall rate increased around .
with the threshold value of .
and above we observed an average precision recall and f measure score of .
.
and .
respectively.
considering the observed values of the performance metrics for the test set data we can say that the clcdsa deepnn model is well designed.
as well training the model with our cross language clone training dataset helped the model to detect clones from the test data set with an average of accuracy for the selected programming languages.
for validating the deepnn model we generated a validation dataset.
we took accepted solutions for each of the programming languages for a single problem statement collected from two open source programming contest sites atcoder and google code jam.
along with this dataset we collected accepted solutions from another open source contest site coderbyte10.
for a single posted problem we collected at least accepted solutions for each of the programming languages from this site in the same manner as we did for atcoder and google code jam.
in this way we built our dataset for validating the deepnn model.
from this dataset we randomly selected questions and their solutions for validation purpose.
we also randomly selected the probable clone pairs from this dataset.
from table v we can see clcdsa s deepnn model s performance in detecting cross vi performance of a v ailable tools in detecting clones among java p ython programming languages tools precision recall f1 measure licca .
.
.
clcminer .
.
.
ast learner .
.
.
11since ast learner currently only supports java and python we only performed our clone detection experiment with java and python language clones for our validating dataset.
comparing it with that of table iv we see that we did not loss much accuracy in detecting cross language clones for the testing and validating dataset.
with threshold which means accepting a classification unit value of .
or more as clones for the validation dataset deepnn of clcdsa detected clones with an average precision of .
with an average recall rate of .
along with an average f measure score of .
.
from this we can say that we observed a little bit of a reduced precision rate for validating data compared to test data.
however it was acceptable because reducing the threshold value to .
we observed around or less reduction in average precision rate but got an increment of around in average recall rate.
these results are still less compared to the test data but their differences were very little and we could easily accept this since the validated data was completely new to the model.
finally by accepting a classification unit value of .
or more as clones we experienced that clcdsa could detect crosslanguage clones with an average precision of .
an average recall .
and an average f measure .
.
this indicates that clcdsa could detect clones with around accuracy.
from the above discussion we can say that clcdsa deepnn detects cross language clones with greater accuracy than clcdsa on the fly.
this model and the experimental data along with the experimental results available for others .
f .
rq3 performance of available models as discussed in section iv c we found that at present three tools are available which could detect cross language clones.
among them clcminer was designed for a different purpose and was not designed to detect plain cross language functional clones two source codes written in different programming languages performing same operation .
from the table vi we can also see the poor performance of this model.
each of the average precision and recall rates we observed for clcminer was on average below .
.
in addition to that licca also performed poorly for our clone dataset.
the reason behind licca s poor performance is the set of the restrictions it possesses.
for detecting clones licca needs the code blocks size to be the same and requires the same control flow and sequences along with the same flow of statements.
in regular cross language clone data these preconditions are hard to meet because of the nature and structure of different programming languages.
from our observation and the description provided authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table vii performance of action filter in reducing total comparisons comparison styleaction filter statusnumber of comparisonsaction filter statusnumber of comparisons total dataset no filter .8077e filter active .84253e java python no filter .87259e filter active .84217e j a v a c no filter .87259e filter active .7252e c python no filter .87259e filter activ e .63798e in their paper we found that licca could detect crosslanguage clones with an average precision recall and fmeasure score of .
.
and .
respectively.
finally we experimented with the state of the art cross language clone detection model ast learner .
with the acceptance of prediction value score of .
and up ast learner could detect cross language clones with around .
average precision and .
average recall.
this helped them achieve an f measure score of .
.
comparing the experimental results described in table vi with table iii and table v clcdsa outperforms all available tools and methodologies for detecting cross language clones by a high margin in terms of precision recall and fmeasure.
g. rq4 scalability to evaluate the performance of the action filter that we proposed for clcdsa we compared the total number of estimated comparisons among the given source code and the actual comparison performed at the time of validating the model.
for this experiment we selected the dataset that we configured for validating the clcdsa deepnn model.
the dataset consists of problem statements where for each problem statement we selected accepted solutions for each of the three programming languages.
from table vii we see the comparative performance of a number of source code comparisons with the absence and presence of the action filter.
from the data in table vii we see that our proposed action filter successfully reduced the number of source code comparisons by a significant amount which illustrates the scalability and capability of our model in handling large cross language clone datasets.
v. t hreats to validity we consider a number of threats to the validity of our study.
one threat concerns our collected dataset and its validation.
we performed our evaluation and validation process with a collected dataset of 78k source code written in java c and python to solve programming contest problems.
we collected the dataset from open source contest sites where submitted solutions for a single problem are tested with the same input and the expected output.
we only collected the source code which was accepted with full scores.
solutions of one single problem statement are considered clones to each other and solutions of different problem statements are considered nonclones.
based on these considerations we claim that the dataset is a validated source of cross language code clones and that clcdsa will be able to show similar performance in general and with other cross language clone datasets.another probable threat could be the quality of the api call documentation used by our action filter.
we collected documentation from the programming language curators websites.
the documentation has been used and considered of good quality in previous studies.
our consideration of using a similar approach as of previousl studies helps mitigate the threat to validity of our study.
finally the selection of deepnn for our feature vector learning model could be questionable too.
we selected this model based on the model selection process described in oreo .
deepnn has already proved to be applicable to work for similar kinds of problems and usage scenarios.
so we claim that we have selected a reasonable model to learn our feature vector.
in the future we would like to perform a further study to verify whether deepnn is the best choice for our proposed model or not.
vi.
r elated work detecting clones across projects is not a new concept.
considerable research and tools are available to detect clones between and within projects.
single language clones can be detected using source code text similarity e.g.
duploc marcus et al.
and nicad code token based similarity e.g.
ccfinderx clone detective iclones and sourcerercc ast based similarity detection techniques clonedr deckard clonedigger and simscan metrics based techniques clones in binary executables and program dependency graph pdg based clone detection techniques .
although progress has been made in detecting clones for a single language a significant gap still exists in detecting clones across different programming languages.
one of the very first works in cross language clone detection was proposed by kraft et al.
where they leveraged codedom as an intermediate representation.
their model could detect clones across programming languages developed by microsoft .net framework that usually used the same intermediate representations e.g.
c asp.net visual basic and so on.
their model is not able to directly detect clones across different programming languages such as clones between java and c or java and python where intermediate states are totally different at the same time supported platforms and programming language maintainers are different too .
another very closely related work was by al omari et al.
.
they detected clones in .net programming languages by analyzing the generated common intermediate language cil .
vislavski et al.
proposed licca to detect cross language clones.
they leveraged the ssqsa platform s capability of generating common intermediate representations for various programming languages.
although it is a sound approach for detecting cross language clones their model suffers from a number of limitations such as failing to detect cross language clones for non equal length source codes nonsimilar control flow of data and non similar usage pattern of apis in different source codes and so on discussed in section i .
these limitations restrict their model from being authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
usable for real world cross language clone detection.
cheng et al.
proposed clcminer to detect code changes for a 3rd party tool s api calls across different programming languages.
they use a word token matching strategy to track code changes across different revision histories.
their approach fails to detect similar functional code blocks across different programming languages.
very recently an ast learning based approach was proposed by perez et al.
which can learn ast representations across different programming languages.
their model is quite capable of detecting code clones between java and python.
however their model has a very low precision rate even though their model accepts and up similar ast blocks as code clones.
supervised and unsupervised deep learning based clone detection models are have been proposed for detecting clones for a single programming language.
however the approaches used are not suitable for detecting cross language clones.
learning similar apis across different programming languages has already been shown to be capable of detecting similar software applications and finding similar cross language libraries .
currently the performance of these models is acceptable however more research is required to develop a highly accurate similar api calls mapping technique that could easily be adaptable in a cross language software development environment.
vii.
c onclusion while there are many clone detection tools available for detecting clones in software systems written in a single programming language there are only a few approaches for detecting clones in software systems implemented in different programming languages.
given that cross language clones are likely to have more varieties than single language clones and that they may pose additional challenges in software maintenance and evolution it is important to detect cross language clones.
in this paper we proposed clcdsa which can detect cross language clones with greater accuracy compared to all other available models in terms of precision recall and fmeasure.
clcdsa can detect clones both on the fly and with the help of a deepnn network.
in the future we plan to study its suitability for other cross language based source code analysis.
acknowledgment this research is supported by the natural sciences and engineering research council of canada nserc and by two canada first research excellence fund cfref grants coordinated by the global institute for food security gifs and the global institute for water security giws .