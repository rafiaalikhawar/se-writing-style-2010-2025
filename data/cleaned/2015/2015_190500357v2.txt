e2e web test dependency detection using nlp matteo biagiola fondazione bruno kessler trento italy biagiola fbk.euandrea stocco universit della svizzera italiana lugano switzerland andrea.stocco usi.chali mesbah university of british columbia vancouver bc canada amesbah ece.ubc.ca filippo ricca universit degli studi di genova genova italy filippo.ricca unige.itpaolo tonella universit della svizzera italiana lugano switzerland paolo.tonella usi.ch abstract e2e web test suites are prone to test dependencies due to the heterogeneous multi tiered nature of modern web apps which makes it difficult for developers to create isolated program states for each test case.
in this paper we present the first approach for detecting and validating test dependencies present in e2e web test suites.
our approach employs string analysis to extract an approximated set of dependencies from the test code.
it then filters potential false dependencies through natural language processing of test names.
finally it validates all dependencies and uses a novel recovery algorithm to ensure no true dependencies are missed in the final test dependency graph.
our approach is implemented in a tool called tedd and evaluated on the test suites of six open source web apps.
our results show that tedd can correctly detect and validate test dependencies up to faster than the baseline with the original test ordering in which the graph contains all possible dependencies.
the test dependency graphs produced by tedd enable test execution parallelization with a speed up factor of up to .
introduction ideally all tests in a test suite should be independent.
however in practice developers create tests that are dependent on each other .
test dependency can be informally defined as follows.
let t t1 t2 .
.
.
tn be a test suite where each tiis a test case whose index idefines an order relation between test cases that corresponds to the original execution order given by testers.
when tests within tare executed in the original order all tests execute correctly.
if the original execution ordering is altered e.g.
by executing t2before t1 and the execution of t2fails we can say that t2depends on t1for its execution and that a manifest test dependency exists .
test dependencies inhibit the use of test optimization techniques such as test parallelization test prioritization test selection and test minimization which all require having independent test cases.
furthermore test dependencies can mask program faults and lead to undesirable misleading side effects such as tests that pass when they should fail tests that fail when they should pass or significant test execution overheads .
web developers frequently use end to end e2e test automation tools which verify the correct functioning of the application in given test scenarios by means of automated test scripts.
such scripts automate the manual operations that the end user would perform on the web application s graphical user interface gui such as delivering events with clicks or filling in forms .
unlike traditional unit tests e2e tests focus on whole businessoriented scenarios such as logging into the web application adding items to a shopping cart and checking out.
such test scenarios go through all tiers and involve all services required to make the whole application work.
thus in web testing it can be difficult to enforce isolation as web tests might use the application state which is promptly available from previous test case executions i.e.
a polluted state .
this creates potential test dependencies due to read after write operations performed on persistent data such as database records or document object model dom fragments that are written by a test tiand later accessed by a successive test tj where j i .
automated detection of all test dependencies in any given test suite is np complete .
as such researchers have proposed techniques and heuristics that help developers detect an approximation of such dependencies in a timely manner .
in this work we focus on automatically detecting test dependencies in e2e web tests.
existing tools such as dtdetector electrictest or pradet are not applicable because they are based on the extraction of read write operations affecting shared data e.g.
static fields of java objects.
instead web applications are prone to dependencies due to the persistent data managed on the server side and the implicit shared data structure on the client side represented by the dom.
such dependencies are spread across multiple tiers services of the web application architecture and are highly dynamic in nature.
hence existing techniques based on static code analysis are not directly applicable.
the web test dependency problem demands for novel approaches that leverage the information available in the web test code and on the client side.
in this paper we propose a novel test dependency detection technique for e2e web test cases based on string analysis andnatural language processing nlp .
our approach is implemented in a tool called tedd testdependency detector which supports efficient and conservative detection and validation of test dependencies in an e2e test suite using only client side information which makes it independent of the server side technology.
tedd extracts an initial approximated dependency graph tdg .
then it filters potentially false dependencies in order to speed up the validation process.
afterwards it validates each dependency by dynamic analysis and it recovers any manifest dependency that is potentially missing in the initial and or filtered graph.arxiv .00357v2 oct 2019technical report series fbk trento italy m. biagiola et al.
the output of tedd is a validated tdg which ensures that any test execution schedule that respects its dependencies will not result in any test failure.
in our empirical study on six web test suites tedd produced the final tdgs faster than a baseline approach that validates all possible dependencies on average .
also the test suites parallelized by tedd according to the dependencies in the final tdgs achieved a speedup up to on average .
our paper makes the following contributions the first test dependency detection approach for web tests.
our approach introduces string analysis sa to extract an approximated set of test dependencies and nlp sa to filter potential false dependencies.
an algorithm to automatically retrieve all missing dependencies from any given web test dependency graph.
an implementation of our algorithm in a tool named tedd .
an empirical evaluation of tedd on a benchmark of six open source web test suites comprising test cases.
background and motivation ideally running the tests in a test suite in any order should produce the same outcome .
this means tests should deterministically pass or fail independently from the order in which they are executed.
a test dynamically alters the state of the program under test in order to assert its expected behaviour.
in practice some tests fail to undo their effects on the program s state after their execution which can pollute any shared state in tests executed subsequently.
in the web domain testers perform end to end e2e testing of their applications by creating test cases using test automation tools such as selenium webdriver .
such tests consist of actions that simulate an end user s interactions with the application and assertions on information retrieved from the web page to verify the expected behaviour.
unlike unit testing in which tests target specific class methods web tests simulate e2e user scenarios and therefore the program state that persists across test case executions might be left polluted causing test failures if tests are reordered.
motivating example.
table lists six e2e selenium webdriver tests for the claroline web application one of the subject test suites used in our evaluation.
table test cases for claroline numbered according to their test execution order .
test name description t1 addusertest the admin creates a new user account.
t2 searchusertest the admin searches for the newly created user.
t3 loginusertest the newly created user logs in to the application.
t4 addcoursetest the admin creates a new course.
t5 searchcoursetest the admin searches for the newly created course.
t6 enrolusertest the user enrols herself in the course.
figure shows dependencies between tests t1andt2.
the test case addusertest logs in to the application with the administrator credentials lines it navigates to the create user page lines it creates a new user account having username user001 by filling in and submitting the appropriate form lines and it finally verifies that a message is correctly displayed line .
test public void addusertest driver .
findelement by.id login .
sendkeys admin driver .
findelement by.id password .
sendkeys admin driver .
findelement by.
xpath button .
click driver .
findelement by.
linktext platform administration .
click driver .
findelement by.
linktext create user .
click driver .
findelement by.id lastname .
sendkeys name001 driver .
findelement by.id firstname .
sendkeys first name001 driver .
findelement by.id username .
sendkeys user001 driver .
findelement by.id password .
sendkeys pass word001 driver .
findelement by.id password conf .
sendkeys pass word001 assertequals the new user has been created driver .
findelement by.
xpath .
gettext driver .
findelement by.id logout .
click test public void searchusertest driver .
findelement by.id login .
sendkeys admin driver .
findelement by.id password .
sendkeys admin driver .
findelement by.
xpath button .
click driver .
findelement by.
linktext platform administration .
click driver .
findelement by.id search user .
sendkeys user001 driver .
findelement by.
cssselector input .
click assertequals name001 driver .
findelement by.id l0 .
gettext assertequals first name001 driver .
findelement by.
xpath td .
gettext driver .
findelement by.id logout .
click figure two dependent e2e web tests for the claroline web application.
potential dependencies due to shared input data are highlighted.
t1 addusertestt6 enrolusertestt5 searchcoursetestt4 addcoursetestt2 searchusertestt3 loginusertest figure the test dependency graph for the test suite of table .
solid edges represent manifest dependencies namely dependencies that result in a different test result if they are not respected.
the execution of addusertest pollutes the state of the web application which is used by the subsequent test searchusertest to search for the same user user001 created by addusertest line .
thus the shared input data user001 might reveal a potential dependency between the tests see highlighted inputs in figure .
to make these two tests independent and avoid polluted program states a tester for instance should delete the user user001 created in addusertest to clean the polluted program state and re create the same user or a different one in searchusertest .
in practice however testers re use states created by preceding tests to avoid test redundancy higher test maintenance cost and increased test execution time .
in so they also enforce pre defined test execution orders which in turn inhibit utilizing test optimization techniques such as test prioritization .
test dependency graph.
the dependencies occurring between tests can be represented in a test dependency graph t dg .
t dg is a directed acyclic graph in which nodes represent test cases and edges represent dependencies.
t dg contains an edge from a testt2to a test t1ift2depends on t1for its execution notationally t2 t1 .e2e web test dependency detection using nlp technical report series fbk trento italy t1t2t3t4t5t6test suite with ordert1t2t3t4t5t61 dependency graph extractiont1t2t3t4t5t62 filteringt1t2t3t4t5t63 dependency validation recovery disconnected dependency recovery t1t2t3t4t5t64 figure our overall approach for web test dependency detection validation and recovery.
figure illustrates the actual test dependency graph t dg for the test suite of table .
for example t dg contains an edge from searchcoursetest toaddcoursetest because searchcoursetest requires the execution of addcoursetest to produce the expected result in other words addcoursetest must be executed before searchcoursetest in order for it to succeed .
multiple test dependencies can also occur.
for instance enrolusertest depends on both addusertest andaddcoursetest for its correct execution.
in order to be useful t dg should contain all manifest dependencies i.e.
dependencies that do cause tests to fail if violated while retaining the minimum number or none of false dependencies .
one possible application of a t dg that contains only manifest dependencies is test suite parallelization.
for instance if we traverse the graph of figure and extract the subgraphs reachable from each node with zero in degree in our example there is only addusertest we can identify subsets of tests that can be be executed in parallel with the others.
in our example four parallel test suites are possible t1 t2 t1 t3 t1 t4 t6 t4 t5 .
approach the goal of our approach is to automatically detect the occurrence of dependencies among web tests.
detecting dependencies in e2e web tests is particularly challenging due to the stack of programming languages and technologies involved in the construction of modern web applications e.g.
html css javascript on the client side php java or javascript on the server side and back end layers through restful apis or databases.
our insight is that by analyzing the input data used in test cases highlighted in figure we can obtain clues about potential test dependencies caused by shared polluted states.
for example the input string user001 used at line in addusertest is awrite operation of persistent data.
the same input string is used at line insearchusertest as aread operation of persistent data.
we conjecture that such read after write connections on persistent data could indicate potential test dependencies.
given this insight our approach focuses on read after write relationships on persistent data defined as follows.
definition persistent read after write praw dependency .two test cases t1andt2executed in this order in the original test suite are subject to a praw dependency if t1performs an operation that writes some information siinto the persistent state of the web application and t2performs an operation that reads sifrom the persistent state of the web application.examples of the write operations in t1include creating updating or deleting a record in a database or creating a dom element on the webpage.
examples of read operations in t2are reading the same record from the database or accessing the newly created dom element on the webpage.
figure illustrates our overall approach which requires a web test suite as input along with a predefined test execution order.
overall our approach computes an initial approximated test dependency graph filters out potential false dependencies dynamically validates all dependencies in the graph while recovering any missing dependencies and finally handles missing dependencies affecting independent nodes possibly resulting from the previous validation step.
next we describe each step of our approach.
.
dependency graph extraction in the first step from the input test suite our approach computes an initial test dependency graph representing an approximated set of candidate dependencies.
this can be conducted in different ways as described below.
.
.
original order graph extraction.
a baseline approach consists of connecting all pairwise combinations of tests according to the original order.
this results in a directed graph in which every pair of distinct nodes is connected by a unique pair of edges so as to establish a dependency relation between each test and all the others that are executed before it.
if nis the number of test cases in the test suite the graph containsn n 2edges e.g.
dependencies .
since the time to validate a dependency graph increases with the number of dependencies in the graph heuristics can be used to reduce the size of the graph by removing edges that are less likely to be manifest dependencies.
to that end we propose an approach that leverages a fast static string analysis of the input data present in the tests to construct a smaller initial test dependency graph.
.
.
sub use string analysis graph extraction.
algorithm describes our dependency graph extraction based on sub use chain relations.
a sub use relation consists of a submission sub of input data iand all the following uses that submitted value i. starting from the first test case according to the original test suite order algorithm first retrieves the set sof input values submitted by the test like values inserted into input fields by input submitting actions such as the sendkeys methods line .
second the algorithm considers each test case tffollowing t and searches for any input value in the set s which is used in anytechnical report series fbk trento italy m. biagiola et al.
algorithm sub use string analysis graph extraction input to test suite in its original order o i set of input submitting actions output t dg test dependency graph with candidate dependencies to be validated 1t dg 2ta to tests to analyze foreach tintodo 4s getinputvalues t i sis the set of input values submitted by t ta ta t foreach tfintado 7u findusedvalues tf w uis the set of input values used by tf ifu then deptoadd tfu t candidate manifest dependency t dg t dg deptoadd end end end statement of the current test case tf line and adds them to the set of used valuesu.
if at least one string value is found i.e.
a sub use chain a candidate praw dependency between tandtf is created by adding the edge tf t labelled with each retrieved string value line to the test dependency graph line .
the internal loop of algorithm lines searches for the input values indistinctly in any test action because in web tests there is no clear distinction between read and write statements.
for instance in figure the sendkeys action at line is used to write persistent information into the application e.g.
in a database .
however the same sendkeys action at line identifies an action with a read connotation because the string value user001 is used to search for a specific persistent information in the web application.
let us consider the source code of the two test cases in figure namely addusertest and searchusertest .
from the first test addusertest the algorithm extracts the set s admin name001 firstname001 user001 password001 because sendkeys is the only input submitting action.
then the string values in sare looked up in the subsequent test searchusertest producing the set of used valuesu admin name001 firstname001 user001 .
beingunot empty the algorithm creates a candidate test dependency between searchusertest andaddusertest .
.
filtering the second step of our approach applies a filtering process to remove potential false praw dependencies.
the filtering is performed to speed up the subsequent validation step which requires in browser test execution and therefore can be computationally expensive for graphs with numerous candidate test dependencies.
finding an effective filtering technique is however challenging.
aloose filter might remove a few false dependencies whereas a strict filter might mistakenly remove manifest dependencies which would need to be recovered at a later stage.
in this work we propose two novel test dependency filtering techniques based on dependency free values and natural language processing nlp .
.
.
dependency free string value filtering.
we analyze the frequency of string input values used in the test suite to filter potential false praw dependencies.
t1 addusertestt6 enrolusertestt5 searchcoursetestt4 addcoursetestt2 searchusertestt3 loginusertestuser001password001user001password001 user001password001firstname001name001user001user001adminadminadminadminadminadmincourse001course001adminadmincourse001name001firstname001user001figure dependency free string based praw filtering.
solid black edges represent manifest dependencies whereas dashed red edges represent false dependencies.
let us consider the test dependency graph depicted in figure obtained by applying our sub use string analysis graph extraction section .
.
to the motivating example test suite section .
the set of dependencies t5 t2 t5 t1 t4 t1 represents instances of false candidate praw dependencies.
the edges between these test cases are only due to the same login input data used by the tests i.e.
admin a default user created during the installation for which no test case must be executed to create it.
existing techniques refer to such cases as dependencyfree values i.e.
if the test dependency graph includes dependencies that are shared across multiple or all test cases these likely false dependencies could be filtered out.
however in principle these assumptions might not hold in all cases as occurrence frequency alone is not conclusive for safe filtering.
our dependency free string value filtering computes a ranked list of frequently occurring strings and asks the developer to either confirm or discard them if a string value occurs in all test cases the corresponding dependency is automatically filtered .
in our example of figure our approach computes the frequencies of all strings presents it to the the tester who for instance may decide to filter the dependencies due to the admin string hence removing t5 t2 t5 t1 t4 t2.
.
.
nlp based filtering.
developers often use descriptive patterns for test case names which summarize the operations performed by each test.
giving a descriptive name to a test case has several advantages such as enhanced readability i.e.
it becomes easier to understand what behaviour is being tested and debugging i.e.
when a test case fails it is easier to identify the broken functionality .
for instance google recommends test naming conventions in which unit tests need to be named with the method being tested a verb or a verb phrase e.g.
pop and the application state in which the specific method is tested e.g.
emptystack .
forbehaviour based tests such as e2e tests the guidelines propose a naming convention that includes the test scenario e.g.
invalidlogin and the expected outcome e.g.
lockoutuser .e2e web test dependency detection using nlp technical report series fbk trento italy therefore our second filtering mechanism consists of using natural language processing nlp to analyze test case names and classify them into two classes namely read orwrite .
then based on such classification non praw dependencies such as a read test being dependent on another read test are discarded from the test dependency graph.
our approach uses a part of speech tagger pos to classify each token i.e.
word in a tokenized test name as noun verb adjective or adverb.
in particular our approach uses the verb from the test case name as the part of speech that conveys the nature of the test operation and uses it to classify each test into read orwrite classes.
our approach relies on two groups of standardized r w verbs namely crud operations create read update anddelete in which the read operation is pre classified as read whereas the other three are pre classified as write.
our approach uses pos to extract the first verb from each test name and then computes the semantic similarity specifically thewup metrics between the extracted verb and each verb in the pre classified read write groups.
the similarity score quantifies how much two concepts are alike based on information contained in the is a hierarchy of wordnet .
after computing all similarity scores our approach classifies the extracted verb to the group having the maximum similarity score.
in case of ties e.g.
the verb has the same similarity score for both the read and write classes or in case no verbs are found our approach does not perform any assignment and the dependency is not filtered.
our classification of read write verbs achieved a precision of and a recall of on our experimental subjects.
in this work we propose and evaluate three nlp configurations.
verb only nlp verb .
our first nlp filtering configuration considers only the verb of the test case name .
given a dependency ty tx our approach extracts the verb from both tyandtx and classifies them either as read or write.
then it filters the read after read rar dependencies in which both tyandtxhave verbs classified as read and the write after read war dependencies where tyhas a write classified verb whereas txhas a read classified verb.
all other types of dependencies such as read after write raw and waw write operations in web applications often also involve reading existing data are retained.
the dependency searchcoursetest searchusertest figure is filtered because it is classified as rar with search being the read classified verb.
conversely the edge searchusertest addusertest is retained since it is classified as raw being search andaddthe read write verbs respectively.
the word test is considered a stop word and removed before the nlp analysis starts.
verb and direct object nlp dobj .
the second configuration considers the direct object the verb refers to.
given a set of test cases our approach uses a dependency parser to analyze the grammatical structure of a sentence extract the direct object from each test name and construct a set of dobject dependencies.
rar and war dependencies are filtered as described in the previous nlp verb case.
differently raw and waw dependencies are filtered only if the direct objects of two verbs appearing in two tests tyandtxare different.
the intuition is that the two tests may perform actions on different persistent entities of the web application if the involved direct objects in the test names are different.
for example in figure the raw dependency searchcoursetest addusertest is filtered because the two involved direct objects course anduser are different.
verb and nouns nlp noun .
our third configuration takes into account all entities of type noun contained in the test names.
when the test name includes multiple different entities analyzing only the direct object may not be enough to make a safe choice.
for instance in our subject claroline the analysis of the direct object would erroneously filter the manifest dependency addcourseeventtest addcoursetest because it is a waw and the two direct objects event andcourse are different.
however there is an implicit relation between the direct object event and the course object it refers to.
thus the dependency with addcoursetest should be retained.
again rar and war dependencies are filtered as described in the nlp verb case.
here raw and waw dependencies are filtered only if the two tests involved in a dependency have no noun in common.
as such the manifest dependency addcourseeventtest addcoursetest would not be filtered in this configuration because of the shared name course .
.
dependency validation and recovery given a test dependency graph t dg the overall dynamic dependency validation procedure works according to the iterative process proposed by gambi et al.
.
the approach executes the tests according to the original order to store the expected outcome.
next it selects a target dependency according to a source first strategy in which tests that are executed later in the original test suite are selected first i.e.
t3 t2would be selected before t2 t1 .
to validate the target dependency tests are executed out of order i.e.
a test schedule in which the target dependency is inverted is computed and executed.
if the result of the test execution differs from the expected outcome the target dependency is marked as manifest because the failure was due to the inversion.
otherwise the target dependency is removed from t dg .
the process iterates until all dependencies are either removed or marked as manifest.
the dynamic validation procedure described above works correctly under the assumption that the initial tdg contains all manifest dependencies as the original order graph section .
.
.
in our approach the filtering techniques applied in the previous step may be not conservative.
therefore differently from existing techniques our approach features a dynamic dependency recovery mechanism that retrieves all missing dependencies.
to the best of our knowledge this is the first dependency validation algorithm that also includes dynamic dependency recovery.
recovering missing dependencies.
algorithm takes a partiallyvalidated t dg .
for each failing test schedule in which a target dependency is inverted it checks whether the failure is due to a missing dependency in the dependency graph.
more specifically algorithm takes the target dependency and computes a schedule in which the target dependency is not inverted line .
if the execution of such schedule complies with the expected outcome our approach considers the test failure due to the dependency inversion and marks the dependency as a manifest.
on the contrary if one or more tests fail also in the schedule without inversion line our approach assumes that one or more dependencies are missing and need to be recovered.technical report series fbk trento italy m. biagiola et al.
algorithm recovery algorithm input to test suite in its original order o t dg test dependency graph targetdep dependency selected for validation expresults results of executing to execresults results of a test schedule in which targetdep is inverted output t dg updated test dependency graph with missing dependencies recovered require expresults execresults i.e.
targetdep is manifest 1schedule computetestschedulewithnoinversion t dg targetdep 2execresults executetestschedule schedule 3failedtest getfirstfailedtest expresults execresults if failedtest nullthen failure due to a missing dependency get all tests before failedtest.
depcandidates getdepcandidates to schedule foreach depcandidate depcandidates do deptoadd failedtest depcandidate t dg t dg deptoadd end end to do so the algorithm takes the first failing test and retrieves the preceding test cases that were not executed in the schedule line .
those tests are all candidate manifest dependencies for the failed test.
the algorithm connects the failed test case with each such preceding test and adds those dependencies to the graph lines .
the graph obtained this way contains all newly added candidate manifest dependencies that still need to be validated.
at3t4t1t2 dt3t4t1t2bt3t4t1t2 ct3t4t1t2 figure recovery of missing dependencies.
a t4 t3is selected t4fails because t4 t2is missing.
b recovery procedure adds candidate dependencies.
c dependency validation.
d final t dg .
let us take as example figure .a in which t4has a missing manifest dependency ont2 and t3does not modify the application state.
according to the source first strategy the validation selects the dependency t4 t3.
the schedule computed for such dependencies is t4 in which t4fails because t2is not executed.
then our algorithm starts retrieving the missing dependency by computing a schedule in which t4 t3is not inverted t3 t4 in which t4fails again for the same reason.
the recovery procedure concludes that there is at least one missing dependency and connects t4with both t1andt2 figure .b i.e.
the only candidate manifest dependencies.
in figure .c the dependency t4 t3is selected again.
this time the computed schedule is t1 t2 t4 in which none of the tests fail.
therefore the dependency is marked as false and removed.
the next selected dependency is t4 t2 for which the schedule t1 t4 is computed.
the test t4fails because t2is not executed.
to check if the failure is due to a missing dependency our algorithm computes the test schedule t1 t2 t4 in which none of the tests fail.
our algorithm concludes that t4 t2is a manifest dependency and recovers it.
the validation iterates over the other dependencies in the same way and outputs the final t dg figure .d where the initially missing dependency has been recovered.
at3t4t1t2 dt3t4t1t2 et3t4t1t2ct3t4t1t2 bt3t4t1t2 figure disconnected dependency recovery.
a dependencies are validated t3 t1shadows the missing dependency t4 t1.
b t4and t2fail because of missing dependencies.
c recovery procedure adds candidate dependencies.
d dependencies are validated.
e final t dg .
.
disconnected dependency recovery the previous validation step can produce disconnected components in the t dg .
missing dependencies involving tests in disconnected components require a separate treatment.
two cases can occur tests with no outgoing edges zero out degree 1and isolated tests i.e.
tests having neither incoming nor outgoing edges zero in and out degree .
the former case occurs when a false dependency removed during the validation shadows a missing dependency.
in such cases disconnected components of t dg including potentially missing dependencies are created as a result of the validation.
figure .a illustrates an example the manifest dependency t4 t1is missing in the initial dependency graph.
let us suppose that t3does not change the state of the application when executed and therefore its execution does not influence the execution of any other successive test in the original order.
the algorithm selects first the dependency t4 t3 it produces the schedule t4 in which the test fails since t1is not executed.
our approach checks if the failure is due to a missing dependency.
when the dependency is not inverted the computed schedule is t1 t3 t4 in which none of the tests fail.
hence our algorithm concludes that t4 t3is a manifest dependency and no dependency recovery takes place.
in the next step the algorithm validates the dependency t3 t1 which is removed because t3can execute successfully without t1.
the dependency graph produced by dependency validation algorithm is illustrated in figure .b.
in the isolated subgraph t4 t3 the schedule t3 t4 results in a failure of t4.
indeed the dependency t4 t1was not captured by the recovery algorithm because the false dependency t3 t1shadowed the absence of the manifest dependency t4 t1.
figure .a also illustrates how our approach handles isolated tests.
in this example t2is an isolated node.
let us suppose that t2has a manifest dependency on t1 t2 t1 which is missing in the initial t dg because it is either not captured or because it is wrongly filtered out in the second step of our approach.
therefore the validation step would produce the t dg shown in figure .b in which there is no chance to check whether t2executes successfully in isolation.
in fact t2is not part of any test schedule that can be generated from t dg regardless of any possible dependency inversion.
for this reason a further recovery step is required once the validation is completed.
1first test t1excludede2e web test dependency detection using nlp technical report series fbk trento italy algorithm disconnected dependency recovery algorithm input to test suite in its original order o t dg test dependency graph output t dg updated test dependency graph with missing dependencies recovered 1expresults executetestsuite to get isolated nodes and nodes with no outgoing edges.
3disconnectedtests getdisconnectedtests t dg foreach disconnectedtest in disconnectedtests do execresults executetestinisolation disconnectedtest failedtest getfailedtest expresults execresults if failedtest nullthen t dg connectwithprecedingtests failedtest t dg to else if isnotisolated disconnectedtest then out degree in degree .
schedules computeschedules disconnectedtest t dg foreach schedule schedules do execresults exec schedule failedtest getfailedtest expresults execresults if failedtest nullthen t dg connectwithprecedingtests failedtest t dg to end end end end algorithm handles the recovery of missing dependencies within disconnected components.
the algorithm retrieves all isolated nodes and zero out degree nodes line and executes each of them in isolation line .
for each failing test the algorithm connects it with all preceding tests according to the initial test suite order line .
otherwise if a test is not isolated and executes successfully line the algorithm takes all schedules that contain that test and execute them lines .
if a test in those schedules fails the algorithm connects it with all the preceding ones line .
finally for each dependency found and added to t dg during the disconnected dependency recovery step the dependency validation procedure must be re executed.
given the graph in figure .b algorithm executes t2in isolation which fails thus the dependency t2 t1is added.
moreover in figure .b there is only one schedule that involves t3 namely t3 t4 .
in this schedule t4fails hence our approach adds the dependencies t4 t2andt4 t1 figure .c .
next the added dependencies are validated figure .d and the final graph is produced where all initially missing manifest dependencies have been successfully recovered figure .e .
to conclude our validation and recovery algorithms makes sure that newly added dependencies are themselves validated false dependencies are removed in the final t dg .
indeed a node in the final t dg can be either i connected i.e.
in degree 0and outdegree ii without outgoing edges i.e.
in degree 0and out degree or iii isolated i.e.
in degree out degree .
.
implementation we implemented our approach in a tool called tedd testdependency detector .
the tool is written in java and supports selenium webdriver web test suites written in java.
however our overall approach is general because it can be applied to test suites developed using other programming languages or web testing frameworks.
tedd expects as input the path to a test suite and performs the string analysis by parsing the source code of the tests by using spoon version .
.
.
our nlp module adopts algorithms available in the open source library corenlp version .
.
.
the outputoftedd is a list of manifest dependencies extracted from the final validated tdg.
empirical evaluation we consider the following research questions rq1 effectiveness how effective is tedd at filtering false dependencies without missing dependencies to be recovered?
rq2 performance what is the overhead of running tedd ?
what is the runtime saving achieved by tedd with respect to validating complete test dependency graphs?
rq3 parallel test execution what is the execution time speedup of the test suites parallelized from the test dependency graphs computed by tedd ?
.
subject systems we selected six open source web applications used in previous web testing research for which selenium test suites are available .
table lists our subject systems including their names version size in terms of lines of code number of test cases and the total number of lines of test code.
we use cloc to count lines of code.
during our experiments we used the original execution order of each test suite as specified by the developers.
table subject systems and their test suites web app test suites version loc loc avg tot claroline .
.
addressbook .
.
ppma .
.
collabtive .
mrbs .
.
mantisbt .
.
total .
procedure and metrics .
.
procedure.
we manually fixed any flakiness of the test cases of the subject test suites by adding delays where appropriate and we executed each test suite times to ensure that identical outcomes are obtained across all executions.
to form a baseline for comparison we applied dependency validation to the dependency graph obtained from the original order of each test suite section .
.
.
for each test suite we ran different configurations of tedd by combining each admissible combination of graph extraction and filtering technique.
the first evaluated configuration is string analysis sa in which the dependency graph is obtained through sub use chain extraction section .
.
and filtered from the dependencyfree values section .
.
.
then we evaluated three configurations in which we applied the three proposed nlp filters nlp verb nlpdobj nlp noun both to the graph from the original order as well as to the graph obtained with sa.
finally given the validated dependency graph obtained in each configuration we generated all possible test schedules that respect the test dependencies and we executed them sequentially.technical report series fbk trento italy m. biagiola et al.
table effectiveness rq1 performance rq2 and parallelization rq3 average results across all subject test suites.
effectiveness performance parallelization manifest deps.
validation speed up extracted filtered to validate false validated recovered recovered disc.
total praw extraction filtering val.
and recovery recovery disc.
total saving schedules worst case average baseline original order .
.
.
.
.
string analysis .
.
.
.
.
.
.
nlp verb original order .
.
.
.
.
.
.
nlp verb string analysis .
.
.
.
.
.
.
nlp dobj original order .
.
.
.
.
.
.
nlp dobj string analysis .
.
.
.
.
.
.
nlp noun original order .
.
.
.
.
.
.
nlp noun string analysis .
.
.
.
.
.
.
only validation no within recovery.
execution time .
minutes .
seconds .
.
.
metrics.
to assess effectiveness rq for each configuration we measured the number of false dependencies removed by tedd as well as the number of manifest dependencies that are missing and need to be recovered .
the number of false dependencies is obtained by subtracting the number of manifest dependencies retrieved at the end of the recovery step from the total number of dependencies in the initial graph.
we evaluated performance rq by comparing the execution time in minutes of each configuration of tedd with respect to the baseline approach.
concerning parallelization rq we measured the speed up factor of the parallelizable test suites with respect to the original test suite running time.
we considered two speed up scenarios.
average case in which we measured the ratio between the original test suite running time and the average running time of the parallelizable test suites and worst case in which we measured the speed up ratio between the original test suite running time and the parallelizable test suite having the highest runtime.
.
results effectiveness rq .for each configuration of tedd table effectiveness shows the number of extracted dependencies starting from the initial test suite figure step and the number of filtered dependencies figure step .
it also reports information about the validation and recovery steps specifically the number of false dependencies detected the number of dependencies recovered and those recovered from the disconnected components.
the final number column shows the number of dependencies in the final t dg s all of which are manifest praw dependencies.
across all apps the baseline approach validated on average dependencies of which were deemed as false and as manifest.
the most conservative among tedd s configurations is nlpverb original order which validated overall dependencies of which were false less than the baseline and detecting false00.
.
.
.
.
missing recovered .
.
.
.
.
baseline original order string analysisnlp verb original order nlp verb string analysis nlp dobj original order nlp dobj string analysis nlp noun original order nlp noun string analysis figure pareto front manifest dependencies without filtering recovering any.
the least conservative configuration of tedd is nlp noun string analysis which retained only dependencies on average from the initial graphs of which were detected as false five dependencies had to be recovered leading to the final number of manifest dependencies.
overall the number of missing dependencies due to filtering and recovered in steps is very low of the initial number of dependencies .
tedd does not ensure having minimal test dependency graphs.
therefore the number of manifest dependencies retrieved by each configurations is slightly different between and column total praw .
however these differences do not affect the executability of the schedules that respect the dependencies see results for rq .e2e web test dependency detection using nlp technical report series fbk trento italy figure shows the pareto front plotting the ratio between false and missing dependencies for each configuration.
each point represents the average missing false values across all subjects normalized over the respective maximum values.
this essentially shows the tradeoff between the false dependencies remaining after the filtering step and the missing dependencies to be recovered.
from the analysis of the pareto front we can see that the nondominated configurations are those based on nlp verb nlp noun and string analysis sa .
the baseline approach baseline has the highest number of false dependencies on average and no missing dependencies.
on the contrary sa filters many dependencies on average but has the highest number of missing recovered dependencies on average .
interestingly nlp verb original order does not miss any manifest dependency but has more false dependencies compared to the other nlp based configurations.
configurations nlp dobj both sa and original order and nlpnoun both sa and original order are comparable regarding the number of false dependencies remaining after filtering.
however nlp noun sa and original order needs to recover substantially less manifest dependencies.
indeed nlp noun dominates nlpdobj while both nlp noun sa and original order configurations are on the non dominated front being both optimally placed in the lower left quadrant of the pareto plot.
performance rq .table performance reports the average runtime in minutes of each step of tedd across all configurations.
the most expensive step of tedd is validation especially for what concerns validating the connected part of the graph column whereas dependency graph extraction and filtering columns and have negligible costs under one minute on average in all cases .
the cost of disconnected components recovery column is generally low ranging from nearly one minute for nlp verb to maximum nine minutes for nlp dobj .
minutes on average .
the slowest configuration of tedd is nlp verb original order which is faster on average almost hours less than the baseline approach.
the fastest configuration of tedd is nlp noun sa which is faster on average hours less than the baseline approach.
this result confirms the pareto front analysis showing that npl noun sa is the most effective configuration of tedd .
the table reports also the percentage decrease of each configuration with respect to the baseline which took approximately minutes on average hours .
overall all sa or nlp based configurations of tedd are significantly faster.
parallel test execution rq .column schedules reports the average number of test schedules obtained from the final t dg s. isolated nodes in the dependency graphs are counted as singletest schedules.
columns and report the relative speed up of the parallelizable test suites considering the longest test execution schedule worst case and the average case.
first in our experiments no test failures occurred in any of the parallelizable test suite produced by any configuration of tedd .
essentially this testifies that the dependency validation and recovery algorithm does not miss any manifest dependency.
overall all techniques achieve similar speed up scores around in the worst case and on average.
this is expected since the final tdgs are similar across configurations see total number of manifest dependencies in table .
however results differ across applications.
table presents the results for the nlp noun sa table parallelization results for nlp noun sa worst case averageruntime original schedules runtime speed up runtime speed up claroline .
.
.
.
.
addressbook .
.
.
.
.
ppma .
.
.
.
.
collabtive .
.
.
.
.
mrbs .
.
.
.
.
mantisbt .
.
.
.
.
total .
.
.
.
.
average configuration.
column runtime original reports the execution time of the original test suite in seconds.
collabtive has the slowest test suite almost minutes whereas addressbook has the fastest seconds .
the highest speed up in the worst case occurs for claroline where the longest test execution schedule test suite is .
faster.
mantisbt exhibits the highest speed up in the average case .
but the lowest speed up in the worst case .
due to a single slow executing schedule s with respect to the average runtime s .
the lowest speed up in the average case occurs for mrbs but it remains still high .
.
discussion automation and effectiveness.
our results confirm that e2e web tests entail test dependencies such dependencies can be identified by considering praw connections between test cases and tedd can successfully detect all praw test dependencies necessary for independent test case execution.
all proposed filtering techniques proved both very fast and effective at reducing the size of the initial graph without filtering many manifest dependencies.
performance and overhead.
all configurations of tedd achieve substantial improvements with respect to validating the graph extracted from the original ordering whose validation cost is quadratic on the number of test cases.
this means that every time a new test case is added to a test suite containing ntest cases such graph requires that nmore dependencies are validated.
moreover despite the computational cost of nlp processing the dependency graph extraction and filtering steps exhibit negligible costs less than a minute over all test suites compared to the validation time.
relation to test optimization techniques.
tedd can be used by test engineers to detect praw like dependencies in e2e web test cases.
the validated dependency graph produced by tedd can be used as input for devising novel test optimization and regression testing techniques in the web domain.
for instance test prioritization and test minimization can be formulated as constrained optimization problems in which dependencies by tedd play the role of constraints that can be addressed using smt solvers or search based heuristics .
test selectiontechnical report series fbk trento italy m. biagiola et al.
can benefit from the t dg s produced by tedd to identify which test cases are required by modification traversing test cases .
in this work we studied an application to test parallelization as a proxy to assess the correctness of the t dg s produced by tedd .
specifically we applied dependency aware graph traversal to retrieve all potential parallel test suites and measured the runtime speed up with respect to the initial test suites.
test smells.
our test dependency graph can also be utilized for other test analysis activities such as detecting poorly designed tests i.e.
test smells or detecting obsolete tests.
for instance the test case checkentrytagsremoved ofppma executes after addentrytags andremoveentrytags tests.
by analyzing the t dg produced by tedd for this test suite we noticed thatcheckentrytagsremoved executes properly also when no tags have been created yet i.e.
checkentrytagsremoved isisolated in thet dg .
this means that the test checkentrytagsremoved is obsolete because subsumed by the previous removeentrytags test.
therefore it can be safely removed with no impact on the functional coverage or assertion coverage of the test suite.
test suite evolution.
during software evolution if new test cases are added to the test suite dependencies are added to the previously validated t dg through new dependency extraction and filtering.
validation and recovery must be carried out on the entire new t dg .
however this analysis is expected to be faster than the first initial one given that the validated t dg we are adding dependencies to contains only manifest dependencies.
limitations.
tedd depends on the information available in the test source code used to identify potential test dependencies.
as such the effectiveness of our nlp based filtering may be undermined if test case names are not descriptive as in the case of many automatically generated test suites e.g.
test1 test2 .
in such cases testers can rely on the string analysis configuration of tedd sa which also proved effective in our study.
second our tool does not provide information about the root cause of the dependencies i.e.
what part of the program state is polluted by which test.
lastly tedd does not support the analysis of flaky test suites.
threats to validity.
using a limited number of test suites in our evaluation poses an external validity threat.
although more subject test suites are needed to fully assess the generalizability of our results we have chosen six subject apps used in previous web testing research pertaining to different domains for which test suites were developed by a human web tester.
threats to internal validity come from confounding factors of our experiments such as test flakiness.
to cope with possible flakiness of the test cases we manually fixed any flaky test by adding delays where appropriate and we ran each test suite times to ensure having identical results on all executions.
related work test dependency detection.
different techniques have been proposed recently to detect dependencies in unit tests none of which focuses on web tests.
zhang et al.
developed dtdetector which detects manifest dependencies in junit tests with a dependencyaware k bounded algorithm.
they showed that a small value for k e.g.
k 1andk finds most realistic dependent tests.
poldet retrieves tests that pollute the shared state of a javaapplication i.e.
heap or filesystem and provides ways to inspect such polluted states e.g.
access path through the heap that leads to the modified value or the name of the file that was modified.
the tool vmvm uses test virtualization to isolate unit tests of a junit suite by resetting the static state of the application to its default before each test execution.
our tool tedd differently from the previous works targets the web domain by focusing on dependencies due to read after write operations performed on persistent data.
electrictest utilizes dynamic data flow analysis to identify all conflicting write and read operations over static java objects.
on the contrary in e2e web tests the semantics of read and write operations is implicit and mediated by multiple layers of indirection such as client side dom server side application state database entries and remote service calls.
tedd leverages heuristics based on sub use chain relations and nlp to discover potential test dependencies without the need for complex data flow analysis.
pradet detects test dependencies in java unit tests focusing on manifest dependencies that can be traced back to data dependencies.
tedd adopts a similar approach as pradet to validate the test dependencies i.e.
validating a single dependency at a time by inverting the dependency and linearizing the graph.
the main differences between pradet and our approach are the capability of tedd to handle incomplete dependency graphs pradet makes the assumption that the initial dependency graph contains all manifest dependencies computed through static analysis of all read write accesses to global java variables.
due to this assumption pradet only removes false dependencies and it does not recover any missing dependencies.
tedd on the other hand features a novel recovery algorithm to detect and validate all potentially missing manifest dependencies which makes it suitable for web e2e test suites in which applying thorough data flow analysis is neither feasible nor straightforward due to the heterogeneity of technologies and languages used in modern web applications.
lastly tedd introduces novel extraction and filtering heuristics which extend the applicability of pradet beyond read write operations performed on static fields of java classes.
conclusions and future work in this paper we proposed a novel test dependency technique for e2e web test cases implemented in a tool called tedd .
we used tedd for detecting test dependencies in six web test suites.
thanks to an effective combination of novel string analysis and nlp tedd achieves an optimal trade off between false dependencies to be removed and missing dependencies to be recovered.
moreover tedd detected the final set of manifest test dependencies on average faster than the baseline approach.
the resulting test dependency graph supports parallelization of the tests with a speed up factor of up to .
in our future work we plan to apply nlp on the dom and on page object based test suites as well as to devise ways to produce a minimal t dg i.e.
a t dg having the minimum number of dependencies.
we also intend to run tedd on more subject web test suites and to extend it to support mobile test suites.