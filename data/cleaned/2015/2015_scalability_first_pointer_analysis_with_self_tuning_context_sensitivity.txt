general rights copyright and moral rights for the publications made accessible in the public portal are retained by the authors and or other copyright owners and it is a condition of accessing publications that users recogniz e and abide by the legal requirements associat ed with these rights.
users may download and print one copy of any publication from the public portal for the purpose of private study or researc h. you may not further distribute the material or use it for any profit making activity or commercial gai n you may freely distribute the url identifying the publication in the public portal if you believe that this document breaches copyright please contact us providing details and we will remove access to the wo rk immediately and investigate your claim.
if the document is published under a creative commons license this applies instead of the general right s. this coversheet template is made available by au library version .
december coversheet this is the accepted manuscript post print version of the article.
contentwise the accepted manuscript version is identical to the final published version but there may be differ ences in typography and layout .
how to cite this publication please cite the final published version yue li tian tan anders m ller and yannis smaragdakis.
.
scalability first pointer analysis with self tuning context sensitivity.
in proceedings of the 26th acm join t european software engineering conference and sympo sium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
publication metadata title scalability first pointer analysis with self tuning context sensitivity author s yue li tian tan anders m ller and yannis smaragdakis journal proceedings of the 26th acm join t european software engineering conference and sympo sium on the foundations of software engineering esec fse document version accepted manuscript post print the authors .
this is the author s version of the work.
it is posted here by permission of acm for your personal use.
not for redistribution.
the definitive version was published in proceedings of the 26th acm join t european software engineering conference and sympo sium on the foundations of software engineering esec fse november .
scalability first pointer analysis with self tuning context sensitivity yue li tian tan anders m ller aarhus university denmark yueli tiantan amoeller cs.au.dkyannis smaragdakis university of athens greece smaragd di.uoa.gr abstract context sensitivity is important in pointer analysis to ensure high precision but existing techniques suffer from unpredictable scalability.
many variants of context sensitivity exist and it is difficult to choose one that leads to reasonable analysis time and obtains high precision without running the analysis multiple times.
we present the scaler framework that addresses this problem.
scaler efficiently estimates the amount of points to information that would be needed to analyze each method with different variants of context sensitivity.
it then selects an appropriate variant for each method so that the total amount of points to information is bounded while utilizing the available space to maximize precision.
our experimental results demonstrate that scaler achieves predictable scalability for all the evaluated programs e.g.
speedups can reach 10x for object sensitivity while providing a precision that matches or even exceeds that of the best alternative techniques.
ccs concepts theory of computation program analysis keywords static analysis points to analysis java acm reference format yue li tian tan anders m ller and yannis smaragdakis.
.
scalabilityfirst pointer analysis with self tuning context sensitivity.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
introduction pointer analysis is a family of static analysis techniques that provide a foundation for many other analyses and software engineering tasks such as program slicing reflection analysis bug detection security analysis program verification and program debugging and comprehension .
the goal of pointer analysis is to statically compute a set of objects abstracted as their allocation sites that a program variable may point to during run time.
although stating this goal is simple it is permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
01000200030004000500060002obj 2type citimeout figure comparison of running time seconds of object sensitivity type sensitivity and context insensitive analyses.
the y axis is truncated to seconds for readability and all truncated cases reach the time budget seconds.
challenging to produce precise analysis results without sacrificing scalability .
thus for decades researchers have continued to develop sophisticated pointer analysis techniques .
one of the key mechanisms for achieving high analysis precision iscontext sensitivity which allows each program method to be analyzed differently according to the context it is used in .
context sensitivity has different variants depending on the kind of context information used.
for java programs object sensitivity and type sensitivity have proven to be quite effective.
the former is strictly more precise but less efficient than the latter .
however with any available variant although the analysis can gain in precision scalability is known to be unpredictable in the sense that programs very similar in size and other complexity metrics may have completely different scalability profiles.
figure shows time spent analyzing real world java programs1under object sensitivity 2obj which is among the most precise variants of context sensitivity type sensitivity 2type and context insensitivity ci .
we observe that 2obj is not scalable for out of programs within hours while it can finish running for programs within minutes program size is far from a reliable predictor for example jython methods is smaller than briss methods however 2type is not scalable for the former but scalable for the latter 1these are all popular open source applications including the heaviest jython and eclipse of the dacapo benchmarks .esec fse november lake buena vista fl usa yue li tian tan anders m ller and yannis smaragdakis context insensitivity exhibits very good and stable scalability for all programs but it is much less precise .
scalability of context sensitivity is not only unpredictable but also tends to be bimodal when analyzing a given program with existing context sensitivity techniques usually the analysis either terminates relatively quickly we say that the analysis is scalable in this case or it blows up and does not terminate even with very high time limits.
consider a scenario where the task is to analyze a set of programs within a given time budget for example as part of a largescale security analysis.
should one pick a precise context sensitive pointer analysis and take the risk that it may not scale for several programs or pick a scalable context insensitive analysis that sacrifices precision for all programs?
one answer is introspective analysis which tunes context sensitivity according to the results of a first pass context insensitive analysis.
however that approach is as the authors put it an if all else fails analysis that should only be used if traditional context sensitive algorithms fail.
thus computing resources are wasted because one has to wait until the context sensitive analysis reaches a timeout before the introspective analysis is run as a fallback.
in this paper we present a pointer analysis framework named scaler that has the following desirable properties.
i users only have to apply scaler once for each program without a need to experiment with different variants of context sensitivity.
ii scaler prioritizes scalability.
given a reasonable time budget scaler can be expected to finish analyzing any given program pwithin the budget.
more specifically if a context insensitive pointer analysis is scalable for p users can confidently expect thatscaler will also scale for p. iii scaler is able to achieve precision comparable to or better than that of the most precise context sensitivity variant that is scalable for p. that is the user does not need to manually pick a context sensitivity variant a priori but can expect to effectively match the best option that one could have picked on a single analysis run.
experimentally this precision is much greater than prior introspective analyses .
the key insight of scaler is that the size of context sensitive points to information or equivalently the total memory consumed is the critical factor that determines whether analysis of a given program using a particular context sensitivity variant is scalable or not and that it is possible to estimate the size of context sensitive pointsto information produced by different kinds of context sensitivity without conducting the context sensitive analysis.
this estimate can be computed using a cheap context insensitive pre analysis by leveraging the notion of object allocation graphs .
scaler is parameterized by a number called the total scalability threshold tst which can be selected based on the available memory.
for a given tst scaler automatically adapts different kinds of context sensitivity e.g.
object sensitivity type sensitivity and context insensitivity at the level of individual methods in the program to stay within the desired bound and thereby retain scalability while utilizing the available space to maximize precision.
in summary this paper makes the following contributions we propose the scaler pointer analysis framework that given a total scalability threshold automatically selects an appropriate variant of context sensitivity for each method in the program being analyzed section .
the approach relies on the concept ofscalability critical methods that helps explain why contextsensitive pointer analysis is sometimes unscalable.
we present a novel technique to efficiently estimate the amount of points to information that would be needed to analyze each method with different variants of context sensitivity using object allocation graphs section .
we describe our open source implementation section .
we conduct extensive experiments by comparing scaler with state of the art pointer analyses section .
the evaluation demonstrates that scaler achieves predictable scalability for all the evaluated programs in one shot while providing a precision that matches or even exceeds that of the best alternatives.
as an example the jython benchmark from dacapo is known to cause problems for context sensitive pointer analysis 1type is the most precise conventional pointer analysis that is scalable for jython taking around minutes on an ordinary pc while 2obj and2type do not complete within hours.
in comparison scaler achieves significantly better precision than both 1type and the state of the art introspective analysis and takes under minutes.
background pointer analysis typically consists of computing the points to sets of variables in the program text.2points to sets form a relation between variables and abstract objects i.e.
a subset of var obj with varbeing the set of program variables and objthe set of abstract objects.
abstract objects are typically represented as allocation sites i.e.
instructions that allocate objects e.g.
newin java .
an allocation site stands for all the run time objects it can possibly allocate.
this representation by nature loses significant precision.
a program variable corresponds to many run time incarnations during program execution not just for executions under different inputs but also for different instances of local variables during distinct activations of the same method.
to combat the loss of precision context sensitive pointer analysis enhances the computed relations to maintain a more precise abstraction of variables and objects .
variables get qualified with contexts to distinguish their different incarnations.
the analysis effectively computes a subset of ctx var obj where ctxis a set of contexts for variables.3this precision is valuable for intermediate analysis computations even though the final analysis results get collapsed in the easily understandable var obj relation distinguishing the behavior of a much used variable or 2a second formulation is that of alias analysis which computes the pairs of variables or expressions that can be aliased.
for most published algorithms computing points to sets and computing alias sets are equivalent problems one can be mapped to the other without affecting the algorithm s fundamental precision or scalability.
3for simplicity of exposition we ignore context sensitivity for abstract objects a.k.a.
heap cloning which also qualifies objwith a set of heap contexts hctx in much the same way as qualifying variables.scalability first pointer analysis with self tuning context sensitivity esec fse november lake buena vista fl usa object e.g.
in a library method according to the context in which it is used helps the precision at all use sites of the common code.
two observations on context sensitivity will be important in our subsequent discussion.
first contexts qualify variables but are generally chosen per method .
the main use of context sensitivity is to distinguish different activations of the same method e.g.
stack frames for the same procedure in traditional stack based languages which create fresh instances of all local variables at run time.
therefore all local variables of the same method have the same set of contexts.
second the worst case complexity of a context sensitive pointer analysis is much higher than that of a context insensitive one the number of computed points to facts and hence the complexity of the analysis increases in the worst case multiplicatively by the size of the set ctx.
however in the common case the precision of context sensitivity often compensates reducing the analysis complexity different contexts divide the points to sets of variables into non overlapping subsets.
in the ideal case if a context insensitive analysis computes a points to relation pt var obj a contextsensitive analysis would compute a relation pt ctx var obj that is not greater in cardinality than var obj.
the extra precision of the ctxinformation would be enough to split the original pointsto sets into disjoint subsets.
this observation also hints at why context sensitivity has unpredictable scalability when it works well to maintain precision its cost is modest to none.
when it fails to do so however the cost is orders of magnitude higher.
the actual definitions of the set ctxcan vary widely.
three main categories are call site sensitivity object sensitivity and type sensitivity call site sensitivity hasctxbe a sequence of call sites i.e.
instructions that call the method in which the qualified variable has been declared.
object sensitivity hasctxbe a sequence of abstract objects they are the receiver object of the method containing the qualified variable the receiver object of the method that allocated the previous receiver object etc.
type sensitivity keeps the same information as object sensitivity but objects used as contexts are collapsed not per allocation instruction but per class that contains it.
we refer the reader to surveys that discuss the options in full detail .
the scaler framework in this section we present an overview of the scaler approach.
we first describe the idea of scalability thresholds which is the key to predictable scalability section .
.
next we explain how scaler automatically chooses a suitable scalability threshold for each program based on a single parameter called the total scalability threshold that depends only on the available space for storing pointsto information section .
.
these ideas are then collected in the overall scaler framework section .
.
.
scalability thresholds we begin by introducing the concept of scalability critical methods section .
.
and we then leverage this concept to address unscalability section .
.
.
.
.
scalability critical methods.
the scalability of a contextsensitive pointer analysis is closely linked to the amount of points to information it produces i.e.
the size of relation pt ctx var obj.
the challenge that scaler addresses is to closely upper bound the size of pt without performing a context sensitive analysis given only the result pt var obj of a context insensitive analysis.
prior approaches have made similar attempts to obtain scalability but without a concrete model to predict scalability effectively as explained in section .
by then distinguishing which parts of the program can contribute disproportionately to the total size of pt scaler can adjust its context sensitivity at different methods to yield higher precision only when this does not endanger scalability.
to achieve this we introduce the concept of scalability critical methods .
these are methods that are likely to yield very large amounts of context sensitive points to information.
whether a method is scalability critical depends on the context sensitivity in question.
for example a method may be scalability critical under 2obj a object sensitive analysis but not under 2type a typesensitive analysis .
definition .
scalability critical methods .
a method mis scalability critical or for brevity just critical under context sensitivity c if the value of the product ctxcm pts mexceeds a given scalability threshold stp where ctxcmis an estimate of the number of contexts for mif using context sensitivity variant c computed using a fast contextinsensitive analysis pts mis the sum of the sizes of the points to sets for all local variables in m computed using the same context insensitive analysis and stpis a value that can be given by users section .
.
or computed automatically section .
for program p. that is we upper bound the potential contribution of a method to the context sensitive points to set pt ctx var obj by computing the number of potential combinations of possible abstract objects i.e.
a subset of obj and contexts i.e.
a subset of ctx that may pertain to the method.
the individual numbers are guaranteed to be upper bounds since they are computed by a less precise and thus conservatively over approximate context insensitive analysis.
accordingly their product is guaranteed to be an upper bound of the number of potential combinations.
the intuition behind definition .
is that a method mmay cause scalability problems for different reasons mis being analyzed in too many contexts too much points to information is computed within m or although the individual numbers of contexts and points to facts for mare not very large their product is.
for this reason the product of the estimates ctxcmand pts mgives an indication of potential scalability problems.
it makes sense to perform this reasoning at the method level since as discussed in section decisions on context sensitivity are made per method i.e.
for all local variables of a method together.
the number pts mcan be obtained directly from the contextinsensitive points to relation pts m v m pt v .
computing ctxcmis less straightforward.
this is one of the technical novelties of the approach and we postpone its discussion until section .esec fse november lake buena vista fl usa yue li tian tan anders m ller and yannis smaragdakis 2obj2type010 000stmethod 000method 20001method 4000method 1type2type1type2obj1 000p ctx cm pts mc c c figure choosing a context sensitivity strategy cfor the methods of program pbased on a scalability threshold stp.
.
.
choosing context sensitivity strategies.
given a scalability threshold stp scaler classifies each method mas scalability critical or not relative to a context sensitivity variant.
it then ensures scalability for critical methods by selecting a cheaper less precise context sensitivity variant for them.
as a result different methods will be analyzed with different context sensitivity variants.
assume there is a set of context sensitivity variants c c1 c2 ... cn where ciis typically more precise but less efficient than ci .
for example this set could be c 1type 2type 2obj .
figure illustrates imaginary distributions of ctxcm pts m values log scale y axis for a method program with each method m x axis ranked by decreasing value of ctxcm pts m under different context sensitivity variants c c. assume a suitable stpvalue has been chosen we explain in section .
how this can be done .
according to definition .
the first methods are scalability critical under 2obj and the first are scalabilitycritical also under 2type .
all methods are non scalabilitycritical under 1type .
for each method m scaler selects the most precise contextsensitivity variant ci cfor which mis not scalability critical and context insensitivity ci if none exists.
that is method mis analyzed with context sensitivity selectctx m stp selectctx m stp cnifmis non critical under cnforstp ckif ck mis non critical under ck mis critical under ck 1forstp ci otherwise for example in figure methods to will be analyzed using 1type methods to using 2type and methods to using 2obj .
the remaining issue which we address in the following section is how to choose an appropriate scalability threshold for a given program to be analyzed.
.
total scalability thresholds as discussed earlier the overall cost of a context sensitive pointer analysis is closely linked to the size of the points to relation being computed.
an analysis that fails to terminate within realistic time limits often does so because the space needed to represent the tst tst is memory related ctx1 p1 obj1ctx2 p2 obj4ctx3 p3 obj9ctx p objkmn... ...... ...... ...... ...program 1program ctx cm pts m m ctx cm pts m m ctx cm pts m mfigure context sensitive points to information and tst.
points to relation exhausts the available ram.
the scalability of pointer analysis is therefore related to the features of the analysis environment most importantly the memory size.
we now show how scaler determines a scalability threshold for a given program based on the concept of a total scalability threshold that represents the analysis capacity and can be selected based on for example the available memory independently of the program being analyzed.
definition .
total scalability threshold .
atotal scalability threshold tst is a number that satisfies the inequality e stp tstwheree stp is the estimated cost for the given value of stp computed as the sum of the sizes of the points to relations cf.
definition .
for all the methods in the program e stp m ctxselectctx m stp m pts m figure illustrates the concept.
the complexity of a pointer analysis running on a given program is closely related to the size of the points to information which is upper bounded by e stp .
thetstof the environment can be seen as analogous to the volume of a container.
the volume of the points to information computed by the analysis has to fit in the tstvolume for the analysis to scale i.e.
e stp tst.
this effectively normalizes the analysis capacity for all programs regardless of the number of methods they contain or the variants of context sensitivity employed.
we discuss in section the actual tstvalues used in our experiments.
importantly in the tstinequality ctxselectctx m stp m depends on the choice of context sensitivity which can vary based on the approach described in section .
.
the self tuning approach of scaler consists of computing given the tst an appropriate value ofstpfor a program pthat will be used to assign appropriate context sensitivity variants to p s methods.
to maximize precision while ensuring scalability scaler needs to pick the largest stpthat satisfies the tstinequality.
in this way as many methods as possible are analyzed with the most precise context sensitivity that the total analysis capacity can afford.
that is scaler needs to pick max stp e stp tst .
figure illustrates the mechanism using the distribution of figure .
this time stpis not given by the user in advance.
instead its value is computed to satisfy the inequality shown in figure .
for a candidate stpvalue each method is classified as detailed in section .
.
it is assigned the most precise context sensitivity that keeps it from being scalability critical.
in the case of figure the value of e stp for this program corresponds to the sum of the areas of a1 a2 and a3.
ifa1 a2 a3under the candidate stpis below tst then stpis viable butscalability first pointer analysis with self tuning context sensitivity esec fse november lake buena vista fl usa method a1a2a3a1 a2 a3 tst is automatically computed based on the above inequalitymethod 00020001method 4000method 000stp ctx cm pts m pst2type1type2objc c c eee stp figure selecting the scalability threshold stp based on the total scalability threshold tst .
higher stpvalues may also be viable.
the maximum such stp is computed using binary search in a range between and the maximum value of ctxselectctx m stp m pts mfor all m. .
overall approach figure shows the overall structure of the scaler framework.
scaler is a stand alone system that automatically selects a permethod context sensitivity variant that can subsequently be used to configure an existing pointer analysis tool like doop wala or chord .
given a program p a fast but imprecise context insensitive pointer analysis is run first to produce some basic points to information that is used for estimating pts m section .
.
and ctxcm section .
the core of scaler consists of one component that decides the scalability threshold based on a given total scalability threshold section .
and another component that chooses context sensitivity strategies for all methods in the program section .
.
.
the framework relies on the collection cof context sensitivity variants.
all that is needed for each variant is a mechanism for estimating the number of contexts as presented in section for object sensitivity and type sensitivity.
estimating the number of contexts in section .
we postponed the discussion of how to compute an upper bound on the number of contexts ctxcm for a given variant of context sensitivity c using only context insensitive analysis results.
this computation is one of the key elements of scaler and distinguishes it from prior approaches that have also tried to adapt context sensitivity on a per method basis .
the computation of the possible contexts for the context sensitive analysis of a method from only context insensitive analysis results is relatively easy for simple variants of context sensitivity such as call site sensitivity the possible contexts are call sites readily identifiable in the program code.
this computation is nontrivial for object and type sensitivity however the contexts of a method are abstract objects determined by the analysis mechanism.
we are not aware of any prior work that performs a similar computation of possible contexts for object sensitive or type sensitive analyses without running the analyses themselves.scaler performs this computation by leveraging the object allocation graph oag structure proposed by tan et al .
.
with the oag the context computation problem can be formulated as a graph traversal problem.
for any program based on an oag derived from pre analysis context insensitive pointer analysis scaler computes the number of contexts ctxcm of every method for each kind of context sensitivity cused 2obj 2type and1type in our setup by enumerating all contexts of the method.
the oag of a program is a directed graph.
a node of the oag represents an abstract object which is identified by its allocation site in the program.
an edge of the oag say o1 o2 represents an object allocation relation between o1ando2 i.e.
o1is a receiver object of the method that contains the allocation site of o2.scaler leverages the pre analysis to build the oag for the given program.
the oag provides a graphical perspective of object and type sensitivity i.e.
a k depth context in object sensitivity corresponds to ak node path in the oag .
thus to compute k object sensitive contexts of a method m scaler simply enumerates k node paths in the oag leading to the receiver objects of m. figure illustrates the mechanism with a simple example.
the allocation sites are labeled b1 b2 and c1 respectively.
suppose we compute 2obj contexts for method m line and its receiver object is c1 allocated at line .
further b1andb2are two allocator objects of c1according to k object sensitivity .
the possible 2obj contexts of m are and .
the corresponding oag is given in figure which shows two node paths that are exactly the 2obj contexts for m .
since a context insensitive analysis over approximates the fully precise oag some edges may be spurious.
however this is fine since we only need an upper bound of the number of possible contexts to establish scalability.
type sensitivity is an isomorphic approximation of object sensitivity thus we can easily derive the contexts for type sensitivity with the oag.
following the definition scaler computes the contexts for type sensitivity by merely replacing objects in contexts as computed from the oag by the types that contain the allocation sites of the objects.
for example to compute the contexts for method m figure under 2type scaler first obtains the 2obj contexts i.e.
and then replaces b1andb2by type a and c1by type b. as a result there is only one context of m under 2type i.e.
.
implementation we have implemented scaler as a stand alone open source tool in java available at scaler is designed to work with various pointer analysis frameworks such as doop wala chord and soot .
to demonstrate its effectiveness we have integrated scaler with doop a state of the art pointer analysis framework for java.
in practice we found that the ctxcm pts mvalues of some java collection methods under package java.util.
are very large because the collection methods are frequently called thus their ctxcmvalues are large and many objects are passed to the 4we do not consider call site sensitive analyses 1call 2call or object sensitivity 1obj in our evaluation as these analysis variants are typically both less precise and less scalable than at least one of the analyses in our setup.
the scaler approach can be adapted to any collection of context sensitivity variants as long as a relative ordering in both increasing precision and cost is possible.esec fse november lake buena vista fl usa yue li tian tan anders m ller and yannis smaragdakis tst basedst selectionst basedcontext sensitivitystpselected context sensitivityfor every method of p scalable precisepointer analysis pre analysispoints toinformation scalertstoptional program p figure overview of the scaler framework.
class a voidfoo b b1 new b b1 b1.bar voidgoo b b2 newb b2 b2.bar classc voidm ... classb voidbar c c newc c1 c.m b1 b2 c1 figure an example illustrating ctxcmcomputation.
collection methods thus their pts mvalues are large.
this would affect stp based context sensitivity which may make scaler pick less precise context sensitivity for these methods.
however collection methods are important to the precision of pointer analysis thus should be analyzed as precisely as possible by scaler .
to address this problem scaler treats methods under package java.util.
specially explicitly assigning them to be analyzed by the most precise context sensitivity i.e.
2obj in our settings .
evaluation in the evaluation we investigate the following research questions rq1.
does scaler consistently achieve scalability while matching or exceeding the precision of the most precise conventional pointer analyses that use the same variant of contextsensitivity for the entire program?
rq2.
how does scaler fare against introspective analysis that also applies context sensitivity selectively for the different methods of the program?
rq3.
what is the overhead of running scaler ?
what are the computed values of stpand what are the resulting distributions ofselectctx in practice?
rq4.
how does scaler perform with different tstvalues and different memory sizes?
experimental setup.
all pointer analyses were performed using doop with the version published in the artifact of which contains the exact setup for different analyses .
the time budget for all analyses is hours seconds .
to demonstrate the generality of scaler s scalability and precision we consider real world java programs that cover different levels of program complexity.
five of these programs come from the dacapo benchmark suite and include the toughest to analyze programs jython eclipse .
the rest are well known open source applications.
concretely as shown in table programs jython soot pmd briss jedit andeclipse represent complex applications for which 2obj is not scalable within hours programs findbugs andchart represent medium complexity programs for which 2objcosts thousands or hundreds of seconds programs luindex and lusearch represent simple programs for which 2obj costs only dozens of seconds.
these programs are all analyzed with a large java library open jdk .
.0 24. scaler uses a default tstvalue of 30m million which exhibits uniform scalability on all machines in our experimental setup.
to answer rq1 rq3 section .
.
we run the experiments on our default machine with a xeon e5 2697a .6ghz cpu and 48gb of ram with the default tst 30m .
to answer rq4 different tst values are considered and the experiments are also run on machines with smaller and larger memory sizes section .
.
.
scalability and precision of scaler guided pointer analysis in this section we examine the scalability and precision of scaler guided pointer analysis scaler for short by comparing it with conventional context sensitive pointer analyses for java objectsensitivity and type sensitivity which are the mainstream variants adopted in recent analysis clients.
for example objectsensitivity is widely adopted in recent android static analysis frameworks and type sensitivity is used in recent reflection analysis and security analysis tools.
table shows the results for all analyses.
each program has five rows of data respectively representing context insensitive ci conventional context sensitive scaler and two introspective introa and introb pointer analyses.
the last two analyses will be discussed in section .
.
for conventional context sensitive pointer analyses we consider 2obj 2type and 1type .
call site sensitivity and1obj are not considered as the former is not effective for java programs and the latter is usually both less efficient and less precise than 2type .
as it is virtually impossible to predict the scalability of a precise context sensitive pointer analysis in advance to get the most precise conventional analysis results we run2obj 2type and 1type in the given order from the most to the least precise one until one of them terminates within hours.
so in table the second row for each program shows the results of the most precise conventional pointer analysis that is scalable.
.
.
scalability of scaler guided pointer analysis.
in table the third column for each program demonstrates scaler s extremely good scalability.
2obj is not scalable for the first six programs and even the fast 2type also fails to scale for jython andsoot .
however scaler scales for all of them with its one shot principle.
in addition for the first seven complex and large programs scaler runs sometimes significantly faster than the most precise conventional pointer analysis that is scalable even if we add scaler s pre analysis time ci .
for example according to past literature and extensive experience jython is considered the most troublesome program in terms of scalability among the dacapo benchmarks .scalability first pointer analysis with self tuning context sensitivity esec fse november lake buena vista fl usa table efficiency and precision metrics for all programs and pointer analyses.
in all cases lower is better.
program analysistime seconds 3h 800sprecision metrics program analysistime seconds 3h 800sprecision metrics may fail casts poly calls reach methods call edges may fail casts poly calls reach methods call edges ci ci 2obj 2type 1type 3h 3h 2obj 2type 3h jython scaler eclipse scaler introa introa introb 3h introb ci ci 2obj 2type 1type 3h 3h 2obj soot scaler findbugs scaler introa introa introb introb ci ci 2obj 2type 3h 2obj pmd scaler chart scaler introa introa introb introb ci ci 2obj 2type 3h 2obj briss scaler luindex scaler introa introa introb 3h introb ci ci 2obj 2type 3h 2obj jedit scaler lusearch scaler introa introa introb introb to get the most precise results for a conventional pointer analysis seconds 3h 3h 997s are spent before one discovers that1type is scalable for jython on our machine however scaler only costs seconds for jython and with better precision in terms of all precision metrics as described in the next section.
.
.
precision of scaler guided pointer analysis.
to measure precision we use four independently useful client analyses that are often although rarely all together used as precision metrics in existing literature .
together they paint a fairly accurate picture of analysis precision as it impacts clients.
the clients are a cast resolution analysis metric the number of cast operations that may fail may fail casts a devirtualization analysis metric the number of virtual call sites that cannot be disambiguated into monomorphic calls poly calls a method reachability analysis metric the number of reachable methods reach methods and a call graph construction analysis metric the number of call graph edges call edges .
in table columns for each program list precision results.
in all cases lower is better.
we find that overwhelmingly scaler achieves comparable and typically better precision than the most precise conventional pointer analysis that is comparably scalable e.g.
1type forjython 2type forpmd and2obj forluindex .
in the case of findbugs the precision of scaler is marginally lower than that of 2obj but the running time is an order of magnitude lower.
the chart benchmark is the only one for which scaler is slightly less precise than 2obj without also being much faster yet scaler still attains most of the precision gains of 2obj relative to a context insensitive analysis.
moreover as shown in section .
the precision of scaler can improve by simply increasing the tstvalue.
answer to rq1 scaler guided pointer analysis exhibits extremely good and uniform scalability while matching or even exceeding the precision of the most precise conventional pointer analysis that is scalable.
.
comparison with introspective analyses the closest relative of our work in past literature is introspective analysis a technique that also attempts to tune context sensitivity per method based on a pre analysis.
introspective analysis uses heuristics such as total points to information that do not however have the upper bound guarantee scalability emphasis or context number estimation ability of scaler .
there are two published heuristics leading to different variants of introspective analyses introa and introb.
generally introa is faster but less precise than introb.
like scaler introspective analysis also relies on a context insensitive analysis ci as its pre analysis.
unlikescaler which decides on each method what context sensitivity it needs e.g.
2obj 2type 1type or ci introspective analysis decides on each method whether it needs contexts.
despite the difference the computation time of producing the context selection information is very similar a few seconds for each program on average .
as a result the overhead of both analyses decision making can be considered similar.
in table the last three rows for each program show the comparison results.
in most programs except soot andeclipse introaesec fse november lake buena vista fl usa yue li tian tan anders m ller and yannis smaragdakis table performance of scaler .
program jython soot pmd briss jedit eclipse findbugs chart luindex lusearch avg.
scaler time seconds .
.
.
.
.
.
.
.
.
.
.
runs faster than scaler thus also exhibiting good scalability however scaler s precision is significantly better than introa s scaler wins in precision in all the precision metrics of all the programs .
introb exhibits better precision than introa in all cases when it is scalable but it is still less precise than scaler in all the cases except call edges for soot and reach methods for chart .
regarding efficiency scaler runs faster than introb for most programs except eclipse chart and luindex in addition introb is not scalable for two programs jython andbriss .
answer to rq2 comparing with state of the art adaptive analyses introspective analyses introa and introb both scaler and introa exhibit extremely good scalability while scaler s precision is significantly better than introa s and scaler s scalability is better than introb s while being more precise in most cases.
.
scaler s effectiveness as a pre analysis this section answers rq3 by examining the overhead of scaler s adaptivity as well as the stpvalues selected by scaler for each program and the corresponding distribution of different kinds of context sensitivity based on the selected stp.
.
.
overhead of scaler .the overall overhead of the scaler framework figure consists of two components a contextinsensitive pointer analysis ci which provides points to information to scaler and scaler logic itself which performs tstbased stpselection and stp based context sensitivity.
scaler s pre analysis time ci is given in table and table shows the overhead of scaler itself.
the average overall overhead of scaler for each program is .
seconds of which ci pre analysis costs account for the vast majority .
seconds while scaler logic costs only .
seconds.
considering the significant scalability improvements achieved by scaler its overhead is negligible.
scaler spends seconds for jython which is markedly longer than for other programs .
the reason is that jython is especially complicated in the sense that many of its methods have enormous numbers of contexts thus scaler spends much time on ctxcmcomputation.
for instance scaler s ctxcmcomputation shows that methods of jython have more than contexts under 2obj .
for comparison the maximal ctxcmvalue of a single method under 2obj insoot the largest program in our evaluation is only .
in this way scaler also reveals the reason why many context sensitive pointer analyses fail to analyze jython scalably as reported in previous work .
scaler avoids the problem due to its stp based context sensitivity design section .
.
.
.
.
st values and context sensitivity distributions.
figure gives thestpvalues selected by scaler for each program according to the default tst 30m and the distribution of different kinds of context sensitivity over the methods of the programs based on the selected stp.
generally given the same tst scaler automatically selects small stpvalues for complex programs e.g.
soot jython soot pmd briss jedit eclipse findbugs chart luindex lusearch2obj 2type 1type ci stp16607 figure the stpvalue on top of each bar computed by scaler for each program and the distribution of different kinds of context sensitivities selected according to each stp.
andbriss medium stpvalues for medium complexity programs findbugs andchart and large stpvalues for simple programs luindex andlusearch .
scaler automatically selects 2obj the most precise contextsensitivity in our experiments for most methods .
per program on average .
this is the reason why scaler guided pointer analysis achieves very good precision as shown in table .
this also demonstrates that our insight of scalability critical methods section .
.
holds in practice in most cases only a small set of scalability critical methods make the whole analysis unscalable.
we next discuss two outlier cases at both ends of the spectrum since they are informative of scaler s limit behavior.
soot .scaler selects as stpvalue for soot so most of its methods are analyzed with context insensitivity ci .
the reason is thatsoot is very large.
the pre analysis of scaler reports that the total size of points to information of soot i.e.
the sum of pts m of all methods is which already exceeds our default tst 30m .
as a result scaler automatically selects 2obj for only methods in package java.util.
as explained in section and ci for all other methods according to the definition of selectctx in section .
.
to ensure the scalability of pointer analysis.
luindex andlusearch .scaler selects two very large stpvalues for luindex andlusearch the two simplest programs in our evaluation .
accordingly it assigns 2obj for all methods in the two programs which are all classified as non scalability critical methods under 2obj .
there is only one exception the method java.lang.object void init .
in java s type hierarchy object is the supertype of all classes thus its init method will be called whenever a constructor is called yielding too many contexts ctxcm and a this variable that abstractly points to all the objects created in the program which makes the method s pts m value very large.
as a result the ctxcm pts mvalue for this init method exceeds the selected stpvalue so scaler selects less precise contextscalability first pointer analysis with self tuning context sensitivity esec fse november lake buena vista fl usa 20m 30m 60m 80m 150m may fail casts 12gb 48gb 368gbtime timeoutprecision timeout a eclipse 20m 30m 60m 80m 150m may fail casts 12gb 48gb 368gbtime timeoutprecision timeout b pmd figure scaler guided pointer analysis time efficiency and the corresponding number of may fail casts precision with different tst values 20m 30m 60m 80m and 150m on the machines under different memory sizes 12gb 48gb and 368gb .
sensitivity for it.
since java.lang.object s constructor init is an empty method analyzing it with or without 2obj does not affect the precision of pointer analysis.
thus scaler guided pointer analysis achieves exactly the same precision as the conventional 2obj analysis for luindex andlusearch see table .
this again demonstrates the self tuning ability of scaler which can automatically obtain maximal precision for simple programs.
answer to rq3 scaler automatically selects appropriate stp values and context sensitivity to ensure the scalability of pointer analysis for programs of varying complexity.
in addition the cost of such good adaptivity is very low.
.
scaler guided pointer analysis with different tsts and memory sizes as real world analysis settings may differ widely we conduct experiments by running scaler with different tstand memory sizes to further evaluate the adaptiveness and usability of scaler in practice.
we run scaler for the top six costliest to analyze programs in table jython soot pmd briss jedit and eclipse with tstvalues 20m 30m 60m 80m and 150m on three machines with different memory sizes 12gb 48gb and 368gb representing typical memory sizes of a personal laptop a commodity server and a large server respectively.
conventional pointer analyses that are not scalable for these six programs in table with memory size 48gb are also all unscalable on another machine with a much larger memory size 368gb.
however under scaler s default tstof 30m as used in previous experiments scaler guided pointer analysis still scales for all six programs even on a machine with only 12gb memory.
this result further demonstrates the effectiveness of scaler in making pointer analysis scalable in practice.
since the six programs under different tstand memory settings exhibit similar trends for space reasons we only show two representative programs eclipse andpmd in figure .
we illustrate precision changes via the may fail casts metric which is arguably the most common precision metric in java pointer analysis literature .
scaler guided pointer analysis under different tsts.
figure encodes a lot of information it shows how both precision andscalability vary as a function of tstvalues for three different ram configurations.
as the figure demonstrates there is nothing special about scaler s default 30m tst the system adapts appropriately for both larger and smaller values.
users can simply increase or decrease the tstvalue to achieve better precision up to the level the analysis setup can support or better efficiency respectively.
this simple design with tstbeing the only tuning knob is able to drive complicated pointer analyses to adapt to different programs to achieve their preferred scalability and precision.
scaler guided pointer analysis with different memory sizes.
as shown in figure with the largest tstvalue of 150m scaler guided pointer analysis is not scalable for pmdwith 12gb or 48gb memory but it is scalable if using 368gb memory.
with the same tst scaler guided pointer analysis running on a machine with a larger memory is more likely to be scalable.
for example with a tst of 150m scaler guided pointer analysis is not scalable with 12gb memory for any of the programs it is scalable for programs if using 48gb memory and it is scalable for all programs with 368gb memory.
this result indirectly demonstrates that scalability and thus the choice of tst is tied to memory size.
answer to rq4 better precision or better efficiency of scaler guided pointer analysis can be achieved by simply increasing or decreasing the tstvalue.
the scalability is related to memory sizes with the same tst scaler guided pointer analysis is more likely to scale under larger memory thus a small large tstis recommended for a small large memory size for good scalability precision .
related work context sensitivity despite bringing great precision benefits introduces many uncertainties to scalability which may render a pointer analysis useless in practice.
we focus on related work that leverages pre analysis to achieve good efficiency and precision trade offs for context sensitive pointer analysis.
introspective analysis attempts to achieve precision and scalability trade offs by refining a context sensitive analysis while avoiding its worst case cost.
similar to scaler it first performs a pre analysis context insensitive pointer analysis to extract necessary information to guide later pointer analysis.
unlike scaler it relies on a set of six manually selected metrics e.g.
the maximumesec fse november lake buena vista fl usa yue li tian tan anders m ller and yannis smaragdakis field points to set over all fields to define two heuristics resulting in two introspective analyses introa and introb which are compared with scaler in section .
.
benefiting from the new insight of scaler tst based self tuning context sensitivity scaler outperforms introb on both precision and scalability and achieves the same level of scalability as introa while being significantly more precise.
moreover the six different metrics in introspective analysis need appropriate values to be set in advance to produce effective analysis results scaler s insights enable its methodology to be quite simple it only needs one value tst for users to achieve better precision or better efficiency as desired resulting in better usability in practice.
finally introspective analysis is not one shot using it will always incur a cost in precision even if the program could be analyzed more precisely.
therefore its user will deploy it only after first attempting a precise analysis and failing.
hassanshahi et al.
leverage similar metrics as introspective analysis to guide selective object sensitive pointer analysis for large codebases.
however their pre analysis involves several phases that need different metrics and heuristics a context insensitive analysis is first performed to extract the program kernel where a contextinsensitive or fixed object sensitive analysis is still not sufficiently precise then a fixed heavy object sensitive pointer analysis is applied to the extracted smaller kernel to determine appropriate context depth for each selected object.
after these pre analyses the selected object sensitive information is used to guide the main pointer analysis which is demonstrated to work well for the openjdk library.
however unlike introspective analysis and scaler the overhead of the pre analysis is uncertain as it heavily relies on the complexity of the extracted kernel which further depends on various metric values selected by users.
thus it is unclear if the technique can exhibit general effectiveness for arbitrary java programs in practice.
both of the above approaches involve metrics and heuristics that are defined manually.
an alternative is to use machine learning techniques as in the two approaches we describe next.
wei and ryder present an adaptive context sensitive analysis for javascript.
they first use a machine learning algorithm to obtain the relationship between some user defined method characteristics extracted from a pre analysis and context sensitivity choice call site object or parameter sensitivity and express the results as a decision tree.
based on domain knowledge the decision tree is further manually adjusted to produce heuristics that are easy to interpret while the classifications can still maintain good accuracy.
finally based on the heuristics methods are analyzed with different context sensitivity resulting in better precision achieved than single context sensitive analysis.
jeong et al.
propose a data driven approach to guiding context sensitivity for java.
unlike scaler where various kinds of context sensitivity with different lengths are applied to different methods only a single kind of context sensitivity is applied to the program and each method is finally assigned an appropriate context length including zero i.e.
context insensitivity .
as deep contexts are finally properly applied to only a subset of the methods more efficient context sensitive analysis can be achieved with still good precision.
to select a context length for each method metrics atomic features are selected and a machine learning approach is used to learn heuristics based on these metrics.
however unlikescaler s lightweight pre analysis the learning phase is heavy and costs hours in the jeong et al.
experimental setting.
generally a machine learning approach is sensitive to the training on input programs and its learned results are usually difficult to explain for example to discern why the learning algorithm selects a given context for a method.
instead scaler is a principled rigorously modeled approach derived from simple insights thus its guided results are tractable and interpretable leading to more stable and uniform effectiveness.
unlike conventional context sensitive pointer analysis which uses consecutive context elements for each context the bean approach by tan et al.
identifies and skips the redundant context elements that are useless for improving the precision of contextsensitive analysis.
as a result the saved space allows more precisionuseful context elements to be involved to distinguish more contexts making the analysis more precise with a small efficiency overhead.
precision is the focus of bean while scaler s is scalability.
in addition as explained in section rather than identifying redundant precision useless context elements scaler leverages the oag from tan et al.
to compute the context numbers in advance.
mahjong a recent heap abstraction for pointer analysis of java is also based on a cheap pre analysis like scaler .
it enables an allocation site based pointer analysis to run significantly faster while achieving nearly the same precision for type dependent clients such as call graph construction.
differently scaler works for general pointer analysis including alias analysis i.e.
not just type dependent clients which cannot be handled by mahjong effectively.
in addition scaler is able to scale for trouble programs such as jython where even the very fast mahjong analysis fails.
conclusions good scalability is hard to achieve for precise context sensitive pointer analysis.
to tackle this problem we have introduced the scaler framework which automatically chooses a suitable contextsensitivity variant for each method in the given program based on a fast context insensitive pre analysis.
the key insight is that it is possible to efficiently identify scalability critical methods and that scalability can be predicted using the ideas of scalability thresholds andtotal scalability thresholds .
the focus of scaler is scalability but at the same time it aims to maximize precision relative to a given total scalability threshold that can be selected based on the available memory.
the experimental evaluation of scaler demonstrates that it is able to achieve extremely good scalability while producing highly precise points to results in one shot regardless of the programs being analyzed.
this may directly benefit many other program analyses and software engineering tools that require scalable and precise pointer analysis.
moreover we expect the ideas behind scaler may help other kinds of static analyses to become more scalable with good precision for real world programs.