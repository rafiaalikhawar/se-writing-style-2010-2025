supporting analysts by dynamic extraction and classification of requirements related knowledge zahra shakeri hossein abad vincenzo gervasi didar zowghi behrouz h. far department of computer science university of calgary calgary canada zshakeri ucalgary.ca department of computer science university of pisa italy gervasi di.unipi.it school of software university of technology sydney australia didar.zowghi uts.edu.au schulich school of engineering university of calgary calgary canada far ucalgary.ca abstract in many software development projects analysts are required to deal with systems requirements from unfamiliar domains.
familiarity with the domain is necessary in order to get full leverage from interaction with stakeholders and for extracting relevant information from the existing project documents.
accurate and timely extraction and classification of requirements knowledge support analysts in this challenging scenario.
our approach is to mine real time interaction records and project documents for the relevant phrasal units about the requirements related topics being discussed during elicitation.
we propose to use both generative and discriminating methods.
to extract the relevant terms we leverage the flexibility and power of weighted finite state transducers wfsts in dynamic modelling of natural language processing tasks.
we used an extended version of support vector machines svms with variable sized feature vectors to efficiently and dynamically extract and classify requirements related knowledge from the existing documents.
to evaluate the performance of our approach intuitively and quantitatively we used edit distance and precision recall metrics.
we show in three case studies that the snippets extracted by our method are intuitively relevant and reasonably accurate.
furthermore we found that statistical and linguistic parameters such as smoothing methods and words contiguity and order features can impact the performance of both extraction and classification tasks.
index t erms requirements elicitation natural language processing requirements classification weighted finite state transducers dynamic language models i. i ntroduction in industrial software development it is not uncommon for an analyst to be assigned to work on the requirements for a project whose domain is not totally familiar to them be it because the analyst does not have specific training or previous experience with the project domain or because the analysts are assigned to an ongoing project.
the analyst is faced with the daunting task of becoming familiar with a possibly large amount of documentation that has already been produced.
in each case efficiently interacting with stakeholders while lacking familiarity with the domain or with pre existing project artifacts might pose major challenges.
in this paper we present a suite of innovative automated techniques aimed at helping the analyst in this scenario.
in particular we envision a system where real time interaction between an analyst and one or more stakeholders is processed in real time we include spoken interaction via a third party speech transcription utility and written in context interaction e.g.
in chat or quick turn emails .
the system is able to identifya sliding window of active topics in the conversation and at any stage recall from a repository of pre existing documents information that is relevant for eliciting requirements on the topic at hand.
in our scenario the analyst is assisted in realtime with extracted snippets of the existing documents which provide context and additional information on the topics under discussion.
such documents might include domain description documents existing requirements pending feature requests and so on in a word the entire menagerie of requirements engineering re artifacts as long as they are expressed in natural language nl .
we propose to use both generative models based on weighted finite state transducers wfsts and statistical language models lms and discriminative models based on kernel methods and support vector machines svms to extract and classify by f nf and by type of nfrs requirements relevant knowledge from the existing documents.
in contrast to other approaches we integrate both techniques so that information to present to the analyst is selected based on both its content i.e.
the text detailing the sub system that is being discussed and on its role i.e.
the text dealing with the robustness requirements on that sub system .
this integration allows us to improve at times significantly over previous approaches.
the main contributions of this paper are we propose a simple novel and dynamic generative model based on the concept of lexical association which is a quantitative measure of the strength of contextual association between two or more words in a corpus.
this model leverages the flexibility and efficiency of wfsts for dynamic modelling and analysis of the incoming text.
by modelling requirements related documents with wfsts we propose to use an extended version of the svm classification approach enriched with non contiguous string kernel in the context of requirements classification.
we evaluate our solution to the problem of dynamic extraction and classification of requirements related knowledge by means of three experiments on real world datasets and compare the results of our classification approach with others published in the literature.
the theoretical basis along with the experimental results demonstrate that the proposed solution advances the state of the art in requirements related term extraction and classification.
moreover this is the first work that aims to assist analysts by surfacing relevant information ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
from documental sources during an interactive interview.
in the remainder of this paper we first provide some background on the current state of the art and in section iii formalize the problem we are addressing and establish needed notations and concepts.
then we present our technical contribution in section iv this is followed in section v by an experimental evaluation and comparison.
we conclude the paper by discussing threats to validity with possible extensions and suggestions for future work in section vii.
ii.
s tate of the art to better set the stage for our contribution in this section we review the salient aspects of existing work on extraction and classification of requirements relevant terms.
term extraction in an early work goldin and berry proposed abstfinder a character based approach to finding non contiguous common substrings in a corpus independently of the order of these substrings.
abstfinder neatly addresses the non contingency of content carrying terms by using circular shifts however it is highly sensitive to the position of terms in a sentence and does not leverage the contextual information implicitly contained in a corpus.
since this work there has been much active research into corpus based language learning and analysis in re.
in particular sawyer et al.
applied a berry rogghes z score to identify frequent n grams within a document.
however important and indicative n grams might not always occur frequently in a document and consequently missed by z scores and similar statistical methods.
gacitua et al.
proposed a relevance driven abstraction identification rai technique which treats documents as a stream of words and applies the corpora based frequency profiling approach to rank a domain document words.
to identify and rank multi word terms they used syntactic patterns e.g.
adjective nouns adverb verbs and prepositions and a heuristic weighting approach in which component words in a multiword term are weighted in descending order from the last word in the term.
likewise quirchmayr et al.
used lexical and syntactical i.e.
part of speech tagging characteristics to semi automatically extract feature relevant fr information from natural language user manuals.
following this technique analysts first conduct a manual revision to ensure syntactical correctness and then define domain specific terms which will be used to automatically extract fr information i.e.
a clause compromises at most one subject and one predicate .
lian et al.
proposed an approach to explore and highlight requirements knowledge from domain documents.
they used a pre defined set of search terms and measured their density and diversity in fixed length sequences of words a.k.a windows .
requirements classification there is a growing body of research investigating the effect of using automatic methods for requirements classification .
an efficient categorization of requirements supports analysts to filter relevant information about their ongoing task and enables focused communication and prioritization of requirements .
verma and kass proposed a requirements analysis tool rat to automatically analyze requirements documents.they used a deterministic finite automata based approach to parse and provided a set of user defined glossaries and controlled syntaxes to formulate business rules to standard i.e.
angbracketleftagent angbracketright angbracketleftmodal verb angbracketright angbracketleftaction angbracketright angbracketleftrest angbracketright and conditional i.e.
angbracketleftif angbracketright angbracketleftcond angbracketright angbracketleftthen angbracketright angbracketleftrest angbracketright requirements.
they leveraged a predefined nfr classification and the web ontology language owl to model and query requirement s relationships and semantics.
however phrasal and semantic analysis steps of this approach need a substantial manual preprocessing effort e.g.
writing sentences in a formalized fashion which makes them highly dependent on the syntactical form of the requirements.
cleland huang et al.
proposed an algorithm that uses the term and inverse document frequencies to measure the weight score w of indicator terms for each nfr qin the training set.
these terms will be used in an indication function f w which defines a classification threshold and represents the likelihood that the new requirement belongs to a certain nfr type.
in this method requirements must be processed and reduced to a set of keywords.
also contextdependent terms such as products and clients names might appear in the list of indicator terms which negatively impact the performance of the classifier in highly unbalanced datasets.
in the same vein rahimi et al.
augmented term weight for indicator terms and phrases found in requirements specification to evaluate the mapping between requirements and their associated goals.
also the output of the indication function in this approach i.e.
the probability score depends on the lexical content of requirements documents as the authors assume that these documents are more likely to contain only indicator terms.
likewise casamayor et al.
used the term frequency inverse document frequency tf idf weighting function to transform requirements specification documents into feature vectors.
they used the expectation maximization em strategy with the na ve bayes algorithm to propose a semi supervised approach for automatic identification and classification of nfrs aiming at reducing the size of the training dataset.
the results of the empirical evaluations on the same dataset used by cleland huang et al.
showed that the approach requires less human effort in labeling requirements and it outperformed the k nn and em na ve bayes classification methods.
na ve bayes and svms classifiers are used in several studies for automatic classification of requirements.
as an example slankas and williams proposed a toolbased approach to extract and classify nfrs in requirements specification documents presented in natural language.
they used words vectors to parse requirements strings and classify them into categories of nfrs.
among k nn sequential minimum optimizer smo and na ve bayes classifying techniques used in this work smo a variation of the svm approach with heuristic training had the highest effectiveness.
abad et al.
used the stanford nlp and proposed a set of context based regular expressions to preprocess requirements documents and to increase the performance of various classification approaches e.g.
lda btm and na ve bayes .
after applying their proposed contextual authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
rules they showed that binarized na ve bayes bnb has the highest performance for classifying nfrs compared to other methods used in this study.
iii.
p roblem formalization and preliminaries we first define a formalization of the problem we address in this paper.
we will then define basic notations of wfsts and will provide the technical description of wfst operations that will be used during extraction and classification processes.
a. defining the problem given a document repository d d1 ... d n with each direpresenting a document and a source stream a real time transcription of an ongoing interview s angbracketlefts1 s2 ...sm angbracketright with each si ai bi representing an exchange where without loss of generality aiindicates text from the analyst biindicates text from the stakeholder the problem we address in this paper is to select inside each dithose textual snippets that are most relevant for the most recent part of the conversation happening in s where relevant is understood in intuitive terms that is according to a not formally defined utility function linked to the effectiveness of the elicitation process.
selection of requirements relevant snippets r r1 r2 ...rv will constitute selecting textual spans which are most likely to be relevant given surrounding context.
the context of an occurrence is defined by substrings or subsequences non contiguous terms surrounding it.
each relevant snippet rjcontains a set of requirements relevant terms t1 t2 ... tk t .
also from a predefined list of classes c c1 c2 ... c p we assign a label class to each selected snippet.
given d andsas inputs the output of our technique will be a tuple angbracketleftr c t1 t2 ... tz angbracketright wherezis defined by analyst and represents the maximum number of relevant terms that should be highlighted in each extracted snippet.
b. preliminaries we briefly describe some of the main theoretical and algorithmic aspects of wfst machines.
a weighted transducers and automata a finite state transducer fst is a finite automaton in which an acceptable path through the initial state to a final state provides a mapping from an input sequence to an output string .
similarly a weighted transducer is an fst that in addition to the input and output strings incorporates a weight into each transition.
this weight may present probability priority or any other quantities assigned to alternative and uncertain transitions.
figure gives a simple familiar example of a wfst to model sample requirement the disputes system shall support concurrent users .
the bold circle represents the initial state and double circle final state.
the input and output labels xand y and weight wof a transition are presented on transition arcs byx y w. we represent the weight associated with a pair of input and output strings x y modeled by transducer tby llbrackett rrbracket x y e.g.
in figure llbrackett rrbracket large .
.
given the alphabet we refer to w as the length of a string w and to as the empty string i.e.
.
fig.
.
an example of using wfst to model natural language text.
this wfst replaces english and contextual stop words with empty string .i t also replaces numbers with a contextual term contextual replacements are out of the scope of this paper .
if we define the weight as the cost of each transition the lower the weight the higher the probability of the term in a context.
in this example relevant terms large and concurrent are receiving lower weights.
q1 q2x y w1 a t1q prime q prime 2y z w2 b t2q1 q prime q2 q prime 2x z w1 w2 c t1 t2 fig.
.
the composition operation for detecting substrings subsequences of interest with transition rule q1 x y w q2 q prime y z w q prime q1 q prime x z w w2 q2 q prime b composition of wfsts wfsts can be composed by a general operation for tying two or more wfsts together to create a pipeline which can be used to represent statistical models of both generative and discriminative models e.g.
lm and svms .
as illustrated in figure a c given two wfsts t1andt2such that the output alphabet of t1coincides with the input alphabet of t2 composition feeds the output of t1 into the input of t2.
the composition t1 t2is identified by summing the weights of x z paths where the weight of all these paths identified by multiplying the weights of x y and y z paths.
substring ypresents the substring of interest appearing in a weighted automata .
c kernel methods and rational kernels figure 3a illustrates a very simple example of distinguishing two different classes i.e.
grey and red circles .
one can simply choose a hyperplane to separate the two populations correctly.
among the infinitely many choices available for this hyperplane the svm approach determines the hyperplane that maximizes the margin which represents the distance between each population and the hyperplane .
however in practice the linear separation of the training data is often not possible figure b .
one way to address this problem is to use a nonlinear mapping x ywhich transforms the problem space x to a higher dimensional space ywith each of the dimensions being combinations of the original features figure c .
even though the space transformation may solve the problem of nonlinearly separable data taking a large number of dot products in a very high dimensional space to define the hyperplane may be very costly.
kernel trick is a solution to this problem which defines a kernel as a similarity function measure .
there are various types of kernels for text classification such as word n gram mismatch spectrum and string kernels which use counts of the common occurrence of subsequences.
kernel kisrational when there exists a weighted transducer tsuch that k x y llbrackett rrbracket x y for all sequences i.e requirements expression xandy where llbrackett rrbracket x y denotes the weight associated to a pair of x y .
this implies that the objects that are aimed to be classified are weighted automata.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a b c fig.
.
the application of kernel trick to distinguish non linear separable data points.
a linear separable b non linear separable c mapped space iv .
p roposed extraction and classification technique in this section we start by discussing the intuition behind using lexical association and the rationale for its application in extracting relevant terms in requirements related documents.
then we provide the technical description of our proposed approach for dynamic control of context dependency as well as the dynamic calculation of lexical association in existing documents.
we conclude this section by describing the process of classifying snippets containing the extracted relevant terms.
more precisely the proposed extraction and classification process is depicted in figure .
more application minded readers can consult section iv d first to get a non technical summary of the extraction scenario and the state of the field and then read sections iv b to obtain more technical details.
a. rationale recall from section ii most existing work on the extraction of relevant terms from nl text in the context of re use basic document features such as terms frequency and length document length and the existence of a term in a repository.
using these features relevant terms stay independent of other content carrying terms in the document which contributes to overlooking the context surrounding terms when measuring their relevance.
this is a weakness shared by all bag of words approaches.
to address this problem in this paper we use lexical association between documents terms which quantitatively determine the strength of association between two or more words or terms based on their co occurrence in a corpus and will assign different weights to terms depending on the context they occur in.
the intuition behind using lexical association is the basic assumption that a context in which a word is used can often influence its meaning .
thus in a document the words that are highly associated with each other and occur together more often than expected by chance have a special function and can be considered as relevant content carrying terms.
in contrary the irrelevant background terms will have a very low association with the other terms in a corpus .
b. extraction to calculate lexical association i.e.
co occurrence knowledge we use statistical language models lms which assign probabilities to sequences of words based on their prior history.
using the chain rule of probability we can decompose the probability of any sequence wn w1w2...wn to p w1w2...wn n productdisplay i 1p wi wi weighted rewriterules .fst .. d .fst .. d .fst .. d .fst .. .. .. d n .fst .. d n.fst fst archiverequirementsrelevanent terms snippets compiling extraction2 classificationextraction labeling input contextual dataset domain repository d symbols table alphabet the most recent window of s ih d i learning.
.
.
.
1s s s s s 1s s language model krafted gappy n gram kernel most recent window of s s i translating dand the most recent window of sto wfstslearning the language model with various configs using perplexity metric n gram order discounting methodrequirements extraction a single source sourceshortest path algorithm n shortest paths requirements classification labeling svms rational kernels labeled training set optimized wfstslabeled snippets fig.
.
the architecture of our proposed extraction classification process.
wherep w h assigns a probability to term hw considering some history h and word w .
a straightforward maximum likelihood estimate of p w h is given by relative frequency count hw count h .
since the parameter space of p w1w2...wn is too large i.e.
size of the language vocabulary and there might be some new sentences or contexts that have never occurred before we use n gram language models which approximate hby just the last few words.
an n gram model is a sequence of nwords that approximates the probability of each word only to the last n 1words software requirements and requirements engineering in software requirements engineering are bigrams gram and the whole expression represents a trigram gram .
while the intuition behind ngram language models helps to manage the complexity of the probability function these models are highly dependent on the corpus we use as the document repository i.e.
d and it underestimates the probability of all possible terms that might occur.
given the fact that the contextual data available for requirements specifications are often not big enough to give us good estimates for the probability of all possible word co occurrences and considering the creativity of language there is always the possibility that stakeholders and clients use requirements relevant terms that have not occurred in the training set i.e.
d but do occur in the test set i.e.
s .
for instance the following repository d d the admin should be able to ensure the confidentiality of information and sensitive data of users includes requirements relevant bigrams it ensure confidentiality confidentiality information information sensitive sensitive data data users .
however given sj s with aj do admins protect information confidentiality?
bj yes all private information are confidential.
the probability in dof both terms information confidential and private information from sjare zero and so is the probability of the entire test set.
this is because terms in aj contain an unseen context despite the words appearing in the vocabulary set of d and terms in bjcontain unseen words i.e.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
out of v ocabulary oov .
we address these two scenarios as follows unseen words to increase the generalizability of our proposed approach we exploit the flexibility of wfsts by dynamically adjusting the symbol table to map all words in the source stream not found in the domain corpus to an oov symbol unk.
unseen context to consider all possible variations of a context and to keep our language model from assigning zero to unseen contexts n grams we use n gram hierarchy1.
in a nutshell this approach takes the view that sometimes using less context is a good thing and helps generalizing the context of the n gram model.
in other words to calculate p wn wn 2wn of trigram wn 2wn 1wnwe can instead estimate this probability by using less contexts i.e bigram probability p wn wn or unigram probability p wn .
to implement the n gram hierarchy approach we use and evaluate the following techniques backoff following this technique if the required n gram has zero instances we approximate its probability by backing off to n gram.
we iteratively back off to a lower order n gram until we find a term with a non zero count.
for instance to compute the probability of trigram wn 2wn 1wn i.e.wn n w eh a v e p wn wn n p wn wn n ifc wn n p wn wn ifc wn n c wn n p wn otherwise in this paper we apply and evaluate two backoff methods katz and witten bell section v c3 .
interpolation no matter what the frequency of different ordern grams is by applying this approach we mix the probability of all the n gram sequences.
in other words we reduce the probability mass from some more frequent terms and give it to the contexts that have never occurred in the dataset .
following presents the general idea of the linear interpolation combining uni bi trigrams each weighted by where summationtext i i .
p wn wn n 1p wn wn n 2p wn wn 3p wn going back to our example using either of back off or interpolation techniques our n gram model assigns non zero probabilities to bigrams private information and information confidential .
absolute discounting subtracts a constant discount dfrom each count and re distributes the probability mass.
kneser ney discounting augments absolute discounting by considering the number of contexts each word has happened in the domain dataset.
by using this approach we hypothesise that words that happened in more contexts in the past are most likely to appear in some new contexts.
for instance to calculate the probability of term 1this approach is also called as smoothing or discounting in literature .
.
.
.
1s s s s s 1s grams a n gram kernel.
.
.
.
1s s s s s 1s s b gappy n gram kernel fig.
.
weighted transducers computing the count of expecting a all bigrams c all gappy bigrams with a fixed penalty factor and maximum gap .
s represents each symbol word d. information confidential considering the following domain corpus our model assigns a higher probability to unigram confidential than admin .
while admin is more frequent compared to confidential it is mainly frequent in the term system admin while confidential has appeared in two different contexts.
d1 the system administrator should be able to configure the confidentiality of sensitive information.
d2 all sensitive confidential data will be secured on the server and only accessible by authorized system administrators.
d3 only system administrators can activate the account.
to identify which language model parameters assign the highest probability to the source stream data i.e.
more accurately predicts the incoming source stream we used the perplexity score .
by lm parameters we mean the n gram order i.e.
n and the discounting method i.e.
katz witten bell absolute and kneser ney .
the perplexity of a lm on a test set2is defined as the inverse n gram probability of the source stream data normalized by the number of words.
the lower the perplexity the higher the ability of the language model in predicting the incoming text.
more details about the dynamic application of this parameter in our proposed approach will be explained in section v. the model outlined above provides a context sensitive weight to each of the lexical components of the document.
the next step is applying these weighted components to the source stream document to explore the relevant terms in this document.
given source stream s a weighted transducer sis built using the same symbol table we used for generating transducerdfromd.
to efficiently capture these requirementsrelevant i.e.
domain specific terms in real time we rank the extracted terms with more probable relevant phrases first.
to do this we use a single source shortest path algorithm applied to the wfst resulted from composing sandd.i n a simple word we first build a language model for d then recompute a language model for the most recent window ofson every addition to s. the intersection3between the two language models returns relevant terms terms that are 2the test set in this context is the transcribed interview or a new specification text.
3note that intersection and composition are two different operations in automata theory and they return different results.
we used this term to imply the intuition behind our algorithm.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
in the intersection .
finally we fetch parts of dithat contain the topnrelevant terms t1 t2 ...tn shortest path n wherendefines the number of top relevant terms i.e.
nshortest paths of the composed wfst .
we illustrate this process with a simple example later in section iv d. c. classification labeling as pointed earlier previous efforts to classify requirements use methods that only work on fixed size feature vectors which are difficult to use in dynamic settings as well as in large scale datasets.
consequently much effort in the area has focused on feature engineering to produce a crafted list of features .
even more to the point non contiguous language models i.e gappyn grams which positively impact the performance of text classifiers have not yet been applied in the context of requirements classification.
in this paper we propose to apply the rational kernels method for use with svms for classifying requirements.
this family of kernels is easy to design and implement and lead to substantial improvements in the classification accuracy .
by using a gappy rational kernel i.e.
a wfst with transitions requirements specifications that diverge through unrelated word sequences e.g.
products clients or end users special names are still likely to contain meaningful subsequences that almost or completely match.
also adding weights to specific sequences of words is one way of incorporating prior knowledge into the classification algorithm.
our approach is very simple.
after extracting text snippets containing requirements relevant terms that have been identified in the extraction phase the rational kernel a.k.a similarity function can be computed efficiently using a general composition operation which is followed by calculating a single source shortest distance from state to find the sum of the weights of all successful paths of the composed transducer.
we define two requirements are similar if they share many common substrings or subsequences non contiguous gappy n grams .
the similarity measure of two transducer e i.e.
extracted from the source stream document and t i.e.
transducer associated with the training set4 can be computed as k e t summationdisplay x y llbrackete rrbracket x .
llbracketx rrbracket x y .
llbrackett rrbracket y summationdisplay x y llbrackete x t rrbracket x y where llbrackett rrbracket x y denotes the similarity measure between two strings xandy.
this measure can be defined as the sum of the expected counts in eandtof the matching substrings.
figure 5a represents an example of transducer xthat is designed to compute the expected counts of all bigrams in extracted transducer e. here is how transducer xdoes so state reads a prefix of the bigram xand outputs the empty string i.e.
ignoring unmatched prefixes .
then an 4the training set for the classification task includes a set of pre existing labeled functional and non functional requirement and it is different from the training set of the extraction taskquestion imagine the ideal situation can you describe the support process as it should be ?
either it be web based or lotus notes it must be easy to get to.
users would then type in their problem and it would go out and search for that issue and potentially give them a solution.
another great solution would be to eliminate the holding queue whereas when the ticket selects a category from the drop down list it would get assigned to a particular individual.
as of right now a ticket can be within the holding queue for several hours until it gets assigned to a particular individual.
a recent window of s s tickets can have an owner the user responsible for working on the ticket or for coordinating the work.
to assign a ticket to someone go to the people form from the ticket display page and select the user from the owner drop down list.
this list contains the usernames of all the users allowed to own tickets in the ticket s current queue.
you can assign only tickets that you own or that are unowned.
if you need to reassign a ticket that you do not own you can steal the ticket and then assign it to someone else.
b extracted snippet from d fig.
.
extracted requirements relevant terms from a transcribed interview question si s using rt essentials as d. occurrence of xis read and output identically.
finally state reads the rest of the sentence and outputs .
the number of ways that we can start from state and follow this procedure to reach state i.e.
the accepting state gives the counts of the specific bigram x. figure 5b represents a gappy bigram kernel which allows for a gap between the occurrences of two symbols while keeping the order of terms .
in this case we say two requirements descriptions are similar if they share such subsequences i.e.
gappy bigrams .
in some contexts ignoring the gap between content carrying terms might impact the context of their application.
for example in s the confidentiality status does not affect the information access is dealing with information access rather than the confidentiality type of the information .
thus to penalize the gap distance between two words of the gappy bigram we use a fixed penalty factor .
the self loop on state represents the way we consider gaps between bigrams or n grams in general .
in section v we investigate how this technique can impact the performance of our labeling classification task.
finally we describe the whole process of proposed extraction classification approach that we name relevance extraction and requirements classification rerc in algorithm .
d. example in this section the extraction method described above is applied to a real world dataset containing a requirements elicitation interview for a help desk ticketing system .
as stakeholders might use contextual technical terms when describing a system s features and requirements we used a book as domain repository d which contains both technical and general information about the ticketing system.
this book is large enough pages to be used as a substitution for the volume of text that an analyst might need to review to understand the domain material.
we are looking for the lexical association between terms in the two datasets d the book and si the most recent exchange of s figure 6a in practice a larger window would be used .
following the central assumption of lexical association we also assume that lexical association can be used to semantically relate authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm relevance extraction and requirements classification rerc algorithm input domain repository d d1 d2 ...d n source stream documents s angbracketlefts1 s2 ...s i angbracketright training set t for the classification task extraction st uniontextn i 1st st represents the symbol table i.e.
vocabulary of the domain repository d n the order of the n gram lm foreachn do foreachm do p perplexity n gram m s srepresents the most recent window of s end end ifn prime gram and method m primeyield the minimum perplexity lm d ngrammake n prime gramd m prime et lmd lm s et the transducer of extracted terms t shortest path et z t t1 ... t z z top ranked relevant terms ifrihas the maximum overlap with t r ri r ris a set of requirements relevant snippets from d labeling classification similarity function k foreachn j andz .
do lm k klngram order nmax gap j z generating the transducer which measures lexical association as the similarity measure n tr lm k tt tr transducer of the snippet contains relevant terms tt transducer related to the documents in training set w shortest distance n to calculate the shortest distance from initial states to final state for all common subsequences between the training and testing datasets end note w will be used in combination with svms which is out of the scope of this paper result classified snippet containing requirements relevant terms angbracketleftr c t1 t2 ... t z angbracketright terms in both document dands.
considering the innovative nature of language and diversity of terms that can be used to describe a specific feature looking for the exact literal match to measure the lexical association between the terms ofdandsis very likely to overlook relevant terms that occur in both documents.
for example after removing stop words and stemming words to their roots contiguous term search issue which appears in sand is a requirementrelevant term never occurs in d but it occurs in a context with the same meaning does a quick search for all of the new or open macintosh issues that are currently unowned .
also the language model created for the book returns zero for the probability of predicting the sentence of swhich contains the term lotus notes as this term never occurs in the book.
however this sentence contains a genuine requirementrelated information about the platform of the system which should be web based in this context e.g.
this ensures that anyone with a web browser can use the system fromd .
we handle out of vocabulary oov words problem by simply replacing all words that do not appear in dwith unk when modeling s and handle the problem of order and contiguity by using a hierarchy of n gram language models when modeling both dandsdocuments.
if either oovs or non contiguous words appear in a relevant context in s our approach would perfectly detect the same context in d. thus the extraction would be treated as finding all common terms which appear in relevant contexts and our algorithm identifies the relevance of terms by computing the intersection between the two hierarchical language models dands .
it then extracts the snippet which has the highest lexical association with the most recent window of s. for example queue appears as a requirement relevant term in both dands.
this term occurred times in different contexts in d b u t our extraction algorithm returns the snippet in which queue appears in the same context as s figure 6b .
by looking at this snippet the analyst can come up with some follow up questions about ticket assignment and its ownership methods which have not been discussed in the interview so far possibly eliciting more information.
v. e xperimental ev aluation a. method our main assumption is that lexical association a sa p proximated by our n gram hierarchy and gappy n gram kernels with decay factor is a useful indicator of semantic relatedness.
to evaluate this assumption we designed and implemented three experiments as follows experiment e as discussed in section iii a the successful extraction of snippets from domain repository d means that the selected snippets have the highest degree of overlap with the top ranked extracted relevant terms.
thus we hypothesized that lexical association can be used as a means to indicate the relevance of extracted snippets and following the central assumption of lexical association the snippet with the highest level of overlap with extracted terms is the most relevant for the purpose of understanding and analyzing the content of the recent window of the incoming text s s .
in this experiment we pose the null hypothesis h0 lexical association has no impact on the relevance of extracted snippets and use a publicly available industrial dataset to test this hypothesis.
experiment e this experiment mimics a situation in which an analyst is in a real time conversation with a client.
in this experiment the same hypothesis as in experiment was tested using a publicly available dataset containing transcribed interviews for exploring the requirements of a help desk ticketing system.
experiment e in this experiment we evaluate the effectiveness of using gappy n gram kernels as a similarity measure which is based on lexical association in requirements classification.
additionally we are interested in measuring how well our lexical association based method performs compared to the classification methods used in .
b. evaluation metrics edit distance given the identified snippets to test the null hypothesis h0 we need to compute the overlap lexical association between the reference set of relevant terms and the relevant terms included in the corresponding extracted snippets.
by a reference set we mean a sequence of relevant terms fromsrequired for understanding and analyzing its content.
our reference for a correct set of relevant terms associated with each snippet are the first author of the paper and an external research assistant who are experienced analysts and familiar with the problem.
to remove any threat of biased decisions we use the kappa statistics to measure the magnitude of agreement between them.
the calculated kappa coefficient for authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
this task is .
which shows an almost perfect agreement for this task.
the most popular metric for measuring the similarity between sequences of words and more specifically between short strings is string edit distance .
this distance is the minimum number of edit operations e.g.
substitutions insertions and deletions to transform the extracted sequence to the reference sequence.
for instance the distance between extracted term confidential information day and reference term access confidential information is two because one insertion access and one deletion day are required.
to calculate the edit distance we use the levenshtein edit distance which incidentally we implemented as a further transducer.
precision recall we evaluate the performance of our proposed requirements classification labeling technique using the standard metrics precision p recall r andfscore.
the recall is the percentage of the correct answers which are retrieved whereas precision is the percentage of the retrieved instances that are correct .
fscore considers both p andrand defines the overall effectiveness of the classification method.
formally these measures are given by p tp tf fp r tp tp fn f 2p r p r c. experimental design datasets the experiments e 3are conducted on the following datasets dataset we used a published dataset by the u.s. department of transportation u.s. dot obtained from .
in addition to contextual data d this dataset provides a repository for both high level and low level requirements governing the design of clarus a system to monitor environmental and road conditions.
the full dataset consists of high level low level requirements.
dataset this dataset was provided by thyssenkrupp presta steering group usa.
the scomponent of this dataset is made up of transcribed requirements elicitation interview questions obtained from two separate interviews of minutes each to explore the requirements of a ticketing system5.
the answer to each question can map to one or more requirements functional and non functional requirements in total .
we obtained dfrom the full source of a textbook on ticketing systems rt essentials .
rt is a high level open source ticketing system.
the book is large enough to be used as the domain document and its subject is representative of the technical domain that the analyst might need to understand.
dataset this dataset is obtained from the openscience tera promise repository6and consists of labeled natural language requirements frs and nfrs .
preprocessing and wfst transformation we utilized the tm and snowball packages in rto remove punctuations english predefined stop words and to stem words 5this dataset is publicly available by thyssenkrupp presta steering group i details of the dataset and comparison results for the classification task nfr cq cq2 te tr te tr us op pe a classification datasetsreference us op pe cleland huang et al.
.
.
.
abad et al.
.
.
.
kurtanovi and maalej .
.
.
our approach .
.
.
b comparison results e to their roots e.g.
confidentiality confidential confidenti based on the popular porter s stemmer .
learning and extraction after transforming dto a set of wfsts we applied perplexity measure to find the most probable lm for each pair of angbracketleftsi d angbracketright.
in particular we built static language models for dand indexed each model with angbracketleftmj n angbracketright wheremj katz witten bell absolute kneser ney and denotes the discounting method and n the order of each lm.
lm angbracketleftmj n angbracketright where discounting method mj and order ngenerate the minimum perplexity will be composed with the most recent window of son every addition tos.
to explore and indentify relevant terms we use the top mranked paths of the wfst generated from lm angbracketleftmj n angbracketright lms.
this process involves a straightforward implementation of a generalization of the dijkstra algorithm i.e.
the n best strings problem .
to provide this flexibility to analysts to manage the number of relevant terms as it might need to be changed based on the size of the s window we used a parametric value which can be changed during the application of the method.
classification this section illustrates and examines the application of the rational kernels describes in section iv to requirements classification.
we did a series of experiments on datasets 3using rational kernels to answer the following classification questions cqs about the performance of our algorithm in identifying various types of nfrs in particular usability operability and performance requirements7 cq1 domain knowledge does combining the recently discussed requirement with its corresponding extracted relevant snippet impact the performance of the classifier?
cq2 n gram kernel characteristics how is the performance of a classifier affected by removing the contiguity constraint the length of the n gram and adding cost decay factor to gaps in non contiguous n gram kernels?
table i a indicates the details of the datasets we used for training and testing the classification tasks.
we used the openfst library for the implementation of the rational kernels used in our classification task and the libsvm library to combine these kernels with svms.
d. results in the following the results of experiments e 3on tasks extraction and classification are discussed and interpreted.
extraction in terms of the overall performance of our proposed extraction technique of low level requirements descriptions in dataset experiment matched 7due to the very few instances of other subclasses of nfrs in this dataset and to be robust to within class imbalance problem in this experiment we focused only on usability performance and operability nfrs.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
irrel relword error rate wer a e101020304050 irrel relword error rate wer b e2 fig.
.
confidence interval of sample means for wer and relevant and irrelevant snippets a e 1on high level requirements and b e 2on transcribed interview answers.
descriptions to a relevant snippet i.e.
the corresponding highlevel requirement .
similarly of transcribed interview questions in dataset experiment assigned questions to a relevant extracted snippet from d i.e.
rt essentials .
with regard to the accuracy of extracted relevant terms i.e.
lexical association in extracted snippets the mean word error rate for e 1is .
standard deviation .
ci mean .
and for experiment e 2is .
.
ci mean .
.
the higher wer for e 2is probably due to datasetdwe used to generate the training lm.
interestingly however while this dataset is taken from a more general domain we achieved an accuracy of .
confidence interval .
for this task.
in terms of the impact of lexical association on the relevance of extracted snippets from d as we could not confirm the normality of the distribution of our edit distance data using q q plots we used the non parametric kruskal wallis test to examine the null hypothesis posed in section v a. with p values at significance or greater .
w ec a n reject the null hypothesis that the lexical association makes no significant difference on the relevance of extracted snippets.
the results of our statistical tests reject this hypothesis for experiment at p value .2e and for experiment atp value .
.
moreover by looking at figure a b we can see that the confidence interval of sample means for wer of extracted relevant snippets is consistently lower than irrelevant snippets which implies that lexical association can be used as an indicator of relevant snippets.
finding lexical association makes a significant impact on the relevance of extracted requirements relevant snippets and can be used as an indicator of relevance in these snippets.
classification to answer cq1 we used the results of experiment and applied a simple n gram kernel used with svms to both low and high level requirements.
figure ac shows the results of using low level requirements alone compared to using low level and high level in combination as the input of the classifier for three requirements types usability performance and operability .
these results show that the classification performance of applying n gram svms to lowlevel requirements combined with their corresponding highlevel description is consistently better than that applied to only low level requirements.
from this we conjecture that using high level requirements descriptions for the classification task .
.
.
.
.
n gram orderf score low level low high level a usability .
.
.
n gram orderlow level low high level b performance .
.
.
n gram orderlow level low high level c operability fig.
.
cq1 the impact of using extracted requirements relevant knowledge on the performance of the classifier.
may provide more contextual information for the similarity measure function which positively impacts its performance.
finding applying the classification technique to the highlevel instead of the low level requirements one best snippet can improve the performance of the classifier .
this is inline with the literature e.g.
that show augmentation in data space can boost the performance of classifiers and reduce overfitting .
table ii presents the results of our experiments for the classification task associated with cq2.
it gives the performance of the classification approach as a function of the order length and the contiguity of the n gram kernel as well as the decay factor assigned to each gap.
the decay factor allows controlling the number of gaps that are allowed in the kernel function .
while for gaps have no impact on the sequence similarity in our kernel function for .
gaps add more cost to corresponding paths which will impact the classification decision when calculating the shortest distance algorithm .
from these results we conclude finding the performance of the classifier varies with respect to the configuration of its similarity measure kernel .
in particular we found that gram non contiguous subsequences are more effective than other configurations in capturing the similarities between strings.
regarding the number of gaps in non contiguous subsequences our results show that the f score peaks at max g for all three types of nfrs and at i.e.
.
for usability and performance categories.
further increase in the number of gaps max g can degrade the power of the similarity function kernel and substantially decreases the recall.
we speculate that this may be partly due the similar behaviour of unigrams andn grams with max g .
although the kernels we used to perform the classification tasks do not incorporate any knowledge about the dataset being used except removing english and contextual stop words it still captures contextual information.
it seems that the noncontiguity of subsequences in the similarity function can better detect the inherent polysemy characteristic of natural language.
finally table i b compares our classification results with some previous work which used the same dataset to classify nfrs.
the results show that non contiguous n gram kernel with 0outperforms the techniques used in previous work except one nfr from .
this could be due to the preprocessing step applied in containing a set of manually450 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii classification performance considering v arious n gram kernel characteristics cq2 using dataset usability operability performance max gg0 g1 g2 g0 g1 g2 g0 g1 g2 0 0.5 0 0 0.5 0 0 0 0.5 02 gramp .
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
.
f .
.
.
.
.
.
.
.
.
.
.
.
gramp .
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
.
f .
.
.
.
.
.
.
.
.
.
.
.
gramp .
.
.
.
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
.
.
.
.
.
.
f .
.
.
.
.
.
.
.
.
.
.
.
developed ad hoc transformation rules for each type of nfrs.
however our approach obtains contextual semantics based on the lexical association between terms and does not apply any manually defined contextual rules.
finding the results of our experiments indicate that the application of svms with gappyn gram kernels and with a non zero decay factor can provide an effective alternative to the existing methods used in previous works for requirements classification.
vi.
t hreats to validity internal validity.
while we used the kappa coefficient to assess the reliability of ce dataset for measuring the edit distance the estimated kappa coefficient itself could be due to chance which poses a threat to the validity of our results.
to address this threat we calculated the pvalue of the resulted coefficient.
this value .
shows that the estimated agreement is not due to chance8.
we can also discount any history or maturation threat as the technique is entirely automated and the computations do not rely on previous state.
external validity.
our experimental results only apply to the three datasets we have used.
we do not claim that those datasets are representative of all situations.
however the main contribution of this paper is the technique in itself which due to the intuitive nature of the requirements relevant concept must to a certain extent rely on experimenting on each given context before generalizing claims.
more precise and extensive case studies are needed to fully address this threat by better scoping the applicability.
construct validity.
as we only used the final outcome of the extraction approach the method we used to evaluate this approach might pose a threat to construct validity.
one may ask other parameters such as the order of n gram language models or using a different discounting method from the one used to generate the final result might impact the error rate or the edit distance differently.
however theoretical considerations and a series of studies conducted by klakow and peters show that the word error rate wer and perplexity are linearly correlated and are related by a power law.
as during the learning process we used the perplexity measure to tune the lm of each incoming dataset 8note that this pvalue only shows that the estimated kappa is not due to chance and does not test the strength of the agreement level.and considered potential factors for this task this threat is mitigated in our evaluation method.
researcher bias.
in some experiment we relied on human judgment.
one of the judges was an author of the present paper however a second judge was not involved in this research and the inter subject agreement was quite strong so we trust our results were not unduly influenced.
content validity.
the metrics section v b we used to evaluate our approach pose a potential threat to content validity.
future studies might be conducted with different metrics to ensure that all interesting dimensions have been considered.
vii.
c onclusion and future work we have presented a technique to dynamically extract requirements relevant knowledge from existing documents in order to assist analysts by surfacing relevant information from documental sources during an interactive interview.
we evaluated this technique by conducting experiments on three different datasets and used the wer parameter to measure the effectiveness of our approach.
the results of our experiments show that the proposed technique is an effective and feasible approach for extracting requirements relevant knowledge.
on the technical side we also proposed to use noncontiguous n gram kernels in the context of requirements classification and applied rational kernels combined with svms to implement this technique.
although these kernels do not incorporate any contextual information apart from the preparation process they can still capture semantic information.
the results of our experiments show that non contiguous medium length n grams with decay factor 0better capture the similarity between strings that contiguous or short bigram non contiguous n gram subsequences which improves over the previous state of the art at the nfr classification task.
our work advances current techniques by combining both generative and discriminating models to support analysts in their requirements elicitation tasks in real time.
although the paper includes a fairly varied evaluation of the application of the proposed approach there are still some factors e.g.
human interface factors that might impact the application of this technique.
thus one possible direction for improvement is developing a tool that can perform the extraction classification tasks for any industrial contexts to be able to gather users feedback on real cases.
to this end we have implemented a prototype tool that we intend to use for a field study .
the techniques we have developed in this work can also be applied to different problems.
for example a dynamically updated language model like the one we have used for finding relevant snippets could be used instead to identify unexplored areas of a domain i.e.
issues in a change requests repository which are not addressed by a requirements document or to match incoming bug reports or feature requests to commit messages in open source projects.
we intend to explore these possibilities in future work.
moreover the proposed approach can assist analysts to manage the issues of memory recall after resuming an interrupted elicitation task.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.