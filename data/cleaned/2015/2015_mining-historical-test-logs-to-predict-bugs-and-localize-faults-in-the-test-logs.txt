mining historical t est logs to predict bugs and localize faults in the t est logs anunay amar department of computer science and software engineering concordia university montreal canada an mar encs.concordia.capeter c. rigby department of computer science and software engineering concordia university montreal canada peter.rigby concordia.ca abstract software testing is an integral part of modern software development.
however test runs can produce thousands of lines of logged output that make it difficult to find the cause of a fault in the logs.
this problem is exacerbated by environmental failures that distract from product faults.
in this paper we present techniques with the goal of capturing the maximum number of product faults while flagging the minimum number of log lines for inspection.
we observe that the location of a fault in a log should be contained in the lines of a failing test log.
in contrast a passing test log should not contain the lines related to a failure.
lines that occur in both a passing and failing log introduce noise when attempting to find the fault in a failing log.
we introduce an approach where we remove the lines that occur in the passing log from the failing log.
after removing these lines we use information retrieval techniques to flag the most probable lines for investigation.
we modify tf idf to identify the most relevant log lines related to past product failures.
we then vectorize the logs and develop an exclusive version of knn to identify which logs are likely to lead to product faults and which lines are the most probable indication of the failure.
our best approach l ogfault flagger finds of the total faults and flags less than of the total failed log lines for inspection.
l ogfault flagger drastically outperforms the previous work cam .
we implemented l ogfault flagger as a tool at ericsson where it presents fault prediction summaries to base station testers.
i. i ntroduction large complex software systems have thousands of test runs each day leading to tens of thousands of test log lines .
t est cases fail primarily due to two reasons during software testing a fault in the product code or issues pertaining to the test environment .
if a test fails due to a fault in the source code then a bug report is created and developers are assigned to resolve the product fault.
however if a test fails due to a non product issue then the test is usually re executed and often the test environment is fixed.
non product test failures are a significant problem.
for example google reports that of tests that fail for the first time are non product or flaky failures .
at microsoft techniques have been developed to automatically classify and ignore false test alarms .
at huawei researchers have classified test failures into multiple categories including product vs environmental failure to facilitate fault identification .in this work we focus on the ericsson teams that are responsible for testing cellular base station software.
the software that runs on these base stations contains not only complex signalling logic with stringent real time constraints but also must be highly reliable providing safety critical services such as calling.
the test environment involves specialized test hardware and rf signalling that adds additional complexity to the test environment.
for example testers need to simulate cellular devices such as when a base station is overwhelmed by requests from cell users at a music concert.
t o identify the cause of a test failure software testers go through test execution logs and inspect the log lines.
the inspection relies on a tester s experience expertise intuition past run information and regular expressions crafted using historical execution data.
the process of inspection of the failed test execution log is tedious time consuming and makes software testing more costly .
discussions with ericsson developers revealed two challenges in the identification of faults in a failing test log the complex test environment introduces many nonproduct test failures and the logs contain an overwhelming amount of detailed information.
t o solve these problems we mine the test logs to predict which test failures will lead to product faults and which lines in those logs are most likely to reveal the cause of the fault.
t o assess the quality of our techniques we use two evaluation metrics on historical test log data the number of faults found faultsfound and the number of log lines flagged for investigation loglinesflagged .
an overview of the four techniques are described below.
.cam tf idf knn cam was implemented at huawei to categorized failing test logs and the results were presented in the technical track of icse .
t esters had manually classified a large sample of failing test logs into categories including product and environment failures.
cam runs tf idf across the logs to determine which terms had the highest importance.
they create vectors and rank the logs using cosine similarity .
an unseen test failure log is categorized e.g.
product vs environment failure by examining the categories of the k nearest neighbours knn .
ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
although cam categorizes logs it does not flag lines within a log for investigation.
the logs at ericsson contain hundreds of log lines making a simple categorization ofa log as fault or product unhelpful.
our goal is to flagthe smallest number of lines while identifying as manyfaults as possible.
when we re implement cam and run it on ericsson data only of the faults are found.
sincethe approach cannot flag specific lines within a log anylog that is categorized as having a product fault must beinvestigated in its entirety .
.
skewcam cam with eknn ericsson s test environment is highly complex with rf signals and specialized base station test hardware.
thisenvironment results in a significant proportion of environ mental test failures relative to the number of product testfailures.
due to the test environment entire teams of testersexclusively analyze log test failures each day examiningnoisy failures to ensure that all product faults are found.t o deal with this skewed data we modify the standard knearest neighbour knn classification approach to act inan exclusive manner.
with exclusive k nearest neighbour eknn instead of voting during classification if any past log among k neighbours has been associated with a productfault then the current log will be flagged as product fault.
skewcam which replaces knn with eknn fi n d s8 o f faultsfound with of the log lines being flagged for investigation.
.l ogliner line idf eknn skewcam accurately identifies logs that lead to product faults but still requires the tester to examine almost ofthe total log lines.
our goal is to flag fewer lines to provideaccurate fault localization.
the unit of analysis for skewcam is each individual term in a log.
using our abstraction and cleaning approaches weremove run specific information and ensure that each logline is unique.
we are then able to use inverse documentfrequency idf at the line level to determine which linesare rare across all failing logs and likely to provide superiorfault identification for a particular failure.
l ogliner l i n e idf eknn can identify of product faults while flagging only of the log lines.
there is a drastic reductioninloglinesflagged for inspection with a slight reduction in the number of faultsfound .
.l ogfault flagger pastfaults line idf eknn inverse document frequency idf is usually weighted by t erm frequency tf .
instead of using a generic termfrequency for weight we use the number of times a log linehas been associated with a product fault in the past.
theresult is that lines with historical faults are weighed morehighly .
l ogfault flagger identifies of faultsfound while only flagging .
of the log lines.
l ogfault flagger finds the same number of faults as skewcam but flags less than of the log lines compared to skewcam s .
this paper is structured as follows.
in section ii we provide the background on the ericsson test process andthe data that we analyze.
in section iii we detail our fig.
the ericsson integration test process.
code hasalready gone through earlier developer testing stages n and will continue to later integration stages n .
thedata we extract is shown in the square boxes e.g.
logid.
log abstraction cleaning diffwithpass and classification methodologies.
in section iv we describe our evaluationsetup.
in sections iv to viii we provide the results for ourfour log prediction and line flagging approaches.
in sectionix we compare the approaches based on the number offaultsfound and loglinesflagged for inspection discuss performance and storage requirements and describe howwe implemented l ogfault flagger as tool for ericsson testers.
in section xi we position our work in the contextof the existing literature.
in section xii we conclude thepaper and describe our research contributions.
ii.
e ricsson test process and data at ericsson there are multiple levels of testing from low level developer run unit tests to expensive simulations ofreal world scenarios on hardware.
in this paper we focuson integration tests at ericsson.
t esters are responsible forrunning and investigating integration test failures.
our goalis to help these testers quickly locate the fault in a failingtest log.
integration testing is divided into test suites that contain individual tests.
in figure we illustrate the integrationtesting process at ericsson.
there are multiple levels ofintegration testing.
the passing builds are sent to the nextlevel of integration tests.
for each integration test case testid w er e c o r dt h e testexecutionid which links to the result logid and the verdict.
the log contains the runtime information that is output by the build that is under test.for each failing test we store the log and also store theprevious passing run of the test for future comparison withthe failing log.
failing tests that are tracked to a productfault are recorded in the bug tracker with a troublereportid.
environmental and flaky tests do not get recorded in thebug tracker and involve re testing once the environment authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
has been fixed.
in this work we study a six month period with hundreds of thousands of test runs and associated test logs.
iii.
m ethodology discussions with ericsson developers revealed two challenges in the identification of faults in a failing test log the complex test environment introduces many nonproduct test failures and the logs contain an overwhelming amount of detailed information.
t o overcome these challenges we perform log abstraction to remove contextual information such as run date and other parameters.
lines that occur in both failing and passing logs are unlikely to reveal a fault so we perform a set difference between the failing log and the last passing log to remove lines that are not related to the failure i.e.
diffwithpass .
finally we extract the rarest log lines and use information retrieval techniques to identify the most likely cause of a fault.
we elaborate on each step below.
a. log abstraction logs at ericsson tend to contain a large number of lines between and with a median of lines.
the size makes it difficult for developers to locate the specific line that indicates a fault.
log abstraction reduces the number of unique lines in a log.
although the logs do not have a specific format they contain static and dynamic parts.
the dynamic run specific information such as the date and test machine can obscure higher level patterns.
by removing this information abstract lines contain the essence of each line without the noisy details.
for example in figure the log line latency at sec above normal contains static and dynamic parts.
the static parts describe the high level task i.e.
an above normal latency value.
the latency values are the dynamic parts of the log line i.e.
seconds.
in another run we may obtain a latency at sec above normal .
although both logs contain the same high level task without log abstraction these two lines will be treated as different.
with log abstraction the two log lines will record the same warning.
we build upon shang et al.
s log abstraction technique modifying it for test logs.
anonymization during this step we use heuristics to recognize the dynamic part of the log line.
we use heuristics like staticvocabulary to differentiate between the static and the dynamic part of the log line.
for example the test source code contains the log line print latency at d sec above normal latencyvalue .w e wrote a parser to find the static parts of the test code which we store as the staticvocabulary .w i t ht h eh e l po f staticvocabulary we replace the dynamic parts of a log with the placeholder.
in our example the output of log abstraction would be latency at sec above normal .
unique event generation finally we remove the abstracted log lines that occur more than once in the abstract 1ericsson requested that we not report specific test and log numberslog file.
we do this because duplicate log lines represent the same event.
b. diffwithpass the location of a fault should be contained in the lines of a failing log.
in contrast a passing log should not contain the lines related to a failure.
lines that occur in both a passing and failing log introduce noise when attempting to find the fault in a failing log.
we introduce an approach where we remove the lines that occur in the passing log from the failing log.
in our example in figure the failing log contains an above normal latency .
however the passing log also contains this warning so it is unlikely that the failure is related to latency .
in contrast the line power below watts occurs only in the failing log indicating the potential cause for the failure.
performing the diffwithpass operation with all the previous passing logs is computationally expensive and grows with the number of test runs o n .
for each failure we have to compare with the test s previous passing runs which would lead to over million comparisons across our dataset.
the number of passes makes this impractical.
t o make our approach scalable we note that a passing log represents an acceptable state for the system.
we perform a set difference of the current failing log with the last passing log.
computationally we perform one diffwithpass comparison o .
this approach reduces the number of noisy lines in a log and as we discuss later reduces the storage and computational requirements.
c. frequency of test failures and faults t ests with similar faults should produce similar log lines.
for example when a test fails due to a low power problem it produces the following abstract log line power below watts.
a future failure that produces the same abstract log line will likely have failed due to a low power problem.
unfortunately many of log lines are common and occur every time a test fails regardless of the root cause.
these noisy log lines do not help in identifying the cause of a specific test failure.
in contrast log lines that are rare and that occur when a bug report is created are likely more useful in fault localization.
our fault location technique operationalized these ideas by measuring the following linefailcount the count of the number of times a log line has been in a failing test.
linefaultcount the count of the number of times a log line has been in a log that has a reported fault in the bug tracker.
after performing log abstraction and diffwithpass w e store a hash of each failing log line in our database.
in figure we show how we increment the count when a failure occurs and a bug is reported.
we see that lines that occur in many failures have low predictive power.
for example t estcase failed at is a common log line that has occurred times out of test failures.
in contrast power below is a rare log line that occurs times out authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
processing stages first the logs are abstracted.
second a set difference operation is performed between the passing and failing log diffwithpass .
third only the lines present in the failing log are stored.
of failures likely indicating a specific fault when the test falls.
not all test failures lead to bug reports.
as we can see the generic log line t estcase failed at has only been present in failures that ultimately lead to a bug report being filed.
in contrast when the log line power below occurs testers have filed a bug report out times.
when predicting future potential faults this latter log line clearly has greater predictive power with few false positives.
we further stress that the individual log lines are not marked by developers as being related to a fault or bug report.
while this data would be desirable we have not come across it on industrial projects.
instead as can be seen in figures and the failing build and associated test failure are linked to a bug report.
after performing abstraction and diffwithpass we store and increment the failure and fault count for each line in the log for later ir processing to determine which log lines have high predictive power.
d. tf idf and line idf identifying faults based on test failures and bug reports is too simplistic.
t erm frequency by inverse document frequency tf idf is used to calculate the importance of a term to a document in a collection .
the importance of a term is measured by calculating tf idf tf idf t d ft d logn nt where ft ddenotes the number of times term toccurred in a log document d ndenotes the total number of logs for a test and ntdenotes the number of logs for a test that contains the term t .
we have discussed in earlier sections that rare log lines should be strong indicators of faults.
we use idf inverse document frequency to operationalize the importance of a log line to a test log.
line idf is defined as line idf l d logn nl where ndenotes the number of logs for a test and nl denotes the number of logs for a test that contains the log line l. e. log vectorization t o find similar log patterns that have occurred in the past we transform each log into a vector.
each failed log is represented as a vector and the log lines in our vocabulary denotes the features of these vectors.
for example if we have n failed logs in our system then we would generate n vectors a vector for every failed log.
the dimension of the vectors is determined by the number of unique log lines in our corpus.
if we have m unique log lines then the generated vectors would be m dimensional.
many techniques exist to assign values to the features.
we use three techniques.
for cam andskewcam were the features are the terms in a log we use the standard tf idf formula see equation .
for l ogliner w e r et h e feature is a line we use use line idf see equation .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
the mapping between the log line failure count and bug report count.
logs lines that have been associated with many bug reports have high predictive power.
for example power below has occurred in fails and times a bughas been reported.
for l ogfault flagger we multiply the fault frequency of a line by the line idf which is formally defined later inequation .
f .
cosine similarity t o find similar logs and log lines to predict faults we use cosine similarity .
it is defined as similarity cos vectorl vectorl2 bardbll bardbl2 bardbll bardbl2 where l1and l2represent the feature vectors of two different test logs.
we represent each past failing log and current failing log as vectors and compute the cosinesimilarity between the vector of current failing log and thevectors of all the past failing logs.
during the calculation of cosine similarity we only take top nlog lines features from the vector of current failing log.
since our prediction is based only on these lines weconsider these nlines to be flagged for further investigations.
we are able to predict not only which log will lead toproduct faults but also which log lines are the most likelyindication for the fault.
g. exclusive k nearest neighbours eknn t o determine whether the current log will lead to a bug report we modify the k nearest neighbours knn approach as follows.
for the distance function we use the cosine similarity of the top n lines as described above.
for the voting function we need to consider the skewin our dataset.
our distribution is highly skewed becauseof the significant proportion of environmental failures.
weadopt an extreme scheme whereby if any of the k nearestneighbours has lead to a bug report in the past we predictthat the current test failure will lead to a bug report.
if noneof the k neighbours has lead to a past bug report thenwe predict no fault.
this approach is consistent with ouroverriding goal of finding as many faults as possible butmay lead to additional log lines being flagged for inspection.
t o set the value of k we examine the distribution of test failures and measure the performance of different values ofk from to .
iv .
e valuation setup ericsson testers evaluate test failures on a daily basis.
as a result we run our simulation on a daily basis training onall the previous days.
this simple incremental simulationframework has been commonly been used in the researchliterature .
our simulation period runs for6 months and covers hundreds of thousands of test runsand logs.
we train and test the approaches on the nightlysoftware test runs for day d 0t o d t. t o predict whether a failure on day d twill reveal a product fault we train on the historical data from d 0t od t and test on d t. we repeat this training and testing cycle for each nightly run until we reach d t. our goal is to capture the maximum number of product faults while flagging the minimum number of log lines forinspection.
we operationalize this goal by calculating thepercentage of faultsfound and the percentage of loglines144 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
flagged .
we define faultsfound and loglinesflagged as the following faultsfound totalcorrectlypredictedfaults totaltesterreportedfaults loglinesflagged totalfailedloglinesflagged totalloglinesallfailedlogs v. r esul t .cam tf idf knn cam has successfully been used at huawei to categorize test logs .
we re implement their technique and perform a replication on ericsson test logs.
we discussed the data processing steps in section iii.
we then apply tf idf to the terms in each failing log.
cosine similarity is used to compare the current failing log with all past failing logs for a test.
cam then calculates a threshold to determine if the current failing log is similar to any of the past logs.
the details can be found in their paper and we use the same threshold value of similarity of .
.
if the value is below the threshold then knn is used for classification.
cam sets k we vary the number of neighbours from k 1t o .
t able i shows that the direct application of cam to the ericsson dataset only finds or fewer of the product faults.
we also see that increasing the value of kneighbours does not increase the number of faultsfound .
for example atk 15cam finds of the product faults.
however when we increase k to it only captures of the product faults.
cam is also computationally expensive and on average it takes hours to process the entire dataset.
there are two main factors that contribute to this computational cost.
first cam performs word based tf idf which generates large term based vectors and then calculates the cosine similarity between the vector of current failing log and the vectors of all the past failing logs.
the time complexity iso v l .
second the algorithm computes a similarity threshold using the past failing logs that increases computational time by o v l .
where vdenotes the vocabulary of terms present in the failing test logs ldenotes the total number of failing test logs and ldenotes a smaller set of failing test logs used during the calculation of similarity threshold.
cam finds of the total faults.
cam flags the entire failing log for investigation.
cam is computationally expensive.
vi.
r esul t .skewcam cam with eknn ericsson s test environment involves complex hardware simulations of cellular base stations.
as a result many test failures are environmental and do not lead to a product fault.
since the data is skewed we modify knn.
in section iii g we define exclusive knn eknn to predict atable i cam tf idf knn k faultcaught loglineflagged execution time mins .
.
.
.
.
.
.
.
.
.
table ii skewcam cam with eknn k faultcaught loglineflagged execution time mins .
.
.
.
.
.
.
.
.
.
table iii l ogliner line idf eknn k n faultcaught loglineflaggedexecution time mins .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table iv l ogfault flagger pastfaults line idf eknn k n faultcaught loglineflaggedexecution time mins .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fault if any of the k nearest neighbours has been associated with a fault in the past.
we adjust cam for skewed data.
like cam skewcam uses tf idf to vectorize each log and cosine similarity to compare the current failing log with all previously failing logs.
however we remove the threshold calculation as both the study on cam and our experiments show that it has little impact on the quality of clusters.
instead of using knn for clustering skewcam uses eknn .
we vary the number of neighbours from k to .
t able ii shows that more neighbours catch more product faults but also flag many lines.
at k skewcam catches of the all product faults but flags of the total log lines.
interestingly as we increase kto the number of faults found increases to only but the lines flagged authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
increases to .
adjusting cam for skewed data by using eknn allows skewcam to catch most product faults.
however the improvement in the number of faultsfound comes at the cost of flagging many lines for inspection.
t esters must now face the prospect of investigating entire log files.
despite removing the threshold calculation skewcam is still computationally expensive because like cam it applies term based tf idf .
hence it has a time complexity of o v l .
skewcam finds of the total faults but flags total log lines for inspection.
it is also computationally expensive.
vii.
r esul t .
l ogliner l ine idf eknn skewcam can accurately identify the logs that lead to product faults however it flags a large number of suspicious log lines that need to be examined by testers.
t o effectively identify product faults while flagging as few log lines as possible we developed a new technique called logliner .l ogliner uses the uniqueness of log lines to predict product faults.
we calculate the uniqueness of the log line by calculating the inverse document frequency idf for each log line.
before calculating idf we remove run specific information from logs by performing data processing as explained in section iii.
idf is used to generate the vectors for the current failing log and all of the past failing logs according to the equation below.
for each unique line in a log we calculate its idf score which is a reworking of equation idf line logtotalnumlogs lineinlogscnt in order to reduce the number of flagged log lines we perform our prediction using the top idf scoring nlines from the current failing log.
we then apply cosine similarity and compare with the kneighbours using eknn to predict whether the current failing test log will lead to fault.
during our experiment we varied kfrom to and nfrom to and studied the relationship between the number of neighbours k top n lines with highest idf score percentage faultsfound and percentage loglinesflagged .
t able iii shows the impact of changing these parameters.
low parameter values n 1a n d k lead to faultsfound at with of loglinesflagged .
by using the top line in a log and examining the result for the top neighbour we are able to perform at similar levels to cam .cam and skewcam use all the log lines during prediction.
with n all the lines in a log l ogliner finds of the faults but flags of the lines a similar result to skewcam not s h o w ni nafi g u r e .
setting l ogliner to more reasonable values k and n we are able to find of the faults by flagging of the log lines for inspection.
drastically increasing k and keeping n we find of the faults but flag of the lines.
logliner finds of the total faults while flagging only of the total log lines for inspection.
viii.
r esul t .
l ogfault flagger past faults line idf eknn logliner flags fewer lines but drops slightly in the number of faultsfound .
we build on l ogliner with l ogfault flagger which incorporates faults into the line level prediction.
idf is usually weighted.
instead of using a generic weight such as term frequency we use the number of times a log line has been associated with a product fault in the past.
we add to this frequency to ensure that the standard idf of the line is applied if a line has never been associated with any faults.
we weight line idf with the line fault frequency ff according to the following equation ff idf line linefaultcount idf line linefaultcount logtotalnumlogs lineinlogscnt as with the previous approaches we vary the number of neighbours from k to and the number of top lines flagged with n and .
t able iv shows that the value of nhas little impact on the number of faults found.
furthermore the number of faultsfound increases only slightly after k .
as a result we use n 1a n d k for further comparisons and find that l ogfault flagger finds of the total faults with .
of total log lines flagged for inspection.
compared to skewcam l ogfault flagger finds the same number of faults but skewcam flags of total log lines compared l ogfault flagger .
compared to l ogliner l ogfault flagger finds percentage points more faults with .
percentage points fewer lines flagged.
logfault flagger finds of the total faults and flags only .
of lines for inspection.
ix.
d iscussion t esters want to catch a maximal number of faults while investigating as few log lines as possible.
we discuss the reasons why the techniques differ in the number of correctly identified test failures that lead to faults faultsfound in figure and the number log lines used to make the prediction i.e.
the lines that are flagged for manual investigation loglinesflagged in figure .
the figures also provide a visual representation of the impact of changing the number ofk neighbours.
we also discuss the performance and storage requirements and the implementation of the best approach l ogfault flagger as a tool for ericsson testers.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
number of neighbors k percentage of total faults found number of neighbors k percentage of total faults found number of neighbors k percentage of total faults found number of neighbors k percentage of total faults found cam skewcam logliner n faultflagger n fig.
faultsfound with varying k.skewcam and l ogfault flagger find a similar number of faults.
cam finds or less of the total faults.
number of neighbors k percentage of total lines flagged number of neighbors k percentage of total lines flagged number of neighbors k percentage of total lines flagged number of neighbors k percentage of total lines flaggedcam skewcam logliner n faultflagger n fig.
loglinesflagged with varying k.skewcam flags an increasing number of lines while l ogfault flagger remains constant around of total log lines.
cam flags relatively few total log lines because it predicts fewer faults only finding of the total faults.cam technique we re implemented huawei s cam technique and evaluated it on a new dataset.
cam uses simple term based tf idf to represent failed test logs as vectors.
then it ranks the past failures with the help of their corresponding cosine similarity score.
finally it uses knn to determine whether the current test failure is due to a product fault and presents its finding to the testers.
cam has two major limitations.
first although the cam tool provides a display option to diff the failing log it uses the entire log in its prediction and so cam does not flag individual log lines that are the likely cause of the fault.
instead it only categorizes test failures into for example product vs environmental failure.
the second limitation is that cam performs poorly on the ericsson dataset see figure and .
we can see that even when we increase the number of kneighbours the number of faultsfound does not increase and stays around .
cam performs poorly because the ericsson data is highly skewed due to the significant proportion of environmental failures which reduces the effectiveness of voting in knn.
skewcam technique we modify cam for skewed datasets.
skewcam uses an exclusive eknn strategy that is designed for skewed data.
if any of the nearest k neighbours has had a fault in the past skewcam will flag the log as a product fault.
figure shows that skewcam plateaus finding of the product faults solving the first limitation of cam .skewcam s major limitation is that it flags an increasingly large number of log lines in making its fault predictions.
figure shows that as the number ofk neighbours increases so too does the number of loglinesflagged .
as a result testers must manually examine many log lines to identify the cause of the failure.
like cam skewcam uses the entire failed log in its prediction providing poor fault localization within a log.
logliner technique t o reduce the number of loglinesflagged we introduce a new technique called l ogliner .
instead of using terms as the unit of prediction l ogliner modifies tf idf by employing idf at the log line level.
the line idf score helps to identify rare log lines in the current failing log.
our conjecture is that rare lines are indicative of anomalies which in turn indicate faults.
l ogliner selects the top nmost rare log lines in the current failing log.
these nlines are vectorized and used to calculate the similarity with past failing test logs.
l ogliner plateaus at identifying of the faults while flagging of the lines.
since only n lines are used in the prediction only n lines are flagged for investigation by developers drastically reducing the manual effort in fault localization.
logfault flagger technique t o improve the number offaultsfound and reduce the number of loglinesflagged we suggest a new technique called l ogfault flagger that uses the association between log lines and linefaultcount .
logfault flagger uses l ogliner s line based idf score and linefaultcount to represent log files as vectors.
we then select the top n log lines that are both rare and associated with the most historical faults.
our experimental authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
result shows that the log line rarity and its association with fault count is a strong predictor of future product faults.
figures and show that l ogfault flagger plateaus finding of the faults while consistently flagging less than of the of the total log lines for investigation.
as the figures show in order for skewcam to find the same number of faults as l ogfault flagger it must flag an increasing and drastically larger number of lines for inspection.
logfault flagger not only outperforms the state ofthe art in terms of effectiveness it also introduces the use of log abstraction and diffwithpass to test log processing which has substantial benefits in terms of performance and storage.
a. performance and log storage the last column of t able i t able ii t able iii and t able iv show the execution time of cam skewcam l ogliner a n dl ogfault flagger respectively .
we can see that both cam andskewcam are computationally more expensive than l ogliner and l ogfault flagger .a t k cam skewcam l ogliner n and l ogfault flagger n take minutes minutes minutes and minutes respectively to analysis six months worth of log files.
cam andskewcam are slower as they both perform term based tf idf which generates large feature vectors as a result they have a time complexity of o v l where v denotes the vocabulary of terms present in the failing test logs and ldenotes the total number of failing test logs.
in contrast l ogliner and l ogfault flagger use line idf where the line is the feature unit v w h e r e v lessmuchv.
as a result logliner and l ogfault flagger have a time complexity ofo v l where vdenotes the set of unique log lines in the set of failed logs.
performing log analysis on huge log files is tedious and expensive.
cam skewcam l ogliner and l ogfault flag ger all require historical test logs for fault prediction and localization.
as a result we are required to store the test logs for a long period of time which increases the storage overhead.
t o ameliorate the storage overhead we reduce the size of the raw log files by performing log abstraction and diffwithpass .
over a one month period we calculate the amount of reduction in the overall log storage size.
we found that with log abstraction we can reduce the log storage size by .
when we employ both log abstraction and diffwithpass we were able to reduce the log storage size by .
this reduction drastically reduces the storage requirements and allows companies to store the important part of test logs for a longer time period.
b. implementing the logfault flagger tool at ericsson logfault flagger was implemented as a tool at ericsson.
t o reduce disruption and encourage adoption a field was added to the existing testing web dashboard to indicate whether the test failure is predicted to lead to a product fault or an environmental failure.
the tester can click toview the log in a diffwithpass view that shows only those lines that are in the current failing log.
while this view is still available feedback from ericsson testers indicated that they preferred to view the flagged lines in the context of the entire log.
the view of the log was modified to highlight the flagged log lines and allows testers to jump to the next flagged line.
another product team at ericsson hired one of our researchers to re implement l ogfault flagger in a new test setting.
as we discuss in the threats to validity a short tuning stage is required but the overall technique is dependent only on storing historical test logs and does not depend on a particular log format or development process.
x. t hreats to validity we report the results for a single case study involving hundreds of thousands of test executions over a six month period.
since the test failure data is highly skewed because of the significant proportion of environmental failures we use a large number of neighbours k .
it is simple to adjust the value of kbased on the number of faults that lead to bug reports for other projects.
indeed the success of l ogfault flagger has lead to its adoption on another ericsson team.
although in the early stages the initial results are promising and since there are fewer environmental failures the data is more balanced and standard knn has replaced eknn .
we have also experimented with other models including logistic regression decision trees and random forests.
although a complete discussion is out of the scope of this paper we note that decision tress and random forests perform less well than simple logistic regression and knn.
our fault identification techniques use log abstraction to pre process the log files.
during the log abstraction process we lose run time specific information from the test log.
though the run time specific information can help in the process of fault identification it adds substantial noise and increases log size.
we reduce the size of the log and increase the fault localization by performing log abstraction.
however we leave the run specific information in when the tester views the log in the l ogfault flagger tool so that they can find for example which specific node the test has failed upon.
although we can find of all faults we cannot predict all the product faults because the reason for all failures is not contained in the log i.e.
not all run information is logged.
furthermore when a test fails for the first time we cannot calculate a line idf score or calculate the cosine similarity with previously failing neighbours.
we found that predicting first time test failures as a product faults leads to many false positives at ericsson.
as a result in this work a first test failure has no neighbours and so we predict that there will be no product fault.
if a first fail test has a fault we will count it as a missed fault i.e.
we are conservative .
this parameter can easily be adjusted for other projects.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
xi.
r elated work a. fault location there is large and successful body of work on locating faults within source code.
traditional fault location techniques use program logs assertions breakpoints in debuggers and profilers .
more advanced fault identification techniques use program slicing based algorithm program spectrum based algorithm statisticsbased algorithm program state based algorithm machine learning based algorithm and search based .
in contrast our algorithms logfault flagger and l ogliner flag log lines that are likely related to faults in the test log not the source code.
t esters can use the flagged log lines to determine the reason behind the test failure.
t echniques to trace log lines back to test cases and ultimately to the part of the system under test are necessary future work so that our log line location technique can be automatically traced back to the faulty source code.
b. fault prediction predicting software faults is an active research field.
most fault prediction techniques predict whether a given software module file or commit will contain faults.
some of the most popular and recent fault prediction techniques use statistical regression models and machine learning models to predict faults in software modules .
herzig performed preliminary work combining measures such as code churn organizational structure and pre release defects with pre release test failures to predict the defects at the file and microsoft binary level.
with the exception of herzig the bug models we are aware of do not include test failure information.
in contrast our model uses not only test outcomes but also the dynamic information from the test logs to predict faults.
c. log analysis and failure clustering logs are an important part of operating a production system.
the majority of log analysis work has focused on production logs of live systems or traces of running code.
these works have used statistical learning approaches to identify sequences in logs find repeating and anomalous patterns and clustering similar logs .
we have adapted the log abstraction approaches to work on test logs .
since we have an external indicator of success i.e.
a test pass or f a i l w eu s e diffwithpass that reduces log storage size and helps testers in identifying the cause of the failure in a log.
d. categorizing test failures the testing literature is vast ranging from test selection and prioritization to mutation testing .
in this work we focus on false alarms i.e.
non product failures that are common on large complex systems .
these falsealarms have received attention because successful classification of false test alarms saves time for testing teams.
throughout the paper we have contrasted and replicated the state of art on test log classification cam .
false alarms can also slow down the development team when test failures stop the build.
for example this issue was addressed at microsoft by automatically detecting false test alarms .
microsoft uses association rules to classify test failures based on configuration information and past pass or fail results.
the classification does not consider the test logs.
in contrast we use historical test logs to find specific log lines that tend to be associated with product faults.
this allows us to not only ignore false alarms but to provide the likely log line location of the failure.
xii.
c oncluding remarks we have developed a tool and technique called l ogfault flagger that can identify of the faults while flagging less than of the total failed log lines for investigation by testers.
while developing l ogfault flagger we make three major contributions.
first using log abstraction we are able to reduce the log storage requirement by .
we also observe that the location of a fault should be contained in the lines of a failing log while the last passing log should not contain the lines related to a failure.
we perform a setdifference between the failing log and the last passing log.
diffwithpass further reduces the storage requirement to .
diffwithpass also reduces the noise present in the failed test log helping testers isolation faults in the log.
second our discussions with testers revealed that they want to find the most faults while investigating the fewest log lines possible.
we evaluate each technique on the basis offaultsfound and loglinesflagged .
previous works can only classify test failures based on logs and do not flag specific log lines as potential causes .
t esters must manually go through the entire log file to identify the log lines that are causing the test failure.
in order to predict product faults and locate suspicious log lines we introduce an approach where we train our model on a subset of log lines that occur in current failing test log.
l ogfault flag ger identifies the rarest lines that have lead to past faults i.e.
pastfaults line idf eknn .
in our ericsson tool logfault flagger highlights the flagged lines in the log for further investigation by testers.
third l ogfault flagger drastically outperforms the state of the art cam .cam finds of the total faults.
cam flags the entire failing log for investigation.
when cam is adjusted for skewed data skewcam it is able to find of the total faults as many l ogfault flagger however it flags of the log lines compared to the less than flagged by l ogfault flagger .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.