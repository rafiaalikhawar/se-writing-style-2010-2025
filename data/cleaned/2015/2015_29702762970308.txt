fine tuning spectrum based fault localisation with frequent method item sets gulsher laghari alessandro murgia serge demeyer ansymo universiteit antwerpen belgium fgulsher.laghari alessandro.murgia serge.demeyer g uantwerpen.be abstract continuous integration is a best practice adopted in modern software development teams to identify potential faults immediately upon project build.
once a fault is detected it must be repaired immediately hence continuous integration provides an ideal testbed for experimenting with the state of the art in fault localisation.
in this paper we propose a variant of what is known as spectrum based fault localisation which leverages patterns of method calls by means of frequent itemset mining.
we compare our variant we refer to it as patterned spectrum analysis against the state of the art and demonstrate on real faults drawn from ve representative open source java projects that patterned spectrum analysis is more e ective in localising the fault.
ccs concepts software and its engineering !software testing and debugging keywords automated developer tests continuous integration spectrum based fault localisation statistical debugging .
introduction continuous integration is an important and essential phase in a modern release engineering pipeline .
the quintessential principle of continuous integration declares that software engineers should frequently merge their code with the project s codebase .
this practice is helpful to ensure that the codebase remains stable and developers can continue further development essentially reducing the risk of arriving inintegration hell .
indeed during each integration step a continuous integration server builds the entire project using a fully automated process involving compilation unit tests integration tests code analysis security checks .
.
.
.
when one of these steps fails the build is said to be bro ken development can then only proceed when the fault is repaired .
the safety net on automated tests encourages software engineers to write lots of tests several reports indicate that there is more test code than application code .
moreover executing all these tests easily takes several hours .
hence it should come as no surprise that developers defer the full test to the continuous integration server instead of running them in the ide before launching the build .
occasionally changes in the code introduce regression faults causing some of the previously passing test cases to fail .
repairing a regression fault seems easy the most recent commits should contain the root cause.
in reality it is seldom that easy .
there is always the possibility of lurking faults i.e.
faults in a piece of code which are revealed via changes in other parts of the code .
for truly complex systems with multiple branches and staged testing faults will reveal themselves later in the life cycle .
luckily the state of the art in software testing research provides a potential answer via spectrum based fault localisation .
these heuristics compare execution traces of failing and passing test runs to produce a ranked list of program elements likely to be at fault.
in this paper we present a variant which leverages patterns of method calls by means of frequent itemset mining.
as such the heuristic is optimised for localising faults revealed by integration tests hence ideally suited for serving in a continuous integration context.
in this paper we make the following contributions.
.we propose a variant of spectrum based fault localisation referred to as patterned spectrum analysis in the remainder of this paper which leverages patterns of method calls by means of frequent itemset mining.
.we compare patterned spectrum analysis against the current state of the art referred to as raw spectrum analysis in the remainder of this paper using the defects4j dataset .
.the comparison is inspired by a realistic fault localisation scenario in the context of continuous integration drawn from a series of discussions with practitioners.
the remainder of this paper is organised as follows.
section lists the current state of the art.
section presents a motivating example followed by section explaining the inner details of our variant.
section describes the case study set up which naturally leads to section reporting the results of the case study.
after a discussion of potential improvements in section and the threats to validity in section we come to a conclusion in section .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
.
state of the art this section provides an overview of the current state ofthe art in spectrum based fault localisation .
in particular we sketch the two dimensions for the variants that have been investigated either the granularity statement block method class or the fault locator function tarantula ochiai t and naish2 .
we also explain what is commonly used when evaluating the heuristics the evaluation metric wasted e ort and the available benchmarks and datasets.
finally we list some common applications of fault localisation which heavily in uences the way people assess the e ectiveness in the past research.
automated fault localisation.
to help developers quickly locate the faults there exist two broad categories of automated fault localisation heuristics information retrieval based fault localisation and spectrum based fault localisation .
both of these categories produce a ranked list of program elements indicating the likelihood of a program element causing the fault.
while the former uses bug reports and source code les for analysis the later uses program traces generated by executing failing and passing test cases.
since spectrum based fault localisation heuritsics only require traces from test runs readily available after running the regression test suite these heuritics are ideally suited for locating regression faults in a continuous integration context.
spectrum based fault localisation is quite an e ective heuristic as reported in several papers .
sometimes other names are used namely coverage based fault localisation and statistical debugging .
to understand how spectrum based fault localisation heuristics work there are three crucial elements to consider the test coverage matrix the hit spectrum and the fault locator.
we explain each of them below.
.allspectrum based fault localisation heuristics collect coverage information of the elements under test in a test coverage matrix .
this is a matrix where the rows correspond to elements under test and the columns represent the test cases.
each cell in the matrix marks whether a given element under test is executed i.e.
covered by the test case marked as or not marked as .
.next this test coverage matrix is transformed into the hit spectrum sometimes also called coverage spectrum of a program.
the hit spectrum of an element under test is tuple of four values ef ep nf np .
whereef andepare the numbers of failing and passing test cases that execute the element under test and nfandnpare the numbers of failing and passing test cases that do notexecute the element under test.
table shows an example test coverage matrix and spectrum.
.finally the heuristic assigns a suspiciousness to each element under test by means of a fault locator .
this suspiciousness indicates the likelihood of the unit to be at fault.
the underlying intuition is that an element under test executed more in failing tests and less in passing tests gets a higher suspiciousness and appears at top position in the ranking.
sorting the elements under test according to their suspiciousness in descending order produces the ranking.
many if not all variants ofspectrum based fault localisation create a new fault locator table gives an overview of the most popular ones.granularity.
other variants of spectrum based fault localisation concern the choice of the elements under test.
indeed spectrum based fault localisation has been applied at di erent levels of granularity including statements blocks methods and classes .
the seminal work on spectrum based fault localisation started o with statement level granularity .
as a result most of the early research focussed at statement level sometimes extended to basic blocks of statements.
the e ectiveness at the method level has been investigated in only a few cases and then even as part of a large scale comparison involving several levels of granularity .
today the space of known spectrum based fault localisation heuristics is classi ed according to two dimensions the granularity statement block method class and the fault locator function tarantula ochiai t and naish2 .
in this paper we explore the hit spectrum as a third dimension.
we expand the four tuple ef ep nf np so thatefandepincorporate patterns of method calls we extracted by means of frequent itemset mining.
in the remainder of this paper we refer to the current state of the art as raw spectrum analysis while our variant will be denoted with patterned spectrum analysis .
evaluation metric wasted e ort.
fault localisation heuristics produce a ranking of elements under test in the ideal case the faulty unit appears on top of the list.
several ways to evaluate such rankings have been used in the past including relative measures in relation to project size such as the percentage of units that need or need not be inspected to pinpoint the fault .
despite providing a good summary of the accuracy of a heuristic absolute measures are currently deemed better for comparison purposes .
today the wasted e ort metric is commonly adopted .
consequently we will rely on the wasted e ort when comparing raw spectrum analysis against patterned spectrum analysis .
the exact formula for wasted e ort is provided in section equation .
data set.
the early evaluations on the e ectiveness of raw spectrum analysis heuristics were done by means of small c programs taken from the siemens set and space .
despite having industrial origins the faults used in the experiments were manually seeded by the authors .
the next attempt at a common dataset for empirical evaluation of software testing and debugging is the software artifact infrastructure repository sir .
unfortunately most of the faults in this dataset are manually seeded as well.
consequently dallmeier et.
al created the ibugs dataset containing real faults drawn from open source java projects .
ibugs contains faults all accompanied with at least one failing test case to reproduce the fault.
the last improvement on fault datasets is known as defects4j .
defects4j has a few advantages over ibugs all the faults in defects4j are isolated the changes in vfixfor corresponding vbug purely represent the bug x. unrelated changes such as adding features or refactorings are isolated.
defects4j also provides a comprehensive test execution framework which abstracts away the underlying build system and provides a uniform interface to common build tasks compilation test runs etc.
.
.
.
to the best of our knowledge the defects4j has not yet been used for evaluating raw spectrum analysis .
hence we will adopt the defects4j dataset for our comparison.
275table an example test coverage matrix and hit spectrum element under testfailing test cases passing test casesef ep nf npt1 tmtm tn unit ixi xi mxi m .
.
.xi npm j 1xi jpn j m 1xi jm ef n m ep tidenotesithtest case xj jtakes the binary value or table popular fault locators faul locator de nition tarantula ef ef nf ef ef nf ep ep np ochiai efp ef nf ef ep t ef ef nf ef ef nf ep ep np max ef ef nf ep ep np naish2 ef ep ep np the current state of the art relies on wasted e ort to evaluate fault localisation heuristics mainly via the sirand ibugs datasets.
when comparing raw spectrum analysis against patterned spectrum analysis we rely on wasted e ort as well yet adopt the more recent defects4j dataset.
applications.
in the initial research papers the main perspective for spectrum based fault localisation was to assist an individual programmer during debugging .
the typical scenario was a debugging window showing not only the stack trace but also a ranked list of potential locations for the fault hoping that the root cause of the fault appears at the top of the list.
this explains why the accuracy of these heuristics was mainly evaluated in terms of percentage of code that needs to be inspected.
recently another application emerged automated fault repair .
the latter techniques repair a fault by modifying potentially faulty program elements in brute force manner until a valid patch i.e.
one that makes the tests pass is found.
the rst step in automated repair is fault localisation which in turn resulted in another evaluation perspective namely whether it increases the e ectiveness of automated fault repair .
the two commonly used applications for fault localisation are debugging andautomated fault repair .
up until now continuous integration has never been considered.
we will present the implications of broken builds within continuous integration in section .
.
motivating scenario since we propose continuous integration as a testbed for validating patterned spectrum analysis it is necessary to be precise about what exactly constitutes a continuous integration tool and what kind of requirements it imposes on a fault localisation heuristic.
as commonly accepted in requirements engineering we specify the context and its requirements by means of a scenario .
the driving force underlying the scenario is the observation that if a build is broken it should be repaired immediately hence the root cause should be identi ed as quickly as possible.
note that at a rst glance this scenario may seem naive.
nevertheless it is based on a series of discussions with soft ware engineers working with the agile development process scrum and who rely on a continuous integration server to deploy their software on a daily basis.
the discussions were held during meetings of the steering group of the cha q project where we confronted practitioners with the scenario below and asked for their input on what a good fault localisation method should achieve.
therefore we can assure the reader that the scenario represents a real problem felt within today s software teams.
prelude geopulse geopulse1is an app which locates the nearest location of an external heart de brillator so that in case of an emergency one can quickly help an individual su ering from a cardiac arrest.
the software runs mainly as a web service where the database of all known de brillators is maintained yet is accessed by several versions of the app running on a wide range of devices smart phones tablets and even a mini version for smart watches .
software team.
there is a person team responsible for the development of the geopulse app work from the main o ce in brussels while work from a remote site in budapest.
the team adopts a scrum process and uses continuous integration to ensure that everything runs smoothly.
it s a staged build process where the build server performs the following steps compilation unit tests static code analysis integration tests platform tests performance tests security tests.
steps are the level tests and xing problems there is the responsibility of the individual team members steps are the level defence and the responsibility of the team.
scene unit testing.
angela just resolved a long standing issue with the smart watch version of the app and drastically reduced the response time when communicating with the smart phone over bluetooth.
she marks the issue report as closed puts the issue id in the commit message and sends everything o to the continuous integration server.
a few seconds later the lava lamp in her cubicle glows orange notifying a broken build.
angela quickly inspects the build log and nds that one unit test fails.
luckily the guidelines for unit tests are strictly followed within the geopulse team unit tests run fast have few dependencies on other modules and come with good diagnosing messages .
angela can quickly pinpoint the root cause as a missing initialisation routine in one of the subclasses she created.
she adds the initialiser commits again and this time the build server nds no problems and commits her work to the main branch for further testing during the nightly build.
the lava lamp turns green again and angela goes to fetch a co ee before starting her next work item.
purpose.
this scene illustrates the importance of the level tests and the role of unit tests in there.
ideally running the whole suite of unit tests takes just a few seconds and if one of the unit tests fails it is almost straightforward to locate the fault.
moreover it is also clear who should x 1the name and description of the app is ctitious.
276the fault as it is the last person who made a commit on the branch.
thus fault localisation in the context of unit tests sensu stricto is pointless the fault is located within the unit by de nition and the diagnosing messages combined with the recent changes is usually su cient to repair e ciently.
scene integration testing.
bob arrives in his o ce in the morning and sees that the lava lamp is purple signifying that the nightly build broke.
he quickly inspects the server logs and sees that the team resolved issues yesterday resulting in separate branches merged into the main trunk.
there are three seemingly unrelated integration tests which fail thus bob has no clue on the root cause of the failure.
during the stand up meeting the team discusses the status of the build and then suspends all work to x the broken build.
team members form pairs to get rapid feedback however synchronising with vaclav and ivor in the budapest o ce is cumbersome skype is not ideal for pair programming.
it takes the team the rest of the morning until angela and vaclav eventually nd and x the root cause of the fault there was a null check missing in the initialisation routine angela added yesterday.
purpose.
this scene illustrates the real potential of fault localisation during continuous integration.
faults in integration tests rarely occur but have a big impact because they are di cult to locate hence di cult to assign to an individual.
moreover software engineers must analyse code written the day before and integration tests written by others the mental overhead of such context switches is signi cant.
finally since these faults block all progress team members must drop all other tasks to x the build.
.
requirements from the above scenario we can infer a few requirements that should hold for a fault localisation heuristic integrated in a continuous integration server.
method level granularity .
the seminal work on raw spectrum analysis named tarantula was motivated by supporting an individual test engineer and chose statement level granularity .
however for fault localisation within integration tests method level granularity is more appropriate.
indeed the smallest element under test in object oriented testing is a method .
this also shows in modern ide where failing tests and stack traces report at method level.
last but not least objects interact through methods thus integration faults appear when objects don t invoke the methods according to the often implicit protocol.
top .
a fault localisation heuristic produces a ranked list of program elements likely to be at fault thus the obvious question is how deep in the ranking the correct answer should be to still be considered acceptable.
in the remainder of the paper we set the threshold to inspired by earlier work from lucia et.
al .
is still an arbitrary number but was con rmed to be a good target during our interviews with real developers.
fault localisation is applicable for complex systems with multiple branches and staged testing.
faults in integration tests in particular are very relevant they seldom occur but when they do they have a big impact on the team productivity.
thus to compare raw spectrum analysisagainst patterned spectrum analysis we should treat integration tests di erently than unit tests.
.
patterned spectrum analysis as explained earlier current raw spectrum analysis heuristics comprise several variants typically classi ed according to two dimensions the granularity statement block method class and the fault locator function tarantula ochiai t and naish2 .
in this paper we explore the hitspectrum as a third dimension incorporating patterns of method calls extracted by means of frequent itemset mining.
below we explain the details of the patterned spectrum analysis variant.
we run the test suite and for each test case collect the trace cf.
section .
slice the trace into individual method traces cf.
section .
reduce the sliced traces into call patterns for a method cf.
section .
calculate the hit spectrum by incorporating frequent itemset mining cf.
section .
and nally rank the methods according to their likelihood of being at fault cf.
section .
.
.
collecting the trace we maintain a single trace per test case.
when a test runs it invokes methods in the project base code.
we intercept all the method calls originating from the base code method.
we do not intercept calls in test methods since we assume that the test oracles themselves are correct.
the trace is collected by introducing logger functionality into the base code via aspectj2.
more speci cally we use a method call join point with a pointcut to pick out every call site.
for each intercepted call we collect the called method identi er caller object identi er and the caller method identi er.
these identi ers are integers uniquely associated with a method name.
listing code snippet for a sample method 1public class a b objb c objc ..... public void collaborate b.getdata while ... if ... c.getattributes if ... c.setattributes ... if ... c.processdata ... while b.savedata method as an example assuming the test case instantiates three objects of class a and calls method collaborate listing for each instance.
a sample trace in a test case speci cally highlighting the method calls originating from thecollaborate method in listing is shown in table .
the three instances of class a are shown id and which each received a separate call to collaborate .
the execution of collaborate on object id resulted into a call to getdata line getattributes line setattributes line and nally savedata line .
execution of collaborate on object id and results in a slightly di erent calls.
the caller object id is the identi er of the caller object which calls the method the caller is the method from which 277table a sample trace highlighting calls in listing caller object id caller idycallee idz .
.
.
.
.
.
.
.
.
ycaller id indicates method collaborate in listing zcallee id is the line number in listing the call is made and the callee is the called method.
when a method is executed in a class context static methods can be executed without instantiating a class there is no caller object hence we mark the object caller id as .
considering the intercepted call getdata line the caller object id is the id of the class a object instantiated in the test case the caller id is the id of method collaborate and the callee id is the id of method getdata .
in a similar manner calls originating from other methods such as method getdata of class b invoked from method collaborate line are recorded in the trace.
.
slicing the trace once a trace for a test case is obtained we slice the trace into individual method traces.
each sliced trace represents the trace for each executed method in the test case.
the sliced trace for a method m in a test casetis represented as a set tm ft1 t2 t ng wheretirepresents the method calls invoked from method m through the same caller object.
if the method m is static the calls appear in a single trace for caller object id .
referring to table t1 h6 15ifor the calls of method collaborate id5 with caller object id t2 h6 13iwith caller object id andt3 h6 15iwith caller object id .
therefore the sliced trace 5for method collaborate id is as follows.
t5 fh6 15i h6 13i h6 15ig .
obtaining call patterns we reduce the sliced trace tmof a method m coming from a test casetinto a set of call patterns stm.
to arrive at set of call patterns stm we adopt the closed itemset mining algorithm .
given the sliced trace tmof method m in a test caset we de ne x itemset a set of method calls.
x support ofx the number of tiintmthat contain this itemset x. minsup minimum support of x a threshold used to tune the number of returned itemsets.
frequent itemset an itemsetxis frequent when x minsup .
closed itemset a frequent itemset xis closed if there exists no proper superset x0whose support is the same as the support of x i e. x0 x .we refer to closed itemset xas a call pattern .
we set minsup to1to include call patterns for the methods executed with one object only or for those executed in a class context.
the set of call patterns stmfor method collaborate id from sliced trace t5 equation is as follows.
st5 ff6 15g f6 13g f6 15g f6 11gg .
calculating the hit spectrum unlike raw spectrum analysis where there is a single test coverage matrix per program patterned spectrum analysis creates a test coverage matrix for each executed method.
in the raw spectrum analysis a row of test coverage matrix corresponds to a method which is a program element per se and the hit spectrum ef ep nf np indicates whether or not the method is involved in test cases.
in patterned spectrum analysis there is a separate test coverage matrix for each method and a row corresponds to a call pattern itemset x of the method.
here the call pattern x is not a program element anymore.
the hit spectrum ef ep nf np ofxnot only indicates whether or not the method is involved in a test case but also summarises its run time behaviour.
the call patterns of a method m inpatterned spectrum analysis are obtained by running the set of failing test cases denoted as tf and the set of passing test cases denoted astp .
we obtain a set of call patterns sm equation for each method m which is the union of i the call patterns of a method resulting from the failing test cases stm2tf and ii the call patterns resulting from the passing test cases stm2tp .
sm fxjx2s tm t tfg fxjx2s tm t tpg the setsm equation is used to construct the test coverage matrix for a method.
as an example consider the set of call patterns for st5 equation of the method collaborate .
assuming this call pattern results from a failing test case it will end up int 2tf.
however the same method collaborate is also executed in a passing test case i e t 2tp and will result in another set of call patterns shown in equation .
st5 ff6 13g f6 15g f6 11gg then the call pattern set s5for the method collaborate becomes the union of equation and equation .
s5 ff6 15g f6 13g f6 15g f6 11gg the hit spectrum is then calculated for each call pattern in the call pattern set which ultimately results in a test coverage matrix for each method.
as an example we show the test coverage matrix for collaborate in table .
.
ranking methods based on the test coverage matrix of call patterns for each method each pattern in the call pattern set sm equation gets a suspiciousness score.
this suspiciousness is calculated by using a fault locator .
then we set the suspiciousness of the method as the maximum suspiciousness of its constituting patterns.
suspiciousness per call pattern.
each call pattern x2 sm equation gets a suspiciousness w x calculated with a fault locator.
in principle any fault locator can be chosen from the literature.
however for our comparison purpose 278table an example test coverage matrix for method collaborate call patternxfailing test cases t 2tf passing test cases t 2tp ef x ep x nf x np x w x t1 t2 f6 15g .
f6 13g .
f6 15g .
f6 11g .
we tested all four fault locators mentioned in table in patterned spectrum analysis and ochiai equation came out as the best performing one.
for our running example the suspiciousness w x for each call pattern of method collaborate is given in table .
w x ef x p ef x nf x ef x ep x suspiciousness per method.
each method m gets a suspiciousness w m which is the suspiciousness of the call patternxwith the highest suspiciousness equation .
we choose the maximum instead of average for the suspiciousness score because the technique is looking for exceptional traces one unique and highly suspicious pattern is more important than several unsuspicious ones.
those methods without call patterns set have suspiciousness .
the suspiciousness for method collaborate w in our running example is .
which is the suspiciousness of the call pattern f6 15g with highest suspiciousness .
w m max x2sm w x ranking.
finally a ranking of all executed methods is produced using their suspiciousness w m .
the suspiciousness of the method indicates its likelihood of being at fault.
those methods with the highest suspiciousness appear a the top in the ranking.
.
case study setup given the current state of the art referred to as raw spectrum analysis and the variant proposed in this paper referred to as patterned spectrum analysis we can now compare the e ectiveness of these two heuristics from the perspective of a continuous integration scenario.
we give some details about the dataset used for the comparison defects4j the evaluation metric wasted e ort to nish with the research questions and protocol driving the comparison.
dataset.
we use real faults from open source java projects apache commons math apache commons lang joda time jfreechart and google closure compiler.
the descriptive statistics of these projects are reported in table .
these faults have been collected by just et.
al.
into a database called defects4j3 adatabase of existing faults to enable controlled testing studies for java programs .
the database contains meta info about each fault including the source classes modi ed to x the fault the test cases that expose the fault and the test cases that trigger at least one of the modi ed classes.
although the framework does not explicitly list the modi ed methods we could reverse engineer those by means of the patches that come with the framework.
note that we excluded faults of apache commons lang of apache commons math and fault of joda time since the fault was not located inside a method.
unfortunately the defects4j dataset does not distinguish between unit tests or integration tests.
as argued in the scenario section this is a crucial factor when assessing a fault localisation heuristic in a continuous integration context.
we therefore manually inspected a sample of test methods and noticed that four projects apache commons math apache commons lang joda time and jfreechart mainly contain unit tests they have a small often empty set up method and test methods contain only a few assert s. one project however closure compiler relies on integration tests.
the test cases there are a subclass of compilertestcase that de nes a few template methods which are the entry point to several classes in the base code of the project.
to corroborate this manual inspection we calculated the number of methods triggered in each fault spectrum analysis.
the assumption here is that integration tests exercise several methods in various classes consequently the fault spectrum analysis should trigger many methods as well.
thus projects which gravitate towards integration testing should trigger many methods while projects gravitating towards unit tests should trigger far fewer.
the results are shown in the last two columns and of table listing the average and standard deviation per project respectively.
the high number of for the closure project is an indication that the closure tests exercise a lot of the base code yet the high standard deviation signals the presence of unit tests as well.
on the other hand the low number of for the other project hints at mostly unit tests yet chart has a standard deviation of compared to an average of indicating a few outlier tests which cover a lot of the base code.
the defects4j dataset does not distinguish between unit tests or integration tests.
however one project closure compiler gravitates towards integration tests.
therefore the results of the closure compiler should serve as circumstantial evidence during the comparison.
wasted e ort.
as mentioned earlier we compare by means of the wasted e ort metric commonly adopted in recent research .
the wasted e ort indicates the number of non faulty methods to inspect in vain before reaching the faulty method.
wasted e ort m n where mis the number of non faulty methods ranked strictly higher than the faulty method nis the number of non faulty methods with equal rank to the faulty method.
this deals with ties in the ranking.
the comparison is driven by the following research questions.
279table descriptive statistics for the projects used in our experiments defects4j project of bugs source kloc test kloc of tests age years methods triggered y methods triggered z math .
.
lang .
.
time .
.
chart .
.
closure .
.
yaverage number of methods triggered by the spectrum based fault localisation zstandard deviation apache commons math apache commons lang joda time jfreechart google closure compiler rq1 which ranking results in the lowest wasted e ort raw spectrum analysis orpatterned spectrum analysis ?
motivation this is the rst step of the comparison assessing which of the two fault localisation methods provides the best overall ranking.
rq2 how often do raw spectrum analysis and patterned spectrum analysis rankings result in a wasted e ort ?
motivation based on the scenario section we investigate how many times the location of the fault is ranked in the rst items.
rq3 how does the number of triggered methods a ect the wasted e ort of raw spectrum analysis andpatterned spectrum analysis ?
motivation again based on the scenario section we gauge the impact of integration tests.
the number of methods triggered by the fault spectrum analysis acts as a proxy for the degree of integration tests in the test suite.
fault locator.
one dimension of variation in spectrum based fault localisation is the fault locator table lists the most popular ones.
as explained in section .
for comparison purpose we use ochiai for patterned spectrum analysis .
however for the optimal con guration of raw spectrum analysis we actually tested all four fault locators .
naish2 performed the best on the defects4j dataset with method level granularity as can be seen in table .
there we compare the wasted e ort of naish2 against the wasted e ort of other fault locators using the defects in the closure project.
for most defects naish2 results in a better or equal ranking only for a few defects is the ranking with other locators better.
for space reasons we do not show the comparison on other projects but there as well naish2 was the best.
hence we choose ochiai for patterned spectrum analysis and naish2 for raw spectrum analysis in the case study.
table naish within raw spectrum analysis vs. tarantula ochiai and t faul locator tarantula ochiai t protocol.
to run the fault spectrum analysis we check out a faulty version vfault for each project.
then we run the actual spectrum based fault localisation for all relevant test cases i.e.
all test classes which trigger at least one of thesource classes modi ed to x the fault as recorded in the defects4j dataset.
given the continuous integration context for this research this is the most logical way to minimise the number of tests which are fed into the spectrum based fault localisation .
note that this explains why the number of methods triggered by a fault spectrum is a good indicator for the integration tests since the tests are chosen such that they cover all changes made to x the defect.
.
results and discussion in this section we address the three research questions introduced in section .
this allows for a multifaceted comparison of the e ectiveness of patterned spectrum analysis against the state of the art raw spectrum analysis .
rq1 which ranking results in the lowest wasted e ort raw spectrum analysis orpatterned spectrum analysis ?
to determine the best performing heuristic we plot the wasted e ort for all of the faults for both heuristics.
to allow for an easy exploration of the nature of the di erence we sort the faults according to the wasted e ort of raw spectrum analysis and plot the wasted e ort for patterned spectrum analysis accordingly.
the result can be seen in figures 1a 1b 1c 1d and 1e .
next we count all the faults for which the wasted e ort in patterned spectrum analysis is strictly less strictly more or the same and list the absolute numbers per project see table .
table comparing wasted e ort patterned spectrum analysis vs raw spectrum analysis project total math lang time chart closure total to illustrate how the rankings of the heuristics di er we inspect fault of the closure project where the wasted e ort forpatterned spectrum analysis is .
the faulty method is ranked rst while for raw spectrum analysis the wasted e ort is .
this is due to the fact that the faulty method has a call pattern which is unique in all failing test cases hence is easily picked up by patterned spectrum analysis .
on the other hand just marking whether or not the method is executed is not discriminating in raw spectrum analysis .
the number of failing test cases covering the faulty method and non faulty methods is the same .
yet the non faulty methods have 280wasted effort 151patterned spectrum analysis raw spectrum analysis a math wasted effort 101patterned spectrum analysis raw spectrum analysis b lang wasted effort 451patterned spectrum analysis raw spectrum analysis c time wasted effort 501patterned spectrum analysis raw spectrum analysis d chart wasted effort 1901patterned spectrum analysis raw spectrum analysis e closure figure the comparison plots of all the rankings in each lang more suspiciousness than faulty method because the number of passing test cases covering the non faulty methods is less.
since more passing test cases cover the faulty method high value ofep it renders the faulty method less suspicious.
for faults in the dataset the wasted e ort with patterned spectrum analysis is lower than raw spectrum analysis .
moreover this improvement is a lot better for the closure project the one system in the data set which gravitates towards integration tests where we see an improvement for of faults out of .
rq2 how often do raw spectrum analysis and patterned spectrum analysis rankings result in a wasted e ort ?
inspired by the scenario in section we count how many times the location of the fault is ranked in the top .
to deal with ties in the ranking especially at position we identify these as having a wasted e ort .
table faults where wasted e ort is project psayrsazy z total math lang time chart closure total ypsa patterned spectrum analysis .
zrsa raw spectrum analysis .
table shows for each project the number of faults where the wasted e ort is within the range of with both heuristics.
for three projects lang time and chart the performance of the patterned spectrum analysis is comparable but still better than the one of the raw spectrum analysis .
whereas for the remaining two projects math and closure the performance of the patterned spectrum analysis is noticeably better.
these ndings con rm that patterned spectrum analysis ranks more faults in the top .
however there are still a large amount of faults where the ranking is poor wasted e ort .
especially for the closure project less than half of the faults are ranked in the top .
hence there is still room for improvement which we will cover in section .
the patterned spectrum analysis succeeds in ranking the root cause of the fault in the top for of the faults against for raw spectrum analysis .
binwasted effort 5825raw spectrum analysis patterned spectrum analysisfigure triggered methods vs. wasted e ort rq3 how does the number of triggered methods a ect the wasted e ort of raw spectrum analysis andpatterned spectrum analysis ?
in section we argued that the number of methods triggered by the fault spectrum analysis is an indicator of the gravitation towards integration tests see also the last two columns in table .
if that is the case a good spectrum based fault localisation heuristic should obtain a good ranking for a particular fault regardless of the number of triggered methods.
again based on the scenario section we gauge the impact of integration tests.
therefore for each fault we calculate the number of methods triggered by the fault spectrum analysis.
we then sort the faults according to the number of methods and inspect the trend with respect to the number of triggered methods.
unfortunately the standard deviation for the number of triggered methods is really high see the column in table and a normal scatterplot mainly showed the noise.
therefore we group the faults according to the triggered methods into bins of elements.
as these numbers did not divide well there were two bins having and triggered methods respectively.
this binning was decided as a trade o for having an equal number of elements per bin and enough bins to highlight a trend in the 281table triggered methods vs. wasted e ort binpsayrsaz q1 median q3 q1 median q3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ypatterned spectrum analysis .zraw spectrum analysis .
number of triggered methods if any.
for each of the bins we calculated the rst quartile median and the third quartile listing them all in table and plotting them in a series of boxplots figure table and figure illustrate that the number of methods triggered has little e ect on patterned spectrum analysis however quite a lot on raw spectrum analysis .
the last four bins in particular contain faults which trigger more than thousand methods.
the median wasted e ort for patterned spectrum analysis is four to eighteen times lower than raw spectrum analysis .
the better rankings for closure in table and table are inconclusive as one case is not enough to generalise upon.
yet based on an analysis of the number of methods triggered by the fault spectrum there is at least circumstantial evidence that patterned spectrum analysis performs better for integration tests.
.
possible improvements upon closer inspection of those faults ranked high by thepatterned spectrum analysis heuristic we can infer some suggestions for improvement regarding future variations.
first of all an inherent limitation is that a faulty method which does not call any other methods will always be ranked at the bottom.
indeed such methods don t have a call pattern which is the primary coverage element appearing in the test coverage matrix thus the method gets suspiciousness .
in our case study we noticed a few cases where none of the faulty methods had any call pattern.
more speci cally there are such cases in the math project in the chart project in the time and lang projects and only in the closure project.
the best example corresponds to the highest wasted e ort on fault of the lang project see listing .
indeed the faulty method contains char in class org.apache.commons.lang.text.strbuilder gets suspiciousness because the for loop only performs direct accesses to memory and never calls any methods.
similarly the highest wasted e ort for fault in the math project is due to the faulty method issupportupperbou ndinclusive in class distribution.uniformreal distribution which again never calls any other methods.
in this case the method body contained a single statement return false the bug x replaced it by return true .
a last example is fault in time project wherethe fault resided in a faulty constructor hence did again not have any method call pattern.
listing code snippet for a sample method 1public boolean contains char ch char thisbuf buffer correct code for int i i this.size i incorrect code 6for int i i thisbuf.length i if thisbuf ch return true listing unique call sequence in faulty method tryminimizeexits node int string 1node.getlastchild 2nodeutil.getcatchblock node 3nodeutil.hascatchhandler node 4nodeutil.hasfinally node 5node.getlastchild 6tryminimizeexits node int string second patterned spectrum analysis is often able to push the faulty method high in the ranking however there are several cases where it never reaches the top .
a nice example is fault in closure where the wasted e ort for patterned spectrum analysis is .
.
this value is still lower than the one given by raw spectrum analysis .
yet it is too high to ever be considered in a realistic scenario.
manually analysing the traces of the faulty method tryminimizeex its node int string in class com.google.javascr ipt.jscomp.minimizeexitpoints we found a unique call pattern listing which is only called in the failing tests.
the bug x4reveals that the developers removed the if check with a finally block.
this if check involves the last calls in listing lines .
despite being unique the reason why this call pattern was not picked up by patterned spectrum analysis is because the order of method calls is crucial.
indeed the call pattern in patterned spectrum analysis is an itemset hence the call pattern is not order preserving and has no repetitive method calls.
note that the importance of the call order was also pointed out by lo et.
al.
.
as a future improvements of patterned spectrum analysis we might incorporate statements or branches into the hitspectrum.
the call order of methods as well is relevant information to incorporate into the hit spectrum.
.
threats to validity as with all empirical research we identify those factors that may jeopardise the validity of our results and the actions we took to reduce or alleviate the risk.
consistent with the guidelines for case studies research see we organise them into four categories.
construct validity do we measure what was intended ?
wasted e ort .
in this research we adopted the wasted e ort metric to compare raw spectrum analysis against patterned spectrum analysis .
however in information retrieval rankings where users do not want to inspect all outcomes other measures are considered such as mean reciprocal 282rank mrr or mean average precision map .
it is unclear whether the use of these relative evaluation metrics would alter the results.
nevertheless the use of an absolute metric alleviates other concerns .
therefore the impact is minimal.
fault masking .
one particular phenomenon which occurs in a few faults in the defects4j dataset is fault masking .
this is a fault which is spread over multiple locations and where triggering one location already fails the test.
the x for fault of project chart for instance comprises two changes in two separate methods of the class renderer.category.minmaxcategoryrenderer .
the rst change is to override equals object method and the second involves changes in method setgroupstroke stroke .
the test case which exposes the defect calls both methods yet the test case fails on the rst assertion calling the equals object method thereby masking the setgroupstroke stroke method.
the question then is what a fault localisation should report one location or all locations ?
furthermore how should we assess the ranking of multiple locations.
in this research inspired by earlier work we took the assumption that reporting one location is su cient and use the highest ranking of all possible locations.
however one could make other assumptions.
internal validity are there unknown factors which might a ect the outcome of the analyses ?
multiple faults .
one often heard critique on fault localisation heuristics in general and spectrum based fault localisation in particular is that when multiple faults exist the heuristic will confuse their e ects and its accuracy will decrease.
two independent research teams con rmed that multiple faults indeed in uence the accuracy of the heuristic however it created a negligible e ect on the e ectiveness .
we ignore the potential e ect of multiple faults in this paper.
nevertheless future research should study the e ect of multiple faults.
correctness of the oracle .
the continuous integration scenario in section makes the assumption that the test oracle itself is infallible.
however this does not hold in practice christophe et.
al.
observed that functional tests written in the selenium library get updated frequently .
we ignore the e ects of the tests being at fault in this paper but here as well point out that this is something to be studied in future work.
external validity to what extent is it possible to generalise the ndings ?
in our study we experimented with real faults drawn from ve representative open source object oriented projects from defects4j dataset the most recent defect dataset currently available.
obviously it remains to be seen whether similar results would hold for other defects in other systems.
in particular there is a bias towards unit test in the defects4j dataset with only the closure project gravitating towards integration tests.
further research is needed to verify whether the patterned spectrum analysis is indeed a lot better on integration tests in other systems.
reliability is the result dependent on the tools ?
all the tools involved in this case study i.e.
creating the traces calculating the raw spectrum analysis and patterned spectrum analysis rankings have been created by one of the authors.
they have been tested over a period of years thus the risk of faults in the tools is small.
moreover for the calculation of the raw spectrum analysis rankings we compared as best aspossible against the results reported in earlier papers.
the algorithm for frequent itemset mining was adopted from open source library spmf5 hence there as well the risk of faults is small.
.
conclusion spectrum based fault localisation is a class of heuristics known to be e ective for localising faults in existing software systems.
these heuristics compare execution traces of failing and passing test runs to produce a ranked list of program elements likely to be at fault.
the current state of the art referred to as raw spectrum analysis comprises several variants typically classi ed according to two dimensions the granularity statement block method class and the fault locator function tarantula ochiai t and naish2 .
in this paper we explore a third dimension the hitspectrum.
more speci cally we propose a variant referred to as patterned spectrum analysis which extends the hitspectrum with patterns of method calls extracted by means of frequent itemset mining.
the motivation for the patterned spectrum analysis variant stems from a series of contacts with software developers working in agile projects and relying on continuous integration to run all the tests.
complex systems with multiple branches and staged testing could really bene t from fault localisation.
faults in integration tests in particular are very relevant they seldom occur but if they do they have a big impact on the team productivity.
inspired by the continuous integration motivational example we compare patterned spectrum analysis against raw spectrum analysis using the defects4j dataset.
this dataset contains real faults drawn from ve representative open source java projects.
despite a bias towards unit tests in the dataset we demonstrate that patterned spectrum analysisis more e ective in localising the fault.
for faults in the dataset the wasted e ort with patterned spectrum analysis is lower than raw spectrum analysis .
also patterned spectrum analysis succeeds in ranking the root cause of the fault in the top for of the defects against for raw spectrum analysis .
moreover this improvement is a lot better for the closure project the one system in the data set which gravitates towards integration tests.
there we see an improvement for defects out of .
the better rankings for closure are inconclusive one case is not enough to generalise upon yet based on an analysis of the number of methods triggered by the fault spectrum there is at least circumstantial evidence that patterned spectrum analysis performs better for integration tests.
despite this improvement we collect anecdotal evidence from those situations where thepatterned spectrum analysis ranking is less adequate and derive suggestions for future improvements.
.