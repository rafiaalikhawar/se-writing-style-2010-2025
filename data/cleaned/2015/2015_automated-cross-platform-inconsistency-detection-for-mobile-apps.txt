automated cross platform inconsistency detection for mobile apps mattia fazzini georgia institute of technology usa mfazzini cc.gatech.edualessandro orso georgia institute of technology usa orso cc.gatech.edu abstract testing of android apps is particularly challenging due to the fragmentation of the android ecosystem in terms of both devices and operating system versions.
developers must in fact ensure not only that their apps behave as expected but also that the apps behavior is consistent across platforms.
tosupport this task we propose d iffdroid a new technique that helps developers automatically find cross platform inconsistencies cpis in mobile apps.
d iffdroid combines input generation and differential testing to compare the behavior of an app on different platforms and identify possible inconsistencies.
given an app d iffdroid generates test inputs for the app runs the app with these inputs on a reference device and builds a modelof the app behavior runs the app with the same inputs on aset of other devices and compares the behavior of the app on these different devices with the model of its behavior on the reference device.
we implemented d iffdroid and performed an evaluation of our approach on 5benchmarks and over platforms.
our results show that d iffdroid can identify cpis on real apps efficiently and with a limited number of falsepositives.
d iffdroid and our experimental infrastructure are publicly available.
i. i ntroduction testing is a difficult and costly activity in general.
when testing android apps the task is further complicated by the extensive fragmentation of the android ecosystem.
androidapps must be able to run on a myriad of devices and oper ating systems developers are thus faced with the problem ofensuring not only that their apps behave as expected but alsothat the behavior of the apps is consistent across platforms.given the large number of possible hardware and softwareconfigurations in android this makes it extremely difficultand expensive to perform adequate testing of an app .
the problem of cross platform inconsistencies cpis i s therefore prevalent in the android environment where userscan observe failures and unexpected behaviors that are causedby differences between their platform and those on which theapp they are using was tested .
to mitigate this problem and help developers identify inconsistencies in behavior before an app is released inthis paper we propose d iffdroid an automated technique whose goal is to identify cpis by combining input generation user interface ui modeling and differential testing.
moreprecisely d iffdroid takes as input an app and performs four main steps.
first it automatically generates a large setof test inputs for the app.
second it runs the app with theseinputs on a reference device and builds a ui model of theapp.
third it runs the app against the same inputs on a largeset of different platforms.
finally d iffdroid compares the ui models of the app on these different platforms with theui model of the reference device and reports the differencesidentified suitably ranked and visualized to the app developer.
in order to assess the effectiveness of our approach we implemented d iffdroid in a tool and performed an empirical evaluation on real world apps and over different platforms.
in the evaluation d iffdroid was able to find 96inconsistencies due to differences in the version of the android system used or in the screen configuration.
overall our results show that d iffdroid can identify cpis on real apps efficiently while generating only a limited number offalse positives.
our implementation of d iffdroid and our experimental infrastructure are publicly available at the main contributions of this paper are a new technique that combines input generation ui mod eling and differential testing to automatically identifycross platform inconsistencies in the ui of android apps.
a publicly available implementation of our technique thatcan be used to replicate our experiments or build on andextend our approach.
an empirical evaluation performed on a large number ofplatforms that provides initial evidence of the effective ness of our approach.
ii.
m otiv ating example to motivate our work we provide an example from a realworld app called d aily dozen a diet tracking app that has been downloaded more than times and reviewed by more than users.
figure shows the mainacitivity of the app running on a lg g3 device while figure shows the same activity running on a lg optimus l70 device.
users can use this activity to track their daily food intake by clickingon the displayed checkbox elements.
figures and show a cpi for the app.
users can tick the checkbox element associated with the cruciferousvegetables label on a lg g3 device but they cannot do the same on an lg optimus l70 as that checkbox element is not visible when the app runs on such device.
this inconsistencyis caused by a bug in the layout file associated with themainacitivity and is revealed because of the different .
c circlecopyrt2017 ieeease urbana champaign il usa t echnical research308 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
daily dozen running on lg g3 .
screen configurations screen resolution and pixel density of the two devices.
bugs of this type can manifests in one of two ways the checkbox element is present on one device but not onthe other or the checkbox element is present on bothdevices but its visual appearance is different.
the checkboxelement associated with the cruciferous vegetables label isan example of the former case.
in this case the differencecan be visually perceived but it can also be identified bycomparing the ui hierarchies of the two devices.
in fact the ui hierarchy of the app running on the lg optimusl70 device does not have a node representing the checkbox element while such a node is present in the ui hierarchy of the app running on the lg g3 device.
the rightmost checkbox element associated with the other vegetables label is an example of the latter case.
in this case the difference betweenthe two devices can only be perceived visually as both nodesare present the ui hierarchies of the two devices.
these typesof issues are far from rare because developers tend to use alimited set of devices when not only one during developmentand testing.
in addition these inconsistencies are hard to detectbecause this testing process tends to be mostly manual.
figures and also highlight the challenges in finding cpis on mobile devices.
because mobile devices can have different screen configurations certain differences should not be classi fied as cpis.
for example the nuts label is displayed on the lg g3 device while it is not displayed on the lg optimus l70 device.
this difference should not be considered a cpi the label is part of a scrollable list and the former devicesimply accommodates more list items due to its larger screen.
iii.
t hediffdroid technique in this section we present d iffdroid our technique for detecting cpis on mobile devices.
the basic idea behind diffdroid is to use differential testing to identify such inconsistencies.
figure provides a high level overview of our technique and shows its main phases.
given an app under test fig.
daily dozen running on lg optimus l70.
aut and a reference device in its input generation phase diffdroid dynamically generates inputs with the goal of testing the app s functionality.
before providing the generatedinputs to the app the technique captures the ui state of theapp by storing the tree of its ui hierarchy and taking ascreenshot of its appearance on the device.
the technique logs ui hierarchy trees screenshots and generated inputs into the trace which is the input to the following phase the test case encoding phase.
in this phase our technique suitably analyzes the inputs together with ui hierarchy trees to generate aplatform independent test case.
while so the techniquealso creates a ui model of the app.
the ui model is composed of a list of window models and each window model contains a ui hierarchy tree and corresponding screenshot.
we call this ui model the reference ui model as it was generated using the reference device.
the test case execution phase takes as input a set of test devices executes the test case generated by the previous phase on the devices and produces as output aui model for each device test ui models .
finally in the cpi analysis phase d iffdroid performs a differential analysis to compare the reference ui model with the test ui models andgenerates a report that contains the detected cpis.
the cpi report is the output of our technique.
a. input generation the input generation phase aims to test the functionality of the aut on a reference device by dynamically generatinginputs and providing them to the app.
we describe this phasein algorithm .
the algorithm takes as inputs the referencedevice rd the aut aut and a timeout t and produces as output a trace trace that contains window models andgenerated inputs.
we present the abstrax syntax of the trace produced by the algorithm in figure .
the algorithm begins with an empty trace line .
it then starts the aut s tart on the reference device line and subsequently enters its main loop where it iterates until atimeout is reached.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
reference device app under test appinput generation tracetest devicestest case encoding reference ui modeltest case test case execution test ui models cpi analysis cpi report fig.
high level overview of the technique.
algorithm input generation.
input rd reference device aut application under test t input generation timeout output trace window models and generated inputs on reference device 1begin 2trace 3t g et current time start rd aut whilet g et elapsed time t do root g et root rd tree t ra verse root screenshot g et screenshot rd trace.
add window model tree screenshot input g enerate input w1 key w2 system w3 touch inject rd input trace.
add input returntrace in the first part of the loop iteration lines the algorithm retrieves the root node of the ui hierarchy g et root and traverses the ui hierarchy t ra verse to build a tree representation of such hierarchy.
each node in the tree is characterized by the following set of properties node id node type left right top bottom text checked enabled focused selected clickable checkable focusable scrollable and long clickable.
we selected this set of properties because it is the minimal set of properties that allows the technique to best differentiate nodes in the ui hierarchy see section iii d .
after building a tree representation of the ui hierarchy the algo rithm captures a screenshot of the aut g et screenshot .
the algorithm then pairs the tree representation of the ui hierarchy with the screenshot of the aut to define the currentwindow model and adds the model to the trace.
in the second part of the loop iteration lines the algorithm generates an input g enerate input provides the input to the aut i nject and adds the input to the trace.
diffdroid generates three types of inputs k ey system and t ouch using a weighted random distribution as done in related work .
it is worth noting that the technique would also work with a different dynamic input generation approach such as .
key inputs are characterized by the value of the key they are representing system inputs express a change in the orientation of the deviceor data used to transfer control between components of theaut and touch inputs represent clicks or gestures on thedevice.
note that our technique does not currently remove inputs that do not affect the state of the aut but they couldbe discarded using an approach based on delta debugging .trace def traceitems items window model def input def window model def input def items window model def window model tree def screenshot def tree def treeroot reference id nodes nodes node def node def nodes node def nodereference id node props children ids node props node id node type text checkable clickable focusable scrollable long clickable checked enabled focused selected left right top bottom children ids reference id children ids screenshot def screenshot image input def inputinput type input type key input def system input def touch input def key input def keykey value system input def systemsystem input type system input props system input type rotate data system input props exprs touch input def touch coords coords x coord y coord pointer id x coord y coord pointer id coords exprs expr expr exprs expr boolean number string fig.
abstract syntax of the generated trace.
indicates that the value is a number indicates that the value is a string and indicates that the value is a boolean.
b. test case encoding the test case encoding phase aims to generate a platformindependent test case based on the content of the trace createdby the input generation phase.
we present this phase in algorithm .
the algorithm takes as input the trace trace generated by the previous phase of the technique and it produces two outputs a platform independent test case tc and a ui model of the reference decvice ruim .
the algorithm begins with an empty test case line and an empty ui model line .
it then processes the content of the trace in its main loop lines .
in the first part of the loop iteration lines the algorithm processes window models.
if the currently processedwindow model has the same tree representation s ame tree and the same screenshot s ame image of a window model already added into the reference ui model lines the model is discarded as superfluous.
function s ame tree performs a breadth first traversal of two trees and comparesthe value of the properties of the traversed nodes.
if thetwo trees have different structure or if their nodes havedifferent properties the algorithm considers the two treesand corresponding window models to be different.
function authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm test case encoding.
input trace window models and generated inputs on reference device output tc test case ruim ui model of the reference device 1begin 2tc 3ruim foreachitem trace do ifitem window model then newmodel t rue foreachwmodel ruim do ifsame tree wmodel.
get tree item.
get tree and same image wmodel.
get screenshot item.
get screenshot then newmodel f alse ifnewmodel t rue then ruim.
add item stmt generate window model statement item tc.add stmt else stmt g enerate input statement item tc.add stmt returntc ruim same image compares two screenshots using the complex wavelet structural similarity cw ssim index which can range between zero different images and one similarimages .
we motivate the use of this index to compute image similarity in section iii d. if two screenshots do not havecw ssim index equal to one we consider the correspondingwindow models to be different.
if a window model is not redundant lines the algorithm adds the window modelto the reference ui model.
it also generates a test casestatement g enerate window model statement that builds a tree representation of the ui hierarchy and takes a screenshot of the device.
in the second part of the loop iteration lines the algorithm generates a platform independent statement generate input statement that replicates the action of the input in the trace it then adds the statement to thetest case.
we define generated statements as being platformindependent because they can run on any device independently from the operating system version and the device configuration e.g.
screen size .
the algorithm creates platform independent statements following the principles presented in our previouswork .
platform independent test cases allow our tech nique to collect ui models on many different devices thusincreasing the likelihood of identifying cpis.
c. test case execution the test case execution phase aims to collect ui models from a set of test devices.
this phase takes as inputs the test case generated by the previous phase of the techniqueand a set of test devices and it executes the test case onthe set of test devices.
the execution is driven by two typesof statements w indow model statement and i nput statement .
statements of the former type traverse the ui hierarchy of the aut to build a tree representation of suchhierarchy and capture a screenshot of the aut.
the tree andthe screenshot are paired together to form a window modelof the test device this window model is then added to the uimodel of the test device.
statements of the latter type providean input to the aut.
these inputs are meant to exercise theaut on the test device in the same way it was exercised inthe generation phase.
the output of this phase is a mapping between test devices and corresponding ui models.
generation of ui models for test devices is amenable to parallelization as the computationof a ui model for one device is completely independent fromthe computation of the ui model for a different device.
d. cpi analysis the cpi analysis phase aims to identify cpis in the aut and is the core of our technique.
we present this phase in algorithm .
algorithm takes as inputs the reference uimodel ruim and the map of ui models of test devices tuimmap generated by the previous phase.
the algorithm produces as output a report that lists the identifiedcpis cpireport this report is also the overall output of d iffdroid .
the algorithm begins with an empty cpi report line iterates over each window model rdmodel in the reference ui model lines and compares the window model athand to the corresponding window model tdmodel in all test ui models lines .
the comparison between a referencewindow model and a test window model is divided into twosteps.
the first step lines matches nodes from the tree representing the ui hierarchy of the reference device rdtree to nodes from the tree representing the ui hierarchy of the test device tdtree .
the second step lines compares the visual representation image of matched nodes.
the firststep can detect structural cpis which consist of missing or additional nodes.
the second step can detect visual cpis which consist of nodes with different visual representations.considering the motivating example of section ii the check box element associated with the cruciferous vegetables label is an example of a structural cpi while the rightmostcheckbox element associated with the other vegetables label is an example of a visual cpi.
the node mapping process begins by initializing the mapping nodemappingmap between nodes in the reference tree and nodes in test tree to the empty value line .for each node rdnode in the reference tree the algorithm then computes a node similarity value nodesim between the reference node and each node tdnode in the test tree using function c ompute structural similarity .
this function computes a value between zero and one that repre sents the structural similarity of two nodes see section iii d1 if the similarity value is greater or equal than a threshold the algorithm stores the similarity value together with the testnode in a list mappednodelist .
threshold is used to avoid matching nodes that are too dissimilar.
the choice ofthe value of is related to function c ompute structural similarity and we describe it in section iii d1.
when the algorithm has processed all nodes in the test tree it stores themapping between the reference node and the computed listintonodemappingmap line .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
oncemappednodelist is computed for all reference nodes the algorithm computes the optimal mapping between reference nodes and test nodes using function f ind bestmapping line .
for every reference node this function sorts the elements in the mappednodelist in descending order based on their node similarity value.
the function then finds the mappednodelist containing the element with the highest node similarity value maps the reference node asso ciated with the mappednodelist to the test node associated with the element of the list and marks the reference node asprocessed.
finally the function removes all occurrences of thetest node from the mappednodelist associated with other reference nodes.
this process continues until all reference nodes are processed.
after finding the best mapping between nodes in the reference tree and nodes in the test tree line the algorithm analyzes the mapping to find structural cpis.
the algorithmiterates over each node in the reference tree lines tofind reference nodes that do not have a mapping to a node inthe test tree.
if such a node is found it means that the node ispresent in the aut while running on the reference device butit is not present in the aut while running on the test device.
achallenging aspect for the classification of the missing nodes isgiven by the fragmentation of the android ecosystem.
devicescome with different screen configurations and it could benormal that two devices have a different number of nodes intheir ui hierarchies.
consider again the motivating example of section ii in which the m ainacitvity displays a list of servings.
when the app is running on the lg g3 the device displays eight elements.
when the app is running on the lg optimus l70 conversely the device displays only seven elements.
in thiscase the eighth element of the list should not be classifiedas an inconsistency because the android system does notrepresent a node in the ui hierarchy if the node is notvisible.
for this reason the algorithm further analyzes the node using function w ithin dynamically sized element to determine whether or not the node should be reported as inconsistency.
if the node does not have an ancestor that isscrollable the node is reported as an inconsistency.
otherwise if the node has a scrollable ancestor and has itspreceding or subsequent sibling depending on the position ofthe node in the tree that is matched to a node that is visibileand it is not at the end of the dynamically sized element thenthe node is also reported as an inconsistency.
in all other cases the node is not reported as an inconsistency.
in case function w ithin dynamically sized element confirms that the node is an inconsistency the node is added to the cpi reportas a structural cpi.
similarly the algorithm iterates over nodesin the test tree lines to find test nodes that do not have a mapping to a node in the reference tree.
if such a node isfound and the node is not part of a dynamically sized element the algorithm reports it as a structural cpi.
after these steps the algorithm visually compares mapped nodes lines .
this part of the algorithm starts by re trieving g et screenshot the screenshots of the referencealgorithm cpi detection analysis.
input ruim ui model of reference device tuimmap map of ui models of test devices output cpireport set of cross device inconsistencies 1begin 2cpireport fori i ruim.length ido rdmodel ruim.
get i foreachtd tuimmap.
key set do node mapping tdmodel tuimmap .
get i rdtree rdmodel.
get tree tdtree tdmodel.
get tree nodemappingmap foreachrdnode rdtree do mappednodelist foreachtdnode tdtree do nodesim compute structural similarity rdtree rdnode tdtree tdnode ifnodesim then mappednodelist.
add mapping nodesim tdnode nodemappingmap mappednodelist find best mapping nodemappingmap structural comparison foreachrdnode rdtree do ifnodemappingmap and within dynamically sized element rdnode nodemappingmap then cpireport.
add structural cpi td rdnode nodemappingmap.
remove rdnode foreachtdnode tdtree do mapped false foreachrdnode rdtree do ifnodemappingmap .
contains tdnode then mapped t r u e ifmapped false and within dynamically sized element tdnode nodemappingmap then cpireport.
add structural cpi td tdnode visual comparison rdscreenshot rdmodel.
get screenshot tdscreenshot tdmodel.
get screenshot foreachrdnode nodemappingmap.
key set do tdnode nodemappingmap .remove rdnodeimage rdscreenshot.
crop rdnode tdnodeimage tdscreenshot.
crop tdnode isinconsistency compute image similarity rdnodeimage tdnodeimage ifisinconsistency then cpireport.
add visual cpi td rdnodeimage tdnodeimage rank cpireport returncpireport and test devices from their window models.
then for eachnode in the reference tree the algorithm retrieves the testnode mapped to it and creates two images rdnodeimageandtdnodeimage from the two screenshots rdscreenshot andtdscreenshot using function c rop.
at this point the algorithm compares the two images using function c ompute image similarity which uses a decision tree classifier to recognize inconsistencies.
we describe the decision treeclassifier we use in section iii d2.
the function uses similarprinciples as the ones we discussed in the context of function w ithin dynamically sized element it does not report as inconsistencies nodes that are partially visible because theyare part of a dynamically sized element.
if the classifier authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
identifies an inconsistency the algorithm reports that the reference node and the test node have a visual inconsistency line .
the algorithm then ranks r ank in line inconsistencies according to the following principles structural inconsistencies are ranked at the top visual inconsistencies thataffect all devices with the same characteristics e.g.
version of the android operating system are ranked next and the remaining inconsistencies are listed last.
finally the algorithmreturns the cpi report line which is the output of thetechnique.
node structural similarity function c ompute structural similarity in algorithm computes the structural similarity between two nodes.
the inputs to thefunction are a reference tree rdtree a reference node rdnode a test tree tdtree and a test node tdnode .
the output of the function is a value between zero and one nodesim that indicates the similarity between the reference node and the test node.
this function is necessary as nodesin the tree are not required to have identifiers and differentversions of the operating system can use different node typesto represent the same node.
the function starts by comparing the identifiers of the reference and test nodes.
if the identifiers are the same andthey are unique in both reference and test trees the functionsets the node similarity value to one and returns it.
in thiscase the function sets the similarity value to its highest valuebecause the identifier is a property manually defined by thedeveloper that is meant to uniquely identify nodes in the tree.
if identifiers are not unique or they are different the function checks the position of the two nodes in the treeby comparing their xpaths using the path expression fromthe root of the tree .
if the nodes have the same xpath thefunction returns one as their similarity.
if the path expressionsof the two nodes differ in more than one path component the function returns zero as similarity value to avoid matches between nodes that are too distant in the tree .
if only one path component is different which may be due to smalldifferences in the tree representation of the test devices thefunction computes the similarity value based on the followingproperties of the nodes checkable clickable focusable scrollable text checked selected long clickable enabled andfocused .
the similarity value in this case is given by the number of matching properties divided by the number ofproperties.
for the evaluation of d iffdroid we chose .
as the value of in algorithm to indicate that we do not want to match nodes having more than one property value thatdiffers.
decision tree classifier function c ompute image similarity in algorithm uses a decision tree classifier to compute whether the visual representation of two nodesshould be reported as a cpi.
the decision tree classifieralgorithm creates a model that predicts the value of a targetvariable based on a set of input variables.
the algorithm learnsthe model using a set of training data.
in our context thetraining data corresponds to images of nodes from the uihierarchies of apps running on different devices.
the trainingset must also include a set of images exhibiting cpis.
afterbuilding the model function c ompute image similarity follows the set of decisions in the model to predict the targetvariable.
d iffdroid uses the following variables as inputs to the the classifier complex wavelet structural similarity index.
our technique uses the complex wavelet structural similarity cwssim index to compare the structural similarity of the content of two images.
cw ssim is an image similarity metric robust to small rotations and translations in the images beingcompared.
this characteristic makes the metric especiallysuitable in our context because different devices have differentscreen configurations therefore the visual representation oftwo nodes may present minor differences that should not be reported as cpis.
earth s mover distance of color histograms.
d iffdroid uses the earth s mover distance emd of the color histograms of two images to compare the color composition of the images.
emd is a measure of the distance betweentwo distributions and intuitively consists of the minimal costthat must be paid to transform one color distribution into theother.
we decided to use this metric to take into account thefact that two images may have similar structure but display different colors.
relative ratio change.
our technique uses the relative ratio change to assess whether two images differ significantly in their proportions.
the relative ratio change is defined as rrc w t ht wr hr wr hr wherewtandht are the width and height of the test node while wrandhr are the width and height of the reference node.
this value allows d iffdroid to identify nodes whose ratio is altered as a consequence of the placement of other nodes.
optical character recognition output.
diffdroid uses the output of optical character recognition ocr to assess whether two images display the same text.
the classifiertakes as input the value of the comparison equal not equal .
we decided to use the output of ocr because nodes might have the same text in their tree representation but might displaythe text differently.
the target variable predicted by the classifier indicates whether the visual representation of two nodes should be reported as an inconsistency.
iv .
i mplementation diffdroid s input generation module is built on top of monkey an input generator for android apps that generates pseudo random sequences of user and system events.
weextended monkey to encode generated inputs into the traceand to save ui hierarchies together with visual representations screenshots of the app being tested.
the tool is able to inspectthe screen content of the app using uiautomation which is a special accessibility service of the android platform.the input generation module can run on any device withoutmodifying the android system.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i benchmarks used in the empirical ev aluation .
id name category v ersion loc k a1 buildm learn education .
.
.
a2 daily dozen health .
.
a3 kitchen timer tools .
.
.
a4 outlay finance .
.
a5 translation studio books .
.
the test case encoding module uses the j avapoet .
library to generate the source code of test cases.
the implementation of d iffdroid extends the espresso framework to generate and execute test cases.
the espressoframework synchronizes test operations with the app beingtested by waiting for ui events to be handled and for defaultinstances of asynctask computation that runs on a background thread to complete.
this capability allows d iffdroid to execute the same test case on different devices even when devices have different hardware configurations which couldlead to different timings in the execution of an app.
however there are cases in which apps perform background operationsusing non standard means e.g.
direct creation and manage ment of threads .
the developer needs to manually handlesuch cases.
for the benchmarks of section v this additionaltask was not necessary.
the test case encoding module usesthe implementation of cw ssim offered by pyssim tocompare screenshots taken by the input generation module and minimize the number of ui hierarchies and screenshotsgenerated during test execution.
the test case execution module leverages the aws device farm to execute test cases on real devices.
the moduleuses the aws command line interface cli to automate theprocess of running test cases and retrieving execution artifacts ui hierarchies and screenshots .
finally the cpi analysis module generates the decision tree classifier used to detect cpis by leveraging the wekadata mining framework .
the cw ssim index used inthe classifier is computed using pyssim the emd value iscomputed using the opencv library and the characterrecognition task is performed used tesseract ocr engine.
v. e mpirical ev aluation to determine the practicality and effectiveness of our technique we performed an empirical evaluation of d iffdroid on a set of real world apps and targeted the following researchquestions rq1 can d iffdroid detect cross platform inconsistencies in mobile applications while reporting a limitednumber of false positives?
rq2 what is the cost of running d iffdroid ?
rq3 are there similarities among devices exhibiting cpis?
a. experimental benchmarks and setup for the empirical evaluation we used a set of real world android apps.
more specifically we selected five open source apps from github .
we used open source apps because thetable ii number of test devices divided by resolution and version of the operating system .
android v ersion resolution x1280 x1280 x1920 x2560 x800 x960 x854 testing environment espresso used in the implementation of our technique requires the source code of an app to build andrun test cases for it.
our technique could be directly appliedto app executables by changing testing framework.
we selected apps based on three parameters presence of at least one known ui based cpi in the app self containment and diversity.
in order to find apps contain ing at least one known cpi we searched github s trackersystem for the following keywords android not clickable android cut off and android missing button .
we usedthese keywords instead of more generic keywords such as android compatibility issue to eliminate results that werenot ui issues which are out of scope for our technique.
from the search results we removed issues that did notcorrespond to android apps and issues that we could not reproduce.
finally we selected apps from different categoriesto have a diverse corpus of benchmarks while prioritizingapps for which we did not have to build extensive stubs.table i provides a summary description of the apps considered.
columns name category v ersion and loc report the name category version and number of lines of code for an app.
the analysis performed by d iffdroid relies on the use of a reference device.
we selected an lg g3 running android 22as reference device for the empirical evaluation because we had the device and it did not exhibit any of the cpis already known in the benchmarks.
to compute the results of section v b we executed the input generation phase of diffdroid on the reference device with a timeout of minutes.
we chose this value because in previous work we found that a set of dynamic input generation tools forandroid apps hit their maximum coverage within minutesof execution.
d iffdroid s analysis also requires a set of test devices.
we used the aws device farm for this purpose.
the aws device farm is an app testing service provided by amazon that allows to run tests on real mobile devices.
table ii reportsthe number of devices used in the empirical evaluation groupedby resolution resolution and version of the operating system android v ersion .
the versions of the operating system were the ones available to us and supported by the technologies usedfor the implementation of d iffdroid .
the total number of devices used was .
finally d iffdroid uses a decision tree classifier to recognize cpis.
we trained the classifier using the following authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii results of running diffdroid .for each benchmark considered d number of test devices wm number of window models nr number of nodes in ui hierarchy trees of the reference device nt a verage number of nodes in ui hierarchy trees per test devices cpis number of structural cpi s cpif number of functional cpi s cpiv number of cpi s related to changes in the version of the android system cpic number of cosmetic cpi s fp false positives reported by the technique .
id d wm nr nt cpis cpif cpiv cpic fp a1 .
a2 .
a3 .
a4 .
a5 8table iv cost of running diffdroid .tg s time to encode inputs and compute the refer ence ui model te s a verage test execution time per device ts ms a verage time to com pare uihierarchies per device tcw s av e r ag e cw ssim computation time per device temd s a verage emd computation time per device and tocr s av e r ag e ocr computation time per device .
id tg s te s ts ms tcw s temd s tocr s a1 .
.
.
a2 .
.
.
.
a3 .
.
.
.
a4 .
.
.
a5 .
.
.
procedure.
first we selected one device from each category combination of resolution and android version in table iiand used this set of devices to compute the training set.
wethen collected cw ssim index emd value ocr output and relative ratio change inputs to the classifier for all the nodes in the view hierarchies showing the known ui based cpis.
these nodes are not included in the results of the evaluation.
this procedure produced entries on which to train the classifier.
we labeled the entries either true orfalse based on whether they represented cpis or not respectively.
welabeled entries by looking at their visual representation andlabeled as true entries such that compared to the reference entry differed in their content structure differed interms of color differed in terms of visibility visualizeda different text and had a different aspect ratio.
followingthese guidelines we labeled entries.
we used the weka data mining framework to generate a c .5decision tree classifier.
the framework created a classifier of size 33with 17leaves in .09seconds.
we evaluated the classifier using fold cross validation resulting in a precision of .
and recall of .
.
the cpi analysis phase was performed on a workstation with 64gb of memory one intel xeon i 6700k skylake .0ghz processor running ubuntu .
.
b. results rq1 to answer rq1 we applied our technique to the experimental benchmarks.
table iii reports the results of theevaluation.
the first part of table iii columns d wm n r and nt provides a picture of the scale of the analysis.
for each benchmark column d reports the number of test devices used in the test execution phase column wm provides the number of window models generated by the test case encodingphase n r is the number of nodes in the ui hierarchy trees for the reference device and nt is the average number of nodes in the ui hierarchies for the test devices.
the numberof devices used for each benchmark differs because whenrunning the evaluation certain devices were not available inthe aws device farm.
for unavailable devices we attemptedto run test cases three times before moving forward.
columnsn r and nt differ for two reasons the app might contain a structural cpi and different devices display a differentnumber of nodes for dynamically sized elements e.g.
list containers .
the total number of nodes analyzed across allbenchmarks and devices is .
the second part of table iii columns cpi s cpif cpiv and cpic presents the cpis reported by d iffdroid .
we analyzed cpis reported by our technique and classified them in four categories inconsistencies in the ui hierarchy tree that affect the functionality of the app structural cpis cpis inconsistencies in the visual representation of a node that affect the functionality of the app functionalcpis cpi f inconsistencies generated by the version of the android system used to run the benchmark version cpis cpi v and inconsistencies in the visual representation of a node that do not affect the functionality of the app becausethe user can infer their meaning given the context in whichthey are visualized cosmetic cpis cpi c .
structural cpis correspond to the inconsistencies with the same name we discussed in section iii d while functional cpis versioncpis and cosmetic cpis correspond to the visual cpis thatwe also discussed in section iii d. the results presented inthis section are deterministic as the classification part of thetechnique is itself deterministic.
in addition we also classifiedcpis reported by d iffdroid that did not correspond to an inconsistency as false positives fp .
finally we randomly selected 5nodes in each benchmark on all test devices total checked for possible false negatives and did not findany.
our technique found cpis in all the benchmarks analyzed 6structural cpis 9functional cpis 7version cpis and 74cosmetic cpis.
we now provide an example from each category to better illustrate the identified cpis and how we classified them.
t ranslation studio is a translation app.
in the registration form of the app there is an icon that when clicked presents a privacy note to the user.
however on certaindevices the icon is not present and the user will miss theopportunity to read the privacy note.
on these devices thenode of the icon is not present in the ui hierarchy tree authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
of the app and d iffdroid reports this difference as an inconsistency structural cpi .
kitchen timer offers a timer functionality.
the app can be used to start and stop three timers.
if one of the timers is started the label of the timer increases in size movingthe button to stop the timer at the bottom of the screen.on certain devices the size of the button becomes small enough to prevent users from stopping the timer.
in these devices the node representing the button is present in the uihiearchy tree but its visual appearance differs from that ofthe corresponding node on the reference device.
d iffdroid reports this difference as an inconsistency functional cpi .
buildm learn is an app that assists users in developing android apps.
the app has a menu that can be used to navigatethe app.
the color of the background of the items in the menuis different when the app is running on devices using androidversion .
in these devices the color of the background is similar to the color of the text of the menu items makingdifficult to read the entries in the menu.
d iffdroid reports this difference as an inconsistency version cpi .
outlay helps users track their expenses.
users can enter their expenses using a numpad.
on certain devices onlyroughly one fourth of the numbers is visible.
also in thiscase d iffdroid reports this difference as an inconsistency cosmetic cpi .
we looked at the nature of the structural functional and cosmetic cpis mentioned above and discovered tha they canbe fixed by changing properties of corresponding elements inthe layout files for the apps.
we reported the issues found andtheir possible solutions to the developers of the apps involved.
d iffdroid also reported 16false positives for the five benchmarks we considered.
the false positives reported canbe grouped into two categories.
the first category false positives includes nodes that display text with additionalspacing at the end.
this behavior causes test nodes to have abig relative ratio change leading the classifier to report themas inconsistencies.
to address this issue we plan to leverageocr to recognize text boundaries and compute relative ratiochanges based on such boundaries.
the second category 2false positives includes test nodes whose image differed fromthe reference one in the color distribution but the differenceis such that it cannot be perceived by the human eye.
thischaracteristic resulted in a significantly high emd value leading the classifier to report these nodes as inconsistencies.to reduce the number of this kind of false positives we planto investigate how the number of bins in the computation ofthe emd value affects false positives and performance.
overall we feel that the current number of false positives generated by d iffdroid is acceptable.
moreover they can be further reduced through improvements of the technique.
we therefore believe that the results presented in this sectionprovide initial evidence that d iffdroid can detect cpis in mobile applications while reporting a limited number of false positives.
rq2 to answer rq2 we measured the time taken by each phase of the technique to process the experimentaltable v devices with the highest number of cpi si n our ev aluation .c olumn av android version reports the version of the android system running on the de vice .
device resolution density av lg optimus l70 x800 samsung galaxy s3 mini x800 samsung galaxy j1 ace x800 samsung galaxy j1 duos x800 samsung galaxy s duos x800 samsung galaxy grand neo plus x800 intex aqua y2 pro x854 samsung galaxy light x800 samsung galaxy star advance x800 samsung galaxy note x1280 benchmarks.
table iv summarizes the results and reports the time required to encode dynamically generated inputs as a test case while computing the ui model of the referencedevice t g s the average test case execution time per device te s the average time required to compare reference ui hierarchies with test ui hierarchies per device t s ms the average time required to compute cw ssim indexes per device t cw s the average time required to compute emd values per device t emd s and the average time required to extract text with ocr per device t ocr s .
the values in column tg s show that the cost to compute the ui model based on the dynamically generated inputs is notlow but still acceptable which validates our choice of notperforming this task during the input generation phase.
theaverage time to execute test cases is less than the time takento generate inputs.
this happens mainly because test cases aresaving significantly less ui hierarchies and screenshots.
theonly exception is a4 for which we had to add a 60sec sleep time to make sure the test would go past the login screen onthe test devices.
in the worst case a4 test cases took a total of1 minutes to execute d from table iii times t efrom table iv .
during the evaluation we took advantage of the fact that this task is highly parallelizable and executed test caseson10devices at the time thus reducing the cost roughly by an order of magnitude.
finally the last part of table iv shows that the time required to compare reference ui hierarchies to test ui hierarchiesis negligible compared to the time to compute values forthe features of the classifier.
in the worst case a2 thecpi analysis phase took minutes to complete d from table iii times the sum of t s tcw temd and tocr from table iv .
this task is also highly parallelizable.
and when running the evaluation we analyzed eight devices ata time.
finally the most expensive part of the cpi analysis phase consists of the computation of cw ssim indexes whichacross all apps and all devices took .7seconds on average.
based on these results we can conclude that the analysis performed by d iffdroid can run overnight at least for the cases considered.
rq3 to answer rq3 in table v we ranked devices based on the number of cpis they exhibited with the device authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
exhibiting the highest number of cpis at the top.
for each device column device shows the name of the device columns resolution and density shows the pixel resolution and density respectively of the device and column avreports the version of the android system running on the device.
the top nine devices all have low values for resolution and density and noother test device has these characteristics.
this result suggests that developers should consider to include a device with these characteristics when testing their apps.
while looking at therelation between inconsistencies and device characteristics however we also observed that considering testing devicessolely based on resolution and density would have not allowedus to identify all the inconsistencies reported in table iii.
in fact there are inconsistencies that derive from different hardware configurations of the devices e.g.
the presence ofa physical menu button .
it is also worth noting that evenif all devices in table v happen to run android version we could not find any reason why that this version should beparticularly problematic.
vi.
t hreats tovalidity as it is the case for most empirical evaluation there are both construct and external threats to validity associated withour results.
in terms of construct validity there might be errorsin the implementation of our technique.
to mitigate this threat we extensively inspected the results of the evaluation manually.in terms of external validity our results might not generalizeto other apps or cpis.
in particular we only considered alimited number of apps.
this limitation is an artifact of thecomplexity involved in manually inspecting results derivingfrom executions on a large set of devices over .
to mitigate this threat we used randomly selected real world appsfrom different domains.
vii.
r elated work the fragmentation of the android ecosystem has been studied in the literature .
han and colleagues are among the first to study the issuesgenerated by such fragmentation.
their work systematicallyanalyzes bug reports from two popular mobile device vendors and proposes a method for tracking fragmentation.
in thisline of research holzinger and colleagues discuss the challenges involved in developing apps due to the differencesin size and display resolution of different devices.
our work helps developers in this challenging task by automaticallyidentifying inconsistencies across devices.
the work on android fragmentation led to studies on device prioritization for app testing .
the recentwork from lu and colleagues in particular proposes atechnique to prioritize android device models for individualapps based on mining large scale usage data.
we believe thisline of research to be complementary to ours as it could beintegrated within d iffdroid in case resources are constrained in terms of number of test devices considered.
other related work tries to find compatibility issues in android apps .
wei and colleagues propose a techniquebased on static analysis that identifies compatibility issuesusing an api context pair model.
the issues identified by theirtechnique are different from those identified by d iffdroid as they are related to platform api evolution and drivers im plementation.
this technique could therefore also be combinedwith d iffdroid to identify a broader set of issues.
finally our work relates to the work on inconsistency identification for web apps .
roy choud hary and colleagues propose a technique that crawlsthe web app under test in different browsers collects domtrees and screenshots for web pages in the app and comparescollected trees and images to identify inconsistencies.
thereare differences between mobile and web apps that prevent thisand similar techniques to be straightforwardly applied in ourcontext.
this technique for instance runs browsers so that the size of their visible area is the same which is an assumption that cannot be made for mobile apps.
viii.
c onclusion because of the fragmentation of the android ecosystem android apps can exhibit inconsistencies in their behavior whenthey are run on different platforms.
to help developers identifythese behavioral inconsistencies before developers release theirapps we propose a technique called d iffdroid .diffdroid aims to identify and suitably report behavioral inconsistenciesin android apps by combining input generation behaviormodeling and differential testing.
we implemented and empirically evaluated d iffdroid by running it on 5real world benchmark apps and over different platforms.
our results provide initial evidence that diffdroid can identify cpis on real apps efficiently and with a limited number of false positives.
in future work we will first extend our evaluation by considering additional apps.
second we will perform a user studywith app developers to assess how useful is the informationour technique provides and how effective is the format inwhich it is provided.
third we will extend d iffdroid so that it handles inconsistencies other than visual ones e.g.
inconsistencies in the state of the app after an event isprocessed .
fourth we will investigate whether precision and accuracy of cpis identification can improve by using a multi class classifier approach.
fifth we will investigate techniquesfor suggesting repairs for the cpis identified by d iffdroid .
finally and more on the engineering side we will investigate ways to automatically build stubs for the apps being tested so as to speed up the execution on the different devices enforce determinism and in general allow for performing testexecutions in a sandbox.
a cknowledgments this work was partially supported by the national science foundation under grants ccf and ccf and by funding from amazon under the aws cloud creditsfor research program google ibm research and microsoftresearch.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.