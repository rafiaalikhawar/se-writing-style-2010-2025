a fast causal profiler for task parallel programs adarsh yoga rutgers university usa adarsh.yoga cs.rutgers.edusantosh nagarakatte rutgers university usa santosh.nagarakatte cs.rutgers.edu abstract this paper proposes taskprof a profiler that identifies parallelism bottlenecks in task parallel programs.
it leverages the structure of a task parallel execution to perform fine grained attribution of work to various parts of the program.
taskprof s use of hardware performance counters to perform fine grained measurements minimizes perturbation.
taskprof s profile execution runs in parallel using multi cores.
taskprof s causal profile enables users to estimate improvements in parallelism when a region of code is optimized even when concrete optimizations are not yet known.
we have used taskprof to isolate parallelism bottlenecks in twenty three applications that use the intel threading building blocks library.
we have designed parallelization techniques in five applications to increase parallelism by an order of magnitude using taskprof .
our user study indicates that developers are able to isolate performance bottlenecks with ease using taskprof .
ccs concepts computing methodologies parallel computing methodologies software and its engineering software performance keywords taskprof profilers task parallelism intel tbb causal profiles acm reference format adarsh yoga and santosh nagarakatte.
.
a fast causal profiler for task parallel programs.
in proceedings of 11th joint meeting of the european software engineering conference and the acm sigsoft symposium on the foundations of software engineering paderborn germany september esec fse pages.
introduction task parallelism is an effective approach to write performance portable code .
in this model the programmer specifies finegrained tasks and the runtime maps these tasks to processors while automatically balancing the workload using work stealing algorithms.
many task parallelism frameworks have become mainstream e.g.
intel threading building blocks tbb cilk microsoft task parallel library habanero java x10 and java fork join tasks .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse september paderborn germany association for computing machinery.
acm isbn .
.
.
.
common metric used to quantify the performance of a task parallel program is asymptotic parallelism which measures the potential speedup when the program is executed on a large number of processors.
it is constrained by the longest chain of tasks that must be executed sequentially also known as the span or the critical work .
hence asymptotic parallelism is the ratio of the total work and the critical work performed by the program for a given input.
a scalable program must have large asymptotic parallelism.
a task parallel program can have low asymptotic parallelism due to multiple factors coarse grained tasks limited work performed by the program and secondary effects of execution such as contention low locality and false sharing.
numerous techniques have been proposed to address various bottlenecks in both multithreaded programs and task parallel programs .
these techniques range from identifying critical paths parallelism synchronization bottlenecks and other performance pathologies .
tools for multithreaded programs identify bottlenecks in a specific execution on a specific machine which does not necessarily provide information about the scalability of the program.
in contrast tools that measure asymptotic parallelism in task parallel programs run the program serially which is feasible only when the task parallel model provides serial semantics e.g.
cilk .
although they identify parallelism bottlenecks they do not provide information on regions of code that matter in improving asymptotic parallelism.
this paper proposes taskprof a fast and causal profiler that measures asymptotic parallelism in task parallel programs for a given input.
taskprof s causal profile allows users to estimate improvements in parallelism when regions of code are optimized even before concrete optimizations for them are known.
taskprof has three main goals to minimize perturbation also known as interference while accurately computing asymptotic parallelism and critical work for each spawn site source code location where a task is created to run the profiler in parallel and to provide feedback on regions of code that matter in increasing parallelism.
taskprof computes an accurate parallelism profile by performing fine grained attribution of work to various parts of the program using the structure of a task parallel execution.
the execution of a task parallel program can be represented as a tree specifically dynamic program structure tree dpst which captures the series parallel relationships between tasks and can be constructed in parallel.
given a task parallel program taskprof constructs the dpst in parallel during program execution and attributes work to the leaves of the dpst.
to minimize perturbation taskprof uses hardware performance counters to measure work performed in regions without any task management constructs which correspond to the leaves in the dpst.
taskprof writes the dpst and the work performed by the leaf nodes of the dpst to a profile data esec fse september paderborn germany adarsh yoga and santosh nagarakatte task parallel programparallelism profilersufficient parallelismprogram with sufficient parallelism or no optimization opportunitiesinsufficient parallelismannotate code regions to optimizeno regions to optimizecausal profilertask parallel program with annotated regionsincrease in parallelism?design optimizations for the annotated regionsnoyesparallelism profilecausal profilestart figure identifying and diagnosing parallelism bottlenecks using taskprof s parallelism and causal profiles.
file.
the profile execution runs in parallel leveraging multi cores and the measurement of computation using performance counters is thread safe.
taskprof s post execution analysis tool uses the data file from the profile run reconstructs the dpst and computes asymptotic parallelism and critical work at each spawn site in the program using the properties of the dpst see section .
.
taskprof maps dynamic execution information to static spawn sites by maintaining information about spawn sites in the dpst.
taskprof s profile for the sample program in figure is shown in figure b .
the spawn sites that perform a large fraction of the critical work in the profile are the parallelism bottlenecks in the program.
however optimizing regions that perform critical work may not increase asymptotic parallelism when the program has multiple regions that perform similar amount of critical work.
designing a parallelization strategy that reduces critical work requires significant effort.
hence the programmer would like to know if optimizing a region of code increases asymptotic parallelism even before the specific optimization is designed.
taskprof provides a causal profile that estimates the improvement in asymptotic parallelism when a specific region of code in the program is optimized even before concrete optimizations for them are known.
taskprof s causal profile is inspired by coz which quantifies the speedup when a selected program fragment is optimized in multithreaded programs by slowing down all code executing concurrently with the fragment.
however coz cannot be used with task parallel programs as it is not possible to slow down all active tasks.
in contrast taskprof is able to generate a causal profile because it builds an accurate performance model of a task parallel execution by performing fine grained attribution of work to the nodes of the dpst.
to quantify the impact of optimizing a region of code the programmer annotates the beginning and the end of the region in the program and the anticipated speedup for the region.
taskprof generates a causal profile that shows the increase in parallelism with varying amounts of anticipated speedup for the annotated regions see figure c .
to generate a causal profile taskprof re executes the program generates profile data and identifies nodes in the dpst that correspond to the annotated regions.
subsequently taskprof recomputes the asymptotic parallelism in the program by reducing the critical work of the annotated region of code by the anticipated improvement.
taskprof s causal profiling enables the programmer to identify improvements in asymptotic parallelism even before the developer actually designs the optimization.
figure illustratestaskprof s usage to generate a parallelism profile and a causal profile.
taskprof prototype is open source and available online .
we have identified parallelism bottlenecks in twenty three intel tbb applications using the prototype.
using taskprof s causal profile we also designed concrete parallelization techniques for five applications to address the parallelism bottlenecks.
our concrete optimizations increased parallelism in these five applications by an order of magnitude.
we conducted a user study involving thirteen undergraduate and graduate students to evaluate the usability of taskprof .
our results show that the participants quickly diagnosed parallelism bottlenecks using taskprof .
background this section provides a quick primer on the tree based representation of a task parallel execution which is used by taskprof to compute parallelism and causal profiles.
task parallelism.
task parallelism is a structured parallel programming model that simplifies the job of writing performance portable code.
in this model parallel programs are expressed using a small set of expressive yet structured patterns.
in contrast to threads task creation is inexpensive and a task is typically bound to the same thread till completion .
the runtime uses work stealing to map dynamic tasks to runtime threads and balances the workload between threads .
task programming models provide specific constructs to create tasks e.g.
spawn keyword in cilk and spawn function in intel tbb and to wait for other tasks to complete e.g.
sync keyword in cilk and wait for all function in intel tbb .
a sample task parallel program is shown in figure .
these models also provide patterns for recursive decomposition of a program e.g.
parallel for andparallel reduce that are built using the basic constructs.
task parallelism is expressive and widely applicable for writing structured parallel programs.
dynamic program structure tree.
the execution of a task parallel program can be represented as a dynamic program structure tree dpst which precisely captures the series parallel relationships between tasks .
further the dpst can be constructed in parallel.
since our goal in this paper is to profile the program in parallel we use the dpst representation of a task parallel execution.
the dpst is a n ary tree representation of a task parallel execution.
there are three kinds of nodes in a dpst step async and finish nodes.
the step node represents the sequence of dynamic instructions without any task spawn or sync statements.
16a fast causal profiler for task parallel programs esec fse september paderborn germany void compute tree sum node n int sum if n num nodes base compute sum serially causal begin sum serial tree sum n causal end else int left sum right sum if n left spawn compute tree sum n left left sum if n right spawn compute tree sum n right right sum sync sum left sum right sum int main causal begin node root create tree causal end int sum spawn compute tree sum root sum sync print sum return figure a program that computes the sum of the nodes in a binary tree.
it creates tasks and waits for tasks to complete using spawn and sync keywords respectively.
each node in the tree holds an integer value number of nodes in the sub tree rooted at the node and pointers to the left and right sub tree.
the create tree function builds the tree.
the serial tree sum takes a node nas argument and computes the sum in the sub tree under n. base is a constant that determines the amount of serial work.
the user has used annotations causal begin and causal end to specify regions for causal profiling which are not used in the regular profiling phase.
all computations occur in step nodes.
the async node in the dpst represents the creation of a child task by a parent task.
the descendants of the newly created task can execute in parallel with the remainder of the parent task.
a finish node is created in a dpst when a task spawns a child task and waits for the child and its descendants to complete.
a finish node is the parent of all async finish and step nodes directly executed by its children or their descendants.
the dpst by construction ensures that two parallel tasks operate on two disjoint sub trees.
dpst s construction also ensures that all internal nodes are either async or finish nodes.
the siblings of a particular node in a dpst are ordered left to right to reflect the left to right sequencing of computation of their parent task.
a path from a node to the root and the left to right ordering of siblings in a dpst do not change even when nodes are added to the dpst during execution.
the dpst was originally used for data race detection because it allows a race detector to check if two accesses can occur in parallel .
in a dpst two step nodes s1ands2 assuming s1is to the left of s2 can execute in parallel if the least wcwew404040w75cw45ew5w5 ss listew85cw145w175 ss list ss listf0s0f1 a1a0 s5s4a2s6f2s2s3s1 line numberparallelismworkl20l25l10l131.
.511175903040span145603040optimization factorparallelismwork2x4x8x2.
.
.38175175175span855540 a dpst b parallelism profile c causal profilecritical path percent58.
.
.58w90cwew0ss list60 w5w5w80w10w40w30 w90cwew0ss list60wcwew303030 ss listfigure a the dpst for an execution of the program in figure .
f0 f1 and f2are finish nodes.
a0 a1 and a2are async nodes.
step nodes are leaves in the dpst.
taskprof maintains four quantities with each intermediate node in the dpst work w critical work cw exclusive work ew and the list of spawn sites performing critical work ss list .
each entry in the spawn site list maintains the line number and the exclusive work done by the spawn site e.g.
l20 .
step nodes have work data from the profile execution.
taskprof updates these quantities for the intermediate nodes by performing a bottom up traversal of the dpst.
b the profile generated by taskprof reports the work critical work parallelism and percentage of critical work with each spawn site.
line with a in the profile corresponds to the main function and reports the parallelism for the entire program.
c the causal profile reports the parallelism for the whole program when the annotated regions in figure are optimized by and .
common ancestor of s1ands2in the dpst has an immediate child that is an async node and is also an ancestor of s1.
in section .
we will highlight the properties of the dpst that we use to profile programs.
illustration of the dpst.
figure a shows the dpst for an execution of the program in figure .
the program in figure will execute the spawn call at line and line once when base n where nis the number of nodes in the tree.
we construct the dpst during program execution as follows.
when the main function starts we add a finish node f0as the root of the dpst to represent the fact that main completes after all the tasks spawned by it have completed.
we add a step node s0as the child of the root finish node to capture the initial computations being performed in the main function.
on a spawn call at line in figure we create a finish node f1because it is the first spawn performed by the task.
we also add an async node a0as the child off1to represent the spawning of a task.
any computation by the 17esec fse september paderborn germany adarsh yoga and santosh nagarakatte newly created task will be added as nodes in the sub tree under the async node a0.
the operations performed in the continuation of the main task will be added to the right of the async node a0under the finish node f1.
hence the continuation of the main task and newly created task operate on distinct subtrees of the dpst and can update the dpst in parallel.
parallelism profiler taskprof computes the total work part of the total work done serially critical work or span and the asymptotic parallelism at each spawn site in a task parallel program.
the key contribution oftaskprof is in fine grained attribution of work while ensuring that the profile execution is fast perturbation free and accurate.
taskprof accomplishes the goal of fast profile execution by using multi cores.
taskprof s profile execution itself runs in parallel and leverages the dpst representation to attribute work to various parts of the program.
taskprof ensures that the profile execution is perturbation free by using hardware performance counters to obtain information about the computation performed by the step nodes in the dpst.
taskprof also maintains a very small fraction of the dpst in memory during profile execution to further minimize perturbation.
taskprof ensures that the parallelism profile is accurate by capturing spawn sites through compiler instrumentation and by precisely measuring work performed in each step node.
taskprof computes the parallelism profile in three steps.
first taskprof provides a modified library for task parallelism that captures information about spawn sites.
taskprof s compiler instrumentation modifies the calls to the task parallel library in the program to provide information about spawn sites.
second taskprof s profile execution runs in parallel on the multi core processors constructs the dpst representation of the execution and collects fine grained information about the execution using hardware performance counters.
taskprof writes the profile information to a data file similar to the grof profiler for sequential programs .
third taskprof s offline analysis tool analyzes the profile data and aggregates the data for each static spawn site.
finally it computes asymptotic parallelism and critical work for each spawn site.
static instrumentation and modified libraries.
taskprof provides a modified task parallelism library that constructs the dpst and reads hardware performance counters at the beginning and end of each step node.
taskprof uses static instrumentation to instrument the program with calls to the modified task parallel runtime library.
in the subsequent offline analysis phase taskprof needs to map the dynamic execution information about asymptotic parallelism and critical work to static spawn sites in the program.
hence taskprof instruments the spawn sites to capture the line number and the file name of the spawn site.
taskprof s static instrumentation is currently structured as a rewriter over the abstract syntax tree of the program using the clang compiler front end.
our instrumented libraries and compiler instrumentation enable the programmer to use taskprof without making any changes to the source code.
.
parallel profile execution the goal of the profile execution is to collect fine grained information about the program to enable a subsequent offline computationof asymptotic parallelism.
typically programs are profiled with representative production inputs that have long execution times.
hence a fast profile execution is desirable.
our goal is to ensure that the execution time of the program with and without profiling is similar.
hence taskprof profiles in parallel leveraging multicore processors.
to ensure a parallel profile execution it needs to construct the execution graph in parallel and collect information about the program in a thread safe manner.
the dpst representation for parallel profile execution.
we use the dpst representation to measure work performed by various parts of the program because the dpst can be constructed in parallel.
taskprof constructs the dpst as the program executes the injected static instrumentation and measures the work performed in each step node.
the dpst once constructed allows taskprof to determine the dependencies between tasks.
this finegrained attribution of work to the step nodes in the dpst enables taskprof to compute the parallelism in the program eventually using an offline analysis.
the dpst of the complete task parallel execution has a large number of nodes.
storing the entire dpst in memory during program execution can cause memory overheads and perturb the execution.
to address this issue taskprof does not maintain the entire dpst in memory.
in a library based task parallel programming model a task is always attached to the same thread.
we leverage this property to minimize the footprint of the dpst in memory.
taskprof maintains a small fraction of the nodes that correspond to the tasks currently executing on each thread in memory.
once a step node of a task completes execution the work performed in the step node along with the information about its parent node is written to the profile data file and the dpst node can be deallocated.
as async nodes do not perform any work taskprof writes the information about its parent in the dpst and the spawn site associated with the async node to the profile data file.
in contrast to step and async nodes only parent node information is written to the profile data file for a finish node.
measuring work with hardware performance counters.
to measure the work performed in each step node without performance overhead taskprof uses hardware performance counters.
performance counters are model specific registers available that count various events performed by the hardware using precise event based sampling mechanisms .
these performance counters can be programmatically accessed.
taskprof can use both the number of dynamic instructions and the number of execution cycles to measure the work done in a step node.
measuring execution cycles allows taskprof to account for latencies due to secondary effects such as locality sharing and long latency instructions.
further the operations on these counters are thread safe.
taskprof reads the value of the counter at the beginning and the end of the step node using static instrumentation injected into the program.
it calculates the work performed in the step node by computing difference between the two counter values.
this fine grained measurement of work performed in each step node using hardware performance counters along with the construction of the dpst while executing in parallel allows taskprof to compute a precise yet fast profile of the program.
18a fast causal profiler for task parallel programs esec fse september paderborn germany the profile data file generated at the end of parallel profile execution contains the work done in each step node.
it also contains the information about the parent for each node in the dpst and the spawn site information for each async node.
the left to right sequencing of nodes is implicitly captured by the order of the nodes in the profile data file.
.
offline analysis of the profile data taskprof s offline analysis reconstructs the dpst using the data from the profile execution and computes the work and critical work span for each spawn site in the program.
the construction of the dpst from the profile data is fairly straightforward as it contains information about nodes their parent nodes and the left to right ordering of the nodes.
in this section we describe the computation of work and span for each intermediate node in the dpst given the work performed in the step nodes.
we also describe the process of mapping this dynamic information to static spawn sites.
computing work and critical work for each intermediate node.
in the dpst representation all computation is performed in the step nodes.
the step nodes have fine grained work information from the profile execution.
taskprof needs to compute the total work and the fraction of that work done serially critical work for each intermediate node in the dpst.
to provide meaningful feedback to the programmer taskprof also computes the list of spawn sites that perform critical work and the portion of the critical work performed exclusively by each spawn site.
taskprof computes the total work and the critical work at each intermediate node by performing a bottom up traversal of the dpst.
the total work performed in the sub tree at each intermediate node is sum of the work performed by all the step nodes in the sub tree.
in contrast critical work measures the amount of work that is performed serially.
computing critical work and the set of tasks performing the critical work requires us to leverage the properties of the dpst.
specifically we leverage the following properties of the dpst to compute the critical work.
the siblings of a node in a dpst are ordered left to right reflecting the left to right sequencing in the parent task.
given an intermediate node all the direct step children of the node execute serially.
all the left step or finish siblings of an async node execute serially with the descendants of the async node.
all the right siblings and their descendants of an async node execute in parallel with the descendants of the async node.
using the above properties of the dpst the critical work at an intermediate node will be equal to either the serial work done by all the direct step children and the critical work performed by the finish children or the critical work performed by descendants of an async child and the serial work performed by the left step and finish siblings of the specific async child in consideration.
since any intermediate node in the dpst can have multiple async children taskprof needs to check if any of the async nodes can contribute to the critical work.
for example consider the intermediate node f2in figure a that has two async nodes a1anda2.
the critical work will be the maximum of the work done by the direct step child s4or the critical work by the async child a1 it does not1 function computeworkspan t for each non step node nin bottom up traversal of tdo cn children n n.work x c cnc.work sn stepchildren n fn finishchildren n n.c work x s sns.work x f fnf.c work n.e work x s sns.work x f fnf.e work n.ss list f fnf.ss list for each a asyncchildren n do lsa leftstepsiblings a lfa leftfinishsiblings a llwa x ls lsals.work x lf lfalf.c work ifllwa a.c work n.c work then n.c work llwa a.c work n.e work x s lsas.work x f lfaf.e work n.ss list lf lfalf.ss list a.ss list end if end for ifnis aasync node then n.ss list n.ss list n.s site n.e work end if end for end function figure algorithm to compute the total work work critical work c work exclusive work e work and the spawn sites that perform the critical work ss list for each intermediate node in the dpst.
the function children returns the set of children of the input node.
similarly functions stepchildren finishchildren and asyncchildren return the set of step finish and async child nodes of the input node respectively.
the function leftstepsiblings returns the set of step sibling nodes that occur to the left of the input node in the dpst.
similarly the leftfinishsiblings returns the set of finish sibling nodes to the left of the input node in the dpst.
have any left siblings or the sum of the critical work by the async child a2and the work done by the step node s4 which is the left step sibling of a2.
each async node in the dpst corresponds to a spawn site in the program because async nodes are created when a new task is spawned.
hence taskprof computes the list of spawn sites performing critical work by computing the list of async nodes that contribute to the critical work in the sub tree of the intermediate node.
algorithm to compute work and critical work.
figure provides the algorithm used by taskprof to compute the total work 19esec fse september paderborn germany adarsh yoga and santosh nagarakatte the critical work and the set of spawn sites contributing to the critical work.
the algorithm maintains four quantities with each intermediate node in the dpst total work performed in the sub tree under the node work the critical work performed in the sub tree c work the list of spawn sites that perform the critical work ss list and the part of the critical work that is performed exclusively by the direct children of the node e work .
the exclusive work of a node is equal to sum total of the work performed by the direct step children and the exclusive work performed by the finish children.
we consider the exclusive work performed by a finish node because it is not yet associated with any spawn site.
the exclusive work of the current node will eventually be associated with a spawn site.
the algorithm does not consider the exclusive work of the async children because it is already associated with a spawn site.
the algorithm traverses each node in the dpst in a bottom up fashion.
all step nodes have work information from the profile data.
for any intermediate node the work performed under the sub tree is the sum of the work performed by all its children lines in figure .
for a given intermediate node taskprof initially computes the serial work performed in all the step and finish children as the critical work lines in figure .
for each async child of the current node it checks if the serial work done by the async node and its left siblings is greater than the critical work computed until that point lines in figure .
to compute the set of spawn sites performing critical work each intermediate node also maintains a list of spawn sites and the exclusive work performed by them.
the algorithm initially sets the spawn site list for a node to be the union of spawn site lists of its finish children lines in figure .
whenever an async child contributes to the critical work the spawn site list of the current node is the union of the spawn site list of the async child and the spawn site lists of the finish children that are to the left of the async child line in figure .
when an async child contributes to the critical work the exclusive work of the current node is equal to sum of the work performed by the left step siblings and the exclusive work performed by the left finish siblings of the async child line in figure .
the algorithm adds the spawn site and the exclusive work performed by the current async node to the node s spawn site list lines in figure .
after the algorithm completes traversing the entire dpst the root of the dpst will contain the list of all spawn sites that perform critical work and their individual contribution to the critical work.
the root node also contains information about the total work performed by the program the work that is computed serially by the program and the exclusive work performed under the entry function of the program i.e.
main .
aggregating information about a spawn site.
a single spawn site may be executed multiple times in a dynamic execution.
hence taskprof aggregates information from multiple invocations of the same spawn site.
taskprof computes the aggregate information for each spawn site by performing another bottom up traversal of the dpst at the end.
when it encounters an async node taskprof uses a hash table indexed by the spawn site associated with the async node and adds the total work and critical work to the entry.
when aggregating this information taskprof has to ensure thatit does not double count work and critical work when recursive calls are executed.
in the presence of recursive calls a descendant of an async node will have the same spawn site information as the async node.
if we naively add the descendant s work it leads to double counting as the work and critical work of the current async node already considers the work critical work of the descendant async node.
hence when taskprof encounters an async node in a bottom up traversal of the dpst it checks whether the descendants of the async node have the same spawn site information.
when a descendant with the same spawn site exists it subtracts such a descendant s work and critical work from the entry in the hash table corresponding to the spawn site.
subsequently taskprof adds the work and the critical work of the current async node to the hash table.
profile reported to the user.
for each spawn site in the program taskprof presents the work the critical work the asymptotic parallelism and the percentage of critical work exclusively done by the spawn site.
the asymptotic parallelism of a spawn site is the ratio of the total work and the critical work performed by a spawn site.
the spawn sites are ordered by the percentage of critical work exclusively performed by the spawn site.
figure b illustrates the parallelism profile for the program in figure that has the dpst shown in figure a .
if a spawn site has low parallelism and performs a significant proportion of the critical work then optimizing the task spawned by the spawn site may increase the parallelism in the program.
this profile information provides a succinct description of the parallelism bottlenecks in the program.
causal profiling taskprof reports the set of spawn sites performing critical work to the user which highlight the parallelism bottlenecks in the program.
a programmer can consider these spawn sites to be initial candidates for optimization to reduce serial computation.
reducing critical work and the impact on parallelism.
designing a new optimization or a parallelization strategy that reduces the critical work typically requires effort and time.
a program may have multiple spawn sites that perform similar amount of critical work.
when a set of spawn sites are parallelized to reduce critical work the resultant execution may have new spawn sites whose critical work is similar to the critical work before the optimization.
in such cases an optimization to a spawn site performing critical work may not improve the asymptotic parallelism in the program.
hence programmers would benefit from a causal profile of program that identifies the improvement in asymptotic parallelism when certain regions of the code are optimized.
causal profile with taskprof .a causal profile provides information on improvements in parallelism when certain parts of the code are parallelized or optimized.
taskprof proposes a technique to generate causal profiles for task parallel programs.
the programmer can get an accurate estimate of the improvement in asymptotic parallelism by reducing the serial work in a region of the program using taskprof s causal profile.
taskprof provides such an estimate even before the programmer has designed a concrete strategy to parallelize or reduce the serial work in the region of code under consideration.
in summary a causal profile enables the programmer 20a fast causal profiler for task parallel programs esec fse september paderborn germany to identify parts of the program that really matter in increasing the asymptotic parallelism.
figure c provides the causal profile for the program in figure where the regions under consideration are demarcated by causal begin and causal end .
next we describe how taskprof generates a causal profile leveraging the accurate performance model of a task parallel execution created with the fine grained attribution of work and the dpst.
static code annotations.
to generate causal profiles the programmer annotates a static region of code that is considered for parallelization and the expected improvement to the critical work from parallelization.
the programmer can provide multiple regions as candidates for optimization.
taskprof generates a causal profile that estimates the improvement in parallelism when all annotated regions are optimized.
in addition taskprof also generates a causal profile for optimizing each region in isolation.
figure illustrates the regions of code annotated for causal profiling with causal begin and causal end annotations.
if the programmer does not specify the amount of expected improvement for the considered region taskprof assumes a default value.
if the annotations are nested the outermost region of code is considered for estimating the benefits.
profile execution and attribution of work.
taskprof uses these annotations profiles the program constructs the dpst to attribute work to various regions and provides the estimated improvement in asymptotic parallelism from optimizing the annotated regions.
during profile execution taskprof measures the work performed in the annotated part of the step node and also in parts of the step node that have not been annotated.
hence each step node can have multiple work measurements corresponding to static regions with and without annotation.
taskprof accomplishes it by reading the performance counter value at the beginning and the end of the each dynamic region.
taskprof maintains a list of work values for each step node and writes it to the profile data file.
algorithm to generate causal profiles.
the algorithm to compute the causal profile is similar to the work and span algorithm in figure .
it takes the dpst as input and a list of anticipated improvements for the annotated regions.
the algorithm outputs a causal profile that computes the improvement in asymptotic parallelism of the whole program for the specified improvements of the annotated regions.
the causal profile algorithm performs a bottomup traversal of the dpst similar to the work and span algorithm in figure .
however the causal profiling algorithm does not track spawn sites and computes the whole program s work and critical work.
the key difference with the causal profiling algorithm is the manner in which it handles the work done by the step nodes which have regions corresponding to user annotations.
specifically taskprof maintains a list of annotated and non annotated regions executed with each step node and the amount of work performed in each region.
to estimate the effect of optimizing parallelizing the annotated region we reduce the critical work contribution of the annotated region by the user specified optimization factor while keeping the total work performed by the regions unchanged.
the output of the causal profiling algorithm is a list that provides the asymptotic parallelism for each anticipated improvement factor for the regions under consideration.illustration.
after analyzing the parallelism profile in figure b for the program in figure the programmer has identified two regions of code lines and lines in figure for optimization.
the regions are annotated with causal begin and causal end annotations to demarcate the beginning and the end.
during execution the region at lines is executed once and is represented by step node s0in figure a .
in contrast the region at lines is executed twice and is represented by step nodes s5ands6in figure a .
in this example the entire step node corresponds to the annotated region.
in general a step node may have multiple annotated and non annotated regions.
to generate a causal profile the critical work performed by nodes s0 s5 ands6are decreased by and and its impact on whole program parallelism is computed.
figure c provides the causal profile with the annotated regions which reports that the asymptotic parallelism in the program increases when those two regions are optimized.
experimental evaluation this section describes our prototype our experimental setup and an experimental evaluation to answer the following questions is taskprof effective in identifying parallelism bottlenecks?
is taskprof s parallel profile execution faster than serial profilers?
is taskprof effective in minimizing perturbation in the profile execution?
is taskprof usable by programmers?
prototype.
we have built a taskprof prototype to profile task parallel programs using the intel threading building blocks tbb library .
the prototype provides a tbb library that has been modified to construct the dpst measure work done in step nodes using hardware performance counters and track file name and line information at each spawn site.
the prototype also handles algorithms for geometric decomposition such as parallel for and parallel reduce .
the prototype also includes a clang compiler pass that automatically adds line number and file name information to the tbb library calls which enables the programmer to use the modified library without making any source code changes.
hence the modified tbb library can be linked to any tbb program.
our prototype adds approximately lines of code to the intel tbb library to perform various profiling operations.
the taskprof prototype is open source .
applications used for evaluation.
we evaluated taskprof using a collection of twenty three tbb applications which include fifteen applications from the problem based benchmark suite pbbs all five tbb applications from the parsec suite and three tbb applications from the structured parallel programming book .
the pbbs applications are designed to compare different parallel programming methodologies in terms of performance and code.
we conducted all experiments on a .1ghz core intel x86 xeon server with gb of memory running bit ubuntu .
.
.
we measured wall clock execution time by running each application five times and use the mean of the five executions to report performance.
we use the perf events module in linux to programmatically access hardware performance counters.
rq1 is taskprof effective in identifying parallelism bottlenecks?
we used taskprof to identify parallelism bottlenecks in 21esec fse september paderborn germany adarsh yoga and santosh nagarakatte table applications used to evaluate taskprof .
we provide a short description of the application the speedup obtained on a core machine when compared to serial execution time the asymptotic parallelism reported by taskprof the number of annotated regions that provides maximum parallelism with causal profiling and the asymptotic parallelism when the critical work in the annotated regions is optimized by which we list as causal parallelism.
application description speedup parallelism of regionscausal parallelism blackscholes stock option pricing .
.
.
bodytrack tracking of a human body .
.
.
fluidanimate simulate fluid dynamics .
.
.
streamcluster clustering algorithm .
.
.
swaptions price a portfolio .
.
.
convexhull convex hull .
.
.
delrefine delaunay refinement .
.
.
deltriang delaunay triangulation .
.
.
karatsuba karatsuba multiplication .
.
.
kmeans k means clustering .
.
.
nearestneigh k nearest neighbors .
.
.
raycast triangle intersection .
.
.
sort parallel quicksort .
.
.
compsort generic sort .
.
.
intsort sort key value pairs .
.
.
removedup remove duplicate value .
.
.
dictionary batch dictionary opers .
.
.
suffixarray sequence of suffixes .
.
.
bfirstsearch breadth first search .
.
.
maxindset maximal independent set .
.
.
maxmatching maximal matching .
.
.
minspanforest minimum spanning forest .
.
.
spanforest spanning tree or forest .
.
.
all the twenty three applications.
table provides details on applications used their speedup on a core machine compared to serial execution the asymptotic parallelism reported by taskprof the number of regions that we identified using taskprof to increase asymptotic parallelism and the resultant asymptotic parallelism from causal profiling when the critical work in the identified regions is decreased by .
typically asymptotic parallelism of a program should be at least or more than the anticipated speedup on a machine to account for scheduling overheads .
taskprof s profile shows that some applications in table have reasonable asymptotic parallelism which accounts for a reasonable speedup on a core machine.
for example fluidanimate application has an asymptotic parallelism of .
which is the maximum possible speedup when the program is executed on a large number of machines.
the fluidanimate application exhibits a speedup of .
compared to a serial execution when the program was executed on a core machine.
table also shows that we were able to identify a small number of code regions which when optimized provide a significant increase in asymptotic parallelism.
taskprof s profile information on spawn sites performing critical work and the causal profiling strategy was instrumental in identifying the specific regions of code as candidates for increasing asymptotic parallelism.
the application maxmatching already had a large amount of asymptotic parallelism and we could not find any region that increases parallelism.
in summary taskprof enabled us to identify a set of code regions that can increase asymptotic parallelism significantly in almost all our applications.
once we identified code regions that critical path percentrefinetime.c 59refine.c .
.
.26hulltime.c 55parallel ism7.
.
.
.11optimization factorparallel ism50x200x400x47.
.
.84critical path percent85.
.
.
.68msttime.c 77spec for.h 82io.h 71graphio.h 167100x49.78parallel ism33.
.
.
.87critical path percent33.
.
.
.58file linemsttime.c 77spec for.h 82samplesort.h 81graphio.h 167i.
minimum spanning forest .
.
.
.
.
.
.
.
.
.
.3hulltime.c 46hull.c 117hull.c .
.
.
.
.
.
.
.
.82hulltime.c 55sequence.h 359io.h 71geoio.h .
.
.
.
.
.
.
.24refinetime.c 59refine.c 260topfromt.c .
.
.
.
.
.
.
.
.22refine.c 257sequence.h 365refine.c 246file line file lineparallel ismparallel ism50x200x400x100xparallel ismcritical path percentfile line parallel ism a original parallelism profile b causal profile c parallelism profile after optimizationoptimization factor a original parallelism profile b causal profile c parallelism profile after optimizationii.
convex hull file lineparallel ism50x200x400x100xoptimization factorparallel ismcritical path percentfile lineiv.
delaunay refinement a original parallelism profile b causal profile c parallelism profile after optimizationcritical path percent critical path percentdeltime.c 55geoio.h .
.
.
.
.
.
.
.
.
.
.07deltime.c 55delaunay.c 385io.h .
.
.
.
.
.
.
.
.46sequence.h 365io.h 179delaunay.c 284parallel ismfile lineparallel ism50x200x400x100xoptimization factorparallel ismcritical path percentfile lineiii.
delaunay triangulation a original parallelism profile b causal profile c parallelism profile after optimization critical path percentbscholes.c 464bscholes.c .
.
.
.
.
.
.72bscholes.c .
.
.
.
.
.52bscholes.c 212bscholes.c 415bscholes.c 145parallel ismfile lineparallel ism50x200x400x100xoptimization factorparallel ismcritical path percentfile linev.
blackscholes a original parallelism profile b causal profile c parallelism profile after optimizationfigure the original parallelism profile the causal profile for the annotated regions and final parallelism profile generated by taskprof after annotated regions were parallelized for each of the five applications.
we list the top four spawn sites from taskprof s parallelism profile.
line with a in the profile corresponds to the main function and reports the parallelism for the entire program.
can increase asymptotic parallelism we designed concrete parallelization strategies to reduce the critical work for five applications which increased the asymptotic parallelism and the speedup of the program.
we describe them below.
improving the speedup of the minspanningforest application.
this pbbs application computes the minimum spanning forest of the input undirected graph.
the program has a speedup of .
over serial execution on a core machine.
the parallelism profile generated by taskprof is shown in figure i a which reports that the parallelism in the program main function atmsttime.c is .
.
the main function performs of the serial work in the program.
we identified two regions of code using annotations for causal profiling in the main function.
figure i b presents the causal profile generated by taskprof which shows 22a fast causal profiler for task parallel programs esec fse september paderborn germany the increase in asymptotic parallelism in the program on potentially optimizing these two regions.
on further investigation of the code regions we realized that annotated regions were performing a serial sort.
we replaced them with a parallel sort function which increased the asymptotic parallelism to .
from .
.
figure i c reports the profile after our parallel sort optimization.
the speedup of the program increased from .
to .
.
improving the speedup of the convex hull application.
this pbbs application computes the convex hull of a set of points using a divide and conquer approach .taskprof s profile shown in figure ii a reveals that the program has an asymptotic parallelism of .
for the whole program.
as expected it did not exhibit any speedup.
figure ii a shows that of the critical work is performed by the spawn site at hulltime.c .
we annotated two regions corresponding to that spawn site which performed sequential read and write operations of the input and output files respectively.
taskprof s causal profile showed that it would increase the parallelism to .
.
subsequently we annotated two additional regions of code corresponding to the spawn site performing the next highest critical work hull.c in figure ii a .
the causal profile shown in figure ii b shows that asymptotic parallelism increases significantly when all the four regions are optimized.
we parallelized a loop at spawn site hull.c using parallel for and parallelized i o at spawn site hulltime.c .
these optimizations increased the parallelism to .
see figure ii c and the speedup of the whole program increased from .
to .
.
improving the speedup of delaunay triangulation.
this pbbs application produces a triangulation given a set of points such that no point lies in the circumcircle of the triangle.
the program has an asymptotic parallelism of .
see figure iii a for the entire program and exhibits little speedup.
the spawn site at deltime.c performs of the critical work.
when we looked at the source code we found that the program is structured as a collection of parallel for constructs interspersed by serial code.
we annotated five regions of code between the invocations ofparallel for .
the causal profile in figure iii b shows that the asymptotic parallelism increases significantly by optimizing the annotated regions.
we parallelized the annotated regions which had serial for loops using parallel for while ensuring they operate on independent data.
the profile for the resultant program is shown in figure iii c .
the parallelism increased to .
and the speedup increased from .
to .
.
improving the speedup of delaunay refinement.
this pbbs application takes a set of triangles that form a delaunay triangulation and produces a new triangulation such that no triangle has an angle less than a threshold value.
taskprof s profile for this program reports an asymptotic parallelism of .
see figure iv a and it had a speedup of .
.
similar to delaunay triangulation this program also had a set of serial code fragments in between parallel for calls.
we identified seven regions of such serial code and annotated them.
taskprof s causal profile shown in figure iv b indicates that optimizing all these seven regions can increase asymptotic parallelism.
we parallelized the serial for loops in these seven regions using parallel for which increased the 0x5x10x speedup blackscholesbodytrackfluidanimatestreamclusterswaptionsconvexhulldelrefinedeltriangkaratsubakmeans nearestneighraycastsort compsortintsort removedupdictionarysuffixarraybfirstsearchmaxindsetmaxmatchingminspanforestspanforestgeomeanfigure speedup of taskprof s parallel profile execution when compared to serial profile execution.
asymptotic parallelism to .
see figure iv c and the speedup increased from .
to .
.
improving the speedup of blackscholes.
this application from the parsec suite computes the price of a portfolio of options using partial differential equations.
it has low asymptotic parallelism for the entire program see figure v a .
this program has a single parallel for that has reasonable parallelism of .
.
however the spawn site at bscholes.c is performing of the program critical work.
our examination of the code revealed that it was reading and writing serially.
we split the input and output into multiple files and parallelized the input output operations which increased the parallelism to .
and the speedup increased from .
to .
.
in summary taskprof enabled us to quantify asymptotic parallelism in the program and its causal profiling strategy enabled us to identify specific regions of code that can increase parallelism.
rq2 is taskprof s parallel profile execution faster than serial profile execution?
taskprof s profile execution executes in parallel compared to prior profilers which execute serially.
to quantify the benefits of parallel profile execution we designed a serial version of taskprof by pinning the execution of the program to a single core.
this is an approximation of serial profiling as tbb programs do not have serial semantics.
figure reports the speedup of a parallel taskprof profile execution compared to a serial profile execution.
on average taskprof s parallel profile execution is .
faster than serial profile execution.
the speedup from a parallel profile execution is proportional to the amount of parallelism in the application.
rq3 is taskprof effective in minimizing perturbation in the profile execution?
taskprof uses hardware performance counters to perform fine grain attribution of work and to minimize perturbation.
the average performance overhead of taskprof s profile execution compared to the parallel execution of the program without any profiling instrumentation is .
a major fraction of this performance overhead is attributed to system calls to read hardware performance counters.
taskprof s profile execution is an order of magnitude faster than instrumenting each dynamic instruction through compiler instrumentation which exhibited overheads of for the applications in table .
hence taskprof minimizes perturbation even with fine grained attribution of work.
rq4 is taskprof usable by programmers?
we conducted a user study to evaluate the usability of taskprof .
the user study had thirteen participants twelve graduate students and one senior undergraduate student.
among them two students had years 23esec fse september paderborn germany adarsh yoga and santosh nagarakatte of experience in parallel programming five students had some prior experience four students had passing knowledge and two students had no prior experience with parallel programming.
the total duration of the user study was four hours.
to ensure that every student had some knowledge in parallel programming we provided a hour tutorial on task parallelism and on writing and debugging task parallel programs using intel tbb.
we gave multiple examples to demonstrate parallelism bottlenecks.
after the tutorial the participants were given a total of four applications and were asked to identify parallelism bottlenecks without using taskprof in a one hour time period.
among them three applications minspanforest convexhull and blackscholes were from table and a treesum application was similar to the example in figure .
we chose these applications as they had varying levels of difficulty in diagnosing parallelism bottlenecks.
we asked the participants to identify the static region of code causing the bottleneck and record the time they spent to analyze each program.
they were not required to design any optimization.
some participants used gprof and others used fine grained wall clock based timing for assistance.
at the end of the time period twelve of them did not correctly identify parallelism bottlenecks in any of the four applications.
one participant who had years of experience in parallel programming identified the bottleneck in one minspanforest application.
subsequently after the first part we gave a brief tutorial of taskprof on a simple example program.
the participants were then asked to identify bottlenecks in the four applications using taskprof within an hour.
using taskprof seven participants found the parallelism bottleneck in all the four applications one participant found the bottleneck in three of them four participant found the bottleneck in two of them and one participant did not find the bottleneck in any application.
among the participants who identified at least one bottleneck for any application it took them minutes on average per application to identify the bottleneck using taskprof .
the participants indicated that once they became familiar with the tool by identifying a bottleneck in one application subsequent tasks were repetitive.
in summary our user study suggests that programmers can quickly identify parallelism bottlenecks using taskprof .
threats to validity.
our user study uses a repeated measures experiment which can introduce order effects.
students had an opportunity to study the code and attempt to optimize it during the first phase before they were given taskprof .
related work there is a large body of work to identify parallelism bottlenecks.
these include techniques to address load imbalances scalability bottlenecks visualizing bottlenecks synchronization bottlenecks and data locality bottlenecks .
data locality and synchronization bottlenecks increase serial work.
hence taskprof will report asymptotic parallelism in their presence.
in contrast to prior proposals taskprof also estimates the improvement in parallelism with causal profiling.
next we focus on the closest related work.
profiling tools for task parallel programs.
profiling tools such as hpctoolkit and intel vtune amplifier can analyzea program s performance on various parameters using hardware performance counters.
hpctoolkit also has metrics to quantify idleness and the scheduling overhead in cilk programs that is specific to a machine.
they do not compute the asymptotic parallelism in the program.
they also do not identify code that matters with respect to asymptotic parallelism.
cilkview computes the whole program asymptotic parallelism.
cilkprof computes asymptotic parallelism per spawn site using an online algorithm.
however these profilers execute the program serially which is only possible with cilk programs with c elision .
many task parallelism frameworks including intel tbb do not have serial semantics which limits their use.
further executing the profiler serially can cause high overheads.
unlike taskprof they also cannot estimate the benefits of optimizing specific regions of code.
performance estimation tools.
an early profiling technique proposed slack which is a metric that estimates the improvement in execution time through critical path optimizations for a specific machine model.
kremlin identifies regions of code that can be parallelized in serial programs by tracking loops and identifying dependencies between iterations.
kismet builds on kremlin to estimate speedups for the specific machine on which the serial program is executed.
these techniques are tied to a specific machine and cannot estimate asymptotic parallelism improvements.
our work is inspired by coz a causal profiler for multithreaded programs that automatically identifies optimization opportunities and quantifies their impact on a metric of interest such as latency or throughput.
it runs periodic experiments at runtime that virtually speed up a single randomly selected program fragment.
virtual speedups produce the same effect as real speedups by uniformly slowing down code executing concurrently with the fragment causing the fragment to run relatively faster.
in a task parallel context it is not possible to slow down all active tasks.
further slowing down threads does not measure the impact of the region as work stealing dynamically balances the load.
further coz s virtual speedups are specific to a particular machine.
taskprof though similar in spirit addresses the above challenges and proposes a causal profiler that leverages the dynamic execution structure and estimates improvements in asymptotic parallelism.
hence taskprof s profile is not specific to a single machine and enables the development of performance portable code.
conclusion taskprof identifies parallelism bottlenecks by performing a lowoverhead yet fine grained attribution of work to various parts of the program using the dynamic execution structure of a task parallel execution.
taskprof reports asymptotic parallelism and serial work performed at each spawn site.
taskprof s causal profile estimates the improvements in parallelism when regions of code annotated by the programmer are optimized.
we have identified bottlenecks and improved the speedup in numerous intel tbb applications.
our user study shows that developers can quickly identify parallelism bottlenecks using taskprof .
a artifact description thetaskprof prototype is open source and is publicly available at the artifact contains the 24a fast causal profiler for task parallel programs esec fse september paderborn germany following folders ptprof lib contains the implementation of the profiler for intel tbb programs tprof tbb lib contains the modified intel tbb library and tests contains simple programs used to illustrate taskprof s usage.
in addition we also separately provide applications that we used to evaluate taskprof .
a. setup requirements.
taskprof must be executed on a modern linux machine that supports hardware performance counters.
the following command can be used to check if hardware performance counters are supported on the machine.
dmesg grep pmu when the output of the command contains performance events unsupported... then the machine does not support performance counters.
taskprof will not be functional on such a machine.
taskprof uses the perf events module in linux to read performance counters.
to check the support for perf events on the machine users can check the existence of perf event paranoid file in the proc sys kernel folder ls proc sys kernel perf event paranoid installation.
the artifact contains a bash script to automate the installation of taskprof and the modified tbb library.
the script uses perf command line utility in linux to check if hardware performance counters are supported.
to install perf on a ubuntu linux machine execute the following command sudo apt get install linux tools common linux tools generic linux tools asciigrave.varuname r asciigrave.var let tp root refer to the base directory of the artifact.
to install taskprof execute the following commands.
cd tp root source build.sh a successful build will create taskprof s shared libraries and also setup the appropriate environment variables.
a. usage we illustrate the usage of taskprof using the tree sum program in thetests directory.
to generate a profile for the program execute cd tp root tests tree sum make .
tree sum the profile execution will create data files which can be analyzed using taskprof s profile analysis tool.
to generate the parallelism profile from the profile data file execute tp genprof gentprof here tp genprof is an environment variable that is already setup by the build script.
the parallelism profile will be generated in the file ws profile.csv .
the first row specifies the parallelism of the entire program and the percentage of critical work performed by the main function.
other rows specify the parallelism and the critical work percentage for each spawn site in the program.
to illustrate causal profiling we have already annotated a region in filetreemaker.h .
the causal profile estimating the improvements in parallelism for the entire program on optimizing the annotatedregion is generated in the file region all.csv .
it reports the increase in parallelism in the program when the critical work in the annotated region is reduced by and .
a. reproducing results effectiveness.
using taskprof we were able to optimize five applications.
these applications and their optimized versions can be downloaded from we provide the optimized version of each application in a separate directory.
assuming the applications have been extracted to the benchmarks folder within tp root directory each application can be compiled and profiled with taskprof as follows cd tp root benchmarks benchmark make sh run.sh the optimized version of each application can be compiled and executed similarly.
to ease this process we also provide a python script to compile and execute all applications and their optimized versions.
cd tp root benchmarks python run bmarks opt.py report opt.txt the parallelism profile before the optimization and the causal profile can in found in the ws profile.csv andregion all.csv files in the folder corresponding to the application.
the parallelism profile after optimization can be found in the ws profile.csv file in the folder containing the optimized version of the application.
performance.
to reproduce the results comparing taskprof s parallel profile execution to serial profile execution download all benchmark applications from and extract them to the benchmarks folder within the tp root directory.
the script included in the artifact uses jgraph a postscript graphing tool to generate graphs.
to install jgraph on ubuntu execute the following command sudo apt get install jgraph to convert the postscript graph generated by jgraph to a pdf we use epstopdf .
to install epstopdf on ubuntu execute the following command sudo apt get install texlive font utils the artifact includes a python script that executes taskprof on all the benchmarks and generates the speedup graph as output.
profile all applications using the script as shown below cd tp root benchmarks python run bmarks speedup.py report.txt many applications take a reasonable amount of time to complete execution.
we suggest using the nohup command to run the script.
our python script reproduces the speedup graph in a pdf file named speedup graph.pdf .