automatically assessing code understandability how far are we?
simone scalabrino gabriele bavotay christopher vendomez mario linares v squezx denys poshyvanykz and rocco oliveto university of molise italy yuniversit della svizzera italiana usi switzerland zthe college of william and mary usa xuniversidad de los andes colombia abstract program understanding plays a pivotal role in software maintenance and evolution a deep understanding of code is the stepping stone for most software related activities such as bug fixing or testing.
being able to measure the understandability of a piece of code might help in estimating the effort required for a maintenance activity in comparing the quality of alternative implementations or even in predicting bugs.
unfortunately there are no existing metrics specifically designed to assess the understandability of a given code snippet.
in this paper we perform a first step in this direction by studying the extent to which several types of metrics computed on code documentation and developers correlate with code understandability.
to perform such an investigation we ran a study with participants who were asked to understand eight code snippets each.
we collected a total of evaluations aiming at assessing the perceived understandability the actual level of understanding and the time needed to understand a code snippet.
our results demonstrate that none of the existing and new metrics we considered is able to capture code understandability not even the ones assumed to assess quality attributes strongly related with it such as code readability and complexity.
index terms software metrics code understandability empirical study negative result i. i ntroduction code understanding is an important foundation to any coderelated activity.
developing new features fixing bugs or refactoring code requires a deep understanding of the involved code components and of their relationships.
unsurprisingly recent studies showed that developers spend most of their time understanding code .
while the importance of code understandability is undisputed for maintenance related activities there is still a lack of metrics to objectively assess the understandability of a given piece of code.
indeed our knowledge of factors affecting positively or negatively code understandability is basically tied to common beliefs or is focused on the cognitive process adopted when understanding code .
for example we commonly assume that code complexity has a direct impact on developers ability to understand the code but we do not have strong empirical evidence supporting such a belief yet.
another aspect possibly related to the understandability of a piece of code is represented by its readability.
in the last years researchers have proposed several metrics and models for assessing code readability .
these metrics and models are generally evaluated by assessingtheir correlation with the readability perceived by developers.
this typically means that the developers participating in the evaluation are just asked to read the code and assess its readability on a given scale e.g.
from low to high readability .
however the perceived readability is something different from the actual understandability of the code a developer could find a piece of code readable but still difficult to understand for example due to unknown apis used.
let us consider the code fragment listed below asynchttpclient client new asynchttpclient string cookies cookiemanager .
getinstance .
getcookie url log .e tag cookies client .
addheader sm.
cookie cookies all readability metrics models existing in the literature would consider this snippet of code as highly readable since it is very short it has a low complexity and uses meaningful and intuitive identifiers.
nevertheless this snippet of code is not necessarily easy to understand for any given developer because the used apis could be unknown to her and even poorly documented.
in other words even if a developer can understand that getcookie url returns a cookie she may not understand all the consequences of this call if she does not know the specifications behind that api.
while we have possible proxies for code understandability such as code complexity and readability we i do not know whether these proxies actually correlate with the effort required to understand a piece of code and ii do not have a metric able to provide an estimation for code understandability.
previous attempts to define a code understandability model have not been empirically evaluated consider understandability as a factor in a quality model or measure understandability at the level of a whole system .
having a metric to estimate the effort required to understand a piece of code would have a strong impact on several software engineering tasks.
for example such a metric would be useful to i improve the estimation of the time needed to fix a bug the lower the understandability the higher the time to fix the bug ii create search based refactoring recommender systems using code understandability as a fitness function or iii assess the quality of code changes during code reviews.
in this paper we make the first step towards the definition of a metric able to capture the understandability of a given piece of code by investigating to what extent a number of different existing and new metrics correlate with code understandability.
.
c ieeease urbana champaign il usa technical research417 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
we consider three types of metrics code related metrics.
we consider metrics characterizing the code to be understood classic code metrics such as loc and cyclomatic complexity as well as metrics used to assess the readability of a code component such as text coherence and the code indentation documentation related metrics.
the availability of documentation can clearly impact the understandability of a given piece of code.
for this reason we consider documentation related metrics e.g.
the availability of external documentation for the given code .
nine of them are introduced in this paper.
developer related metrics.
we consider five metrics related to the developer s experience.
while developerrelated metrics could not be used to define an objective measure for code understandability they are still useful in our study to assess to what extent the developer experience and background play a role during comprehension.
we conducted a study involving participants to assess the correlation of the aforementioned code related documentation related developer related metrics with the understandability of code snippets.
participants were required to understand eight code snippets each for a total of data points.
we adopted the following experimental design to consider both the perceived and the actual code understandability.
we asked participants to carefully read and to fully understand each snippet.
participants could at any moment select the option i understood the snippet or i cannot understand the snippet .
this provided us with a first classification of snippets that are perceived by the participants as either understandable or not understandable .
also since we monitored the time spent before selecting one of the two options we can also measure the understandability effort i.e.
the time required to understand a code snippet .
finally when the participants clicked on i understood the snippet we verified their actual level of understanding by asking questions about the code snippet.
the collected dataset as opposed to the ones used for evaluating code readability metrics see provides indications not only about the perceived understandability but also about the actual understandability.
our extensive statistical analysis provided us with what can be defined as a negative empirical result none of the considered metrics exhibit a significant correlation with the understandability of code snippets.
this holds for both the perceived and the actual understandability.
this result was quite surprising for us especially considering that several metrics e.g.
code complexity and readability are supposed to influence the understandability of code.
ii.
b ackground r elated work in this section we describe metrics and models that have been proposed to measure code readability.
all these metrics have been included in our study.
afterwards we briefly describe related work presenting metrics for measuring understandability at system level as a single quality attribute and as part of a quality model.a.
code readability identifiers and comments play a crucial role in program comprehension since developers express domain knowledge through the names that they assign to the code entities at different levels i.e.
packages classes methods variables .
thus source code lexicon impacts the psychological complexity of a program .
another aspect that also contributes to the readability and potentially understandability of source code are structural aspects such as indentation code entities length and visual spatial aspects such as syntax highlighting code formatting and visual areas covered by code entities .
all the aforementioned aspects have been used as features in binary classifiers able to predict the readability of code snippets .
in the model by buse and weimer source code structural aspects e.g.
number of branches loops operators blank lines comments represent the underlying features in the classifier.
the model was trained a priori on small snippets the snippets were tagged manually as readable or non readable by human annotators.
the reported results provide evidence that readability can be estimated automatically.
posnett et al.
proposed a model based on a reduced set of the features introduced by buse and weimer.
an empirical evaluation conducted on the same dataset used by buse and weimer indicated that the model by posnett et al.
is more accurate than the one by buse and weimer.
dorn introduced a readability model which relies on a larger set of features grouped in four categories visual spatial alignment and linguistic .
this larger set of features highlights the fact that structural aspects are not the only ones that should be considered for code readability aspects representing and modeling how the code is read on the screen such as syntax highlighting variable naming standards and operators alignment should be also considered.
dorn trained and validated the model on a new dataset including programs in java python and cuda for a total of snippets.
such a model achieved a higher accuracy as compared to the one by buse and weimer.
scalabrino et al.
proposed and evaluated a set of features based entirely on source code lexicon analysis e.g.
consistency between source code and comments specificity of the identifiers textual coherence comments readability .
the model was evaluated on the two datasets previously introduced by buse and weimer and dorn and on a new dataset composed by java snippets manually evaluated by nine developers.
the results indicated that combining the features i.e.
structural textual improves the accuracy of code readability models.
b. software code understandability while readable code might directly impact program comprehension code readability metrics are not sufficient to measure to what extent the code allows developers to understand its purpose relationships between code entities and the latent semantics at the low level e.g.
statements beacons motifs authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
and high level structures e.g.
packages classes .
program understanding is a non trivial mental process that requires building high level abstractions from code statements or visualizations models .
there have been several metrics designed to evaluate software understandability by focusing on complexity as well as source level metrics.
linet al.
proposed a model for assessing understandability by building an understandability matrix from fuzzy maximum membership estimation for population of fog index comment ratio the number of components cfs halstead complexity and dmsp.
the authors then used pca and factor analysis to get the weights for the column vectors which can be multiplied by the matrix to get the synthesis vector of understandability.
finally the understandability is calculated by using the fuzzy integral.
the authors did not empirically evaluate the proposed metric.
misra and akman performed a comparative study between existing cognitive complexity measures and their proposed measure cognitive weight complexity measure cwcm which assigns weights to software components by analyzing their control structures.
the authors performed a theoretical validation of these metrics based on the properties proposed by weyuker .
they found that only one metric cognitive information complexity measure cicm satisfied all nine properties while the others satisfied seven of the nine.
thongmak et al.
considered aspect oriented software dependence graphs to assess understandability of aspectoriented software while srinivasulu et al.
used rough sets and rough entropy to filter outliers when considering the following metrics fog index comment ration the number of components cfs halstead complexity and dmsc.
these metrics are computed at system level for nine projects and subsequently the rough entropy outlier factor was calculated for the metrics to identify the outliers which correspond to either highly understandable or not understandable software based on the metric values.
capiluppi et al.
proposed a measure of understandability that can be evaluated in an automated manner.
the proposed measure considers i the percentage of micromodules i.e.
the numbers of files that are within the macromodules i.e.
the directories and ii the relative size of the micro modules.
the authors calculated the proposed measure on the history of open source projects finding that understandability typically increased during the life cycle of the systems.
yet no evaluation is provided for such a measure.
understandability has also been a factor in quality models to assess software maintainability.
aggarwal et al.
investigated the maintainability of software and proposed a fuzzy model which is composed of three parts i readability of code ii documentation quality and iii understandability of the software.
to quantify understandability the authors utilize a prior work that defines language of software as the symbols used excluding reserved words.
the authors constructed rules based on the ranges of the three factors to determine maintainability.similarly chen et al.
investigated the cocomo ii software understandability factors by conducting a study with six graduate students asked to accomplish maintenance tasks and found that higher quality structure higher quality organization and more self descriptive code were all correlated with less effort spent on the tasks which leads to high maintainability.
bansiya and davis proposed a model where metrics are related to several quality attributes including understandability.
in terms of understandability the model considers encapsulation and cohesion to have positive influences while abstraction coupling polymorphism complexity and design size have a negative influence.
the authors validated the model by analyzing several versions of two applications and found that understandability decreases as a system evolves with many new features.
additionally evaluators analyzed versions of a project and the authors found a correlation between the evaluators overall assessment of quality and the models assessment for out of evaluators.
it is worth noting that we do not consider the above discussed understandability metrics in our study since they are defined at systemlevel i.e.
they provide an overall indication of the system understandability while we are interested in studying whether it is possible to measure the understandability of a given code snippet as already done in the literature for code readability.
instead we included in our study the metrics used by kasto and whalley to study the understandability of code snippets in an educational context.
specifically kasto and whalley analyzed the performance of students in their final examination for the java programming course and they correlated their results with five metrics.
several studies have explored software understandability and program comprehension with either students or practitioners.
shima et al.
considered the understandability of a software system by assessing the probability that a system can be correctly reconstructed from its components .
the authors asked eight students to reconstruct a system and the results suggest that faults tend to occur in hard to understand files or very simple files.
roehm et al.
performed an observational study with developers to identify the steps developers perform when understanding software and the artifacts they investigate .
the authors found that developers are more inclined towards relying upon source code as well as discussing with colleagues over utilizing the documentation.
the authors also identified some behaviors that improve comprehension such as consistent naming conventions or meaningful names.
understandability has been mostly analyzed from the perspective of i the quality attribute at the software level i.e.
understandability as the the capability of the software product to enable the user to understand whether the software is suitable and how it can be used for particular tasks and conditions of use and ii the theories challenges and models for program understanding at cognitive levels .
however as of today we still lack models for assessing code understandability at snippet level similarly to code readability.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the only work we found that relates to a code understandability model is based on complexity and size source code level metrics .
iii.
c andidate predictors for code understandability understandability is a multifaceted property of source code and as well as readability is subjective in nature.
in readability the subjectivity is represented by personal taste and habits while in understandability it lies in the previous knowledge of a developer and in her mental models .
consider a method of an android activity in a mobile app its understandability might be high for an android developer while it could be low for a java developer with no experience in android.
in this section we briefly discuss the metrics considered in our study aimed at assessing their ability to capture the understandability of a given piece of code.
table i shows the complete list of metrics rows contain the basic metrics and columns indicate how the metrics are aggregated e.g.
the identifiers length for a given code snippet is computed as suggested by previous work as the average and as the maximum length of the identifiers used in the snippet .
we report in boldface the new metrics introduced in this study.
in the following subsections we discuss the considered metrics grouped by their type.
table i candidate predictors for code understandability .
metric non aggr eg.
min avg max dft visual areacodecyclomatic comp.
nested blocks parameters statements assignments blank lines characters commas comments comparisons conditionals identifiers keywords literals loops numbers operators parenthesis periods spaces strings words indentation length identifiers length line length aligned blocks ext.
of alig.
blocks entropy loc v olume nmi nm itid tc readability imsq docscr cic cicsyn midq aedq devseap pegen pespec a. code related metrics most of the metrics considered in our study assess source code properties.
we include the five metrics used by kasto and whalley cyclomatic complexity which estimates the number of linear independent paths of the snippet average number of nested blocks which measures the average codeblock nesting in the snippet number of parameters number of statements andnumber of operands i.e.
number of identifiers.
we also include in this category all the code related readability metrics defined in the literature .
these include the ones by buse and weimer assessing properties for a single line of code e.g.
number of identifiers orline length and then aggregated with the maximum and or the average to work at the level of code snippet .
lines of code loc token entropy andhalstead s volume are used by posnett et al.
in the context of readability prediction.
dorn presents a variation to the basic metrics introduced by buse and weimer measuring the bandwidth of the discrete fourier transform dft of the metrics theabsolute and the relative area of characters belonging to different token categories e.g.
identifiers keywords or comments the alignment of characters through different lines and the number of identifiers containing words belonging to an english dictionary.
note that the area related metrics defined by dorn are computed both in an absolute way e.g.
total area of comments and in a relative way e.g.
area of comments divided by area of strings .
these variants are not reported in table i due to space constraints but are considered in our study and listed in our replication package .
scalabrino et al.
define narrow meaning identifiers nmi number of meanings nm identifiers terms in dictionary itid and textual coherence tc to capture the readability of a code snippet.
such metrics are computed line by line itid identifier by identifier nmi and nm or block by block tc the authors aggregate the measures using minimum average and maximum in order to have a single measure for the snippet.
we also use code readability as defined by scalabrino et al.
as a separate metric combining together the previously listed metrics.
we followed the steps described by scalabrino et al.
to define the readability model by using a logistic classifier that we train on the java snippets available in the literature .
we also introduce a new code related metric the invoked methods signature quality imsq which measures the quality of the signature of the internal methods invoked by a given code snippet s i.e.
methods belonging to the same system of s in terms of readability and representativeness.
we define the method signature quality msq of an invoked method mas msq m jis m jx id2is m iq id where is m is the set of identifiers used in the m s signature i.e.
method name and parameters and iq id is defined as iq id rd id rp id idis a method name rd id idis a parameter name authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
iq id captures the quality of an identifier in terms of its readability rd and its representativeness rp .
the idea behind the readability is that an identifier should be composed of a possibly small set of meaningful words.
to measure rd for an identifier id we i split idinto the words composing it ii expand each word to bring it in its original form e.g.
ptr!pointer iii create a new identifier idexp composed by the expanded words separated by a and iv measure the levenshtein distance between idandidexp.
the levenshtein distance between two strings aandbmeasures the minimum number of single character changes needed to transform ainto b. the conjecture behind iq id is that the higher the levenshtein distance between idand idexp the higher the mental effort required for the developer to understand the meaning of the identifier by mentally splitting and expanding it during program comprehension.
note also that we separate the expanded terms in idexpby using in order to penalize by increasing the levenshtein distance identifiers composed by several words.
for example the identifier printonstdout is first split into print on std out then each word is expanded which has no effect on the first two words but expands std intostandard andout into output.
therefore printonstdout is transformed in print on standard output.
to have rd id defined in we normalize the levenshtein distance l between idandidexpas follows rd id l id idexp max jidj jidexpj wheremax jidj jidexpj represents the longest identifier among the two.
when the distance equals zero the readability of the identifier equals one indicating no need for expansion splitting i.e.
id is composed by a single expanded word .
note that in the implementation of rd id we used a semiautomatic approach to split expand identifiers.
we first used a naive automatic splitting technique based on camel case and underscores then we automatically checked the presence of each resulting word in an english dictionary.
if the word was not found we manually expanded further split the specific word.
for example for the word cmdline there would not be automatic split.
since the word cmdline does not exist in the dictionary we manually convert it to command and line .
we save all the manual substitutions in order to minimize the human effort.
in the literature there are many automatic approaches for identifier splitting expansion but we preferred to implement a simpler and more effective strategy at this stage since the number of identifiers to split expand was limited and our goal was to assess the correlation of the defined metrics with the understandability effort.
thus we wanted to be sure to avoid introducing imprecision while computing the metrics.
when dealing with the identifier used to name a method we also verify whether it is representative of what the method does rp .
we compute the textual overlap between the terms used in the identifier and in the method body.
we tokenize the method body to define its dictionary.
then we count thenumber of times each word from the identifier expanded or not is contained in the dictionary extracted from the method body.
we consider only names and verbs from the identifiers ignoring other parts of speech such as conjunctions since they do not carry semantic information.
following the printonstdout example we check whether the method body contains the words print standard std output and out.
we measure the representativeness as the ratio between the number of words from the identifier i.e.
method name contained in the method body and the total number of words in the identifier.
if all the words from the identifier are used in m s body we assume that the method name is representative of mand thus should ease the understanding of methods invoking m. if instead words are not found in the method body this could hinder the understandability of the methods invoking m. in our study we consider the minimum the average and the maximum values of the msq metric for a given code snippet e.g.
the average msq of all methods invoked in the code snippet .
b. documentation related metrics scalabrino et al.
introduced three metrics to capture the quality of the internal documentation of a snippet comments readability cr measures the readability of the comments in a snippet using the flesch reading ease test comments and identifiers consistency cic measures the consistency between comments and code and cic syn a variant of cic which takes synonyms into account.
we also introduce two new metrics aimed at capturing the quality of both the internal midq and external aedq documentation available for code components used in a given snippet.
the methods internal documentation quality midq for a snippet sacts as a proxy for the internal documentation i.e.
javadoc available for the internal methods the ones belonging to the same project as s invoked in s. given m an internal invoked method we compute midq m using a variation of the approach proposed by schreck et al.
midq m dir m readabilityd m where dir m is the documented items ratio computed as the number of documented items in mdivided by the number of documentable items in m. we consider as documentable items form i its parameters ii the exceptions it throws and iii its return value.
such items are considered as documented if there is an explicit reference to them in the javadoc through the tags param throws and returns.
readability d m represents instead the readability of the javadoc comments assessed using the flesch reading ease test .
the higher midq the higher the internal documentation quality for m. we consider the minimum the average and the maximum values of the msq metric for a given code snippet.
concerning the api external documentation quality aedq it tries to capture the amount of information about apis used in the given snippet sthat can be acquired from external sources of documentation such as q a websites.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the conjecture is that if external documentation is available it is more likely that developers are able to understand the usage of an api in a code snippet s. we compute the availability of external documentation for each external class cused in s via the aedq c metric.
first we identify all stack overflow discussions related to cby running the following query title how to c hasaccepted yes in other words we select all stack overflow discussions that i contain how to and the class name in the title ii have an accepted answer and iii concern java since our study has been performed on java snippets .
then we sum the votes assigned by the stack overflow users to the question in each retrieved discussion in order to have a quantitative information about the interest of the developers community in such a class.
we assume that higher interest in a given api class implies a higher availability of external sources of information e.g.
discussions code examples etc.
.
we consider in our study the minimum the average and the maximum values of the aedq metric for the external classes used in s. c. developer related metrics since understandability is a very subjective feature of code we introduced three developer related metrics.
we measure the programming experience of the developer who is required to understand a snippet pe genandpespec and the popularity of the api used in the snippet eap .
the common wisdom is that the higher the programming experience of developers the higher their capability of understanding code.
pegenmeasures the programming experience in years of a developer in general i.e.
in any programming language .
pespec assesses instead the programming experience in years of a developer in the programming language in which a given snippet sis implemented.
the higher pespec the higher the developer s knowledge about the libraries available for such a programming language.
with external api popularity eap we aim at capturing the popularity of the external apis used in a given snippet.
the assumption is that the lower the popularity the lower the probability that a typical developer knows the api.
if the developer is not aware of the apis used in a snippet it is likely that she has to look for its documentation or to inspect its source code thus spending more effort in code understanding.
we rely on an external base of java classes eto estimate the popularity of an external class.
we chose as ea random sample of classes from java android projects hosted on github in totaling 2m classes from 57k java projects.
we used google bigquery to extract all the imports of all the classes belonging to such projects using a regular expression.
then we counted the number of times each class imported in eoccurred in the import statements.
note that in java it is possible to import entire packages e.g.
import java util .
in this case it is difficult to identify the actual classes imported from the package.
for this reason we applied the following strategy.
let us assume that a class foo is imported only once with the statement import bar foo but it is part of a quite popular package bar that is importedtable ii systems used in our study system ja va kloc category description antlr desktop lexer parser car report mobile car costs monitoring hibernate framework orm framework jenkins web continuous integration k9 mail mobile mail client myexpenses mobile budget monitoring opencms web content management system phoenix framework relational database engine spring framework generic application framework weka desktop machine learning toolkit times in ethrough the statement import bar .
the class foo2 belonging to the same package is imported times with the statement import bar foo2.
in this case we increase the number of occurrences of classes belonging to imported package in a proportional way.
in the presented example we add to the number of foo s imports and to the number of foo2 imports.
we found that imports of entire packages represent only .
of all the imports and therefore their impact is very low.
eap c is defined as the number of cimports normalized over the number of imports of cmax where cmax is the most imported class we found in e i.e.
java util list .
iv.
e mpirical study design the goal of our study is to assess the extent to which the considered metrics correlate with code understandability.
the perspective is of researchers interested in analyzing whether code related documentation related and developerrelated metrics can be used to assess the understandability level of a given piece of code.
this study aims at answering the following major research question what is the correlation between the considered metrics and the understandability level of a given developer for a specific code snippet?
given the wide and heterogeneous set of considered metrics answering this research question would allow us and more in general the research community to understand how far we are from defining a set of metrics capable of automatically and objectively assessing code understandability.
a. data collection thecontext of the study consists of java android methods extracted from ten popular systems listed in table ii five methods from each system .
we first extracted all the methods having 20elocs i.e.
effective lines of code excluding blank and comment lines from the systems.
the choice of the methods size i.e.
20elocs was driven by the decision of excluding methods that are too trivial or too complex to understand.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
afterwards we computed all the metrics described in section iii for the selected methods1.
then we used a greedy algorithm for center selection to select the most representative methods based on the defined metrics.
given a set of candidate methods mand a set of already selected centers c such algorithm chooses in each iteration arg maxm2m dist c m i.e.
the candidate method which is the farthest possible in terms of considered metrics from the already selected centers.
the first center is randomly selected.
in order to select exactly five snippets from each system we used the set of candidate methods from a specific system as muntil the five methods for such a system were selected then we changed mwith the set of candidate methods from another system and so on until jcj .
note that i we did not empty the cset when changing the candidate methods i.e.
when moving from one system to another to always keep track of the methods selected up to that moment thus avoiding the risk of adding to cmethods similar to the ones already inc ii we did not run the algorithm on the union of all candidate methods to ensure the selection of five methods per system thus increasing the heterogeneity of the final sample .
after having selected the methods and computed the values of the metrics of interest for each of these methods we needed to define a ground truth reporting the understandability of each method.
to this aim we invited java developers and cs students to participate in a survey where they were required to understand the selected methods.
the survey was implemented in a web application and featured the following steps.
first we collected demographic data about participants i years of experience in programming and more specifically in java and ii current position e.g.
cs student developer etc.
.
this information was used in part to compute the developer related metrics.
after this preliminary step each participant was required to understand a subset of eight methods randomly selected from the methods.
the web application was designed to automatically balance the number of evaluations for each of the methods i.e.
the number of participants understanding each method was roughly the same .
in total we collected evaluations across the methods .
evaluations per method on average since not all participants completed the survey.
the eight methods were presented individually i.e.
each method in a different page to participants and the web application allowed navigation of the method and access to the methods classes invoked used by it.
also participants were allowed to browse the web to collect information about types apis data structures etc.used in the method.
this was done to simulate the typical understanding process performed by developers.
we asked participants to carefully read and fully understand each method.
participants could at any moment click on the button i understood the method or the button i cannot understand the method .
in both cases the web application stored the time spent in seconds by the developer 1excluding the developer programming experience and the developer java experience .for the method s understanding before clicking on one of the two buttons.
if the participant clicked on i understood the method the method was hidden and she was required to answer three verification questions about the method she just inspected.
the provided answers were stored for future analysis.
an example of verification question is what does the invoked method xdo?
.
to answer such a question the participant should have understood also all the consequences of invoking a specific external method.
b. analysis method in the context of our study we measure the understandability level of a given piece of code from different perspectives and correlate it with considered metrics.
we defined the following four different independent variables.
perceived binary understandability pbu .
this binary metric gets value if the participant clicked on the i cannot understand the method button otherwise i.e.
the participant clicked on i understood the method it gets value .
this metric simply aims at discriminating between understandable vs.non understandable methods as perceived by participants.
time needed for perceived understandability tnpu .
measured as time in seconds spent by the participant while inspecting the method before clicking on i understood the method .
this metric cannot be computed when the participant clicked on i cannot understand the method .
actual understandability au .
it gets value if the participant clicked on the i cannot understand the method button.
otherwise it is computed as the percentage of correct answers the participant provided to the three verification questions.
the metric is defined in the range where indicates high understandability.
timed actual understandability tau .
it gets value if the participant clicked on the i cannot understand the method button.
otherwise it is computed as tau au tnpu no maxtnpu no where au is the percentage of correct answers tnpu is the time needed to understand the method and tnpu nois a modified tnpu where outliers detected using the tukey s test with k are substituted with the maximum value of tpnu which is not an outlier.
the higher au the higher tau while the higher tnpu the lower tau.
also tau is defined in .
we considered the relative time tnpu no maxtnpu no instead of the absolute time so that tau gives the same importance to both the correctness achieved au and the time needed tnpu .
we removed outliers from tnpu because the maximum value of such a variable is seconds much greater than the third quartile seconds .
using such a value as the maximum would have flattened down all the relative times.
we computed these four variables for each of the evaluations performed by participants i.e.
for each method each participant tried to understand .
before calculating the correlation between each of the metrics described in authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
section iii and pbu tnpu au and tau we excluded of the considered metrics i.e.
nmi min and itid min because the value of such metrics was for all the snippets taken into account.
then we verified which metrics strongly correlate among the .
this was done to exclude from our analysis redundant metrics capturing the same information in different ways.
we compute the kendall rank correlation coefficient i.e.
kendall s to determine whether there are pairs exhibiting a strong correlation.
we adopted the kendall s since it does not assume the data to be normally distributed nor the existence of a straight linear relationship between the analyzed pairs of metrics.
cohen provided a set of guidelines for the interpretation of the correlation coefficient.
it is assumed that there is no correlation when j j small correlation when j j medium correlation when j j and strong correlation when j j .
for each pair of metrics exhibiting a strong correlation i.e.
with a kendall s j j we excluded the ones which presented the highest number of missing values2or if equals one at random.
this allowed us to reduce the number of investigated metrics from to .
note that such a removal of metrics was only done for presentation purposes considering the high number of metrics taken into account in our study.
finally we computed the kendall correlation between each of the remaining metrics and pbu tnpu au and tau to verify whether some of them are able to capture the actual and perceived understandability of code.
c. replication package all the data used in our study is publicly available .
we provide the research community with our dataset reporting the perceived and actual understandability achieved by the participants hoping that the availability of such a dataset will help in fostering research on the definition of a metric to automatically assess code understandability.
v. e mpirical study results fig.
provides information about the participants involved in our study.
most of them are bachelor s students mixed in terms of years of programming experience.
the sample of participants also included six master s students three ph.d. students and five professional developers.
fig.
reports a heat map showing the kendall s between each metric and each understandability variable.
we only show in fig.
correlations having j j .
as it is evident by looking at fig.
very few metrics have a correlation with understandability variables higher than j0 1j.
specifically nine metrics have a weak correlation with pbu with tnpu with auand with tau.
out of the metrics taken into account show no correlation at all.
in the following we discuss the observed correlations by understandability metric.
however it must always be kept in mind that these are all weak correlations and as such do not allow for making 2some metrics cannot be computed in some cases.
for example area of comments literals cannot be computed if the method does not contain literals.
32position ph.d. student professional developer master s student bachelor s studentfig.
.
participants to the study any strong claims about the ability of these correlated metrics to act as proxies for code understandability.
a. perceived binary understandability pbu the metric which has the highest correlation with pbu is max line length one of the metrics introduced by buse and weimer for readability prediction.
note that buse and weimer also found that such a metric is the most important one for readability prediction .
therefore this is a further confirmation of the fact that generally developers tend to perceive code with long lines as less pleasant.
we also found that there is a low correlation between pe specandpbu developers with more experience in the specific programming language tend to have a slightly higher confidence and they tend to perceive snippets of code as understandable more frequently than developers with less experience.
there is also a low correlation between pbu andaidq min if the internal apis are well documented developers tend to answer that they understood the snippet slightly more frequently.
b. time needed for perceived understandability tnpu the time needed to answer i understood the snippet is weakly correlated with the programming experience both pegen and pespec .
surprisingly the correlation is positive the higher the experience the higher the time needed to complete the comprehension step and click the button i understood the snippet .
this result seems to suggest that experienced developers tend to take more time to understand the snippets while less experienced developers tend to complete the comprehension process more quickly.
however it is worth remembering that tnpu assesses the time needed for the perceived understandability and not the actual one tau .
c. actual understandability au the metric that has the highest correlation with auis the average textual coherence of the snippet .
the fact that the correlation is negative is surprising because we expected a higher textual coherence to imply a higher understandability.
other examples of metrics correlated with auare number of parameters anddft of conditionals indicating that higher complexity reduces understandability.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
pbu tnpu au tau identifiers avg loops avg line length avg keywords avg indentation length avg conditionals avg identifiers length avg comparisons avg comments avg commas avg blank lines avg assignments avg tc max tc min tc avg nm max nm avg crcicsyn max cicsyn avg cic avg nmi max nmi avg itid avg readabilitymidq max imsq max eap avg aedq avg midq avg eap min aedq min midq min imsq min cyclomatic comp.pe spec pe gen .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
correlation pbu tnpu au tau statements parameters nested blocks avg aligned blocks operators identifiers area keywords identifiers area keywords comments area operators area strings area keywords area identifiers area literals visual x strings visual y strings visual x numbers visual x identifiers visual x comments visual x operators dft numbers dft line length dft indentation length dft conditionals dft comparisons dft assignments dft locvolumeentropy words max characters max identifiers max line length max keywords max identifiers length max periods avg parenthesis avg operators avg numbers avg .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.11color keyfig.
.
heat map of the correlation among metrics and understandability proxies d. timed actual understandability tau the metric with the highest correlation with tau isdft of conditionals .
again high complexity reduces the understandability.
it should be noticed that for tau i.e.
actual understandability we do not observe any correlation with the programming experience.
e. discussion given the large and diverse set of metrics considered in our study we expected to observe some form of correlation allowing us to start designing an understandability metric model.
however what we obtained is a clear and bold negative result none of the investigated metrics show a significant correlation with the snippets understandability.an important implication of this finding is that readability and complexity metrics are not correlated with understandability which is a result deserving additional empirical investigations.
for instance the snippet in fig.
from weka was understood by all the participants who evaluated such a snippet during the study i.e.
mean pbu .
note also that the mean tau i.e.
the participants answered correctly two out of three the verification questions on average.
however the snippet is considered as unreadable .
when using the readability model proposed by scalabrino et al.
.
another metric that has no correlation with understandability is loc.
even if in our study we considered snippets with a small variation of locs maximum it would have authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
private void addbuttons jbutton okbut new jbutton ok jbutton cancelbut new jbutton cancel jpanel butholder new jpanel butholder .
setlayout new gridlayout butholder .
add okbut butholder .
add cancelbut add butholder borderlayout .
south okbut .
addactionlistener new actionlistener override public void actionperformed actionevent e if m modifylistener !
null m modifylistener .
setmodifiedstatus classifierperformanceevaluatorcustomizer .
this true if m evaluationmetrics .
size stringbuilder b new stringbuilder for string s m evaluationmetrics b. append s .
append string newlist b. substring b. length m cpe .
setevaluationmetricstooutput newlist if m parent !
null m parent .
dispose cancelbut .
addactionlistener new actionlistener override public void actionperformed actionevent e customizerclosing if m parent !
null m parent .
dispose fig.
.
example of understandable snippet with low readability been reasonable to expect at least a small correlation with understandability.
almost all of the metrics that we introduced in this study to capture aspects of code reasonably related to understandability do not correlate with any of the considered proxies.
the only exception is aidq min api internal documentation quality .
indeed such a metric has a low correlation with both pbu andtau.
this result does not completely exclude the fact that external documentation quality of identifiers and popularity of external apis could play a role in the automatic assessment of code understandability but it underlines that designing metrics able to capture such an aspect is far from trivial and represents a big challenge for the research community.
finally we should not exclude the possibility that code understandability is simply too subjective to be captured by any metric.
however the research community has been able to successfully define proxies for other highly subjective code properties in the past such as code readability.
for this reason we believe that more research targeting the automatic assessment of code understandability still represents an important and valuable research direction.
vi.
t hreats to validity threats to construct validity concerning the relation between theory and observation are mainly due to the measurements we performed both in terms of the metrics that we studied as well as when defining the four independent variables for the understandability level.
concerning the 121metrics we tested our implementation and when needed e.g.
for theimsq metric during the identifiers splitting expansion relied on manual intervention to ensure the correctness of the computed metrics.
as for the independent variables we tried to capture both the perceived and the actual code understandability.
however different results might be achieved by exploiting different variables.
threats to internal validity concern external factors we did not consider that could affect the variables and the relations being investigated.
since two of the understandability proxies are time related i.e.
they are based on the time participants spent while understanding the code it is possible that some participants were interrupted by external events while performing the comprehension task.
for this reason we excluded outliers i.e.
participants requiring more than q3 iqr seconds to understand a code snippet where q3is the third quartile and iqr is the inter quartile range .
threats to conclusion validity concern the relation between the treatment and the outcome.
although this is mainly an observational study wherever possible we used an appropriate support of statistical procedures.
threats to external validity concern the generalizability of our findings.
our study has been performed on a large but limited set of metrics and by involving participants comprehending a subset of methods extracted from java systems.
clearly our findings hold for the considered population of participants and for java code.
larger studies possibly involving more participants and code snippets written in other languages should be performed to corroborate or contradict our results.
vii.
c onclusion and future work we presented an empirical study investigating the correlation between code understandability and metrics related to the code itself to the documentation available for it and to the developer understanding it.
we asked developers to understand java snippets and we gathered a total of evaluations.
we assessed the perceived andactual participants understanding for each snippet they inspected and the time they needed for the comprehension process.
our results demonstrate that in most of the cases there is no correlation between the considered metrics and code understandability.
in the few cases where we observed a correlation its magnitude is very small.
the most important and surprising conclusions of our study are that i there is no correlation between understandability and readability and ii metrics generally used for effort estimation and commonly associated with understandability such as cyclomatic complexity actually have low or no correlation with understandability.
our study lays the foundations for future research on new metrics actually able to capture facets of code understandability.
for this reason we publicly release our dataset to enable the research community to investigate this direction further.