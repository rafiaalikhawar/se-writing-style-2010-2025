supporting the statistical analysis of v ariability models ruben heradio universidad nacional de educacion a distancia madrid spain rheradio issi.uned.esdavid fernandez amoros universidad nacional de educacion a distancia madrid spain david issi.uned.eschristoph mayr dorn johannes kepler university linz austria christoph.mayr dorn jku.atalexander egyed johannes kepler university linz austria alexander.egyed jku.at abstract variability models are broadly used to specify the configurable features of highly customizable software.
in practice they can be large defining thousands of features with theirdependencies and conflicts.
in such cases visualization techniquesand automated analysis support are crucial for understanding themodels.
this paper contributes to this line of research by pre senting a novel probabilistic foundation for statistical reasoningabout variability models.
our approach not only provides a newway to visualize describe and interpret variability models butit also supports the improvement of additional state of the artmethods for software product lines for instance providing exactcomputations where only approximations were available before and increasing the sensitivity of existing analysis operations forvariability models.
we demonstrate the benefits of our approachusing real case studies with up to features and writtenin two different languages kconfig and feature models .
index t erms variability modeling feature modeling software product lines software visualization binary decision diagrams.
i. i ntroduction a common challenge in software engineering is enabling and coping with many variants of software products that are customized for different market segments or contexts of use.
this is explored in paradigms such as software product lines spls or context aware software .
an essential tool to tackle this challenge are v ariability models vms which specify the common and variable features available for thesoftware products together with the inter feature conflicts and dependencies .
numerous visualization methods and analysis operations support the reasoning on non trivial vms.
introduced in1990 feature diagrams are the prevalent way to visualize vms as graphs whose nodes and edges depict features and inter feature relationships.
such representation works nicely for small vms but it becomes ineffective for large models because the resulting graphs are overly complicated.
many analysis operations are excessively rigid.
for instance current approaches for detecting dispensable features only identifythose that due to conflicts dependencies with the remaining features cannot be included in any product at all overlooking thus features with a reusability insignificantly above zero.
this paper proposes an alternative way to reason about vms.
the basic idea is adopting a method that in many otherknowledge domains has proven to be successful for describingand interpreting variation in large samples populations statistics.
for that it presents two algorithms that compute theprimary elements needed for the vm statistical analysis i the feature inclusion probability fip algorithm determines the probability for a feature to be included in a valid product and ii the product distribution pd algorithm determines the number of products having a given number of features.
spl engineering typically distinguishes two roles the domain engineer and the application engineer .
whereas the domain engineer undertakes the product line development i.e.
she engineers forreuse the application engineer obtains particular systems from the product line through a configuration process i.e.
she engineers with reuse .
our approach assists both roles.
regarding the domain engineer our method supports representing the feature and product variation using general statistical plots e.g.
histograms box plots etc.
and summarizing the variation through descriptive statistics e.g.
mean standard deviation etc.
.
this way the engineer receives informationabout the complexity of the software products and the spl itself.
moreover our approach supports augmenting the sensitivity of binary analysis operations by redefining them intoprobabilistic terms hence providing a continuous range of values instead of a simplistic yes no categorization.
engineers may use this for instance to detect highly dispensable featureswhose reuse probability is close to zero but not exactly zero.
regarding the application engineer our method provides information about the implications of her decisions i interms of features e.g.
if feature fis selected which other features become selected excluded due to their dependencies conflicts with f?
and also ii in terms of the final product e.g.
if feature fis selected what size will the final product probably have?
.
moreover some procedures have been pro posed to guide the engineer through the configuration spaceby using the concept of feature probability .
however as existing methods for computing feature probabilities do not scale for large vms probabilities are often roughly approximated from samples of historical product configurations or set manually by theengineer according to her beliefs .
this paper contributes to configuration guidance procedures by supporting the exact and scalable feature probability computation.
ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
most existing methods for automated reasoning on vms convert the models into boolean logic formulas for subsequent processing with logic engines .
this translation of vms into boolean logic is a well studied problem supported for most vm notations such as feature models kconfig or cdl .
our algorithms work with practically every vm notation as they build on the binary decision diagram bdd encoding of the vm boolean formulas.
we demonstrate the feasibility and benefit of our approach with real vms specified in two distinct languages kconfig and feature models .
the investigated vm examples differ in the number of features ranging from small to huge with up to features and come from different application domains open source software projects the automotive industry and web configurators .
among other issues the experiments reveal that some models have a surprisingly high number of features with extremely low reusability.
the remainder of this paper is organized as follows section ii motivates the statistical analysis of variability models illustrating its benefits with a real example.
section iii describes our algorithms in detail.
section iv reports the application of the approach to distinct case studies.
section v discusses related work.
finally section vi summarizes this paper s main conclusions and outlines directions for future research.
ii.
m otiv a ting the sta tistical analysis of v ariability models most approaches for providing engineers with visualization assistance to understand non trivial vms use graphs or trees whose nodes and edges represent features and constraints respectively .
feature models are the most widespread graphical notation for vms .
in practice vms can be huge and for those cases their visual graph representation becomes ineffective.
for example the embtoolkit project eases the application development and firmware generation for highly customized embedded linux products.
its vm is specified in a text based language called kconfig which is also used in other popular open source projects such as the linux kernel uclib or axtls.
the kconfig specification of embtoolkit encompasses configurable features together with inter feature constraints.
figure shows the graph representation of the kconfig specification of embtoolkit .
.
.
such visual representation offers little value even when zooming in to make the node labels readable.
in contrast we propose a statistical approach to describe and interpret the variation of the features and products specified by a vm.
in the following subsections we outline how this method assists both the domain and the application engineers.
a. domain engineer s support our algorithms provide the fundamental information to enable answering the following key questions how complex are the products?
the complexity of a product can be roughly measured by its number of features .
our pd algorithm computes the products distribution fig.
graph representation of the embtoolkit kconfig regarding their number of features.
this distribution is the basis for distinct plots and descriptive statistics further characterizing products complexity.
for instance the density plot in figure and the descriptive statistics in table i summarize the product distribution for the kconfig specification of embtoolkit .
.
.
this way the engineer becomes aware that the most frequently occurring number of features for a product is that the smallest and largest products have and features respectively etc.
fig.
embtoolkit product distribution mean standard median median mode min max range deviation absolute deviation .
.
.
table i product distribution descriptive statistics how complex is the product line?
the complexity of a spl may be characterized by the following three core metrics the number of features the spl manages the number of valid products that can be derived and the resulting homogeneity of those products i.e.
how much does one product differ from the others .
the pd algorithm in combination with our authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
feature inclusion probability fip algorithm provides a clear picture of the products homogeneity.
figure presents two extreme cases one of extreme homogeneity top the other of extreme heterogeneity bottom the top row describes a spl where products are very homogeneous because i most products contain a similar number of features i.e.
its distribution has low variance see the plot on the left and ii most features are nearly always included i.e.
the feature probabilities are close to one and have low variance right plot .
the bottom row describes a spl where the products are very heterogeneous because i some products may contain only a few features while others may contain a high number of features i.e.
the product distribution has high variance and ii most features are nearly never included in a product i.e.
the feature probabilities are close to zero and have low variance .
fig.
products homogeneity characterization should the spl be refactored to simplify its maintenance?
the histogram in figure depicts the feature probability distribution for embtoolkit .
.
.
three zones have been highlighted in the plot whose detailed information is summarized in table ii the red shaded area left highlights the features with probability less than or equal to .
of being included in a valid product.
the extreme cases are those with zero probability which are commonly called dead .
interestingly .
of the embtoolkit features are dead and thus they should be removed from the kconfig specification as they are completely without value.
the green shaded area right emphasizes the features that are required by almost every valid product being the extreme cases those with probability one which are usually called core as they are present in all products.
the yellow shaded zone middle identifies low constraint features.
in particular those with probability .
are typically pure optional features whose selection is unconstrained.
dead features unconst.
opt.
features core features p p .
p .
.
p .
p p .
.
.
.
.
.
.
table ii dead core and potentially optional features fig.
embtoolkit feature probability distribution our approach also provides assistance when historical data about the actual feature inclusion are available e.g.
the debian popularity contest gathers information about how many times each debian package has been installed debian.org .
in this case the domain engineer compares the vm statistics with the historical ones.
if for example the actual products tend to be much smaller than the product distribution mode obtained from the vm then perhaps the spl is unnecessarily complex and could be simplified.
understanding the answers to these questions is thus of essential value for spl and product testing evolution and reuse.
b. application engineer s support our approach supports the application engineer s decision making by showing the impact that a decision has on the remaining features for example if the engineer selects the arm architecture for embtoolkit embtk arch arm then our fip algorithm will show that some other features will necessarily be excluded from the product e.g.
the probability of kembtk uclibc target mips becomes zero and that the selection of other features will become difficult e.g.
the probability of embtk clib glibc decreases to .
.
it is worth noting that our approach determines feature exclusion beyond explicit constraints among two features by considering the overall set of constraints and currently selected features.
the product under configuration for instance our fip and pd algorithms support providing plots such as the one in figure which shows how the configuration space shrinks with each engineer s decision about selecting excluding features.
note that the product distribution variance decreases progressively until it becomes zero at the end of the configuration process.
fig.
visualizing the product derivation progress authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
several heuristics have been proposed to speed up product configuration by taking advantage of the fact that due to the inter feature constraints some decisions can be automatically derived from other decisions previously made.
some of those heuristics are based on approximating feature probabilities .
since our fip algorithm computes those probabilities it provides better support for the aforementioned heuristics.
iii.
c omputing fea ture and product distributions this section describes a new method to compute the feature and product distributions from a vm.
first section iii a introduces some probability definitions and the bdd technology our approach is built upon.
then sections iii b and iii c explain our algorithms in detail.
a. preliminaries the kconfig file in figure will be used throughout this section as running example.
it is composed of several configs that specify three features a bandc and their interdependencies.
1config a bool a value?
select c if !b 4config b bool b value?
depends on a 7config c bool fig.
running example a kconfig fileall features are boolean lines and meaning that they can be either selected or deselected.
features can acquire their value from the user input but also from other feature values.
for instance configs aandbspecify aprompt to request the user about their feature values e.g.
a value?
.
in contrast cdoes not specify any prompt and its value is derived as follows cis selected whenever ais selected but not b line .
finally feature bdepends ona i.e.
to be selected in a product brequires that ais selected as well.
as a result the configuration space encompasses only three valid products a b c a b c a b c where forfrepresents that feature fis selected or deselected respectively.
therefore the product distribution regarding the number of features each product has is one product with zero features a b c zero products with one feature two products with two features a b c and a b c and zero products with three features.
the probability of a b and c to be selected in a valid product is 3and1 respectively.
boolean representation of variability models most approaches for automated reasoning on vms are based on converting the models into boolean logic formulas which are then processed with logic engines.
the details of this translation can be found in and for feature and kconfig models respectively.
for instance the vm in figure is equivalent to the formula a b c b a whose truth table is summarized in table iii and means true and false respectively .abc table iii running example truth tablethe truth table contains all possible configurations.
the valid and invalid products are represented by rows where is and respectively.
for each row the inclusion exclusion of a feature is represented by in its corresponding column.
for example the seventh row depicts the valid product a b c .
the following probabilities are defined from the truth table p andp are the probabilities of to be and respectively p andp are calculated as the number of rows where is and respectively divided by the total number of rows.
in table iii p 8and p .
p x is the joint probability ofxand to be both it is computed as the number of rows where both xand are divided by the total number of rows.
for example p a .
it is worth noting that joint probabilities are symmetrical i.e.
p x p x .
obviously other joint probabilities can be defined negating xor e.g.
p a p a etc.
the conditional probability p x is the probability that xis knowing beforehand that is .
in other words it is the number of rows where both xand are divided by the number of rows where is .
for example p a p a etc.
in this paper we are especially interested in getting the probability each feature has to be included in a valid product i.e.
p x .
nevertheless this computation will be built upon other probabilities.
in particular by definition p x p x p p x p x p likewise p x p x p x p x p x p x .
as joint probabilities are symmetrical then p x p x p x p p x p x p x p x p x p .
this last relation which supports deriving p x from p x is known as bayes rule and it will be used in section iii b to get p x .
binary decision diagrams truth tables are convenient to understand the concepts we will handle to get the feature probabilities and product distribution but not to make the computations because their size grows exponentially with the number of variables a table with nvariables has 2nrows .
in contrast bdds which can be thought as compressed truth tables without redundancies are by far more scalable .
an example that illustrates their compression power is reported in this paper experimental section the kconfig specification of the uclibc library for developing embedded linux systems has features and thus its truth table would have 2306rows nevertheless its bdd encoding has only nodes.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a non reduced b reduced fig.
bdd encoding for the kconfig example in figure a bdd is a rooted directed acyclic graph where i all terminal nodes are labeled with or and ii all non terminal nodes are labeled with a boolean variable.
each non terminal node has exactly two edges from that node to others one labeled and the other .
they are called the low and high edges and are usually represented graphically with dashed and solid lines respectively.
a bdd is ordered if the variables always appear in the same order for all the paths from the root to the terminal nodes.
for instance figure 7a represents a bdd with the ordering for our running example.
it has eight nodes two terminals n0andn1 and six nonterminals n2 n3 ... n7.
likewise rows in truth tables paths in bdds represent variable assignments.
in a path xis assigned to or if it goes through the low or high outgoing edge of a node labeled x and the resulting evaluation is or if the path ends up in the terminal or .
for example the 6throw in table iii a b c corresponds to the path n7 n6 amsdasharrowaxis amsdasharrowaxis amsarrowheadrightn4 n1in figure 7a.
to save memory bdds are usually reduced by r1 removing duplicated nodes i.e.
nodes that are the roots of structurally identical subbdds and r2 deleting nodes with identical outgoing edges.
in figure 7a r1 was performed but not r2 as the shaded node n2could be removed.
figure 7b shows a completely reduced bdd without these redundant nodes.
from here on we will assume that bdds are ordered and totally reduced.
thus the algorithms we present in the next sections deal not only with the existing nodes in the bdd but also with those removed due to r2.
in section ii b we saw that in order to assist the application engineer to understand the impact of her decisions it is convenient to restrict the configuration space according to a given set of selected excluded features.
fortunately most bdd libraries include a function called restrict that provides exactly this functionality .
finally algorithm shows bryant s method to traverse a bdd in a depth first fashion which will be used by our algorithms.
traverse is called at the top level with the bdd root as argument and with a boolean mark for every node being either all true or all false.
traverse visits all nodesby recursively visiting the low nlo and high nhisubbdds rooted by n. whenever a node is visited its mark value is complemented.
comparing the marks of nand its children it can be determined if they have already been visited.
the method ensures that each node is visited exactly once and that when traverse finishes all node marks have the same value.
algorithm .
bryant s method for bdd traversing 1function traverse n 2mark n mark n ifnis non terminal then ifmark n mark nlo then traverse nlo ifmark n mark nhi then traverse nhi 6traverse root b. computing feature probabilities algorithm fip obtains for each feature the proportion of valid products that include it i.e.
p x .t od os o i t applies bayes rule to ultimately derive p x from p n .
first the definition of conditional probability is used in line p x p x p being p andp x computed by the auxiliary functions getnodepr andgetjointpr .
computing node probabilities in a bdd let us define the probabilities p n andp n for a node nas the number of paths that go from the root to the terminal nodes by traversing nthrough its high and low outgoing edges respectively divided by the total number of paths.
let us start reasoning on how to compute p n when reduction r2 has not been done yet.
for instance in figure 7a p n6 8since there are eight paths in total from root to terminals and two of them go through the high edge of n6 n7 n6 n3 n0and n7 n6 n3 amsdasharrowaxis amsdasharrowaxis amsarrowheadrightn1 by construction in a truth table every variable xis half the rows and it is the other half.
for instance in table iii there are four rows where bis and there are other four rows where bis .
this fact can be expressed as p x p x .
if r2 is not applied p x p nlabeled xn being p nlabeled xn nlabeledp n because all bdd paths are mutually exclusive as they represent independent variable assignments.
for example in figure 7a p b p n5 p n6 .
the first variable in the bdd ordering is represented by a single node the root.
so p root p root .
the next variable in the ordering is encoded with two nodes roothiandrootlobecause every node has exactly two outgoing edges.
hence the variable probability is shared out both nodes and thus p root hi p roothi andp root lo p rootlo .
proceeding this way the node probabilities will be subsequently divided by two until the terminal nodes are reached.
finally we need to be aware that whereas a node always has two outgoing edges it may have any number greater than one of incoming edges.
therefore for a non terminal node nwith parents u1 u2 ...u s then p n s i 1p ui and for a terminal node authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm .
feature inclusion probability fip 1function getnodepr n 2mark n mark n ifnis non terminal then explore low ifnlo is terminal then p nlo p nlo p n else p nlo p nlo p n ifmark n mark nlo then getnodepr nlo explore high ifnhi is terminal then p nhi p nhi p n else p nhi p nhi p n ifmark n mark nhi then getnodepr nhi function getjointpr n 11mark n mark n ifnis non terminal then explore low ifnlo n0then p n else if nlo n1then p n else ifmark n mark nlo then getjointpr nlo p n p nlo nlo 2p nlo p n p n p n explore high ifnhi n0then p n else if nhi n1then p n else ifmark n mark nhi then getjointpr nhi p n p nhi nhi 2p nhi p n p n p n combine both low and high p n n p n p n p var n p var n p n add joint probabilities of the removed nodes foreach xjbetweenvar n andvar nhi do p xj p xj p n foreach xjbetweenvar n andvar nlo do p xj p xj p n 31p root 32p ni 0for all nodes niexcept the bdd root 33getnodepr root 34p xj 0for all variables xj 35getjointpr root 36p p n1 foreach xjdop xj p xj p p n s i 1p ui the parents probability is not divided as the node has no outgoing edges .
let us move now to realistic bdds where r2 is performed.
in this case we need to take into account the removed nodes p x p nlabeled xn n labeled x but removedn np n n p n let us see how to compute the number of redundant nodes removed between any two nodes due to r2.
if the variables follow the ordering l e tvar n be the position of the variable that labels the node nin the ordering.
for example in figure 7b var n4 since n4is labeled b and bis in the second position of the ordering .
finally letvar n0 v a r n1 s .
then var nlo var n 1is the number of nodes that have been removed between n andnlo and var nhi var n 1is the number of nodes that have been removed between nandnhi.
for example as var n0 var n4 it can be deduced that one node was removed in the high edge that goes from n4to n0 i.e.
the shaded node n2in figure 7a .
when a non reduced bdd has a path u n1 amsdasharrowaxis amsdasharrowaxis amsarrowheadrightn2 amsdasharrowaxis amsdasharrowaxis amsarrowheadright... amsdasharrowaxis amsdasharrowaxis amsarrowheadrightv after applying r2 the path becomes u v according to what was previously discussed above p n1 p u .
for the rest of the nodes n2 n3 ... v the probability is not divided again since both the high and low edges go to the same node e.g.
p n2 p n1hi p n1lo p u p u p u .
to sum up i the probability of the reduced nodes between any two nodes uandvisp u and ii the probability of vis not affected by the amount of reduced nodes being equal to p u 2as well.
function getnodepr combines the ideas discussed above with bryant s traverse method.
in algorithm fip p root is set to and p n is initialized to for the remaining nodes lines .
then getnodepr traverses the bdd in preorder to update p n .
finally it is worth noting that p p n1 andp p n0 being p andp the proportion of valid and invalid products of the vm respectively.
computing joint probabilities following the same argumentation line than in the previous section p x np n n p n let us start first with the non reduced nodes.
by definition p n p n p n .
as we rely on bryant s recursive method to perform the computations let us define p n in function of nhigh descendant as the probability is conditioned to n in principle we only care about the high descendant .
two cases need to be considered when nhiis terminal a if nhi n0it means that the path is evaluated to i.e.
is for the variable assignment the path represents and so p n b otherwise as nhi n1thenp n .
when nhiis non terminal p n is calculated as p n p nhi nhi p nhi nhi p nhi nhi p nhi p nhi p nhi p nhi p nhi p nhi 2p nhi equation summarizes the cases above to compute p n .
as it needs knowing p nhi equation is used which is indeed the symmetrical case of equation .
p n ifnhi n0 ifnhi n1 p nhi p nhi 2p nhi otherwise p n ifnlo n0 ifnlo n1 p nlo p nlo 2p nlo otherwise authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
function getjointpr in algorithm fip uses both equations and to get the joint probability p x for nonremoved nodes lines .
then equation is applied to obtain p n for the removed nodes n lines .
it is worth noting that such equation follows the same reasoning presented in section iii b1 to obtain p n .
p n p n 2ifn was between nandnhi p n 2ifn was between nandnlo c. computing product distribution algorithm pd sketches the computation of the product distribution accounting for how many products have no features one feature two features ... all features.
it uses bryant s method to traverse the bdd in post order by calling the auxiliary function getproddist with the bdd root as argument.
from the terminals to the root it progressively obtains the partial distributions that correspond to the subbdds rooted by each node being the final distribution placed at the root.
algorithm .
product distribution pd 1function getproddist n 2mark n mark n ifnis non terminal then traverse ifmark n mark nlo then getproddist nlo compute lowdist to account for the removed nodes through low removednodes var nlo var n letlowdist b eav e c t o rw i t h removednodes length of dist nlo zeros for i i removednodes i do for j j length ofdist nlo j do lowdist lowdist dist nlo removednodes i traverse ifmark n mark nhi then getproddist nhi compute highdist to account for the removed nodes through high removednodes var nhi var n lethighdist be a vector with removednodes length of dist nhi zeros for i i removednodes i do for j j length ofdist nhi j do highdist highdist dist nhi removednodes i combine low and high distributions iflowdist is longer than highdist then distlength length ofdist nlo elsedistlength length ofdist nhi letdist n be a vector of length distlength filled with zeros for i j length oflowdist i do dist n lowdist for i j length ofhighdist i do dist n dist n highdist 24dist n0 no products 25dist n1 one product with no features 26getproddist root return dist root figure shows each node s distribution for our running example which is stored in different vectors dist.
starting from the position iin a dist vector accounts for the number ofproducts that have ifeatures e.g.
dist n3 because the subbdd with nodes n0 n1 andn3represents no products with zero features and one product with one feature i.e.
product c .
fig.
dist vectorsgetproddist s recursive base cases are node n0 representing no products at all and node n1 representing a single product with no features.
accordingly dist n0 and dist n1 lines .
to understand the more advanced recursive cases three observations need to be done including new features into all products is achieved by shifting the dist vector to the right o1 let us imagine that dist i.e.
there is product with features products with feature and products with features.
if no new features are added dist remains the same.
if one feature is added to all products dist becomes i.e.
there are no products without features because all of them have at least the new feature the product that had zero features now have feature and the products that had features now have features.
if two features are added to all products dist becomes and so on.
in general the addition of sfeatures to all products means shifting dist spositions to the right.
combining dist vectors is accomplished by adding them o2 let us think about how to get dist n from dist nlo anddist nhi .
first let us suppose that no nodes were removed between nand its descendants.
imagine that dist nlo anddist nhi .
according to o1 dist nhi needs to be shifted one position to account for the additional feature that labels n. then both descendants distributions are combined by just adding them dist n .
removed nodes require taking into account both observations o1 and o2 and blending them by means of combinatorial numbers o3 if a non reduced bdd had a path u amsdasharrowaxis amsdasharrowaxis amsarrowheadrightn1 amsdasharrowaxis amsdasharrowaxis amsarrowheadrightn2 amsdasharrowaxis amsdasharrowaxis amsarrowheadright...ns amsdasharrowaxis amsdasharrowaxis amsarrowheadrightv r2 would remove thesredundant nodes and thus the path would become u amsdasharrowaxis amsdasharrowaxis amsarrowheadrightv.
hence dist ulo should be adjusted as any of the removed nodes could be set to and so one new feature would be added to all products.
furthermore any pair of redundant nodes s could also be set to any combination of three nodes s ... and finally the combination of snodes s s .
original dist ulo adding features adding features adjusted final dist ulo table iv distribution adjustmentlet us see how the adjustment should work with an example imagine that dist nlo and two nodes where removed between nandnlo.
table iv summarizes the computations.
the first and last rows authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
represent the initial and adjusted distributions respectively.
the two intermediate rows represent the required adjustments.
first adding one feature to all products implies shifting dist one position to the right o1 .
as there are !
!
!
2different combinations of one feature two shifted vectors should be added o2 .
as a result becomes .
second there is only one possibility to add two features to all products.
so becomes .
finally all distributions are combined by adding them o2 .
lines and of algorithm pd adjust the low and high distributions of the non terminal nodes to account for the removed nodes.
then lines combine both adjusted distributions.
d. computational complexity both algorithms fip and pd traverse the whole bdd and thus their complexity depends linearly on the number nof bdd nodes.
visiting each node requires i one loop on the number vof variables for fip and ii two nested loops on the variables for pd.
as a result the time complexities are o nv ando nv2 for fip and pd respectively.
iv .
e xperimental analysis of vm s this section reports the analysis of seven vms gathered from open source projects and academic repositories with the aim of illustrating the usefulness and generality of our approach.
all the material described in this section implementation of the fip and pd algorithms vm benchmark bddencoding of the vms and results of the analysis is available at the following public repository a. experimental setup our algorithms have been implemented as an extension of the library cudd .
for bdds cudd .
the benchmark is composed of vms coming from different application domains and specified in distinct languages i axtls fiasco uclibc busybox and embtoolkit are open source projects to enable the creation of highly customizable products whose variability models are written in kconfig ii the dell feature model specifies a laptop configurator reverseengineered from the dell homepage and iii automotive is a feature model coming from the automotive industry.
table v summarizes i the models ii the size of the bdds that encode them iii and our algorithms running times on an hp proliant dl360 g9 with an intel xeon e5 2660v3.
b. results our approach enables reasoning on vms under two perspectives the products view .
table vi provides descriptive statistics for the vms product distribution regarding theirvm vm reference features clauses bdd running time name notation nodes fip pd axtls .
.3kconfig sourceforge.net .018s .019s dell feature .055s .052s laptops model fiasco 2014092821kconfig .020s .033s uclibc 20150420kconfig uclibc.org .362s .315s busybox .
.2kconfig net .213s .323s embtoolkit .
.0kconfig embtoolkit.org .863s .716s automotive 02feature model 1m .321s 1m .922s table v vm benchmarkvm name mean sd min max p p .
p .
.
p .
p p .
axtls .
.
.
.
.
.
dell .
.
.
.
.
.
fiasco .
.
.
.
.
.
.
uclibc .
.
.
.
.
.
.
busybox .
.
.
.
.
.
.
.
embtoolkit .
.
.
.
.
.
.
.
automotive .
.
.
.
.
.
.
.
table vi descriptive statistics for product distribution and percentage of dead core and unconstrained optional features number of features and figure visualizes that distribution.
the features view .
figure shows the feature probability distribution and colored columns in table vi detail the number of features in the zones dead unconstrained optional and core .
the product distribution graphs and feature probability distribution graphs respectively table vi highlight the existence of two rough vm groups.
in the first group axtls uclibc busybox and embtoolkit represent families of loosely constraint products.
v alid products may range from consisting of only a few features as low as three features for axtls to close to all features e.g.
over of all features in the case of busybox .
hence also the feature probability distribution graphs for these models show more features in the range .
p .
compared to the range p .
.
in contrast the second group consisting of dell laptops fiasco and automotive represents spls with rather restricted products.
v alid products may contain at a maximum and respectively of available features compared to the first group with and respectively.
spls in the second group also tend to come with highly rare features.
between and of all features have a reusing probability less or equal than .
.
a detailed list of all feature probabilities for every vm in the benchmark is published at our repository.
this list will help domain engineers to polish their vms especially for fiasco which has a surprisingly high percentage of dead features .
.
for dell laptops and due to the sensitivity augment that our fip algorithm provides some low reusable features are discovered where current approaches do not detect any problem at all although there are no dead features .
of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
dell s features are allowed in at most .
of the valid products.
fig.
product distribution v. r ela ted work the seminal work by kang et al.
established what has been the mainstream for visually representing vms from to nowadays graphs whose nodes depict features and whose edges represent inter feature constraints.
the most popular notation is feature modeling which puts the emphasis on those constraints that enable arranging the features hierarchically as a tree .
there are also other graph notations e.g.
decision diagrams the ovm language etc.
nevertheless the differences among notations are minor and so most approaches can be considered equivalent .
as in practice variability models can include thousands of features some efforts have been made to clarify the fig.
feature probability distribution visualization of large graph vm representations applying 3d techniques to visualize the graphs in the space instead of the plane supporting zooming on different graph areas focusing the visualization on a selected feature decomposing the graphs etc.
our work complements existing research by introducing an alternative way to look at vms through statistics supporting thus the use of centrality spread measures plots etc.
sections v a and v b discuss related work that aims to assist domain and application engineering respectively.
a. domain engineer s assistance a literature review by benavides et al.
reports thirty analysis operations on vms most of them oriented to domain engineering.
this paper supports augmenting the sensitivity of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
some of those operations.
for instance a feature is typically considered dead if it cannot appear in any product at all.
the main reason why most approaches stick to this definition for detecting dispensable features is due to the current limitations of the technology they are built upon as they detect whether a feature fin a vm is dead by checking with a sa t solver if f is unsatisfiable .
in contrast our algorithms support a more flexible definition detecting features with an extremely low probability of being selected.
beek et al.
point out the convenience of providing the domain engineer with information about the product distribution regarding distinct quantitative attributes e.g.
number of features product cost failure probability etc.
.
their approach requires i that the domain engineer sets manually the feature probabilities or ii that the feature probabilities are derived from historical data.
then the product distribution is estimated by generating multiple samples through a simulation process.
compared to beek et al.
s method our procedure provides the exact product distribution instead of an approximation.
nevertheless algorithm pd currently supports only one quantitative attribute the number of features and could be extended to consider domain specific properties.
b. application engineer s assistance there are several approaches to guide the application engineer through product configuration.
some of them are built upon historical data about previous configurations.
for instance pereira et al.
proposes a recommender system that limits the engineer s decision space towards configurations included in historical data.
in addition martinez et al.
provide the engineer with feedback on the impact of her decisions by estimating the feature probabilities from historical data.
these approaches have several shortcomings first the historical data may not be a representative sample of the product population especially if the sample size is small and its variance is high and most important feature selectivity cannot be strictly constrained to a sample.
for example if a non dead feature is not included in any configuration of the historical data then the system could conclude erroneously that the engineer should never select such feature.
other approaches instead of relying on previous configurations work directly with the vm.
for example czarnecki et al.
suggest the application of the entropy measure to guide the vm configuration process which is calculated from the feature probabilities.
in addition n ohrer et al.
propose another heuristic also based on the feature probabilities.
however none of those works scale to large vms.
fernandez amoros et al.
provide an algorithm to compute the feature probabilities from a feature model.
however the algorithm is specific for feature models and it does not scale when many constraints cross the tree structure of the feature model.
to the extent of our knowledge algorithm fip is the most scalable and general approach to compute the feature probabilities from a vm.
this way our work not only supports the configuration heuristics that rely on the feature probabilitiesobtained from the vm but also the ones based on historical data.
in the latter case our algorithms can be used to overcome the limitations of reasoning exclusively on the basis of a single product sample by applying bayesian inference to combine both the prior probabilities coming from the vm with the posterior probabilities coming from historical data.
vi.
c onclusions and future work in this paper the algorithmic foundation for analyzing vms from an innovative perspective has been presented where the features and products variation is visualized and described using statistics.
we have justified why this approach benefits both the domain and the application engineer exemplifying such benefits on real models gathered from open source projects and academic repositories.
we have shown that our approach not only enables new ways to reason about vms but also supports the improvement of current vm related methods increasing the sensitivity of existing analysis operations on vms and providing exact computations for approaches that currently work with approximations.
we believe that our work opens a range of additional opportunities for future research.
applied to other product line related activities such as testing our work enables checking whether current methods for spl testing are able to generate suites covering the whole product distribution range and thus avoid missing any rare boundary cases.
also our approach may be of assistance during maintenance of projects for highly customizable software e.g.
we have reported that the vms of some relevant open source projects have an alarming amount of dead features.
the causes of those useless features need investigation.
a longitudinal study would provide insights under which circumstances these projects exhibit these problems and whether they are corrected or stay in the successive versions of the vms.
finally our algorithms rely on the bdd encoding of vms.
it is well known that a bdd s size is extremely sensitive to its variable ordering and that finding an optimal ordering is an np complete problem.
therefore our approach s scalability greatly depends on the performance of existing heuristics for variable ordering.
hence future research might look for adapting our algorithms to other alternative logic technologies that also support model counting such as sat solvers orsentential decision diagrams sdds .
acknowledgment we thank armin biere and tom van dijk for their insight and helpful comments about the strengths and weaknesses of bdds and other logic related technologies in the earlier stages of this work which has been supported by i the spanish ministry of education and v ocational training under the projects with reference dpi2016 p and cas17 ii the austrian science fund fwf p29415 nbl funded by the government of upper austria and iii the ffg contract no.
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.