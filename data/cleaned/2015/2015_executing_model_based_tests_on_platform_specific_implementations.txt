executing model based tests on platform specific implementations dongjiang you1 sanjai rayadurgam1 mats p.e.
heimdahl1 john komp2 baekgyu kim3 oleg sokolsky3 1department of computer science and engineering university of minnesota usa 2medtronic plc usa 3department of computer and information science university of pennsylvania usa email cs.umn.edu john.komp medtronic.com cis.upenn.edu abstract model based testing of embedded real time systems is challenging because platform specific details are often abstracted away to make the models amenable to various analyses.
testing an implementation to expose non conformance to such a model requires reconciling differences arising from these abstractions.
due to stateful behavior naive comparisons of model and system behaviors often fail causing numerous false positives.
previously proposed approaches address this by being reactively permissive passing criteria are relaxed to reduce false positives but may increase false negatives which is particularly bothersome for safety critical systems.
to address this concern we propose an automated approach that is proactively adaptive test stimuli and system responses are suitably modified taking into account platform specific aspects so that the modified test when executed on the platform specific implementation exercises the intended scenario captured in the original model based test.
we show that the new framework eliminates false negatives while keeping the number of false positives low for a variety of platform specific configurations.
i. i ntroduction advances in automated test generation from system models do not always translate to realizable benefits in terms of testing an implementation of the system.
while it is now routinely possible to generate hundreds or even thousands of test cases from models the ability to use those for testing a particular realization of the system is hampered by two main bottlenecks translating the tests generated from the model which by definition abstracts away some implementation specific details into equivalent scenarios for the actual system.
deriving an oracle an arbiter of correctness that can decide whether the actual system passed or failed such a test when it is executed.
at a conceptual level these do not appear to be problematic.
translating tests derived from a model to a particular implementation is simply a matter of concretizing the abstract test scenario.
and the model itself is a good oracle for judging the correctness of the implementation in fact typically tests generated from executable models include not only the inputs used to trigger a particular model behavior but also the corresponding outputs produced by the model which can then be considered as the output expected from any implementation of the system for that test modulo abstraction.
figure shows this view if fis an abstraction function that maps the system to the model then to address for each abstract inm iin this work has been partially supported by nsf grant cns .the generated test pick some concrete ins iinf inm i and to address check that the concrete output produced by the system maps to the abstract output produced by the model i.e.
check that f outs i outm i. inm inm ... ins ins ... outs outs ...model systemoutm outm ... f fig.
concretizing model based tests a simplified view however the situation is often more complex especially for testing real time control systems which is our focus.
in practice real time systems often exhibit non deterministic behaviors such as run to run variations in timing.
executing model based tests on such systems can lead to false positives a system may behave correctly but still not match the model s behavior exactly as captured in the test.
the effects of the hardware platform on which the system executes have to be taken into account.
figure shows a better reflection of the typical situation.
inm inm ... ins ins ... outs outs ...model outm outm ... f software controller execution platform fig.
concretizing model based tests a typical scenario the model we deal with here is that of an embedded software controller which continually processes inputs updates its state and produces outputs in discrete steps.
abstracted away typically are the notion of time specifically the exact relationship between the execution steps and real time and the analog hardware interfaces in the sensors and actuators that interact with the real world.
the system under test sut is an implementation i.e.
software controller of the model.
ideally for conformance testing the behavior of the model must be compared to that of the sut represented by dotted linesin figure .
in practice the inputs and outputs have to be mediated through the hardware execution platform.
tests generated from models provide the inputs and outputs of the model at each execution step.
replicating the test scenario on an actual system in a test environment that simulates the real world requires accounting for these differences which usually involves more than a simple step wise concretization of inputs.
timing differences may require that the actual test environment must stimulate the hardware interfaces within a certain real time window.
further the environmental inputs that would lead to a particular input required for the modelbased test may depend on the real time instant at which the test environment provides the stimulus.
in these circumstances finding a concrete sequence of inputs to the target system to replicate the scenario produced by the inputs in the modelbased test is non trivial.
further the output produced by the system even when it is behaving correctly could be different from the outputs produced by the model because of timing and abstraction induced differences.
this makes using the model as the oracle problematic.
the majority of existing model based testing approaches are concerned with generating test cases and demonstrating that the sut conforms to the model.
techniques specifically for testing real time behaviors such as extending the model with non deterministic real time behaviors using timed automata or uppaal can potentially characterize system non determinism accurately.
these approaches however do not scale well since the introduced non determinism can increase reachable state space exponentially which also contradicts the original intent of using platform independent models.
oracle steering is an alternative approach that attempts to slightly change the model behaviors in order to accept nondeterministic real time behaviors.
in oracle steering the system is deemed to have passed the test if some model behavior can be found that is similar to the observed system behavior but there is no guarantee that the modified test inputs used to steer the model still retain the intent of the original test scenario.
if as is typical the test was generated to achieve a certain purpose we need a way to ensure that the purpose is indeed realized when each time the test is executed on the target platform.
in this work we propose a complementary approach that translates a sequence of abstract model inputs to an equivalent sequence of concrete system inputs which would considerably advance the utility and practicality of model based testing.
equivalence here is to be construed broadly as a relation between finite sequences of test inputs and outputs that is sufficient to capture the notion of test scenario.
ii.
b ackground and problem statement a. model based testing model based testing broadly refers to the use of models of software to perform software testing.
in particular models are frequently used to derive test suites and oracles.
the model describes in some abstract fashion input output sequences that are possible or acceptable.
the sut is considered a black box whose input output sequences must conform to that described by the model .of particular interest to us are the use in testing of models of reactive systems which are typically specified as extended finite state machines or labeled transition systems .
such models provide an operational view of the system that enables execution simulation of the model over a series of steps.
tests generated from such models capture the input provided to the model and the corresponding output produced by the model which becomes the oracle information against which the output produced by the sut is compared.
notionally one may view a test execution as providing the same input to the sut and the model and checking that the corresponding outputs match at each step.
there is a rich body of research in the use of such behavioral models for test generation .
these models by necessity abstract away implementation details which makes them amenable to various automated analyses.
we call these platform independent models .
however since these models also provide an operational view executable representations are often derived from these models using automated translation as well as manual coding.
executable implementations for the target hardware environment can often be derived from these models.
such implementations are called platform specific implementations .
these typically have additional components e.g.
input and output devices and details that are not represented in platform independent models.
in particular there are typically timing delays associated with input and output devices code execution and communications between components.
furthermore these timing aspects are non deterministic e.g.
a sensor s sampling routine may take a non deterministic amount of time to process data .
however the platform independent model is specified as a discretetime transition system where time progresses in discrete steps between computations.
reliably reproducing a test scenario on the platform specific implementation that is equivalent to a given test scenario for the platform independent model requires reconciling differences induced by the timing abstraction and non determinism which we address in the present work.
b. the four variable model in order to precisely characterize timing at different system boundaries our approach uses and extends parnas fourvariable model.
figure shows the four variable model defined by parnas et al.
.
monitored variables are used to express physical environment changes that can be observed by the execution platform.
the execution platform typically uses input devices i.e.
sensors to observe the status of monitored variables.
controlled variables are used to express physical environment changes that can be enforced by the execution platform.
the execution platform typically uses controlled variables to characterize changes and uses output devices i.e.
actuators to enforce them.
input variables andoutput variables are used to express inputs and outputs of the software controller or the platformindependent model which are represented by sof.
for a given variable v we use vtto represent the timefunction of its value .
given definitions of the four variables the following relations can be defined.
in relation mt it 2in where mtand itrepresent the vectors of monitored and input variables respectively environmentmonitored variables in relationinput devicesoutput devicessoftware environmentinput variablesoutput variablescontrolled variablessystem out relation sof relationfig.
the four variable model represents the physical interpretation of the input devices.
sof relation it ot 2sof where itand otrepresent the vectors of input and output variables respectively represents the software system with input output behavior.
out relation ot ct 2out where otand ctrepresent the vectors of output and controlled variables respectively represents the effects of the output devices.
c. problem statement we can now describe the problem as follows.
given a test case it ot forsof we want to find a test case mt ct for in sof out such that it ot 2in mt out ct .
further executing the system level test mt ct onin sof out should exercise sof in a way that is equivalent to executing the test it ot onsof .
we will leave the notion of equivalence to be informally understood as the intended scenario in the test it ot .
in words we seek a system test case that is equivalent to a given software test case.
however in practice we do not need an equivalent system test case but rather a method to test the system in an equivalent way.
this can be achieved by first finding equivalent system inputs then executing the system with those inputs and finally verifying that the output produced by the system is equivalent to the output expected of the software.
the problem can then be formulated as given input variable vector itand the relation in find monitored variable vector mt such that mt it 2in.
similarly given controlled variable vector ctand the relation out find output variable vector ot such that ot ct 2out .
the first goal would enable execution of model based tests on platform specific implementations and the second goal would enable the use of the model as the oracle when executing those tests on platform specific implementations.
as used in the literature of testing real time systems we define false positive andfalse negative as the following false positive if a test fails on a system that is acting correctly we call it a false positive.
false negative if a test passes on a system that is acting erroneously we call it a false negative.
d. motivating example we use a pca patient controlled analgesia infusion pump system as an example to illustrate the problem and our approach throughout this paper.
a pca infusion pump systemis a safety critical medical device that physically interacts with a patient by injecting medication for the purpose of pain relief.
the infusion is controlled using several sensors and actuators.
the patient can control the device via a user interface and a pump motor is used to apply force so that the medication can flow from the syringe to the patient through intravenous tubes.
various sensors are used to detect abnormal conditions such as empty reservoirs and air in line when happened the patient is notified by actuators such as buzzers and led lights.
t1t2t3c flow rate timet0 fig.
flow rate change due to starting infusion and detecting air in line on the platform specific implementation in a simplified scenario from the infusion system suppose there are two monitored variables mstart infusion which is a button that the patient can press to start infusing and mairinline which is a sensor that monitors if there is air in the flow of the medication.
correspondingly there are two input variables istart infusion which indicates if the startinfusion button has been pressed and iairinline which indicates if air in line has been detected by the sensor.
furthermore oflow rate is an output variable that represents the computed infusing flow rate and cflow rate is the actual flow rate enforced by the pump motor.
suppose we have a model based test scenario in which istart infusion andiairinline become true at the same time.
when executing this test scenario on the model oflow rate is always which is the expected behavior.
otherwise the air could be infused and cause serious consequences to the patient.
when executing this test scenario on a platform specific implementation however this test may pass or fail depending on the platform.
specifically an execution platform may have a longer delay in converting the quantity ofmairinline into iairinline than converting the quantity of mstart infusion into istart infusion which can happen because data sampling and processing in the sensor can take longer time than transmitting an electrical signal in the button.
then figure illustrates how cflow rate changes.
specifically mstart infusion and mairinline become true att0 istart infusion becomes true att1 whichstarts infusing iairinline becomes true att2 which tries to stop infusing by setting oflow rate to eventually cflow rate becomes at t3because of the delay in the pump motor.
as a result a failure is rendered even if the system is acting correctly which is a false positive because the model oracle does not take into account delays from the platform specific implementation and the original test scenario has also been changed.
the effect of such shifting may affect testing effectiveness in multiple ways.
first although the scenario at system level is pressing start infusion button with air in line detected the scenario at the model level becomes air in line is detected during infusion .
thus it is not surprising that the model based oracle does not match system outputs.
second depending on the types of tests generated from the model for example model based tests may execute a certain part or behavior of the system with a specific combination of inputs.
when such a combination is lost testing may fail to find the faults that should have been found.
alternatively considering the longer delay the air in line sensor has we could potentially schedule mairinline to betrue before mstart infusion becomes true such that iairinline andistart infusion can be true at the same time on the software controller.
iii.
a pproach a. framework definitions we build on the four variable model and introduce a few additional definitions.
for a given variable v we use vtto represent the time function of its value where the domain consists of real numbers i.e.
time and the range consists of all possible values of vin a real time environment.
we also define vkto represent the step function of its value where the domain consists of integers i.e.
steps and the range consists of all possible values of vin a discrete time environment.
furthermore the value of vat time tand step kare represented byvt t andvk k respectively.
model based tests and sof delay in a system with rinput variables and soutput variables we characterize a specific model based test interaction in the following form ik k1 ik k1 ik r k1 and its oracle in the form ok k1 ok k1 ok s k1 where k1is a specific step number e.g.
the first step .
test execution on the model and its implementation is considered to be step wise i.e.
in terms of discrete time .
for example when executing a test case for conformance testing of a simulink model and a corresponding c implementation running on the target platform at each step a test interaction is executed on both the model and the implementation and their outputs are compared.
the implementation conforms to the model for this test case if there is no discrepancy between their outputs at every step.
thus defined model based tests and oracles implicitly have the following two features.
model based tests and oracles are in terms of discrete time which makes it far easier to record the actual outputs andcompare them with the oracle.
a naive value comparison for each output variable at each step would do the work.
the test and oracle execute at the same time which abstracts away the fact that the execution itself takes time and this zero delay assumption can be problematic in modelbased testing.
now we lift this test interaction and its oracle from discrete time to real time it t1 it t1 it r t1 and its oracle would be ot t1 sof ot t1 sof ot s t1 sof where t1is the time of step k1and sofis the execution delay of the software controller.
note that while we consider timing of different sensor and actuator components separately we view the software controller as a single synchronous component.
that is the software controller takes all input variable values at the same time t1and produces all output variable values at the same time t1 sof1 where sof represents the execution delay of the software controller and it is a platform specific non deterministic value.
in delay the in relation maps monitored variables to input variables and we define the delay from a change to monitored variables to a change to input variables as in delay .
in relation is defined between two vectors i.e.
mtand it while the exact mapping between elements of mtand it are implicit.
when it comes to in delay informally within an input device component ijis the delay from the time that the corresponding monitored variable changes values to the time that input variable ijchanges values.
and we do not explicitly define which monitored variable s map to ijin order to simplify our definition but such a mapping indeed exists.
for example if we have an input device component that monitors flow rate the mflow rate variable represents the actual flow rate and the iflow rate variable is a sampled flow rate from the flow rate sensor.
therefore mflow rate representing the delay from the time mflow rate changes value to the time its input variable s change values would be equivalent to iflow rate representing the delay from its monitored variable s change values to the time iflow rate changes value.
in a special case where pmonitored variables have a oneto one mapping to pinput variables we would have that mj is equivalent to ijwhere j p. given a specific test interaction at system level mt t1 mt t1 mt p t1 the corresponding test interaction at the software controller level would be it t1 i1 it t1 i2 it r t1 ir where ijrepresents the timing delay defined above.
ideally if all the delays are known values and if we want an input variable vector it t1 it t1 it r t1 1technically it is impossible even for two consecutive assignments to happen at exactly the same time but the difference is often too small to be captured so we still treat them as at the same time.the monitored variable vector would be mt t1 m1 mt t1 m2 mt p t1 mp therefore if we know all the exact values of which is unfortunately non deterministic but can often be characterized all monitored variable values can be perfectly aligned such that given this monitored variable vector the software controller will get the input variable vector and produces an output variable vector that matches its oracle.
out delay the out relation maps output variables to controlled variables and we define the delay from a change to output variables to a change to controlled variables as outdelay which can further be defined in the same way as indelay by replacing monitored variables with output variables input variables with controlled variables and sensors with actuators.
in a system with qcontrolled variables when executing the above monitored variable vector we will get a controlled variable vector as the following ct t2 ct t2 ct q t2 similarly the output variable vector would be ot t2 o1 ot t2 o2 ot s t2 os other relationships can be characterized similarly as in monitored and input variables.
environment software controller hardware platformm event input devicesoutput devices o eventc event i event fig.
the architectural view of the platform specific implementations events we define an event as setting any one of the m i o c variables to a specific value at a specific time.
we do not require that two consecutive events to be different that is if we take flow rate as an example samples obtained within a certain period of time may all be the same.
physical events e.g.
pressing a button can be defined similarly that is a button pressed electrical signal will set the button pressed input variable to be true.
specifically if we take the flow rate sensor as an example the sensor itself runs a sampling routine at a certain rate.
the actual flow rate is a monitored variable and the sampled flow rate is an input variable.
the software controller always takes existing input variables values to update its internal state.
in our case of executing model based tests each test interaction has events for all the m i o c variables.
we use m i o cevent to represent events that set values of m i o c variables respectively.b.
platform specific implementations figure shows an overview of the execution environment of platform specific implementations.
specifically a platformindependent model can often be translated and compiled to the code that runs on the hardware platform and we refer to this piece of code as software controller .
besides a platformspecific implementation also contains input devices i.e.
sensors and output devices i.e.
actuators .
the sensor takes stimuli from the physical environment while a stimulus can be a physical event e.g.
pressing a button or pre processed inputs e.g.
sampling flow rate and the actuator causes some effect in the physical environment e.g.
starting a motor or turning on a light .
computingwaitingstep step inputs outputs fig.
the input output timing in real time step figure shows typical real time steps in the software controller.
the step starts with obtaining inputs to update its internal state then producing outputs and waiting for a fixed time frame.
we define step size on the implementation as the time interval between two consecutive i events or o events.
we have described how monitored and output variables can be derived from input and controlled variables respectively in an environment of known delays of input and output devices.
however all the delays are non deterministic on the execution platform.
that is when executing a reverse mapping mtfrom iton the platform the resulting input variable vector i0tis not always equivalent to it.
low delayhigh delayi event time windowm event time window500 ms ms ms ms fig.
time window for m and i events here we define the notion of time window as a period of time during which an event should occur such that when executed on the platform with non deterministic delays it can trigger expected effects.
figure shows an example.
we assume that one step is mapped to ms in this example.
suppose that an i event is expected between the time ms and ms as shown then the time window for thecorresponding m event is estimated based on the upper bound shown as high delay and lower bound shown as low delay of delays.
note that however when the variance of delays is too large the time window for m event can be too small or even does not exist.
c. direct execution of model based tests figure shows executing a test case on a platform specific implementation.
when an m event is given a c event i.e.
a response is expected during the time we refer to as estimated c event time window.
due to various delays i.e.
in delay sof delay and out delay the actual c event cannot be produced within the expected time frame.
thus there is a mismatch and this test fails although the software controller works as expected.
environment input and output devices software controlleractual c eventm eventestimated c event time window in delay sof delayout delay fig.
real time execution d. scheduling m events figure shows how our approach schedules m events.
environment input and output devices software controllerestimated i eventscheduled m event estimated i event time windowmean delay fig.
scheduling m events given a model based test interaction we start with estimating a future time window during which all i events should occur for all input variables.
suppose the largest delay of all input variables is max and the step size is step.
both max andstep are mean delay values obtained from the execution history .
in order to maximize the possibility that an input event can occur during our expected time window the estimated future i event time should be in the middle of two consecutive steps.
therefore if we have current as the timestamp at which the software controller finishes one step then the future time time i events when all i events of the current test interaction should happen is estimated as time i events current step ceil max step step stepthen a monitored variable mjwith delay mjshould be sent at time time m event j time i events mj which is shown as scheduled m event in figure .
although this is also an online real time system testing approach the overhead is minimized.
model based test generation is offline to take advantages of many automated tooling support.
during online execution the scheduling overhead is often too small to create timing discrepancies.
however when too many events are scheduled to happen at the same time the overhead can be significant enough to affect system behavior.
therefore we also set a threshold such that events with similar scheduled time specifically similar scheduled time is in this case all subsequent events that have a scheduled time no later than ms from the current event will be combined and sent together to mitigate the scheduling overhead.
e. adjusting c events figure shows how our approach adjusts c events.
environment input and output devices software controllerestimated o eventactual c event estimated o event time windowmean delay fig.
adjusting c events if scheduling m events has been done successfully the software controller should have executed the expected test scenario originated from model based tests and produced oevents that match model based test oracles.
but what the environment receives are c events.
then all we need to do is to bring the timing of c events back to the model level and compare to see if they match the oracle output.
specifically given the same time i events from scheduling m events we define a time window during which the corresponding o event should happen i.e.
between time i events andtime i events step.
therefore given a c event at time time c event j an output variable ojwith delay ojshould have been produced at time time o event j time c event j oj which would fit into one of the expected o event time windows and its value is compared with oracles to determine if the test passes or fails.
f .
benefits of our approach test cases for system testing are usually written manually which is a time consuming process and often only a small number of test cases can be written and executed leaving potentially many aspects of the system untested.
although the software itself may have been tested heavily it is unlikely to have been exercised in the context of system level test scenarios.when model based tests are used to test an implementation frequently the wrong test cases may be executed because the scenario exercised is quite different and then compared with the wrong oracle because the model and the implementation exercised different behaviors .
our approach provides a way to execute model based tests or any test cases that can be generated from a platform independent model in a way that is appropriate for the specific implementation platform.
while test cases can be generated relatively easily from platform independent models without the proposed framework detailed models of platform specific components are needed to derive tests that can be executed on the implementation.
this makes test generation harder because of model complexity and affects scalability.
the proposed approach addresses this by abstracting the platform specific details to a minimal set of parameters e.g.
the mean and variance of timing delays and using that information during test execution instead of test generation.
iv.
e valuation a. research questions we wish to evaluate the following two aspects first to what extent does the proposed approach reduce false positives?
false positives naturally arise during testing platform specific implementations using model based tests.
starting with a test suite that passes on the platformindependent model we would like to understand the percentage of tests that can still pass on the platform specific implementations with different timing configurations.
then we can evaluate how well the false positive rate can be reduced using the proposed approach.
furthermore what are the platform specific characteristics that affect the effectiveness of our approach?
specifically we would like to characterize the applicability of the proposed framework in terms of timing delays at different system boundaries.
second does the proposed approach introduce additional false negatives?
some existing techniques that test platformspecific implementations aiming at reducing false positives often introduce additional false negatives which can be particularly bothersome for safety critical systems.
empirical evidence of limiting false negatives using the proposed approach will strengthen the case for its applicability to safetycritical system domains.
b. case study example we used a pca patient controlled analgesia infusion pump system as our case study example .
this system is modeled using mathworks simulink and stateflow .
the top level system has input variables and output variables.
this system also contains seven subsystems.
table i shows basic information of the subsystems.
some of the subsystems have been used as case examples in previous studies .
c. device configuration code for platform testing was generated from the simulink and stateflow models using the matlab simulink coder .table i infusion pump subsystem information subsystem input vars output vars alarm config infusion manager logging system statistics system monitor top level mode this tool produces platform agnostic c code.
to execute this code on a target platform specific code is needed to configure the target peripherals and memory.
the target platform manufacturer provides this code in the form of a board support package bsp .
other than a small assembly language bootstrap the remainder of the bsp consists of libraries of short routines for interfacing with the platform s peripherals e.g.
clocks i o timers .
a small amount of handwritten code was required to create the interrupt service routines isr for asynchronous sensor inputs such as button presses and limit switch activations.
the top level executive is a simple infinite loop that repeatedly executes the simulink generated code and then sleeps until the next execution cycle time.
during sleep isrs are still supported to prevent sensor inputs from being lost between increments.
the platform used for this testing was an atmel arm91sam7x development board.
in order to show the generic nature of the development procedure described here the same code was recompiled and executed on a pololu orangutan svp robot controller.
external hardware was added to simulate the functions of the infusion pump including a motor buttons to simulate user input and limit sensors annunciators and led displays.
the on chip usart was used to create a simple monitor port to allow printf statements to be observed during execution.
a ms timing loop was used to slow down execution to allow ample time for user interaction with the system.
the selection was arbitrary and could have been significantly shorter.
in order to automate interactions with the device during testing for sending sensor events and receiving actuator events the device was connected to a pc through a serial port.
d. simulating sensors and actuators we identified multiple sources of timing delays and we wanted to assess how the delays affect testing and how our approach reconciles timing induced mismatches.
in order to create a variety of platform specific configurations we further created additional sensor and actuator timing delays.
specifically the modeled sensor has two threads.
the first thread constantly reads m events from the test driver and the second thread creates additional delays for each m event.
when an m event s delay time has elapsed the m event will be sent to the software controller as an i event.
i events will update the software controller s input variable states used to update the software controller s internal state in the next step.
similarly the modeled actuator has two threads in which the first thread constantly reads o events from the software controller and the second thread creates additional delays foreach o event.
when an o event s delay time has elapsed the o event will be sent to the test driver as a c event.
in this study randomized sensor and actuator delays follow a normal distribution.
for a platform specific configuration we characterize it using max mean andstandard deviation .max mean is used to provide an upper bound of mean delay for each event.
in a specific platform each event would have a random mean delay between and max mean and a fixed standard deviation .
the mean delay and standard deviation of each event is used to create a randomized delay during execution.
note that on both the sensor and the actuator if the generated random delay is negative is used.
in our study max mean values used were and ms and standard deviation can be and ms. therefore we would have different platformspecific configurations plus the ideal situation in which both max mean and standard deviation are i.e.
there is no delay2.
e. test case generation though test cases from any source can be used with our approach we generated our test suite using simulink design verifier to take advantage of its automated model based test generation capability.
tests were generated for the branch coverage criterion to provide a rich and realistic set of cases.
specifically there were a total of branch coverage objectives for the model.
among them test cases could be generated to satisfy objectives.
objectives were proven to be unsatisfiable and objectives were undecidable within the given time budget hours in our experiment .
combinedobjectives nonlinear extended option was used to reduce the test suite size while maintaining coverage.
as a result the test suite was reduced to test cases with a total of test interactions.
test oracles were also generated from the model and later used to check if each test interaction passes or fails on platform specific implementations.
we executed the test suite times for each of the different platform specific configurations once with the framework mediated timing adjustments and once directly executed for a total of times with a total of more than 220k test interactions.
f .
mutation generation we generated mutants using milu a mutation testing tool for c programs .
we generated mutants evenly distributed across the seven subsystems based on the total number of possible mutants in each subsystem.
equivalent mutants which are behaviorally equivalent to the original system can jeopardize the use of mutation testing.
although detecting equivalent mutants is possible specifically in the case of finite state systems it does not scale well.
therefore we did not attempt to remove equivalent mutants since it is cost prohibitive for our case example systems.
2this is essentially the same as executing test cases on the platformindependent model although the system still exhibits negligible delays due to e.g.
transition.we ran our test suite on all the mutants in an execution platform with max mean to be ms and standard deviation to be ms. we first executed the test suite directly on the implementation without delays to determine if the mutant can ever be killed.
if so we then executed the test suite using our approach in the above platform specific configuration and check if those failed tests can still fail.
v. r esults d iscussion in this section we address our research questions and discuss the implications of our results.
we begin by presenting false positives that can be reduced using our approach.
a. reducing false positives we ran the full test suite with test interactions on different platform specific configurations.
on each configuration we ran the full test suite times in order to account for non determinism in terms of timing from the hardware platform as well as from the injected sensor and actuator delays.
all test interactions are supposed to pass on the platform specific implementations.
table ii median number of passed tests and percentage point decrease in false positives max std.
scheduled direct fp mean deviation execution execution decrease .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table ii shows the number and percentage of passed test interactions using the proposed approach i.e.
scheduled execution and direct execution of model based tests on each platform specific configuration.
the percentage point decrease in false positives is simply calculated by subtracting failed percentage of test interactions using scheduled execution from that using direct execution.
it is not surprising that direct test execution and output comparison are very sensitive to time fluctuation.
as timing delays are randomly injected direct execution would start to fail quickly when delays increase.
in general mean delay dominates the number of passed failed tests in direct execution which is also intuitively straightforward since an event is more likely to miss its time window with larger delays leading to unexpected output.
standard deviation may affect direct execution in multiple ways.
a larger standard deviation in general leads to more failed tests with small max mean delays i.e.
and ms but since direct execution is completely unguided a larger standard deviation may also have more passed tests e.g.
and for the same max mean delay.
this happens because the configuration randomly assigned higher delays to those variables that are more sensitive to time fluctuation.
with large max mean delays e.g.
and ms around tests would fail in spite of standard deviation.
the proposed approach reduces false positives in all cases except the case where there is no false positive .
as shown in table ii our approach is robust to absolute delay values i.e.
max mean but can still be sensitive to delay variance i.e.
standard deviation when it is large.
specifically our approach can reduce most false positives with a standard deviation less than ms despite mean delays but start accumulating false positives with the ms standard deviation.
the event scheduling mechanism in our approach ensures that each event can fit in the right time window but if we recall the definition of time window in figure for example a larger delay variance would lead to a narrower time window for m events.
when the delay variance is large enough the time window for the corresponding m event may not exist thus the event scheduling cannot guarantee that the i event can be received by the software controller at the right step.
our approach always estimates and expects i events to happen in the middle of two consecutive steps which already give both direct and scheduled executions certain tolerance to timing delays.
in our study we used an almost fixed delay to schedule m events from i events and convert c events to oevents.
since the actual sensor actuator have non deterministic timing delays it would be difficult for the event scheduling to be accurate when the delay variance is too large.
it would still be possible to schedule input events accurately if there are more known characteristics of the delay distribution.
then applying a more sophisticated scheduling algorithm would reduce false positives further.
for example the delay may vary widely overall but may not change much during a short period of time.
in such a case using a short period of history delays would make the scheduling much more accurate.
we leave such improvements as future work.
we used large variances of delays for the purpose of evaluation.
although our approach did not perform well on the ms standard deviation settings they may not even be realistic in practice.
for example even if we assume normal distribution of delays with a mean of ms and a standard deviation of ms then .
of the values will be within the range of ms and .
of the values will be within the range of ms which are already unrealistically large ranges of delays.
b. eliminating false negatives reducing false positives in our approach is essentially accepting good system behaviors that would be rejected otherwise but it may also run the risk of accepting bad system behaviors.we first performed a mutation testing on the platformindependent model.
for each of the generated mutants we ran the full test suite with test interactions and recorded the test interactions that failed on the mutant.
specifically in our i.e.
test interactions on mutants test interaction executions test interaction executions failed due to injected mutants.
we selected a representative platformspecific configuration with a max mean delay of ms and astandard deviation of ms. we then performed the same mutation testing on the platform specific implementation and observed whether each of the failed test interaction executions still fails and we call it false negative if it passes.
in our experiment we did not observe any false negative as a result of using our approach.
existing approaches e.g.
oracle steering may accept system behaviors that are similar enough to the model behaviors while a similarity threshold is often set manually in order to constrain and balance the number of false positives and negatives.
unlike prior work our approach does not modify the model or the oracle for the purpose of accepting system behaviors with discrepancy avoiding the risk of accepting bad system behaviors.
nevertheless since testing itself is incomplete and has false negatives i.e.
testing can only show the presence of faults not their absence our approach only ensures that no additional false negatives can be introduced.
thus the quality of the original model based tests plays an important role in finding faults and reducing overall fault negatives.
vi.
t hreats to validity external validity we used only one system as the case example to evaluate our approach.
our experiment could be limited but we actively worked with domain experts to set up a realistic experimental environment.
although there is only one case example in our study the infusion pump system consists of subsystems some of which have been used in previous studies as standalone systems.
we used several mathworks tools for building models and generating code and test cases.
we also used the actual hardware device in our study.
since these are widely used commercial tools we believe that our results can be generalizable to other systems in these domains.
besides the actual delays on the device we also simulated additional delays in order to create a variety of platformspecific configurations.
these simulated delays however may not represent realistic hardware specifications but they do show that our approach can work in a variety of platformspecific configurations.
internal validity we used a specific test suite generated from simulink design verifier satisfying branch coverage on the model in our experiments.
the results may change if other forms of test suites are used.
however both branch coverage and model based tests are commonly used in practice.
we used measured delay information in our approach to schedule and adjust events.
the measured delays in our experiments may not reflect the precision that one could obtain in practice.
however we also simulated unrealistically largedelays and large variances of delays in order to account for possible biases.
construct validity we used mutants rather than real faults to demonstrate that our approach can eliminate false negatives that can possibly be introduced by other similar techniques.
it is possible that using real faults would lead to different results although mutation testing has been widely used and has been shown to be similar to real faults .
vii.
r elated work kim et al.
defined testing and verification frameworks using an approach similar to the one described here that can precisely capture timing at different system boundaries .
specifically they manually created a limited number of system level tests from system timing requirements.
additional delays due to other components such as hardware are measured and added to the timing requirements to relax passing criteria reducing false positives from executing model based tests on platform specific implementations.
however the frameworks do not attempt to reproduce the intended scenario of the original model based tests and use only single stimulus e.g.
pressing a button .
the goals also do not include automated test generation and execution.
for a system at the scale of our case example creating thousands of test interactions each of which involves more than variables would be impractical without automation.
several techniques support limited forms of nondeterminism by introducing extended model formats such as timed automata with real valued clocks and uppaal .
specifically non deterministic timing behaviors are introduced in the models.
false positives can be reduced if the software model can explicitly account for the same amount of non determinism as the system has.
these approaches however often result in an exponential increase in the difficulty of demonstrating conformance restricting the amount and type of non determinism that can be handled.
our approach does not require changes to the original software model to account for non determinism.
in fact adding system non determinism contradicts the original intent of using platform independent models which abstract away platformspecific details to make the models amenable to various analyses.
besides introducing non determinism can often increase false negatives.
while false positives in testing embedded realtime systems often lead to wasted manual effort false negatives i.e.
missed faults can lead to catastrophic consequences.
furthermore our approach decouples test generation and execution as well as platform independent models and platformspecific non determinism and therefore can be used to extend many existing approaches.
larsen et al.
defined an online testing framework based on uppaal models for testing real time systems .
in their work test cases can be generated and executed and test results can be checked all online.
a test step is generated from the model and executed on the sut at one time which reduces the size of reachable state space and thus improves scalability.
despite the fact that their work also requires changes to the original model to accept non deterministic system behaviors this online testing relies on fast test and oracle generation and behavior comparison which often introduce significantoverhead that can change input output events and thus system behaviors.
therefore its application is restricted by the size and complexity of systems.
while our approach to test execution and comparison is also online model based tests and oracles are generated offline .
event scheduling happens online but the overhead is typically negligible.
oracle steering was recently proposed as an alternative approach to reduce false positives in which the model iscarefully steered to exhibit behavior that is closer to the observed behavior of the sut.
to ensure that undesirable system behaviors are not overlooked by the oracle constraints are placed on how far the model could be steered to accommodate the observed system behavior.
this is achieved by first attempting to execute equivalent tests one step at a time on both the system and the model and when there is a mismatch of the corresponding outputs modifying the model inputs and often parts of the model state within reasonable bounds so that the model behavior can be nudged closer to the system behavior.
empirical assessment of the approach confirmed that such a steered model is more effective as an oracle the number of false positives reported mismatches not attributable to real system defects is greatly reduced but the number of false negatives has also increased.
furthermore the approach is defined and built over discrete time and may not be able to handle the richness of real time behaviors.
viii.
c onclusions we have described our testing framework based on the four variable model defined by parnas et al.
and the framework for testing timing defined by kim et al.
.
our testing framework precisely captures timing of different hardware components.
our scheduling based approach enables executing model based tests on platform specific implementations.
our approach brings together the advantages of existing automated model based testing and oracle generation tools.
tests and oracles are generated offline while test scheduling execution and test result checking are online.
this approach has the benefit of online testing the scheduling can be adaptive to the timing change at runtime but avoids common problems in online testing tests are generated offline using existing tools to reduce online overhead.
future work is based on the following observations.
our approach takes a fixed mean delay of each hardware component and schedules events.
this approach does not perform very well in an execution platform where delays have large variance.
instead a more sophisticated scheduling algorithm can be applied that can make better use of delay information.
event sequences generated by our approach can contain important timing information.
for example the timing gap between two events specifically between an m event and a cevent can reflect whether the platform specific implementation satisfies system level timing requirements.
acknowledgment we would like to thank anitha murugesan at the university of minnesota for providing support for the simulink and stateflow models.