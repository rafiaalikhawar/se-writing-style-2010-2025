learning to spot and refactor inconsistent method names kui liu dongsun kim tegawend e f. bissyand e taeyoung kim kisub kim anil koyuncu suntae kim yves le traon interdisciplinary centre for security reliability and trust snt university of luxembourg luxembourg kui.liu dongsun.kim tegawende.bissyande kisub.kim koyuncu.anil yves.letraon uni.lu department of software engineering chonbuk national university south korea rlaxodud1200 jipsin08 gmail.com abstract to ensure code readability and facilitate software maintenance program methods must be named properly.
in particular method names must be consistent with the corresponding method implementations.
debugging method names remains an important topic in the literature where various approaches analyze commonalities among method names in a large dataset to detect inconsistent method names and suggest better ones.
we note that the state of the art does not analyze the implemented code itself to assess consistency.
we thus propose a novel automated approach to debugging method names based on the analysis of consistency between method names and method code.
the approach leverages deep feature representation techniques adapted to the nature of each artifact.
experimental results on over .
million java methods show that we can achieve up to percentage points improvement over the stateof the art establishing a record performance of .
f1measure in identifying inconsistent method names.
we further demonstrate that our approach yields up to accuracy in suggesting full names while the state of the art lags far behind at .
accuracy.
finally we report on our success in fixing inconsistent method names in a live study on projects in the wild.
if you have a good name for a method you don t need to look at the body.
fowler et al.
i. i ntroduction names unlock the door to languages.
in programming names i.e.
identifiers are pervasive in all program concepts such as classes methods and variables.
descriptive names are the intuitive characteristic of objects being identified thus correct naming is essential for ensuring readability and maintainability of software programs.
as highlighted by a number of industry experts including mcconnell beck and martin naming is one of the key activities in programming.
naming is a non trivial task for program developers.
studies conducted by johnson concluded that identifier naming is the hardest task that programmers must complete.
indeed developers often write poor i.e.
inconsistent names in programs due to various reasons such as lacking a good thesaurus conflicting styles during collaboration among several developers and improper code cloning .
method names are the intuitive and vital information for developers to understand the behavior of programs or apis .
therefore inconsistent method names can make programs harder to understand and maintain and may even lead to software defects .
poor method namespublic boolean containsfield field f return fieldslist.contains f private resolvedmember findfield resolvedtype resolvedtype string fieldname for resolvedmember field resolvedtype.getdeclaredfields if field.getname .equals fieldname return field return null public field containsfield string name for iterator e this .field vec.iterator e.hasnext field f field e.next if f.getname .equals name return f return null fig.
.
motivation examples taken from project aspectj .
are indeed prone to be defective.
for example the commonlyused findbugs static analyzer even enumerates up to ten bug types related to method identifiers.
figure provides examples from project aspectj to illustrate how inconsistent names can be confusing about the executable behavior of a method.
the name of the first method containsfield suggests a question and is consistent with the method behavior which is about checking whether the fieldslist contains the target field f. the second method implements the search of a field in the target dataset and is thus consistently named findfield .
the third method is implemented similarly to the second method findfield b u t is named containsfield as the first method.
this name is inconsistent and can lead to misunderstanding of api usage.
as a preliminary study on the extent of the inconsistent method naming problem we investigated posts by developers and users on fora and code repositories.
we performed a search using composite conjunctions of method name and a category of keywords i.e.
inconsistent consistency misleading inappropriate incorrect confusing wrong bug and error t o match relevant questions in stackoverflow and commit logs in github .
as a result we managed to spot questions and commits.
figures show some excerpts of retrieved results.
additionally to assess the extent to which developers are prone to fix method names we investigated the history of changes in all projects collected for our experiments in commits a method name is changed without any change to the corresponding body code.
we further tracked future changes and noted that in of the cases the change is final i.e.
neither the method body nor the method name is changed again in later revisions of the project .
these findings suggest that developers are indeed striving to choose appropriate method names often to address consistency with the contexts of their code.
ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a from questions in stackoverflow.
b from commit logs in github.
fig.
.
excerpts of spotted issues about inconsistent method names.
to debug method name h st and stvold explored method naming rules and semantic profiles of method implementations.
kim et al.
relied on a custom code dictionary to detect inconsistent names.
allamanis et al.
introduced the na turalize framework learning the domain specific naming convention from local contexts to improve the stylistic consistency of code identifiers with n gram model .
then building on this framework they proposed a log bilinear neural probabilistic language model to suggest method and class names with similar contexts .
the researchers leveraged attentional neural networks to extract local timeinvariant and long range topical attention features in a contextdependent way to suggest names for methods.
overall their context information is limited to local identifier sub tokens and the data types of input and output.
while the state of the art has achieved promising results a prime criterion of naming methods has not been considered the implementation of methods that is a first class feature to assess method naming consistency since method names should be mere summaries of methods behavior .
examples shown in figure illustrate the intuition behind our work methods implementing similar behavior in their body code are likely to be consistently named with similar names and vice versa.
it should be possible to suggest new names for a method in replacement to its inconsistent name by considering consistent names of similarly implemented methods.
in this paper we propose a novel automated approach to spotting and refactoring inconsistent method names.
our approach leverages paragraph v ector and convolutional neural networks to extract deep representations of method names and bodies respectively.
then given a method name we compute two sets of similar names the first one corresponds to those that can be identified by the trained model of method names the second one on the other hand includes names of methods whose bodies are positively identified as similar to the body of the input method.
if the two sets intersect to some extent which is tuned by a threshold parameter the method name is identified to be consistent and inconsistent otherwise.
we further leverage the second set of consistent names to suggest new names when the input method name is flagged as inconsistent.
to evaluate our proposed approach we perform experiments with methods of training data and methods with changed names of test data which are collected from open source java projects.
our experimental results show that the approach can achieve an f1 measure of .
in theidentification of inconsistent method names representing an improvement of about percentage points over the state ofthe art.
furthermore the approach achieves accuracy on suggesting first sub tokens and accuracy on suggesting accurate full names for inconsistent method names again outperforming the state of the art.
finally we report how our approach helped developers in fixing inconsistent method names in projects during a live study in the wild.
ii.
b ackground this section briefly describes three techniques from the field of neural networks namely word2v ec paragraph v ector and convolutional neural networks .
our approach relies on these techniques to achieve two objectives embedding tokens from method names and bodies into numerical vector forms and extracting feature representations for accurately identifying similar method names and bodies.
paragraph v ector paragraph v ector is an unsupervised algorithm that learns fixed length feature representations from variable length pieces of texts such as sentences and paragraphs .
this technique was proposed to overcome the limitations of bag of words features which are known to lose the ordering of the words and ignore the word semantics.
recent literature has provided evidence that paragraph vector outperforms other state of the art techniques for text representations and can effectively capture semantic similarities among words and sentences .
in our work we use paragraph v ector for training a model to compute similarities among method names considering sequences of method name sub tokens as sentences .
we expect this model to take into account not only the lexical similarity but also the semantic similarity for example function names containsobject andhasobject should be classified as similar names since both of them describe the functionality of code implementation to check whether a set contains a specific object put in argument s .
we detail in later parts of this paper how method names are processed in our approach to feeding the paragraph v ector algorithm.
convolutional neural networks cnns cnns are biologically inspired variants of multi layer artificial neural networks .
initially developed and proven effectiveness in the area of image recognition cnns have gained popularity for handling various nlp tasks.
for text classification in particular these deep learning models have achieved remarkable results by managing to capture the semantics of sentences for relevant similarity computation.
recently a number of studies have provided empirical evidence authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
to support the naturalness of software .
thus inspired by the naturalness hypothesis we treat source code in particular method bodies as documents written in natural language and to which we apply cnns for code embedding purpose.
the objective is to produce a model that will allow to accurately identify similar method code.
a recent work by bui et al.
has provided preliminary results showing that some variants of cnns are even effective to capture code semantics so as to allow the accurate classification of code implementations across programming languages.
in this study we use lenet5 a specific implementation of cnns which consists of lower layers and upper layers see figure for its architecture .
the lower layers are composed of alternating convolutional and subsampling layers which are local connected.
the upper layers are fully connected and correspond to a traditional mlp hidden layer logistic regression .
the input to the first fully connected layer is the set of all feature maps at the previous layer.
word2v ec when feeding tokens of a method body to cnns it is necessary to convert the tokens into numerical vectors.
otherwise the size of a cnn s input layer would be too large if using one hot encoding or interpreting its output can be distorted if using numeric encoding i.e.
assigning a single integer value for each token .
the machine learning community often uses vector representation for word tokens .
this offers two advantages a large number of unique tokens can be represented as a fixed width vector form dimensionality reduction and similar tokens can be located in a vector space so that the similar tokens can be dealt with cnns in a similar way.
our approach uses word2v ec to embed tokens of method bodies.
word2v ec is a technique that encodes tokens into ndimensional vectors .
it is basically a two layered neural network dedicated to process token sequences.
the neural network takes a set of token sequences i.e.
sentences as inputs and produces a map between a token and a numerical vector.
the technique not only embeds tokens into numerical vectors but also places semantically similar words in adjacent locations in the vector space.
iii.
a pproach this section presents our approach to debugging inconsistent method names.
as illustrated in figure it involves two phases training and identification suggestion.
in the training phase taking as input a large number of methods from real world projects it uses paragraph v ector for method names and word2v ec cnns for method bodies to embed them into numerical vectors hereafter simply referred to as vectors respectively.
eventually two distinct vector spaces are produced and will be leveraged in the next phase.
the objective of the training phase is thus to place similar method names and bodies into adjacent locations in each vector space.
the identification suggestion phase determines whether a given method has a name that is consistent with its body code by comparing the overlap between the set of method names that are close in the method name vector space and the set ofmethods names whose bodies are close in the method body vector space.
when the overlap is the name is considered to be inconsistent with the body code and suggested with alternative consistent names.
before explaining the details of these two phases we first describe an essential step of data processing that adapts to the settings of code constructs.
a. data preprocessing this step aims at preparing the raw data of a given method to be fed into the workflow of our approach.
we consider the textual representations of code and transform them into tokens i.e.
basic data units which are suitable for the deep representation learning techniques described in section ii.
given that method names and bodies have different shapes i.e.
names are about natural language descriptions while bodies are focused on code implementations of algorithms we propose to use tokenization techniques adapted to each method name tokenization method names are broken into sub token sequences based on camel case and underscore naming conventions and the obtained sub tokens are brought to their lowercase form.
this strategy has been proven effective in prior studies .
for example method names findfield and find field are tokenized into the same sequence find field where find andfield are respectively the first and second sub tokens of the names.
method body tokenization method bodies are converted into textual token sequences by following the code parsing method proposed in our previous study this method consists in traversing the abstract syntax tree ast of a method body code with a depth first search algorithm to collect two kinds of tokens ast node types and raw code tokens.
for example the declaration statement int a will be converted into a four token sequence .
since noisy information of code e.g.
non descriptive variable names such asa b can interfere with identifying similar code all local variables are renamed as the concatenation of their data type with the string var .
eventually the previous declaration code will be represented by the sequence .
b. training this phase takes tokens of method names and bodies in a code corpus to produce two numerical vector spaces that are leveraged to compute similarities among method names on the one hand and among method bodies on the other hand for eventually identifying inconsistent names and suggesting appropriate names.
note that the objective is not to train a classifier whose output will be some classification label given a method name or body.
instead we adopt the idea of unsupervised learning and lazy learning to embed method names and bodies.
token sequences of method names are embedded into vectors by the paragraph vector technique described in section ii since token sequences of method names resemble sentences describing the methods.
in contrast all tokens in a authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
method corpus method name tokenization method body tokenization data preprocessingtoken embedding word2vec method name embedding paragraph vector method body embedding cnns training identification and suggestion method being identified yesnoconsistent method name inconsistent method name with suggested new names mhd method name vector space method body vector spaceinconsistent ......name vector body vector fig.
.
overview of our approach to spotting and refactoring inconsistent method names.
method body are first embedded into vectors using word2v ec.
the embedded token vectors are then fed to cnns to embed the whole method body into a vector which will be used to represent each method body as a numerical vector.
token embedding for method bodies as shown in figure tokens of method bodies are embedded into individual numerical vectors before they can be fed to the cnns.
to that end the token embedding model is built as below tvb ew tb where ew is the token embedding function i.e.
word2v ec in our case taking as input a training set of method body token sequences tb.
the output is then a token mapping function tv b tw b vbw where tw b is a vocabulary of method body tokens and vbw is the vector space embedding the tokens in tw b. after token embedding a method body is eventually represented as a two dimensional numerical vector.
suppose that a given method body bis represented by a sequence of tokens tb t1 t2 t3 ... t k where ti tw b and vbis a twodimensional numerical vector corresponding to tb.
then vbis inferred as follows vb l tb tvb where lis a function that transforms a token sequence of a method body into a two dimensional numerical vector based on the mapping function tv b. thus vb v1 v2 v3 ... v k vb where vi tvb ti andvbis a set of two dimensional vectors.
since token sequences of method bodies may have different lengths i.e.
kcould be different for each method body the corresponding vectors must be padded to comply with a fixed width input layer in cnns.
our approach follows the workaround tested by wang et al.
and appends p ad vectors i.e.
zero vectors to make all vector sizes consistent with the size of the longest one see section iv for how to determine the longest one .
for example the left side of figure see section iii b2 for its description shows how a method body is represented by a two dimensional n k numerical vector where nis the vector size of each token andkis the size of the longest token sequence of bodies.
each row represents a vector of an embedded token and the last two rows represent the appended zero vectors to make all two dimensional vector sizes consistent.
embedding method names and bodies into v ectors v ector spaces are built by embedding method names and bodies into corresponding numerical vectors.
for method...... ...... n k at w o dimensional numeric vector representation of a method body input layer c1 feature maps s1 feature maps c2 feature maps s2 feature mapsoutput layerforstatement for type iteration va r i a b l e iterationv ar operator 0dense layer output is the embedded vector of method body ...... fig.
.
architecture of cnns used in our approach to vectorize method bodies where c1 and c2 are convolutional layers and s1 and s2 are subsampling layers respectively.
names we feed the sub token sequences i.e.
one sequence per method name to a paragraph embedding technique.
specifically we leverage the paragraph vector with distributed memory pv dm technique which embeds token sequences into a vector space as follows nvname epv tn where epv is the paragraph vector embedding function i.e.
pv dm which takes as input a training set of method name sub token sequences tn.
the output is a name mapping function nv name tn vn where vnis an embedded vector space for method names.
this step is similar to classical word embedding with differences in the mapping relationships.
the paragraph vector embeds a token sequence into a vector while word2v ec embeds a token into a vector.
for method bodies we need another mapping function where the input is a two dimensional numerical vector for each method body.
the output is a vector corresponding to a body.
this mapping function is obtained by the formula below vvbody ebv vb where ebv is an embedding function i.e.
cnns that takes the two dimensional vectors of method bodies vb as training data and produces a mapping function vv body .
note that vb vb1 vb2 vb3 ... v bm is obtained by l equation where vbi i and mis the size of training data.
vvbody is defined as vv body vb v prime b where v prime bis an embedded vector space of method bodies.
based on vv body we defined the body mapping function nv body as nvbody tb v prime b where nv body is the composition of land vv body in equations and respectively i.e.
nv body vvbody l tb vvbody l tb .
nv body takes a token sequence of a method body and returns an embedded vector representing it.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
our approach uses cnns as the embedding function ebv in equation .
figure shows the architecture of cnns that our approach uses.
the input is two dimensional numeric vectors of method bodies as stated in section iii b1.
the two pairs of convolutional and subsampling layers are used to capture the local features of methods and decrease dimensions of input data.
the network layers from the second subsampling layer to the subsequent layers are fully connected which can combine all local features captured by convolutional and subsampling layers.
we select the output of dense layer as the vector representations of method bodies which synthesizes all local features captured by previous layers.
note that vectors in the two vector spaces i.e.
vnand v prime b can be indexed by each method name.
for a given body vector of a method we can immediately find its corresponding name vector in the name vector space and vice versa.
this index facilitates the search of corresponding method names after locating similar method bodies for a given method.
c. identification suggestion this phase consists of two sub steps.
first the approach takes a given method as a query of inconsistency identification.
by leveraging the two vector spaces i.e.
vnandv prime b and the two embedding functions i.e.
nv name and nv body it identifies whether the name of the given method is consistent with its body.
second if the name turns out to be inconsistent the approach suggests potentially consistent names for it from the names of similarly implemented methods.
inconsistency identification for a given method mi w e can take a set of adjacent vectors for its name ni and body bi respectively i.e.
adj ni andadj bi .
after retrieving the actual names i.e.
name corresponding to vectors in adj ni andadj bi we can compute the intersection between the two name sets as cfull cfull name adj ni name adj bi ifcfull is we consider bito be inconsistently named ni.
however cfull in equation is too strict since it relies on exact matching.
in other words there should exist the same character sequences between two name sets.
for example suppose that there is findfield inname adj ni and findelement inname adj bi with similar implementations.
this relationship cannot be identified by cfull even if they have similar behavior of looking up something.
in the java naming conventions the first sub token often indicates the key behavior of a method e.g.
get contains .
thus if the key behavior of a given method is similar to those of other methods with similar bodies we can regard that the name is consistent.
thus we relax the condition of consistency.
instead of comparing the full name we take the first sub token of each name in the two name sets to get the intersection as below crelaxed first name adj ni first name adj bi where first is a function that obtains the first sub token set by the same rule of method name tokenization described in section iii a. other subsequent tokens are often highlyalgorithm inconsistency identification and new names suggestion.
input target method name and body mi ni bi input threshold of adjacent vectors k input set of name vectors obtained from a training set vn input set of body vectors obtained from a training set v prime b input indexes of actual names from all vectors v vnorv prime b idxname v n input function embedding a name to a vector nv name input function embedding a body to a vector nv body output pair of the consistency determinant of mi boolean and a set of suggested names c sg n where sg nis ifcisfalse .
1function identify mi vn v prime b compute name and body vectors of mi.
vn nv name ni v prime b nv body bi get adjacent name vectors similar to the name vector vn o fmi.
namev adj gettopadjacent vn vn k get actual names for adjacent name vectors namev adj .
namesni adj namev adj .collect idx name v namev adj get adjacent body vectors similar to the body vector v prime b o fmi.
bodyv adj gettopadjacent v prime b v prime b k get actual names for adjacent body vectors bodyv adj .
namesbi adj bodyv adj .collect idx name v bodyv adj take the first tokens of actual names for adjacent name and body vectors.
ftni adj namesni adj.collect tokenize name n namesni adj .first ftbi adj namesbi adj.collect tokenize name n namesbi adj .first ifftni adj ftbi adjis then mihas an inconsistent name and suggest new names.
newnames ranknames namesbi adj bodyv adj c sg n false newnames else mihas a consistent name.
c sg n true project specific.
therefore those subsequent tokens would be different across projects even if their bodies are highly similar.
algorithm details the precise routine for checking whether the name niof a method is consistent with its body bi or not.
our approach computes the cosine similarity for a given method to search for similar methods.
after retrieving the embedded vectors of the name niand body bi cf.
lines and the approach looks up the top kadjacent vectors in the respective vectors spaces for method names and bodies cf.
lines and .
since threshold kcan affect the performance of identification our evaluation described in section v includes an experiment where kvalues are varied.
after remapping the set of adjacent vectors to sets of the corresponding method names cf.
lines and the sets are processed to keep only first sub tokens cf.
lines and since our approach uses crelaxed as specified in equation to compare the two sets of first tokens ftni adjandftbi adj cf.
line .
if their intersection is the approach suggests names for the given method body bi cf.
section iii c2 for details .
otherwise our approach assumes that niis consistent with bi.
name suggestion our approach suggests new names for a given method by providing a ranked list of the similar names cf.
line with four ranking strategies as below r1 this strategy purely relies on the similarities between method bodies.
the names of similar method bodies namesbi adj are ranked by the similarities to the given method body between v prime band bodyv adj .
r2 this strategy first groups the same names in namesbi adj since there might be duplicates.
it then ranks distinct names authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
based on the size of the associated groups.
ties are broken based on the similarities between method bodies as r1.
r3 similarly to r2 this strategy groups the same names in namesbi adj.
then the strategy computes the average similarity to biof each group and ranks the groups based on the average similarity but the group sizes are not considered.
r4 to avoid having highly ranked groups with a small size as per strategy r3 this strategy eventually re ranks all groups produced in r3 by downgrading all size groups to the lowest position.
iv .
experiment al setup empirical validation of the approach is performed through various experiments.
before describing the results and conclusions we present the research questions and the data collection as well as details on the parameter settings in implementation to facilitate replication.
research questions to evaluate the approach we propose to investigate the following research questions rqs rq1 how effectively does the approach identify inconsistent method names?
rq2 can the approach suggest accurate method names?
rq3 how does the approach compare with the state ofthe art in terms of performance?
rq4 to what extent applying the approach in the wild produces debugging suggestions that are acceptable to developers?
data collection we collect both training and test data from open source projects from four different communities namely apache spring hibernate and google.
we consider java projects with at least commits to ensure that these have been well maintained.
training data is constituted by all methods of these projects after filtering out noisy data with criteria as below main methods constructor methods and example methods1 are ignored since they have the less adverse effect on program maintenance and understanding and can pollute the results of searching for similar methods.
empty methods i.e.
abstract or zero statement methods have no implementation and thus are filtered out.
methods names without alphabetic letters e.g.
some methods are named are removed as they are undescriptive.
as a result methods are collected.
in practice we further limit the training data to methods with reasonable size to avoid the explosion of code tokens which can degrade performance.
the sizes of token sequences of collected method bodies range from to .
according to the sizes distribution shown in figure most methods have less than tokens.
we focus on building the training data with methods containing at most tokens which is set based on the upper whisker value2from the boxplot distribution of method body token sequence sizes in figure .
the sizes beyond the upper whisker value are considered as outliers .
1the package class or method name includes the keyword example sample or template .
2the upper whisker value is determined by .
iqr interquartile ranges where iqr 3rd quartile 1st quartile as defined in .
tokens fig.
.
sizes distribution of collected method body token sequences.
t able i size of da t asets used in the experiments .
classification methods all collected methods methods after filtering for training methods for testing eventually methods are selected to be the training data as indicated in table iv .
note that methods in the training data are not labeled as consistent orinconsistent since the objective of training is to construct a vector space of methods with presumable3consistent names.
test data is the oracle that we must constitute to assess the performance of our proposed approach.
we build it by parsing the commit history of our subjects i.e.
projects .
specifically we consider methods whose names have been changed in a commit without any modification being performed on the body code and the names and body code have become stable after the change i.e.
no more changes up to the current versions .
the first criterion allows to ensure that the change is really about fixing method names and to retrieve their buggy and fixed versions.
overall within commit changes we identified methods satisfying this criterion.
the second criterion increases the confidence that the fixed version of the name is not itself found buggy later on.
with this criterion the number is reduced to methods.
we further observe that some method names are changed due to simple typos cf.
figure .
such changes can constitute noise in the oracle.
given that our approach heavily relies on first sub tokens of method names to hint at inconsistency we conservatively ignore all change cases where this part is not changed.
at this stage the dataset still includes buggy fixed pairs of method names.
the final selection follows the criterion used for collecting training data i.e.
no constructor or example methods etc.
.
the final test data includes distinct methods.
commit 70106770ea61a5fe845653a0b793f4934cc00144 public double inversecummulativeprobability final double p public double inversecumulativeprobability final double p fig.
.
a typo fix for a method name in apache commons math .
.to ensure that there is no data leakage between training and test data that will artificially improve the performance of our approach we eliminate from the training data all methods associated to the test data i.e.
there is no the same instance between methods in training data and methods in test data .
our test data include method names for each of which we have two versions the buggy name and the fixed one.
to build our oracle we need two sets one for the inconsistent class and the other for the consistent class.
we randomly divide our test data into two sets.
in the first set we consider only the buggy versions of the method names and label them as inconsistent.
3the majority of the methods in the world have names that are likely to be consistent with their bodies.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
in the second set we consider only the fixed versions and label them as consistent.
implementation of neural network models the paragraph v ector word2v ec and cnns models are implemented with the open source dl4j library which is widely used across the research and practice in deep learning 800k people and 90k communities according to data on the gitter networking platform .
these neural networks must be tuned with specific parameters.
in this study all parameters are set following the parameters setting proposed by kim and our previous work .
their subjects and models are similar to ours and their yielded models were shown to achieve promising results.
tables ii iii and iv show the parameters used in our experiment for each model.
t able ii parameters setting of paragraph vector .
parameters values parameters values min word frequency size of vector learning rate .
window size t able iii parameters setting of word 2v ec.
parameters values parameters values min word frequency size of vector learning rate 1e window size t able iv parameters setting of cnn s. parameters values parameters values nodes in hidden layers learning rate 1e activation output layer softmax pooling type max pool activation other layers relu optimization algorithm stochastic gradient descent loss function mean absolute error v. e v alua tion a. rq1 effectiveness of inconsistency identification as the first objective of our approach is to identify inconsistent method names we examine whether our approach effectively identifies methods with inconsistent names.
we train the model with the collected training data and apply it to the separated test data whose collection was described in section iv .
given that the performance of identification depends on the threshold value krepresenting the size of the sets of adjacent vectors lines and in algorithm we vary kas and n 10with n .
inconsistent identification is a binary classification since the test data explained in section iv are labeled in two classes ic inconsistent c consistent .
thus there are four possible outcomes ic classified as ic i.e true positive tp ic classified as c i.e.
false negative fn c classified as c i.e.
true negative tn and c classified as ic i.e.
false positive fp .
we compute precision recall f1 measure and accuracy for each class.
the precision and recall for the class ic are defined as tp tp fp and tp tp fn respectively.
those for the class c are defined as tn tn fn and tn tn fp respectively.
the f1 measure of each class is defined as precision recall precision recall while the accuracy is defined as tp tn tp fp tn fn .table v provides the experimental results on the performance.
due to space limitation we show the metrics for variations of kup to instead of .
overall our approach yields an accuracy metric ranging from .
to .
for the presented results and an f1 measure up to .
for the inconsistent class.
in particular the approach achieves the highest performance when k i.e.
the single most adjacent vector is considered .
the general trend indeed is that the performance decreases as kis increased.
t able v ev alua tion resul ts of inconsistency identifica tion .
evaluation metrics k k k k k k inconsistentprecision .
.
.
.
.
.
recall .
.
.
.
.
.
f1 measure .
.
.
.
.
.
consistentprecision .
.
.
.
.
.
recall .
.
.
.
.
.
f1 measure .
.
.
.
.
.
accurac y .
.
.
.
.
.
since kdetermines the number of similar methods retrieving from the vector spaces of names and bodies in the training data higher kvalue increases the probability of non empty intersection i.e.
ftni adj ftbi adj cf.
line in algorithm .
thus the recall of the inconsistent class tends to decrease as kis getting higher.
in contrast the recall of the consistent class increases for higher values of k. overall the approach to identifying inconsistent method names can be tuned to meet the practitioners requirements.
when the criteria are to identify as many inconsistent names as possibles kshould be set to a low value.
b. rq2 accuracy in method names suggestion this experiment aims at evaluating the performance of our approach in suggesting new names for identified inconsistent names.
the suggested names are ranked by a specifiable ranking strategy either r1 r2 r3 or r4 as described in section iii c2 .
prior studies compare the first tokens of suggested names and oracles or token sets without considering token ordering .
to ensure fair and comprehensive assessment we consider three different scenarios to evaluate the performance of our approach.
these scenarios are defined as follows t1 inconsistency avoidance in this scenario we evaluate to what extent the suggested names are different from the input buggy name ni.
the accuracy in this scenario is computed by i icd n1 i ft bodyvadj ic where n1 iis the first token of ni.ft collects the first tokens of method names corresponding to each vector in bodyvadj similar body vectors shown at line in algorithm .
d checks whether the items in the second argument contain the first argument if so returns .
otherwise .
icis the set of inconsistent method names identified by our approach.
t2 first token accuracy in this scenario we evaluate to what extent the suggested names are identical to the name that developers proposed in debugging changes.
we remind the reader that our test data indeed include pairs authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
of buggy fixed names extracted from code changes history cf.
section iv .
the accuracy is computed as i icd rn ni ft bodyvadj ic where rn ni 1is the first token of the actual developer fixed version of the name.
t3 full name accuracy in this scenario we evaluate to what extent the full name of each suggested name is identical to the name that developers proposed in debugging changes.
the accuracy in this scenario is computed as i icd rn ni fn bodyvadj ic where rn ni is the actual fixed version of the name and fn retrieves the full names of methods corresponding to each vector in bodyvadj.
we perform different experiments varying k. for these experiments the approach produces new names for all methods identified as inconsistent i.e.
true positive false positive .
thus the results include the performance of false positives i.e.
names that are already consistent .
we compute the performance based on the number of suggested names varying the threshold value thr .
for the ranking strategy r1 kis set only to or since higher values do not affect the results when thr or note that r1 produces the same number of suggested names with k .
for other ranking strategies kis set to and to have large numbers of suggested names that can be aggregated as per ranking strategy working .
according to the results for t1 listed in table vi our approach is highly likely to suggest names that are different from the identified inconsistent names with ranking strategies even when kis high if thr and if thr .
when matching the first tokens cf.
results for t2 and the full name cf.
results for t3 the ranking strategy r4 slightly outperforms others regardless of the kvalue while its accuracy is .
thr gives a better probability to find consistent names than thr .k yields the best performance for ranking strategies r2 and r3 while ranking strategy r4 performs best when k andthr and k and thr .
t able vi accuracy of suggesting method names with the four ranking stra tegies i.e.
r1 r2 r3 and r4 .
accuracy k thr r1k k k k r2 r3 r4 r2 r3 r4 r2 r3 r4 r2 r3 r4 t1thr .
.
.
.
.
.
.
.
.
.
.
.
.
thr .
.
.
.
.
.
.
.
.
.
.
.
.
t2thr .
.
.
.
.
.
.
.
.
.
.
.
.
thr .
.
.
.
.
.
.
.
.
.
.
.
.
t3thr .
.
.
.
.
.
.
.
.
.
.
.
.
thr .
.
.
.
.
.
.
.
.
.
.
.
.
the best case of each ranking strategy in each row is highlighted as bold .
the results above show that ranking strategies r2 r3 and r4 perform better than r1 but they need more candidates higher kvalues do not increase the accuracy of name suggestion and more number of suggested names i.e.
higher thr values would improve the accuracy but users of our approach will need to look up more names.
note that it is promising that achieving and accuracy for first token t2 when looking up only top and top suggestions respectively.
suggesting exact first tokens of method names is challenging since there are a large number of available words for the first tokens of method names.
findingthe exact full name of a method is even more challenging since full method names are often very project specific .
our approach achieves and accuracy respectively forthr and thr .
c. rq3 comparison against the state of the art techniques we compare our approach with two state of the art approaches in the literature which are based on the ngram model and the convolutional attention network can model .
the latter includes two sub models conv attention which uses only the pre trained vocabulary and copy attention which can copy tokens of input vectors i.e.
tokens in a method body .
these techniques are selected since they are the most recent approaches for method name debugging.
given that the n gram model approach by suzuki et al.
cannot suggest full names or even the first token of methods we compare against them with respect to the performance of inconsistent name identification.
the can model on the other hand does not explicitly identify name inconsistency.
instead the model suggests names for any given method.
thus in this experiment we make the can model and our approach suggest names for all test data buggy method names .
for both techniques we use the same training data described in section iv .
it should be noted that while the tool for the can model has been made available by the authors we had to replicate the n gram models approach in a best effort way following the details available in .
table vii shows the comparison results with the n gram model .
while the performance of the n gram model stays in a range from .
.
for all measures our approach outperforms the model when k and .
with k the improvement is up to percentage points.
in particular our approach achieves a higher f1 measure by percentage points.
t able vii comp arison resul ts of identifying inconsistent method names against the n gram model .
evaluation metricsour approachn gram modelk k k precision .
.
.
.
recall .
.
.
.
f1 measure .
.
.
.
accuracy .
.
.
.
to compare our approach against the can model we propose two evaluations.
the first follows the evaluation strategy proposed by the authors themselves in their paper.
the second evaluation is based on our own strategies already explored for rq2 cf.
section v b .
table viii shows the performance based on the per subtoken basis metric which is the evaluation metric used by the authors originally to present the performance of the can model .
this metric estimates to what extent sub tokens of method names can be correctly suggested without considering their order within the method names.
we compute precision recall and f1 measure of correctly suggesting sub tokens.
when applying the per sub token basis our approach outperforms the can model in all configurations except for the precision of copy attention with thr cf.
section v b .
while the precision of our approach can be higher by up authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
t able viii comp arison of the can model and our approach based on the per sub token criterion .
precision recall f1 meansure t h r t h r t h r t h r t h r t h r conv attention .
.
.
.
.
.
copy attention .
.
.
.
.
.
r1 k thr .
.
.
.
.
.
r2 k .
.
.
.
.
.
r3 k .
.
.
.
.
.
r4 k .
.
.
.
.
.
to percentage points we achieve substantial performance improvement in terms of recall and f1 measure with up to percentage points margin.
table ix presents the comparison results when applying the three evaluation strategies of rq2 described in section v b. regardless of the evaluation strategy our approach outperforms the can models.
notably our approach achieves accuracy for t3 i.e.
full name suggestion while the can model is only successful for at most .
.
note that specific values of accuracy in table ix are different from table vi since in the experiment for rq3 our approach suggests names for all test data.
t able ix comp arison of the can model and our approach based on three ev alua tion scenarios .
accuracyt1 t2 t3 t h r t h r t h r t h r t h r t h r conv attention .
.
.
.
.
.
copy attention .
.
.
.
.
.
r1 k thr .
.
.
.
.
.
r2 k .
.
.
.
.
.
r3 k .
.
.
.
.
.
r4 k .
.
.
.
.
.
while state of the art techniques directly train a classifier for identification or a neural network for suggestion by using a set of training data our approach first transforms method names and bodies into vectors by using neural networks and then searches for similar vectors by computing distances between them.
in that sense our approach is implemented based on unsupervised learning.
overall the results imply that looking up similar methods in vector spaces is more effective both for identification and suggestion than other techniques.
d. rq4 live study to investigate practicability of our approach to debugging inconsistent method names rq4 we conduct a live study on active software projects we submit pull requests of renaming suggestions from our approach and assess acceptance rates.
for this experiment we randomly sample of the training data to be used as test data.
indeed the labeled test data collected for previous experiments represent cases where developers debugged the method names.
the remaining of method names now constitute the training data for this phase.
we apply this version of our approach to the target subjects to identify whether they have inconsistent names using k as per result of previous experiments .
overall methods among the methods in the test set have been identified as inconsistent by our approach.
given that we cannot afford to spam project maintainers with thousands of pull requests werandomly select cases of identified inconsistent methods.
we then collect the ranked list of suggested names for each of the methods we use thr with ranking strategy r4 since these parameters show the best performance for full name suggestion t3 .
from each ranked list of suggested names we select the top name and prepare a patch that we submit as a pull request to the relevant project repository.
t able x resul ts of live study .
agr ee agr ee but not fixeddisagr eeignor ed totalmer ged approved improved cannot won t as listed in table x developers agreed to merge the pull requests for renaming out of the methods.
renaming suggestions have been validated and approved based on developers reply by developers but the pull requests have not been merged as of submission date since some projects systematically apply unit test and complete review tasks of external changes before accepting them into the main branch.
four inconsistent method names have also been fixed after improving our suggested names.
interestingly one developer used our suggestion as a renaming pattern to fix six similar cases other than the ones submitted in our pull requests.
furthermore some developers have welcomed our suggestions on inconsistent method names and showed interest in applying even more suggestions from our approach given that it seems to provide more meaningful names than their current names.
note that out of names agreed by developers are for public methods while remaining eight and are for private and protected methods.
we also report on cases where developers did not apply our suggested name changes.
in one case the developers could not merge the pull request as it would break the program the method is actually an overridden method from another project.
the developer nevertheless agreed that our suggestion was more intuitive.
for other two methods developers agree that the suggested names are appropriate but they would not make the changes as the names are not in line with innerproject naming conventions.
for nine methods however the pull requests are rejected since developers judge the original method names to be more meaningful than the suggested ones.
the remaining cases are simply ignored we did not receive any reply up to the date of submission.
we summarize developers feedback as follows some method names should follow the naming convention of specific projects.
this is a threat to the validity of our study since it is implemented in a cross project context.
some method should be named considering the class names.
e.g.
in a class named xxxbuilder the developers do not want to name a method as build although the method builds a new xxxbuilder object.
vi.
d iscussion naming based on syntactic and semantic information as stated in section i our approach is based on the assumption that similar method implementations might be associated with similar method names.
however there could be several authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
different definitions of similarity.
while our approach relies on the syntactic similarity of method bodies although using ast tokens one can use dynamic information e.g.
execution traces to compare different method implementations as experimented for the detection of semantic i.e.
type code clones .
however obtaining dynamic information is not scalable.
although we can leverage code to code search techniques it might not precise enough for inconsistent name detection.
thus we leverage only static and syntactic information in our approach and rely on deep learning representations that have been shown to be effective capturing semantics even for code .
threats to v alidity a threat to external validity is in the training data since it is impossible to absolutely ensure that all methods in training data have consistent names.
to address this threat we collect training data from the well maintained open source projects with high reputation.
although the number of projects may not be representative of the whole universe it is the largest dataset used in published literature about debugging method names.
our live study further demonstrates that the training set is sufficient to build a good model.
another threat to external validity is the typos and abbreviations in method names that can noise method name embedding and suggestion.
threats to internal validity include the limitation of parsing method names since some method names are named without following camel case or underscore naming convention.
it is challenging to parse this kind of method names.
this threat could be reduced by developing more advanced method name parse tools with natural language processing.
another threat to validity is the size of data set for testing since the test data is no less than for evaluation in recent machine learning and natural language process literature where the training and test data are split from collected data.
in our study test data must be actual fixed method names to evaluate the performance of debugging inconsistent method names but projects used for training do not have such a high number of fixed method names to satisfy the requirement of the balanced training and test data.
vii.
r ela ted work there have been several empirical studies investigating the impact of a naming scheme on program comprehension readability and maintainability.
takang et al.
and lawrie et al.
conducted empirical studies on code identifiers and concluded that inconsistent names can make source code harder to understand and maintain.
caprile and tonella analyzed function identifiers from the lexical syntactical and semantic structure and reported that identifiers can be decomposed into fragments and further classified into several lexical categories.
liblit et al.
examined how human cognition is reflected in naming things of programs.
several approaches have been presented to detect inconsistent identifiers.
deissenboeck and pizka and lawrie et al.
relied on the manual mapping between names and domain concepts to detect inconsistent identifiers in code.binkley et al.
developed a tool with part of speech tagging to identify field identifiers that violate accepted patterns.
the ultimate goal of debugging names is to automatically replace inconsistent names into consistent ones rather than just helping identifier naming.
haiduc et al.
used natural language summarization techniques and the lexical and structural context in code to improve code comprehension .
sridhara et al.
designed an automatic technique for summarizing code with the idioms and structure in a method.
lucia et al.
proposed an ir based approach to improve program comprehension with the textual similarity between the code under development and related artefacts.
h st and stvold used method naming rules and semantic profiles of method implementations to debug method names.
recently allamanis et al.
leveraged deep learning techniques to suggest method names with local contexts which are similar to this paper on embedding method names and bodies.
their work learns method body features from code sub tokens this paper further consider code nodes at abstract syntax tree level since they can capture code semantic information .
this paper performed various evaluations on the actual fixed method names which were not done in with a large sample of projects against the projects in their work.
in addition this paper tried various configurations and strategies and used various indicators for method name suggestions which was not exactly the same as they did.
thus our conclusions are likely to be more solid than those in their work.
furthermore we performed a live study not done in their work and showed the technique has strong potential to be useful by actually fixing inconsistent method names in the wild.
viii.
c onclusion method names are key to readable and maintainable code but it is not an easy task to give an appropriate name to a method.
thus many methods have inconsistent names which can impede the readability and maintainability of programs and even lead to some defects.
to reduce the manual efforts of resolving inconsistent method names we propose a novel approach to debugging inconsistent method names by leveraging similar methods with deep learning techniques.
our experimental results show that the performance of our approach achieves an f1 measure of .
on identifying inconsistent method names improving about percentage points over the state of the art.
on suggesting appropriate first sub tokens and full names for inconsistent method names it achieves and accuracy respectively outperforming the state of the art as well.
we further report that our approach helps developers to fix inconsistent method names in the wild.
the tool and data used in our study are available at al dtf debug method name.