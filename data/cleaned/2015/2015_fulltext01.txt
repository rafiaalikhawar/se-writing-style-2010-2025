grt program analysis guided random testing lei ma cyrille arthoy cheng zhangx hiroyuki sato johannes gmeinerzand rudolf ramlerz university of tokyo japan fmalei schukog satolab.itc.u tokyo.ac.jp yaist itri japan c.artho aist.go.jp xuniversity of waterloo canada c16zhang uwaterloo.ca zsoftware competence center hagenberg austria fjohannes.gmeiner rudolf.ramler g scch.at abstract we propose guided random testing grt which uses static and dynamic analysis to include information on program types data and dependencies in various stages of automated test generation.
static analysis extracts knowledge from the system under test.
test coverage is further improved through state fuzzing and continuous coverage analysis.
we evaluated grt on real world projects and found that grt outperforms major peer techniques in terms of code coverage by and mutation score by .
on the four studied benchmarks of defects4j which contain real faults grt also shows better fault detection capability than peer techniques finding faults .
furthermore in an in depth evaluation on the latest versions of ten popular real world projects grt successfully detects over unknown defects that were confirmed by developers.
keywords automatic test generation random testing static analysis dynamic analysis i. i ntroduction aunit test for an object oriented program consists of a sequence of method calls.
manually crafting test sequences is labor intensive.
random testing automatically generates test sequences to execute different paths in a method under test mut .
to optimize coverage of test cases feedbackdirected random testing frt uses information generated in earlier iterations of test generation to direct latter iterations.
techniques adopting frt such as eclat and randoop incrementally build more and longer test sequences by randomly selecting an mut and reusing previously generated method sequences that return objects as input to execute the mut until a time limit is hit.
while having greatly improved random testing frt still suffers low code coverage in many cases .
with the advancement of other testing techniques e. g. searchbased testing random testing and frt seem to become less competitive .
we show that combined static and dynamic analysis can guide random testing and significantly improve its effectiveness.
in this paper we propose guided random testing grt .
grt extracts both static and dynamic information from the software under test sut and uses it to guide random testing.
grt works in two phases a static analysis over the classes under test cut extracts knowledge such as possible constants during execution method side effects and their dependencies.
based on these analysis results grt creates comprehensive pools of initial constant values and determines the properties of methods that form the basis of method sequence generation.
at run time the static information is intelligently combined with dynamic feedback such as exacttype information and test coverage information to support demand driven object construction and to guide testing to those muts with low coverage.
we implemented the proposed approach of grt as a test generation tool based on the random testing framework of randoop .
grt is fully automatic and does not require input specifications or existing test cases.
we perform a thorough evaluation of grt on a large set of benchmarks containing popular real world applications.
the experiments demonstrate the effectiveness of grt with respect to code coverage mutation score and the ability to detect real known and unknown defects in open source projects.
furthermore grt obtained the highest overall score in a contest of automatic test tools competing with six other well known testing tools .
in summary this paper makes the following contributions we propose grt a fully automatic testing technique using six collaborating components that extract and use static and run time information to guide test generation.
we evaluate grt on real world programs in terms of code coverage and mutation score comparing it with major peer techniques i. e. randoop and evosuite by using multiple time budgets and scenarios.
we investigate the defect detection ability of our proposed technique on real bugs in defects4j .
we perform an in depth investigation of the usefulness of grt in detecting new previously unknown bugs on ten widely used open source projects.
grt successfully found 23unknown and now confirmed bugs.
this paper is organized as follows section ii provides relevant background information and presents an overview of grt.
section iii describes each of grt s components in detail.
section iv shows the evaluation results.
section v compares grt with related work and section vi concludes and discusses future work.
ii.
b ackground and overview a. random testing the general process of software testing consists of three major steps creating test inputs executing tests and checking test outputs .
test automation techniques aim at automating one or more of these steps.
a software under test is often called an sut for short.
similarly class and method under test are abbreviated to cut andmut respectively.
testing an mut with method signaturem tin1v1 tin2v2 t innvn toutrequires creating objects with types of tin1 t innas the inputs of m andtable i. o verview of grt program analysis components and their usage for testing guidance .
component static dynamic description constant mining static extract constants from sut for both global usage seed the main object pool and for local usage as inputs for specific methods.
impurity static dynamicperform static purity analysis for all muts.
at run time fuzz selected input from the object pool based on a gaussian distribution and purity results.
elephant brain dynamic manage method sequences to create inputs in the object pool with the exact types obtained at run time.
detective static dynamic analyze the method input and return type dependency statically and construct missing input data on demand at run time.
orienteering dynamic favor method sequences that require lower cost to execute which accelerates testing for other components.
bloodhound dynamic guide method selection and sequence generation by coverage information at run time.
successful sequencemethod poolsecondary obj.
pool detective main object pool elephant brainrun time phase input selection elephant brain orienteering detectiveresult evaluation elephant brain detective orienteering bloodhoundmethod executionmethod selection bloodhoundsequence generation detectivestatic phase impurity fuzzingimpurity analysisconstant miningmethod dependency detective fig.
.
the workflow overview of guided random testing grt which combines information from static analysis with run time guidance.
the execution of mreturns an object with type tout.
the returned object can be further used as input to test another method that requires an argument of type tout.
a method sequence in testing consists of a sequence of statements mseq fs1 s2 sng where each si2mseq is either an assignment statement vout vinor method invocation statementvout m vin1 v inn that invokes mwith inputsvin1 v innand assigns the output to variable vout.
in the context of testing object oriented programs test inputs are either primitive values or objects with particular states.
to construct useful object states object oriented testing often starts with a set of primitive values and uses them as arguments for specific constructors or methods.
in this way the object states are not directly specified as a set of primitive values but created through the combination of initial values and the execution of method sequences.
objects obtained from a method sequence can be used as input for other methods.
when using such an approach method sequences are conceptually equivalent to input objects.
it is usually impossible to exhaustively enumerate all possible initial values and combinations of method calls.
various methods such as systematic white box testing and searchbased testing are proposed to select or create a relatively small number of initial values and method sequences.
one of the early ideas is random testing which feeds the sut with randomly generated inputs.
it is easy to use straightforward to automate and scalable.
random testing has been found effective in detecting program errors .
however randomness without additional guidance is not optimal in practical settings where testing resources are often limited.
a number of techniques have been proposed to improve the effectiveness of random testing using extra information such as run time feedback to control the test generation process while allowing certain degrees of randomness or to study method sequence patterns from manually written test cases to guide random test generation .b.
guided random testing grt leverages knowledge extracted from the software under test to guide each step of run time test generation.
as shown in fig.
the overall process of grt begins with extracting constant values from classes under test through a lightweight static analysis.
the extracted constant values are used throughout the entire process as seeds to create complex object states.
furthermore a static purity analysis see section iii b categorizes all muts into pure and impure methods.
the result of the purity analysis is used to generate unseen object states efficiently.
the third static analysis focuses on dependencies between parameter types of methods input types and return types of methods output types .
the purpose is to identify the types of objects that are essential for testing muts.
since exact types may be determined only at runtime grt also performs dynamic analysis to capture type dependencies.
grt s run time phase is executed in two or more iterations.
in each iteration run time information is collected to guide subsequent iterations.
the first step of each iteration is selecting a method to be tested from all muts in the method pool .
grt guides the method selection using code coverage information obtained during the test execution of previous iterations.
for the execution of the selected mut grt chooses method inputs from two object pools.
method inputs are maintained in the form of generated method sequences.
the main object pool contains method sequences that have been successfully executed in previous iterations while the secondary object pool contains method sequences generated on demand.
when selecting input objects grt takes the cost of creating each object i. e. the cost of executing the corresponding method sequence into account.
costs are extracted from executions in previous iterations.
after the necessary inputs have been selected grt combines muts with their inputs to generate new method sequences.
these method sequences are executed to test the sut.
the execution completes the current iteration of the run time phase.
grt continues with further iterationsconstant miningelephant brain orienteering bloodhoundimpurity detectivefig.
.
grt component interaction and mutual enhancement.
until certain stop criteria e. g. test coverage are met or the test time budget is exhausted.
grt consists of six collaborative components each of which is briefly described in table i. we use the component names shown in table i for brevity.
the components closely work together as they extract useful static and dynamic information at specific points and then pass it to other components to facilitate their tasks.
the overall effectiveness of grt results not only from the individual components but also from their orchestration.
mutual enhancements between different components are summarized in fig.
.
an arrow from one component to another signifies that the former enhances the latter a double arrow shown in blue shows mutual benefits.
constant mining improves the diversity of the initial object pool by using constants extracted by static analysis.
impurity boosts the effect of constant mining through input fuzzing to create objects with more diverse states.
elephant brain further diversifies the input object types by using dynamic type information to find objects that cannot be generated using static type information alone.
detective constructs necessary objects that cannot be created from the original fixed mut pool on demand.
orienteering accelerates the overall testing speed of grt and makes the effect of the other components more apparent.
finally bloodhound intelligently selects muts that are not well covered.
upon covering more code of an mut more program states are reached which potentially creates more diverse objects to cover even more code.
the next section presents each component in detail.
iii.
p rogram analysis techniques of grt a. constant mining automatic constant extraction primitive types booleans numbers characters and strings are the basis of creating complex objects.
however values chosen purely at random often fail to satisfy branch conditions.
consider the example from class patternoptionbuilder in fig.
branches of method getvalueclass are not covered by randoop as randoop does not start with the required predefined primitive values and is unable to derive the right values that satisfy these branch conditions at run time.
although manual constant selection is helpful in covering such branches e. g. as an option in randoop it requires much human effort.
to obtain relevant input values without incurring too much overhead we perform a lightweight static analysis called constant mining .
our key observation is that many useful constant values are used as instruction operands.
constant mining extracts constants from the classes under test andperforms constant propagation and constant folding to compute input values as candidates for the initial value pool.
practical software usually contains a large number of constants and a constant may only be related to specific branches.
simply selecting the extracted constants as inputs at random for all muts is of little help to cover specific branches.
in addition putting irrelevant constants in the value pool can increase the overhead for test sequence generation and decrease the overall performance.
therefore we use the extracted constants on two levels on a global level among all classes and on a local level a constant is only used for the class containing it .
for global usage we prioritize the extracted constants by weighting them according to their frequency.
the weight is computed based on a modified version of term frequencyinverse document frequency tf idf which is often used to measure the importance of a term in a set of documents .
in the context of constant mining we treat each class of the sut as a document and each extracted constant as a term resulting in the weight tf idfv t d tf t d logjdj jdj jd2d t2dj here tf t d represents the frequency of a constant t occurring in a set of classes d.jdjis the total number of classes in the sut and jd2d t2djis the number of classes that contain the constant t. the formula favors a constant if it is used more frequently and in more classes.
each constant has the chance to be picked although the selection probability is lower for a constant with smaller weight.
some constants are used locally as they may be only relevant to methods of a class that contains the constants.
in this case we register each extracted constant for the classes containing it and select the constants by a predefined probability noted as pconst as inputs for muts of the corresponding class.
to obtain even more relevant values we also use state fuzzing see section iii b .
b. impurity purity based object state fuzzing in order to generate sequences with a broad variety of object states we randomly alter or fuzz the states of existing input objects and pass the fuzzed results to the mut.
we handle primitive numbers based on a gaussian distribution and non primitive objects based on method purity analysis.
primitive value fuzzing primitive inputs are either extracted by constant mining or from method sequence execution results at run time.
to cover a wider range of inputs we use a heuristic given values are already close to satisfying some of the branch conditions.
when a primitive number cis selected as an input we adopt a gaussian distribution to probabilistically fuzz its value and use the altered result as input.
specifically we use the original value of cand a predefined constant as the mean value and standard deviation respectively.
we use a gaussian distribution because it creates new values following our heuristic in that it gives higher probabilities to generate values closer to .
of fuzzed values probabilistically lie in while still generating values distant from .to fuzz a string value we randomly choose a string operation among inserting a character removing a character replacing a character and taking a substring of the given string.
purity analysis based object state fuzzing to test a methodm grt selects the input objects of mfrom the previously generated sequences stored in the object pool.
to obtain inputs with more diverse object states we fuzz nonprimitive objects by identifying and using methods that have side effects that alter the state of the receiver instance method arguments or global static fields.
method purity analysis classifies muts into pure and impure methods.
methods without side effects are pure and methods with side effects are impure .
only impure methods can change the state of an object .
while invoking pure methods is useful to check object states selecting such methods often creates long and redundant sequences where object states stay unchanged slowing down overall growth of coverage.
therefore impure methods are favored over pure methods in order to frequently mutate object states to satisfy more branch conditions.
given an input object oiof typeti we perform static purity analysis to gather all impure methods that can change the state of an object reference of type ti.
among these impure methods we randomly select a method m t1o1 tioi tnon at run time and invoke mon oito fuzz the state of oi each of the impure methods can be selected multiple times to fuzz different input objects .
since mmay also require other input types we first search and reuse such objects from the method sequence of oi and select the remaining missing objects from the object pool.
the fuzzed object ofoiis then passed as input to test the target method.
for example when testing class list impure methods such asadd element andremove element are used to fuzz a list objectlfor more states.
static and dynamic purity analysis techniques exist.
we adopt a static technique to avoid additional overhead at run time.
c. elephant brain dynamic input sequence management subtyping is pervasive in object oriented programs.
an object reference obj of typetcan be assigned to another reference of its super type t0such as t0obj0 obj which makes the usage of the object referenced by objconform to the interface of t0.
such an assignment brings benefits of simplifying the interface by allowing diverse implementations through dynamic binding.
however it poses a challenge to test case generation since the exact run time type of an object may not be the same as its declared type.
this limits many existing automatic testing techniques that adopt a static typebased method sequence management .
in the example shown in fig.
branch coverage in method createval requires both suitable primitive values and a class descriptor returned by getvalueclass method.
however static type management stores the object oreturned bygetvalueclass only as the type object according to its declaration.
an instance of the type object cannot be used as the input for createval that requires the argument of the type class unless it is aware that the dynamic type of ois compatible with or can be used as the type class and1package org.apache.commons.cli public class patternoptionbuilder public static final class string val string.
class public static final class object val object.
class public static final class number value number.
class more similar fields omitted.
public static object getvalueclass char ch switch ch case return patternoptionbuilder.object val case return patternoptionbuilder.string val case return patternoptionbuilder.number value more case branches omitted.
return null more methods omitted.
public class typehandler method omitted.
public static object createval string s class c if patternoptionbuilder.string val c return s else if patternoptionbuilder.object val c return createobject s else if patternoptionbuilder.number value c return createnumber s more else if branches omitted.
else return null more methods omitted.
fig.
.
two classes from apache cli.
branch coverage requires both domain knowledge on constant values and accurate type management.
1package org.apache.commons.compress.utils public final class ioutils private ioutils public static long copy final inputstream input final outputstream output throws ioexception return copy input output public static long skip inputstream input long n long available n while n long skipped input.skip n if skipped break n skipped return available n methods omitted.
fig.
.
methods in compress require inputs outside the fixed method pool.
the type cast is performed on otoclass explicitly before using it as the input of createval .
without the exact type management many branches e. g. line in the method createval cannot be covered although instances that are able to cover these branches do exist.
grt stores all successfully executed method sequences in its object pools see fig.
which can return objects as further inputs to test muts.
to improve the effectiveness of input object selection we manage the objects using their run time types.
this increases the type diversity of generated objects method sequences and thus the coverage of methods that depend on the exact type of their inputs.
when outputting the generated sequences as test cases we compare the static type of each method return value with its dynamic type adding explicit type casts where necessary.
otherwise the generated tests may fail to compile because the static types of method parameters including the receiver do not match the dynamic types of the objects passed to them.
the dynamic type management identifies many diverse data types and never forgets we therefore call it elephant brain .demanddriveninputcreation t input the type tof an object to create.
output a set of generated objects method sequences of type t. dependentmethodset m extractdependentmethods t fg foreach method m2mdo seq getinputandgenseq mainobjpool secondobjpool m .get inputs for method m and generate new method sequences ifseq!
null then execsuccess exec seq .execute method sequence seq ifexecsuccess then secondobjpool add seq end if end if end for candidatemethodseqs getmethodseq secondobjpool t mainobjpool addall candidatemethodseqs return candidatemethodseqs extractdependentmethods t processedset input a dependent type t and processedset are types we have performed method extraction on.
output a set of methods that constructs objects of type t. m fg .set of dependent methods that construct objects of type t deptypes fg .set of dependent types of methods in t ift2processedset tis primitive type then return m end if foreach visible method min class tdo ifisconstructor m getreturntype m tthen m m m deptypes deptypes getinputtypes m end if end for processedset processedset t foreach visible type t02deptypes do m m extractdependentmethods t0 processedset end for return m fig.
.
demand driven object creation algorithm for missed input objects.
d. detective demand driven input construction to test an mut m all input arguments including the receiver object of mmust be prepared.
if any input of m cannot be created mcannot be tested.
therefore the ability of creating objects that muts depend on greatly affects the number of testable muts.
in order to create auxiliary objects diverse api types and methods are often required.
consider class ioutils see fig.
where both methods copy andskip require an object of type inputstream .
the required object cannot be generated by tools like randoop because the creation of the object of type inputstream requires an external library the java core library and cannot be performed by using only the methods in the sut.
as a result no method in class ioutils is ever covered.
it is tempting to use accessible methods from dependent classes such as all library classes of an sut but this increases the search space and wastes effort on methods that are not the target.
we propose a demand driven approach to construct missing input objects in two phases we statically analyze the method type dependency of muts to identify those types that cannot be created by running muts only at run time we use a demand driven approach to construct inputs of types that are not directly available by maintaining a secondary object pool.
our method type dependency analysis first statically computes dependencies of muts by checking their input andreturn types.
then it analyzes each input type of muts and determines if the objects of an input type can be obtained at run time from other muts.
using this analysis grt identifies a set of unavailable types as candidates input for demanddriven input construction.
fig.
shows the demand driven algorithm for creating sequences for missing input types.
when an unavailable input of type tis required during test generation the algorithm calls function extractdependentmethods line to search all available packages for constructors and methods that return the required type lines to .
tis marked as processed when we have extracted the necessary methods from it.
the algorithm recursively searches for inputs needed to execute a method mthat returns the sought after type t lines to .
the recursive search terminates if the current tis a primitive type or if it has already been processed lines to .
for each method mrequired to produce objects of type t grt searches for necessary inputs of min both the main and the secondary object pool.
if all inputs of mare available grt combines the corresponding method sequences with m to generate a new method sequence ending with a call to m lines to .
then grt executes the newly created sequence and stores the resultant object in the secondary object pool lines to .
we use a secondary object pool because adding all objects to the main object pool can add additional overhead and decrease the query performance for the main test generation procedure.
grt selects the method sequences that produce objects of type tfrom the secondary object pool and adds them to the main object pool for future use lines to .
this makes constructing missed input objects and querying efficient without interfering with the main test generation procedure.
like a detective this component works by following the clues i. e. relationships between methods.
e. orienteering cost guided input sequence selection to test a method m grt prepares all its input objects mostly by selecting existing method sequences from the object pools.
as there are often a large number of method sequences that return the objects of the same type randomly selecting type compatible method sequences as input makes the generated test method sequence grow considerably in length and execution cost.
even worse repeatedly executing lengthy sequences may take up too much execution budget leaving many other relevant sequence combinations untested.
for better run time performance it is desirable to use method sequences that have lower execution cost as input.
the idea is inspired by orienteering where a path that takes lower cost is preferable.
therefore we randomly select a sequence as an input based on its execution cost measured by weight seq pk i 1seq exec time i pseq meth size where seqis a sequence for selection kcounts how many times seqhas been selected so far seq exec time iis the execution time during the ith execution of seq and seq meth size is the number of methods in seq excluding statements for the assignment of primitive values.
this weight formula favors sequences with less execution effort while it still includes highcost sequences with diverse states.f .
bloodhound coverage guided method selection the difficulty of covering a branch varies between branches.
some branches can be easily covered with simple inputs while others require complex object states.
an equally balanced selection of muts wastes time on methods that are already well covered.
on the other hand too much emphasis on muts containing uncovered branches may waste time in challenging the difficult branches without much payoff.
to direct testing towards uncovered code we perform a coverage analysis during test generation and favor those muts that are not well covered so far.
although it is desirable to update the coverage information after each execution of an mut this is expensive therefore the coverage information is updated at time interval t. during each interval we prioritize method selection for a method mamong all muts mby using the following function to compute its weight w m k w m k uncovratio m succ m maxsucc m ifk max ln p pk k!
ln size m !
w m ifk in this function krepresents the number of selections of methodmsince the last update of the coverage information uncovratio m is the uncovered branch ratio the number of uncovered branches over all branches of m pis the parameter of a logarithmic series that determines how fast the factor decreases as kincreases succ m is the total number of successful invocations of m maxsucc m is the maximal number of successful invocations of all muts max a b returns the larger value of the two given values size m is the number of muts m and is the parameter to adjust the weight of the first formula.
the overall effect of the weight function is that initially k we favor those methods with low code coverage.
once a method has been tested successfully k we downgrade its weight logarithmically the first part of max function .
after several rounds of selection the weight of each method returns to a uniform distribution again the second part ofmax function .
at each update of the coverage the weights are recalculated and kis reset to .
our method selection strategy is inspired by the multiarmed bandit algorithm .
this algorithm balances exploitation methods that are well tested and exploration methods with low coverage for a higher payoff.
the algorithm is useful because some branches of an mut can be difficult to cover even if the mut is tested over and over again.
a weight function only based on the uncovered ratio of code would waste resources on methods with difficult branch conditions without gaining much benefit.
our approach considers both code coverage and the execution history of each mut for the initial weight but decreases this weight later to avoid investing too much effort in difficult branches.
like a bloodhound this enhancement hungers for coverage while intelligently balancing the deeper search of each mut against the breadth given by the entire problem set.
iv.
e xperiments we implement grt based on the random testing framework of randoop.
constant mining is implemented as antable ii.
b enchmarks size and complexity metrics .
software version ncloc class insn.
bran.
mut.
a4j .0b apache bcel .
apache c. codec .
apache c. collection .
apache c. compress .
apache c. lang .
apache c. math .
apache c. primitive .
apache commons cli .
apache shiro core .
.
asm .
.
classviewer .
.5b dcparseargs easymock .
fixsuite r48 guava .
.
hamcrest core .
jcommander .
java simp.
arg.
parser .
java view control .
javassit .
n. a. javax mail .
.
jaxen .
.
jdom .
joda time .
mango .
nekomud r16 pmd dcd .
.
sat4j core .
.
scch collection .
slf4j api .
.
tiny sql .
total abstract interpreter using asm .
impurity is based on reim reiminfer .
bloodhound is implemented by adapting jacoco to support on line coverage collection during test generation.
based on our experience of developing grt we empirically set its parameters for the experiments further parameter tuning is possible.
for constant mining we set the probability as pconst .
for primitive value fuzzing we select as the standard deviation for gaussian distribution fuzzing this covers boundary conditions and character constant ranges well.
for coverage guidance we set parameters of the weight formula and time interval asp t seconds see section iii f .
using this configuration we evaluate grt by investigating the following questions q1 what code coverage and mutation score are achieved by grt compared to randoop and evosuite?
q2 how does each tool perform given different time budgets?
q3 how much does each component of grt contribute to code coverage?
q4 how many existing defects can be detected by grt in a controlled study?
q5 how many new defects can grt reveal in real world software?
a. subject programs and setting we compare grt with randoop .
.
and with evosuite snapshot oct. .
we select evosuite because it represents the state of the art in search based testing .
2s 10s 30s 60sinstruction coverage grt evosuite randoop 2s 10s 30s 60sbranch coverage 2s 10s 30s 60smutation score fig.
.
instruction coverage branch coverage and mutation score of randoop grt and evosuite over subjects for a time budget of s to s class.
table iii.
r esults average instruction branch coverage and mutation score over 32benchmarks for different time budgets .
time budgetinsn.
cov.
branch cov.
mutation score ran.
grt evo.
ran.
grt evo.
ran.
grt evo.
s .
.
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
.
.
s .
.
.
.
.
.
.
.
.
to answer q1 q3 we run all tools on a collection of popular real world programs.
the overview in table ii shows for each program its name and version its overall size in terms of non comment lines of source code ncloc measured by cloc .
the number of classes the number of instructions and branches in the bytecode measured by jacoco v0.
.
and the number of mutants generated by the mutation analysis tool pit .
our experiments were executed on a computer cluster.
each cluster node ran a gnu linux system ubuntu .
lts with linux kernel .
.
on a core .
ghz amd bit cpu with gb of ram.
we used oracle s java vm jvm version .
.
allocating up to gb for the jvm.
b. code coverage and mutation score q1 and q2 we compare the effectiveness of grt randoop and evosuite in terms of code coverage and mutation score.
we run each tool on each study subject with four test time budgets s class s class s class and s class.
pre and post processing such as loading classes and writing test cases to disk are not counted towards that time budget.
as also discussed in other work we use different time budget configurations to account for different use cases from testing during a coffee break to generating tests over night.
for each configuration time budget tool subject the experiments are repeated times to mitigate the influence of the randomness of the tools.
as each tool sometimes generates tests that do not compile our experimental platform automatically removes uncompilable code at a method level.
all the compilable tests are then evaluated by jacoco for code coverage.
as a conventional procedure for mutation analysis we first filter out generated test cases that fail on the original programs and then send the passing tests to pit to compute the mutation score that measures the ability of killing automatically generated mutants.
constant mining impurityelephant brain detectiveorienteering bloodhoundcombinedbranch coverage fig.
.
branch coverage for each component on our benchmarks s .
with a time budget of s class the tools mostly reach a state in which code coverage and mutation score grow much slower or stop growing with few exceptions when running evosuite .
since the amount of time is allocated for each class instead of the entire sut the results are largely independent of the size of the subject programs.
when extending the time budget large subjects e.g.
guava run for many hours as mutation analysis incurs a high computational cost and sometimes takes longer than test generation itself.
for example on jaxen each tool takes about hours to finish test generation when choosing s class as time budget but it takes more than hours for running the mutation analysis with pit.
in total over all cluster nodes the experiments consumed more than one year of computation time.
figure shows instruction coverage branch coverage and mutation scores achieved by randoop grt and evosuite over the study subjects for each time budget configuration ranging from s class.
table iii summarizes the results in terms of average code coverage and mutation score.
the results show that when running with a short time budget s or s grt has a clear advantage on both higher code coverage by and mutation score by compared with randoop and evosuite.
when the provided test time budget increases the overall code coverage and mutation score of all tools increase too.
evosuite shows a noticeable improvement from s to s reducing the coverage gap between grt and evosuite.
this is because evosuite first performs an initial random search and then uses evolutionary search to improve its results the latter phase requires a certain amount of time to become effective.
with the largest time budget of s class the coverage of evosuite tends to plateau out.
for randoop coverage tends to saturate after about s class.for the largest time budget of s class the average branch coverage from grt is 6higher than with randoop and higher than with evosuite.
for the average instruction coverage the values are and respectively.
on the average mutation score grt also outperforms randoop by and evosuite by indicating that the tests generated by grt have better performance in revealing automatically seeded faults mutants .
the evaluation of the coverage and mutation scores over all subjects shows that the improvement of grt over randoop and evosuite is statistically significant wilcoxon matched pairs signedranks test p 05in all cases .
the effect size is determined using vargha and delaneys a measure a .
to .
.
for assessing the results of the individual subjects we follow the guidelines proposed by arcuri and briand .
the results including code coverage and mutation scores for each benchmark are available on our website .
these results were also confirmed by the search based software testing competition where grt competed with six other tools also including randoop and evosuite.
the tools were compared over a benchmark that was not revealed to participants a priori following a fully automated competition protocol that evaluated the effectiveness and efficiency of the tools.
the benchmark contained classes taken from open source java packages.
grt achieved the highest score of all tools which was calculated based on obtained code coverage mutation score and the time used to prepare generate and execute the test cases .
q3 we run grt with each of its six components enabled individually in comparison to grt with all components enabled on our subjects with s as global time budget.
we observe a coverage improvement for each component and for full grt as time increases.
in general each individual component of grt contributes to the overall effectiveness the impact of each component varies across different subjects.
the combination of all six components is usually stronger than any single component as can be seen from the branch coverage boxplots over all subjects .
fig.
fig.
show three examples of how each component improves code coverage.
constant mining is effective when extracted constants relate to branch conditions .
sometimes detective makes a breakthrough by automatically constructing objects of specific types .
fig.
shows another example where orienteering outperforms the other components of grt.
other plots can be found online .
c. defect detection q4 and q5 we first evaluate the defect detection ability of each tool on the defects4j framework and then use grt to find new unknown defects in popular open source projects.
defects4j the defects4j framework enables testing studies using existing real faults.
it contains real faults reported in five open source projects .
for each fault defects4j uses two versions of the program a faulty version and a correct version.
defects4j first runs a testing tool on the correct version to generate test suites and then runs the generated test suites on the faulty version to see ifthe bug is detected.1the outcome of a technique on a specific fault is pass fail or broken which means the fault is not detected successfully detected or the tests fail on the bugfree version in this case the generated tests cannot be used to determine whether they could detect bugs respectively.
to make experiments efficient defects4j provides the information on fault related classes so that tools can focus on these classes instead of the entire sut when generating test cases.
we run grt randoop and evosuite on defects4j to compare their fault detection ability in a controlled environment i. e. the faults are known .
we use seconds seconds and seconds as the global time budget when comparing grt and randoop in different use cases and allocate seconds seconds and seconds for each class when running evosuite on defects4j.
this should be sufficient for each tool to generate test cases and is reasonable for our available computing resources.
to mitigate the effects of randomness we run each tool times to generate test suites one test suite each time to detect each fault.
we then measure the faults detected by each tool by aggregating the faults found by the generated test suites in each setting.
as shown in table iv each tool detects more bugs when using a larger time budget setting and grt shows the largest improvement when the time budget increases from s to s. grt also detects more faults than randoop and evosuite in all subjects.
in particular grt can detect out of faults in jfreechart out of faults in jodatime and more than of the real faults in both apache math and apache lang using s as global time budget.
this result demonstrates grt s strong fault detection ability in a controlled study using a large number of real faults under different time budget settings.
table iv shows the four cases where we were able to replicate most of the data on randoop and evosuite from a previous study on defects4j .
table iv does not include data for the fifth subject closure compiler because all tools detect unexpectedly few faults.2other minor deviations from the previous study can be attributed to differences in our computing environment including hardware and software and the exact configurations we used mostly default settings.
open source software to evaluate grt s ability to find new previously unknown defects we apply it to the latest versions of popular widely used open source projects.
we use system exceptions crashes and the behaviors stipulated for the base class java.lang.object e. g. the reflexivity property of equals as the test oracle .
as the failed tests generated by grt require manual analysis to determine whether they reveal truebugs or generate false positives we selected projects that are still under active 1defects4j removes all uncompilable tests failed tests and nondeterministic tests before running a test suite for bug detection.
2in private communications with an author of defects4j we confirm that it is quite challenging to generate useful test cases for closure compiler but we could not confirm the root causes this deserves future research.
3in their study on correlations between mutants and real faults the authors of defects4j generated test suites with evosuite per configuration and test suites with randoop per configuration for each subject.
as our aim is to compare different techniques we generate the same number of test suites i. e. with randoop grt and evosuite with similar time budget on each setting to make the comparison as fair as possible.
600branch coverage time constant mining impurity elephant brain detective orienteering bloodhound combinedfig.
.
branch coverage over time tiny sql .
600branch coverage time constant mining impurity elephant brain detective orienteering bloodhound combined fig.
.
branch coverage by time asm .
600branch coverage time constant mining impurity elephant brain detective orienteering bloodhound combined fig.
.
branch coverage by time scch coll.
.
table iv .
d efect detection in defects 4jbenchmarks programisolated defectsran.
global time grt global time evo.
time class s s s s s s s s s jfreechart apa.
math joda time apa.
lang total table v .
d efect detection grt finds 23previously unknown defects .
softwarefailed testsfiltered testsidentified issues issue numbersissues false unkn.
true a. cli a. codec a. collection a. compress a. math a. primitive guava javamail mango tinysql total development the last update being less than a year ago and for which the number of failed test cases is not prohibitively high i. e fewer than failed tests see table v. from the failed tests we first filter out tests that confirm a problem that is either known or not going to be fixed in the code such as bugs caused by using deprecated methods and infinite recursion in container data structures.
we then manually simplify the remaining tests and identify duplicates by comparing the stack traces and the sequences of method calls of different tests.
this results in distinct issues.
we reported these using the projects bug tracking systems combining similar issues into one bug report.
according to the developers feedback grt found 23new previously unknown defects see table v .
d. summary compared with randoop and evosuite grt significantly improves code coverage and mutation scores q1 .
the advantage of grt is observed for all time budget configurations from s class to s class the tools mostly tend to reach aplateau at s class.
compared with randoop and evosuite grt achieves a high coverage sooner q2 .
not all components of grt are equally effective in all cases yet the overall effectiveness of grt results from the synergy between all six components q3 .
grt is able to detect about two thirds of the known faults on the studied subjects of defects4j q4 as well as a number of new previously unknown faults in the latest versions of real world programs q5 .
e. threats to validity the selection of study subjects is always a threat to validity.
we try to counter this by choosing diverse programs from various application domains with their sizes ranging from very small to fairly large.
an external threat to validity is caused by the randomness of the three tools.
we run each tool on the same configuration 10times to diminish this threat and have not observed significant variance caused by randomness.
a related threat is that different tools may require different amounts of time to exhibit their best performance.
as a countermeasure we use four different time budgets to study the effectiveness of each tool in most typical use cases.
we have fully utilized our computing resources to extend the time budget as much as possible up to s class .
another threat is that we have not examined all tool configurations.
in particular evosuite can be configured to satisfy one of three criteria including branch coverage weak mutation testing or strong mutation testing.
our study uses the default configuration which is branch coverage.
however as indicated in a previous study on defects4j the other two configurations would yield similar overall results in terms of detected bugs.
from the authors of defects4j we also obtained the breakdown of their earlier study and confirmed that there are minor differences between results generated by different configurations.
we did not compare grt with test generation tools based on symbolic execution.
this may miss an important aspect of our study.
it is because we could not find an existing symbolic execution based automatic test tool that supports to test java programs and works on the large set of subjects that we used.
however the idea of symbolic execution is orthogonal to the framework of grt and could be integrated as another analysis component in grt in the future.
v. r elated work given the large body of work on automated testing we discuss only work closely related to grt.
for further work we refer readers to representative surveys .
variants of random testing the critical step in automatic test case generation for object oriented programs is to prepare input objects with desirable object states.
an input object can be constructed by either direct construction or method sequence construction returning the desired objects .
direct construction approaches e. g. korat and testera construct objects by assigning fields directly.
they use specifications defined in languages such as alloy and are therefore not fully automated.
most random techniques create required input objects by method sequence construction .
jcrasher creates input objects by using a parameter graph to find method type dependencies similar to our dependency method extraction described in section iii d .
eclat and randoop use feedback from previous tests.
the runtime phase of grt is based on the same basic idea however it performs sophisticated dynamic analysis to generate finergrained feedback.
in addition the static phase of grt extracts useful information of the sut to support the run time phase.
adaptive random testing art improves the defect detection effectiveness of random testing by evenly spreading test input selection across the input domain.
since its introduction by chen et al.
various studies have shown that art requires fewer tests to detect defects than random testing.
however it has also been shown that art has a high computational overhead and has difficulties in testing large suts that require complex inputs .
it would be interesting to include adaptive random testing art tools in the analysis of grt as well as our constant mining technique is related to it.
unfortunately we are not aware of any publicly available art tools that support java and work on the large set of benchmarks we used.
we leave the study on the usefulness of art as a grt component as future work.
random testing guided by domain knowledge several tools take advantage of the information contained in existing test cases method sequences .
mseqgen mines frequently used sequence patterns from code bases.
palus trains a method sequence model from existing test cases which is used for test generation at run time.
ocat adopts object capture and replay techniques where object states are captured from running sample test cases and then used as input for further testing.
similar to these techniques grt also makes use of program analysis to guide random testing but grt does not require extra information sources such as existing test cases and code bases.
systematic testing in contrast to random testing symbolic execution represents input as symbolic values execution is based on abstract semantics and path conditions are computed by leveraging constraint solvers.
tools like java pathfinder and symbolic pathfinder generate test cases in this way.
hybrid approaches of random concrete and symbolic execution called concolic execution are implemented by tools like dart cute and jcute pex and dsc .
an alternative to symbolic execution is bounded exhaustive testing which exhaustively generates method sequences up to a small bound of sequence length.
however real world software usually requires longer test sequences to examine more program states beyond a small bound.
evolutionary testing evolutionary testing leverages evolutionary algorithms to evolve and search for test sequences that optimize their fitness e. g. branch coverage in a limited search budget.
evosuite implements such an approach.
yet it goes beyond traditional techniques as it adopts a hybrid approach to automatically generate and optimize the whole test suites towards satisfying coverage criteria.
it has been shown effective in achieving high coverage on real world software .
grt shares some ideas with evosuite such as extracting constants from sut.
using evosuite fraser and arcuri study the influence of seeding constants extracted from sut on the search based testing techniques.
the constant mining component of grt is based on a similar assumption constants used in the sut are more likely to be useful in testing.
however grt uses different strategies namely frequencybased prioritization and value fuzzing to improve the usefulness of the extracted constants.
we have not investigated how constants extracted from existing test cases can improve grt the third strategy studied by evosuite .
although this can be a promising enhancement to grt it would make grt dependent on external knowledge i. e. existing tests .
vi.
c onclusion and future work in this paper we propose grt a technique that combines static analysis and run time analysis to guide random testing.
grt does not rely on knowledge outside of the sut.
our static analysis extracts domain knowledge from the sut as input for run time test generation.
our dynamic analysis systematically improves test coverage in the generation phase.
we have evaluated grt thoroughly on a large set of real world projects.
our approach exhibits significant improvements on code coverage mutation score and the ability to find defects.
our work shows that random testing has not reached its limits yet.
grt itself can be improved in a number of ways.
it is tempting to incorporate symbolic execution techniques to achieve higher code coverage especially in the face of complicated branches.
simple specialized treatments such as handling less visible code may be surprisingly effective.
we also plan to enhance the test oracle of grt.
currently grt focuses on leveraging program analysis to obtain high code coverage using simple oracles such as software crashes and exceptions.
sometimes the oracles are too weak to detect the faults even though the faulty code is executed.
automated specification mining that extracts information on valid uses of a system would be a promising next step towards stronger test oracles.
considering the sheer number of generated test cases reducing false positives is another important task.
possible solutions include options to avoid deprecated code and recursive data structures.
developing an efficient test simplification technique is also helpful to ease the validation of failed tests.
enabling the application of grt in more scenarios is another direction of our future work.
vii.
a cknowledgments we thank reid holmes mauro pezz e and sai zhang for their insightful comments and gordon fraser jos e campos and ren e just for their help on evosuite and defects4j.
this work was supported by the seut project from the university of tokyo and kaken hi grants and .