are students representatives of professionals in software engineering experiments?
iflaah salman 1department of information processing science university of oulu oulu finland iflaah.salman oulu.fi ayse tosun misirli faculty of computer and informatics istanbul technical university istanbul turkey tosunmisirli itu.edu.tr natalia juristo1 2facultad de inform tica universidad polit cnica de madrid madrid spain natalia fi.upm.es abstract background most of the experiments in software engineering se employ students as subjects.
this raises concerns about the realism of the results acquired through students and adaptability of the results to software industry.
aim we compare students and professionals to understand how well students represent professionals as experimental subjects in se research.
method the comparison was made in the context of two test driven development experiments conducted with students in an academic setting and with professionals in a software organization.
we measured the code quality of several tasks implemented by both subject groups and checked whether students and professionals perform similarly in terms of code quality metrics.
results except for minor differences neither of the subject groups is better than the other.
professionals produce larger yet less complex methods when they use their traditional development approach whereas both subject groups perform similarly when they apply a new approach for the first time.
conclusion given a carefully scoped experiment on a development approach that is new to both students and professionals similar performances are observed.
further investigation is necessary to analyze the effects of subject demographics and level of experience on the results of se experiments.
index terms experimentation empirical study test driven development code quality i. introduction the external validity of experiments conducted with students is very often criticized.
the generalizability issue and concerns about the validity of experiments run with students raise doubts about how transferrable the results in academia are to the software industry .
the comparison of students and professionals as experimental subjects has been under debate for the past few years in response to questions concerning the realism of the experiments .
experimental subjects are not easy to recruit which is why the best option in software engineering se experiments is considered to be using students.
one important reason is the cost.
researchers use the students that they are teaching .
conducting experiments with professionals in a real environment on the other hand is much more costly and the research must be very well funded .
h fer and tichy state that the fact that students participate in experimental studies more often than professionals is a reflection of the difficulties of conducting controlled experiments outside a laboratory environment.
given the constraints faced by researchers students are an important mechanism for assessing technologies conducting pilot studies and comparing alternative experimental designs at least in early phases of the assessment.
despite the valid reasons for using students in experiments there are major concerns about the generalizability of the results.
according to sjoberg et al.
controlled experiments with students are unrealistic in terms of environment tasks and subjects.
experiments with students often involve solving pen and paper tasks which are easy to do in a classroom environment.
this is then a major hurdle when results are to be transferred to industry .
consequently the results of student experiments are difficult to generalize .
this also limits the understanding of industry processes .
when the experiments lack realism the findings are only valid in a specified experimental situation making the results less significant for both applied and theoretical research .
in this paper we present an empirical study whose goal is to compare students and professionals in order to gain an understanding of how representative students might be of professionals in se experiments.
we address the research goal by executing two experiments one with students in an academic setting and another with professionals within a software organization.
the experiments are conducted to observe the effects of test driven development tdd on code quality.
we analyze the quality of the code produced by students and professionals during the execution of these two experiments and answer our research question how much does the code quality of a task implemented by a professional differ from that of a task implemented by a student?
we compare the code quality of the two subject groups using statistical tests and we observe that professionals write higher quality code than students to implement tasks when subjects apply a development approach with which they are already familiar.
however students and professionals perform similarly when they apply a development approach in which they are inexperienced.
ieee acm 37th ieee international conference on software engineering .
ieee ieee acm 37th ieee international conference on software engineering .
ieee ieee acm 37th ieee international conference on software engineering .
ieee icse florence italythe paper is organized as follows.
section ii presents relevant literature reporting similar comparisons between students and professionals as experimental subjects.
section iii explains our study context.
section iv describes the research method used.
section v presents the details of the subjects.
section vi summarizes the analysis.
section vii reports the results.
this is followed by a discussion in section viii.
section ix presents the threats to validity.
lastly section x concludes the study.
ii.
r elated work a. comparing students and professionals the research methodologies that are mostly applied in empirical software engineering research are experiments case studies correlational studies and surveys .
several studies report the rates of participation of students and professionals in se experiments.
h fer and tichy report that of experimental studies in se used students whereas employed professionals and only used both subject groups.
a survey by sjoberg et al.
reports similar rates regarding the recruitment of professionals and students as experimental subjects.
of selected articles on controlled experiments conducted between and used students and only employed professionals .
moreover undergraduates are used more often than graduates.
apart from the external validity issue very few studies have been specifically carried out to compare the performance of students and professionals in their respective environments e.g.
.
t a b l e i p r e s e n t s s t u d i e s t h a t a i m e d t o c o m p a r e t h e performance of students and professionals in the context of an se experiment.
in other words we have only listed studies that were conducted to answer a similar research question to ours.
other empirical studies investigate both students and professionals e.g.
a tool usage was demonstrated from the perspective of both subject groups in .
however these studies did not aim to compare the performance of the two groups and hence they are not included in table i. w e o b s e r v e f r o m t a b l e i t h a t a l l t h e s t u d i e s r e p o r t experiments except one which is conducted using a survey methodology.
in some cases instead of conducting the study with two subject groups the researchers compared the data with the data from an earlier study.
for example two studies and were conducted with students and the researchers later compared their data with the data for professionals reported in their earlier experiments.
i n m o s t o f t h e s t u d i e s l i s t e d i n t a b l e i t h e s t u d e n t s recruited to perform a particular task outnumbered the professional participants.
two studies and reported significant differences between professionals and students e.g.
professionals performed better than students in due to their experience.
however the other studies did not reveal major differences between the two subject groups.
table i. comparison of students and professionals in previous studies study objective aim context type subjects results porter votta comparison of the performance of students and professionals extension of the external credibility of a previous study fault detection techniques for software requirements specification inspections experiment with professionals professionals48 students all hypothesis tests revealed the difference in the performances of students and professionals.
host et al.
difference in the performance of students and professionals factors affecting the project lead time experiment professionals students minor differences in the conception of students and professionals for the factors no significant differences between students and professionals when compared with actual effect of factors runeson feasibility of using students as subjects improvements and performance in personal software process psp experiment with students professionals students similar improvements between the levels of psp by students and professionals berander cases of students usage in research studies an exercise in release planning requirements prioritization software development effort experiment students professionals in their industrial setting were better than students in classroom setting.
students in projects setting were similar to professionals in industry.
svahnberg et al.
students capability of imaging industry professionals requirements engineering requirements selection survey with students students may work well as subjects in this study context.
mcmeekin measuring differences between students and professionals comparison of inspection techniques software inspection techniques for objectoriented development empirical study professionals students professionals performed significantly better than student developers.
inspector s experience has a significant effect on his her ability for detection.
icse florence italy the researchers of both of the studies that found a major difference between the two groups and address inspection techniques e.g.
for requirements specifications and object oriented development.
mcmeekin argues that experience has a significant effect on detecting faults during inspection and the choice of subjects is judged to be a critical issue in such a context.
the data used for analysis in most of the previous studies were collected directly from the participants i.e.
participants gave their subjective opinions about several factors of a study.
in for example subjects were asked to prioritize the factors that affect project lead time and the researchers compared the factor ranking based on student and professional ratings.
similarly in subjects were asked to prioritize requirements for measuring software development effort and these requirements were compared between two subject groups.
our study is similar to the previous studies listed in table i in that it investigates the differences between the two subject groups.
we also employed the data from two experiments and compared the tasks implemented by the two subject groups to address our research goal.
however our study investigates this difference in the context of a technology oriented experiment that observes the effects of the tdd approach on code quality.
we recruited two subject groups students and professionals in their respective environments and executed the same experiment using the same technological setup.
unlike previous comparisons we collected the artifacts source codes produced during the experiment and measured code quality using a set of metrics rather than eliciting personal impressions about the dependent variable from the subjects.
the usage of students as experimental subjects has been a concern in other fields too.
in the field of business studies for example remus conducted an experimental study with the aim of comparing students and industry managers.
the study compared undergraduate students with experienced managers enrolled in an evening mba program.
the context was the usage of decision support systems for the problem of production scheduling.
they found significant differences between the subject groups as students made more costly decisions.
moreover students also used less effective heuristics for making decisions and were found to be more inconsistent than managers.
our study also differs from in terms of the context and data collection.
in experience has an impact on the decision of production scheduling as it does in and .
in our study we do not collect personal opinions from the two subject groups but measure the artifacts that they produce and compare the code quality achieved by each group.
b. experimentation on te st driven development test driven development tdd is an important development practice within agile methodologies .
tdd requires developers to divide a requirement into smaller implementable user stories write the unit tests for each story and later implement the actual code for that user story .
some software organizations have been quick to adopt tdd as a development practice while many others are still evaluating its benefits in terms of costs quality and productivity .
since systematic literature reviews and meta analyses have been published that summarize the types of empirical studies conducted on tdd the level of rigor applied to the research the set of measures used to address dependent variables and the findings.
in this research we focus on the effects of tdd on code quality in software projects implemented by students and professionals.
therefore we identified previous studies investigating the effects of tdd on internal code quality rather than external code quality.
turhan and layman and shull et al.
define internal code quality as the design quality of a software system and they list the set of metrics for quantifying internal code quality as object oriented metrics cyclomatic complexity metrics and halstead metrics.
on the other hand a system s external code quality is usually measured in terms of the number of pre release and post release system defects .
of the recent systematic reviews on tdd literature munir et al.
show that out of experimental studies around investigated the effect of tdd on internal code quality.
four out of these experimental studies used professionals as subjects whereas employed students.
furthermore only two studies out of the reported that tdd was beneficial in terms of internal code quality improvements.
out of these two one study was conducted with professionals and was a controlled experiment .
the other study was a pilot study with students and formed two groups of students where one group acted as a control group .
the other studies reported no difference between tdd and the test last approach in terms of code quality.
table ii lists all metrics used for measuring internal code quality by the studies reviewed in .
table ii shows that researchers mostly choose object oriented metrics such as nested block depth coupling and cohesion whereas some studies consider coverage metrics and mutation score as indicators of internal code quality.
in our study we also use metrics such as cyclomatic complexity a n d number of parameters as code quality attributes.
we did not include other metrics e.g.
coverage mutation score since these are known as quality attributes for test suites rather than for source code .
furthermore we added static code attributes that measure the code quality from the perspective of source lines of code complexity and operator operand counts as they have been found to be good indicators of code defects in software systems .
the full list of metrics used in our study can be found in section iv.
table ii.
metrics used for internal code quality in tdd experiments munir et al .
metrics for internal code quality method statement condition coverage branch coverage nested block depth cyclomatic complexity number of parameters coupling between objects information flow weighted class per method lack of cohesion metrics mutation score indicator icse florence italyiii.
study context this empirical study was conducted using the data from two experiments.
we conducted these experiments in an academic setting with students and in an industrial setting with professionals.
the academic site chosen for the study was universidad polit cnica de madrid upm spain.
graduate students of different nationalities i.e.
of them were from south american countries and the others came from all around the world enrolled for a five day seminar course.
the experiment with professionals was conducted at three different sites of the same software development organization helsinki finland oulu finland and kuala lumpur malaysia .
in total professionals participated in the training experiment.
the company has been operating as a multinational for over years and has around employees at offices around the world.
it has been supplying security services and products for protecting digital devices and consumer and business digital data for over two decades.
the research goal of both experiments was to observe the effects of tdd on quality and productivity.
the treatments independent variables were tdd and the incremental test last development tld approach whereas the dependent variables were quality and productivity.
incremental test last development differs from the traditional waterfall approach in that requirements are divided into smaller user stories in tld each of which is implemented and tested in small increments before moving on to the next user story.
the experiment was replicated twice once in academia and once in industry using the same researchers same trainer training content and language english tasks instrumentation and subject selection strategy.
however the replications differed in terms of the experimental design regarding assignment of tasks to treatments and the experiment protocol.
in table iii we summarize the similarities and differences pertaining to the experimental setup at both sites.
table iii.
comparison of experimental setup experimental design the experimental design followed is a one factor two level within subjects design .
according to the design both industry and academia had the same sequence of treatments as shown in table iv.
in table iv es in dicates experimental session.
during the industry experiment one factor i.e.
development approach was studied using two implementation styles applied twice abb tld tdd on a toy example and tdd on a real life application.
in the academic setting tdd was applied three times abbb and four tasks were randomly assigned to the treatments.
table iv.
sequence of the experiments .
training on unit testing es training on tdd es es es tld mr in industry mr bsk mp and s randomly assigned tasks in academia tdd1 bsk in industry mr bsk mp and s randomly assigned tasks in academia tdd2 mp in industry mr bsk mp and s randomly assigned tasks in academia tdd3 mr bsk mp and s randomly assigned tasks in academia assignment of tasks t h e t a s k s w e r e r a n d o m l y assigned to treatments in the academic experiment.
the student subjects were assigned to a different task each day and they implemented all the four tasks during the experiment.
in industry on the other hand three out of the four tasks all of which were also used in the academic setting were paired with the experimental treatments i.e.
all professionals implemented the same task for the same treatment.
in both groups tasks were implemented individually.
more details about the four tasks are given in section iv.b.
protocol the protocol was slightly different in terms of the allocation of treatments to experimental sessions.
for professionals the first day was allocated to unit testing training and tld implementation on a control task.
the second day was reserved for tdd training a practice session with tdd and tdd implementation on the experimental task tdd1 .
the third and fourth days were reserved for the practice sessions i.e.
professionals were asked to practice tdd at work and no data were collected on these two days.
on the fifth day professionals implemented another task in tdd tdd2 .
hence data were collected from the industry experiment on the first day control task second day tdd1 task and fifth day tdd2 task .
students on the other hand received unit testing training on the first day whereas they implemented the control task using tld on the second day and received tdd training later on the same day.
the third day was reserved for tdd implementation on several programming tasks tdd1 .
on the fourth day students implemented different tasks using tdd again tdd2 .
the final day was reserved for another tdd implementation tdd3 .
therefore data were collected from the academic setting on all but the first day.
data collection using the same technological setup in both settings the data source and test codes for the specified tasks were collected at the end of each day.
in the case of students we copied the workspaces from the workstation on which they worked whereas for professionals we copied industry experiment academic experiment experimental design abb abbb objects tasks mr bsk mp mr bsk mp s trainer same time for the tasks same protocol five day protoco l sampling convenience samplin g instrumentation eclipse ide with junit testing framework java programming language english as the training language icse florence italyvirtual machine vm spaces from the computers on which they worked.
iv.
research method in this paper we report an empirical study in which data from two experiments are compared to analyze how representative students are of professionals in se experiments.
according to the goal question metric template our research goal is as follows analyze students as experimental subjects for the purpose of comparison with professionals with respect to t h e c o d e q u a l i t y o f s o f t w a r e t a s k s implemented by both subject groups from the point of view of the students and professionals in the context of a t d d e x p e r i m e n t c o n d u c t e d i n academia with students and in industry with professionals.
we chose a tdd experiment because we wanted a technology oriented experiment.
this should help us get a more objective evaluation of the two subject groups by eliminating subjective ratings by subjects.
furthermore we have been conducting multiple experiments on tdd as part of an international project with several research and industry partners.
these experiments are good basis for comparing two subject groups.
even though these experiments analyzed the effect of tdd on quality and productivity we do not set out to report the respective results in this paper.
instead we recount whether the results of using students and professionals as experimental subjects are similar.
in order to address the goal of our study we stated two research questions and their associated hypotheses rq .
how much does the code quality of a task1 produced by students using tdd differ from the code quality of a task produced completed by professionals using tdd?
h0 the code quality of a tdd task implemented by a professional and of a tdd task implemented by a student is the same.
rq .
how much does the code quality of a task produced by students using tld differ from the code quality of a task produced by professionals using tld?
h0 the code quality of a tld task implemented by a professional and of a tld task implemented by a student is the same.
we divided the above two hypotheses into further null hypotheses based on the code quality metrics extracted from tdd and tld implementations in the academic and industry in our rqs and hypotheses code quality of a tdd tld task means the code quality measured on a software system that is implemented for a particular tdd tld level.
experiments.
for example the null hypothesis for rq .
regarding the cyclomatic density metric is defined as follows h0cd p tdd s tdd the cyclomatic density of a tdd task implemented by professionals and a tdd task implemented by students is the same.
a. variables the independent and dependent variables of our empirical study are type of subjects and code quality respectively.
the independent variable is manipulated by choosing two subject groups professionals a n d students who worked in two experiments.
on the other hand the dependent variable code quality is associated with software tasks implemented by two types of subjects.
it is measured in terms of static code attributes extracted from the source codes written by the subjects.
b. objects four objects programming tasks are used in the experiments.
three of these programming tasks namely bowling scorekeeper bsk mars rover mr and sudoku s can be categorized as toy tasks.
the toy tasks are all greenfield i.e.
there is no written code or existing constraint for the task.
all of the toy tasks are of similar complexity in terms of the number of user stories estimated time and difficulty level of their implementations.
two of these toy tasks bsk s are well known games and the third one mr is a popular programming exercise within agile communities.
the fourth task musicphone mp can be categorized as a real i.e.
brownfield task that has an existing architecture and code base.
the subjects were supposed to implement the greenfield tasks from scratch i.e.
they were given a project skeleton only whereas they had to add new or modify existing method s for the brownfield task.
as already mentioned tasks were assigned differently to treatments in the academic and industry experiments.
in the industry experiment two of the tasks bsk and mp were associated with tdd treatments and mr was associated with the tld treatment.
on the other hand all tasks in the academic experiment are randomly assigned to treatments.
the professionals worked on the three tasks bsk mr and mp while students worked on four bsk mr mp and s .
therefore the number of tasks used in both experiments differs by one.
the specification documents of these programming tasks can be found in .
c. metrics several metrics have been used to measure internal code quality some of which are presented in table ii in the context of tdd experiments.
in this study we defined code quality in terms of static code attributes metrics and extracted the attributes from the source codes implemented by subject groups.
whether the developed code was complete and correct is out of the scope of our study.
table v shows the descriptions of the full list of static code metrics their calculations are also available in .
d. data collection w e u s e a t o o l c a l l e d p r e s t t o e x t r a c t t h e s t a t i c c o d e attributes.
prest is an open source metric extraction tool which icse florence italysupports metric collection from multiple programming languages including java .
table v. metrics representing code quality in this study we extracted code attributes metrics from the source codes of each subject at the method level.
we chose to collect method level metric data since subjects are supposed to implement new methods for or in some programming tasks e.g.
mp modify existing methods of a partially implemented task.
therefore it is necessary to pinpoint even minor changes that each subject made during implementation.
after the extraction of method level metrics they were transformed into subject level metrics.
we aggregated methodlevel metric values to minimum maximum median mean and standard deviation values at subject level and formed two datasets for two subject groups professionals and students .
the transformation from method to subject level was necessary because all subjects implemented a different number of methods for the assigned tasks and we needed to represent each subject as a single instance five values of metrics in our dataset.
v. s ubjects the experiment at the university academia was performed as part of a seminar course that was offered to the students of an international graduate level degree program.
so the sample population in the academic setting was nearly as diversified as in the industry setting in terms of subject nationality and language.
the experiment involved a total of students who enrolled for the course in academia.
the experiment at the software organization consisted of professionals.
table vi presents the experience levels of the experimental subjects students and professionals.
the table shows that professionals are more experienced than students in programming skills.
eight students have more than five years of experience in programming and six have more than five years in java programming whereas only one has more than five years in unit testing.
on the other hand of the professionals have medium to high experience in programming whereas have two or more years of unit testing experience.
furthermore five professionals claim that they have experience to years in tdd.
we discussed with them and learned that although they received training on tdd they never applied the technique in practice.
so both subject groups are similarly experienced with regard to the technology studied in the experiment tdd .
table vi.
experience levels of experimental subjects vi.
analysis in order to analyze the differences between students and professionals we first performed normality tests on the metrics collected from the source codes that were produced by both subject groups.
the metrics extracted from tld and tdd tasks were tested against the normality assumption using the shapiro wilk test.
the test results reveal that none of the metrics were normally distributed p value .
.
thus we decided to use a non parametric test namely the two sample kolmogorov smirnov test which assesses the hypothesis that two samples students and professionals were drawn from different populations .
unlike the parametric t test or its non parametric alternative the mann whitney u test which checks the differences in population mean the kolmogorovsmirnov ks test evaluates the differences in the general shape of the distributions between two samples differences in skewness etc.
.
we chose the ks test since our aim is to check the general differences between two subject groups rather than differences in medians or variance and we would like to check if there are differences in terms of the minimum maximum median mean and standard deviation of the metric data collected from the two subject groups.
furthermore the ks test does not account for any pairing between the data from the two samples or place any limitation on sample size.
vii.
results this section presents the results of the comparative study.
the results are presented as a comparison of the tld tdd1 and tdd2 tasks for the two subject groups.
we first describe the data reduction.
we then report the descriptive statistics prepared from the data on professionals and students cyclomatic density cd halstead programming time hpt decision density dd maintenance severity ms essential density ed branch count bc cyclomatic complexity cc condition count cnd.c essential complexity ec decision count dc halstead difficulty hd lines of code loc halstead length h.len total operands t.oprnds halstead volume hv total operators t.oprtr halstead level h.ll unique operands count u.oprnd.c halstead programming effort hpe unique operators count u.oprtr.c programmi ng java programmin g unit testin g junit tdd students years years years years total professionals years years years years total icse florence italyseparately.
finally we present the results of the hypothesis testing using the statistical tests explained in the section vi.
a. data reduction our analysis did not include the data on subjects who did not attend all the experimental sessions since we needed the source codes for all control and experimental tasks for each subject.
thus the final number of subjects was out of professionals and out of students.
b. descriptive statistics in this section we present the descriptive statistics of the code quality metrics collected for all three levels tld tdd1 tdd2 implemented by students and professionals.
these are prepared at project level from the method level metrics data extracted using prest.
table vii presents the median values of the metrics for the tasks using two treatments tld tdd1 and tdd2 for students and professionals respectively.
the other metric values for minimum maximum mean and variance are reported in .
table vii shows for tld that there are clear differences between students and professionals for the loc total operands t.oprnds total operators t.oprtr unique operands and operators count metrics and for all the halstead metrics.
applying the tld approach the professionals may appear to produce more lines of code and use more operands and operators in each method although there is no increase in complexity.
comparing the median values for tdd1 metrics we find that the differences between metric values are negligible e.g.
the halstead volume is .
for students whereas it is .
for professionals .
regarding the tld level we observe that professionals produce more loc and use more operators and operands.
the halstead volume hv for professionals increases in tdd1 and tdd2 whereas it is not the case for students.
looking at other descriptive statistics we find that the minimum values of maintenance severity ms for all tasks performed by students are lower than for professionals.
considering the mean values we observe that the code produced by professionals implementing in tld fashion is easier to maintain whereas the source code that they implement in tdd fashion is harder to maintain.
the mean values for the loc metric indicate that students produced smaller methods than professionals in tld task.
in the tdd1 task students produced bigger methods in terms of loc than professionals whereas the loc values for the tdd2 tasks are comparable.
the mean value for cyclomatic complexity cc in tdd1 is higher for students as is the median in table vii .
the standard deviation values for halstead programming effort hpe are higher for tld and tdd1 but lower for tdd2 in student data.
c. hypothesis testing in order to statistically analyze the differences in terms of code quality between students and professionals we ran kolmogorov smirnov ks tests on the minimum maximum mean median and standard deviation values of each metric.
we checked the differences between students and professionals in terms of three tasks.
therefore we ended up making values x tasks x metrics comparisons between data for students and professionals.
furthermore we performed two comparisons.
the first comparison was based on the experiment t r e a t m e n t s t l d tdd1 and tdd2 e.g.
we compared the code quality for all the tasks implemented by both groups in tld fashion.
table vii.
treatment based descriptive statistics median values the second comparison was based on treatment task combinations e.g.
we compared the code quality for the mr task implemented by both groups in tld fashion only.
below we report the results of the ks test for each of these comparisons separately.
table viii and ix present the ks test results for the median v a l u e s o f c o d e m e t r i c s o n l y .
t h e other hypothesis test results for minimum maximum mean and standard deviation values can be found in .
comparison in terms of experiment treatments table viii presents the results of the comparison based on experiment treatment ks test p values between the two subject groups.
the cells highlighted in table viii show the cases in which we found a significant difference between the two subject groups.
student entries are compared with professionals for the tld treatment.
for tdd1 and tdd2 treatments the sample size for students is students and it is compared with professionals using ks tests .
t a b l e v i i i s h o w s t h a t r e g a r d i n g t h e c o m p a r i s o n o f t l d between students and professionals out of metric hypotheses were rejected i.e.
of the code quality metrics for the two subject groups follow different distributions.
of all the metric values reported in we find that out of hypotheses were rejected for the tld comparison.
metric students professionals tld tdd1 tdd2 tld tdd1 tdd2 cd .
dd ed bc .
cnd.c cc dc .
ec loc t.oprnds t.oprt r u.oprnd.c u.oprtrc hd .
.
h.len .
h.ll .
.
.
.
hpe .
.
.
.
.
.
hpt .
.
.
.
.
.
hv .
.
.
.
.
.
ms icse florence italy r e g a r d i n g t h e c o m p a r i s o n o f t d d o u t o f o f code quality metrics were found to be significantly different between the two subject groups whereas the ratio is to for all metric values.
table viii.
comparison in terms of experiment treatments in the last comparison for tdd2 only seven out of of the code metric values were found to be significantly different between the two groups.
contrary to the previous two treatments tld and tdd1 we were unable to find a major difference in the median values of two student and professional samples for tdd2.
we found that taking into account the minimum maximum mean and standard deviation values out of of all the hypotheses were rejected .
i n s u m m a r y t h e t w o s u b j e c t g r o u p s d i f f e r i n t e r m s o f t h e median values of code quality metrics for the tld and tdd1 treatments as more than half of the metrics for the two samples for tld and for tdd1 follow different distributions.
overall and of code quality metric values differ between the two subject groups for the tld and for tdd1 treatments respectively.
comparison in terms of treatment task combination this analysis was conducted by comparing a task implemented with a particular treatment by one group professionals with the same task implemented with the same treatment by the other group students .
therefore we consider three tasks linked by three treatments mr tld bsk tdd1 and mp tdd2.
it is relevant to make this oneto one comparison since the professionals worked on these three tasks on three treatment days only whereas the tasks were randomly assigned to treatment days in the academic setting.
hence the treatment based comparison may indicate the effect of the task on our statistical test results.
we checked the combination of the mr task and tld treatment on student data and ended up with metric data for six students.
we then compared these data with data collected from professionals for the same task and treatment.
subsequently data collected from seven students about the bsk task implemented in the tdd session were compared with data collected from professionals about bsk tdd1.
finally metric data collected from five students about the mp task implemented in the tdd session were compared with data for professionals about mp tdd2.
table ix presents the results of the hypothesis tests for the comparison based on the treatment task combination for the median values of all code quality metrics.
table ix.
comparison in terms of treatment task combinations according to the results in table ix the treatment task comparisons differ substantially from what we found for the comparisons based on treatment in table viii.
for the mrtld combination only the median of the cyclomatic complexity metric denotes significant differences between the two subject groups whereas the other median metric values do not show up any differences between students and professionals.
from the results of mr tld for the other metric values in we conclude that out of hypotheses could not be rejected.
t h e r e s u l t s f o r t h e b s k t d d c a s e s h o w t h a t t h e r e i s n o significant difference for any of the code quality metrics in terms of median values whereas only one out of hypotheses in mean values of the decision density metric tld tdd1 tdd2 cd .
.
.
dd .
.
.
ed bc .
.
.
cnd.c .
.
.
cc .
.
.
dc .
.
.
ec loc .
.
.
t.oprnds .
.
.
t.oprtr .
.
.
u.oprnd.c .
.
.
u.oprtr.c .
.
.
hd .
.
.
h.len .
.
.
h.ll .
.
.
hpe .
.
.
hpt .
.
.
hv .
.
.
ms .
.
.
metric mr tld bsk tdd mp tdd cd .
dd ed bc .
cnd.c .
cc .
dc .
ec loc .
.
.
t.oprnds .
.
.
t.oprtr .
u.oprnd.c .
.
u.oprtr.c .
.
hd .
.
.
h.len .
.
.
h.ll .
.
hpe .
.
.
hpt .
.
.
hv .
.
.
ms .
icse florence italymetric was rejected.
finally for the mp tdd case table ix shows that three out of of the median values for code quality metrics point to significant differences between the two subject groups.
this proportion climbs to out of hypotheses were rejected when all metric values are examined.
viii.
discussion the results of the hypothesis tests for the comparisons by experiment treatment and by treatment task combination are different.
the comparison of the two subject groups in terms of experiment treatments namely tld and tdd2 shows significant differences for more than of code quality metrics.
i n t h e t l d c a s e w e f o u n d t h a t p r o f e s s i o n a l s p r o d u c e d more lines of code per method than students.
even though students produced smaller methods the cyclomatic complexity of their methods was higher than for the methods produced by professionals.
we observed a similar trend for four other metrics total operands count total operators count unique operands count and unique operators count i.e.
professionals produce bigger methods with more operators and operands.
a possible reason for such differences observed in the tld case between students and professionals is experience.
according to the experience levels shown in table vi professionals are more experienced in programming skills than students.
therefore professionals produce higher code quality in terms of modularity smaller methods and complexity less complex in the context of this tdd experiment.
however the effect observed in this context could also be due to tasks since professionals worked on only one task mr while students worked on four different tasks one of them mp is considered as a difficult one .
i n t h e t d d c a s e s t u d e n t s p r o d u c e d m o r e c o m p l e x c o d e in terms of median cyclomatic complexity lines of code and unique operands and operators count metrics.
professionals on the other hand produced fewer loc with fewer unique operators and operands.
therefore their code is less complex.
a smaller cyclomatic complexity value in professionals data also leads to a lower value for halstead metrics.
looking at all hypotheses however we failed to observe major differences between the two subject groups.
both students and professionals were using tdd in practice for the first time i.e.
neither had previous experience in the new approach.
therefore they appear to act similarly during their first implementation.
another reason for this similarity might be the task assignment in the tdd1 case.
the students worked on a randomly assigned task on the tdd1 day whereas all professionals worked on the same bsk task.
the complexity of a task on the other hand might influence subject performance in the tdd1 case.
c o n s i d e r i n g t h e t d d c a s e w e f o u n d t h a t s t u d e n t s a n d professionals performances are very similar in terms of median code quality metrics except for the fact that students produced more loc and used more operators and operands as in the case of tdd1 .
considering all the hypotheses however we observe differences between students and professionals in of the cases.
in the tdd2 case professionals implemented a brownfield task which is more complex to understand and modify than the other tasks.
thus the differences between tasks are likely to be responsible for the differences observed between the performances of the two subject groups.
therefore this result had to be confirmed through the second comparison that we made.
surprisingly most of the tests in the treatment task comparison fail to reveal any differences between the two subject groups.
this result strengthens our hypothesis that the tasks might have an impact on the performance of subjects.
in other words differences among tasks seem to have a bigger impact on code internal quality than the subject type.
i t i s i m p o r t a n t t o n o t e t h a t t h e s a m p l e s i z e s f o r s t u d e n t d a t a in this second comparison were quite small five to seven students compared to the sample size for professional data.
so the results might have little statistical power.
to analyze the power we need to interpret significance level sample size and effect size all together.
we computed an effect size measure c o h e n s d for the metrics that revealed significant differences between the two subject groups in order to interpret the size of the differences.
we found that the effect sizes in all the comparisons are medium e .
g .
.
f o r d d metric in bsk tdd1 to large e.g.
for cc in mr tld and .
.
for hd h. length in mp tdd2 .
in other words there is high probability greater than approximately that the code quality metrics extracted from the source codes are significantly different in the two groups.
based on the effect sizes data sample size and significance levels we could conclude that the few differences observed between subject groups on different tasks have a large statistical power.
taking into consideration our main research question how much does the code quality of a task implemented by a professional differ from that of a task implemented by a student?
we could argue that for tld and tdd applied on certain tasks there are differences between the code quality of a task implemented by a student and a professional in terms of cyclomatic complexity loc operator and operand counts.
however inconsistencies between the comparisons based on experiment treatment and the treatment task show that a the selection of programming tasks for a particular development approach and b the experience of a subject in the development approach would significantly affect the findings.
subjects act differently depending on the size and complexity of the task at hand and produce software systems with different levels of complexity and size.
they also act differently if the technology or development approach at hand has not been used before e.g.
professionals perform better in the tld case but similarly to students in the tdd1 case as it is the first time that they are applying the approach.
ix.
threats to validity we followed the checklist on possible threats to the validity of empirical studies provided by wohlin et al.
and addressed the relevant ones to our study.
icse florence italya.
internal validity selection the study is potentially exposed to this threat since all subjects were volunteers and might not therefore be representative of the general community.
however we had to choose convenience sampling in the case of tdd experiments within industry as it was not possible to form a randomly generated sample in the software organization due to business strategies workloads and group dynamics.
we followed the same selection technique in both environments for the sake of consistency.
diffusion or imitation of treatments in the academic setting all students worked on all four tasks using a random assignment.
as a result of this random assignment students might have known the task descriptions in advance because students could discuss the tasks at the end of each day.
to avoid this threat students were told that it was important not to disclose activities.
furthermore even though students may have discussed the tasks with each other they were asked to follow different practices each day i.e.
slicing coding and testing in tld slicing testing and coding in tdd1 testing coding and refactoring real code in tdd2.
therefore it was unlikely for the development style to be transferred across consecutive days of the experiment.
b. external validity in this study we selected samples from intended populations students in an academic environment and professionals in an industrial setting in order to study the differences.
we took into account heterogeneity in the formation of subject groups and questioned subjects about their skill and experience levels in particular technologies and tools.
table vi confirms that both groups contain subjects with different levels of knowledge and experience in programming unit testing and tools.
until replications of this study are run in other contexts the scope of our findings would be limited to our study conditions tld and tdd as technology and subjects skills shown in table vi.
c. construct validity mono operation bias we addressed this threat by assigning multiple objects the effect on which has been studied separately and by choosing multiple subjects each of which performed individually on the objects.
mono method bias we addressed this threat by quantifying code quality through static code attributes that are well known in quality prediction research and utilized to detect potential faults in software systems e.g.
.
we used an open source metric extraction tool prest to extract these metrics .
no subjective measures that might pose a threat to the reliability of study measures were applied.
d. conclusion validity violated assumptions of statistical tests we addressed and avoided this threat by testing the data for normality and then choosing the relevant test for data analysis.
we chose the kolmogorov smirnov test as it can be used as a non parametric test to observe differences between two samples.
we further discussed our choice in section vi.
repeated hypothesis testing there is a potential risk of accumulated type i error a positive conclusion even though there is no significant difference since we check multiple hypotheses metrics for a single variable code quality .
we could have addressed this risk by applying the bonferroni adjustment to the significance level of ks test and checking the test results with a significance level of .
.
.
based on this significance level we would reject fewer hypotheses in all treatments.
however we still observe that professionals and students perform differently while applying a tld approach out hypotheses are rejected in table viii whereas the difference is not so evident for tdd.
but a reduction of the p value might increase type ii error which is favorable to our conclusion.
thus we prefer to stick with a p value of .
in this study.
x. c onclusion in this research we investigated if students are representative of professionals as experimental subjects in the context of a test driven development experiment.
we observe that professionals and students differ in terms of code quality when they apply an incremental test last approach tld i.e.
a development approach that subjects are used to.
however both subject groups perform similarly when they apply tdd for the first time.
further comparison when both treatment and task are fixed shows that differences in code quality between subject groups might be due to the tasks they are assigned to.
however we need larger samples to generalize our claims.
conclusively our results support previous findings reported in that neither of the subject groups performs better than the other when they apply a new technology during experimentation.
the assessment of subject performance applying an already known technique like tld confirms the results reported in that experience plays a significant role in subject performance.
we acknowledge that the research question addressed here cannot be answered by one experiment with a limited sample case and process.
thus results cannot be generalized to all se experiments.
however studies like ours have the potential to incrementally build knowledge and contribute to the body of evidence.
note that a major differentiating factor affecting the results might be subject s experience levels r a t h e r t h a n t h e experiment setting classroom or industry .
in an academic setting we can find students who already possess industrial experience or in a field experiment we can face novice professionals with regard to a particular technology.
a key condition for subjects characterization might be the experience rather than which type of site they belong to.
a cknowledgment our sincere thanks go to hakan erdogmus burak turhan and oscar dieste for their tireless effort and valuable contribution to the planning and execution processes of the experiments in the context of the fidipro eseil project.
this research is supported in part by tin2011 .
icse florence italyreferences p. runeson using students as experiment subjects an analysis on graduate and freshmen student data proc.
7th int.
conf.
empir.
assess.
eval.
softw.
eng.
pp.
.
d. i. k. sj berg b. anda e. arisholm t. dyba m. jorgensen a. karahasanovic e. f. koren and m. vokac conducting realistic experiments in software engineering proc.
int.
symp.
empir.
softw.
eng.
no.
.
d. i. k. sj berg j. e. hannay o. hansen v. b. kampenes a. karahasanovic n. k. liborg and a.c. rekdal a survey of controlled experiments in software engineering ieee trans.
softw.
eng.
vol.
no.
pp.
.
a. h fer and w. f. tichy status of empirical research in software engineering no.
january pp.
.
m. h st b. regnell and c. wohlin using students as subjects a comparative study of students and professionals in lead time impact assessment empir.
softw.
eng.
vol.
no.
pp.
.
a. porter comparing detection methods for software requirements inspections a replication using professional subjects vol.
pp.
.
m. kamalrudin s. sidek n. yusop j. grundy and j. hosking mereq a tool to capture and validate multi lingual requirements in frontiers in artificial intelligent and applications ios press in press .
p. berander using students as subjects in requirements prioritization proceedings.
int.
symp.
empir.
softw.
eng.
.
isese .
pp.
.
m. svahnberg a. aurum and c. wohlin using students as subjects an empirical evaluation in proceedings of the second acmieee international symposium on empirical software engineering and measurement pp.
.
d. a. mcmeekin b. r. von konsky m. robey and d. j. a. cooper the significance of participant experience when evaluating software inspection techniques aust.
softw.
eng.
conf.
pp.
.
w. remus using students as subjects in experiments on decision support systems proc.
twenty second annu.
hawaii int.
conf.
syst.
sci.
vol.
iii decis.
support knowl.
based syst.
track vol.
.
a. causevic d. sundmark and s. punnekkat factors limiting industrial adoption of test driven development a systematic review fourth ieee int.
conf.
softw.
testing verif.
valid.
pp.
mar.
.
y. rafique and v. b. misic the effects of test driven development on external quality and productivity a meta analysis ieee trans.
softw.
eng.
vol.
no.
pp.
.
versionone 8th annual state of agile survey versionone inc. tech.
rep. .
b. turhan and l. layman how effective is test driven development mak.
softw.
what really works and why we believe it no.
pp.
.
l. madeyski the impact of test first programming on branch coverage and mutation score indicator of unit tests an experiment inf.
softw.
technol.
vol.
no.
pp.
feb. .
h. munir m. moayyed and k. petersen considering rigor and relevance when evaluating test driven development a systematic review inf.
softw.
technol.
vol.
no.
pp.
apr.
.
b. george and l. williams a structured experiment of testdriven development inf.
softw.
technol.
vol.
no.
spec.
iss.
pp.
apr.
.
r. kaufmann and d. janzen implications of test driven development a pilot study in companion of the 18th annual acm sigplan conference on objectoriented programming systems languages and applications pp.
.
t. menzies j. greenwald and a. frank data mining static code attributes to learn defect predictors ieee trans.
soft w. eng.
vol.
no.
pp.
.
w. r. shadish and t. d. cook experimental and quasiexperimental designs for generalized causal inference handb.
ind.
organ.
psychol.
vol.
p. .
c. wohlin p. runeson m. h st m. c. ohlsson b. regnell and a. wessl n experimentation in software engineering an introduction kluwer academic publishers norwell ma usa vol.
.
p. .
n. juristo and a. m. moreno basics of software engineering experimentation springer publishing company vol.
.
p. .
b. a. kitchenham i. c. society s. l. pfleeger l. m. pickard p. w. jones d. c. hoaglin k. el emam and j. rosenberg preliminary guidelines for empirical research in software engineering ieee trans.
soft w. eng.
vol.
no.
pp.
.
task descriptions used in our experiment available e. kocag neli a. tosun a. bener b. turhan and b. a layan prest an intelligent software metrics extraction analysis and defect prediction tool seke pp.
.
f. j. j. massey the kolmogorov smirnov test of goodness of fit j. am.
stat.
assoc.
vol.
pp.
.
hypotheses test results and descriptive statistics of our experiment available z7of5in1rupmuaa?dl .
r. coe it s the effect size stupid what effect size is and why it is important in paper presented at the annual conference of the british educational research association university of exeter england september pp.
.
f. shull g. melnik b. turhan l. layman m. diep and h. erdogmus what do we know about test driven development?
ieee software voice of evidence v o l .
pp.
icse florence italy