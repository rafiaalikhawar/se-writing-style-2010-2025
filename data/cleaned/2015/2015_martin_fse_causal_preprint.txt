causal impact analysis for app releases in google play william martin federica sarro and mark harman university college london london united kingdom w.martin f.sarro mark.harman ucl.ac.uk abstract app developers would like to understand the impact of their own and their competitors software releases.
to address this we introduce causal impact release analysis for app stores and our tool cira that implements this analysis.
we mined popular google play apps over a period of months.
for these apps we identified releases for which there was adequate prior and posterior time series data to facilitate causal impact analysis.
we found that of these releases caused a statistically significant chan ge in user ratings.
we use our approach to reveal important characteristics that distinguish causal significance in goog le play.
to explore the actionability of causal impact analysis we elicited the opinions of app developers companies responded concurred with the causal assessment of which claimed that their company would consider changing its app release strategy as a result of our findings.
ccs concepts software and its engineering software creation and management keywords app store mining and analysis causal impact .
introduction rapid release strategies can offer significant benefits to both developers and end users but high code churn in releases can correlate with decreased ratings .
releases occur for a number of reasons such as updating libraries or stimulating downloads and ratings in addition to more traditional bug fixes feature additions and improvements.
mcilroy et al.
studied update frequencies in google play finding that of their studied apps were updated in a two week period.
nayebi et al.
found that half of surveyed developers had a clear release strategy and experienced developers believed it affects user feedback.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
fse november seattle wa usa c circlecopyrt2016 acm.
isbn .
.
.
.
this paper we introduce an approach to causal impact analysis to help app developers understand the impact of their releases.
we mine and record time series information about apps in order to identify how releases can affect an app s rating and rating frequency.
causal impact analysis is a form of causal inference which we use in this paper to identify sets of app releases that have caused a statistically significant p .
change in subsequent user rating or rating frequency.
causal inference has previously been used primarily in economic forecasting but it has also seen use for software defect prediction .
we introduce our causal impact release analysis tool cira which runs causal impact analysis for app store datasets.
we follow up on the causal impact analysis with more traditionally familiar frequentist inferential statistical ana lysistofurtherinvestigatetheprobabilisticevidenceforpoten tial causes.
we investigate the influence of properties that are under developers control such as price and release text on both the positive and negative statistically significant releases.
we use information retrieval to investigate the top terms and topics that occur in the release text of statisticall y significant releases and also investigate the overall effects of release frequency on user rating behaviour.
furthermore we contacted the developers of releases that were identified by ciraas significant to ask whether causal impact analysis is useful to them.
to facilitate our study we mined data weekly from google play between february and .
our contributions are as follows .we introduce our tool cira for performing causal impact analysis on app store data1.
.we contacted developers of significant releases developers responded of whom agree with cira s assessment of which claimed that their company would consider changing their release strategy.
.we study google play app releases using cira finding .
.paid app releases have a greater chance of affecting subsequent user ratings a chance of for paid apps compared with for free apps .
.
.paid apps with releases that had significant positive effects have higher prices.
.
.free apps with significant releases have a greater chance for their effects to be positive for paid apps compared with for free apps .
.
.releases that positively affected user ratings had more mentions of bug fixes and new features.
.
.releases that affected subsequent user ratings were more descriptive of changes.
1availableat ra2.
definitions in this section we define the app metrics recorded and the terminology used to refer to our data throughout the study.
.
success metrics in order to assess app success the following app level metrics are used2 r rating the average of user ratings made for the app since its first release on the store.
n number of ratings the total number of ratings that the app has received.
nw number of ratings per week the number of ratings that the app has received since the previous snapshot which is taken a week earlier.
.
developer controlled properties the following observable app store properties are under developers control p price the amount a user pays for the app in gbp in order to download it.
this value does not take into account in app purchases and subscription fees thus it is the up front price of the app.
l description length the length in characters of the description after first processing the text as detailed in section .
and further removing whitespace in order to count meaningful characters only.
rt release text the app s description andwhat s new sections from its app store page in each case included only when they have changed from the app s previous release.
an app sdescription andwhat s new sections can change at any point in time but changes typically coincide with releases.
version identifier we determine a release to have occurred if and only if the version identifier changes.
.
data we mine app data from google play between february and february as detailed in section .
.
full set all apps mined in the time period.
some apps drop out of the store for unknown reasons but they are included in the full set for the duration in which they appear in the store.
this full set consists of apps.
control set the set of apps that have no new releases over the studied time period.
we refer to this as the control set as it is the benchmark by which we can measure changes in the releasing apps.
apps that drop out of the store are not included in the control set because consistency is required for a reliable control set.
this control set consists of apps.
target set thesetofappreleasesthatoccurinthestudied time period and occur at least weeks after the previous releases and at least weeks before the next release.
the target releases have some longevity which suggests they are more than hotfixes for recently introduced bugs.
they also have a non trivial window of data on either side so that we can observe any effect the release may have had on the app s success.
this ensures sufficient data availability to perform causal impact analysis.
apps that drop out of the store are included in the target set if they include adequate prior and posterior information as defined above.
this target set consists of apps and releases.
2number of downloads can be used as a metric however most app stores including the one analysed in this work do not make this information publicly available.
.
train local variance .
local only prediction2.
train spike coefficients .
train slab coefficients .
counterfactual predictionfigure cira workflow showing the local and global variance components.
the resultant counterfactual predicition is compared with the observed vector after the release in order to compute the probability that the observed vector would have been predicted.
if this probability is low .
it indicates that there was a significant impact at the time of the release.
.
causal impact release analysis cira causal inference is a method used to determine the causal significance of an event or events by evaluating the postevent changes using observational data.
a traditional approach to causal inference is differences in differences analysis which compares the differences between the prior and posterior vectors i.e.
observations of a group e.g.
mobile apps in a store that receives a given intervention which in our case is a release against a group that does not receive this intervention i.e.
control set .
conversely the causal impact analysis method based on state space models treats each vector separately in each case using the full control set to provide global variance.
multiple apps typically do not receive the same intervention release at the same time which makes the former method difficult to apply in our case.
this motivates our use of causal impact analysis which we apply to app releases using our tool cira.
ciratrains a bayesian structural time series model on the data vector for each target release using a set of unaffected data vectors known as the control set defined in section .
.
this enables the model to make a prediction of the data vector in the posterior time period accounting for local and global variations.
each metric r n nw and each release requires an individual experiment as the method works each time on a single data vector.
fig.
shows the overall ciraworkflow .train the local trend parameters using the deviation of the observed vector in the prior time period.
this is used to sample changes and compute the confidence interval.
.compute the spike for the observed vector from the set of controls and assign coefficients for them in order to help us make an accurate prediction.
.use the rest of the control set as the slab assigning equal coefficients for them in order to account for global variations in the dataset.
.make a set of predictions using the local trend trained earlier sampling for changes and noise.
.combine local only prediction with control changes mutliplied by their coefficients.
this is the counterfactual prediction .weekratingfigure causal impact analysis for the rating of an invoice management application in google play.
after the target release shaded vertical bar the observed vector solid line plotted throughout clearly moves outside the confidence interval shaded region and deviates significantly from the counterfactual prediction solid line inside shaded region .
we then compute the cumulative pointwise difference between the observed vector and our prediction normalised to the length of time at which each deviation occurs and compute the cumulative probability from our local trend parameters.
a low value p .
indicates that the success metric changed significantly see for example fig.
which shows that the rating of an invoice management application deviates significantly from the predicted vector.
by setting our threshold to be .
we have a .
probability of claiming a significant change where one does not exist and therefore expect roughly false positive rate.
we use cira to identify the set of releases see rq3 for which there is evidence of a statistically significant effect o n the metrics we collect.
our subsequent inferential statistical analysis complements this pointing to the set of potential properties which may play a role in this causal significance.
of course the degree to which we can assume causality relies on the strength of our causal assumptions that the control set is unaffected by the release and the relationship of the control to the released app is unchanged.
inanyandallcausalimpactanalyses itisofcourseimpossible to identify external properties that might have played a role in the changes observed.
in the case of app stores our current analysis cannot for example identify advertising campaigns timed to coincide with the new release.
however if such an external factor does have a significant effect then our causal impact analysis may detect it.
we ask developers in rq5 if they are aware of external factors which may have caused the observed changes in order to determine how often this may be the case.
.
research questions this section explains the questions posed in our study and how we approach answering them.
rq1 do app metrics change over time?
before performing a detailed analysis of the changes over time we first set a baseline by establishing whether app successmetricschangeovertimeorbetweendifferentsnapshots.
using the metrics r n nw and l as defined in section we compute their standard deviation over weeks.thesedistributionsaredrawnusingboxplots whichenables us to establish whether metrics change over time and to what extent any such changes occur.
we determine whether app success changes more or less for those apps that have releases by comparing plots for those apps that have no releases the control set with releasing apps the target set .
if the metrics for the target set change over time more than those in the control set this motivates further analysis of releases that could cause the observed changes.
it is expected that releasing apps would show greater deviation in description length describing changes and features.
rq2 do release statistics have a correlation with app success?
we build on this analysis by measuring whether app success is correlated with the number of app releases in the time period studied and whether it is correlated with the time interval between releases.
this will show whether a large or conversely small number of releases might lead to increased success and likewise for release interval.
for both of these experiments only apps in the target dataset are used.
we perform correlation analysis between the number of releases of each app and their value for the metrics r and n at the end of the time period.
we do not use nw because this number is set on a per week basis but instead use the change in r and n from the first snapshot to the last denoted r and n respectively.
additionally we do not use l because this does not represent app success.
rq2.
does the number of releases have a high correlation with app success?
we perform correlation analysis between the number of releases in the studied time period and the metrics r r n and n at the end of the time period.
rq2.
does the median time interval between releases have a correlation with app success?
we performcorrelationanalysisbetweenthemedianintervalbetween releases of each app and the metrics used in rq2.
.
rq3 do releases impact app success?
there are two limitations to correlation analysis.
firstly correlationanalysisseeksanoverallstatisticaleffectinwhi ch a large number of apps and their releases participate.
however it may also be interesting from the developers point of view to identify specificreleases after which an atypically high change occurs in subsequent success accounting for background app store behaviour general correlation analysis is not well suited to this more specific question.
secondly of course as is well known any correlation observed does not necessarily imply the presence of a cause correlation is not causation .
therefore even were we to find strong correlations this would not in itself help us to identify causes.
this motivates our use of causal impact analysis.
we apply causal impact analysis on each target release to see whether it caused a statistically significant change in any of the metrics r n or nw defined in section .
causal impact analysis is described in section .
rq3.
what proportion of releases impact app success?
wecomputetheproportionofappswhosereleases have affected success and the proportion of overall releases.
we group results under the metrics affected r n nw and the intersection of r and nw overall and for positive or negative changes.rq3.
how does the causal control set size affect results?
causal impact analysis uses a control set a set of unaffected data vectors which in our case is the set of apps that have zero releases in the period studied.
as the set is used in two different ways spike and slab as explained in section we test whether the set size makes a difference that could influence our results.
our approach is similar to the experiment using different control sets in the study by brodersen et al.
.
we compute the causal impact analysis results for each metric with the full target dataset using repeatedly halved control sets of size and which are each randomly sampled from the maximal set of non releasing apps.
we compute the agreement between each set of results and the results used to answer rq3.
.
agreement is defined as parenleftbigy y nn total parenrightbig where yy indicates a significant change as detected on both datasets nn indicates no significant change detected on both datasets and total is a count of all results including disagreements .
we also compute the cohen s kappa which takes into account the chance for random agreement and is bounded above by the agreement.
we computea secondsetofresults using680controlvectors which will show the expected difference between consecutive runs with the same control set since there is a random component in the predictive model.
rq4 what characterises impactful releases?
we use the causal impact analysis results from rq3 to analyse significant and non significant releases.
rq4.
what are the most prevalent terms in releases?
we pre process the release text from all releases in a given store as described in section .
then identify the top terms most prevalent for each set of releases using two methods tf.idf and topic modelling .
wetrainbothmethodsonthereleasetextcorpus treating each instance of release text as a document.
for both methods we sum the resultant scores probabilities for terms topics over each set of releases.
we restrict ourselves to only the top three terms to avoid over specialisation.
the topic model is trained using topics.
we chose topics for three reasons i the number of documents in the corpus updated release text from target releases each consisting of tens to hundreds of words which is a large sized corpus ii we do not wish to over generalise nor over fit topics will allow diversity without assigning t he same topic to every document with a release iii topics is a common selection for non trivial datasets serving as the default setting in gibbslda andjgibblda .
intuitively the choice of topics allows for diversity in t he trained topics without unduly elevating the risk of training a topic that is overly specific to an app or release.
rq4.
how often do top terms and topics occur in each set of releases?
wecomputethecountsineachset of releases that contain top terms that emerge from tf.idf and topic modelling as performed in rq4.
.
we apply a bag of words model which ignores the ordering of the words in each document.
this eliminates the need to check for multiple forms of text that discuss the same topic.
rq4.
what are the effects of each of the candidate causes?
we select the sets of statistically significant and non significant releases as well as the sets of significan t releases that increased and decreased rating and compare the distributions of several developer controlled properties.this will help us to establish potential causes that may have led to the releases being significant or positively affecting an important success metric for developers the rating.
we consider properties of the release that lie within the control of the developer p rice rt size the size of release text defined in section .
in words and rt change the change in rt sizeon the week of release.
for each of these properties we use the wilcoxon test and vargha and delaney s a12effect size comparison to identify statistically significant differences between causally significant releases and non significant releases.
we use the untransformed vargha and delaney s comparison because we are only interested in the raw probability value it produces.
in case a statistical difference is found for a given aspect we use box plots to understand if this property may play a role in the changes observed i.e.
it may be a candidate cause .
at this point we will have identified a subset of releases that exhibit statistically significant success effects and we will have identified further differences between sets of significant and non significant releases as well as the differences between positive and negative significant releases.
to further explore the actionability potential of the tool and our findings we ask the following research question.
rq5 is causal impact analysis useful to developers?
because there is no ground truth only different implications of any discovered statistical significance we can only answe r this question semi quantitatively.
to determine whether our results are useful we simply ask the developers of causally significantreleases asdeterminedbyourtool cira whether they agree with the classification.
we email developers via the email addresses contained on their app store pages informingthemofthesignificantreleaseandproposingtosend a report detailing the tool s findings.
we expect a large proportion of these emails may fail to reach a human and can only confirm contact is established if we hear back from a developer.
once contact is established with the app s devel opers we ask them the following questions3 agree with detected significance we ask if the developers of the app agree with cira s assessment that the release was significant.
external cause of changes we ask if the company is aware of an external event which may have caused the detected significant change such as advertising campaigns.
would change strategy we ask if the company would consider changing their release strategy based on findings.
receiving further reports we ask if the company is interested in receiving further reports for their app releases.
learning contributing factors we ask if the company is interested in learning more about the characteristics of significant releases from the results of our study.
.
methodology this section describes the methods used in our study for extracting and analysing app store data.
.
data mining we gathered a set of apps from the google play store from february to february which had appeared in a google play free paid or any category s top list at least once in the year before february .
3questionnaire available on the accompanying web page.we used this list because it includes apps that are likely to have been downloaded and reviewed and so have measurable success yet enables a large dataset that is likely to consist of many app releases.
only of apps release software updates more frequently than once per week and these have already been studied in detail by mcilroy et al.
.
for this reason we set the time interval between between data collection to one week.
some apps dropped out were removed or deleted from the store or had other technical issues that prevented successful data mining on a particular week.
gaps in the data such as this are to be expected and do not prevent causal impact analysis from running.
we extract app metrics from each of the apps including price rating number of ratings description what s newand version.
google play reports rounded app ratings rounded to decimal place thereby creating a potential source of imprecision which we would like to overcome.
we therefore calculate the more precise google play average ratings using the extracted numbers of ratings in each of the five star rating categories from .
.
explanation of the frequentist inferential statistical analysis techniques used this subsection explains the role played in our overall analysis bytraditionalfrequentistinferentialstatisticalt echniques with which software engineers are most likely to be already familiar while section explains causal impact analysis which is comparatively less widely used in the domain of software engineering .
we use correlation analysis in order to understand correlations between the observed app metrics and both the quantity and interval of their releases.
however since it is well known that correlation does not imply causation we further investigate the causal effect of releases using causal impact analysis explained in more detail in section .
we follow up the causal impact analysis with a nonparametric inferential statistical analysis to provide further evidence as to the relative likelihoods that each of the developercontrolled app properties that we observed plays a role in the effects detected by causal impact analysis.
in rq2 and rq4 we use pearson and spearman statistical correlation tests.
pearson introduced the measurement of linear correlation and spearman subsequently extended pearson s work to include rank based correlation .
each correlation metric reports a rho value and a pvalue.
the p value denotes the probability that a rho value is different to zero no correlation .
a rho value of indicates perfect correlation while indicates perfect inverse correlation and indicates no correlation.
values between and indicate the degree of correlation inverse correlation respectively present.
in rq3 and rq4 we also compare distributions using a two tailed unpaired non parametric wilcoxon test that tests the null hypothesis that the result sets are sampled from the same distribution.
we also compare the result sets using vargha and delaney s a12effect size comparison test which results in a value between and indicating the likelihood that one measure will yield a greater value than the other.
in our case we apply these inferential statistical tests to examine differences in properties between sets of releases.we compare the set of releases that have caused a significant change according to the previous causal impact analysis to those which have not and compare the set of releases that positively affected rating to those that negatively affected rating.
the pvalue is the probability that we would observe thedifferenceinmedianvalueswefind oronemoreextreme given that there is in fact no difference in the two distributions from which the releases are drawn.
it is a conditional probability usually used to reject the null hypothesis that there is no difference .
however in our case a prior causal impact analysis has revealed that there is a significant causal difference between the two sets yet it remains unknown what this cause is.
causal impact analysis does not fully overcome the problem that correlation is not causation but it does provide evidence for causal significance in our case causal effect of a release on subsequent success .
therefore each pvalue from our subsequent wilcoxon tests can be interpreted as an indication of a potential cause for the difference observed which if low p .
warrants further investigation of the associated property.
sincewearecomputingmultiple pvalues thereadermight expect some kind of correction such as a bonferroni or benjamini hochberg correction for multiple statistical testing at the traditionally popular .
probability level corresponding to the confidence interval .
however since we are notusingpvalues to test for significance should apvalue lie above this corrected threshold then this does notnecessarily indicate that the property does not contribute to the observed causal significance.
quite the contrary since we have already observed that there exists a causal significance then any property that exhibits a low p value p .
remains that with a chance of having some influence on the causal effect from amongst those properties assessed using inferential statistics.
.
information retrieval techniques to answer rq4 we perform information retrieval analysis onreleasetext usingbothtf.idfandtopicmodelling.
we use two different techniques to increase the confidence with which we can identify the top terms that occur in the release text of app releases that exhibit causal significance.
filtering text is cast to lower case and filtered for punctuation and stopwords using the english language stopwords from the python nltk data package4.
lemmatisation each word is processed by the python nltkwordnetlemmatizer in order to be transformed into its lemmaform tohomogenisesingular plural gerundendings and other non germane grammatical details.
tf.idf tf.idf finds top terms in release text each term in each document is given a score of tf term frequency multipliedbytheidf inversedocumentfrequency .
the idf is equal to the log of the size of the corpus divided by the number of documents in which the word occurs.
topic modelling we use topic modelling to find the top topics in release text.
topic modelling is a generative techniquethattrainsaprobabilisticgraphicalmodelonaset of unstructured textual documents under the assumption that they are generated from a set of latent topics.
4nltk.corpus.stopwords.words english r n nw l figure rq1 standard deviation box plots for r ating n umber of ratings nw number of ratings per week and l ength of description.
each plot shows the standard deviations for apps in the con trol set tar get set and full set.
.
results this section answers the questions posed in section .
rq1 do app metrics change over time?
we can see from the box plots in fig.
that the metrics r ating n umber of ratings nw number of ratings per week and l ength of description do change over time because their median standard deviation is always positive.
the deviation in number of ratings and number of ratings per week is high but very low for rating.
this is because for apps with many ratings even a small change corresponds to thousands of users rating higher or lower than the established mean.
we can see that the deviation is approximately even between the control and target datasets for rating deviation this is a surprising result and indicates that either a ratings are unstable even for stable established non releasing apps or b app releases have little effect over all globally detectable ratings.
thefindingofalowdeviationinthetarget set means that a causally significant release that affects rating has a good chance to stand out from the crowd .
the box plots show that deviations in number of ratings and rating frequency length are significantly higher for the target releasing dataset than for all apps in the dataset and for the control set.
this is expected and shows some utility to app releases to increase user activity in downloading and rating the apps and perhaps to increase the user base.
the deviation in description length is higher for the target dataset as expected suggesting that descriptions are updated to provide information about releases.
this finding supports the intuition that descriptions may be used as a channel of communication between developers and users.
answer to rq1 do app metrics change over time?
the metrics n umber of ratings and nw number of ratings per week show a high standard deviation for apps between february and february but r ating shows only a small deviation.
the deviation in user rating frequency is higher for the target releasing dataset suggesting that app releases lead to user activity.
rq2 do release statistics have a correlation with app success?
having established a baseline we now measure the correlations between the number and interval of releases and success metrics.
rq2.
does the number of releases have a high correlation with app success?
table presents the results of correlation analysis between release quantity and median interval and app metrics for the target dataset.table rq2 significant p .
correlations between each of number of releases and median release interval and success metrics r ating and n umber of ratings at the end of the time period studied as well as the change in these metrics from first to last week.
release statistic method r r n n quantityspearman .
.
.
.
pearson .
.
.
median intervalspearman .
.
.
.
pearson .
.
.
.
we only report correlation coefficients the rho values that are deemed significant p .
i.e.
where there is sufficient evidence that rho negationslash .
the results in table indicate only weak significant correlations between the success metrics and their change over the time period studied.
the strongest correlation for n where rho .
is still too weak to definitely suggest a strong relationship.
we therefore conclude that there is no strong overall correlation between release frequency and the app metrics we collect but there is evidence for a weak correlation between number of releases and number of reviews accrued over a year.
rq2.
does the median time interval between releases have a correlation with app success?
table presents the results of correlation analysis between release interval and app metrics.
as these results revealed there is little evidence for any strong correlation between the medianinter releasetimeperiodandtheappmetricswecollect.
our findings corroborate and extend the recent findings by mcilroyetal.
whoreportedtheratingwasunaffectedby release frequency in the google app store.
this is interesting because there is evidence that app developers release more frequently when an app is performing poorly our results indicate that this perhaps rather desperate behaviour is unproductive.
answer to rq2 do release statistics have a correlation with app success?
neither higher numbers of releases nor shorter release intervals correlate strongly with changes in success.
rq3 do releases impact app success?
the results from rq1 and rq2 have established that app rating metrics do vary over releases but that the number of releases and time intervals between releases are not important factors in determining these changes.
this makes causal impact analysis potentially attractive to developers.
with it developers can seek to identify the set of specific releases that had a higher effect on success using evidence for significant changes in post release success compared with the set of non releasing apps.
this is the analysis to which we now turn in rq3.
rq3.
what proportion of releases impact app success?table presents overall summary statistics for the results of causal impact analysis.
the apps row indicates the number of apps summarised as part of the target dataset and the target releases row indicates the number of releases these apps underwent in the studied time period that were outside of a week window of other releases.table rq3.
causal impact analysis results indicating the number of casually significant releases over the target dataset and the number of causally significant releases in each sub group as determined by a release s effects on subsequent app success .
details of target releases percentages reported over targ et releases type total of target apps non significant .
target releases significant .
control apps details of significant releases percentages reported over significant releases metric total of significant ve ve r .
.
.
n .
.
nw .
.
.
r nw .
.
.
table rq3.
results reveal minimal effects arise from different control set size choices using the full set of target releases.
control sets were sampled randomly from the maximal control set of apps.
the result calibrates by assessing maximal agreement between repeated runs on maximal sets capturing variance due to the inherent underlying stochastic nature of causal analysis.
control set metric 680agreementr .
.
.
.
.
.
.
.
.
n .
.
.
.
.
.
.
.
.
nw .
.
.
.
.
.
.
.
.
all .
.
.
.
.
.
.
.
.94cohen s kappar .
.
.
.
.
.
.
.
.
n .
.
.
.
.
.
.
.
.
nw .
.
.
.
.
.
.
.
.
all .
.
.
.
.
.
.
.
.
those releases that occur near the beginning or end of the time period will therefore not have sufficient information available and so causal impact analysis cannot be applied.
hence we select a subset of releases that must also belong in the range of weeks out of a possible .
of these target releases some are significant and some are not according to causal impact analysis.
as table reveals we found that .
of releases were significant.
the remainder of table reports the observed change in success metrics for significant releases thereby identifying candidate causes of these effects.
for each success metric change we report the total number of releases that exhibited a significant change in the associated metric and the percentage of all app releases that exhibited the change.
wefurthersubdividethistotalintothosethatareconsidered positive and negative from a developer s perspective .
from the .
of significant app releases in google play approximately a third .
affected more than one success metric.
the releases of most potential interest to developers are likely to be those that affect both rating and rating frequency duetotheirpotentialforincreasinguserbaseand revenue .
of these there were significant releases.
these results support the hypothesis that there is a subset of releases that cause significant changes to their app s success in the store.
rq3.
how does the causal control set size affect results?
table reports the effect of choosing different control set sizes from among those apps that did not undergo any release during the time period studied.the results in table reveal strong agreement for each control set size indicating that the model is stable in our case using non releasing apps .
we can see from our two runs using the full apps in the control set that there is between .
and .
agreement due to the stochastic element of causal impact analysis.
our results show that restricting our choice of control set does not have advantages and so we opt to use the full apps for our control set for subsequent experiments.
answer to rq3 do releases impact app success?there is strong evidence p .
that of the target releases in the google play store significantly affected a success metric and approximately significantly affected more than one success metric.
rq4 what characterises impactful releases?
the finding from rq3 tells us that there are significant releases but it cannot identify the causes merely that there has been a significant change in post release success.
we now turn to identify any candidate causes which may have played a significant role in the changes we have observed and analyse their effects.
rq4.
what are the most prevalent terms in releases?
table reports the results of information retrieval using tf.idf and the topic modelling on the release text of significant releases.
in this table we consider only those apps for which release text is available.
the results show only the top tf.idf terms and terms from the top topic respectively thereby indicating the most prevalent terms and topic.
of the target releases those with sufficient evidence for causal impact analysis have release text available.
the remainder of the table consists of four overall columns giving the metric for which a release is found to be significant leftmost column followed by the most prevalent terms for tf.idf and topics for topic modelling followed by a subdivision of these prevalent terms into those whose effects are positive and negative from a developer s perspective .
table reveals that terms and topics themed around bug fixes and features occur frequently overall in the text of significant releases.
the text of releases that positively or neg atively affected rating is slightly more specific to certain sets of apps i.e.
card map feature wallpaper live christmas yet still appear to refer to features.
these observations motivate our subsequent analysis in rq4.
for the terms bug and fix and new and feature .
rq4.
how often do top terms and topics occur in each set of releases?
table shows the number of occurrences within significant and non significant releases of the terms bug and fix and new and feature which emerged as the top terms from our information retrieval in rq4.
.
we can see from the results in table that the terms bug and fix are far more common than new and feature in both significant and non significant releases.
this is unsurprising because bug fixing occupies a large proportion of development effort .
however it is noteworthy that in both cases the terms are more common in the releases that positively affected metrics as opposed to those that negatively affected metrics.table rq4.
top release text terms tf.idf terms on the le ft and topic modelling topics on the right.
terms which occur in all groups are removed for comparison n ew app game play free type overall apps non significant feature fix time mode challenge friend target releases significant feature fix device hero monster battle release text metric all ve ve r feature word fix hero monster battle feature word time wallpaper live christmas bible fix support account mobile card n feature word time wallpaper live christmas fix feature word wallpaper live christmas nw map feature word tip local city card map feature hero monster battle map feature time tip local city r nw feature video map wallpaper live christmas account card feature account mobile card video time feature hero monster battle table rq4.
occurrences of the terms bug and fix and n ew and feature in release text.
type overall apps non significant .
target releases significant .
release text metric total ve ve r .
.
.
n .
.
nw .
.
.
r nw .
.
.
occurrences of the terms bug and fix in release text.type overall apps non significant .
target releases significant .
release text metric total ve ve r .
.
.
n .
.
nw .
.
.
r nw .
.
.
occurrences of the terms new and feature in release text.
table rq4.
probabilistic analysis of candidate contributions p rice rt size release text size and rtchange release text changes.
significant non significant ve r ve r metric wilcoxon a12wilcoxon a12 p .
.
.
.
rtsize .
.
.
.
rtchange .
.
.
.
rq4.
what are the effects of each of the candidate causes?
based on the low p values reported in table we further analyse price and the size of the release text as candidate causes.
fig.
presents box plots showing the distribution of paid app price and release text size comparing significant releases against non significant releases and releases that positively affect rating against those that negatively affect rating.
since of the apps are free we plot paid apps in the price boxplot and compute the proportion of free and paid apps in each set as well as the mean and median prices.
the results for price are interesting somewhat surprising and nuanced.
we found that significant releases positive and negative have higher prices than non significant releases.
the mean prices of all significant releases including free and all non significant releases are .
and .
respectively.
paid releases were more likely to be significant .
of paid app releases were significant compared with .
of free app releases.
a somewhat surprising finding is that higher priced paid app releases are more likely to have a positive effect a mean of .
and median of .
compared with .
and .
for those with negative effects respectively.
however a greater proportion of significant paid app releases negatively affected rating .
compared with .
for free apps.
overall a larger proportion of significant releases are paid than non significant releases .
compared with .
respectively .
as a result the mean price of significant releases is higher by .
.
of course a price difference of .
in google play appears relatively trivial.
however the difference in revenue that accrues can be substantial.
figure rq4.
box plots of price and release text size comparing s ignificant and ns nonsignificant releases and releases that increased rating ver and decreased rating ver .
we conservatively calculate that for the app officesuite pro pdf which had a release on 23rd december for which we observed a significant effect on the subsequent rating over its minimum installs the price difference of .
extrapolates to minimum in accrued revenue.
our revenue calculation is particularly conservative since at the time of this release officesuite pro was priced at .
.
this finding suggests that developers need not fear a race to the bottom with competitors over pricing.
unsurprisingly these results confirm the intuition that users can be expected to be price sensitive.
we can also see from table that there are significant differences between the distributions of price and release text size comparing significant with non significant releases and releases that increase rating to those that decreased rating.
the median number of changed stopword filtered words in significant release text is compared to only for nonsignificant release text.
this provides evidence that users may be influenced by release text but the effect size is relatively small.
nevertheless developers might wish to spend time carefully choosing their release text to maximise positive influences on the users.
answer to rq4 what characterises impactful releases?
there is evidence that releases that significantly affect subsequent app success have higher prices and more descriptive release text.
releases that positivelyaffectratingaremorecommoninfreeapps andin paidappswithhighprices.
wealsonotethattherelease text of releases that positively affect success make more prevalent mentions of bug fixes and new features.table rq5 developer responses to questions after contact was established and a cira report was shared with them.
not all developers responded to all questions and so the bottom total row indicates the number of developers who expressed an opinion on a given question.
receiving further reportsagree with detected significanceexternal cause of changeslearning contributing factorswould change strategy yes no total rq5 is causal impact analysis useful to developers?we sent emails to the email addresses available in the google play app store pages of companies with significant releases as detected by our tool cira.
of course this is something of a cold call and we suspect that many emails never even reached a human.
we can report that immediately failed due to invalid email addresses used on app store pages and were immediately assigned to a support ticket.
those that received a response are the only cases in which we can verify that contact was established with developers of which there were .
of these developers distributed across of app store categories replied to express their opinion in response to follow up questions.
all respondents apps were established in the store the smallest had reviews and the largest had at the time of our experiments.
we summarise developers opinions in table in each case indicating the instances where developers expressed an opinion the most answered question concerned agreement with cira s assessment of which there were respondents.
we can observe that out of development teams who expressed an opinion agreed with cira s assessment that their app release was causally significant.
for example the developers of a dictionary application said in our case it wasobviousforourselvesbecauseitwasatotallynewrelease and with lots of new features resonating with our earlier finding that new features increase the chance for significant releases see rq4.
.
only developers disagreed with cira s assessment.
however some of them were still able to identify a cause for the significant change detected by cira but did not think that the release itself could be the cause .
for example the developers of a security caller app said we did not release anything.
we just upload builds for our beta version which uses few users so your tools are wrong.
in this case although the developers did not consider a beta version as a full release the app s version identifier changed on its app store page resulting in a release by our definition in section .
.
this detected release combined with increased user activity resulted in the causal significance detected by cira.
about one half of those who expressed an opinion out of indicated that they knew of an external reason for the changes and several teams elaborated further.
we might expect the developers who know of an external cause to be a subset of those who believe there to be a significant causal effect.
however of developers who claimed to know of external causes also disagreed with cirathat the corresponding release was significant.
one set of developers described the release as a minor bugfix only release.therefore i doubt that this release was the main reason for this change.
.
however as we know from rq4 results mentions of bug fixes in release text are more prevalent in significant releases that positively affect rating.
indeed this particular release did mention bug fixes in its release text and significantly and positively affected rating .
over half of the developers who expressed an opinion of were interested in receiving further reports.
the majority of developers of indicated that they would like to learn more about the characteristics of significant releases and of indicated they would consider changing their release strategy based on our findings.
only of the developers who would change their strategy knew of an external cause of the changes thus suggesting that this consideration will be based on our general findings.
these results provide initial evidence that causal impact analysis can be useful to app developers.
answer to rq5 is causal impact analysis useful to developers?
three quarters of developers who expressed an opinion of agreed with cira.
most developers of were keen to learn more about the characteristics of significant releases and of said that they would consider changing their release strategy based on cira s findings.
this provides initial evidence that causal impact analysis is useful to developers.
.
threats to v alidity in this section we discuss threats to the construct conclusion and external validity of our findings.
construct validity the gap between data analysis and causality is large forcing any user to make very strong assumptions if they hope to effectively imply causality.
causa l analysis can hope to reduce this gap but no such analysis could hope to fully close it there will always be unknown factors which may nevertheless have affected the data.
in the case of app stores there will always be potential external influences for which no data is available to capture them.
we apply causal impact analysis in our experiments but there are other forms of causal analysis such as differencesin differences.
nonetheless we believe this method is the most suitable due to its independent consideration of each app release and the ability to use all non releasing apps as the control set in every experiment thus reducing the risk of control set choice influencing results.
we have shown how causal impact analysis is a useful way to identify releases that cause significant changes in subsequent success.
in order to assess the actionability of the technique we ask rq5.
conclusion validity our conclusion validity could be affected by the qualitative human assessment of top terms and topics for sets of releases in rq4.
.
we mitigate against this threat by asking a quantitative question of the number of times bug and fix and new and feature occur in each set of releases in rq4.
.
external validity naturally care is required when extending our findings to other app samples and app stores.
nevertheless the methods we used to analyse causal effects can be applied to other app stores.
we believe that conclusions about the characteristics of significant releases ove r this sample will yield interesting and actionable findings for developers and we contact app developers for their opinions on the usefulness of our technique in rq5.we can only report the views of those developers with whom contact could be established see section and care is required when interpreting their responses.
since we were able to effectively reach developers we cannot claim that our sample is representative of the entire population.
however this is still a fairly large sample with respect to those used in other studies involving app developers e.g.
.
.
related work for an overview of app store analysis literature we point the reader to our survey on app store analysis for software engineering .
in this section we discuss previous work on software releases and causal analysis in software engineering.
there has been a large amount of recent work linking software quality with user perceived quality.
ruiz et al.
studied how ad library usage affected user ratings.
bavota et al.
investigated how the changes and faults present in apis used affected apps ratings.
panichella et al.
classified user reviews for software maintenance.
palomba et al.
studied how developers responding to user feedback can increase the rating.
moran et al.
presented an android bug reporting tool that can increase the engagement between users and software quality.
it therefore stands to reason that software releases affect quality and consequently may affect user rating behaviour.
in henze and boll analysed release times and user activity in the apple app store and found that sunday evening is the best time for deploying games.
in datta and kajanan found that apps receive more reviews after deploying updates on thursday or late in the week.
in gui et al.
found that of releases from frequentlyreleasing apps contained ad related changes .
comino et al.
studied the top apps in apple and google stores finding that releases can boost user downloads.
mcilroyetal.
studiedupdatefrequenciesinthegoogle play store finding that only of studied apps received more than one update per week.
these findings support our weekly data collection schedule as very few releases can be missed by collecting data weekly additionally the target releases we use defined in section .
mandate that very frequentlyupdatedappsareexcludedduetolackofsufficient prior and posterior time series data.
mcilroy et al.
also foundthatratingwasnotaffectedbyupdatefrequency however the findings by guerrouj et al.
indicate that high code churn in releases correlates with lower ratings.
nayebi et al.
surveyed developers and users finding that half of developers had clear releasing strategies and many experienced developers thought that releasing strategy affects user feedback.
users were not more likely to install apps based on release date or frequency but preferred to install apps which have been infrequently but recently updated.
all of these previous findings on app releases tantalisingly point to the possibility that certain releases may have higher causal significance than others.
however no previous study excepting our technical report and two page student research abstract has specifically addressed the question of how we identify the set of releases that are significant.
furthermore no previous work attempted to identify the characteristics of highly significant app releases the primary technical and scientific contributions of the present paper.
in our preliminary work we studied apps which were in most popular lists every week in google play and windows phone store between july and july .thisverystrictinterpretationof popular resultedinamuch lowernumberofappsthantheonesconsideredinthepresent study and apps in the google and windows sets respectively.
in the present study using a larger dataset that does not suffer from the app sampling problem we confirm and extend the findings of our preliminary study .
indeed some of the results suggest that certain findings hold between different stores and datasets suggesting that they may be more widely applicable releases mentioning the terms new and feature are more likely to positively affect success higher priced paid app releases are more likely to have a positive effect.
other results such as the mentions of the terms bug and fix perhaps highlight the difference between dataset maturity releases are more likely to significantly affect success if they mention bug fixing in this large dataset but slightly less likely in the dataset used in our technical report .
we can speculate that this is expected as the apps used in this study are not consistently the most popular apps in the store so it is more acceptable for them to fix bugs in releases while in the most popular apps used in our technical report it is perhaps less acceptable to acknowledge bugs or to spend a large proportion of development effort on bugs.
in future work we will investigate this possibility by comparing sets of apps which have different popularity.
to the best of our knowledge we are the first authors to apply causal impact analysis to app store analysis.
however causal inference has been discussed in empirical science papers and it has been previously used in software engineering.
for example the work using the granger causality test by couto et al.
and zheng et al.
as a means of software defect prediction and the work by ceccarelli et al.
on identifying software artefacts affected by a change.
sinceciraallows us to apply causal impact analysis on any time series vector future work will use it to analyse other metrics from app store data and other time series datasets.
future work may also seek to identify the causal effect of app feature migration on susequent success.
.
conclusions in this work we propose the use of casual impact analysis to identify causally significant releases in app stores.
in particular we introduce our tool cirato perform causal impact analysis on google play app releases between february and february for all apps that appeared at least once in the top rated apps in the year prior to february .
for these apps we found that overall release frequency is not correlated with subsequent app success but that there is evidence that price and release text size and content all play a role in whether a release is significant and the type of effect it has on success.
higher priced releases are more likely to be significant and perhaps surprisingly to positively affect rating there were more prevalent mentions of new features and bug fixes in releases that positively affected rating and successful releases also had longer more descriptive release text.
we have shown that causal analysis can be a useful tool for app developers by eliciting their opinions most of those who responded were interested in our findings and agreed with cira s assessment and some said they would consider changing their releasing strategy based on our findings.
.