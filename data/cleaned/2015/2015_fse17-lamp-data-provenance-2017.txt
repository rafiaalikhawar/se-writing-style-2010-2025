please do not remove this page lamp data provenance for graph based machine learning algorithms through derivative computation ma shiqing aafer yousra xu zhaogui et.al.
ks?index ma s. aafer y. xu z. lee w. c. zhai j. liu y. zhang x. .
lamp data provenance for graph based machine learning algorithms through derivative computation.
.
published version document version version of record vor downloaded on this work is protected by copyright.
you are free to use this resource with proper attribution for research and educational purposes.
other uses such as reproduction or publication may require the permission of the copyright holder.
lamp data provenance for graph based machine learning algorithms through derivative computation shiqing ma1 yousra aafer1 zhaogui xu2 wen chuan lee1 juan zhai2 yingqi liu1 xiangyu zhang1 1purdue university usa 2nanjing university china abstract data provenance tracking determines the set of inputs related to a given output.
it enables quality control and problem diagnosis in data engineering.
most existing techniques work by tracking program dependencies.
they cannot quantitatively assess the importance of related inputs which is critical to machine learning algorithms in which an output tends to depend on a huge set of inputs while only some of them are of importance.
in this paper we propose lamp a provenance computation system for machine learning algorithms.
inspired by automatic differentiation ad lamp quantifies the importance of an input for an output by computing the partial derivative.
lamp separates the original data processing and the more expensive derivative computation to different processes to achieve cost effectiveness.
in addition it allows quantifying importance for inputs related to discrete behavior such as control flow selection.
the evaluation on a set of real world programs and data sets illustrates that lamp produces more precise and succinct provenance than program dependence based techniques with much less overhead.
our case studies demonstrate the potential of lamp in problem diagnosis in data engineering.
ccs concepts computing methodologies learning in probabilistic graphical models software and its engineering software testing and debugging keywords data provenance machine learning debugging acm reference format shiqing ma yousra aafer zhaogui xu wen chuan lee juan zhai yingqi liu and xiangyu zhang.
.
lamp data provenance for graph based machine learning algorithms through derivative computation.
in proceedings of 11th joint meeting of the european software engineering conference and the acm sigsoft symposium on the foundations of software engineering paderborn germany september esec fse pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse september paderborn germany copyright held by the owner author s .
publication rights licensed to association for computing machinery.
acm isbn .
.
.
.
introduction we are entering an era of data engineering.
compared to traditional software engineering the complexity of data engineering largely lies in data and models.
for example many data processing programs such as well known machine learning programs have a small size.
but the data processed by these programs and the models generated are often large and complex.
it poses new challenges to engineers such as how to validate outputs and how to diagnose problems.
note that faults may likely reside in data and models while they are more likely present in programs in traditional engineering scenarios.
graph based machine learning gml is an important kind of data processing with increasing popularity.
provided with an input graph model and initial weight values gml algorithms generate an updated model.
most of these algorithms are iterative.
in each iteration a vertex communicates with its neighbors and updates its value until all the values converge.
through multiple iterations a vertex can affect other vertices that are many edges away.
this is called the rippling effect .
due to the nature of such computation it is highly challenging to determine the correctness of the generated models as a fault may get propagated through many steps and faulty states may get accumulated obfuscated during propagation.
even if the user suspects the incorrectness of final outputs she can hardly diagnose the procedure to identify the root cause which could be present in the input graph model the initial weight values or even in the gml algorithm itself.
data provenance is an important approach to addressing the problem.
it identifies the input output dependencies and or records the operation history.
there are a number of existing efforts that aim to provide general frameworks for collecting data provenance for gml algorithms.
most of them focus on selectively collecting intermediate results at run time in an effort to provide crash recovery mechanisms debugging support and so on.
however these techniques can hardly reason about input output dependencies.
dynamic information flow tracking or tainting computes the set of inputs related to a given output by monitoring program dependencies.
however it cannot quantify the importance of individual inputs.
due to the rippling effect of gml algorithms an output tends to be dependent on a huge set of inputs even though most of them have negligible impact on the output.
in this paper we propose lamp a technique to quantitatively reason about input importance.
inspired by automatic differentiation ad techniques lamp works by computing output derivatives regarding inputs.
a large derivative indicates the input is of high importance for the output.
a zero derivative indicates the input has no impact on the output.
however existing ad techniques cannot be directly used in gml provenance tracking due to the following reasons.
derivative computation is 786esec fse september paderborn germany s. ma y. aafer z. xu w. c. lee j. zhai y. liu and x. zhang closely coupled with the original computation leading to high overhead for production runs derivatives cannot be used to quantify the importance of inputs related to discrete behavior such as the inputs whose changes may lead to control flow changes.
they are common in gml algorithms ad techniques typically compute derivatives for a smaller number of inputs.
however in gml all inputs need to be considered.
lamp addresses these challenges.
in particular it considers the initial weight values of vertices edges as the input even though they may be initialized to constants and do not come from the program input.
the iterative procedure then aggregates propagates and updates these values driven by the graph structure.
as such variations to the initial weight value of a vertex have impact to the final outputs proportional to the structural importance of the vertex.
therefore derivatives regarding these initial constant values reflect the importance of vertices.
lamp features a novel design that separates the original data processing from provenance computation so that very little overhead is introduced during production runs.
it quantifies input importance for those that may induce control flow variations by spawning processes to concretely determine the output variations.
the workflow oflamp is as follows.
it first collects a lightweight trace during production run that contains branch outcomes and a small set of states.
the trace allows the decoupling of the original computation and the provenance computation.
it transforms the original program to a new program that solely focuses on provenance computation.
the transformed program takes the original inputs and the trace and produces the provenance i.e.
the importance measurement for each input .
we make the following contributions.
we formally define the problem of provenance computation for gml algorithms which has the key challenge of quantifying input importance.
we propose a novel solution based on computing the partial derivatives of each output variable with respect to the related input variables.
we propose a novel design that decouples original data processing from provenance computation.
we propose an execution based approach to quantify input importance for those related to control flow.
we develop a prototype lamp .
our evaluation on a set of real world gml algorithms and large data sets show thatlamp substantially outperforms program dependence tracking based approaches in terms of accuracy and efficiency producing provenance sets that are orders of magnitude smaller with overhead that is times lower.
our case studies demonstrate the potential of lamp in data engineering for helping the development process and finding bugs in input data graph models and even in gml algorithm implementations.
motivation pagerank is one of the most popular gml algorithms.
it computes a numerical weight for each vertex in a graph to measure its relative importance within the graph called the rank value .
it is widely used in practice such as in search engines and social network analysis.
based on the assumption that a vertex becomes popular when it has many links with other popular vertices the algorithmiteratively computes the rank value for a vertex by aggregating the rank values from its predecessor vertices.
during each iteration the rank value of a vertex is computed by the following equation pr u d v d j bupr j kj cj where bucontains all the vertices which have at least one link edge tou cjis the sum of outgoing edge weights of vertex j and kjis the weight of the edge from jtou.
v is the number of vertices in the graph and dis a user defined damping coefficient to ensure fairness in computation for leaf vertices.
figure presents a sample pagerank implementation in python .
it first assigns the initial rank values to all vertices in lines and then updates the ranks inside a loop.
in each iteration the algorithm traverses all vertices collects ranks from the parent nodes and updates the ranks accordingly lines .
after updating all the vertices it calculates the difference between the current and the previous ranks to check if it has reached a relatively stable state by comparing delta with an epsilon threshold line .
if the difference is smaller enough the algorithm returns the ranks line .
otherwise it continues computing until the condition is satisfied or a maximum number of iterations is reached line .
motivation case.
we apply the algorithm to rank the accounts in the weibo social network .
in the data set a node represents an account and an edge denotes that an account follows another account.
input edge weights are used to represent the number of actions comment re tweet etc.
between followers and followees.
in particular an edge weight is calculated as the number of actions multiplied by a constant and subtracted by the number of tweets for which the follower does not do anything.
we cross check the ranking results with those by other methods .
we observe that most results are consistent and popular accounts have high ranks.
in figure 2a we show the provenance graph for a popular account kai fu lee who is an it celebrity.
the node size represents the rank value of the node and the edge width denotes the impact of a node on another node.
as we can see many accounts including popular ones like newsplus and unpopular ones are following kai fu lee and actively commenting and re tweeting his tweets.
effectively the red node in the center representing kai fu lee has many edges of different weights from many other nodes of various sizes which represents a typical provenance graph for popular accounts.
however we noticed that an unknown account yangqi is mysteriously ranked very high i.e.
within top while it has less than followers far less than popular accounts.
given the complexity of the dataset it is difficult to diagnose why yangqi ranks so high especially that his followers may have their own followers that transitively contribute to his rank.
thus we employ lamp to investigate this case.
figure 2b presents the provenance graph foryangqi .
observe that it is quite different from kai fu lee s graph.
it suggests that the rank of yangqi is highly influenced by another unpopular account qing fei .
note that the node for qing feihas a very small size but its edge is very heavy implying that although qing fei s rank value is small it substantially inflates the rank of yangqi .
further inspection shows that qing fei has many negative weights on its outgoing edges which makes the 787lamp data provenance for graph based machine learning algorithms through derivative computation esec fse september paderborn germany def pagerank graph damping epsilon debug true init rank values for key in graph.nodes ranks attributes input init iter for in range max iteration first store old rank values old ranks copy.deepcopy ranks iter iter if debug print iter iter and ranks ranks update ranks for all nodes in the graph for key in graph.nodes rank sum for n in graph.out edges key outlinks float weights graph.out edges n if outlinks rank sum weight n old ranks outlinks page rank compution ranks damping num v damping rank sum if converge return the ranks delta for key in graph.nodes delta delta abs old ranks ranks if delta epsilon return ranks hit the max iteration return return attributes output ranks def pagerankprov graph damping epsilon debug true for key in graph.nodes prov ranks key for in range max iteration for key in graph.nodes .
.
.
for n in graph.out edges key if load line20 cnt line20 plus true for k in graph.nodes prov rank sum prov rank sum prov old ranks n load line21 cnt line21 plus true for k in graph.nodes prov ranks key damping prov rank sum if load line30 cnt line30 if size line30 cnt line30 and unstable test delta epsilon p spawn ranks load line30 cnt li ne30 line7 unstable logging p return prov ranks else similar to true branch cnt increase line30 return prov ranks 5a b c d e a pagerank code b transformed pagerank code i ii s figure pagerank and the corresponding transformed code sum of its outgoing edge weights i.e.
cjin equation far smaller than the edge weight from qing fei toyangqi i.e.
kjin equation .
as a result kj cjis much larger than such that the rank of yangqi pr u is essentially the rank of qing fei pr j multiplied by a very large factor according to equation .
the root cause is that when the dataset was generated by other researchers the parameter was not well defined which has led to negative weights.
to fix this problem we redefine the value of to preclude negative values.
the updated rank of yangqi correctly represents its unpopularity.
its provenance graph is shown in figure 2c.
this case illustrates a typical fault in data engineering.
more cases can be found in .
table provenance by tainting graph node edges avg max run mem stanfordweb .
googleweb .
tencentuser .
tencentmsg .
twitter .
dependency analysis based provenance tracking.
a traditional way of collecting provenance for programs is by tracking data and control dependencies .
however in gml an output tends to be transitively dependent on a large set of inputs through program dependencies due to the rippling effect.
such huge provenance sets for outputs are hardly useful as the importance of individual input values cannot be distinguished.
a lot of inputs in the provenance set have very little impact on the output.
furthermore taint propagation for individual operations is usually implemented as set operations e.g.
set unions which are very expensive on provenance sets.
we run pagerank for iterations on a few data sets while collecting provenance using dynamic tainting.
the results are shown in table which presents the input data set column the number of vertices edges column in the graph the average maximum size of provenance set column and the runtime memory overhead column .
observe that even though we only run it for a small number of iterations the provenance sets are already of large size which cause substantial overhead.
in comparison we show the distribution of derivatives computed bylamp when running the pagerank algorithm on the tencent message data set in figure .
the x axis represents the logarithm of derivative value.
and the y axis represents the number of occurrences.
as we can see most derivatives are small meaning that the rank of a vertex is mostly determined by a small number of vertices.
problem statement our goal is to compute data provenance for gml programs.
definition .
a graph based machine learning gml program ptakes a graph model gand its nodes initial weight values i x0 ...xm where xiis the initial weight of node i and produces updated weight values fwithout changing the graph structure.
the initial weights of a given graph s nodes may be explicitly provided as part of the input e.g.
as in bayesian networks belief propagation or using a pre defined constant e.g.
as in the original pagerank .
edge weights are handled no differently from node weights as both are represented as program variables.
hence for simplicity we only assume node weights in our discussion.
lamp can be applied to many ml algorithms that fall in our gml definition.
problem statement .
consider the output weight for a node nas a function over the initial weights denoted as fn x0 ... xm .
for each node iwith an initial weight xi and a given execution x0 a0 ... xm am we aim to compute the partial derivative d fn a0 .. am dxi if the partial function fn a0 ... ai xi ai ... am 788esec fse september paderborn germany s. ma y. aafer z. xu w. c. lee j. zhai y. liu and x. zhang kai fu lee newsplusjingdonglaoliujun t ang jiangnan chunjun wu wei guoqiaosheng hanxiao wei lan yangshi wangchuan luo yaqin zhangqingwei yangxianggang wujun lei jingxinweiyu yifei leeweiyumomo dreamyzhen hanying zheng xunsun jiahaolei weizhan peng long chao jiang wuyingqiu1 guanting peng delanwen hongmao fu tian guanyu xuan xunlai yaling he yang gong rong jiang meixun xuegang t eng hengzou wu lai yanlang yang yao ru kang jint an qiu huo guanyu shi songfan zhenhou tian qin liang kong liuxianyao lin ye cai xun qingzhan yixiong kangduan yuhan xian zhengfang jing he hui yin zhelan gu mengbai leigong linsu zhenya jin da huang chaoliu zeminxia fangzeng tu lai cai long luoyangchen kang ma guanyujiang dong liang shuisong tu zhong gengxin huo jianhong ren shidu xuefengjiang junzeng chun yao jia ye fenyin shilian jianzhang jing yuan lixue zexi a kai fu lee yangqi qing feixiang huawan feng t eng pengcui zemin shao zexi yang weiyao zhang zhu sut ang qiu liao xinyima chenxue t ai hou ah han mingjin lim xia jiefu yijun zhou yatingxu hualai yinpeng yang ma liangjin delan yu chin qian licui jiang dai t aizhao yi xia yuyi wu cheng mingong changqiao shuntian da han zhenkangchen xun dai jingyiye xinya lang jinxiang zhong deng huo yi ahmeng jieshao xuefenghao chin du rongt ang henghe qing yuan yuhuo shui zi zexihuang ping gong qingyuan jie ren xiuying yan fen zhao xue guo huliu hong su t ai han zhelandeng yazhu jiang minsun ning shen lili long ma jiehuo andai hua xun zemin luo shunqiao ruogang xue lim cai long cheng guogu yan zi chenxuan kunhao qigang deng hongbai kangcai zemindong zan su linlang yimu ma gengxinqiu1 xue xie xun lin su b yangqi yangqi qing fei xiang huawan feng t eng peng cui zeminshao zexi yang weiyao zhang zhu su t ang qiu liao xinyima chen xue t ai hou ah han mingjin lim xia jie fu yijunzhou yating xu hua lai yin peng yangma liang jin delanyu chinqian li cui jiangdai t aizhao yi xia yuyi wucheng min gong chang qiao shuntian dahan zhenkang chen xundai jingyiye xinya lang jinxiang zhong deng huo yi ahmeng jie shao xuefenghao chindu rong t ang henghe qing yuan yuhuo shui zi zexihuang ping gong qingyuan jie ren xiuyingyan fen zhao xueguo huliu hong su t aihan zhelan deng yazhujiang min sun ning shen lili long ma jie huo andai huaxun zeminluo shunqiao ruogang xue limcai long cheng guogu yan zi chenxuan kun hao qigangdeng hong bai kangcai zemin dong zansu linlang yimu ma gengxinqiu1 xue xie xunlin su c yangqi new data figure weighted pagerank data debugging example number of valuesthe logarithm of partial derivative figure the distribution of impact values tencent message graph is continuous at xi ai.
otherwise we compute fn a0 ... ai ... am fn a0 ... ai ... am with being an infinitely small value.
our goal is to compute the partial derivative that represents the impact of each input on each output.
intuitively if a vertex is important i.e.
by having a high initial weight or by being connected to many other vertices a small perturbation of its initial value will change the values of all the connected vertices and eventually lead to substantial output changes.
however while mathematical functions are largely continuous gml programs have a lot of discrete behaviors.
as a result the fnfunctions are usually discontinuous.
note that in a discontinuous function an arbitrarily small input variation does not lead to arbitrarily small output variation.
as a result the derivative is infinite.
in this case derivatives do not represent the impact of input variations.
therefore we report the output variations instead.
figure depicts an illustrative example.
the program presented in the small box behaves as follows based on the input x f x f x x c x otherwise f x is discontinuous at x c. for any x tandt c lamp computesd f t dx which is the slope of the tangent line to f x at x t denoting how the output varies at the neighborhood of x t. upon x c the derivative is not informative due to the discontinuity.
hence lamp computes f c c instead which gauges the impact of input variation.
in this paper we do not consider floating point rounding errors.
since we reason about input variations at a much larger scale compared to rounding errors the effects of rounding errors are largely t xdx x dft f x f figure essence of lamp shadowed.
we will leave a thorough study of the interference of rounding errors to the future.
overview basic idea.
lamp computes the partial derivatives for each variable on the fly.
given a statement y f t1 ... tm with t1 ... tmthe operands lamp computes the partial derivatives of yregarding each initial weight leveraging the derivative chain rule.
it is a rule to compute the derivative of function composition d dx d du du dx where f u is the final output uis an intermediate result and xis the input variable.
intuitively it says that the derivative of a function regarding its input can be computed from the derivative of the function regarding an intermediate result and the derivative of the intermediate result regarding the input.
leveraging the chain rule derivative computation can be done locally to a statement based on the operand values and their partial derivatives that were computed when the operands were defined.
upon a predicate lamp checks if a small variation to any initial weight value can cause the predicate to take a different branch outcome.
this can be done by linear approximation using the computed partial derivative of the predicate expression.
if so lamp spawns a new process to take the other branch.
at the end output variations are derived by comparing the outputs from all the processes.
according to the problem statement in the partial derivatives and the output variations caused by discontinuity are the resulting data provenance that gauges the impact of inputs on outputs.
consider the code example on the left side of figure .
the program takes two initial weight values x1andx2.
the predicate at line makes the output weights discontinuous functions.
the next two columns in the same figure show the conceptual provenance computation regarding x1andx2 respectively.
at line the partial 789lamp data provenance for graph based machine learning algorithms through derivative computation esec fse september paderborn germany 2x1 x1 x1 x1 x1 x1 x1x2 x2 x2 x2 x2 x2 x2 figure example to illustrate the basic idea.
symbol p x1denotesd p d x .
program p s statement s x le x lyopz while x l0 s ifx l0then s1else s2 expr e v x xinput youtput operator op ... value v true false ... label l l1 l2 ... variable x y z identifier figure language abstraction derivatives of pwith regards to x1andx2are2and respectively.
at line the partial derivatives of qwith regards to x1andx2are 0and x2 respectively.
at line according to the chain rule the derivatives of rare computed from the values and the previously computed derivatives of pandq.
note that the symbolic value ofrregarding x1andx2isr x1 x2 2x1x2 3x3 d r x1 x2 d x 4x1x2 9x2 which is exactly the value computed through the three steps .
upon the predicate 4tindicates that if the original execution takes the true branch i.e.
r but the linear approximation of r s variation with a small variation ofx1leads to the opposite branch outcome a new process is spawned to take the false branch so that the outputs along this different path can be computed and contrasted with the original output.
workflow.
the aforementioned procedure is just a conceptual explanation.
in practice it is too expensive to perform provenance computation during production runs.
an important design choice oflamp is hence to decouple the original computation from the provenance computation.
observe that in the provenance computation the values computed in the original run are usually not needed.
for instance at steps 1and the values of p x1 orx2 are not needed at all.
in fact the operand values are only needed in multiplication and division operations which are relatively rare compared to additions and subtractions.
therefore lamp records the needed variable values during production runs such as values pandqat line .
lamp then transforms the original program to a new one dedicated to provenance computation which is triggered on demand.
during provenance computation the logged values are used to avoid most of the original computation.
design to facilitate discussion we introduce a simple language in figure .
we use superscripts to indicate input and output variables.
note that here input variables are the initial weight values which may beloaded from input files or initialized to some constant e.g.
line in figure .
each statement is identified by a label l. the language is just for discussion.
our implementation supports python.
.
run time information collection during production runs lamp conducts very lightweight tracing to collect branch outcomes and the results of some operations e.g.
multiplications .
the tracing semantics are explained in figure .
the expression rules are standard.
according to the evaluation contexte expressions are first evaluated to values before the statement rules are applied.
statement evaluation has the configuration of andc is the store is the tracing log that consists of a sequence of trace entries each containing a statement label the execution counter value of the statement and a set of values c records the current counter value for each statement.
the evaluation rules of most statements are standard and hence elided.
rules and indicate that lamp may log the operand values for multiplications because such values are needed in derivative computation in the later provenance computation phase .
if the compiler statically determines that both operand variables yandzare related to annotated input variables their values are logged by attaching an entry to .
the counter is also increased.
similarly if only one operand is input related the other operand value is logged.
when neither operand is input variable related lamp does not need to compute the derivatives and hence the operand values are not recorded rule .
upon a conditional statement lamp determines if the predicate is related to input variables.
if so it further detects if the branch outcome may be unstable by function unstable .
we say that a branch outcome is unstable if a small input perturbation flips the branch outcome.
specifically for a predicate v 0and a related input xi the predicate is unstable if dv d xi v .
however during production run we do not know the derivative.
hence we test if v is smaller than the pre defined maximum partial derivative.
if so the predicate may be unstable and we log the branch outcome the value v and the values of critical state variables rule .
we determine if the predicate is truly unstable during provenance computation.
if so a process is spawned to take the other branch.
logging the critical state variables is to support the child process.
for example in pagerank figure the comparison at line is input related and possibly unstable when delta epsilon is smaller than maxd .
thus ranks values are recorded which are sufficient for execution along the other branch.
the set of critical variables at a program point l cs l is pre computed by the compiler.
in our experience cssets are small 790esec fse september paderborn germany s. ma y. aafer z. xu w. c. lee j. zhai y. liu and x. zhang definitions log label index v alue store v ariable v alue c stmtindex label index input rel x if xis transitively data dependent on any input cs l the set of critical state variables at l unstable v v maxd with the input variation bound and maxd the upper bound of partial derivatives.
it determines if the predicate may potentially take the opposite branch in the presence of input variation.
semantic rules e e s s x e x eope x vop e if e ythen s1else s2 if v ethen s1else s2 expression rule ee v ve v xe x statement rule c ss c s c x lvy vzs l c l vy vz c skip if input rel y input rel z c x lvy vzs l c l vy c skip if input rel y input rel z c x lvy vzs c skip otherwise c ifv l0then s1else s2s l c l t rue v cs l c s1 ifv input rel x unstable v c ifv l0then s1else s2s l c l t rue c s1 ifv input rel x unstable v global rules ee v c e e c v c ss c s c e s c e s figure semantic rules in gml programs and the number of unstable predicates at run time is very small thus the space overhead is low .
if a predicate is not input related or is stable lamp simply logs the branch outcome rule .
the branch outcomes will be reused during provenance computation to ensure the same control flow.
.
code transformation lamp transforms the original program to a new program which takes the original input graph and the log generated in the tracing phase and performs provenance computation.
figure describes the set of transformation rules.
a number of terms and helper functions are defined in the top of the figure.
in particular two global variables are declared in the transformed program that maps a variable to its partial derivatives regarding input variables andcntthat maps a statement to its current execution count.
rule specifies that for a statement in the original program that copies an input variable yinputtox statements are added to the transformed program to set the derivative of x regardingyinputto and the derivatives for other input variables to .
in all the rules the transformed statements are boxed.
note that the original assignment is precluded from the transformed program.
for an addition statement the transformed statement adds the corresponding derivatives rule .
rule specifies that given a multiplication statement x y z if only yis input related the transformed statement computes the partial derivative of xby multiplying the derivative of yand the recorded value of zfrom the log .
when both yandzare input related the multiplication is transformed to statements that compute the derivative of xfrom both the derivatives and the values of yandz rule .
when the variable in predicate is not input related the statement is transformed to loading the branch outcome from the log rule .
according to rule when the variable is input related the following statements are addedto the transformed program.
line tests if the recorded branch outcome is true.
if so line further tests if the log entry contains additional information i.e.
which indicates the predicate is potentially unstable and if a small input variation induces a value change on xlarger than the recorded value leveraging the partial derivative.
if so the branch outcome can be flipped.
hence lamp spawns a process to continue execution along the other branch in the original program lines .
before executing the branch lamp restores the critical state.
the parent process continues the derivative computation in the true branch line .
line is to log the annotated input variables whose variations may flip the branch outcome and the child process id.
at the end of computation for each input that causes unstable predicates lamp collects the values of an output variable zacross all the associated processes with zmax andzmin the maximum and the minimum z which denote the impact of the input on z. figure shows the transformed pagerank.
.
discussion property .
if an output is a continuous function of an annotated input variable within a range the partial derivative computation of lamp is precise in the range.
for example in figure the derivative computation in the ranges x candx cis precise.
this is because lamp strictly follows the mathematical rules of derivative computation.
however since lamp does not model derivative variation with regard to input variation.
as such if a derivative varies substantially e.g.
when an output function oscillates rapidly the derivatives computed by lamp may not be a good indicator for input impact.
in practice most gml algorithms are iterative algorithms that have very slow derivative variation as supported by our experiments in .
property .
assuming the variation of an output function f x within an input variation bound of is bounded by .
in the 791lamp data provenance for graph based machine learning algorithms through derivative computation esec fse september paderborn germany definitions the array storing the derivatives of xregarding inputs cnt the execution counter for statement l the first recorded value for the log entry of the nth instance of statement l spawn s l spawn a process that first executes sand then original code starting from lwith the tracing semantics lo p t record that process pis spawned because of instability caused by input t iv the set of annotated input variables transformation rule sctx s x yinputctx f or t in iv x yctx x y zctx f or t in iv input rel y input rel z x ly zctx skip input rel y input rel z x ly zctx f or t in iv cnt input rel y input rel z x ly zctx f or t in iv cnt input rel x s1ctx s 1s2ctx s if x l0 then s 1else s 2ctx if then s 1else s input rel x s1ctx s 1s2ctx s if x l0 then sl1 1else sl2 2ctx 1if then 2if d d then p spawn cs l2 f or t iv s.t.
lo p t 5s 6else ... !
7cnt l figure transformation rules presence of discontinuity caused by control flow the error of data provenance computed by lamp is bounded by .
recall upon discontinuity lamp computes output variations instead of derivatives .
take figure as an example.
at x c ideally lamp should compute f c c .
however according to our semantics when x c c the branch outcome may be flipped lamp hence computes f x x .
note that although f x is undefined in c c in the original program lamp essentially approximates it in this range by spawning a process to take the else branch.
according to our assumption the computed f x x has a bounded error when compared to f c c .
lamp uses the derivatives as provenance and outputs a weighted dependency graph with partial derivatives as weights.
it can detect bugs that affects such dependency relationships and corresponding weights.
its capability is also affected concrete numerical values.
bugs with invisible or little effect on the weights have less probability to be detected by lamp .
additionally as lamp targets the machine learning computing process it will not be able to help if the bugs occurring in the workflow e.g.
choosing the wrong data sets inappropriate machine learning algorithms .
evaluation in this section we report the evaluation results regarding efficiency and effectiveness.
our implementation is based on a python analysis platform we developed which can perform static analysis and instrumentation.
all experiments were conducted on a machine with cores and gb memory running ubuntu .
lts.table code size loc of transformed program algorithms original transformed pagerank weighted pagerank undirected pagerank visit of links based pagerank visit of links based weighted pagerank personalized pagerank collusionrank simrank ascos textrank belief propagation gibbs sampling table shows the real world algorithms used in our evaluation.
weighted pagerank computes rank values in weighted graphs visit of links based pagerank requires the visit information to rank web pages personalized pagerank requires a user input vector i.e.
user preference to calculate the rank values for individual users.
simrank and ascos calculate the similarity of two nodes in a graph with different methods.
belief propagation inferences the marginal distribution for unobserved nodes conditional on observed nodes.
gibbs sampling is a well known markov chain monte carlo mcmc sampling method.
the 2nd and 3rd columns show the sizes of the original program and the transformed program.
note that although these programs are not large they are the typical gml programs used in practice.
popular graph analysis tools e.g.
networkx graph tool implement pagerank in locs.
our adaption of gml programs of such sizes is also consistent with experiment setup in the literature .
792esec fse september paderborn germany s. ma y. aafer z. xu w. c. lee j. zhai y. liu and x. zhang .
efficiency run time different programs may require data sets in different formats.
for programs we used the same real world data sets.
for the remaining programs we used other data sets.
the data sets are acquired from public sources e.g.
stanford large network dataset collection and movielens or crawled by scrapy .
the sources and the data set sizes number of vertices number of edges and text file size for gibbs sampling are reported in table .
some data sets e.g.
tencent messages are huge so that we can only use part of them due to our memory capacity.
here we only report the overhead of provenance computation.
the tracing overhead is less than for all the programs due to the limited instrumentation and the small logs generated.
as shown in table provenance computation takes to times the original computation time.
lamp s provenance computation is optimized.
the optimizations reduce the overhead by an average factor of .
details about optimizations can be found in .
while we consider the overhead reasonable it cannot be afforded during production runs.
this strongly supports our design that separates the original computation from the provenance computation.
0100200300400500600700013274155698397111125139153167181195209223237251265279293307321335349363377391405419433447memory mb time s pagerankundirected pagerankweighted pagerankvol pagerankweighted vol pagerankpersonalized pagerankcollusionranksimrankascostextrankbelief propagationgibbs samping figure memory usage of provenance computing memory consumption we measure the memory consumption of provenance computation using python memory profiler that samples every .1second.
figure presents the results.
in the first few minutes it grows very fast.
this is attributed to two reasons.
first the process first needs to load the whole graph.
second during the first a few iterations the number of inputs related to a variable rapidly grows.
after this the sets of related inputs are relatively stable and only their derivative values are being updated in memory.
thus the memory consumption has a slow growth.
for most programs our optimizations can reduce the memory consumption by a factor of .
log space aslamp needs to log some variables and branch outcomes we also measure the log size.
for the data sets used with millions nodes and edges the log size ranges from 7kb.
a key reason is that the running encoding optimization is very effective in compressing branch outcomes details in .
comparison with tainting to demonstrate the advantage of lamp over tainting we compare the efficiency of the two techniques.
our experiments show that the average runtime and space overheads for tainting are .
and .
whereas those for lamp are .
and .
respectively.
this is because tainting has to manipulate large sets.
details are in .
unstable predicates the stability of predicates varies according to the input variation threshold .
we conduct experiments with and .
we observe totally and forks respectively for our test data sets.
for a single execution the maximum number of forks we observe is .
most cases have only one fork.
recall that lamp forking a child process means that the input variation flips a branch outcome leading to output variations that cannot be described by derivatives.
.
effectiveness we study the distribution of the derivatives for all the data sets.
our results show that only .
.
of derivatives are larger than .
.
larger than and are smaller than .
details are in .
this clearly indicates that most of the related inputs are insignificant.
unfortunately traditional tainting based approaches would report all these insignificant inputs.
we also perform another experiment to validate the correctness of the computed derivatives.
for each data set we randomly select an input whose derivative is larger than .
we then mutate the input value by run the program again with the mutated data and measure the output differences.
we then compute the observed derivative as the ratio between the observed output difference and the input variation and compare it with the reported derivative by lamp .
we repeat it times i.e.
randomly selecting different inputs for each data set and report the average.
our results show that the average difference of observed derivatives and reported derivatives range from .32e 7to1.02e 05with the standard deviation from .42e 08to3.42e details in .
the differences are very small indicating that the derivatives by lamp can precisely measure input importance.
this supports our assumption that derivatives change slowly with input changes .
.
case studies .
model evolution ml models are often generated in an incremental fashion due to the cost of training and the availability of new training data.
we use a bayesian classifier to demonstrate how lamp can facilitate data engineers in this process.
the classifier implements paul graham s algorithm to classify spam comments.
the original model was trained on manually labeled youtube comments.
given a new comment it outputs a score between and to predict if the comment is a spam.
the score is computed from the scores of individual words in the comment.
for example the word click which appears frequently in spam comments and rarely in benign ones strongly hints a spam comment.
we applied the original model to a data set collected from other youtube videos and observed many mis classifications.
we show two examples in the following.
c1 benign billie jean this is one of the my favorite videos i have ever seen this year.
it is performed by michael jackson.
the dance is known as the moonwalk.
c2 spam geico is the best auto insurance company.
the price is low.
you should apply now .
the computed score of c1 is .
indicating a spam.
but it is a false positive.
we generate its provenance graph figure 10a using lamp .
the size of a node indicates the probability of 793lamp data provenance for graph based machine learning algorithms through derivative computation esec fse september paderborn germany table provenance computation overhead algorithmweb graph tencent message cit hepph tencent user ego twitter p2p gnutella08 wikipedia link 1k 1m 382k 13m 34k 421k 438k 6m 81k 2m 6k 21k 4k 823k pagerank .
.
.
.
.
.
.
per pagerank .
.
.
.
.
.
.
collusionrank .
.
.
.
.
.
.
simrank .
.
.
.
.
.
.
ascos .
.
.
.
.
.
.
belief prop .
.
.
.
.
.
.
algorithmtwitter p2p gnutella09 wiki vote movielen 10m web google soc slashdot0922 email euall 696k 2m 8k 26k 7k 104k 72k 10m 876k 5m 82k 948k 265k 420k pagerank .
.
.
.
.
.
.
per pagerank .
.
.
.
.
.
.
collusionrank .
.
.
.
.
.
.
simrank .
.
.
.
.
.
.
ascos .
.
.
.
.
.
.
belief prop .
.
.
.
.
.
.
algorithmlda sample lda sample lda sample lda sample lda sample lda sample lda sample 121m 143mb 147mb 100mb 137mb 125mb 134mb gibbssamp .
.
.
.
.
.
.
algorithmweb graph web graph web graph web graph web graph web graph web graph volprank .
.
.
.
.
.
.
wvolprank .
.
.
.
.
.
.
algorithmmovielen 100k movielen 20m movielen 1m movielen 10m ego facebook ego amazon com dblp 1k 100k 138k 20m 6k 1m 72k 10m 4k 88k 335k 924k 317k 1m undirprank .
.
.
.
.
.
.
simrank .
.
.
.
.
.
.
collusionrank .
.
.
.
.
.
.
videos performedmy known dance favouritemichaelby year.have onec1 a c1 priceyou auto insurancegeico should now.bestis thecompany.low.
c2 b c2 figure provenance graphs for c1 and c2 the word comment being spam while the weight of an edge represents the partial derivative.
as shown in the graph c1 s score is determined by words.
it is strange that the word videos which is very common in benign youtube comments has a high score and is the most influential node in the classification decision.
moreover the stop word myis the 3rd most influential word while we expected such words should have been filtered out.
the classifier produces a score of .
for c2 which is a false negative.
the provenance graph in figure 10b indicates that the words geico andauto insurance are assigned a predefined score value of .
which the model assigns to any word not appearing in the training set.
however we argue that the default value undermines the influence of these two words which are strong indicators of spam.
in fact when cross checking the words used in benign and spam comments with the most commonly used english words we find that spam comments are more likely to use uncommon words such as company names.
based on the above analysis we improve the model by adding a pre processing step to filter highly common words and stop words and initializing the default score of unknown words to .
.
table shows the performance before and after our improvement.
the firsttable models for spam comments detection original newtotal spam benign spam benign spam benign total row reads as follows spams are classified as spam and spams are classified as benign by the original model and the numbers become and respectively by the new model.
observe that the new model has much smaller false positive and false negative rates.
we also use lamp on a few other public spam filtering models to improve performance.
the results are shown table .
for these models we can improve their accuracy from to .
table model evolution modelaccuracymodelaccuracy original new original new spamfilter .
.
bayesspam .
.
nbsf .
.
bayesianfilter .
.
antispam .
.
sms filter .
.
.
model error debugging bayesian networks are widely used in decision making such as diagnosing cancer .
in this case we use a lung cancer diagnosing network with nodes to demonstrate how lamp can be used in debugging faults in models.
we inject a fault to the model by changing a few conditional probabilities related to gender e.g.
p cou hin ender male ... from .
to .
.
we then apply the model to a public cancer data set .
we encounter a number of misdiagnosis cases with the faulty model.
note that 794esec fse september paderborn germany s. ma y. aafer z. xu w. c. lee j. zhai y. liu and x. zhang gender age smoker living locationdyspnea hemoptysis coughing wheezingchest pain cachexia dysphasia pneumonia heamoptysis air pollutionhoarseness vertigoheadaches seizureslung cancer figure provenance for model bugs due to the model complexity it is really difficult to spot the fault by inspecting the model.
for instance the lung cancer node is connected to nodes which are further connected to nodes andgender is one of these nodes.
we then use lamp to generate the provenance for the misdiagnosis in figure .
from the graph the decision is evenly attributed to many nodes.
nothing seems suspicious.
but when we further investigate the provenance of the nodes connected to lung cancer we find that a few of them have unexpectedly heavy influence from gender indicated by the thick arrows in comparison with other second layer nodes such as age.
this suggests that the faulty conditional probabilities of gender transitively lead to the wrong decision.
we inject other bugs into the model by changing the network structure or modifying the probabilities for other nodes edges.
the provenance graphs by lamp are always able to point to the faulty places.
.
debugging gml implementation a b cd e f a network dab b b c a figure pagerank debugging example debugging ml programs can be hard because sometimes we do not have clear constraints to determine if the returned results are correct.
in this case we show how lamp is used to discover that a popular python pagerank implementation on github with forks and stars is buggy.
while the implementation produces very reasonable ranking results for some undirected graph data sets we observed some seemingly incorrect results on some directed graph data sets.
we apply lamp to further analyze the suspicious results.
figure 12a shows an example input network connection graph.
the pagerank implementation generates a very low rank for node a which does not seem right.
the provenance for a s rank figure 12c indicates that other nodes have no impact on a which is buggy.
in contrast figure 12b shows that node d has influence from nodes a and b but not from c e or f to which it is also connected.
from the two provenance graphs it becomes clear that the implementation is considering outgoing edges as incoming edges.
it hence does not work properly for directed graphs.
table bug list bug provenance constructing a wrong graph structure wrong dependencies feeding a directed graph to an algorithm designed for undirected missing dependencies missing initial values leads to a nanoutput missing dependencies missing computation steps leads to incorrect output missing dependencies getting the wrong dependencies during computation wrong dependencieswe also reproduce other bugs reported on stackoverflow and github and use lamp to help identify the root causes.
in table we show the brief description of the bugs and the corresponding anomalies in the provenance graphs.
details are in .
related work lamp is inspired by ad but differs from ad.
first ad computes derivatives alongside with the original computation whereas lamp decouples the two so that provenance computation can be activated on demand.
second ad cannot reason about output variations caused by control flow differences while lamp can.
third ad typically computes derivatives regarding some inputs but lamp considers all inputs.
lamp also has a number of optimizations specific to provenance computation.
gleich et al.
studied the pagerank algorithm sensitivity with respect to the damping parameter.
a few projects study the behavior of various pagerank algorithms for multiple values of the damping parameter .
zhang et al.
infer spam pages by investigating pagerank with different damping parameters.
their argument is that spam pages should be sensitive to a given damping parameter thus changing it will disclose them there are also other works trying to develop testing techniques for machine learning algorithms.
other approaches such as aim to support data provenance in disc systems.
provides a general wrapper for mapreduce jobs providing data provenance capabilities.
matteo et al.
proposed titian a general provenance collection system for the spark system that enables data scientists to interactively trace through the intermediate data of a program execution and identify the input data at the root cause of a potential outlier or bug.
while the intermediate results define the provenance for titian lamp calculates the partial derivative as provenance which can quantitatively measure the output sensitivity with regards to the input and produce precise and succinct dependence relationships with low overhead.
these proposed approaches are system specific.
lamp is not bound to any data processing systems.
conclusion in this paper we propose lamp a data provenance computation technique for gml algorithms.
it features the capability of quantifying input importance.
it is inspired by ad techniques and goes beyond them by decoupling derivative computation from the original computation and supporting control flow path variations.
the experimental results show that lamp is much more efficient and effective than program dependence tracing based techniques.
the results by lamp can be used in optimizing the machine learning models debugging implementations and debugging data bugs.