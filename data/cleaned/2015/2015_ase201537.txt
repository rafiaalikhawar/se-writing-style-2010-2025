test analysis searching for faults in tests matias waterloo suzette person sebastian elbaum computer science and engineering department university of nebraska lincoln lincoln ne usa waterloo sperson elbaum cse.unl.edu abstract tests are increasingly specified as programs.
expressing tests as code is advantageous in that developers are comfortable writing and running code and tests can be au tomated and reused as the software evolves.
tests expressedas code however can also contain faults.
some test faults aresimilar to those found in application code while others are moresubtle caused by incorrect implementation of testing conceptsand processes.
these faults may cause a test to fail when it shouldnot or allow program faults to go undetected.
in this work weexplore whether lightweight static analyses can be cost effective inpinpointing patterns associated with faults tests.
our explorationincludes a categorization and explanation of test patterns andtheir application to open source projects that include over 40ktests.
we found that several patterns detectable through simpleand efficient static analyses of just the test code can detect faultswith a low false positive rate while other patterns would requirea more sophisticated and extensive code analysis to be useful.
i. i ntroduction as with any software test code may be written by developers who are inexperienced rushed or not fully aware of what is to be tested or how to test it.
as a result test codemay contain faults.
some may be similar to those found inapplication code such as null pointer dereferences.
other faultscan be quite distinct as they are caused by improper implemen tation of testing concepts associated with how the applicationstate is set manipulated or checked by the test code or byassumptions about the underlying testing infrastructure.
consider for example a test written to compare the results computed by software under test sut during the test runwith the expected results using a string representation of twoobjects in an assert statement.
in this case the comparison may include parts of the object that are updated during testexecution but not relevant for determining if the test passesor fails.
hence the test may fail because the test author wasunsure of which parts of the object to check in the assert statement rather than failing because a fault in the sut causedit to update the sut state incorrectly.
to detect this over checked fault in test code requires an analysis of the implicit testing process of comparing the expected and actual results.now consider test code that does not include any explicitbehavior checks.
in this case the test code may fail to reveala fault because the application behavior is under checked e.g.
checked only through exceptions that may be thrownduring test execution.
these two cases illustrate test faultsassociated with state checking but similar test correctnessissues may emerge when test code does not set the applicationstate properly or incorrectly assumes an environmental setting.
faults in test code may cause a test to report a failure when it should not resulting in wasted developer time toinvestigate a fault that does not exist.
they may also allowapplication faults to go undetected.
sadly detecting such testcode correctness issues mostly occurs when a test fails andis typically performed manually.
this challenge is aggravatedby the increasing numbers of tests that can be generatedand executed automatically and the continuous scheduling oftest executions which also pose a tremendous scalingchallenge for approaches that attempt to check for test anoma lies at run time e.g.
running the tests repeatedly to removeflakiness dynamic analysis techniques .
techniquessuch as mutation testing can help to assess the overalleffectiveness of a test suite but its costs often makes itimpractical to apply at such large scales and it does not identifyfaults in individual test cases.
automated static program analysis techniques have been successfully used to identify coding patterns that may indicatea fault e.g.
.
these techniques whenapplied to test code tend to focus on misuses of the testframework api e.g.
a test class without test methods.
moresophisticated test code pattern detectors have been recentlyexplored by our community particularly to identify test smellsassociated with style and maintenance issues and assessing the presence and impact oftest smells but have not focused on test correctness.
in this work we explore and assess a set of patterns targeting correctness faults in test code.
we encode patternsidentified by practitioners and researchers of xunit tests thatare associated with tests that may either indicate a failure whenthe sut does not actually fail or tests that may miss a faultthey are intended to capture.
such faults may render tests oflimited value or worse waste testing resources.
we focuson patterns that can be recognized with light weight staticanalyses of the abstract syntax tree associated exclusivelywith the test code not the sut and perform at most a basicdata flow analysis.
the contributions of our work include identification of the problem space of automaticallydetecting faults in test code.
a collection of test code patterns that may indicatefaults in xunit test code.
a prototype implementation of a light weight staticanalysis to detect those patterns.
a study of the prevalence of test code patterns inxunit tests found in software projects and a deter mination of whether the patterns indicate real faults.
an analysis of which patterns matter which ones havepotential what is needed to improve their effective ness and which ones seem to be misguided.
30th ieee acm international conference on automated software engineering .
ieee ii.
t est code pa tterns the goal of our approach is to quickly analyze large test suites to detect faults in the test code.
inspired by existing techniques with a similar goal but that operate on applicationcode our approach aims to detect test code patternsthrough a light weight static analysis of only the test code enabling our technique to work when the sut is not availableor it is too expensive to analyze or execute.
light weightenables us to analyze large test repositories with thousandsof tests in just a few seconds.
we scope the work to tests implemented in xunit and in particular we study those in the popular junit framework written in the java programming language.
each test class contains one or more test methods.
test classes are combined to create a test suite.
xunit uses annotations or naming conventions to mark test methods and to configure the testrun.
common test data can be specified as test fixtures which can be created and initialized using a setup method and thenreleased using a teardown method to prevent unintentionaldependencies between tests.
the sequence of steps performedby an xunit tests typically follow a pattern beginning witha setup phase to establish a particular state followed by asequence of calls to the sut.
then a check is performedcomparing the expected and actual sut state often usingmethods in the assert class.
finally the test performs the clean up phase.
critical to the success of our approach is the selection of coding patterns that are effective at pinpointing only trulyproblematic test code.
in this work we developed patternsbased on coding observations and experiences of our own orothers that we prototyped tested and refined to strike a balancebetween recall and precision.
for brevity the following patterndescriptions focus only on the most interesting surprising or potentially valuable patterns we found.
we discuss otherpatterns and lessons learned during this process in section iv.
a. categories of patterns in this work we describe three categories of patterns for identifying problematic test code.
inter test dependencies patterns.
this category includes patterns that identify potential dependencies between tests where a test relies on the state set by a previously executed test.this type of dependence is problematic because it implies atest ordering that is rarely enforced by engineers and that is notdefined by default for junit.
it is particularly bothersome forsoftware test engineers because it can manifest as intermittenttest failures.
common test maintenance tasks e.g.
move remove add a test and parallelizing test code to reduce thetotal test suite execution time are two of the ways the implicittest ordering assumption can be broken.
test code that exhibitsthis pattern can fail when it should not because an implicitordering assumption was violated.
although less common testcode with this pattern can also miss a failure that is intended tobe detected because a previously executed test helped to maskit.
the recent study by zhang et al.
on the test independentassumption supports the value of this family of patterns .
external dependencies patterns.
this category includes patterns that are associated with unchecked dependencies on theunderlying test execution infrastructure.
as tests target larger pieces of software test developers tend to leverage more ex ternal resources and infrastructure ranging from environmentvariables to a cloud of services.
this reliance on externalresources comes with specific usage requirements that canchange without affecting the sut but can impact the test andintroduce implicit assumptions that may not always hold.
thistype of pattern can lead to tests that fail intermittently wastingdeveloper s time trying to reproduce and isolate the problem.the recent study by luo et al.
showed that such test faults canbe frequent and consequential .
intra test patterns.
this category of test code patterns identifies test code that may be inappropriately setting or checkingthe sut state to determine whether it behaves as expected.
inthis work we focus on two variants patterns that identify teststhat check too much of the state potentially indicating a lackof understanding of the sut or what the test is intended tocheck and patterns that identify tests that check too little ofthe state.
test code that exhibits an intra test pattern may missfaults because it is not checking enough state or the right partof the program state or may detect differences in the programstate that are not relevant to what the test is validating.
based on our experience and those of others working with similar approaches it seems more effective to miss someproblematic issues in order to increase speed and report fewerfalse positive hereby increasing the chances of applicability inpractice.
with this perspective in mind we made three key de cisions regarding the implementation of the pattern detectors.first the patterns had to be detectable through a lightweightstatic analysis.
all of the patterns shown in the next section canbe identified through the information provided in an abstractsyntax tree ast including the junit annotations.
from theast we make some inferences about control flow data flow and type but as shown in the performance analysis in sectioniii the analyses remains very efficient.
second the analysiswas restricted to the scope of a single test class.
this decisionsometimes limited the precision and recall of our analysis butprovided the performance necessary to analyze thousands oftests in seconds and enabled us to explore how much we cando with the analysis when the sut code is not available.
third the analysis depth could vary across the patterns going deepenough within the bounds of the other two criteria to eitherreduce the number of false positives or obtain more instancesof each pattern.
this inconsistency reflects the tension betweentrying to only detect pattern instances that correspond to testfaults while it quickly.
b. patterns table i provides a summary of the tests patterns that resulted from our design prototype assess refine cycle grouped by their category.
a common building block for all patternsis how we determine what methods to analyze.
we recognizetest methods through their annotations such as test in junit or by the use of the test suffix on the method name a standard test coding convention or by the extension of a classending with testcase junit .
we ignore tests annotated with ignore.
inter test dependencies patterns.
all patterns in this category ignore test classes where the test ordering is en forced through the fixmethodorder ... .
interestingly 150table i correctness patterns for test code.
family pattern description inter test test calls test test reaches other tests and shared a static field.
shared static field a field is written to and read from different tests without resetting it.
shared stream a stream is written to and read from different tests without resetting or checking it.
external timing dependency test invokes hard codes a wait period.
system dependency test uses and changes the underlying system state.
network dependency test assumes a requested network resource is available.
intra test over checked test uses a serialized version of an entire object within assertion like statements.
v ague exception test catches the most general exception instead of specific ones.
under checked test cannot reach assertion like statements.
enough this annotation was used infrequently in all of the projects we analyzed enforcing the notion that the syntactictest ordering is typically assumed in practice.
the first twopatterns in this category are closely related and focus on testdependencies that manifest through shared static fields.
sharingstatic fields is problematic as the order of test execution mayrender different results.
the test calls test pattern identifies a test t i that invokes another test tj either directly t i tj or indirectly through other methods t i ... tj where at least one of the tests writes to a static field potentially affectingthe behavior of the other test.
as defined one can easilyimagine cases where this pattern would render false positives.however we found that even when considering transitivity there were no instances of this pattern in our study.
this wassurprising enough that we kept the pattern as a negative resultworth highlighting.
the second inter test dependencies pattern shared static field also involves two test methods where one test t u updates a static field and another test tr reads the static field becoming dependent on the state set by tu.
although dependencies of this type are possible among more thantwo tests most faults found through this type of pattern areexpected to involve just two tests whose execution ordermay generate different results.
we ignore instances in whichthesetup method or methods annotated with before write to the field as we assume the field is being reset betweentests.
we also ignore the pattern in test code if t ralso writes to the field before reading it.
the third pattern in this category shared stream is similar to the shared static field pattern but differs in that it is associated with dependencies on inputs and outputs.
more specifi cally we look for occurrences of instantiation of objects whoseclass is filereader fileinputstream scanner or randomaccessfile in a test t rthat may indicate an implicit dependence on a test tu that updates the shared resource.
we identify tuby looking for instantiation of objects whose class is filewriter fileoutputstream printwriter or randomaccessfile.
after identify ing the set of tests that either read or write or both to objectsof these types we determine if there is a test t rthat may perform a read operation on an object written by anothertest t uin the same test class.
we ignore tests where the input output object may have been written prior to the readoperation either in the setup or in a method invoked prior tothe read .
external dependencies patterns.
the first test code pattern in this category is the timing dependency pattern.
this type of dependency has been identified as one of the mainsources of flaky tests .
our technique searches test codefor calls to thread.sleep ... a common instance of this pattern other instances include empty loops or calls toother libraries that provide timing features .
we also performa check to ensure this coding pattern does not occur within apredicate that may trigger other actions and checks to handlethe dependency.
test code with this pattern is problematicbecause it can cause tests to fail but not for the reasons thatwere intended if the specified time is not long enough.
tests ex hibiting this pattern may also set the timer too conservatively wasting cycles or causing other conflicts when for example the test timeout n annotation is used.
the system dependency pattern identifies test code that uses or sets parts of the system state that is not checkednor set by the test setup.
this pattern identifies invocationstosystem.
properties orruntime.exec .
both types of calls generate dependencies on external resources either by setting particular properties or resources in theenvironment or by reading environmental variables that mayaffect how the sut state is set or checked.
furthermore someof these invocations are platform dependent and may causefurther instability in the test results if the underlying executionenvironment changes.
the last pattern in this category is the network dependency pattern.
the current implementation of this pattern looks for invocations to getcontent openstream or openconnection onurl objects whose return value is not included as part of an assertion like statement nor arethey checked by a predicate to determine the test flow.
intra test patterns.
the first pattern in this category is the over checked pattern which identifies test code that checks an entire object instead of checking only the relevant partsof the object.
the problem with this code is that a test mayfail when it should not e.g.
when irrelevant parts of theobject are updated by the environment.
the presence of thispattern in test code may indicate confusion on the part of thetester in terms of what is to be tested or may indicate thetester was attempting a shortcut in the test.
this pattern isdetected by identifying calls to tostring on an object in an assertion statement.
we ignore instances of this patternwhen the return value of the call to tostring is further manipulated through calls to methods such as startswith contains endswith andsubstring as we assume the intent in these cases is to check a particular part of theobject.
we also ignore cases where the object on which thetostring is invoked is an instance of a standard library object such as stringbuffer stringbuilder stringwriter andbytearrayoutputstream or any 151wrapper of a primitive type in which case it has different semantics.
we also ignore instances of calls to the tostring method appearing in the message part of an assertion state ment or has multiple parameters that may render differentbehavior.
a variant of the over checked pattern is the v ague exception pattern which occurred frequently enough in our study to merit its own pattern.
this pattern identifies test methods thatintend to throw a specific exception within a try block but capture a more general type of exception in the catch block.
test code exhibiting this pattern is problematic in that it maygenerate false positives when the exception that causes the testto fail is forced by issues other than the one specified in thetry block.
test code that follows this pattern can also make it more difficult to diagnose the fault.
we ignore this pattern intest code if there is a fail statement inside the catch blocks or if there is an assert statement that checks the variable related to the exception as this may indicate an alternativehandling of the exception.
the under checked pattern identifies tests that may not check enough of the expected sut behavior and as a resultmay fail to reveal a fault.
more specifically this patternidentifies tests that lack any implicit checking of the actualresults with the expected results.
although tools like pmdidentify test methods without an assertion our analysis is moreextensive.
first we identify various flavors of assert statementsincluding those containing keywords that are often associatedwith checking the behavior of the sut such as check validate verify this last one is used extensively by a common mocking framework .
second since assertion like statements canbe placed in shared helper methods this pattern performs atransitive analysis checking every method invoked by a testthat is within the same test class.
third to reduce the numberof false positives our approach ignores tests that call onlyconstructors since those tests often do not contain any type ofchecking.
iii.
s tudy the goal of our study is three fold explore the extent to which the test fault patterns we have identified are presentin test code found in existing java projects determine thecost of the analysis in terms of run time and learn whetherthe code detected with our patterns represents true faults.
toachieve these goals we implemented our test code patterndetector using pmd version .
.
.
our implementationand data are available at waterloo ase2015.
a. artifacts the artifacts analyzed in our study are listed in table ii.
they correspond to java projects with test suites implementedin junit.
the first two artifacts joda time and jetty are twopopular systems with many unit tests that we were familiarwith and that helped us to evolve the patterns as we initiallydeveloped them.
the rest of the artifacts were obtained fromthe list of projects analyzed by luo et al.
.
from theirlist we picked the first implemented in java.
the artifactsanalyzed include test classes and test methods.for each artifact table ii includes the version for apachegit repository for apache svn repository for github x for eclipse git repository metrics about the size of thetable ii test code artifacts.
tc test classes tm testmethods t 1a tests w at least one assert not transitive am assert calls sm setup methods tdm tear downmethods ee expected exceptions sf static fields.
artifact tc tm t 1a am sm tdm ee sf joda 986d905 jetty x a71cc69 hbase 9b7f36b hadoop 85aec75 derby 314a19a lucene tomcat .
023fd55 servicemix ead7e67 zookeeper 6ebd23b cxf 318501b flume 199684b maven 32053c9 b182c5b test suites number of statements that perform a check on theprogram state and the number of methods that setup and teardown program states.
the test code artifacts vary in the densityof assert statements per test make different use of setup andtear down methods and in general have different structuralattributes e.g.
derby and hbase seem to use static fields twiceas often as other projects .
all artifacts use junit .
exceptfor joda time which uses junit .
.
b. results extent of pattern instances.
table iii provides a count of the pattern instances detected by our tool across artifacts.
only one of the ten artifacts servicemix which is the smallest artifact presented no patterns.
the rest of the artifacts have between67 and instances of the patterns somewhat proportionalto the number of test classes and methods but with variabilityin terms of what patterns are most dominant across artifacts.
inter test dependencies patterns do not occur as often in practice as we and others had expected .
in fact only twoprojects hbase and hadoop had one and five instances ofthe shared static field pattern respectively.
we note that the density of static fields per test class is the highest for hbase .
but average for hadoop .
so this structural featurealone does not seem to indicate this pattern.
the scarcity ofthis pattern is interesting as it provides further context on thevalue proposition of techniques for detecting and removing testdependencies.
as noted previously the test calls test pattern was not found in any of the artifacts evaluated.
external dependencies patterns were found much more frequently.
in particular we note that simple patterns can beused to detect the timing dependency pattern identified in recent work on test flakiness .
this pattern was presentin of projects and on average in of the test classeswe analyzed.
instances of the network dependency pattern were less frequent but were detected in seven of the projects.instances of the system dependency pattern were the most rare in this category of patterns.
instances of the intra test patterns were detected most frequently across the majority of the artifacts occurring hundredsof times.
instances of the v ague exception and under checked patterns were found across all projects except service mix andinstances of the over checked pattern were found in all but service mix and maven.
152table iii pattern instances found cross artifacts and results of selective analysis of these instances.
in italics true positives false positives uncertain cases .
artifacts in bold were selected for analysis due to their size.
artifact pattern combinations with were selected for analysis because they were outliers.
joda jetty hbase hadoop derby lucene tomcat servicemix zookeeper cxf flume maven total test calls test shared static field shared stream timing dependency system dependency network dependency over checked v ague exception under checked total cost of analysis.
when performed on a macbookpro .
laptop with an intel core i5 .
ghz processor and 8gb ofmemory the detection of each pattern on the largest artifact hadoop takes approximately seconds.
the detection ofall patterns together on the same artifact only takes to10 seconds more than any individual pattern as most of theanalysis time is used to parse the code in the project and buildthe ast for those files containing test classes.
extent of true faults.
we now discuss the results of our analysis to determine if the patterns detected constitute real test faults.
we began this process by selecting the five artifactswith the largest number of test classes hadoop lucene cxf maven and hbase .
we then randomly chose at most fiveinstances of each detected pattern instance for each of theselected artifacts.
table iii summarizes the findings.
cells withvalues in italics show three values enclosed by parentheses true positives false positives uncertain cases next to the number of pattern instances found.
the first value true positives corresponds to pattern instances that we deemed tobe real faults.
the second value false positives identifies the number of pattern instance that do not represent a fault in thetest code.
the third value uncertain cases is the number of pattern instances that we could not classify either way with anycertainty given our understanding of the test and sut code.
the shared static field pattern was found in only two projects hbase and hadoop.
four out of six instances are falsepositives caused by test code that invokes methods outsidethe scope of the test and hence outside the scope of theanalysis.
extending the analysis to include the applicationcode could remove such false positives but given the smallnumber of occurrences of this pattern and the cost of suchan analysis this may not be cost effective.
the two uncertaincases appear to be faults as per the fixture design but wewere not confident enough to make this determination withouta better understanding of the application code.
for the timing dependency pattern all projects reported true faults.
this confirms the value of detecting this particularlypervasive pattern.
four of the system dependency pattern instances we analyzed for hadoop were true positives andone was a false positive caused by a helper method that reliedon an operating system call but was not reachable by any ofthe test methods.
a deeper reachability analysis would be ableto remove this false positive.
the network dependency pattern appeared in all five projects but with low frequency.
onlyone of the instances is a true positive the one in lucene .when analyzed further we discovered that the false positivesand uncertain cases are caused when there are indirect checkson the returned content.
in some cases the returned object istransformed into another object i.e.
a string and checks areperformed over this object.
a more sophisticated analysis overa scope that extends beyond the test code would be necessaryto remove such false positives.
instances of the over checked pattern appeared in hadoop lucene and cxf with mixed results.
a total of seven truepositives five false positives and three uncertain cases werereported.
the false positives occur for two reasons.
first thesut builds heavily on a string representation so it is naturalfor tests to perform calls to the tostring method to check the object state.
second the sut offers no alternative methodsto check its state.
extending the analysis to the sut could helpremove some of these false positives.
the v ague exception pattern instances in hbase hadoop lucene cfx and maven correspond to true positives fourfalse positives and seven uncertain cases.
the false positivesare caused when the exception is analyzed in ways morecomplex than the pattern can recognize.
among the uncertaincases we found it common for catch blocks to print the stack trace to standard output for the developer to later analyze.
thiswill not scale well but it may work for smaller scopes so weleft it marked as uncertain.
a simple additional analysis couldremove such doubtful cases.
the under checked pattern resulted in true positives and false positives of which are in cxf .
cxf wasparticularly challenging because it uses a test helper classthat is outside the scope of analysis to aggregate assertionmethods but does not follow the standard naming conventionwe assumed in our pattern implementation.
in addition to analyzing the five largest artifacts we also inspected the results of the analysis for four cells in tableiii that reported more than pattern instances for the otherprojects.
these cells are marked with in the table.
fifteenpattern instances were true positives while the five in joda time were uncertain.
these uncertain patterns were causedby the state of datetime objects which are often checkedby invoking the tostring method.
clearly not all of the datetime object are changed by the invocations made so theassertions are over checking the program state.
yet sincedatetime objects rarely change the risks of reporting a false 153failure seem small.
in any event a small adaptation of the pattern to the project could remove this type of instance.
iv .
l essons learned during the course of our project we considered several patterns that intuitively seemed like good ideas or that wereempirically supported by previous studies but found theydid not perform as we expected on real and large test codebases.
some like the inter test dependencies patterns do not occur in practice as often as we expected based on our studyresults.
in particular the patterns relying on static fields sharedamong tests which was found to be the source of most faultsassociated with the test independence assumption wasquite rare even with very relaxed conditions.
this negativefinding was particularly surprising which is why we retainedthis category of patterns in our study.
in general the wide range of test implementations we encountered was unexpected despite our familiarity withjunit and frequently led to many pattern refinements anddiscussions about their potential impact.
test code involvingtest class hierarchies or external test support libraries andtest code meant to be executed across multiple configurationsseemed to be increasingly pervasive resulting in increased testcomplexity and making test analysis more challenging.
an interesting case we discovered in our study was the occurrence of patterns that sometimes appear in practice butthat were obfuscated by the behavior of the sut.
for example within the intra test pattern category we investigated a pattern that identifies getter methods whose return value could notinfluence an assertion statement.
the key to this pattern isidentifying getter methods.
to do that we employed a simpleheuristic that reflects standard and recommended practices weassumed that methods with a get prefix would be getters.
in practice we found that many sut getter methods have sideeffects.
this caused many false positives and illustrates thelimitations of analyzing only test code however it also illus trates how analyzing test code can help to identify anomaliesin the sut.
e.g.
a mis use of standard coding conventions.
v. c onclusion we found that patterns associated with faults in test code can be detectable using light weight static analyses.
the bigwinner is the timing dependency pattern which was pervasive across projects and was consistently associated with incorrecttests.
the under checked pattern was the most pervasive but with a false positive rate all from a single project .
itcould be improved with simple pattern tailoring to accommo date the non standard naming conventions used by this project.the v ague exception pattern was the third most pervasive pattern has a false positive rate of slightly over but hadmany cases that we could not classify given the complexityof the analysis context.
still its frequency and rate make itenticing to pinpoint tests that may fail when they should notbecause of an assertion that can be easily refactored.
theover checked pattern was also common but with a false positive rate of over is not one to be adopted in practicewithout analyzing the sut.
this is interesting as it serves toillustrate the limits of what we can learn just within the testscope.
the rest of the patterns including all of the patterns intheinder test dependencies category may be refinedthrough more sophisticated and extended scope analyses butthey do not occur often enough to justify such refinement.the next steps in this exploration will include richer patternsto analyze tests generated by different techniques and tools while relaxing the scope of analysis.
vi.
a cknowledgments this work was supported in part by a google faculty research award and a national science foundation award shf and ccf .
r eferences coverity.
fxcop.
.aspx.
intellij.
junit.
mockito.
pmd.
g. bavota a. qusef r. oliveto a. de lucia and d. binkley.
an empirical analysis of the distribution of unit test smells and their impact on software maintenance.
in icsm pages sept .
g. bavota a. qusef r. oliveto a. de lucia and d. binkley.
are test smells really harmful?
an empirical study.
empirical software engineering pages .
a. bessey k. block b. chelf a. chou b. fulton s. hallem c. henrigros a. kamsky s. mcpeak and d. engler.
a few billion lines ofcode later using static analysis to find bugs in the real world.
commun.
acm feb. .
s. g. elbaum g. rothermel and j. penix.
techniques for improving regression testing in continuous integration development environments.infse pages .
m. greiler a. van deursen and m. a. storey.
automated detection of test fixture strategies and smells.
in icst pages .
b. hauptmann m. junker s. eder l. heinemann r. v aas and p .
braun.
hunting for smells in natural language tests.
in icse pages may .
d. hovemeyer and w. pugh.
finding bugs is easy.
sigplan not.
dec. .
c. huo and j. clause.
improving oracle quality by detecting brittle assertions and unused inputs in tests.
in fse pages .
y .
jia and m. harman.
an analysis and survey of the development of mutation testing.
ieee transactions of software engineering .
q. luo f. hariri l. eloussi and d. marinov.
an empirical analysis of flaky tests.
in fse fse pages .
g. meszaros.
xunit test patterns refactoring test code.
addisonwesley .
h. neukirchen and m. bisanz.
utilising code smells to detect quality problems in ttcn test suites.
in a. petrenko m. v eanes j. tretmans and w. grieskamp editors testing of software and communicating systems volume of lecture notes in computer science pages .
springer berlin heidelberg .
a. van deursen l. moonen a. van den bergh and g. kok.
refactoring test code.
technical report amsterdam .
b. v an rompaey b. du bois s. demeyer and m. rieger.
on the detection of test smells a metrics based approach for general fixtureand eager test.
ieee trans.
softw.
eng.
dec. .
s. zhang d. jalali j. wuttke k. mus lu w. lam m. d. ernst and d. notkin.
empirically revisiting the test independence assumption.
inissta pages .