mining performance specifications marc br nink national university of singapore nus graduate school for integrative sciences and engineering school of computing singapore singapore marc comp.nus.edu.sgdavid s. rosenblum national university of singapore school of computing singapore singapore david comp.nus.edu.sg abstract functional testing is widespread and supported by a multitude of tools including tools to mine functional specifications.
in contrast non functional attributes like performance are often less well understood and tested.
while many profiling tools are available to gather raw performance data interpreting this raw data requires expert knowledge and a thorough understanding of the underlying software and hardware infrastructure.
in this work we present an approach that mines performance specifications from running systems autonomously.
the tool creates performance models during runtime.
the mined models are analyzed further to create compact and comprehensive performance assertions.
the resulting assertions can be used as an evidence based performance specification for performance regression testing performance monitoring orasafoundationformoreformalperformancespecifications.
ccs concepts general and reference performance software and its engineering software performance software evolution requirements analysis keywords specification mining performance modeling regression testing .
introduction previous work on performance focuses often on high performancecomputing .
typicallyhigh performancecomputing incurs high execution costs.
this justifies significant investments in performance optimizations and detailed performance modeling .
other scenarios require up front performance specifications too.
for instance if latency is a key consideration.
in contrast in many other applications performance is less mission critical.
in these scenarios it ishard to justify the heavy investment required to explicitly specify and verify a performance model.
we target applications in which performance is not a primary concern but an important secondary objective.
many consumer grade applications fall into this area e.g.
word processors web browsers .
we mine performance models during runtime automatically and autonomously.
our goal is to mine performance specifications of deployed applications in the field.
the mined models specify the expected performance behavior and can be used to monitor deployed systems and detect performance deviations.
insights gained by inspecting the gathered models might also be used to verify previously specified performance requirements and augment traditional approaches towards elicitation of performance specifications.
in this paper we make the following contributions we present a tool that extracts performance specifications during runtime.
these specifications come in the form of performance models.
the models are an evidence basedperformancespecificationthatdescribe the expected behavior of a system.
we automatically detect interesting key features in the models and analyze their root causes.
the root causes are captured in performance assertions.
assertions are small and easy to understand.
we use dynamic binary instrumentation to enable adaptation of data collection during runtime.
the adaptation operates autonomously and collects data incrementally.
it does not require human guidance.
wepresentadataanalysisapproachthatextractscomprehensive performance models.
we show that mined models are reliable and accurate.
the models captureperformanceinrelativeterms notabsolutevalues.
this increases platform independence and is crucial if we want to mine performance specifications collaboratively across platforms.
in our evaluation we show that the tool mines performance models autonomously.
mined models are reproducible and consistent between multiple executions and across platform boundaries.
the paper is structured as follows.
in section we give a brief high level overview of the system.
specific aspects are discussed in detail in section .
we evaluate our approach in section .
we review related work in section .
finally we examine possible future work and conclude the paper in section .permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
fse november seattle wa usa c circlecopyrt2016 acm.
... .
39proc mutex sysv acquire .
.
.
.
.
.
.
.
.
.
.
.
e1 proc mutex sysv releasestartup childrenap mpm runmain semope2behavior behavior behavior selected edges figure a mined performance model for the semop function.
di fferent colored edges denote different performance behavior.
.
overview thorough performance testing helps to increase the quality of software and reduce the number of in field defects.
however we observe that performance testing sometimes fail to get the required attention.
we believe this is due to two factors cost and time.
in this work we introduce an automatic performance specification mining approach that can operate on deployed software and helps to reduce costs by extracting performance models from deployed systems autonomously without human guidance.
figure shows a performance model mined by our approach from an execution of apache .
.
.
depending on the calling context the semop functions exhibits di fferent performance behaviors.
if the call path to semop includes the edge e1 then we expect the semop function to follow performance behavior .
if the call path includes edge e2 we expect behavior .
finally if none of the edges is included we expect behavior .
the key insight is that behavior is very di fferent from and .
behavior postulates that the execution of semop will be faster than 70ms in of the cases compared to .
for behavior and .
for .
it is easy to understand why there is a performance di fference behavior reflects the path that releases a mutex whereas behavior and are on paths that acquire one.
the model was collected autonomously and illustrates the capabilities of our approach.
we discuss this model in greater detail in our evaluation in section .
.
mining such a performance specification is not easy.
because software is complex it is possible to collect many different types of data at many di fferent locations.
collecting all the data at all locations in parallel would result in a high performance overhead probably rendering the system unusable.
to tackle this challenge our tool explores the monitored system incrementally.
it starts with a minimal amount of data collection and uses the collected data to steer the mining process to promising targets progressively accumulating an expressive body of knowledge.
figure provides a high level overview of our system.
a softwaresystemisdeployedacrossmultiplesites.
tosupport ongoing software engineering tasks e.g.
software evolution storage server mining controlleranalyseranalyseranalysis controldatadeployed systemstubdeployed systemstub figure overview of the approach.
start measure timedetect hot function analyze timing behavior has stable behavior has unique classesyes measure time and collect call stack extract call path patterns store call path patternsyesno.
of samples thresholdno yesno detect callerno figure incremental data collection flowchart.
we want to mine in field performance specifications.
the deployed and running systems are registered with a local stub controller that connects to a remote mining controller.
the mining controller controls which information is collected in each deployed system.
once registered the deployed systems start to generate and aggregate data which is sent to a storage backend.
one or more analysis processes retrieve the collected data correlate it and mine performance models and assertions.
during the analysis the tool also judges about the stability and quality of the data and recommends next steps to the mining controller.
the mining controller reviews the recommendations and selects a subset.
next it sends instrumentation requests to the stub controller of the deployed systems leading to an adaptation of the information collected at each deployed system.
this design is similar to the one presented in .
.
mining and analysing data figure presents a flowchart illustrating how the tool decides which data to collect where.
assuming that the user did not provide hints on where to collect which kind of data via command line options collection starts by detecting hot code functions.
these are functions that are executed frequently.
the tool simply samples the instruction 40pointer at a ms interval.
in the absence of user supplied hints measuringfrequentlyexecutedfunctionsisreasonable stakeholders are typically more interested in the execution characteristics of frequently executed functions.
once the system detects a hot function it starts to monitor the function by accumulating timing data.
the tool aggregates individual timings into histograms that are sampled as observations.
after data is collected it is analyzed.
as a first step the system detects whether it collected a su fficient amount of data.
to this end we partition the data into multiple clusters section .
and compare the partitioning across time cf.
section .
.
if the partitioning does not change we call it stable.
if the timing behavior is not stable the system obtains more data until a threshold is met.
the default threshold is set at data points.
it can be changed via a command line option.
however in our experience data points are sufficient to detect stable key features if they exist.
if the system finds a stable partitioning then this indicates that the function shows distinguishable timing behavior.
as a next step the system continues to collect not only the time but also the call stack.
the goal of collecting the call stack is to explain why the di fferent clusters exist.
the tool uses the gathered call stacks to extract call path patterns and stores them for later reference cf.
section .
and .
.
finally afterthesystemcollectedtimingbehaviorandcall path patterns where appropriate there is nothing else to do for this specific function at the moment.1the tool continues by detecting call sites of the function.
detected callers are fed back to the system and used as new starting points.
in this way the system propagates data collection up the call tree and incrementally covers the monitored application.
.
data partitioning to partition the data we tried gaussian mixture models and peak detection with limited success.
only a few of our collected distributions fit a gaussian mixture model well.
peak detection works well with exponential distributions but the resulting partitions tend to be fragile.
we settled on a di fferent third approach.
the tool extracts the local minima of the gradient of the cumulative distribution function.
the gradient is low at locations with a low density of observed values.
the system partitions the data into multiple disjoint clusters along those areas of low density.
since each location partitions the data into two clusters we call these locations cluster borders .
sometimes our tool finds multiple cluster borders local minima in close proximity.
these neighboring borders do not represent multiple useful partitions.
instead they are artifacts caused by the limitations of the collected data.
in particular the data is discrete has limited precision and accuracy and is finite.
to accommodate this the tool aggregates detected borders that are close to each other.
it does not match borders directly.
small fluctuations in the collected data might have a big impact on the precise location of the detected borders.
instead the tool matches two borders if their mapped cumulative distribution function cdf values are close.
for example consider figure .
let us assume the tool detected two local minima x1andx2.
the distance between the 1the system validates the results periodically to make sure that they did not change.
execution timecdf1 x1x2 a1b1 a2b20 0figure two positions on the x axis x1andx2 might be significantly di fferent.
however the di fference vanishes is we map them to their corresponding cdf values f x1 andf x2 .
locations of these two minima might be large.
however their cdf values are very close.
because the cdf values do not di ffer much the number of elements between the two is small.
these few elements are the only data points that will change cluster membership depending on whether the tool partitions at x1or atx2.
because the di fference in the resulting labeling is small both minima partition the data virtually in the same way.
our tool considers both borders as representatives of the same key feature if the di fference between their cdf values is less than .
definition .letfbe a cumulative distribution function.
two positions x1andx2on the x axis match with respect to fiff x1 andf x2 differ by less than in absolute terms.
match x1 x2 f def f x1 f x2 .
the threshold value of can be set by the user.
the value depends on the noise in the data as well as the preferred level of detail.
we established as a good threshold value empirically.
besides accommodating noise it prevents the detection of very small clusters by aggregating them into largerones.
thisreducesthecomplexityoftheresultingpartitioning and led to good results in our evaluation studies.
after borders are matched the tool picks one to represent the whole set.
most often the set only contains a single element and choosing one is trivial.
in all other cases it chooses the leftmost one.
since all matched borders are close neighbors the di fference between them is small and the method of selection is non critical.
.
utility score data collection is separated into whatto collect and where to collect it.
the decision is based on a utility score that quantifies how useful it would be to collect a specific type of data at a specific location.
we use the utility score to decide which type of data to collect next.
we only collect data for the functions with the highest utility score.
the utility score should have three properties .
if new data is di fferent from previously collected data collecting it did add information and potentially increased our understanding of the system.
the utility score should be high.
.
if collected data stays the same over an extended period of time we expect it to stay the same in the near future as well.
collecting this data will not add much value and the tool could use the available resources more productively.
for example it could collect a different type of data instead.
thus if the data stays the same for some time then the utility of collecting this type of data at this specific location is low the utility score should reflect this and should be low as well.
.
as the monitored system continues to be used observedbehaviormightchangeduetoshiftingworkload evolving complexity of state or a multitude of other reasons.
thus collected data gets stale after some time its value decreases over time.
we have to recollect it regularly to confirm that it is still accurate.
consequently the expected utility of re collecting the data increases over time.
thus the utility score should increase over time too.
from these three properties it is apparent that we have to be able to judge whether the data is the same over a period of time.
to this end we separate the stream of observed data into discrete observations.
definition .an observation ois a tuple ts te d withtsbeing the time collection of this observation started tethe time collection stopped and dthe raw data collected in the time frame ts te .
identifies the type of the collected data for example whether it is timing data or call stack data.
then start o returns the start time tsofo a n d end o returns the end time teofo.
definition .leto be the set of all observations collected so far that are of type .
we can define an order relation using the start time of the observations.
o1 o2def start o1 s t a r t o2 since no two observations in o have the same start time o is a totally ordered set.
definition .leto be a totally ordered set of observations.
then last o emits the newest observation in o .
o1 last o o1 o o2 o o1 start o1 s t a r t o2 prev o returns the observation immediately preceding o. o1 prev o2 o1 o2 o start o1 s t a r t o2 o3 o start o1 s t a r t o3 s t a r t o2 to determine whether to collect data of a certain type at a specific location the tool calculates the utility score.
it compares newly gathered observations with all the other observations in o .
our tool leverages the partitioning approach introduced in section .
.
it compares detected borders in new observations with borders extracted from previously collected observations.
if the borders match then all observations support the same partitioning.
definition .a set of borders b1matches another set of bordersb2with respect to a cumulative distribution functionfif we can match every border in b1with at least one border in b2.
matchbordersets b1 b2 f b1 b1 b2 b2 match b1 b2 f function findstableborders o f cdf merge o b bcdf for allo odo for allb getborders o do bcdf bcdf f b borderclusters meanshift bcdf for allc borderclusters do if c o then b b centroid c returnb algorithm calculate a set of common borders for the observations in o. function getfirststable o b f lastmatch undefined o last o whilematchbordersets getborders o b f do lastmatch o o prev o returnlastmatch algorithm find the first observations that matches bwith all subsequent observations also matching b. the tool does not directly compare new observations with old ones.
instead it extracts a set of borders that can be found in the vast majority of previous observations.
the reason for this extra step is simple while most borders can be detected reliably in all observations a few are more fragile.
sometimes a rather small di fference in the collected data causes our data classification to detect a border in one observation but not in the other.
algorithm presents how our tool extracts stable borders.
first the algorithm merges all input observations o and obtains the corresponding cumulative distribution function cdf line .
next it iterates over all observations ino.getborders implements the data partitioning approach described in section .
.
it maps the detected borders to their respective cdf value and accumulates them inbcdf lines to .
next it applies the common mean shift algorithm .
the mean shift algorithm returns a set of sets each containing borders that belong to the same cluster.
the algorithm filters clusters that have less than a threshold of o elements2and aggregate the centroids of the remaining clusters in b lines to .
finally it returnsb.
the tool compares new incoming observations to this set of stable borders b. algorithm iterates over all observationsstartingwiththenewest i.e.
last observation last o .
it stops as soon as it finds a non matching observation.
it returns the last observation that matched thus it returns the earliest observation in a chain of matching observations.
using algorithm and algorithm the tool calculates the utility score in algorithm .
first it determines stable borders for the last observations using algorithm line and .
next it finds the first observation ofirst 2in our evaluation we use a threshold value of .
.
function utilityscore o olast10 last10observations o b findstableborders olast10 .
.algo.
f cdf olast10 ofirst getfirststable o b f .algo.
tstart start ofirst tend end last o u current time tend tend tstart ifu 1then return returnu algorithm calculate the utility score.
that fits the extracted stable borders under the condition that all newer observations ot ofirstalso fit using algorithm in line .
finally with tstart start ofirst and tend end last o the utility score is calculated as u min current time tend tend tstart note that the utility score fulfills our requirements and is low if observations are stable for a longer time high if new observations are significantly di fferent from previous ones and increases over time so that data is periodically re verified.
.
capturing performance models if the tool detects multiple performance classes for a specificfunctionusingourpartitioningapproachfromsection3.
then it attempts to explain whythere are di fferent classes compare the flowchart in figure introduced in section .
to locate the cause for the existing clusters the tool has to collect additional data.
at the moment it collects the call paths that lead to a specific function.
the assumption is that the calling context is an important factor that might contain crucial information to help discriminate between different performance behaviors.
alternatively one might also collect di fferent data for example function arguments.
arguments are likely to influence the executing time of the called function.
however arguments of functions su ffer from the deficit that they require interpretation.
except for trivial arguments interpretationrequiresin depthunderstandingoftheirstructureand contained data.
this calls for either manual annotation or elaborate static or dynamic analysis.
in contrast call paths are easy to capture and easy to interpret.
furthermore in our experience they are su fficient to gain significant insights into system behavior.
if our approach finds interesting features in the timing behaviorofaspecificfunction i.e.
itfindsatleastonestable cluster border then it automatically augments the data collection to extract not only the execution time but also call path information.
using the call path information the tool examines whether the different classes can be explained by di fferences in the call paths traversed to reach the function.
is there a specific set of call paths that show similar performance behavior compared to all other call paths?
if yes we found a performance pattern that occurs only if the function is reached via a specific set of call paths.
findingthesesetsofcallpathsisatwostepprocess.
first our system analyzes the aggregated data of a specific func tionfand detects stable cluster borders.
we described this step previously in section .
.
algorithm returns a set of stable borders.
second it uses the detected cluster borders to classify observed execution data and group call paths that behave similar.
the borders partition the data into multiple stable clusters c. definition .lettpbe the set of execution times observed while reaching the function via call path p.l e tcbe a set of stable clusters.
we assign each t tpto a cluster that fits best using fit t .
next we count for each cluster c chow many of the observed execution times t tpare a member of this cluster.
count c def t t tp fit t c we normalize the result and obtain the normalized cluster distribution vector bpfor call path p. bp count c1 count c2 ... count cn tp thei th element in bprepresents the percentage the executions times for path pfall into cluster ci c. to decide whether two call paths p1andp2have similar behavior the tool groups call paths using their normalized cluster distribution vector bp.
it clusters these points using kmeans clustering.
on important aspect of using kmeans is to choose k the number of clusters.
we start with k and increase kas long as the adjusted mutual information score increases.
kmeans groups the call paths into disjoint sets g1 g2 ... g n. call paths that end up in the same group have similar bp.
during runtime they are expected to behave similarly.
definition .a performance model mis a tuple f g b with f being a function in the binary of an application.
let gbeagroupingofcallpaths g g1 g2 ... g n ba set of normalized cluster distribution vectors b bg1 bg2 ... bgn and a bijective mapping g b. thus aperformancemodelconsistsofasetofgroupedcall paths.
the behavior of each call path group is described by one cluster distribution vector.
to denote the performance model for function f we write mffor short.
whenever we need to distinguish models collected in di fferent executions we write mn fto identify the modelmfcollected in the n th execution.
we compare multiple performance models in our evaluation to reason about reliability and platform independence of mined performance models.
it is not di fficult to compare two performance models.
a di fference occurs when two call pathp1andp2are members of the same group in one model while they belong to two di fferent groups in the other model.
if we detect such a di fference between the models than one model postulates that both call paths should behave the same while the other model postulates that they should behave differently.
it is noteworthy that a performance model itself does not containanydirectnotionofexecutiontime.
executiontimes areexpectedtobeverydi fferentacrossplatforms.
instead it groups call paths by behavior.
call paths in the same group are expected to behave similarly independent of concrete values of execution times.
abstracting the execution times and only retaining their relationships is crucial to make a performance model portable across platforms.
.
extracting performance assertions performance models tend to be large and contain a lot of information.
except for very small models the complexity of the models directly impacts readability.
complex models are difficult to understand.
to simplify a performance model mf our tool generalizes it.
it searches for a minimal set of call edges that is su fficient to distinguish all call path groups from each other.
our approach uses a common a algorithm to find these edges.
as input we supply the call path groups g1 g2 ... gn .
for each input path group githe algorithm returns a boolean expression ei.
thus as output we obtain a vector of boolean expressions e1 e2 ... e n .
the atoms of the boolean expression represent call edges.
the tool uses the total number of call edges as a cost function.
thus the returned expressions are guaranteed to use a minimal number of call edges to distinguish between the call path groups.
abooleanexpressionscanbeinstantiatedwithacallpath.
toinstantiateanexpressionwithacallpath p wesubstitute an atom with trueif and only if the corresponding call edge is present in p. we write ei p to denote an expression ei that was instantiated with call path p. the boolean expressions filter call paths.
if and only if a previously observed call path is a member of the path group gi then the boolean expression evaluates to true.
thus for all observed call paths pwithp sn i 0githe resulting expressions satisfy the two constraints ei p p gi ei p p p gi the expressions are a building block for compact performance assertions.
definition .a performance assertion is a tuple f e1 bg1 e2 bg2 ... en bgn with f being a function in the binary of an application.
let eibe a boolean expression that identifies all paths in cluster gi andbgithe aggregated normalized cluster distribution vector of gi.
for example assume the tool observed that function zis reached via three di fferent call paths p1 p2 andp3 .
g1 g2 p1 a b c z p2 a b z p3 a c z a b c zare functions in the monitored application.
a b means that function acalls function b. furthermore assume function zshows different performance behavior during runtime depending on the context of the call.
for example z always executes very fast if called via p1.
however if zis called via p2orp3it takes a bit longer to complete in of the cases.
ourtoolwilldetectthedi fferenceinperformancebehavior and group the three call paths into two groups g1andg2 using the mechanisms described in section .
.
next the system will use the a algorithm to extract the minimal set of edges that are su fficient to distinguish all call paths in g1 from all call paths in g2.
the minimal solution is e1 e ande2 ewitherepresenting the edge b c. the path groupsg1andg2can be distinguished by the single edge e. all call paths in g1contain the edge b cwhile all paths ing2do not contain it.
using these two expressions the tool will create the following performance assertion a1 a1 z b c .
.
b c .
.
this performance assertion a1captures the essence of the collected data.
if function zis called via a call path that contains edge b c then we expect all calls to behave the same and execute fast.
however if edge b cis not present in the call path only of the calls should execute fast while the remaining are expected to behave di fferently they should execute slower.
.
ev aluation our evaluation is designed to answer the following research questions rq1 can we mine performance models reliably?
rq2 aretheseperformancemodelsplatformindependent?
rq3 how difficult is it to understand the resulting performance models and assertions?
rq4 howhighistheoverheadincurredbydatacollection?
rq5 does data collection perturb the system behavior?
weevaluateourapproachonthreesubjects apache2.
.
mysql .
.
and okular .
.
.
these subjects cover different aspects of the design space.
mysql .
.
is multithreaded apache httpd .
.
uses multi processing and okular .
.
is a gui application.
to answer rq1 rq3 we run apache and expose it to the banking workload of the spec web2009 benchmark version .
.3we run a single spec web2009 client on the same machine as apache to simplify the setup.
we reduced the mean wait time between requests in spec web2009 to speed up execution of the benchmark.
we run our experiments on two di fferent machines.
machine a is equipped with a intel core i7 processor with .
ghz and gb of ram.
machine b is equipped with two intel xeon e52680 processor with .
ghz and gb of ram.
.
rq1 reliability to show that our approach mines performance patterns reliably we run the experiment five times each on two different machines.
then we compare the models mined on the same machine with each other.
in this way we show that we can mine coherent models in fixed environments.
each execution of the experiments creates a set of performance models mi mi f1 mi f2 ... mi fn for functions f1 f2 ... fn.
figure summarizes our findings for all ten measurements using box plots.
to create a graph we compare collected performance models.
we compare models using the mechanism introduced in section .
that is we check whether two models group the call paths in the same way.
each data point represents the statistics of a performance model mfof a specific function f. on the x axis we show four entries to .
each data point in entry is created by comparing the model m1 ffor a single function fobtained during the first execution with the corresponding model m2 fcollected during the second execution i.e.
cmp m1 f m2 f .
to create the data points for entry we merge the two models and compare it with the model gathered during the third execution cmp merge m1 f m2 f m3 f .
similarly for data points a machine a. b machine b. c combined performance models of machine a and b. figure percentage of call paths left and data points right that show consistent behavior across multiple runs for two machines.
we use tukey whiskers.
in entry we again merge all previous models and compare it with the current one cmp merge merge m1 f m2 f m3 f m4 f .
entry is created accordingly.
intheleftgraphinfigure5aweplotthepercentageofcall paths that show consistent behavior for each model.
each data points represents one model mffor function f.w ec a n see that most call paths for most models match reasonably well.
most call paths show consistent performance behavior across executions of the experiment.
however we also note that for some models we can only match a relatively low number of call paths down to less than .
we investigated further and found that call paths that could be matched tend to have a higher number of supporting observations compared to call paths that show inconsistencies during merging.
since call paths with a small number of supporting observations are over represented in the set of call paths that cannot be matched we present another graph in which we weight the call paths with the number of supporting observations right hand graphs in figure .
each data point is a performance model for a function.
for each performance modelmfwe plot the percentage of observations that support call paths with consistent behavior.
for example a valueof90 meanswecanalign90 oftheobservationscollected at a specific function facross multiple experiments.even after merging all five executions into a single model we can still match more than of the data points collected at machine a for most models figure 5a right .
in fact the box plots are hardly noticeable because the tool can align of the observations for many models.
there are two outliers at about for which our analysis fails to create reliable performance models.
we tested whether problems during the merging of behavioral clusters could be caused by complex models.
we were unable to find a correlation.
we conclude that the performance behavior of these two functions is just too erratic to allow merging.
infigure5bwepresentequivalentmeasurementscollected on machine b. the results are even slightly better than on machine a. in summary we succeed in collecting reliable performance models on both machines.
answer to rq1 most performance models can be reliably aligned across multiple executions.
.
rq2 platform independence after showing that performance models can be reliably created and matched across multiple executions on the same machine weproceedtoshowthatmodelscanalsobemerged across machines.
this illustrates the ability to mine stable performance models across di fferent hardware platforms.
we align the models collected on machine a with models created on machine b. note that the two machines not only have different hardware configurations but also run slightly different binaries even though we run the same software we compiled the binary on each machine thus the binaries are different.
especially some functions present in the version on machine b have been inlined in the version on machine a. while this creates some di fficulties during merging a reliable approach should handle these challenges.
toshowwhetherperformancemodelscanbealignedacross platforms we merge the models collected on machine a with the corresponding models collected on machine b. we merge the models at position nin figure 5a with the corresponding models at the same position in figure 5b.
the resulting statistics for the merged models is depicted in figure 5c.
for most functions the performance models can be merged across di fferent executions on di fferent machines.
answer to rq2 performance models collected on different platforms and with di fferent binaries show high stability and can be matched successfully to a large extent.
.
rq3 complexity mined performance models tend to be complex.
we already presented a small example of a performance model extracted from apache .
.
in figure .4other models tend to be much larger.
the size of the performance models motivated our approach to reduced them to their key components cf.
section .
.
our tool found the model depicted in figure in the following way.
the approach detects semop as a frequently executed function it starts to measure the execution time.
after collecting some data the tool analyses it and finds one stable border at ms. this border separates the observed data into two distinct behavioral clusters.
instead of re4we chose to show the model for the semop function mainly because the model is small enough to fit into the manuscript.
a larger model for polland apache .
.
is available at d 0b9fegnllmm1atw1xdkhlrehizda 45table the performance assertion extracted from the model of semop cf.
figure .
behavioral clusters behavior expr.
t 70mst 70ms e1 e241.
.
e1 e235.
.
e1 e299 figure the number of atomic formulae necessary to distinguish between clusters depends on the number of clusters.
porting the whole performance model and hoping for proper interpretation by the stakeholders the system extracts distinguishing features that discriminate di fferent performance behavior section .
.
after finding the stable border at 70msour tool continues and collects the execution times as well as the call stack.
analyzing the gathered call stacks the tool detects three distinct performance behaviors.
using the a algorithm it finds two edges that are su fficient to distinguish between the three performance behaviours.
our algorithm chooses edges e1ande2in figure .
the resulting performance assertion annotated with concrete time values is shown in table .
performance behavior is the most distinguished one.
whenever we enter the semop function and we neither passed edge e1nor edge e2 that is we reached semop from proc mutex sys release semop returns after less than 70msin of the cases.
the other two performance behaviors and are very di fferent from .
for behavior .
of the executions take longer than ms. behavior is similar .
of the executions take longer than 70ms.
both pass through proc mutex acquire .b e haviors and are similar but the di fference is significant across multiple executions.
using only two unique atoms edges e1ande2 we can reduce the performance model of semop with three di fferent behavioral clusters to three simple boolean expressions.
these expressions are shown in the second column of table .
we believe that these expressions can help stakeholders to understand performance di fferences.
investigating performance behavior by scrutinizing the complete call tree can be a daunting task especially for larger trees.
instead a stakeholder can inspect the boolean expressions.
these expressions quickly point to the important di fferences between call paths that result in di fferent performance behavior.
how difficult it is to understand the mined performance models and assertions?
to answer this question rq3 the bestapproachwouldbetomeasuretheperceivedcomplexity using a large population of engineers that leads to statistically sound results.
unfortunately we do not have such a instrumented uninstrumented benchmark iterationruntime figure performance overhead of mysql .
.
.
large population at our disposal.5thus we use a proxy metric and quantify the complexity of a performance model using the number of unique edges in the performance assertion.
the number of unique edges in the performance assertion is just the number of atoms in the boolean expressions.
in figure we compare the number of behavioral clusters to the number of atoms.
not surprisingly the number of atoms increases with the number of clusters.
performance models that represent more complex performance behavior that is models with many clusters require expressions with more atoms to distinguish between these behaviors.
answer to rq3 mined performance models tend to be complex.
we succeed to reduce them to performance assertions that use small boolean expressions to distinguish performance behavior.
with a maximum number of eight required atomic formulae we believe our performance assertions are compact enough to be read and understood by stakeholders.
.
rq4 performance overhead we collect performance data during execution.
to collect this data we add instructions into the running application.
executing these instructions has an e ffect on many measurable metrics especially execution time is easily a ffected.
we evaluated the performance impact of our mining approach using all three subjects apache mysql and okular.
the experiments in this subsection are executed on machine a. okular is the default pdf viewer under kde.
to get a first impression we mined performance specifications from okular .
.
while we were using it for our daily work.
not surprisingly we did not perceive any performance impact.
okular is a gui application and thus waits for user input most of the times.
to asses the actual overhead incurred we measured the time it took to parse and render a mb large pdf file containing pages.
once the file is loaded we load it again.
in this way we eliminate idle time and keep the application busy at cpu load.
in total we load the file times.
mean as well as median time to load the file is .
seconds with .
.
next we employ our performance mining approach.
mean time increases by .
to .
seconds and median time increases by .
to .
seconds .
.
the impact of data collection on execution time is hard to predictandoftendoesnotcorrelatelinearlywiththeamount of instrumentation inserted.
figure presents an extreme example.
we started a single instance of the mysql server.
next we ran the test select benchmark.
once the benchmark terminates we start it again leading to another iter5a comprehensive field study is left as future work.
46ation x axis .
we leave the same mysql server running across benchmark invocations.
mining performance specifications for mysql .
.
leads to a speedupfor most iterations.
we attribute this speedup to cache and pipeline effects.
even minor and seemingly harmless changes to the execution environment can have a severe impact on execution times our approach modifies and rearranges the execution image a much larger change.
figure also confirms an observation we made while evaluating okular the variability in the runtime measurements for the instrumented case is much larger.
this is caused by our automatic adaptation approach new data is constantly pouring in leading to adaptations of the instrumentation and changing overhead levels.
asathirdsubjectweranapachehttp2.
.
.
thisversion uses multiprocessing instead of multithreading in the default configuration.
we use the same setup as in rq1 to rq3 except that we increased the execution time to .
hours to observe long term e ffects.
average response time increases from ms .
to ms .
an increase of .
.
all numbers in this subsection should be taken with a grain of salt the incurred overhead can be easily manipulated and reduced using sampling.
our approach does not aim at minimal overhead instead we try to gather data as fast as possible without incurring a too high overhead.
whatdefinesatoohighoverheadissubjecttodebate therefore a user can specify the target amount of overhead.
in this section we used a target overhead of .
we could have used a smaller target overhead which would have decreased the reported performance overheads as well.
answer to rq4 collecting data a ffects execution time.
incurred overhead is modest and stays reasonably close to our configured target overhead of .
.
rq5 perturbation the effect that data collection has on an application is interesting from two di fferent perspectives.
first the performance overhead has to be reasonably small to ensure that the impact on total performance and throughput is manageable and does not render the system unusable.
second measuring should not perturb the behavior of an application beyond reasonable bounds.
collecting data at runtime not only delays execution it is likely to perturb the collected data in unpredictable ways.
to minimize perturbation of collected data we carefully designed the collection process to inject an almost constant number of instructions into the application.
we strive for a constant overhead.
a constant overhead has the main benefit that it only a ffects the nominal execution time but not the shape of the distribution itself.
to illustrate trends and get a grip on worst case overhead and perturbation we looked into how data collection a ffects the runtime of a fast executing function.
we executed mysql .
.
and ran thetest select benchmark.
we sampled the instruction pointer at random intervals.
we collected samples that map to di fferent unique functions.
from these functions we randomly selected a function that executes very fast is called frequently and has a non trivial distribution of execution times.
by chance we selected the mi bin search function.
to measure the e ffects of runtime instrumentation we manually augmented the source code of mi bin search withfigure measured execution time of mi bin search running mysql .
.
and the test select benchmark.
unaltered application top perturbation due to dynamic instrumentation middle and execution time collected by dynamic instrumentation bottom .
we use a bucket size of 10ns.t h e l a bels on the x axis represent the lower bound of the bucket.
two calls to clock gettime to measure the execution time.
note that manually adding instructions to the source code will itself perturb the time measurements.
there is an inherent limitation towards achievable accuracy and precision regarding time measurements without relying on extra hardware support or full scale simulation.
however we expect static instrumentation to a ffect time measurements less than dynamic instrumentation.
thus we feel comfortable to use the executions times collected using static instrumentation as a reference point to evaluate the relative overhead and perturbation of our dynamic instrumentation.
figure shows three graphs generated from runs each.
the graph at the top plots the execution times measured by static instrumentation without any instrumentation.
the graph in the middle plots the execution times measured by static instrumentation while we also injected instructions to collect execution time dynamically.
thus it represents the overhead added to a function by measuring execution times dynamically.
finally the graph at the bottom plots the execution times measured by dynamic instrumentation.
we can draw several conclusions.
first using dynamic runtime instrumentation to measure execution time adds an average overhead of nsand a median overhead of ns.
second comparing the top graph with the graph in the center we notice that dynamic instrumentation not only adds overhead but also perturbs the execution time.
it follows that the overhead is not constant.
we attribute the perturbation largely to cache e ffects.
comparing the statically instrumented version with the dynamically instrumented one we noticed an increase in the number of instruction cache misses by .
from .
billion to .
billion.
third the execution times measured by an external observer graph in the middle are more perturbed that the execution times collected by our approach bottom graph .
the data collected by our approach is less perturbed be47cause after time is measured it is aggregated and communicated this adds another layer of overhead and perturbation.
this observation led to another improvement in our approach.
perturbation is especially problematic if the collection of executions times is nested.
we detect nested collections and annotate gathered data with statistics about the level of nesting.
the analysis module takes these statistics into consideration.
if the analysis module decides the perturbation is too large to draw reliable conclusions it ignores the data and re collects it in isolation that is it only collects data for one function.
in this way we prevent the accumulation of perturbation and maximize data validity.
answer to rq5 collected data is perturbed by the observer e ffect.
however even for a function we chose deliberately to illustrate the expected worst case scenario key features of the collected data are kept intact.
this supports ourperceptionthatminedperformancemodelsreflectactual behavior.
.
threats to validity we limited our evaluation to three subjects and two machines.
this threatens the external validity of our results.
furthermore we used the spec web2009 banking benchmark for our evaluation.
while the benchmark randomizes the workload requests are still relatively structured with limited diversity.
this might decrease the complexity of our models and might increase the correlation between mined performance models.
an extensive field study would provide more support and could eradicate these threats but is beyond the scope of this paper.
.
related work mining functional specifications focuses on extracting interaction patterns .
system logs can also be used to mine behavioral performance models by leveraging existing timestamps .
it is hard to reason about non functional properties and measure them precisely .
this work contributes to the existing approaches.
our approach is scalable collects data collection in field without human intervention and creates meaningful performance models that aid in comprehension and can be further refined into performance assertions.
our performance assertions can be interpreted as a specification of an acceptable performance spectrum and thus might be considered an instantiation of a program spectrum .
in an attempt to determine the complexity of a piece of code some related work correlates di fferent run time parameters most often input size with execution cost .
even though it might be possible to calculate the input size dynamically at runtime for standard structures measuringinputsizeformorecomplexstructures e.g.
graphs is harder and might require consultation of an expert.
due to this potential pitfall we do not take input sizes into account.
the notion of a performance assertion is not new .
however our performance assertions di ffer significantly in that they incorporate information about the call stack and are collected autonomously.
6we detect recursive functions but currently do not handle them in a special way.
one could sample recursive function at a recursion depth that is randomly picked each time the function is entered and current recursion depth is .automatic behavioral regression testing is an in house functional testing tool that runs di fferent versions of a software against generated test inputs records changes in the behavior and reports it to the developer .
another approach uses probabilistic symbolic execution during inhouse testing to approximate the distribution of execution times .
one major challenge of in house performance testing is the dependency on a representative workload profile that reflects real world usage.
creating a representative test workload that reflects real world usage is challenging .
as a consequence wrong workload assumptions account for more than of all performance bugs .
our work differs.
we target in field performance regression testing.
we believe it can be used to leverage the user base and extract patterns in deployed systems to augment in house testing .
in field testing is not new.
for example it has been used to monitor di fferent coverage criteria as well as exploring the configuration space of software programs .
to gain platform independence stochastic performance logic compares relative executions times of functions .
in our work we compare the grouping of call paths leading to a single functions based on the execution times instead.
we use dyninst as an instrumentation backend .
we believe it is possible to use existing profilers to replace or augment our data collection.
one prerequisite is to have fine grained control about what data is collected when.
for example we believe we could leverage the probes mechanism in the yourkit7profiler to extend our data analysis to java processes.
we could also use oprofile8to include hardware performance counters.
however the sampling nature of oprofile might proof problematic especially for functions with low total runtime.
our approach tries to be as accurate as possible we do not sample unless the overhead actually observed during runtime is too high.
other profilers e.g.
gprof9 require static instrumentation during compilation.
we believe dynamic instrumentation is crucial to enable the monitoring of deployed applications.
our tool is lightweight as compared to more heavyweight binary instrumentation approaches like e.g.
valgrind or pin .
.
conclusion defining non functional specifications like performance is costly.
regularly especially in agile environments precisely defining performance specifications up front is counterproductive.
we tackle these challenges and mine performance specifications automatically without human intervention.
we demonstrated that we can match models across machine boundaries.
this is a crucial step to achieve our overall goal a system that uses volunteer computing to collaborative mine performance specifications.
there are multiple interesting avenues that should be explored in the future.
it would be worthwhile to study the stability of performance models across an exhaustive set of subjects.
the goal of performance specification mining is to help stakeholders accomplish their goals.
a comprehensive field study that evaluates the usability and applicability of mined performance specifications would be valuable.
.