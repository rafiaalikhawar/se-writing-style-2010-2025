maximal multi layer specification synthesis yanju chen university of california santa barbara santa barbara california usa yanju cs.ucsb.eduruben martins carnegie mellon university pittsburgh pennsylvania usa rubenm cs.cmu.eduyu feng university of california santa barbara santa barbara california usa yufeng cs.ucsb.edu abstract there has been a significant interest in applying programming byexample to automate repetitive and tedious tasks.
however due to theincomplete nature of input output examples a synthesizer may generate programs that pass the examples but do not match the user intent.
in this paper we propose mars a novel synthesis framework that takes as input a multi layer specification composed by inputoutput examples textual description and partial code snippets that capture the user intent.
to accurately capture the user intent from the noisy and ambiguous description we propose a hybrid model that combines the power of an lstm based sequence tosequence model with the apriori algorithm for mining association rules through unsupervised learning.
we reduce the problem of solving a multi layer specification synthesis to a max smt problem where hard constraints encode well typed concrete programs and soft constraints encode the user intent learned by the hybrid model.
we instantiate our hybrid model to the data wrangling domain and compare its performance against morpheus a state of the art synthesizer for data wrangling tasks.
our experiments demonstrate that our approach outperforms morpheus in terms of running time and solved benchmarks.
for challenging benchmarks our approach can suggest candidates with rankings that are an order of magnitude better than morpheus which leads to running times that are 15x faster than morpheus .
ccs concepts software and its engineering programming by example automatic programming .
keywords program synthesis machine learning neural networks max smt acm reference format yanju chen ruben martins and yu feng.
.
maximal multi layer specification synthesis.
in proceedings of the 27th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august tallinn estonia.
acm new york ny usa pages.
this work was sponsored by the national science foundation under agreement number of and and by a cmu portugal grant under agreement number of cmu air .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august tallinn estonia association for computing machinery.
acm isbn .
.
.
.
introduction in today s data centric world data analytics has become one of the key elements in our daily life including science politics business and international relations.
on the other hand due to the messy nature of data in different application domains data scientists spend close to of their time performing data wrangling tasks which are considered to be the janitor work of data science.
to mitigate this problem in recent years there has been significant interest in end user program synthesis for data science in which the goal is to automate tedious data analytics tasks from informal specifications such as input output examples or natural language .
for instance programming by example pbe has been used to automate tedious tasks such as string manipulations in excel data wrangling tasks on tabular and hierarchical data and sql queries .
despite significant progress in pbe systems expressing the user intent still remains a major challenge.
as a result due to the incomplete nature of input output examples a synthesizer may generate programs that pass the examples but do not match the user intent.
in that case the user has to provide additional examples to refine the results generated by the synthesizer which imposes a huge burden to the end user as it is tricky to figure out the root cause of the wrong candidates and come up with new examples to refine the output of the synthesizer.
to address the above limitation this paper aims to design a synthesis framework that accurately captures the user intent.
by looking at hundreds of relevant data analytics questions from stackoverflow we observe that an end user typically describes her problem in a combination of input output examples natural language description partial code snippet etc.
to give readers our insight consider an example from stackoverflow in figure .
here the user has an input table and wants to transform it into an output table with a different shape.
as shown in figure the correct solution on the right requires merging two column i.e.
unite aggregating i.e.
group by summarise the sum of another column and finally pivoting spread the returning table.
to solve this benchmark it takes morpheus the state of the art synthesizer for data wrangling tasks around five minutes.
moreover if the program found by morpheus does not match the user intent she has to refine the input output examples and rerun the synthesizer.
in a lot of cases the information provided by the end user typically goes beyond input output examples.
in most helper forums e.g.
stackoverflow we observed that people usually describe problems in the combination of natural language and input output examples.
for instance looking at the example in figure the user not only provides input output examples but also indicates a rough sketch of the solution through natural language.
for instance the reshape and count keywords indicate that the solution should use library functions that perform pivoting i.e.
spread orgather and aggregation i.e.
group by summarise respectively.
other esec fse august tallinn estonia yanju chen ruben martins and yu feng keywords such as total found suggest that sumshould be used together with summarise and the keyword sp b pos that the function call unite should be used.
if we use arrows to visualize the connection between text description and function calls from datawrangling libraries we can observe a strong connection between the user intent and the solution.
however real world textual information is inherently noisy and ambiguous.
as a result it is very challenging to derive the right mapping from the textual information to their corresponding function calls.
second even if we have the right mapping it is still unclear how to integrate this information into most existing pbe systems which typically rely on their efficient search algorithms by leveraging the syntax or semantics of the input output examples.
in this paper we propose mars a novel synthesis framework that takes as input a multi layer specification that appears in a large class of applications.
here a multi layer specification is composed of inputoutput examples textual description and partial code snippets that express the user intent.
to solve a multi layer specification synthesis mss problem mars encodes input output examples as hard constraints which have to be satisfied and denotes additional preferences e.g.
textual description partial code snippet etc as soft constraints which are preferably satisfiable.
after that the mss problem is reduced to the maximum satisfiability modulo theories max smt problem which can be efficiently solved by an off theshelf smt solver .
the max smt encoding of the mss problem aims to satisfy the input output constraints and maximize the user intent that is obtained from natural language partial code snippet and intermediate results.
to accurately capture the user intent from the noisy and ambiguous description we propose a hybrid neural architecture that combines the power of an lstm based sequence to sequence i.e.
seq2seq model and the apriori algorithm for mining association rules.
in particular our seq2seq model encodes the probability of asymbolic program i.e.
a program of which constants are unknown.
given its corresponding textual description.
however like other deep learning applications the performance of a seq2seq model heavily relies on the quality and quantity of the training data.
therefore as shown in section for benchmarks of which solutions are complicated and rarely appear in the training set our seq2seq model may not suggest the right candidates.
to mitigate this problem we leverage the apriori algorithm for mining the extra hidden information that cannot be covered by the seq2seq model.
intuitively through unsupervised learning the apriori algorithm is used to mine association rules that indicate the hidden connections between words and individual functions.
after that we use the association rules for refining the original rankings of the seq2seq model.
to evaluate the effectiveness of our technique we instantiate mars into the data wrangling domain and compare it against morpheus the state of the art pbe synthesizer for data wrangling tasks.
we evaluate both approaches on the benchmarks from the morpheus paper and show that mars outperforms morpheus in terms of running time and number of benchmarks being solved.
for challenging benchmarks our approach is on average 15x faster than the morpheus tool.
to summarize this paper makes the following key contributions we design a customized deep neural network architecture for learning the user s preference using an aligned corpus that maps the user s textual information to the desired solutions.
we design a novel multi layer specification that allows the end user to specify her intent using soft and hard constraints.
we propose a max smt based synthesis framework that takes as input a multi layer specification and enumerates solutions that are close to the user s intent.
our framework is parameterized with the underlying neural networks and the dsl which can be easily instantiated to different domains.
we integrate mars s hybrid model into the morpheus tool and empirically evaluate our approach in the data wrangling domain by showing that mars outperforms the state of the art in running time and number of benchmarks solved.
overview in this section we give an overview of our approach with the aid of the motivating example in figure .
specifically as shown in figure we use a simplified domain specific language dsl based on dplyr andtidyr which are two popular libraries for data wrangling tasks in r. in this example the user wants to perform a complex data wrangling task which requires concatenating two columns i.e.
unite aggregation i.e.
summarise and table pivoting i.e.
spread .
we now explain the key ideas that enable mars to solve this complex problem.
we use abstract syntax trees ast to represent programs.
for example figure shows an ast that represents a symbolic program where some of the nodes are still unknown.
a symbolic program can be instantiated in many ways and can generate several thousand concrete programs.
for instance the concrete program represented in figure corresponds to the following assignment n17 select n27 gather n37 n47 x0 n57 this approach while being general has several drawbacks.
first since input output examples are imprecise specifications a synthesizer may generate a candidate that does not match the user intent which requires the user to provide additional examples to refine the result .
second given a specific task there can be many candidates satisfying the input output examples but only a few of them match the user intent.
in this case a synthesizer typically enumerates solutions according to some heuristic such as the size of ast or keywords provided by the user .
none of the previous work proposes a systematic solution for unifying the user intent from different sources.
mars takes a different step by proposing a multi layer specification that combines input output examples with additional hints from the user.
for instance looking at the stackoverflow example in figure in addition to the input output tables the user also provides extra hints using natural language and intermediate results.
specifically the word reshape in the title indicates that the solution should use either spread orgather and count suggests the occurrence of aggregate functions i.e.
summarise group by .
to incorporate the additional information we propose a novel hybrid neural architecture by leveraging the advantages of a seq2seq model and the apriori algorithm for learning association rules .
603maximal multi layer specification synthesis esec fse august tallinn estonia rscripttoreshapeandcountcolumnswithindataset ineedtoreformatthedatasothatthereisjustonerowpersitevisit i.e.inagivensitenameanddatecombo withcolumnsfortotalfoundbyspeciesandthefishstatus i.e.speciesa pos speciesa neg sp b pos..etc .figuredicouldusethereshapefunctionbutstillneedtosumwithinsitevisitsasreshapewilltakethefirstrow.mythoughtsweretousesplit apply aggregate forloopsetcbuttriedvariouscombinationsandnotgettinganywhere.apologiesi mnotfamiliarwithr.anycommentsappreciated!tbl 7 unite input col species inf status tbl 3 group by tbl 7 site col tbl 1 summarise tbl 3 col2 sum tot output spread tbl 1 col col2 titledescriptiondescriptionunitegroup bysummarisesumspreadsolutioni oexamplei o exampledescription figure a motivating example from stackoverflow.
t xi spread t col col unite t col col group by t list summarise t ag col gather t list select t list list ... col ... ag sum mean max min figure the grammar of a dsl for data wrangling tasks in dplyr and tidyr .
in particular the seq2seq model takes as input the text description and returns the most likely symbolic program according to a statistical model trained from a corpus.
for the example in figure our seq2seq model suggests some of the following candidates mutate group by summarise spread group by summarise mutate select .
.
.
unite group by summarise spread .
.
.
each item in the list is a pair p wi whereprepresents a symbolic program that we learn from the data and widenotes the likelihood of being part of the solution.
by leveraging the additional description from the user the seq2seq model is able to suggest candidates that are close to the user intent.
however due to the size and quality of the training data for complex solutions which rarely appear in the corpus the seq2seq model is unlikely to suggest the correct symbolic program.
as a result a synthesizer may still spend a significant amount of time enumerating wrong candidates.
for instance by following the ranking generated from the seq2seq model a synthesizer has to explore symbolic programs before finding the right candidate.
to mitigate the above limitation we leverage the apriori algorithm for mining association rules.
intuitively an association rule which is learned from a corpus of data through unsupervised learning aims to identify the hidden connections among the keywords.
for instance given the text description in figure our algorithm is able to discover the following rule which suggests that has a high chance to appear in the solution reshape count spread and the following rule indicates that unite should also appear in the solution reshape unite using our refinement algorithm discussed in section .
our system is able to incorporate the hints from the association rules to adjust the distribution of the seq2seq model.
for instance after running the refinement algorithm the previous ranking is adjusted to ... unite group by summarise spread ... mutate group by summarise spread group by summarise mutate select observe that the score of all three candidates get increased as they are connected to association rules learned from data.
the score of the correct candidate increases more as this candidate matches more rules than others.
as a result a synthesizer only needs to explore less than symbolic programs before reaching the right one.
to incorporate the above ranking from our statistical model mars provides soft constraints in the form of f s1 ... sk wi where fis ak nary predicate over dsl constructs with likelihood weight wi.
for instance the symbolic program of the correct candidate can be expressed with the following soft constraints occurs unite occurs group by occurs summarise occurs spread haschild group by unite haschild summarise group by haschild spread summarise here haschild si sj is a binary predicate which indicates that the dsl construct sishould be the parent of sjin the solution.
similarly occurs si is a unary predicate asserting that sishould occur in the solution.
given the soft constraints generated by the hybrid model the underlying max smt solver in mars can enumerate candidates in a way that iis maximized.
in other words mars always prioritizes candidates that not only pass the input output 604esec fse august tallinn estonia yanju chen ruben martins and yu feng n1n2n3n4n5s4selectgathers5s3 figure an example of a symbolic program.
examples but are also consistent with the user intent expressed in natural language.
problem formalization this section proposes a general setting for our synthesis problem and formally states the definitions of our multi layer specification and maximal synthesis.
given a domain specific language dsl described by a contextfree grammarg our synthesis framework searches the space of all possible programs up to a given depth.
a dsl is a tuple r s where r and srepresent the set of symbols productions and the start symbol respectively.
each symbol corresponds to our built in dsl construct e.g.
spread gather select etc constants and variables.
program inputs are expressed as symbols x1 .
.
.
xk .
every production p rhas the form p a a1 .
.
.
ak where is a dsl construct and a1 .
.
.
ak are symbols for the arguments.
symbolic andconcrete programs are defined using symbols from the dsl.
definition .
symbolic program.
a symbolic program pis an abstract syntax tree ast where some labels of the ast nodes are represented as symbolic variables yet to be determined.
example .
figure shows a symbolic program with depth of size two.
here s3 s4 and s5denote symbolic variables which corresponds to unknown symbols.
this symbolic program corresponds to select gather ?
?
?
where the ?
denotes symbolic variables that still need to be determined.
intuitively a symbolic program prepresents partial programs where some of the symbols are unknown.
in section we will introduce a neural architecture for learning the most likely symbolic programs from a corpus of data.
definition .
concrete program.
a concrete program pis an ast where each node is labeled with a symbol from the dsl.
example .
figure shows an ast which corresponds to the concrete program select gather x0 .
definition .
hard specification.
the hard specification expresses a set of constraints that the symbolic program phas to satisfy.
in classical pbe systems we often refer to the input output examples as the hard specification.
in particular p e in eout.
example .
inmars the hard specification is used to encode the input output requirement from the end user.
e.g.
in figure the input and output tables are translated into hard constraints in mars .
n1n2n3n4n5x0select gather figure an example of a concrete program.
definition .
soft specification.
the soft specification denotes a set of constraints that the symbolic program ppreferably satisfies.
in particular each soft constraint is denoted by a pair pr 1 .
.
.
k where pr 1 .
.
.
k is a k ary predicate over the dsl constructs and represents the predicate confidence.
example .
inmars the soft specification is used to encode the user preference in the form of natural language.
for instance the unary predicate occurs i i encodes that a dsl construct i should appear in the program with confidence i. similarly the binary predicate haschild i j j denotes that a dsl construct ishould appear as the parent of jin the program with confidence j. note that the weight of each predicate is automatically learned from a corpus of data.
now we are ready to formally state our synthesis problem.
definition .
maximal multi layer specification synthesis.given specification e wheree tin tout i i and represents all symbols in the dsl the maximal multispecification synthesis problem is to infer a program psuch that p is a well typed expression over symbols in .
p tin tout.
iis maximized.
neural architecture in this section we propose a hybrid neural architecture for inferring the most promising symbolic programs given the user description.
in particular our architecture incorporates a sequence to sequence seq2seq model and the apriori algorithm for discovering association rules through unsupervised learning .
while the seq2seq model is for estimating the initial score of a symbolic program the association rules are further used to adjust the initial score by mining hidden information that can not be identified by the seq2seq model.
.
sequence to sequence model the problem of inferring the most promising symbolic programs from user description can be viewed as a translation between two different languages.
in particular our goal is to translate from natural language to symbolic programs expressed in our dsl.
inspired by the recent success in natural language processing we apply a seq2seq model with long short term memory lstm cells.
as shown in figure given a question solution pair d s where aquestion is a user description composed by word tokens d d d1 d2 .
.
.
dn 605maximal multi layer specification synthesis esec fse august tallinn estonia lstmcellembedding......lstmcellreshapecommentappreciate sos unitegroup bysummariseunitegroup bysummarisespread encoderdecodercount...... .
.
.
.
.
unite group by summarise spread!
count group by summarise !
aggregate summarise !
reshape spread !
unique filter ...rule set programdescriptionreshape count ... comment appreciaterules applied ...score final score2.
figure the hybrid neural architecture in mars and a solution is a symbolic program composed by a sequence of functions si s s1 s2 .
.
.
sm theseq2seq model is used to estimate the probability of p s d which is then given by p s d p s1 s2 .
.
.
sm d1 d2 .
.
.
dn m t 1p st v s1 s2 .
.
.
st wherevis a fixed dimensional vector representation of the user description generated by the encoder .
internally the seq2seq model is composed by two components theencoder and the decoder .
the encoder is an lstm cell that takes as input a question dand generate its corresponding vector representation v. at every time step t we feed each token dtfrom thequestion to the encoder and compute the following functions as given by the lstm mechanism zt wz rt wr ht tanh w ht zt ht zt ht where at time step t htis the hidden state w are network parameters that will later be learned from data is the vector concatenation operation is matrix multiplication and sigmoid and tanh are both activation functions that are given by x e x tanh x ex e x ex e x the final vector representation of a question is given by the last hidden state v hn.
2each symbolic program ignores all constant variables and only preserves the name of each function.similar to the encoder the decoder is also composed by an lstm cell which takes as input a symbolic program represented by a sequence of functions.
the output of the decoder is a distribution of functions given the current hidden state hi ui wu hi bu where wuandbuare both learnable parameters and the probability for a specific function for example the jth function at time step i is estimated by p si j p si j v s1 s2 ... si exp ui j jexp ui j where ui jis the jth element of the vector.
finally we use the back propagation method with negative log likelihood loss to learn the parameters of the neural network.
the probability of a symbolic program given a question is computed by estimating the product of the probability at each time step.
we take logarithm of every time step to prevent underflow of the final result which gives the equation of the probability score as follows p s d p s1 s2 ... sm d1 d2 ... dn m t 1logp st v s1 s2 ... st where the most promising symbolic programs have higher scores.
.
learning association rules as shown later in section due to the quality of the training data ourseq2seq model alone does not always achieve good performance.
specifically for complex benchmarks of which solutions rarely appear in the training data it is difficult for the seq2seq model to suggest the right candidates.
on the other hand even though the user cannot figure out the exact solution for her problem she may still indicate partial information of the desired solution using some keywords or phrases.
in order to discover hidden information that can not be inferred by the seq2seq model we leverage the apriori 606esec fse august tallinn estonia yanju chen ruben martins and yu feng algorithm to mine association rules that will later be used to adjust the rankings from the seq2seq model.
as shown in figure let qbe the union of all tokens that appear in the questions and all functions that appear in a solution q q1 q2 ... qc and let ebe the set of all tokens in a question solution pair s d e si dj where si s dj d anassociation rule rof a given set eis defined by r x y where x y q. for example unite wide spread indicates that if the two keywords unite and wide appear in thequestion then the function spread is also appearing in the corresponding solution .
also rules can apply on functions filter summarise group by which means if both filter andsummarise appear in the solution then the function group by also appears in the same solution .
to learn the association rules we run the apriori algorithm on more than answers3from stackoverflow.
since the apriori algorithm is based on unsupervised learning it may generate rules that are not useful.
to address this issue we further filter out the association rules of which confidence are low according to the following formulas supp x e e x e e conf x y supp x y supp x .
here supp indicates the frequency of xthat appears in the dataset andconf represents how often the rule holds.
.
score refinement algorithm in this section we describe an algorithm that refines the score of theseq2seq model using the association rules in section .
.
as shown in algorithm the key idea of our refinement procedure is to take as input a symbolic program stogether with its original score cfrom the seq2seq model and produce a new score craccording to the association rules rdiscussed in section .
.
internally the refined score cris computed based on an accumulative boosting ratio bthat is initialized at line .
then for each association rule ri the algorithm updates the accumulative boosting ratio based on a weight function as well as a match function that decides whether the current rule ri x yapplies to the current symbolic program together with its description d s match r d s e x y e dore s otherwise furthermore the weight function is used to measure the quality of association rule riby taking several factors into account including the confidence i.e.
conf and support i.e.
supp discussed in section .
number of keywords that appear in rule ri and cost 3an answer towards a specific question is usually composed by some natural language description and solution code which fits the prerequisits of association rules mining.
n1n3n4n2figure an example of a bounded symbolic program.
of the dsl construct e.g.
compared to select mutate is more computationally intensive .
algorithm symbolic program score refinement algorithm procedure refinement r d s c input association rule set r question d solution swith its corresponding score cand weight function output refined score cr b accumulative boosting ratio forruleri rdo b b ri match ri d s cr c b c update score return cr maximal specification synthesis in this section we describe how mars leverages the statistical information discussed in section to enumerate programs that are close to user intent.
as we mentioned earlier most pbe synthesizers perform program enumeration until they find a program that satisfies the input output examples provided by the user.
in order to perform program enumeration we first need to represent the set of all possible programs up to a given depth.
consider a dsld r s where mrepresent dsl constructs with arity m andmis the greatest arity between dsl constructs.
a symbolic programprepresented by a tree of depth kwhere each node has exactly mchildren can represent all programs that use at most k production rules.
figure shows a ary tree with depth 2that represents all programs that can be constructed using at most production rule from the dsl shown in figure .
note that m since the greatest arity between dsl constructs is .
example .
assigning n17 unite n27 input n37 n47 2corresponds to the program unite input which unites columns 1and2from table input.
given a symbolic program pand a dsl d we encode the set of all possible concrete programs as an smt formula .
the satisfiability modulo theories smt problem is a decision problem for formulas that are composed of multiple theories.
to encode symbolic programs we use the quantifier free fragment of the theory of linear integer arithmetic lia .
a model of can be mapped to a concrete program by assigning a symbol to each node in p. variables.
for each node ni we use an integer variable with domain between 0andr where r .
assigning ni7 kmeans that we assign to nithe corresponding symbol.
let idx n0be a mapping between a symbol and its position.
since some production rules pmay have arity smaller than m there may exist 607maximal multi layer specification synthesis esec fse august tallinn estonia some children nodes njthat are not assigned any symbols.
to enforce the invariant that each node is assigned exactly one symbol we introduce a special symbol pewith index that is assigned to nodes without symbols i.e.
nj7 .
example .
consider the dsl in figure .
idxmaps each symbol to a corresponding integer that identifies its position.
for example the input xiis mapped to index spread to index unite to index etc.
constraints.
leti ocorrespond to all symbols that are consistent with the input and output examples respectively.
to guarantee that all models correspond to well typed concrete programs we must enforce the following constraints.
the root node n1ofpwill be assigned a symbol that is consistent with the output type.
p on1 idx p example .
leto xi spread unite group by summarise gather select .
the following constraint enforces that the output type is consistent with the output example n1 idx xi n1 idx spread n1 idx unite idx group by n1 idx summarise n1 idx gather n1 idx select .
letnbe the set of all nodes and chnithe set of children nodes of ni n. furthermore let c p ni be the set of production rules that are consistent with production pand can be assigned to ni.
if a production rule p a a1 .
.
.
ak is assigned to node nithen all mchildren nj .
.
.
nj mwill have to be consistent with a1 .
.
.
ak.
p ni nni idx p nj chni pj c p nj nj idx pj example .
to guarantee that if production p unite is assigned to node n1then its children are consistent with p we add the following constraints to n1 idx unite n2 idx xi n2 idx spread n2 idx unite idx group by n2 idx summarise n2 idx gather n2 idx select .
similar constraints are added to guarantee the consistency of n3and n4when unite is assigned to n1.
letlthe set of leaf nodes and tthe set of terminal symbols.
only terminal symbols can be assigned to a leaf node.
ni l p tni idx p example .
consider the leaf node n2.
to restrict the occurrence of terminals in n2 we add the following constraints n2 idx xi n2 idx n2 idx .
.
.
n2 idx n2 idx .
.
.
n2 idx .
.
enumerating maximal programs enumerating models from the smt formula described in section will correspond to concrete programs.
however this enumeration does not take into consideration the user intent captured by the neural network described in section .
to capture this information we extend the smt formula to a max smt maximum satisfiability modulo theories formula.
a max smt formula is composed bya set of hard and softconstraints.
the max smt problem is to satisfy all hard constraints while maximizing the number of soft constraints that can be simultaneously satisfied.
this problem can be further generalized to the weighted max smt problem where each soft constraint cican be associated with a weight wi.
as hard constraints we use the constraints described in section that guarantee all enumerated programs are well typed.
as soft constraints we use the predicates occurs andhaschild encoded as follows.
let predicate occurs pi wi denote that a production rule pioccurs with likelihood wiin the final program.
this predicate can be encoded into max smt with the following soft constraints with weight wi.
pi ni nni idx pi example .
the predicate occurs spread is encoded by adding the following soft constraint to with weight n1 idx spread n2 idx spread n3 idx spread n4 idx spread .
let predicate haschild pi pj wi denote that production pihas production pjas its children with likelihood wi.
this predicate is encoded as follows where all soft constraints have weight wi.
pi pj ni nni idx pi nj chninj idx pj example .
the predicate haschild summarise group by is encoded by adding the following constraints to with weight n1 idx summarise n2 idx group by n1 idx summarise n3 idx group by n1 idx summarise n4 idx group by maximizing the satisfaction of these soft constraints will guarantee that we enumerate programs that are closer to the user intent.
note that even though the predicates occurs andhaschild suffice to capture the information extracted by the neural network our approach is not limited to these predicates and can be extended by adding additional predicates e.g.
happens before .
implementation data collection and preparation.
we collect pages from stackoverflow using the search keywords tidyr and dplyr with testing benchmarks excluded where each page contains a single question and multiple solutions .
by removing duplicate contents and questions with no solutions we obtain questionsolution pairs.
each question is pre processed by a standard nlp pipeline that includes stop word removal lemmatization and tokenization and a solution is represented as a sequence of dsl constructs i.e.
function names .
the question solution pairs are then used to train a seq2seq model.
for the association rules mining we extract descriptions from answers and their corresponding solutions and totally obtain transactions as the input to the apriori algorithm.
to ensure the validity of our experiments we remove all the benchmarks from the collected data.
608esec fse august tallinn estonia yanju chen ruben martins and yu feng neural network and hybrid architecture.
we build a seq2seq neural network using the pytorch framework .
the hyperparameters e.g.
numbers of dimensions of the word function embedding layer and lstm hidden layer are obtained through a simple grid search.
for the seq2seq model in mars we set both the dimensions of word function embedding layer and lstm hidden layer to be where the embedding layer maps words and functions4to vectors of the dimension .
furthermore a single layer perceptron is connected to the hidden layer of each output time step in the decoder mapping from a dimension of 5125to which is used to predict the probability of each function given the previous hidden state and the current input.
as for the association rule mining we apply the efficient apriori package to discover useful association rules that can be further applied to refine the original ranking generated by the seq2seq model.
we then select valid rules according to the following criteria confidence .9or support .
.
each valid rule should have at least word and function .
and the number of functions in the rules shall not exceed .
each valid rule should not contain any stop words which builds upon the english stop words and includes additional words andfunctions that we consider less indicative.
by filtering out less relevant rules we obtain association rules.
machine configuration.
we train our seq2seq model on a machine from google cloud platform with a .20ghz intel xeon cpu and an nvidia tesla k80 gpu.
all synthesis tasks were run on a laptop equipped with intel core i5 cpu and 16gb memory.
since themorpheus tool is only available on a virtual machine we used this virtual machine to run all program synthesis experiments.
it took around hours to train our hybrid model.
evaluation we evaluated mars by conducting experiments that are designed to answer the following questions q1 do our multi layer specification and neural architecture suggest candidates that are close to the user intent?
q2 what is the impact of the neural architecture in mars on the performance of a state of the art synthesizer for data wrangling tasks?
q3 how is the performance of mars affected by the quality of the corpus?
.
quality of suggested candidates to evaluate the benefit of the multi layer specification and neural architecture in mars we instantiate the tool to the data wrangling domain where data scientists tend to spend about of their time tedious and repetitive tasks.
in particular we use the data in section to train the n gram model from the morpheus paper theseq2seq model discussed in section .
and the hybrid neural architecture described in figure .
since the output of each model 4there are natural language words in the word vocabulary and functions in the function vocabulary.
each vocabulary contains special helper tokens namely namely pad empty placeholder sos and eos the start and end of a sequence ukn out of vocabulary word .
5since we are using separate seq2seq structures for titleandquestion the concatenation of the hidden layers from both are of a dimension of .is a distribution of symbolic programs we run all three models on the original benchmarks from morpheus which contains data wrangling tasks using two popular r libraries namely tidyr and dplyr .
in particular the data wrangling dsl contains production rules and can induce a gigantic search space of the symbolic programs posing a challenge for state of the art synthesizers.
as shown in morpheus user study data scientists solved on average two benchmarks in one hour.
for each benchmark we then use the seq2seq model and hybrid neural architecture to enumerate symbolic programs and record the ranking of the correct candidate that matches the user intent.
finally we manually checked all solutions synthesized by mars and made sure that they are semantically equivalent to the reference solutions.
because the n gram model in morpheus only considers programs in the posts on stackoverflow and ignore user description it provides a global ranking shared by all benchmarks.
results.
as shown in table the average ranking and standard deviation of the n gram model are and respectively.
in other words a synthesizer would need to explore symbolic programs on average.
recall that a symbolic program may correspond to several thousand concrete programs.
the standard deviation is used to quantify the stability of the model.
in contrast by incorporating the user descriptions the seq2seq model achieves an average ranking of and a standard deviation of .
finally with the help of the association rules the hybrid model obtains the best performance with an average ranking of and a standard deviation of .
the result shows that our hybrid model not only suggests candidates that are close to user intent i.e.
low average and it is also more stable i.e.
low standard deviation across different benchmarks.
table statistics for different model rankings.
model n gram seq2seq hybrid average std.
1standard deviation.
computed based on the rankings of the correct solutions.
table counts of top 1s and top 3s in different models.
model n gram seq2seq hybrid top total top total computed based on the rankings of the correct solutions.
we further look into the number of top and top candidates that are correctly suggested by each model.
as shown in table without user descriptions the n gram model fails to predict any correct candidates in top and only suggests correct candidates in top for two benchmarks.
by leveraging user descriptions the seq2seq model is able to figure out the right top and top candidates for and benchmarks respectively.
finally our hybrid 609maximal multi layer specification synthesis esec fse august tallinn estonia 1100101102n gram is better seq2seq is better figure comparison of run times in seconds between ngram x axis used in morpheus and seq2seq y axis used in mars using a logarithmic scale.
model successfully suggests top and top candidates for and benchmarks.
.
effectiveness of hybrid neural architecture in this section we further investigate the impact of a better ranking on the end to end performance of a synthesizer.
specifically we integrate the previous three statistical models into morpheus a state of the art synthesizer for data wrangling tasks.
results.
figure and figure show the results of running morpheus on its original benchmarks with three different models namely n gram model in original morpheus andseq2seq hybrid model in mars and a time limit of seconds.
in particular each dot in the figure represents the pairwise running time of a specific benchmark under different models.
as a result the dots near the diagonal indicates that the performance of the two models is similar in those benchmarks.
for instance figure shows the comparison between the n gram model and our hybrid model in terms of running time.
specifically our hybrid model outperforms morpheus original n gram model in of benchmarks.
in the meantime morpheus times out on benchmarks with the n gram model whereas it only times out on benchmarks with our hybrid model.
the performance of the seq2seq model is between the above two models by outperforming morpheus n gram model in of benchmarks and timing out on benchmarks.
table shows the table statistics of running time.
model avg.
speedup1 timeouts ngram 1x seq2seq 6x hybrid 15x 1average speedup on challenging solved benchmarks.
number of timeouts on all benchmarks.
1100101102n gram is better hybrid is better figure comparison of run times in seconds between ngram x axis used in morpheus and hybrid y axis used in mars using a logarithmic scale.
average speedup for challenging benchmarks i.e.
3library calls with respect to the n gram model for benchmarks that can be solved by both models.
on average the seq2seq model is 6x faster than the n gram model and the hybrid model is 15x faster than the n gram model.
the result further confirms that a statistical model that accurately captures user intent tends to have a better performance in running time.
remarks.
to understand the cases where our technique runs significantly faster we manually look into some of the benchmarks.
we notice that our technique performs especially well if the user states her problem in a clear way.
for instance in this post from stackoverflow 6although the user does not know the exact solution for her complex task she is still able to convey the transformations using keywords e.g.
count and unique and partial code snippets.
even with these discrete signals our hybrid model manages to guide morpheus to the correct program in less than a second tbl 7 filter p25 input1 b tbl 3 unite tbl 7 key ab a b tbl 1 group by tbl 3 key ab morpheus summarise tbl 1 e n in contrast morpheus with its original n gram model takes several minutes to find the right candidate.
.
discussion like any other technique our approach also has its own limitations.
for instance in figure there are still some benchmarks where n gram performs better we manually inspect all these cases and notice that the issue is caused by the following reasons insufficient text.
in this post 7the user only provides inputoutput examples but her description barely contains any useful signals that allow our hybrid model to make a good prediction.
contextual text.
in this post 8the user explicitly states that she does not want to use the mutate function 610esec fse august tallinn estonia yanju chen ruben martins and yu feng ... i can solve my problem using dplyr s mutate but it s a time intensive roundabout way to achieve my goal.
... however after tokenizing the natural description and removing all the stop words e.g.
but our hybrid model loses the contextual information and takes mutate as the keyword.
misleading text.
in contrast to the previous example in this post 9the user explicitly wants to use the mutate function ... iwant to use mutate to make variable d which is mean of a b and c. ... however since we directly adopt the dsl from morpheus and the dsl does not support this special usage of mutate our hybrid model proposes candidates that do not lead to the correct solution.
.
threats to validity quality of the corpus.
even though the hybrid neural architecture is more resilient to the limitation of the existing data set the performance of mars is still sensitive to the quality of the training data.
to mitigate this concern we train our statistical model using all relevant posts from stackoverflow.
in the future we also plan to leverage transfer learning to incorporate resources written in other languages e.g.
python and matlab .
benchmark selection.
due to the expressiveness of the dsl in terms of complexity the benchmarks from morpheus may not represent the actual distribution of the questions on stackoverflow.
while the comparison on the morpheus benchmarks may not completely unveil the benefit of our hybrid neural architecture and a representative test suite may provide a more comprehensive view we believe our comparison is sufficient to show the strength of our technique.
furthermore since both our neural architecture and the enumerator are designed in domain agnostic way we also believe our technique can generalize to other domains.
related work program synthesis has been extensively studied in recent years.
in this section we briefly discuss prior closely related work.
programming by example.
our technique is related to a line of work on programming by example pbe .
pbe has been widely applied to different domains such as string manipulation data wrangling and sql queries .
among these techniques the morpheus tool is directly related to the data wrangling client to which mars is instantiated.
however unlike morpheus that is specialized to table transformation the techniques in mars can be generalized to other synthesis tasks.
compared to existing pbe systems mars proposes a novel neural architecture that can learn user preferences from natural language.
programming by natural language.
programming by natural language pbnl is another paradigm that is related to our approach.
specifically the sqlizer tool takes input as natural language and generates its corresponding query in sql.
there are other pbnl systems that translate natural language into simple commands in smartphone ifttt scripts and scripts for text editing .
compared to previous pbnl systems neural architecture can reasonably capture the user intent even in the presence of low quality training data.
furthermore in addition to natural language the multi layer specification in mars also accepts input output examples as hard constraints which provide a stronger correctness guarantee.
machine learning for program synthesis.
the neural architecture in mars is relevant to two major directions for applying machine learning to program synthesis.
in particular the first line of work is to directly generate programs from inputs in the form of natural language or input output examples which is inspired by the seq2seq model in machine translation.
although we also incorporates a seq2seq model as part of the neural architecture we further leverage the apriori algorithm for mining association rules to mitigate the quality of training data.
the second approach incorporates statistical information to guide a program synthesizer.
in other words a statistical model is used to suggest the most promising candidates a synthesizer has to explore.
for instance deepcoder uses a deep neural network that can directly predict programs from input output examples.
themorpheus tool adopts an n gram model for synthesizing data wrangling tasks.
similarly the slang tool integrates ann gram model for code completion.
raychev et al.
extend the previous approach to obtain a statistical model that can guide a synthesizer in the presence of noisy examples.
the neo synthesizer generalizes previous approaches by incorporating an arbitrary statistical model as its decider to guide the enumerative search.
while mars proposes a novel neural architecture to suggest the most promising candidates it can also leverage advanced techniques from previous work such as pruning infeasible candidates through deduction and conflict driven learning .
interactive program synthesis.
the goal of our technique is also aligned with tools in interactive program synthesis where the goal is to iteratively refine user intent through incorporating user decision in the synthesizer loop.
while our approach leverages natural language to capture the user intent we believe the idea of interactive synthesis is complementary to our approach and can further refine the distribution of our statistical model.
conclusion we propose mars a novel synthesis framework that takes as input amulti layer specification which combines input output examples textual description and partial code snippets to capture the user intent.
to solve a multi layer specification synthesis mss problem mars encodes input output examples as hard constraints and denotes additional preferences e.g.
textual description partial code snippet etc as soft constraints .
the mss problem is reduced to a max smt formula which can be solved by an off the self solver .
to accurately capture user intent from noisy and ambiguous descriptions we propose a novel hybrid neural architecture that combines the power of a sequence to sequence model and the apriori algorithm for mining association rules.
we instantiate our hybrid model to the data wrangling domain and compare its performance against morpheus on its original benchmarks.
our results show that our approach outperforms morpheus and it is on average 15x faster for challenging benchmarks.
611maximal multi layer specification synthesis esec fse august tallinn estonia