testing probabilistic programming systems saikat dutta university of illinois usa saikatd2 illinois.eduowolabi legunsen university of illinois usa legunse2 illinois.eduzixin huang university of illinois usa zixinh2 illinois.edusasa misailovic university of illinois usa misailo illinois.edu abstract probabilistic programming systems pp systems allow developers to model stochastic phenomena and perform efficient inference on the models.
the number and adoption of probabilistic programming systems is growing significantly.
however there is no prior study of bugs in these systems and no methodology for systematically testing pp systems.
yet testing pp systems is highly non trivial especially when they perform approximate inference.
in this paper we characterize previously reported bugs in three open source pp systems edward pyro and stan and propose probfuzz an extensible system for testing pp systems.
probfuzz allows a developer to specify templates of probabilistic models from which it generates concrete probabilistic programs and data for testing.
probfuzz uses language specific translators to generate these concrete programs which use the apis of each pp system.
probfuzz finds potential bugs by checking the output from running the generated programs against several oracles including an accuracy checker.
using probfuzz we found previously unknown bugs in recent versions of these pp systems.
developers already accepted bug fixes that we submitted to the three pp systems and their underlying systems pytorch and tensorflow.
ccs concepts software and its engineering software testing keywords probabilistic programming languages software testing acm reference format saikat dutta owolabi legunsen zixin huang and sasa misailovic.
.
testing probabilistic programming systems.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
introduction probabilistic programming has recently emerged as a promising approach for helping programmers to easily implement bayesian inference problems and automate efficient execution of inference tasks.
both research and industry have proposed various probabilistic programming systems e.g.
church stan and many permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
.
these systems automate various parts of common inference tasks and support many approximate inference algorithms from machine learning and statistics including deterministic variational inference and randomized markov chain monte carlo mcmc simulation.
systems like edward and pyro embed probabilistic inference within the general deep learning infrastructures e.g.
pytorch and tensorflow .
a probabilistic programming system pp system typically consists of a language a compiler and inference procedures.
a programmer writes a program in a probabilistic programming language which extends a standard programming language by adding constructs for random choice such as sampling from common distributions conditioning on data such as observation statements and probabilistic queries such as obtaining a posterior distribution or an expected value of a program variable .
next a pp system compiles the probabilistic program to an efficient inference procedure by adapting well known inference algorithms.
finally the programmers run the compiled program on a set of data points to compute the query result.
probabilistic programming systems provide many benefits to programmers who are non experts in probability and statistics but ensuring the correctness of probabilistic programs is notoriously difficult .
the inherent uncertainty and complexity of probabilistic inference which is p hard even with just discrete variables make most practical inference algorithms numerically intensive and approximate.
therefore a testing approach for pp systems must account for both numerical errors and errors due to the approximate nature of inference algorithms.
current approaches for testing pp systems are typically manual and ad hoc.
although recent research looked into analysis of pp systems none of the proposed approaches can analyze all stages of modern pp systems.
understanding previously known bugs in pp systems and finding effective approaches to improve the systems reliability remain open research questions.
.
bugs in probabilistic programming systems to motivate the design of tools for systematic testing of pp systems we characterized the kinds of bugs that are common in existing open source systems.
to the best of our knowledge this is the first systematic study of bugs in pp systems.
we studied three systems edward pyro and stan .
they are written in multiple programming languages are hosted on github have been adopted by both industry and researchers are actively developed and implement many language features and inference algorithms that are common to most pp systems.
in total we categorized of commits about bugs as being pp systems related and describe them in more detail.
many of the identified bugs required domain specific knowledge to detect debug and fix.
moreover testing pp systems often requires reasoning about result accuracy in contrast to the standard esec fse november lake buena vista fl usa saikat dutta owolabi legunsen zixin huang and sasa misailovic figure probfuzz architecture notion of binary pass fail result correctness .
we identified two domain specific classes of bugs in pp systems algorithmic accuracy bugs anddimension bugs .
algorithmic bugs influence computations of probability expressions and the steps of the inference algorithms often resulting in decreased result accuracy and are typically hard to identify and fix.
dimension bugs occur when computations do not properly handle the dimensions of data or allowable ranges of probability distribution parameters.
section describes the lessons learned from studying these historical bugs which we leveraged to design probfuzz.
.
probfuzz we present probfuzz a novel approach and system for systematic testing of pp systems.
figure shows the architecture of probfuzz.
the inputs to probfuzz are a specification of the primitive discrete and continuous distributions the number of programs to generate and a template that specifies the skeleton of a probabilistic program model of interest written in a high level probabilistic language notation ir .
probfuzz outputs a set of programs that are likely bug revealing in the pp systems.
probfuzz has three main components generator completes holes in the template to produce a probabilistic program in an intermediate language and accompanying data necessary to run probabilistic inference.
template completion is a form of fuzzing generator produces many programs with different concrete distributions distribution parameter values and data values.
to generate programs that are more likely to identify non trivial bugs generator incorporates domain specific information e.g.
legal connections among distributions ranges of their parameters and data properties.
translator converts the intermediate probabilistic program to a specific api or language of a pp system under test and selects system supported inference algorithms.
we implemented three versions of translator for edward pyro and stan.
program checker runs the generated programs and determines whether the outputs indicate likely bugs in the pp system on which the programs were run.
program checker produces a set of likely bug revealing programs for developers to inspect and supports checks for standard problems like crashes or nanerrors in the output and accuracy of inference results.
we designed generator to be general it represents probabilistic models in the intermediate first order probabilistic language and can target various pp systems.
we designed translator to be flexible and extensible.
our experience is that adding support for a new pp system is relatively easy.
moreover support for multiple pp system in the translator enables differential testing as an oracle in the program checker.
probfuzz is available at probfuzz leverages the observation that testing pp systems is conceptually similar to the well studied field of compiler testing.
a prominent approach in compiler testing is compiler fuzzing which randomly generates many test programs and checks whether a compiler produces code or crashes and whether generated programs are correct i.e.
produce same results as reference programs.
our study of existing bugs and evaluation of probfuzz show the importance of domainspecific knowledge about probability distributions and inference algorithms joint generation of programs and corresponding data to run inference and reasoning about accuracy.
these traits are out of reach for state of the art compiler fuzzing techniques.
.
results we evaluated probfuzz on three pp systems edward pyro and stan.
our evaluation shows the effectiveness of probfuzz in generating probabilistic programs and data that reveal dimension boundaryvalue and algorithmic accuracy bugs in all three systems.
we discovered potential previously unknown bugs in these systems.
further we used probfuzz to target existing bugs in each pp system we characterized in section to see in how many categories per pp system probfuzz would have caught a bug.
probfuzz caught at least one existing bug in of categories that we targeted.
section presents quantitative results of probfuzz.
as part of our bug discovery and understanding process we submitted all potential bugs revealed by probfuzz to developers of the pp systems.
so far developers have accepted rejected are still pending and was already fixed before we could submit it.
the bugs that we found and fixed were not just in edward pyro and stan but also in the underlying software infrastructure on which they are built i.e.
pytorch for pyro and tensorflow for edward .
we describe some of the identified bugs their fixes and lessons that we learned in section .
.
contributions this paper makes the following contributions concept.
we extend compiler fuzzing to probabilistic programming systems.
we generate both probabilistic programs and data to run inference by encoding domain knowledge and reasoning about accuracy of inference results.
bug characterization.
we present the first study of bugs in pp systems.
our investigation of previously fixed bugs in three open source pp systems showed that these bugs require domain knowledge to find and fix and to reason about accuracy.
methodology and system.
we propose probfuzz a novel general approach for systematically testing pp systems.
our current implementation of probfuzz works for three open source pp systems and is extensible.
evaluation.
we evaluated probfuzz on both historical and recent versions of edward pyro and stan.
probfuzz found bugs in each category of previously reported bugs.
we also found and reported previously unknown bugs by running probfuzz on recent versions of the pp systems.
575testing probabilistic programming systems esec fse november lake buena vista fl usa 1x 2y 3w gamma .
.
4p beta .
.
5observe 6n ormal w x p y 7posterior w a probabilistic program b stan result c edward result d pyro result e bugs result figure example program and the posterior distributions computed by various systems illustrative example figure 2a shows an illustrative example of a potentially bug revealing probabilistic program generated by probfuzz.
the program shown in probfuzz s intermediate language defines two data sets of constants xandy.
each yiis seven times the value of xi.
the program first assigns prior distributions to the variables wandp.
then it conditions the linear model w x pon the observations of y. the probabilistic query on line seeks the posterior distribution w. probabilistic inference is a procedure for computing the change in the distribution of variables based on the observations of data.
most inference algorithms today are approximate with the two dominant approaches being markov chain monte carlo simulation which re executes the computation with many random samples and is implemented in e.g.
stan and edward and variational inference which approximates the posterior distribution deterministically by substituting it with computationally simpler distributions and is implemented in e.g.
edward and pyro .
figures 2b 2e show the posterior distributions computed by stan edward pyro and another probabilistic inference system called bugs which is a precursor of stan and shares most of its syntax .
the x axis presents the numerical values and the y axis presents its probability density function.
given the data xandy we expect the mean of the posterior of wto be equal to .
.
the posterior distributions computed by three systems are similar and centered at .
.
however stan s distribution has a different shape and its mean is close to .
.
we discuss the reasons behind this accuracy problem in section .
.
probfuzz generates the program in figure 2a and many similar programs with different prior distributions distribution parameters and data.
probfuzz then compiles the programs down to each pp system generating specialized api calls or dsl programs.
the translation is non trivial and cumbersome for a human but can be easily specified in probfuzz.
next probfuzz runs generated programs automatically compares the output from different pp systems and computes accuracy metrics section .
.
finally a developer can inspect probfuzz results and investigate any potential bugs.
we discuss probfuzz in section .
bug characterization study we characterized existing bugs in three open source pp systems stan edward and pyro .
table shows some statistics about the pp systems.
the three pp systems support various approximate probabilistic inference algorithms.
methodology.
we manually searched for bug fixes among commits in the github repositories of the pp systems in our study.
we use commits to get a larger data set than we could get when startingfrom github issues .
given the active development of these pp systems many bugs are fixed without first being reported as issues and most closed issues involve one or more commits.
we obtained all commits in the three pp systems that contained the keywords bug inference error fix nan exception overflow underflow infinity infinite precision unstable instability r inging unbounded roundoff truncation rounding diverge cancel lation cancel accuracy accurate .
this resulted in commits.
we then filtered out commits that are not specific to the domain of pp systems or probabilistic inference and could occur in any software domain.
first we filtered out commits containing the following keywords typo docstring notes example examples tuto rial print doc document messaging test messages manual doxyg en cpplint jenkins submodule header .
next we split the remaining commits between two student coauthors each of whom read descriptions and reasoned about modified code.
each coauthor marked a commit as an inference related code fix general code fix a refactoring or a duplicate.
we filtered out refactoring duplicates e.g.
covered by incremental commits fixing the same bug or related commits from multiple branches merge commits with many files changed and commits that changed only non source files.
we were left with commits that fix code out of which our manual inspection identified commits that are directly related to the domain of probabilistic inference.
the remaining are general coding problems e.g.
i o errors api misuses and documentation problems.
two coauthors inspected these commits.
they compared notes and classified bugs as inference related only if they agreed on the final classification therefore making a conservative determination about the domain specific nature of each bug.
similar to a previous work on analyzing numerical bugs we put inference related bugs into four categories.
our bug categories are algorithmic accuracy dimension boundary values numerical and language translation.
we made a second pass through the bugs that satisfy the selection criteria and categorized them based on error sources and bug manifestations.
when possible we matched each commit to its related github issue.
table project statistics edward pyro stan first commit date feb jun sep no.
of contributors no.
of commits latest commit studied 992ce08 8db8972 14981a3 lines of code prog.
language python python c infrastructure tensorflow pytorch own 576esec fse november lake buena vista fl usa saikat dutta owolabi legunsen zixin huang and sasa misailovic table breakdown of commits category edward pyro stanp algorithmic accuracy dimension boundary numerical language translation 26p26 .
characterizing bugs in pp systems table shows the distribution of the categorized commits.
column category shows category names.
the second to fourth columns show the number of commits per category in each pp system.
finally columnppresents the sum of the commits in each bug category.
the database of inference related bugs is available at algorithmic accuracy bugs.
this category contains bugs due to incorrect implementation of inference algorithms and other related bugs in the implementations of probability distributions and statistical procedures.
they manifest as inaccurate although plausible and therefore hard to catch results of inference.
these bugs affected a variety of inference algorithms and implementations of probability distributions in all three pp systems.
in edward the bugs affected three inference algorithms and two built in distributions bernoulli and uniform .
in pyro the bugs affected three inference algorithms and the cauchy distribution.
in stan the bugs affected two inference algorithms one distribution bernoulli logit and two auxiliary functions.
these bugs can be further subdivided into logical errors mathematical errors and one regression error.
examples of logical bugs include re normalizing already normalized data double counting the values of specific variables and using only the first element instead of a whole collection to fill a tensor .
examples of mathematical errors include incomplete formulae e.g.
missing terms and wrong formulae e.g.
.
finally a regression in stan led to lower statistical efficiency .
dimension boundary value bugs.
these bugs occur when functions do not properly handle the dimensions of input data a scalar vector matrix or cube the ranges of input data and the ranges of distribution parameters.
they manifest as exceptions or special numerical values e.g.
nanorinf in the output in the case of boundary value bugs .
the examples of dimension bugs include those where the functions assumed a particular dimension of input data e.g.
scalars and crash if data with different dimension is passed as input or assumed a wrong dimension of output which caused crashes in the function s clients e.g.
.
one bug resulted from using only one ordering of a list a vector to compute entropy instead of using all possible orderings a matrix .
missing boundary condition checks often happen in implementations of various probability distributions e.g.
not checking for boundary values of a parameter leading to nan .
such bugs typically manifest substantially late during inference e.g.
computing logof zero resulting in nan .
we also observed some off by one errors e.g.
in where ifconditions used instead of .
general numerical bugs.
these bugs are found in general mathematical functions and may manifest as an inaccurate result or a special value nanorinf .
most of these bugs are in stan whichimplements its own mathematical back end in contrast to edward and pyro which use external back ends tensorflow and pytorch respectively .
example numerical bugs that we identified include improper handling of inf e.g.
or nan including when these special values propagate to the output initializing integer values to nan overflow errors and convergence bugs.
language translation bugs.
these bugs occur due to wrong use of features in the programming language in which the pp system is written.
they can manifest as failed builds runtime errors or wrong results.
these can be errors in the interface e.g.
returning a real instead of an array as expected from the api specification errors in the back end or changes in their implementations e.g.
errors that break compilation or error reporting e.g.
and errors in using functionality.
one functionality usage error involved calling a stateful inference function making different runs of the same probabilistic program producing widely different results .
.
discussion we highlight several important observations from our characterization study which motivate our approach for testing pp systems observation domain knowledge is required to detect analyze and fix bugs.
most of the inspected algorithmic accuracy and dimension boundary value bugs and some numerical bugs require knowledge of theory of probability or inference.
bugs in the dimension boundary value category are similar to general bugs that occur when one does not satisfy the specification of a method.
however without specification related assertions which require domain specific knowledge and are tedious to write in the code such bugs occur in the pp systems resulting in nanor silent errors.
observation algorithmic bugs require detailed reasoning about accuracy.
for many of the inference and accuracy bugs the developers report in accuracy of the results and compare the results either to known expected values or against another tool e.g.
edward or pyro against stan .
for algorithmic errors existing numerical analyses are typically not applicable.
identifying errors and their causes requires probabilistic reasoning detailed error reports and discussions with pp system developers in order to diagnose the error e.g.
.
observation testing pp systems requires careful generation of both programs and valid data.
reproducing many of the bugs that we manually inspected required both a probabilistic program and the data to run it on.
the github issues related to the commits that we inspected had both programs or program fragments and data necessary to reproduce the bug.
such data is sampled from probability distributions and is required for setting up priors and posteriors distribution parameters and as inputs for inference.
this is different from compiler testing where it is sufficient to simply generate programs that take no inputs and encode arbitrary scalar values of variables.
observation many errors are revealed by small programs.
most github issues related to the commits that we inspected had small reproducible programs.
the observation that many bugs can be found by small programs is well known and has been used extensively in conventional testing.
while standard compiler testing e.g.
csmith often generates large programs to maximize bug finding capability small programs seem sufficient for successful detection and debugging in the pp system domain.
577testing probabilistic programming systems esec fse november lake buena vista fl usa probfuzz probfuzz takes as inputs the template of the probabilistic model the number of programs to generate and the systems to test.
the developer writes the templates of probabilistic models in an intermediate probabilistic language with holes which represent missing distributions parameters or data section .
.
figure presents the pseudo code of the probfuzz algorithm.
the generator generates probabilistic models by completing holes in the template with concrete distributions parameters and data section .
resulting in a program in an intermediate language.
the translator then translates the probabilistic program from the intermediate language into a program that uses the api of the target pp system section .
.
next probfuzz runs the programs collects output and its program checker computes metrics and checks for symptoms that may reveal potentially buggy programs section .
.
finally probfuzz reports any warnings issued by the program checker to the developer.
.
template and intermediate language probfuzz represents the templates and the generated programs in an intermediate language ir .
figure 4a presents the syntax of the ir language of probfuzz.
the key aspect of the template isa hole denoted as ?
?
.
it represents a missing distribution or parameter.
the distributions and parameters are completed with concrete values from respective sets dists andconsts by replacing the hole.
a template consists of four sections which specify data prior distributions model that relates posterior and prior distributions and the query.
the data section presents the input and the output data set s .
a data vector is a typed multidimensional array which is instantiated by probfuzz or a specific list of numerical constants.
the prior section specifies the prior distributions of the program variables.
a prior distribution can be an instance of a distribution or a hole.
similarly one or more parameters of the distribution can be either expressions or holes.
the expressions are typical with arithmetic and comparison operators.
the language is similar to the loop free fragment of the prob language from .
themodel section conditions the random variables to the specific observations.
the observe clause states that the observations of the model specified as the first parameter are found in the vector denoted as the second parameter as is a standard interpretation in most probabilistic languages .
the models can also be composed using conditionals.
finally the query instructs the probabilistic language to return the marginal posterior distributions for the specified variables or their expected values.
examples.
figure 4b presents a template from our experiments and figure 4c presents an example program that has the holes completed.
the template is for a linear regression model which has two sets of observations xandy both are one dimensional vectors of length .
the prior parameters are weight w bias b and the noise p with unknown distributions.
the template conditions an unspecified distribution with two parameters the first is the linear expression w x b the second is p on the data from the vector y. distribution specification.
for each distribution probfuzz specifies its properties including the names and ranges of parameters and the range of the distribution support.
knowing the propertiesof the distributions allows probfuzz to complete the template with the concrete values of parameters.
to illustrate the specification of the normal distribution is name normal type continuous support float args name mu type float name sigma type float it specifies that the distribution is continuous and its support the range of values that can be sampled from the distribution is not constrained.
it has two parameters the mean muis an arbitrary floating point value while the standard deviation sigma must be positive.
the support and parameters of the distribution can be bounded.
for instance in the case of gamma distribution the support is only positive real numbers and in the case of bernoulli the support is .
.
generator the generator generates a concrete program and data from the provided template.
a concrete program consists of complete ir and data.
in a concrete program all ?
?
symbols have been replaced by the corresponding distribution expressions or constant expressions as in figure 4c .
the user defined program templates plus domain knowledge about distributions and data ranges enable generator to achieve more targeted fuzzing.
the generator has two components the distribution selector which matches the distribution expressions with holes ?
?
in the template and the data selector which produces the concrete values of the parameters of the distributions and computes the values of the data points.
for each generated program the generator performs the following steps complete the distribution of the model.
for the model expression the distribution selector finds all the distributions that can match the pattern e.g.
have two parameters and uniformly at random selects one of those distributions to fill in the hole.
once fixed this distribution provides the legal values for the data to generate based on the distribution support and the constraints on the parameters.
this bounds the set of allowed distributions of the priors in the template.
for instance if we select the normal distribution for the linear regression template figure 4b the model constrains the distribution of the variance pto have positive support.
complete the distributions of the priors.
based on the constraints from the model the distribution selector randomly selects a distribution whose support satisfies the range of values admissible by the model s parameter.
to propagate the information about distributions we implement a simple dependence analysis with interval analysis to keep track of the ranges.
for instance in figure the distribution selector may choose exponential as the distribution of the prior for p but not normal since its support is all floating point values but pcan have only positive values .
complete the distribution parameters.
data selector picks the numerical values of the parameters of the distributions with holes using a method that randomly chooses between two strategies.
the first strategy randomly selects a value within the range of the parameter as denoted in the distribution specification.
a developer may express preference for larger or smaller values 578esec fse november lake buena vista fl usa saikat dutta owolabi legunsen zixin huang and sasa misailovic inputs program count n template t pp systems under test s output likely bug revealing programs report p function probfuzz n t s p fori 1tondo progir data generator t results fors sdo progs translator s progir data status s outs executeprogram s progs data results results status s outs end for warnings programchecker results ifwarnings none then p p warnings end if end for return p end function figure probfuzz algorithmx vars c consts aop bop ... dist normal uniform beta ... type int float range c c type data x type x x expr expr c x expr aopexpr expr bopexpr param ?
?
expr prior x ?
?
x dist param model observe dist expr x observe ?
?
expr x x expr if expr then model else model query posterior x expectation x template data prior model query a grammar for probabilistic program templatesx float y c1 x c2 w ?
?
b ?
?
p ?
?
observe ?
?
w x b p y posterior w b p b linear regression template x y w gamma .
.
b normal .
.
p exponential .
observe normal w x b p y posterior w b p c linear regression example figure grammar and example for probfuzz input templates to be inserted here.
the second strategy randomly picks values that are close to the boundary values of the parameter ranges these values may be either legal or illegal and can stress test the sensitivity of pp systems to boundary conditions and numerical instabilities.
the developer can provide a probability that prefers one strategy over the other.
for instance in figure 4c data selector picks the values .
and .
as the parameters of the gamma distribution in the prior of w. similarly it could try generating programs where the second parameter of gamma which should be positive is .
or .
to test the capability of the pp system to identify wrong values.
generate the inputs outputs.
data selector uses the input range and formulas provided by the developer to compute expected outputs.
it then randomly generates the desired number of elements in the input vectors and computes the values in the output vectors.
.
translator translator produces a legal program in the language of the target pp system.
the inputs to the translator are the concrete program and data produced by generator.
each pp system has its own translator.
in addition translator takes a configuration file with the list of inference algorithms and a mapping of distributions to corresponding pp system specific api calls.
translator in edward.
first the inference selector chooses an inference algorithm that the pp system supports based on the concrete specification.
second the translator replaces distribution names in the input programs with the corresponding api call in edward and creates one ast node each for the input data x the model in the program and the selected inference.
third several ast nodes are created for the following one node for the posteriors or each prior depending on the inference algorithm to be run a node for a placeholder for x and optional one node for the proposals for each prior which is needed for some inference algorithms e.g.
the metropolis hastings mh sampling algorithm.
fourth a dict node is created which connects the node for each prior to its respective proposal and posterior nodes and a dict nodeis created which connects nodes for the data placeholder and the output data y. fifth the dict nodes from the last step are merged with the node for inference.
sixth the data node the model node and the inference node are combined together as the final ast.
finally this ast is converted into a python program.
translator in pyro.
the first two steps in the translator are the same as for edward select inference algorithm replace distributions with corresponding api calls and make ast nodes for x the model and the selected inference.
the third step is to create a function node for a pyro model a combination of posterior nodes for each prior which are then connected to the data node.
then a function node for a pyro guide is created with a posterior node for each prior.
next if the selected inference algorithm is a variational algorithm an optimization algorithm is chosen together with its parameters based on the concrete specification and a node is created.
finally a node for running the inference is created.
the generated ast is converted to a python program.
translator in stan.
stan s translator does not create asts.
rather each model is translated line by line to stan code stored in model.stan file with data stored in data.json file.
finally a file driver.py is generated and used to run the stan model.
.
program checker the task of the program checker is to decide whether output from running the generated programs may be indicative of bugs in the pp system on which the program was run.
for edward and pyro the generated python programs are run directly.
the driver.py script is run for stan.
program checker performs a battery of checks inspired by the bugs from our characterization study section crash checks they find problems with unexpected termination or assertion failures.
crash checks will output programs which crash as likely bug revealing since all programs generated by probfuzz are syntactically and semantically valid.
nanand overflow checks they will report programs that neither crash nor produce exceptions but contain nanas output values as observed in section they are often related to numerical 579testing probabilistic programming systems esec fse november lake buena vista fl usa and boundary checking problems.
programs which produce nan as output values are potentially bug revealing because it means that the pp system allowed invalid computations to succeed instead of warning the developers.
performance checks they report if one pp system converges much slower than other pp system.
differential testing with exact result these checkers aim to identify accuracy bugs by comparing the results of approximate inference with the exact result.
the exact result can be obtained in two ways using optional data generators or using exact inference engine.
for exact inference we translate programs to psi .
exact inference when it scales removes approximation and numerical errors modulo bugs in the exact inference tool.
this approach works when the generated programs are small.
differential testing with approximate results these checkers aim to identify accuracy bugs by comparing the differences in the results produced by different tools and different algorithms within a single tool or even different versions of the same algorithm e.g.
and different interfaces to the same inference algorithm.
result comparison across tools or algorithms is useful for accuracy and numerical bugs.
comparisons across different interfaces of the same pp system e.g.
rstan pystan can primarily help find language translation bugs.
the program checker issues a warning about a program from which the results of one approximate inference pp system differs significantly from all other approximate inference pp systems and the other systems produce similar outputs or if the outputs of all approximate inference differ from the expected output.
accuracy comparisons.
analysis of accuracy is a key challenge in testing pp systems.
the computations have various sources of noise some inference algorithms are randomized e.g.
mcmc while others make algorithmic approximations e.g.
variational inference .
in both cases there may be rounding errors or overflows.
to quantify the magnitude of errors probfuzz allows a developer to specify custom comparison metrics.
in this paper we compute an accuracy metric based on relative error of the mean.
symmetric mean absolute percentage error computes the distance between the means of the posterior distributions computed by two systems or comparing the result from one system to the exact result .
it is computed as smape x1 .
.
.
xn y1 .
.
.
yn nnx i xi yi xi yi the arguments x1 .
.
.
xnare the means produced by the first system and y1 .
.
.
ynare the means produced by the second system.
in contrast to the usual relative error which divides the difference by the value from one of the systems smape does not prefer the result of any of the systems and is always guaranteed to produce a result in the range .
a program may have an accuracy bug if the value of the metric is above a threshold which effectively acts as a knob for how many programs to inspect .
if so probfuzz reports the generated program as revealing a potential accuracy bug.
when more than systems are involved we do a pairwise comparison.
if only one of the pp systems shows a significant deviation from the others probfuzz reports that system as likely faulty.
quantitative evaluation we describe the research questions that we answer our experimental setup and the quantitative aspects of the results in this paper.
we answer the following research questions rq1 how many new bugs per category does probfuzz find?
rq2 how many categories of existing bugs does probfuzz find?
rq3 how sensitive is the accuracy metric to the threshold choice?
rq4 how does probfuzz compare with conventional fuzzing?
experimental setup.
for our experiments we used four templates.
we discussed linear regression template in section .
.
other templates include simple posterior template which samples from a single distribution with a prior for each of its parameters and conditions on data conditional template which chooses between two models based on the if expression and multiple linear regression template with a weight vector for the prior instead of scalar as in linear regression and conditioned on dimensional data sets.
we also varied the data vector sizes.
we generated programs per template for each tool.
we group the programs based on the determination that program checker makes and then randomly sample a subset of programs in each class for manual inspection.
to find performance bugs we randomly sampled for manual inspection the programs that did not run to completion in the default time out limit of mins.
for accuracy bugs we used the accuracy metric discussed in section .
to select wrong programs to manually inspect.
the threshold for smape that we used in selecting the programs for our manual inspection was .
.
we ran all experiments on an intel xeon .60ghz machine with cores and 32gb ram.
.
rq1 new bugs discovered by probfuzz table shows the number per category of the new bugs found during our evaluation of probfuzz.
columns exceptp are the pp systems in our study while the rows exceptp are the various categories for which found some bugs that we found.
bug categories were described in section .
we counted as bugs either as the number of distinct code locations where we made a fix in pull requests or one bug for each issue that we submitted to the developers without a corresponding pull request.
note that by counting each yet to be fixed submitted issue as one bug we are under counting the number of bugs in the code and the actual number of bugs that probfuzz found in our experiments is likely higher.
we submitted issues each one counts as one bug and pull requests which fixed bugs in the code.
the results show that the dimension boundary value bugs are the most common among the bugs that we found.
we provide more details in section .
about how prone the pp systems are to dimension boundary value bugs.
among the pp systems we found the least number of bugs in stan followed by edward and then pyro.
interestingly this matches the maturity of the pp systems.
we also discuss in section .
one step that stan developers have taken over the years to reduce the amount of bugs in this category.
one key benefit that probfuzz provides in the testing of pp system is the ability to find accuracy bugs and not just bugs that lead to crashes or invalid values e.g.
nanorinf in the output.
accuracy bugs are much more tricky to find and debug coming up 580esec fse november lake buena vista fl usa saikat dutta owolabi legunsen zixin huang and sasa misailovic table new bugs per category discovered by probfuzz category edward pyro stanp algorithmic accuracy dimension boundary numerical language translation 5p16 with oracles that catch them is quite involved and requires domain knowledge.
as shown in table we found potential accuracy bugs in all three pp systems during our manual inspection.
we reported all the bugs in table to the developers of each pp system.
so far developers have accepted rejected are still pending and was already fixed before we could submit it accepted bugs were in a single pull request to pytorch.
.
rq2 old bugs rediscovered by probfuzz this experiment checks whether probfuzz can catch a variety of previously fixed bugs that we identified during our characterization study section .
for each pp system we attempted to reproduce at least one bug per category such that they cover all categories of interest algorithmic accuracy dimension boundary value and numerical .
we did not target language translation bugs which are specific to each pp system and targeting them requires more involved back ends.
we first checked if these bugs may be reproduced by re running the tests that failed due to the bug or programs in the corresponding github issue.
we stopped if we could no longer run the tests programs.
we did not try to reproduce bugs that cannot be exercised by our four templates.
since some older versions of the pp systems use different syntax and api to specify models or have since undergone major changes we had to create four additional versions of translator for bugs .
in addition we found the versions of the infrastructure pytorch and tensorflow which were in use in the older versions.
for accuracy and numerical bugs we manually reasoned whether the difference was caused by the bug.
table shows the numbers and links to bugs that we successfully reproduced with probfuzz.
for each of these bugs probfuzz generated a program and the data to exercise it.
each cell contains the bug count in each category per pp system.
in addition each cell contains the exact reference to the commit with the bug fix.
the results show that probfuzz successfully found bugs in eight out of nine categories of interest.
out of these bugs six were found using the simple posterior template three using the linear regression template and one using multiple linear regression template .
overall these results demonstrate that probfuzz could have caught a variety of existing bugs had it been available prior to the discovery of those bugs.
comparison of tables and shows that probfuzz was able to reproduce existing bugs in categories where we did not find any new bug on recent versions of the pp systems e.g stan dimension boundary .
also probfuzz reproduced the only previously known numerical bugs in edward and pyro from our characterization study.
.
rq3 sensitivity of accuracy threshold the number of programs to inspect depends on the threshold set for the accuracy metric.
figure presents the sensitivity of the numbertable old bugs per category rediscovered by probfuzz category edward pyro stanp algorithmic accuracy dimension boundary numerical language translation n a n a n a n ap3 of programs reported to potentially reveal accuracy problems as a function of the bound on the smape metric for the linear regression template.
the x axis presents the threshold.
the y axis presents the fraction of the programs whose accuracy metric compared to the reference solution is above the threshold.
for the computation we removed the programs that crashed the programs that resulted in nan and the programs that timed out.
the results show that the threshold value can serve as a knob for the fraction of the programs to return.
for instance if the threshold is .
then the number of programs with large accuracy loss is less than for pyro and stan and around for edward.
stan shows an interesting trend of having many programs that have small accuracy loss of the mean while edward and pyro have more programs that have larger accuracy differences.
.
.
.
.
.
.
.
.
smapefraction of programsedward .
.
.
.
.
.
.
.
smapepyro .
.
.
.
.
.
.
.
smapestan figure sensitivity of accuracy metric .
rq4 benefit of domain knowledge we compared our results with an uninformed fuzzer that does not utilize domain knowledge about distributions and legal ranges.
table shows the detailed comparison for generated programs per tool per template.
each cell contains the percentage of generated programs that produced results without crashing numerical errors or timeout for uninformed u or informed i fuzzer.
on average less than stan .
edward .
pyro .
of programs generated by the uninformed fuzzer produce useful results compared to over stan .
edward .
pyro .
with probfuzz.
as such uninformed approach can be fit for boundary condition bugs e.g.
when a system fails to recognize a program with wrong values but it will not be efficient for bugs that can be revealed by only well formed probabilistic programs.
table comparison of informed vs uninformed fuzzing template stan edward pyro u i u i u i simple .
.
.
.
.
.
lr .
.
.
.
.
.
mlr .
.
.
.
.
.
conditional .
.
.
.
.
.
581testing probabilistic programming systems esec fse november lake buena vista fl usa qualitative analysis during our development and evaluation of probfuzz we encountered several potential bugs in pp systems for which we created fixes and submitted pull requests to the developers.
we made fixes in edward pyro stan and also contributed patches to the underlying frameworks pytorch and tensorflow.
we present interesting cases lessons learned and developer responses.
.
dimension boundary bugs are common dimension boundary value bugs accounted for previously unknown bugs and bugs in our characterization study.
in pyro we found bugs in this category.
one of these bugs in pyro would lead to a crash whenever the input data is of size another bug caused an overflow in the adam optimizer.
we also found similar previously unknown dimension boundary value bugs in edward four bugs were also due to failure to check that parameter values are in the correct range.
interestingly probfuzz did not find any previously unknown dimension boundary value bugs in stan despite the fact that our characterization study revealed eight dimension boundary value bugs that were previously reported in stan.
we attributed this to stan s relative maturity compared with pyro and edward.
indeed since march stan developers have added a milestone to every major release with the title make sure all distributions throw exceptions at undefined boundaries .
lesson learned the similarity of dimension boundary value bugs found across pp systems suggests that that these bugs are commonly introduced by the developers of the pp systems that we studied.
going forward developers should continuously test their probabilistic codes for this kind of problems.
automated testing such as probfuzz can be quite effective for these problems.
.
accuracy problems are hard to analyze accuracy problems can be difficult to identify and debug and they can have serious consequences.
section presented one such problem.
while this problem was present in stan it is interesting that stan s precursor bugs which shares most of its modeling syntax and principles computes the correct result.
for a non expert it is often hard to figure out the reasons behind this discrepancy.
next we provide some insights into how we analyzed this particular case.
we observed that the error is reproduced for any value of the parameters of beta distribution which is the prior for p. stan produced warning messages stating that the random variable used for computing the beta logpdf in a particular step is negative but was expected to be positive.
the stan manual describes such messages as follows warning messages arise in circumstances from which the underlying program can continue to operate .
stan often converges to the correct result despite such warnings but in this case it did not.
when such warnings persist stan developers suggest investigating the arithmetic stability of the stan program .
one way to address the accuracy problem is to change the model.
stan developers often recommend to manually bound the variables that have finite support .
for pin figure 2a we can set the bounds as follows real lower upper p this makes stan produce the correct output .
.
the origin of the problem lies in the way stan does sampling.
for any sampling statement of the form p beta a b stan computes the log probability as target beta lpdf p a b and updates the log density logpdf of the model.
beta distribution has a support of .
ifpis assigned a value outside this range it causes logpdf to be undefined which affects convergence.
when the bounds are manually bound stan ensures that the parameter is in the valid range.
such properties that are important for inference are not enforced.
stan s development has been influenced by folk theorem which implies that in case of a wrong inference the problem can be overcome by changing the model and moreover the inference algorithms should not be made to work for various uncommon extreme program data cases .
however the folk theorem assumes that a developer has an intuition about the correct result which may often not be the case as the pp systems are becoming mainstream.
to help developers overcome such challenges pp systems should provide additional information about the problems in the interaction between the model and the system.
recently stan developers proposed a pedantic mode as a way to diagnose various errors and bad modeling practices before running the inference including range checks.
we find this an interesting direction that can demonstrate the power of both probabilistic reasoning and static analysis similar to lightweight static analyses in the traditional software development e.g.
.
lesson learned debugging accuracy problems requires not only domain knowledge but also a reasonable understanding of the pp system under test.
the warning messages often provide hints if there is something wrong with the model.
but the messages might not be informative enough to guide the user in fixing the model.
this parallels the observations from compiler research on the importance of informative warnings for subsequent developer action .
going forward we note a promising application of static analysis to provide explicit hints about the model and its interaction with the inference algorithm without having to run the program.
like in compilers they could provide useful warnings to alert developers to potentially problematic code fragments and suggestions to eliminate the warning in the probabilistic setting.
tools like probfuzz have the potential to empirically discover the kinds of models that do not work well with a specific inference algorithm and inform such static analysis.
.
fixing bugs in pp systems is non trivial we found out that fixing the bugs even the relatively straightforward dimension boundary value ones is highly non trivial and often involves changes to the design of the infrastructure e.g.
pytorch and tensorflow that pp systems are built on.
as an example of a non trivial problem we reported a bug to stan developers which appears in some situations when the model is provided with an empty data array.
in those cases the programs fails unexpectedly.
the developers acknowledged the issue immediately but even after an extensive discussion the developers still have not been able to resolve the problem after several months.
in edward we submitted a pull request to ensure that the n samples parameter of klpq inference was .
the developers asked for the same fix to be made in several places in the klqp inference cool!
can you also add this change to klqp.py for each initialize method?
we did as requested and our pull request was accepted and merged.
582esec fse november lake buena vista fl usa saikat dutta owolabi legunsen zixin huang and sasa misailovic in pyro we identified that many distributions used from pytorch do not have range checks.
as we were discussing the potential fix with the developers a pytorch contributor independently started implementing their version of the fix.
we discovered that the contributor s proposed fix had several bugs.
our tests that revealed bugs in the contributor s fix were driven by the failures that we had seen while running probfuzz generated programs.
consequently the contributor agreed to let us lead the fix which has been approved for merging to the pytorch repository.
lesson learned bugs in pp systems are not trivial to fix.
tests generated by probfuzz can help identify the causes of the problems.
moreover automated testing can help discover incorrect and incomplete fixes.
.
fixes extend across system boundaries as we were analyzing bugs and developing fixes we found out that the failures that manifest in a pp system are often due to faults in the underlying infrastructure pytorch and tensorflow .
therefore some of the fixes we submitted were accepted by the developers of the underlying infrastructure.
the accepted changes include a part of distribution checks in pytorch plus many accompanying test cases and a fix to a bound for error reporting in tensorflow.
we submitted our initial issues to the developers of edward and pyro and they would typically direct us to propagate the fix to the underlying infrastructure.
for edward the developer s response to our request to enable detailed checking of distribution ranges was that s an interesting suggestion...potentially useful utility.
can you raise this in tensorflow?
in pyro we submitted a pull request which added a check to prevent division by zero errors.
multiple pyro developers responded and asked us to make the checks in pytorch so more people in the community will benefit hello.
thanks for the contribution!
... a more appropriate place... would be in pytorch... thanks this looks helpful...thumbs up!
i agree the pytorch folks would appreciate better error checking then a larger community could benefit from this fix.
the fix was accepted by pytorch.
lesson learned in pp systems the errors and fixes can often extend across the boundaries of individual systems.
probfuzz was effective in identifying such bugs since it analyzed and compared the end to end results of these composed systems.
discussion comparison with traditional testing approaches.
inference bugs often require probabilistic reasoning and reasoning about accuracy using e.g.
domain specific oracles metamorphic relations or multiple implementations.
as such this category of bugs can be hard to catch using traditional testing techniques.
common techniques such as coverage based testing would have problems because many of these bugs were caused by faults of omission .
further even bugs in covered code may require special values to manifest.
mutation testing of pp system code can potentially identify some bugs that result in program crashes or special values but non equivalent mutant survivals may indicate valid approximations rather than bugs especially as tests in pp systems often only check whether the result lies in a loosely defined interval.
for the other bug categories we give examples of previously unknown bugs that illustrate the advance of probfuzz over traditional testing approaches.
for example a dimension boundary value bugin pyro manifested only when required parameters in two different functions were simultaneously out of acceptable ranges .
conventional boundary value analysis that targets one function at a time will not reveal this bug.
as another example in edward intermediate floating point values produced by the sgld inference algorithm led to nanoutput when those values are close enough to the support bound .
traditional boundary value analysis may need to try many values near the bound to catch this bug.
this bug remains open even after two workarounds that required advanced domain knowledge from the edward developers.
a language translation bug in stan led to program crashes only on empty intarrays in the data but not on empty real arrays .
empty arrays are allowed in the data.
the root cause of the bug was that empty intarrays were implemented to be of data type float .
interestingly the bug does not manifest in stan itself but in stan s pystan and rstan front ends.
without the combination of domain knowledge on valid data elements and fuzzing it will be difficult to catch such bugs with traditional testing techniques.
scope.
in our experiments we used four templates which focused on simple probabilistic models.
simple models can help developers understand potentially faulty executions and they were effective in finding bugs in the pp systems but we did not aim for completeness of models in our evaluation.
going forward pp system developers may also be interested in other common models that can be represented as templates in our language e.g.
hierarchical models mixture models and can be used to test various inference procedures general or specialized for different model classes.
however probfuzz cannot generate arbitrary probabilistic programs since its template language does not support while loops.
also probfuzz is not suited for bugs that require precise analysis e.g.
.
threats to validity.
they include internal external and construct.
internal.
the results of our bug study depend on the set of pp systems and bugs we examined.
we mitigated this risk by studying real bugs in three state of the art pp systems.
we may have wrongly characterized existing bugs as being inference related.
to mitigate this two coauthors independently inspected the bugs and when possible the corresponding github issues.
we only mark a bug to be inference related if both coauthors eventually agree thus achieving a conservative estimate of the number of inferencerelated bugs.
we mitigate probfuzz implementation errors with unit testing.
as differential testing may wrongly flag a program as potentially buggy so we had multiple rounds of discussion among ourselves and finally reported potential bugs to the pp system developers to make the final decision.
external.
the results of the characterization study and probfuzz may not generalize to all pp systems.
certain aspects of our experimental design help to mitigate this risk.
the three pp systems are being actively developed well tested and adopted.
we also demonstrated that probfuzz can reproduce existing bugs in each of the three bug categories across the pp systems.
construct.
probfuzz is designed to catch the categories of bugs identified by our study and may not find arbitrary bugs in pp systems.
discovery of these bugs is not exclusive to probfuzz.
other general and emerging testing techniques can in principle find some of the bugs identified in our evaluation.
583testing probabilistic programming systems esec fse november lake buena vista fl usa related work verification and analysis of probabilistic programs.
there are various approaches for verification of probabilisitic programs including probabilistic abstract interpretation symbolic execution probabilistic model checking and other methods .
unlike these systems probfuzz aims to find bugs in the systems on which probabilistic programs run and not for debugging or analyzing probabilistic programs.
program generation for compiler and system testing.
several techniques have been proposed for generating programs that are used in system testing.
these include techniques for generating programs to test compilers and to test refactoring engines and symbolic execution engines .
probfuzz also generates programs but does so for a different class of systems pp systems which are characterized by various probabilistic constraints on how to construct programs and measure accuracy of the output instead of binary correctness .
one technical difference between probfuzz and earlier program generation approaches is that probfuzz can generate programs in multiple languages we currently generate stan and python from probfuzz but more can be easily added.
lastly probfuzz generates both programs and the data needed to run the programs whereas all prior techniques generate only the programs for compiler and system testing or only the data for testing programs .
fuzzing.
researchers have previously proposed many fuzzing techniques .
grammar based fuzzers encode knowledge about the structure of valid programs more generally inputs but have no knowledge about the domain for which programs are typically written.
the closest fuzzing approach that we found to probfuzz in terms of encoding domain knowledge is langfuzz .
langfuzz improves grammar based fuzzing by first generating valid programs according to the grammar and then mutating the programs based on knowledge about programs that previously caused invalid behavior.
therefore langfuzz incorporates domain knowledge in the form of historical invalid behaviors.
in contrast to langfuzz the generation of programs by probfuzz already incorporates domain knowledge without needing to perform any mutation or consider history.
differential and metamorphic testing.
differential testing or multiple implementation testing use multiple implementations as oracles to find programs that can likely reveal bugs in pp system.
probfuzz uses such approach in its program checker.
many problems in machine learning do not have a reference result known as the no oracle problem .
one solution to this problem is metamorphic testing where metamorphic relations between the inputs and outputs of a program or function are leveraged to find inputs which cause outputs to diverge.
because metamorphic relations are hard to design srisakaokul et al.
recently proposed multipleimplementation testing of supervised machine learning algorithms to find bugs.
implementations which classify differently from the majority are considered potentially buggy.
our differential testing of multiple inference algorithms is similar to .
conclusion we presented the first study of existing bugs in probabilistic programming systems pp systems and proposed probfuzz for testing for such bugs.
probfuzz generates probabilistic programs from a user specified template for three pp systems edward pyro and stan.
our study of historical bugs in edward pyro and stan showed that numerical bugs accuracy bugs and dimensional and boundaryvalue bugs form the majority of bugs.
we demonstrated the ease of extending probfuzz by supporting several pp system versions in our study and the applicability of probfuzz by showing that it can find existing bugs in the aforementioned categories in all three pp systems.
probfuzz is already providing practical value we reported previously unknown bugs that we found by running probfuzz on recent versions of the three pp systems.
we created pull requests with fixes for many of these bugs of which have been accepted by the developers.
we believe that probfuzz opens a new line of research on testing probabilistic programming systems.