dire a neural approach to decompiled identifier naming jeremy lacomis pengcheng yin edward j. schwartz miltiadis allamanis claire le goues graham neubig bogdan v asilescu carnegie mellon university.
jlacomis pcyin clegoues gneubig cs.cmu.edu vasilescu cmu.edu carnegie mellon university software engineering institute.
eschwartz cert.org microsoft research.
miallama microsoft.com abstract the decompiler is one of the most common tools for examining binaries without corresponding source code.
it transforms binaries into high level code reversing the compilation process.
decompilers can reconstruct much of the information that is lost during the compilation process e.g.
structure and type information .
unfortunately they do not reconstruct semantically meaningful variable names which are known to increase code understandability.
we propose the decompiled identifier renaming engine dire a novel pr obabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler .
we also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming which we use to create a corpus of unique x86 binaries generated from c projects mined from g ithub.1our results show that on this corpus dire can predict variable names identical to the names in the original source code up to .
of the time.
i. i ntroduction software reverse engineering is the problem of understanding the behavior of a program without having access to its source code.
reverse engineering is often used to predict the behavior of malware discover vulnerabilities and patch bugs in legacy software .
for malware and malicious botnets reverse engineering enables understanding and response and helps identify and patch infection vectors.
for example by reverse engineering the torbig botnet which caused 180k infections and collected 70gb of credit card bank account information responders were able to predict future domain names that bots would contact and redirect the bots to servers under the responders control .
reverse engineering can also help identify who created a piece of malware as was done for the uroburos rootkit which captured files and network traffic while propagating over networks of companies and public authorities and estimate the extent of infection .
one of the main tools reverse engineers use to inspect programs is the disassembler a tool that translates a binary to low level assembly code.
disassemblers range from simple tools like gnu binutils objdump to more advanced tools like ida which can be used interactively and have more sophisticated features.
however reasoning at the assembly level requires considerable cognitive effort even with 1data available at advanced features .
more recently reverse engineers are employing decompilers such as hex rays and ghidra which reverse compilation by translating the output of disassemblers into code that resembles highlevel languages such as c to reduce the cognitive burden of understanding assembly code.
these state of the art tools are able to use program analysis and heuristics to reconstruct information about a program s variables types functions and control flow structure.
even though decompiler output is more understandable than raw assembly decompilation is often incomplete.
compilers discard source level information and lower its level of abstraction in the interest of binary size execution time and even obfuscation.
comments variable names user defined types and idiomatic structure are all lost at compile time and are typically unavailable in decompiler output.
in particular variable names which are highly important for code comprehension and readability become nothing more than arbitrary placeholders such as var1 andvar2 .
in this work we present dire decompiled identifier renaming engine a novel neural network approach for assigning meaningful names to variables in decompiled code section iii .
to build dire we rely on two key insights.
our first insight is that software is natural i.e.
programmers tend to write similar code and use the same variable names in similar contexts .
therefore because of this repetitiveness if given a large enough training corpus one can learn appropriate variable names for a particular context.
prior approaches exist to predict natural variable names from both source code and compiled executables .
however approaches to predict variable names from executables either operate directly on the binary semantics or on the lexical output of the decompiler .
the former ignores the rich abstractions that modern decompilers are able to recover.
the latter is an improvement but a lexical program representation is by its very nature sequential and lacks rich structural information that could be used to improve predictions.
in contrast dire uses the extended context provided by the decompiler s internal abstract syntax tree ast representation of the decompiled binary which encodes additional structural information.
to train such models one needs training data that specifies 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
what names are natural in what contexts.
our second key insight is that unlike other domains where creating training data often requires manual curation e.g.
machine translation it is possible to automatically generate large amounts of training data for identifier name prediction to that end we mine open source c code from g ithub compile it with debugging information such that the binaries preserve the original names and decompile those binaries so that the output contains the original names.
we then strip the debug symbols decompile the binary again and identify the alignment between the identifiers in the two versions of the decompiler outputs.
while this is conceptually straight forward the two outputs are not simply renamings making the process of calculating these alignments far from trivial.
prior work identified alignments based entirely on heuristics .
in contrast we observe that the set of instruction addresses that access each variable uniquely identifies that variable and this can be used to generate accurate alignments section iv .
with these insights we train and evaluate dire on a large dataset of c code mined from g ithub showing that we can predict variable names identical to those chosen by the original developer up to .
of the time.
in short we contribute decompiled identifier renaming engine dire a technique for assigning meaningful names to decompiled variables that outperforms previous approaches.
a novel technique for generating corpora suitable for training both lexical and graph based probabilistic models of variable names in decompiled code.
a dataset of decompiled x86 functions and parse trees annotated with gold standard variable names.
ii.
b ackground before diving into the technical details of our approach we start with some background on decompilation statistical models of source code and the two particular classes of deep learning models we rely on recurrent neural networks rnns and gated graph neural networks ggnns .
a. decompilation at a high level a compiler generates binaries from source using a pipeline of processing stages and decompilers try to reverse this pipeline using various techniques .
typically a binary is first passed through a platform specific disassembler .
next assembly code is typically lifted to a platformindependent intermediate representation ir using a binaryto ir lifter.
the next stage is the heart of the decompiler and is where a number of program analyses are used to recover variables types functions and control flow abstractions which are ultimately combined to reconstruct an abstract syntax tree ast corresponding to an idiomatic program.
finally a code generator converts the ast to the decompiled output.
decompilation is more difficult than compilation because each stage of a compiler loses information about the original program.
for example the lexing parsing stage of the compiler does not propagate code comments to the ast.
similarly converting from the ast to ir can lose additional1inti 2for i i i 4z i 1intn 2while n 3x n 4n var1 dword ptr var2 dword ptr ... mov jmploc 4a5 49b loc 49b 49b mov eax 49e add eax 4a1 add 4a5 loc 4a5 4a5 cmp 4a9 jleloc 49b fig.
two different c loops that compile to the same assembly code.
note the normalized structure and names.
information.
this loss of information allows multiple distinct source code programs to compile to the same assembly code.
for example the two loops in fig.
are reduced to the same assembly instructions.
the decompiler cannot know which source code was the original but it does try to generate code that is idiomatic using heuristics to increase code readability.
for example high level control flow structures such as while loops are preferred over goto statements.
the choice of which code to generate is largely heuristic but can be informed by the inclusion of dw arf debugging information .
this debugging information which can optionally be generated at compile time greatly assists the decompiler by identifying function offsets types of variables identifier names and user defined structures and unions.
b. statistical models of source code a wide variety of statistical models for representing source code have been proposed based on the naturalness of software .
this key property states that source code is highly repetitive given context and is therefore predictable.
statistical models capture the implicit knowledge hidden within code and apply it to build new software development tools and program analyses e.g.
for code completion documentation generation and automated type annotation .
predicting variable names is no exception.
work has shown that statistical models trained on source code corpora can predict descriptive names for variables in a previously unseen program given the contextual features of the code the variable is used in.
these naming models can help to distill coding conventions or analyze obfuscated code .
several classes of statistical models have been used for renaming including n grams conditional random fields crfs and deep learning models .
two recent approaches aim to suggest informative variable names in decompiled code.
our prior work proposed a lexicaln gram based machine translation model that operates on decompiler output.
that approach used heuristics to align variables in the decompiler output and original source which are needed for training and is able to exactly recover .
of the original names in the test set.
contemporaneously he et al.
proposed a two step approach that operates on a stripped binary rather than the decompiler output.
first the authors predict whether a low level register or a memory offset authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
maps to a variable at the source level.
then using structured prediction with crfs they predict names and types for the mapped variables.
.
of the variables in the test set for which the first step succeeded could be recovered exactly.
c. neural network models our approach builds on two advances in statistical models for representing source code recurrent neural networks rnns and gated graph neural networks ggnns .
recurrent neural networks rnns are networks where connections between nodes form a sequence .
they are typically used to process sequences of inputs by reading in one element at a time making them well suited to sequences such as source code tokens.
in this work we use long shortterm memory lstm models a variant of rnns widely used in text processing.
formally an lstm has the following structure given a sequence of tokens xi n i an lstm flstm processes them in order maintaining a hidden state hifor each subsequence up to token xiusing the recurrent function hi flstm emb xi hi where emb is an embedding function mapping xiinto a learnable vector of real numbers.
as we will elaborate later in section iii we use two types of lstms in dire encoding lstms and decoding lstms.
an encoder lstm reads the input sequence e.g.
a sequence of source code tokens as in section iii b1 and encodes it into continuous vectors while a decoder lstm takes these vectors and generates the output sequence e.g.
the sequence of predicted names for all identifiers as in section iii c .
gated graph neural networks while lstms are useful for modeling sequences they do not capture any additional structural information.
within the decompilation task structured information provided by the ast is a natural information source about choice of variable names.
for this purpose we also employ structural encoding of the code using ggnns a class of neural models that map graphs to outputs .
at a high level ggnns are neural networks over directed graphs.
initially we associate each vertex with a learned or computed hidden state containing information about the vertex.
ggnns compute representations for each node based on the initial node information and the graph structure.
formally let g angbracketleftv e angbracketrightbe a directed graph describing our problem where v vi is the set of vertices and e vi mapsto vj t is the set of typed edges.
let nt vi denote the set of vertices adjacent to viwith edge type t. in a ggnn each vertex viis associated with a state hg i t indexed by a time step t. at each time step t the ggnn updates the state of all nodes in vvia neural message passing nmp .
concurrently for each node viat timet nmp is performed as follows first for each vj nt vi we compute a message vector mvj mapsto vi t wt hg j t where wtis a typespecific weight matrix.
then all mv mapsto vi are aggregated and summarized into a single vector xg ivia element wise mean pooling xg i meanpool mvj mapsto vi t vj nt vi t .token stream astdecompiler direbinaryv ariable names fig.
high level overview of our approach.
finally the state of every node viis updated using a nonlinear activation function f hg i t f xg i hg i t .
ggnns use a gated recurrent unit gru update function fgru introduced by cho et al.
.
by repeatedly applying nmp for t steps each node s state gradually represents information about that node and its context within the graph.
the computed states can then be used by a decoder similarly to the lstm based decoder architectures.
as in lstms all ggnn parameters parameters of fgru and the wts are optimized along with the rest of the model.
iii.
t hedire a rchitecture we start with an overview of our approach then dive into the technical details of each component.
a. overview we designed dire to work on top of a decompiler as a plugin that can automatically suggest more informative variable names.
we use hex rays a state of the art industry decompiler though our approach is not fundamentally coupled to hex rays and can be adapted to other decompilers.
fig.
gives a high level overview of our workflow.
first a binary is passed to a decompiler which decompiles each function in the binary.
for each function our plugin traverses the ast inserting placeholders at variable nodes.
this produces two outputs the ast and the tokenized code.
these outputs are provided as input to our neural network model dire which generates unique variable names for each each placeholder in the input.
the decompiler output can then be rewritten to include the suggested variable names.
fig.
gives an overview of the neural architecture.
dire follows an encoder decoder architecture an encoder neural network section iii b first encodes the decompiler s output both the sequence of decompiled code tokens and its internal ast and computes distributed representations i.e.
real valued vectors or embeddings for each identifier and code element.
these encoded representations are then consumed by a decoder neural network section iii c that predicts meaningful names for each identifier based on the contexts in which it is used.
the key takeaway is that dire uses both lexical information obtained from the tokenized code as well as structural information obtained from the corresponding asts.
this is achieved by using two encoders a lexical encoder section iii b1 and a structural encoder section iii b2 to separately capture the lexical and structural signals in the decompiled code.
as we will show this combination of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
lexical and structural information allows dire to outperform techniques that rely on lexical information alone .
b. the encoder network each encoder network in dire outputs two sets of representations acode element representation for each element in the decompiler s output.
depending on the type of the encoder a code element will either be a token in the surface code for the lexical encoder or a node in the decompiler s internal ast for the structural encoder .
an identifier representation for each unique identifier defined in the input binary which is a real valued vector that represents the identifier in the neural network.
the lexical and structural representations are then merged to generate a unified encoding of the input binary dashed boxes in fig.
.
by computing separate representations for code elements and identifiers the dire decoder can better incorporate the contextual information in the encodings of individual code elements to improve name predictions for the different identifiers see section iii c. lexical code encoder the lexical encoder sequentially encodes the tokenized decompiled code projecting each token xiinto a fixed length vector encoding xi.
specifically the lexical encoder uses the sub tokenized code as the input where a complex code token e.g.
the function name mystrcopy i s automatically broken down into sub pieces e.g.
my str and copy using sentencepiece based on sub token frequency statistics.
sub tokenization reduces the size of the encoder s vocabulary and thus its training time while also mitigating the problem of rare or unknown tokens by decomposing them into more common subtokens.
we treat the placeholder and reserved variable names e.g.
var1 var2 and the decompilerinferred name result in the decompiler s output as special tokens that should not be sub tokenized.
dire implements the lexical encoder using lstms described in section ii c1 .
we use a bidirectional lstm the forward network flstm processes the tokenized code xi n i sequentially.
the backward lstm processes the input tokenized code in backward order producing a backward hidden state hifor each token xi.
intuitively a bidirectional lstm captures informative context around a particular variable both before and after its sequential location.
element representations we encode a token xiby concatenating its asssociated state vectors i.e.
xi a common strategy in source code representations using lstms .
for a particular token xiwe compute the forward resp.
backward representation using both its embedding and the hidden states of its preceding resp.
succeeding tokens.
this is important because the resulting encoding xicaptures both the local and contextual information of the current token and its surrounding code.
to compute the identifier representation vfor each unique identifier v we collect the set of subtoken representations hv ofv and perform an element wise mean over hvto get a fixed length representation v meanpool hv .
structural code encoder the lexical encoder only captures sequential information in code tokens.
to also learn from the rich structural information available in the decompiler ast dire employs a gated graph neural network ggnn structural encoder over the ast section ii c2 .
this requires a mechanism to compute initial node states as well as design choices of which ast edges to consider in the node encoding a initial node states the initial state of a node vi hg i t 0is computed from three separate embedding vectors each capturing different types of information of vi a n embedding of the node s syntactic type e.g.
the root note in the ast in fig.
has the syntactic type block .
for a node that represents data e.g.
variables constants or an operation on data e.g.
mathematical operators type casts function calls an embedding of its data type computed by averaging the embeddings of its subtokenized type.
for instance the variable node var1 in fig.
has the data type char its embedding is computed by averaging the embeddings of the type subtokens char and .
for named nodes an embedding of the node s name e.g.
the root node in fig.
has a name mystrcopy computed by averaging the embeddings of its content subtokens.
the initial state hg v t 0is then derived from a linear projection of the concatenation of the three separate embedding vectors.
for nodes without a data type or name we use a zero valued vector as the respective embedding.
b graph edges our structural encoder uses different types of edges to capture different types of information in the ast.
besides the simple parent child edges solid arrows in the ast in fig.
in the original ast we also augment it with additional edges we add an edge from the root block node containing the function name to each identifier node.
the function name can inform names of identifiers in its body.
in our running example the two arguments var1 andvar2 defined in themystrcopy function might indicate the source and destination of the copy.
this type of link function name to args.
in fig.
captures these naming dependencies.
to capture the dependency between neighboring code we add an edge from each terminal node to its lexical successor successor terminal .
to propagate information among all mentions of an identifier we add a virtual supernode rectangular node labeledvar1 for each unique identifier vi and edges from mentions of vito the supernode super node link .
finally we add a reverse edge for all edge types defined above modeling bidirectional information flow.
c representations for the element representation we use the final state of the ggnn for node ni hg i t a si t s representation ni hg i t the recurrent process unrolls t times t for all our experiments .
for the identifier representation for each unique identifier vi its representation viis defined as the final state of its supernode as the encoding ofvi.
since the supernode has bidirectional connections to all the mentions of vi its state is computed using the states of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
var2char my str copy char var1.
.
.
.
.
block if asg call result strcopy var1land var1 var2asg result oll var1name mystrcopyvar1identifier representationsvar2result charcode element representations ... my str blockiflandasg ...lexical encoder structural encoder.
.
.
destaddr i src var1var1var2 var1 encoder networkdecoder networklexical encoder structural encoderattention type char char mystrcopy char var1 char var2 char result if var1 var2 result strcopy var1 var2 else result 0ll return result idiomatic names var1 destaddr var2 src result resultdecompiled code example inputcondition elsebody ast function name to args.
successor terminal super node link fig.
overview of dire s neural architecture.
for clarity we omit the data flow links in the ast in the structural encoder.
all its mentions.
therefore vicaptures information about the usage of viin different occurrences.
combining outputs of lexical and structural encoders the lexical and the structural encoders output a set of representations for each identifier and code element.
in the final phase of encoding we combine the two sets of outputs.
code elements are combined by unioning the lexical set of code tokens and structural set of ast nodes of element representations as the final encoding of each input code element identifiers are combined by merging the lexical and structural representations of each identifier vusing a linear transformation as its representation.
c. the decoder network the decoder network predicts names for identifiers using the representations given by the encoder.
as shown in fig.
the decoder predicts names based on both the representations of identifiers and contextual information in the encodings of code elements.
specifically as with the encoder we assume an identifier name is composed of a sequence of sub tokens e.g.
destaddr mapsto dest addr see section iii b1 .
the decoder factorizes the task of predicting idiomatic names to a sequence of time indexed decisions where at each time step it predicts a sub token in the idiomatic name of an identifier.
for instance the idiomatic name for var1 destaddr is predicted in three time steps s1throughs3 using sub tokens dest addr and i the special token i denoting the end of the token prediction process .
once a full identifier name is generated the decoder continues to predict other names following a pre order traversal of the ast.
as we will elaborate in section iv not all identifiers in the decompiled code will be labeled with corresponding ground truth idiomatic names since the decompiler often generates variables not present in the original code.
dire therefore allows an identifier s decompiler assigned name to be preserved by predicting a special identity token.
the probability of generating a name is therefore factorized as the product of probabilities of each local decision while generating a sub token yt p y x t productdisplay t 1p yt y t x wherexdenotes the input code and yis the full sequence of sub tokens for all identifiers and y tdenotes the sequence of sub tokens before time step t. we model p yt y t x using an lstm decoder following the parameterization in .
specifically to predict each subtokenyt at each time step t the decoder lstm maintains an internal state stdefined by st flstm st where denotes vector concatenation.
the input to the decoder consists of two representations the embedding vector of the previously predicted name yt and the encoder s representation of the current identifier to be predicted vt. our decoder also uses attention to compute a context vector ct generated by aggregating contextual information from representations of relevant code elements.
ctis computed by taking the weighted average over encodings of ast nodes and surface code tokens for each current sub tokenized name yt.
the decoder s hidden state is then updated using the context vector incorporating the contextual information into authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the decoder s state st w where w is a weight matrix.
then the probability of generating a sub token yt is p yt exp parenleftbig y intercal t st parenrightbig summationtext y primeexp parenleftbig y prime intercal st parenrightbig d. training the neural network since dire is constructed from neural networks training data is required to learn the weights for each neural component.
our training corpus is a set d angbracketleftxi yi angbracketright consisting of pairs of code xand sub token sequences y denoting the decoder predicted sequence of identifier names.
dire is optimized by maximizing the log likelihood of predicting the gold sub token sequence yifor each training example xi summationdisplay angbracketleftxi yi angbracketrightlogp yi xi summationdisplay angbracketleftxi yi angbracketright yi summationdisplay t 1wt logp yi t xi whereyi tdenotes the t th sub token in the decoder s prediction sequence yi.
as discussed in section iii c there are intermediate variables in the decompiled code.
to ensure the decoder network will not be biased towards predicting identity for other identifiers we use a tuning weight wi and set it to .
for sub tokens that correspond to intermediate variables and .
otherwise .
iv .
g enera tion of training data training dire requires a large corpus of annotated data.
fortunately it is possible to create this corpus automatically starting from a large repository of existing c source code.
at a high level each entry in our corpus corresponds to a source code function and consists of the information necessary to train our model.
an entry in the training corpus is illustrated in fig.
.
each entry contains three elements a the tokenized code with variables replaced by an id that uniquely identifies the variable in the function b the decompiler s ast section ii a modified to contain the same unique variable ids and c a lookup table mapping variable ids to both the decompiler and developer assigned names.
it is important to assign a unique variable name to each variable to disambiguate any shadowed variable definitions.
the tokenized code and ast representations are used in both the model s input and output.
the input representation uses the decompiler assigned names while the output uses the developer assigned names.
generating the placeholders and decompiler chosen names is relatively straightforward.
first a binary is compiled normally and passed to the decompiler.
next for each function we traverse its ast and replace each variable reference with a unique placeholder token.
finally we instruct the decompiler to generate decompiled c code from the modified ast tokenizing the output.
thus we have tokenized code an ast and a table mapping variable ids to decompiler chosen names.
the remaining step mapping developer chosen names to variable ids is the core challenge in automatic corpus generation.
following our previous approach we leverage the decompiler s ability to incorporate developer chosen identifierfor var1 ...var2 var3 ... a tokenized decompiled code with variable placeholders.
var1var2var4 var2var3var1 b ast with placeholders.id decompiler developer v1 ans v2 size ii ptr head c v ariable lookup table.
fig.
entry in the training corpus.
each corresponds to a function and contains a tokenized code b the ast both with variables replaced with unique ids and c a lookup table containing decompiler and developer assigned names.
block 49b while 49e block4a9 sle 4a5 num 94a5 v14a1 preinc 4a1 v1492 asg v1492 num 49e expr 49e asgadd 49e v249e v1 a ast without dw arf.
for 49e block4a9 sle4a1 preinc 4a1 i492 asg i492 num 049e expr 49e asgadd 49e z49e i4a5 num 94a5 i b ast with dw arf.
fig.
decompiler asts for the code in fig.
.
hexadecimal numbers indicate the location of the disassembled instruction used to generate the node.
while the asts are different operations on variables and their offsets are the same enabling mapping between variables i.e.
v1 mapsto iandv2 mapsto z .
names into decompiled code when dw arf debugging symbols are present in the binary.
however this alone is not sufficient to identify which developer chosen name maps to a particular variable id generated in the first step.
specifically challenges arise because decompilers use debugging information to enrich the decompiler output in a variety of ways such as improving type information.
recall from section ii that decompilers often make choices between semantically identical structures the addition of debugging information can change which structure is used.
unfortunately this means that the difference between code generated with and without debugging symbols is not always an renaming.
in practice the format and structure of the code can greatly differ between the two cases.
an example is illustrated in fig.
.
in this example the first pass of the decompiler is run without debugging information and the decompiler generates an ast for awhile loop with two automatically generated variables namedv1andv2.
next the decompiler is passed dw arf debugging symbols and run a second time generating the ast on the right.
while the decompiler is able to use the developerselected variable names iandz it generates a very different ast corresponding to a for loop.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
an additional challenge is that there is not always a complete mapping between the variables in code generated with and without debugging information.
decompilers often generate more variables than were used in the original code.
for example return x is commonly decompiled to intv v x returnv1 .
the decompiled code introduces a temporary variable v1that does not correspond to any variable in the original source code.
in this case there is no developer assigned name for v1 since it does not exist in the original code.
the use of debugging information can change how many of these additional variables are generated.
one solution to these problems proposed by prior work is to post process the decompiler output using heuristics to align decompiler assigned and developer assigned names .
however this technique can only correctly align .
of variable names therefore limiting the overall accuracy of any subsequent model trained on this data.
instead we developed a technique that directly integrates with the decompiler to generate an accurate alignment without using heuristics .
our key insight is that while the ast and code generated by the decompiler may change when debugging information is used instruction offsets and operations on variables do not change .
as a result each variable can be uniquely identified by the set of instruction offsets that access that variable.
for example in fig.
although there is not an obvious mapping between the nodes in the trees the addresses of the variable nodes in the trees have not changed.
this enables us to uniquely identify each variable by creating a signature consisting of the set of all offsets where it occurs.
the variables v1andihave the signature 49e 4a1 4a5 whilev2andzhave the signature 49e .
note that some uses of variables overlap e.g.
v1 i is summed with v2 z in the instruction at offset 49e .
this necessitates collecting the full set of variable uses to disambiguate these instances.
in summary to generate our corpus we decompile binaries containing debugging information.
collect signatures and corresponding developer assigned names for each variable in each function.
strip debugging information and decompile the stripped binaries.
identify variables by their signature and rename them in the ast encoding both the decompiler and developer assigned names.
generate decompiled code from the updated ast.
post process the updated ast and generated code to create a corpus entry.
the final output is a per binary file containing each function s ast and decompiled code with corresponding variable renamings.
v. e v alua tion we ask the following research questions rq1 how effective is dire at assigning names to variables in decompiled code?
rq2 how does each component of dire contribute to its efficacy?
2while it is possible for two variable signatures to be identical we found these collisions to occur very rarely in practice.
in these cases we do not attempt to assign names to variables.table i evaluation of dire.
v alues are percentages higher accuracy and lower character error rate cer are better.
dire le xical enc.
structural enc.
acc.
cer acc.
cer acc.
cer overall .
.
.
.
.
.
body in train .
.
.
.
.
.
body not in train .
.
.
.
.
.
rq3 how does provenance and quantity of data influence the efficacy of dire?
rq4 is dire more effective than prior approaches?
a data preprocessing to answer our first two research questions we trained dire on decompiled functions extracted from binaries mined from g ithub.
first we automatically scraped g ithubfor projects written in c. next we modified project build scripts to include debug information when compiling the project and collected all successfully generated bit x86 binary files.
we then hashed each binary to remove any duplicates.
we then passed these binaries through our automated corpus generation system.
finally we filtered out any functions that did not have any renamed variables and for practical reasons any functions with more than ast nodes.
after filtering functions with an average ast size of nodes remained.
these functions were randomly split per binary into training development and testing sets with a ratio of .
splitting the sets per binary ensures that binary specific identifiers are not included in both the training and test sets.
b evaluation methodology after training we ran dire to generate name suggestions on the test data.
we evaluate the accuracy of these predictions comparing the predicted variable names to names used in the original code i.e.
names contained in the debugging information counting a successful prediction as one exactly matching the original name.
however there can be multiple equally acceptable names e.g.
file name fname filename for a given identifier.
an accuracy metric based on exact match cannot detect these cases.
we therefore use character error rate cer a metric that calculates the edit distance between the original and predicted names then normalizes by the length of the original name assigning partial credit to near misses.
recall from section iv that there are often many more variables in the decompiled code than in the original source these variables will not have a corresponding original name.
in our corpora the median number of variables in each function is with having a corresponding original name.
although dire generates predictions for all variables we do not evaluate predictions on variables that do not have a developer assigned name.
we do this because it is not necessarily incorrect for a renaming system to assign names to variables not present in the original source code.
recall the example where return x is decompiled to intv v x returnv1 .
the name sum is likely more informative than v1 and it would be unhelpful to penalize a system that suggests this renaming.
however although authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
1void file mmap intv1 intv2 3void v3 4v3 mmap v2 v1 5if v3 void perror mmap exit 9returnv3 id dire dev .
1fd fd 2size size 3buf ret fig.
decompiled function simplified for presentation dire v ariable names and developer assigned names.
renaming in these cases could be helpful we do not want to overapproximate the effectiveness of our system by claiming any renaming of these variables as correct it is also possible to assign variables a misleading name that decreases the readability of code by obfuscating the purpose of a variable.
for example suggesting the name filename to replace v1in the above code would likely be misleading.
c neural network configuration for our experiments we replicate the neural network configuration of allamanis et al.
.
we set the size of word embedding layers to be .
the dimensionality of the hidden states for the recurrent neural networks used in the encoders is while the hidden size for the decoder lstm is .
for both the sequential and structural encoders we use two layers of recurrent computation adding another identical recurrent network to process the decompiled code using the output hidden states of the first layer.
for both dire and the baseline neural systems we train each model for epochs.
at testing time we use beam search to predict the sequence of sub tokenized names for each identifier section iii c with a beam size of .
a. rq1 overall effectiveness experimental results are summarized in table i. the overall row shows the performance of our technique on the full test set and the leftmost column shows the accuracy of dire.
from this we can see that dire can recover .
of the original variable names in decompiled code demonstrating that it is effective in assigning contextually meaningful names to identifiers in decompiled code.
figure shows an example renaming generated by dire.
here dire generates the variable names shown in the dire column of the table.
the developer chosen names are shown in the dev.
column.
two of three names suggested by dire e xactly match those chosen by the developer.
though dire suggests buf instead of ret as the replacement for v3 the name is not entirely misleading mmap returns a pointer to a mapped area of memory that can be written to or read from.
work has shown that large code corpora may contain nearduplicate code across training and testing sets which can cause evaluation metrics to be artificially inflated .
though our corpus contains no duplicate binaries splitting test and training sets per binary still results in functions appearing in both.
a common cause of duplicate functions in different binaries is the use of libraries.
we argue that it is reasonable to allowtable ii example identifiers from the body not in train testing partition and dire s top 5most frequent predictions.
len value new node bytes read len value node size n data child bytes read size val treea .
len length name tree .
cmd code l key root .
read such duplication since reverse engineering binaries that link against known e.g.
open source libraries is a realistic use case.
nevertheless to better understand the performance of our system we partition the test examples into two sub categories body in train and body not in train .
the body in train partition includes all functions whose entire body matches at least one function in the training set similarly the body not in train set includes only functions whose body does not appear in the training set.
the last two rows in table i show the performance on these partitions.
dire performs well on the body in train test partition .
.
this indicates that dire is particularly accurate at name prediction when code has appeared in its training set e.g.
libraries or code copied from another project .
dire is still able to exactly match .
of variable names in the body not in train set indicating that it still generalizes to unseen functions.
table ii contains example identifiers from the body not in train test set along with dire s most frequent predictions.
we observe that inexact suggested names are often semantically similar to the original names.
dire also performs best on simple identifiers such as len andvalue .
this is because it is difficult to predict the exact name for complex identifiers with compositional names.
however dire is still often able to suggest semantically relevant identifiers e.g.
node child .
rq1 answer we find that dire is able to suggest variable names identical to those chosen by the original developer .
of the time.
b. rq2 component contributions table i also shows the results for models using only our lexical or structural encoders.
we find that the lexical encoder is able to correctly predict .
of the original variable names while a model using the structural encoder is able to correctly predict .
of the original variable names.
these simpler models still perform well but by combining them in dire we are able to achieve even better performance.
figure illustrates how dire can ef fectively combine these models to improve suggestions.
here the placeholders v1 v2 andv3are variables which should be assigned names.
the lexical structural and dire columns show the predictions from each model and the developer column shows the name originally assigned by the developer.
in this example the lexical and the structural models are unable to authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
1file f open char v1 char v2 intv3 2intfd 3if !v3 returnfopen v1 v2 5if v2!
assert fail fopen 7fd open v1 8if fd returnfdopen fd v2 10else return0 id lexical structural dire dev eloper 1file fname filename filename 2name oname mode mode 3mode flags create is private fig.
decompiled function simplified for presentation suggested names and developer assigned names.
the lexical and structural models are unable to correctly predict the name filename for variable but dire can by combining them.
predict any of the original variable names while dire is able to correctly predict two of the three names.
this example also shows the contributions from each of the submodels.
for example for v1 the lexical model predicts file while the structural model predicts fname .
combining the predicted subtokens generates filename the same name chosen by the developer.
for v2 the lexical and structural models both fail to predict mode but note that the lexical model does predictmode forv3.
by combining the models dire instead correctly predicts mode forv2.
rq2 answer each component of dire contributes uniquely to its overall accuracy.
c. rq3 effect of data to answer rq3 we varied the size of the training data and measured the change in performance of our models.
training data was subsampled at rates of and .
the results of these experiments are shown in fig.
.
figures 8a and 8b show the change in accuracy and cer of dire respectively.
the size of the training data is plotted on thex axes while accuracy and cer are plotted on the y axes.
while dire has low accuracy on the body not in train set at the lowest sampling rates at a sampling rate it is still able to correctly select names over of the time for the body in train test set suggesting that it is possible to use much less data to train a model if the target application is reverse engineering of libraries rather than binaries in general.
note however that the cer of dire is still high at low sampling rates.
this implies that in the cases where dire selects an incorrect variable name the chosen name is quite different from the correct name.
sampling at a higher rate dramatically decreases the cer allowing for namings that are closer the developers choices.
at a sampling rate of dire comes quite close to the performance of the modeltrained on the full training set with an overall accuracy of .
vs. .
and a cer of .
vs. .
.
figure 8c shows the effect of training set size on the performance of dire and its component neural models on the body not in train test set.
note how at sampling rates at or below the models have similar performance.
in cases where there is little training data training time can be further reduced by using only one of the two submodels.
rq3 answer dire is data efficient performing competitively using only of the training data.
dire is also robust outperforming the lexical and structural models in most sub sampling cases.
d. rq4 comparison to prior work to answer rq4 we compare to our prior work and to debin the state of the art technique for predicting debug information directly from binaries.
in our earlier work which used a purely lexical model based on statistical machine translation smt we were able to exactly recover .
of the original variable names chosen by developers.
in contrast dire is able to suggest identical variable names .
of the time.
we attribute this improvement to two factors the improved accuracy of our corpus generation technique and the use of a model that incorporates both lexical and structural information.
to better understand the performance of dire we also compare to d ebin a different approach to generating more understandable decompiler output.
d ebin uses crfs to learn models of binaries and directy generate dw arf debugging information for a binary which can be used by a decompiler such as hex rays.
the debugging information generated by d ebin contains predicted identifiers types and names.
to choose a variable name d ebin proceeds in two stages it predicts which memory locations correspond to function local arguments and variables and then predicts names for the variables it identified.
in contrast dire lev erages the decompiler to identify function offsets and local variables.
building on top of the decompiler helps dire maintain the quality of pseudocode output.
an example is shown in fig.
which contains a c function for converting between a numbera1and its gray code representation in a2bits .
figure 9a shows the output of hex rays when passed a binary with no debug information.
although these variables do not have meaningful names it is clear that gray is a function that takes two arguments and returns a long .
figure 9b shows the output of hex rays using debugging information generated using d ebin s bundled model.3we observe that d ebin does not accurately recover variable names in this case perhaps since its model was trained on a different set of code.
models.tar.gz accessed april authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a accuracy of dire higher is better .
b cer of dire lower is better .
c accuracy of each neural model on the body not in train partition.
fig.
the impact of training corpus size on the performance of dire.
figures a and b show how increasing the amount of training data improves the performance of dire c shows the performance of each of the submodel as training size changes.
1longgray unsigned a1 inta2 3unsigned v3 v4 4intv5 5if a2 returna1 a1 7v 8v4 a1 9while v v v v4 v4 v5 if v3 v5 break v5 17returnv4 a hex rays.1voidgray 2unsigned v0 3intv1 4unsigned i v3 5intx 6if v x v3 v0 while i v x v3 v3 x if i x break x b hex rays w d ebin .
fig.
effects of incorrect debugging information on decompiler output.
the gray function computes the gray code of a1 ina2bytes .
on the left a is the output of hex rays without debugging symbols it is able to correctly identify the arguments and return type.
on the right b is the output with incorrect dw arf information generated by d ebin note missing arguments return statements and incorrect type.
however this example also surfaces a fundamental limitation of the d ebin approach both the inferred structure and the types of the variables in the program have changed.
this occurs because hex rays prioritizes debugging information over its own analyses and heuristics.
in this case the debugging information generated by d ebin does not indicate a return value of the gray function nor any arguments misleading the decompiler.
by starting at the point shown in fig.
9a dire maintains structure and typing even in the presence of incorrect predictions.
to evaluate our performance compared to d ebin w e trained it on binaries in our dataset.
due to time restrictions we found it impractical to train d ebin on the full dataset.
for a fair comparison we instead subsampled our training set at and and trained both d ebin and dire on thesetable iii comparison of dire and d ebin trained on and of our full corpus of binaries.
all accuracy values are percentages higher accuracy is better.
of corpus of corpus dire d ebin dire d ebin training time h .
.
.
.
accuracy overall .
.
.
.
accuracy body in train .
.
.
.
accuracy body not in train .
.
.
.
sets.4after training we ran d ebin on binaries in our test set extracted names using our corpus generation pipeline and measured the accuracy of predictions.
our results are shown in table iii.
we find that dire is able to outperform d ebin at all sampling sizes.
when trained on of the corpus dire is able to exactly recover .
of all identifiers while d ebin recovers .
.
on the partition dire is able to recover .
of names while d ebin is able to recover .
.
the lower performance of d ebin we observed could be attributed to compound error in addition to variable names themselves debin must predict what memory locations correspond to variables.
if a memory location is not predicted to be a variable d ebin cannot assign it a name.
we also note that we were able to train dire much faster than d ebin although dire is gpu accelerated while debin as distributed is limited to execution on the cpu.
rq4 answer dire is a more accurate and more scalable technique for variable name selection than other state of the art approaches.
vi.
t hrea ts tovalidity when collecting code and binaries to generate our corpus we did no filtering of the repositories beyond ensuring that 4the subsampling we used is a slightly larger training set than the binaries used to train d ebin in the original paper .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
they were written in c and built.
it is possible that the code we collected does not accurately represent the types of binaries that are typically targets of reverse engineering effort.
additionally we did not experiment with binaries compiled with optimization enabled nor did we experiment with intentionally obfuscated code.
it is possible that dire does not perform as well on these binaries.
however reverse engineering of these binaries is a general challenge for decompilers and we do not believe that our technique applies exclusively to the test code we experimented with.
although we have found that it is possible to uniquely identify variables in hex rays based on the code offsets where it is accessed we have found that other decompilers do not have this property.
in particular our approach did not work well with the newly released ghidra decompiler .
one of the primary causes is the way that hex rays and ghidra utilize debug symbols to name variables.
hex rays uses debug symbols in a very straight forward manner and generally does not propagate local names outside of their function.
ghidra however will actually propagate variable names at some function calls.
for example if an unnamed variable is passed as an argument to a function whose parameter has a name in some cases ghidra will rename the variable to match the parameter s name.
this behavior is problematic for corpus generation because it does not reflect the developer s intended names.
a new approach for corpus generation would be required for compatibility with ghidra but ghidra s open source nature as opposed to hex rays closed model allows potential modification of the decompiler including disabling the problematic propagation of names at function calls.
we leave ghidra integration to future work.
vii.
c onclusion semantically meaningful variable names are known to increase code understandability but they generally cannot be recovered by decompilers.
in this paper we proposed the decompiled identifier renaming engine dire a novel probabilistic technique for variable name recovery which uses both lexical and structural information.
we also presented a technique for generating corpora suitable for training dire which we used to generate a corpus from unique x8664 binaries.
our experiments show that dire is able to predict variable names identical to the names used in the original source code up to .
of the time.
viii.
a cknowledgments this material is based upon work supported in part by the software engineering institute line project and national science foundation awards and .
we also gratefully acknowledge hardware support quadro p6000 gpu from the nvidia corporation.
many thanks to both prem devanbu and members of the cert division at the software engineering institute for helpful feedback on earlier drafts.