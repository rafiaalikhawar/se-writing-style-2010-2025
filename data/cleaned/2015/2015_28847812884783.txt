the emerging role of data scientists on software development teams miryung kim ucla los angeles ca usa miryung cs.ucla.edu thomas zimmermann robert deline andrew begel microsoft research redmond wa usa tzimmer rdeline andrew.
begel microsoft.com abstract creating and running software produces large amounts of raw data about the development process and the customer usage which can be turned into actionable insight with the help of skilled data scientists.
unfortunately data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by only recently have software companies started to develop competencies in software oriented data analytics .
to understand this emerging role we interviewed data scientists across several product groups at microsoft.
in this paper we describe their education and training background their missions in software engineering contexts and the type of problems on which they work.
we identify five distinct working styles of data scientists insight providers who work with engineers to collect the data needed to inform decisions that managers make modeling specialists who use their machine learning expertise to build predictive models platform builders who create data platforms balancing both engineering and data analysis concerns polymaths who do all data science activities themselves and team leaders who run teams of data scientists and spread best practices.
we further describe a set of strategies that they employ to increase the impact and actionability of their work.
categories and subject descriptors d. .
general terms management measurement human factors.
.
introduction software teams are increasingly using data analysis to inform their engineering and business decisions and to build data solutions that utilize data in software products .
the people who do collection and analysis are called data scientists a term coined by dj patil and jeff hammerbacher in to define their jobs at linkedin and facebook .
the mission of a data scientist is to transform data into insight providing guidance for leaders to take action .
one example is the use of user telemetry da ta to redesign windows explorer a tool for file management for windows .
data scientists on the windows team discovered that the top ten most frequent commands accounted for .
of all of invoked commands but only two of these were easily accessible from the command bar in the user interface .
based on this insight the team redesigned the user experience to make these hidden commands more prominent.
until recently data scientists were found mostly on software teams whose products were data intensive like internet search and advertising.
today we have reached an inflection point where many software teams are starting to adopt data driven decision making.
the role of data scientist is becoming standard on development teams alongside existing roles like developers testers and program managers.
online service oriented businesses such as bing or azure often require that software quality to be assessed in the field testing in production as a result microsoft changed the test discipline and hires data scientists to help with analyzing the large amount of usage data.
with more rapid and continuous releases of software software development teams also need effective ways to operationalize data analytics by iteratively updating the software to gather new data and automatically produce new analysis results.
so far there have been only a few studies about data scientists which focus ed on the limitations of big data cloud computing tools and the pain points that data scientists face based on the experiences of participants from several types of businesses .
however these studies have not investigated the emerging roles that data scientists play within software development teams.
to investigate this emerging role we interviewed data scientists from eight different product organizations within microsoft.
during the period of our interviews microsoft was in the process of defining an official career path for employees in the role of data scientist that is defining the knowledge and skills expected of the role at different career stages.
this process made microsoft a particularly fruitful location to conduct our research and several of our participants took part in this process.
we investigated the following research questions q1 why are data scientists needed in software development teams and what competencies are important?
q2 what are the educational and training backgrounds of data scientists in software development teams ?
q3 what kinds of problems and activities do data scientists work on in software development teams?
q4 what are the working styles of data scientists in software development teams?
this paper makes the following contributions we characterize the roles of data scientists in a large software company .
section we explore various working styles of data scientists.
section the paper concludes with a discussion of implications section .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with cre dit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may austin tx usa .
copyright is held b y the owner author s .
publication rights licensed to acm.
acm ... .
.
ieee acm 38th ieee international conference on software engineering .
related work the work related to this paper falls into general data science and software analytics.
data science has become popular over the past few years as companies have recognized the value of data for example as data products to optimize operations and to support decision making.
not only did davenport and patil proclaim that data scientist would be the sexiest job of the 21st century many authors have published data science books based on their own experiences e.g.
o neill and schutt foreman or may .
patil summarized strategies to hire and build effective data science teams based on his experience in building the data science team at linkedin .
we found a small number of studies which systematically focused on how data scientists work inside a company.
fisher et al.
interviewed sixteen microsoft data analysts working with large datasets with the goal of identifying pain points from a tooling perspective .
they uncovered tooling challenges in big data computing platforms such as data integration cloud computing cost estimation difficulties shaping data to the computing platform and the need for fast iteration on the analysis results.
however they did not describe the roles that data scientists play within software development teams.
in a survey harris et al .
asked data science practitioners how they viewed their skills careers and experiences with prospective employers .
then they clustered the survey respondents into four roles data businesspeople data creatives data developers and data researchers.
they also observed evidence for so called t shaped data scientists who have a wide breadth of skills with depth in a single skill area.
harris et al.
focus on general business intelligence analysts rather than data scientists in a software development organization.
due to the nature of a survey research method harris et al.
also do not provide contextual deeper findings on what types of problems that data scientists work on and the strategies that they use to increase the impact of their work.
kandel et al.
conducted interviews with enterprise analysts in healthcare retail marketing and finance .
companies of all kinds have long employed business intelligence analysts to improve sales and marketing strategies.
however the data scientists we study are different in that they are an integral part of the software engineering team and focus their attention on software oriented data and applications.
unlike our work the kandel et al.
study does not investigate how data scientists contribute to software debugging defect prediction and software usage data telemetry collection in software development contexts.
software analytics is a subfield of analytics with the focus on software data .
software data can take many forms such as source code changes bug reports code reviews execution data user feedback and telemetry information.
davenport harris and morison define analytics as the use of analysis data and systematic reasoning to make decisions.
according to an accenture survey of us managers in industry however up to percent of major decisions are based on gut feel rather than facts .
due to the recent boom in big data several research groups have pushed for greater use of data for decision making and have shared their experiences collaborating with industry on analytics projects .
analysis of software data has a long tradition in the research communities of empirical software engineering software reliability and mining software repositories .
software analytics has been the dedicated topic of tutorials and panels at the icse conference as well as special issues of ieee software july and september .
zhang et al.
emphasized the trinity of software analytics in the form of three research topics development process system users as well as three technology pillars information visualization analysis algorithms large scale computing .
buse and zimmermann argued for a dedicated data science role in software projects and presented an empirical survey with software professionals on guidelines for analytics in software development .
they identified typical scenarios and ranked popular indicators among software professionals.
begel and zimmermann collected questions that software engineers would like to ask data scientists to investigate .
none of this work has focused on the characterization of data scientists on software teams which is one of the contributions of this paper.
many software companies such as linkedin twitter and facebook employ data scientists to analyze user behavior and user provided content data .
however the authors of these published reports concentrate mainly on their big data pipeline architectures and implementations and ignore the organizational architecture and work activities of the data scientists themselves.
according to our study data scientists in software teams have a unique focus in analyzing their own software teams engineering processes to improve software correctness and developer productivity.
it is common to expect that action and insight should drive the collection of data.
goal oriented approaches use goals objectives strategies and other mechanisms to guide the choice of data to be collected and analyzed.
for example the goal question metric gqm paradigm proposes a top down approach to define measurement goals lead to questions which are then answered with metrics.
other well known approaches are gqm which adds business alignment to gqm balanced scorecard bsc and practical software measurement .
basili et al.
proposed the experience factory which is an independent organization to support a software development organization in collecting experiences from their projects.
the experience factory packages these experiences for example in models and validates and reuses experiences in future projects.
some of the team structures that we observed in the interviews were similar to an experience factory in spirit however many data scientists were also directly embedded in the development organizations.
while some experiences can be reused across different products not all insight is valid and actionable in different contexts.
.
methodology we interview ed people who acted in the role of data scientists then form ed a theory of the roles that data scientists play in software development organizations.
protocol.
we conducted one hour semi structured interviews giving us the advantage of allowing unanticipated information to be mentioned .
all interviews were conducted by two people.
each was led by the first author who was accompanied by one of the other three authors as schedules permitted who took notes and asked additional questions.
interviews were audio taped and later transcribed for analysis.
the interview format started with an introduction a short explanation of the research being conducted and demographic questions.
participants were then asked about the role they played on their team their data science related background their current project s and their interactions with other employees.
we also asked for stories about successes pitfalls and the changes that data is having on their team s practices.
our interview guide is in appendix a.
97participants.
in total we interviewed participants women men from eight different organizations at microsoft advanced technology lab participant advertisement and monetization azure bing engineering excellence exchange office skype windows and xbox .
we selected participants by snowball sampling first we identified presenters at data driven engineering meetups and technical community meetings since these have been responsible internally for sharing best practices.
next we selected additional data scientists by word of mouth asking each participant to introduce us to other data scientists or other key stakeholders whom they knew.
at the time of this study in summer there was no easy way to identify those who do data science work at microsoft.
in fact microsoft was in the process of creating a new job discipline called data and applied science.
therefore we used snowball sampling because it helped us locate hidden populations of data science practitioners such as those employees working on data science tasks who do not have data or data science in their job title see table .
as mentioned by p15 a lot of people kind of moonlighted as data scientists besides their normal day job.
our sampling method may have caused us to miss some data scientists however to mitigate this threat we seeded our sample with data science thought leaders from various product teams identified through companywide engineering meetups and technical community talks.
our findings reached saturation after interviewing people.
there was enough diversity in the participants responses to enable us to find meaningful themes and draw useful interpretations.
stopping after saturation is standard procedure in qualitative studies.
data analysis.
the authors individually used the atlas.ti qualitative coding tool to code emerging themes from the transcripts together we discussed the taxonomies derived from the interview transcripts.
in concert we employed affinity diagramming and card sorting to make sense of our data.
figure shows a screen snapshot of atlas.ti with an interview transcript excerpt and corresponding code describing emerging themes.
in order to further help with traceability and to provide the details of our data analysis process our technical report lists the codes we derived along with supporting quotes .
to infer the working styles of data scientists q4 we performed two card sorts based on the roles data scientists played.
one was done by the first author another by the second and third authors.
when participants shared experiences from multiple roles we put each role on a separate card.
this happened when participants shared experiences from previous jobs on different teams p2 and p12 or had multiple responsibilities p15 manages one team of engineers building a machine learning platform and another team of data scientists using the platform to create models .
both card sorts led to similar groupings of the participants which are discussed later in the paper.
table .
participant information title education p1 data scientist ii bs in cs statistics ms in se currently pursuing phd in informatics p2 director app statistics eng ineer ms in physics p3 principal data scientist mba bs in physics cs currently pursuing phd in statistics p4 principal quality m anager bs in cs p5 partner data science architect phd in applied mathematics p6 principal data scientist phd in physics p7 research s oftware design engineer ii ms in computer science ms in statistics p8 program manager bs in cognitive science p9 senior p rogram manager bse in cs and bas in economics finance p10 director of test bs in cs p11 principal dev m anager ms in cs p12 data scientist phd in cs machine learning p13 applied scientist phd in cs machine learning and database p14 principal g roup program manager bs in business p15 director of data science phd in cs machine learning p16 senior data scientist phd in cs machine learning figure .
an interview transcript excerpt in atlas.ti .
using this tool w e added code s describing emerging themes .
98we categorized our results in terms of how and why data scientists are employed in a large software company section the working styles that data scientists use with software development teams and their strategies to increase the impact of their work section .
limitations.
this is a qualitative study based on interviews.
in this paper we share the observations and themes that emerged in the interviews.
because experts in qualitative studies specifically warn the danger of quantifying inherently qualitative data we do not make any quantitative statements about how frequently the themes occur in broader populations.
we follow this guideline to focus on providing insights that contextualize our empirical findings especially when they can serve as the basis for further studies such as surveys.
although this study was conducted only in one company the participants came from eight different organizations each working on different kinds of products.
several of our participants spoke of data science experiences they had prior to joining microsoft.
we believe that the nature of data science work in this context is meaningful given that very few companies of a similar scale exist in the software industry.
in addition the software engineering research community also uses large scale analysis of various types of software artifacts.
.
data scientists in software development teams data science is not a new field but the prevalence of interest in it at microsoft has grown rapidly in the last few years.
in one year after this study over six hundred people are now in the new data and applied science discipline and an additional employees are interested in data science work and signed up to mailing lists related to data science topics.
we observed an evolution of data science in the company both in terms of technology and people.
product leaders across the company are eager to be empowered to make data driven engineering decisions rather than relying on gut feel.
some study participants who initially started as vendors or individual contributors have moved into management roles.
participants also reported that infrastructure that was initially developed to support a single data driven task for example the windows error reporting tool used for collecting crash data from in field software deployments was later extended to support multiple tasks and multiple stakeholders.
separate organizations within the company merged their data engineering efforts to build common infrastructure.
the term data scientist in this paper is a logical role rather than the title of a position.
our goal is to understand and define this data scientist role by studying the participants in terms of their training and education background what they work on and how they fit in their organization rather than restricting our study to those with the title data scientists.
.
why are data scientists needed in software development teams?
data driven decision making has increased the demand for data scientists with statistical knowledge and skills.
specifically participants described the increasing need for knowledge about experimental design statistical reasoning and data collection.
demand for experimentation.
as the test in production paradigm for on line services has taken off our participants recognized the opportunity and need for designing experiments with real user data .
real customer usage data is easier to obtain and more authentic than the simulated data test engineers create for anticipated usage scenarios.
instead of having an army of testers to go off and generate a bunch of data that data s already here .
it s more authentic because it s real customers on real machines real networks.
you no longer have to simulate and anticipate what the customer s going to do.
participants mentioned an increase in the demand for experimenting with alternative software implementations in order to assess the requirements and utility of new software features.
over the last decade randomized two variant experiments called a b testing have been used to assess the utility of software prototypes and features particularly for online services like web search.
because there are endless possibilities for alternative software designs data scientists and engineering teams build software systems with an inherent capability to inject changes called flighting .
you create an environment where for example in search where i can actually experiment based on a mockup if you will of the idea.
i can actually come up with a set of ideas broad ideas about my work and i can actually deploy them in some easy way.
do i change the size?
do i change the font?
there are so many things you could do... we re trying to flight things.
it has capability to inject changes.
several participants took it upon themselves both to design incentive systems that get users to adopt a product feature and to create user telemetry and surveys that measure whether the systems worked.
so we create a game that gets people to repetitively use the feature.
and then we watch what happens when we take the game away.
did it stick or did it not stick?
demand for statistical rigor.
in the analysis of data participants told us that there is an increasing demand for statistical rigor.
data scientists and their teams conduct formal hypothesis testing report confidence intervals and determine baselines through normalization.
for example when participant p2 who worked on estimating future failures reported her estimate to her manager the manager asked how confident she was.
she gave him a hard number surprising him because whenever he had asked the question to previous employees he had just been told highly confident or not very confident.
he was like so you are giving me some predictions .
how confident are you that this is what we get ?
and i m looking and go what do you mean?
it s percent !
it s implied in all the testing.
this is how we define this whole stuff.
and he goes wow this is the first time i m getting this answer.
there has been a similar increase in the demand for conducting formal hypothesis testing.
for example instead of reasoning about averages or means engineering teams want to see how different their observation is from random chance when i do my analyses i always have a null hypothesis and an alternative hypothesis.
data scientists also have to determine a baseline of usual behavior so they can normalize incoming data about system behavior and 99telemetry data that are collected from a large set of machines under many different conditions.
i ve got all of these different clients out in the wild running on all these different servers.
i want to get a general sense of what things feel like on a normal monday .
demand for data collection rigor .
when it comes to collecting data data scientists discussed how much data quality matters and how many data cleaning issues they have to manage.
many participants mentioned that a large portion of their work required the cleaning and shaping of data just to enable data analysis.
this aligns with a recent article on new york times that said that of data science work requires janitor work .
we need to cleanse the data because there are all sorts of data quality issues imperfect instrumentation.
furthermore data collection itself requires a sophisticated engineering system that tries to satisfy many engineering organizational and legal requirements.
what about storage what about speed?
what about legal what about privacy?
there is an entire gamut of things that you need to jump through hoops to collect the instrumentation.
.
background of data scientists one column in table shows the educational background of the study participants.
data scientists often do not have a typical fouryear degree in computer science .
in our study of participants have degrees in computer science however many also have joint degrees from other fields such as statistics physics math bio informatics applied math business economics and finance.
their interdisciplinary backgrounds contribute their strong numerical reasoning skills for data analysis.
participants have higher education degrees phd or ms and many have prior job experiences with dealing with big data.
several non cs participants expressed a strong passion for data.
i love data looking and making sense of the data.
i ve always been a data kind of guy.
i love playing with data.
i m very focused on how you can organize and make sense of data and being able to find patterns.
i love patterns.
when data scientists hire other data scientists they sometimes look for skill sets that mirror how they were themselves trained.
when one team manager with a phd in machine learning spoke about hiring new employees for his data science tools team he said that he looks for machine learning hackers.
so the typical guys on my team have some phd in a quantitative field with machine learning background and the ability to code.
they have to manipulate data.
the other very essential skill is we want programming.
it s almost like ... a hacker type skill set.
another data science team manager with strong statistics background demanded the same from everyone on his team my people have to know statistics.
they need to be able to answer sample size questions design experiment questions know standard deviations p value confidence intervals etc.
our participants background in higher education also contribute s to how they view the work of data science.
usually the problems and questions are not given in advance.
a large portion of their re sponsibility is to identify important questions that could lead to impact.
then they iteratively refine questions and approaches to the analyses.
participants from a variety of product teams discussed how their training in a phd program contributed to the working style they use to identify important questions and iteratively refine questions and approaches.
it has never been in my four years that somebody came and said can you answer this question ?
i mostly sit around thinking how can i be helpful?
probably that part of your phd is you are figuring out what is the most important questions.
i have a phd in experimental physics so pretty much i am used to designing experiments.
data science is kind of like research.
it looks like a good problem and looks like a good idea.
you think you may have an approach but then maybe you end up with a dead end.
.
problems that data scientists work on our participants said they worked on many kinds of problems ranging from performance and quality regression user engagement and feature assessment debugging and root cause analysis bug reproduction server log anomaly detection failure rate estimation and failure planning.
they also worked on business specific problems such as detecting fraud in e commerce identifying a mode of transportation for mobile users and assessing advertisement ranking and news recommendations.
here are just a few of the example tasks that participants told us they work ed on.
performance regressio n. are we getting better in terms of crashes or worse?
how long did it take to detect when a new feature has blown up your system?
requirements identification.
if you see the repetitive pattern where people don t recognize the feature is there.
fault localization and root cause analysis.
what areas of the product are failing and why?
how many failures are there per day?
bug prioritization.
oh cool.
now we know which bugs we should fix first.
then how can we reproduce this error ?
server anomaly detection.
we are interested in anomaly detection on real time servers in general.
is this application log abnormal w.r.t.
the rest of the data?
failure rate estimation.
is the beta ready to ship?
customer understanding.
how long do our users use the app ?
what are the most popular features?
is my feature used in a way that improves the customer s productivity?
cost benefit analysis .
how much money can we save if we improve the auc i.e.
area under the curve for this machine learning classifier?
how many customer service calls can we prevent if we detect this type of anomaly?
.
activities of data scientists we found that data scientists worked on a variety of activities which we organize into three categories data collection data analysis and data use and dissemination.
please note that this list is not meant to be exhaustive.
it is simply an overview of the activities we learned about from our study.
the mapping of activities to individual participants is shown in table .
collection data engineering platform building a system for collecting data from multiple sources continuously telemetry injection inserting instrumentation code to gather software execution and usage profiles experimentation platform building inherent capability for experimentation with alternative software designs analysis data merging and cleaning joining data from multiple sources and dealing with missing values and imperfect instrumentation sampling selecting a subset set of behavior and weigh profiles to approximate normal behavior data shaping including selecting and creating features transforming data into a new format and creating new attributes in a feature vector defining sensible metrics defining metrics that are sensible to data consumers building predictive models building predictive models by applying machine learning data mining and statistics.
defining ground truths defining class labels and scenarios of anomalies hypothesis testing setting a null hypothesis and an alternative hypothesis and estimating the confidence level of rejecting the null hypothesis using various statistical methods.
use and dissemination operationalizing predictive models integrating predictive models into software products and systems by invoking right models at a right point defining actions and triggers defining automated actions and triggers for different labels of predictions.
translating insights and models to business values explaining the value of insights and predictive models using domain specific terms.
.
impact when we asked the participants about their experiences in data science work that had impact and or led to action we heard several success stories.
for example several participants mentioned that their work on user engagement analysis led to new features which emerged from repetitive sequences of user actions that did not map to existing features.
in some cases their work also led the team to deprecate unused features.
for example participant p3 said that there was a feature that required a large amount of code but nobody used it.
his data science work led to identifying and deprecating the unused feature.
participant p2 s work on failure rate estimation led to releasing a product two weeks earlier than the expected schedule.
another project on defect prediction enabled the team to rebalance resources to focus on bug fixing rather than adding new features.
root cause analysis of crash data led to automated bug filing and monitoring to reduce crash rates.
server log anomaly detectio n work led to reducing development operation cost.
actionability is actually a big thing.
if it s not actionable the engineers then look at you say i don t know what to do with this so don t even bother me.
.
organization of data science teams among the interviewees we observed five different ways of organizing a data science team or employing data scientists within an existing organization.
the triangle model.
in a triangle team structure a third of the team are data scientists who perform analysis work and who have a strong statistics background another third are called data stewards who perform data shaping and cleaning tasks and the rest collects customer usage data telemetry through instrumentation of software and hardware.
the hub and spoke model.
in this model a centralized team builds a common platform for data collection and analysis which is used by spoke teams with product specific knowledge to build product specific models.
the consulting model.
an organization consults both internal and external customers by creating custom models and solving data problems of other teams within microsoft.
the individual contributor .
a software development team has a data scientist as an individual contributor.
the virtual team model.
the individual contributors from different teams form a virtual team and share common data collection and analysis tools for data science work.
.
data scientist working styles though the role of data scientist is relatively new in software development the interviews reveal commonalities in how the participants function on their teams.
nonetheless each of our participants followed a unique path to their current role.
based on two independent card sorts described in section we grouped the participants into five distinct styles of data scientists.
the first author initially grouped participants by their primary activities.
for example the first author noticed that p2 p3 and p9 table .
activities that participants stated they did themselves or managed p1 p2 p3 p4 p5 p6 p7 p8 p9 p10 p11 p12 p13 p14 p15 p16 collecti ng building the d ata collection platform injecting t elemetry building the e xperimentation platform analy zing data merging and cleaning sampling shaping feature selection defin ing sensible metrics build ing predictive models defin ing ground truth hypothesis testing using and disseminating operationaliz ing models defin ing actions and triggers applying insights models to business 101participate in a similar set of activities with the goal of communicating their insights to managers see columns in table while p1 p4 p8 p11 and p14 focus on building a data engineering pipeline.
the second and third authors performed another separate card sort.
all authors then collaboratively refined the groups to the ones listed in this section.
these working style groups are not mutually exclusive because some participants discussed their work on several different product teams.
in the next subsections we characterize the nature of each style and include a participant success story to exemplify it.
.
insight providers this working style characterizes data scientists who play an interstitial role between managers and engineers within a product group .
the managers want to take actions to achieve business goals such as increased customer adoption improved product quality or shipping products.
with a strong background in statistics insight providers main task is to generate insights and to support and guide their managers in decision making.
these data scientists guide the managers actions by analyzing prod uct and customer data collected by the teams engineers.
their communication and coordination skills are key they negotiate with engineers to get the data they need iterate with managers to understand and refine their goals and communicate their findings clearly to the team.
example .
p2 worked on a product line in which the managers needed to know whether an upgrade was of sufficient quality to push to all products in the family.
at first she struggled to get quality crash data from the engineers i basically tried to eliminate from the vocabulary the notion of you can just throw the data over the wall ... she ll figure it out.
there s no such thing.
i m like why did you collect this data?
why did you measure it like that?
why did you measure this many samples not this many?
where did this all come from?
she worked with management to get a clear goal it should be as good as before.
it should not deteriorate any performance customer user experience that they have.
basically people shouldn t know that we ve even changed .
her analysis was able to determine a confidence interval on the probability of field failures allowing the team to know when they reached the quality bar.
as part of the strategy to increase the impact of their work insight providers go a step further by defining actions and triggers associated with the resulting insight.
participant p9 provided an example in the context of server log anomaly detection where each anomaly should trigger some action you need to think about if you find this anomaly then what?
just finding an anomaly is not very actionable.
what i do also involves thinking these are the anomalies i want them to detect.
based on these anomalies i m going to stop the build.
i m going to communicate to the customer and ask them to fix something on their side.
as insights providers need to communicate their results to managers and engineers it is important for them to translate analysis results to concepts familiar to stakeholder s decisions.
occasionally information is lost in translation i.e.
when findings are simplified for people with no statistical training.
so i think part of the problem is that a lot of the people aren t given training in statistics... so you got some p value of .
.
oh gee should i be happy or sad?
what does that mean?
and they don t necessarily know that.
so i have to explain things in terms that might not be forceful enough.
another strategy for getting their insights heard is to interact closely and engage with the stakeholders who plan to consume the results from the data analysis.
they often set up channels such as weekly data meet ups .
.
modeling specialists this working style is practiced by data scientists who act as expert consultants and build predictive models .
with a strong background in machine learning their main task is to build predictive models that can be instantiated as new software features e.g.
server telemetry anomaly detection or to support other team s data driven decision making.
in this case both p7 and p12 are experts in machine learning though conceptually other forms of expertise statistics survey design would fit here as well.
example .
p7 is an expert in time series analysis and works with p9 to help her team automatically detect anomalies in their telemetry data.
the pms and the dev ops from that team...through what they daily observe come up with a new set of time series data that they think has the most value and then they will point us to that and we will try to come up with an algorithm or with a methodology to find the anomalies for that set of time series.
modeling specialists sometimes partner with insight providers to define ground truths to assess the quality of their predictive models .
as an example participant p7 a modeling specialist and participant p9 an insight provider iteratively defined which events should be considered as server operation anomalies because the ground truth required for each analysis was often not known in advance.
you have communication going back and forth where you will find what you re actually looking for what is anomalous and what is not anomalous in the set of data that they looked at.
when you re seeing this part of the data this one s good versus here s setting that ground truth.
here s where you should have alerted.
here s where you shouldn t have done anything.
that s something that we are continuing to iterate on but that s something that was fairly labor intensive.
several interviewees reported that operationalization of their predictive models building new software features based on the predictive models is extremely important for demonstrating the value of their work.
however this step of going the last mile is often difficult for modeling specialists such as participants p7 and p12.
with each product team they were assigned to help they had to get their algorithms running on a new infrastructure and too often had to make code changes to the infrastructure itself.
getting your algorithm at the right point to make sure right models are loaded .
that s a big issue we face.
they accepted and they understood all the results and they were very excited about it.
then there s a phase that comes in where the actual model has to go into production.
... you really need to have somebody who is confident enough to take this from a dev side of things .
102modelling specialists also said it was important to translate findings into business values such as dollars saved customer calls prevented or the number of days early that a product can be shipped .
precision recall and roc curves while popular with data scientists and academics are less useful when presenting findings to analytics consumers.
in terms of convincing if you just present all these numbers like precision and recall fa ctors... that is important from the knowledge sharing model transfer perspective.
but if you are out there to sell your model or ideas this will not work because the people who will be in the decision making seat will not be the ones the model transfer.
so for those people what we did is cost benefit analysis where we showed how our model was adding the new revenue on top of what they already had.
.
platform builders this working style is demonstrated by seven data scientists who build shared data platforms used across several product teams p1 p4 p6 p8 p11 p14 p15 .
of these seven six work on data pipelines for data collection storage and querying while p15 works on a service for building and deploying machine learning models.
with a strong background in big data systems their main task is to build a data engineering platform .
a defining characteristic of this working style is that they produce software systems designed to be reusable across many different product and business goals.
the platform bu ilders work balances both engineering and scientific concerns.
for example data collection software must be reliable performant low impact and widely deployable.
on the other hand the software should provide data that are sufficiently precise accurate well sampled and meaningful enough to support statistical analysis.
their expertise in both software engineering and data analysis enables them to make tradeoffs between these concerns.
we found two kinds of data platform builders.
participants p1 p4 p8 and p11 work with on systems that involve platforms to collect in field software failure data including windows error reporting and reliability analysis component .
their work unites these data sources into a common platform to fit current business goals.
participants p6 p14 and p15 work on new data collection and measurement platforms.
for example p14 works on a new common logging platform and has the freedom to design new data schemas.
example .
p4 worked on a data platform that collects crash data and worked on making it actionable to developers.
you come up with something called a bucket feed.
it is a name of a function most likely responsible for the crash in the small bucket.
we found in the source code who touch last time this function.
he gets the bug.
and we filed numbers a year with percent fix rate.
as platform builders construct data engineering pipelines to collect measurements from various sources data quality and cleansing is very important.
they often triangulate multiple data sources to increase the confidence in the analysis results.
they validate quantitative data through qualitative channels to ensure that measurements are meaningful and lead to correct actions.
for example participant p4 discussed the importance of validating his product s telemetry data through subjective channels if you could survey everybody every ten minutes you don t need telemetry.
the most accurate is to ask everybody all the time.
the only reason we do telemetry is that is slow and by the time you got it you re too late.
so you can consider telemetry and data an optimization.
so what we do typically is are surveyed and we get telemetry.
and then we calibrate and infer what the other have said.
talking to non experts also required the development of intuitive measurements.
participant p4 measured the impact of a software crash by the associating it with how many minutes his customers wasted because of it a number that is easy to understand and assess over time.
.
polymaths this working style descri bes data scientists who do it all e.g.
forming a business goal instrumenting a system to collect the required data necessary analyses or experiments and communicating the results to business leaders .
in this working style the boundary between the data scientist role and a software engineer role is not strict.
they are naturally intertwined because they undertake activities common to both roles.
example .
p13 works on a product that serves advertisements and explores her own ideas for new advertisement data models.
so i am the only scientist on this team.
i m the only scientist on sort of sibling teams and everybody else around me are like just straight up engineers.
she expressed enthusiasm for her ability to operationalize her own models.
for months at a time i ll wear a dev hat and i actually really enjoy that too.
... i spend maybe three months some analysis and maybe three months some coding that is to integrate whatever i did into the product.
... i do really really like my role.
i love the flexibility that i can go from being developer to being an analyst and kind of go back and forth.
polymaths embedded in some product teams often switched modes between modelling and deployment.
i kind of flip back and forth.
i say i spend maybe three months some analysis and maybe three months some coding that is to integrate whatever i did into the product.
polymaths set up regular channels such as brown bag lunches to deliver their project outcomes to their team .
.
team leaders the last working style describes senior data scientists who run their own data science teams .
in addition to managing their teams they also act as data science evangelists pushing for the adoption of data driven decision making within their business organization or the company as a whole.
data team leaders work with senior company leaders to inform broad business decisions.
example .
p10 and his team of data scientists estimated the number of bugs that would remain open when a product was scheduled to ship.
when the leadership saw this gap between the estimated bug count and the goal the allocation of developers towards new features versus stabilization shifted away from features toward stabilization to get this number back.
p10 emphasized his role as intermediary between his data scientist and his management sometimes people who are real good with numbers are not as good with words laughs and so having an intermediary to sort of handle the human interfaces between the data sources and the 103data scientists i think is a way to have a stronger influence.
an intermediary so that the scientists can kind of stay focused on the data.
to increase the impact of their work team lead ers emphasized the importance of choosing the right questions for the right team .
participant p5 described three conditions that must be met before his data science team engages in a project priority actionab ility and commitment a is it a priority for the organization b is it actionable if i get an answer to this is this something someone can do something with?
and c are you as the feature team if you re coming to me or if i m going to you telling you this is a good opportunity are you committing resources to deliver a change?
if those things are not true then it s not worth us talking anymore .
team leaders also mentioned the importance of working closely with consumers from day one.
their team must communicate with stakeholders early and often to define the questions and scenarios.
they also emphasized the need to explain findings in simple terms to non experts especially to management.
you begin to find out you begin to ask questions you being to see things.
and so you need that interaction with the people that own the code if you will or the feature to be able to learn together as you go and refine your questions and refine your answers to get to the ultimate insights that you need.
a super smart data scientist their understanding and presentation of their findings is usually way over the head of the managers...so my guidance to is dumb everything down to seventh grade level right?
and whether you re writing or you re presenting charts you know keep it simple .
.
implications the findings in this paper have several implications for research practice and education.
.
research many development teams now include data scientists as a standard role alongside developers testers and program managers.
for researchers this new team composition changes the context in which problems are pursued.
many development teams are already collecting and monitoring data about user behavior software execution and team activities as well as contextual data sources like social media.
this means that researchers can assume the availability of such data as well as an expert team member to handle them as an ingredient for solving problems.
conversely new technology that ignores the availability of such data could be less attractive for adoption in industry.
given the novelty of the role emerging data scientist will also experience frustrations and inefficiencies which are another target for research.
while some of frustrations have been explored in some related work we expect distinct challenges and opportunities for software oriented data scientists.
we observed a strong influence of higher education on data science of the participants had phd or ms degrees .
to an extent this is a testament to the transfer of many years of software engineering research to practice.
the problems that data scientists work on bug prediction debugging release planning and anomaly detection in server telemetry and the analysis methods that they use for solving these problems are similar to those employed in the software engineering research field for the past decade.
as techniques that were once novel in the research literature become standard among data scientists researchers will need to refocus to stay ahead.
as one example researchers could invent new analysis techniques to allow data scientists to analyze new kinds of data.
or they could focus on better tool support to automate the collection and analysis of data.
validating operationalized data solutions is a challenging task that requires careful inspection to underst and the provenance and distribution of each piece of data of the problem domain and of the practices used including assessing the quality of data.
we expect that debugging for software oriented data scientists is an important research topic to be addressed by the software engineering research community.
we believe that the strategies that data scientists use to ensure the impact and actionability of their work can also be used to increase the impact of software engineering research.
the data scientists shared the importance of going the last mile to operationalize the predictive models and tailor the value of their insights for each beneficiary.
our participants reported that while precision recall and roc curves are commonly used to evaluate predictive research they are not effective in gaining buy in from engineers or prompting management to take action.
to increase the impact of research we software engineering researchers must also operationalize the results of data analytics by defining actions and triggers than simply reporting increases in precision and recall.
.
practice the software world has changed over the past years .
with cloudbased systems the availability of operational data has significantly increased.
monetization of software now more heavily relies on a good understanding of how customers use software.
there are also new opportunities for more efficient software development such as testing in production and the ability to flight changes for a short time before making them final .
we believe that these changes lead to an increased demand for data scientists in the software industry similar to what we see in other industries .
by the u.s. may face a shortage of as many as people with analytical expertise and of .
million managers and analysts with the skills to make data driven decisions according to a report by the mckinsey global institute .
in this new world that is ruled by data software companies have to figure out what data scientists really are what skills they need who to hire and where to put them in their organization.
more concretely testers should be trained with a data science skill set as the assessment of software quality and correctness increasingly depends on analysis of large scale usage data .
the success stories activities and working styles of data scientists which we reported in this paper can serve as guidelines for structuring software organizations to include data scientists and to improve data driven engineering decision making.
for example organizations that would like to adopt and employ data scientists may structure their teams using the triangle model or the hub and spoke model described in section .
.
data scientists who are hired into software development teams can also learn how to improve the impact and actionability of data science work from the strategies shared by other data scientists.
.
education as illustrated by the polymath working style data science is not always embodied as a distinct role on the team but sometimes as a skillset that blends with other skills such as software development.
polymaths may become the prevalent work style if data science follows the precedent of software testing.
historically testing was the domain of a distinct role of testers.
later however with the rise of unit testing and test driven development testing became a skill the developer role practiced as well .
similarly over time data science may become less of a distinct role and more a skillset that 104many team members employ .
indeed every team role has distinct information needs that could be answered through data analysis .
for instance program managers have questions about feature use testers have questions about hot paths developers have questions about execution.
this implies that there is increasing demand for an integrated software engineering and data science curriculum.
the characterization of data scientists in this paper can also be used in software engineering courses to illustrate real life data science.
the activities that data scientists participate in and the skill sets required can be useful to undergraduate and graduate educators who teach computer science statistics and data science courses.
data scientists need to combine a deep understanding of software engineering problems strong numerical reasoning skills strong programming skills and the ability to communicate the value of models and insights in domain and business specific terms.
computer science students commonly hold wildly inaccurate preconceptions about the character of the work they will encounter in their future software engineering careers .
conclusions in this paper we characterized the role of data scientists in a large software company.
we observed a demand for designing experiments with real user data and reporting results with statistical rigor .
we shared activities several success stories and five distinct styles of data scientists.
we reported strategies that data scientists use to ensure that their results are relevant to the company.
for future work we plan to conduct a large scale survey of data scientists to quantify the working styles and tasks observed in this study and to shed light onto the challenges associated with data science work .
.