evolutionary robustness testing of data processing systems using models and data mutation daniel di nardo fabrizio pastore andrea arcuri lionel briand interdisciplinary centre for security reliability and trust snt centre university of luxembourg luxembourg email daniel.dinardo fabrizio.pastore andrea.arcuri lionel.briand uni.lu abstract system level testing of industrial data processing software poses several challenges.
input data can be very large even in the order of gigabytes and with complex constraints thatdefine when an input is valid.
generating the right input data tostress the system for robustness properties e.g.
to test how faultydata is handled is hence very complex tedious and error pronewhen done manually.
unfortunately this is the current practicein industry.
in previous work we defined a methodology to modelthe structure and the constraints of input data by using umlclass diagrams and ocl constraints.
tests were automaticallyderived to cover predefined fault types in a fault model.
inthis paper to obtain more effective system level test cases wedeveloped a novel search based test generation tool.
experimentson a real world large industrial data processing system showthat our automated approach can not only achieve better codecoverage but also accomplishes this using significantly smallertest suites.
i. i ntroduction data processing software is an essential component of systems that aggregate and analyse real world data thereby enabling automated interaction between such systems and thereal world.
examples are search engines that return stockquotes or personal information collected from the web web applications that show real time airplane positions devices such as specialised eyeglasses that capture images andprovide contextual suggestions and phones able to translatein real time the words of a road sign .
robustness is the degree to which a system or component can function correctly in the presence of invalid inputs orstressful environmental conditions .
robustness testing ofdata processing software in the presence of invalid inputs is ofparticular importance because of the impact unexpected fail ures may have.
for example data faults in the transmission ofan airplane position may crash airplane tracking applications while malformed html pages may lead a web crawler toreport stock market collapses and cause panic to end users .
robustness testing of data processing software is often complicated by the complex structure of the input data.
awell known example is an html page that contains manyblocks some of which are kept hidden or contain dynamicinformation.
similar complexity characterises other kinds ofprocessing systems for example the data acquisition daq systems developed by our industrial partner ses to pro cess satellite transmissions .
when performing robustnesstesting software engineers need to handcraft complex datastructures where valid and faulty values need to be insertedwhile taking care to preserve all the relationships among thedata fields.
handcrafting huge amounts of complex data isparticularly time consuming and error prone.in we introduced a technique to test the capability of data processing software to identify invalid test inputs thatmatch a given fault model.
the technique uses a set of genericmutation operators to generate test inputs by sampling andmutating field data.
the technique ensures that each possiblefault instance characterised by a fault model is covered by thegenerated test cases.
in this paper we tackle the more general problem of generating minimal robustness test suites with high faultrevealing power for data processing systems.
when dealingwith robustness testing a single test generation criterion forexample the coverage of the given fault model implementedin is not enough.
multiple factors must be considered forexample the presence of multiple data faults in the same input the coverage of functional specifications in addition to faultmodels and the generation of a minimal number of test cases.satisfying multiple criteria when generating robustness testsuites may easily lead to combinatorial explosion and specifictechniques able to deal with scalability issues are required.
when addressing complex problems where there is a large space of candidate solutions and one wants to choose asolution that maximises some chosen criteria metaheuristicsare a plausible solution .
metaheuristic algorithms such asevolutionary algorithms identify optimal solutions for a prob lem by iteratively building candidate solutions and by testingthem to identify the one that best achieves the objectives.
ateach iteration new candidate solutions are built by means ofa tweak operation that is applied on a copy of a candidatesolution.
the tweak operation allows the algorithm to explorethe search space looking for an optimal solution.
testing techniques based on metaheuristic search focus mostly on unit testing while techniques that tackle testingat the system level either address the problem of testing non functional properties such as execution time or deal withthe problem of testing systems where the costs of testing donot depend on complex input data structures such as in thecase of embedded systems working with input signals .
in this paper we propose a model based evolutionary algorithm that relies upon a data model and a set of data mu tation operators to build system test suites for data processingsystems that optimises multiple objectives.
the evolutionaryalgorithm uses data sampling and data mutation operatorsto generate new test inputs and relies upon four differentmodel based and code based fitness functions to evaluate howwell each test input contributes to a proper robustness testsuite.
model based fitness functions exploit the data modelto generate test cases that cover important aspects of thebehaviour of the system by ensuring the coverage of all the 30th ieee acm international conference on automated software engineering .
ieee g31 g32 g33 g32 g34 g35 g33 g33 g36 g37 g36 g38 g32 g37 g36 g38 g32 g37 g36 g38 g37 g36 g38 g37 g36 g38 g37 g36 g39 g40 g34 g41 g33 g42 g39 g40 g42 g39 g40 g32 g33 g32 g43 g43 g44 g44 g45 g43 g44 g44 g45 g43 g44 g44 g45 g36 g43 g43 g43 g43 g43 g44 g44 g45 g46 g37 g36 g38 g43 g43 g43 g43 g43 g44 g44 g45 fig.
.
simplified input data model for the ses running example.
sa streamattributes sc streamclass.
different types of test inputs processed by the system the presence of different types of data faults and the possibleviolations of the constraints among test inputs.
the code based fitness function has the goal of achieving the maximumstructural coverage of the system under test.
this paper contributes to the state of the art by proposing an evolutionary algorithm to automate robustness testing of data processing systems defining four fitness functions model based and code based that enable the effective generation of robust ness test cases by means of evolutionary algorithms performing an extensive study of the effect of fit ness functions and configuration parameters on theeffectiveness of the approach using an industrial dataprocessing system as case study.
the paper proceeds as follows.
section ii provides background information on the data modelling and mutation op erators used by the algorithm.
section iii presents a listof challenges that need to be addressed when building asearch algorithm for robustness testing.
section iv providesan overview of the evolutionary algorithm that we designedto generate robustness tests.
section v describes how datamutation is adopted to generate new test inputs during thesearch.
section vi presents the heuristic functions used toevaluate candidate solutions.
section vii describes the seedingstrategy integrated into the algorithm.
section viii details howwe automate the execution and validation of the generatedtest suites.
section ix presents the empirical results obtained whereas section x discusses related work.
finally section xiconcludes the paper.
ii.
b ackground on data modelling the evolutionary algorithm presented in this paper uses data models to automatically generate test inputs.
such datamodels are designed according to the data modelling method ology introduced in previous work .
this sectionprovides an overview of the methodology.
the methodology presented in uses uml class diagrams to capture the structure of inputs and outputs relies uponobject constraint language ocl expressions to definerelationships between the inputs and outputs and uses uml1 context vcdu inv let framecount integer self.header.vcframecount vdcuindex integer self.virtualchannel.vcdu indexof self previous vcdu self.virtualchannel.vcdu at vcduinde x previousframecount integer previous.header.vcframecount7 in ifpreviousframecount then framecount previousframecount else previousframecount and framecount 0endif implies vcduevents.allinstances exists e e.eventtype counter jump fig.
.
input output constraint for the counter jump error event.
stereotypes to capture a fault model driving the generation of test cases.
to briefly present the methodology we show how it can be applied to model the transmission data processed by ses daq a daq system developed by ses see for thecomplete specifications of the transmission data .
ses daqprocesses bytestreams of transmitted satellite data.
developingsuch a model requires time and engineering effort but the costof modelling was considered acceptable by ses engineers .
figure shows how we model ses daq input data according to our methodology.
the model captures the structureof a transmission we use uml classes to represent elementsthat contain multiple fields while we use uml attributesto model elements that cannot be further decomposed.
forexample it shows that each transmission consists of a sequenceofvirtual channel data units vcdus .
each vcdu begins with a header followed by a packetzoneheader and a packetzone that may contain a sequence of packets if the packet zone is active .
the vcdus in a transmission may belong to different virtual channels.
associations are used to represent containment relationships.
in figure the classes that model the vcdu and itsheader are connected by an association.
we use generalisationsto indicate when a data field can have multiple differentdefinitions.
for example in the case of ses daq the packetzones can be active i.e.
they transmit data or idle i.e.
they donot transmit anything .
the data model of ses daq reflectsthis characteristic with generalisations for packetzoneheaderand packetzone both of which have idle and active subtypes.
we use class attributes to represent the transmitted binary information e.g.
checksums frame counters or data .
forexample attribute sequencecount of class packet is used to store information about the packet order.
constraints between inputs and outputs are represented using ocl.
these constraints are used as an oracle to validatethe execution of automatically generated test cases a violatedconstraint indicates that the system under test does not producethe expected output.
in the case of ses daq we use oclconstraints to model the error messages expected in the pres ence of specific faults in the input data.
for example figure 2shows an ocl constraint that states that the frame count ofa vcdu should be greater by one than the frame count ofthe previous vcdu on the same virtual channel.
otherwise an error event counter jump should exist in the system output log file.
stereotypes are used to capture the fault model of the system and are used to identify the fields that can be mutatedto generate new inputs.
the stereotype inputdata is used by the software engineer to tag the classes that model input data to 127contrast them with output data classes which do not need to be mutated.
the two stereotypes identifier and measure are used to indicate the mutation operators to apply to class attributes.the stereotype derived is used to tag class attributes that need to be derived from other attributes after every mutation inorder to prevent trivial inconsistencies e.g.
it is used to updatethe checksum field when other fields are mutated .
the stereotypes streamclass and streamattributes are used to automate the loading of data from bytestreams but are outof the scope of this paper see for details .
iii.
c hallenges for the search based genera tion ofrobustness system tests search algorithms are useful when addressing complex problems where there is a large space of candidate solutions and one wants to choose one that optimises some chosencriteria.
a typical example in software engineering is test datageneration in which a tester might want to find test data thatmaximises code coverage or triggers new failures.
there are many different kinds of search algorithms including genetic algorithms which have been widely appliedin many fields.
not all search algorithms perform well onall types of problems.
each problem can have special char acteristics that are better exploited by some search algorithms whereas others might struggle.
in this paper we identify four main challenges that need to be addressed when designing an algorithm for the automaticgeneration of a system test suite configuring the algorithmto properly find a tradeoff between exploration and exploitation of the search landscape building a tweak operation defining effective fitness functions for the problem under investigation and integrating effective seeding techniques i.e.
techniques that speed up search by exploiting knowledgeabout the input domain .
one of the main discerning characteristics among search algorithms is the tradeoff they have between exploration and exploitation of the search landscape.
on one hand some algorithms e.g.
hill climbing put more emphasis on theexploitation of the search landscape.
this means that givenan evaluated solution they will only look at close solutionsto see if any small change could improve the chosen criterion.on the other hand other algorithms put more emphasis onthe exploration of the search space.
typical examples arepopulation based algorithms in which a diverse set of solu tions is maintained to consider different areas of the searchlandscape at the same time in case one area turns out to featuremore fitting solutions than the others.
in the context of robustness testing exploration is very important since it enables the construction of test suites witha high diversity of test cases.
in the case of ses daq for example one wants to generate test inputs that includeidlepacketzones and test inputs that include activepacketzones at the same time.
however exploitation leads to coveringinvalid inputs that include a specific set of data faults.
forexample ses daq might be able to properly process aninvalid input containing a packet that breaks the constraint infigure but it may crash in the presence of both a brokenconstraint and a duplicated packet.
to satisfy this case thealgorithm must be able to exploit the search space by bothbreaking the constraint of figure and by duplicating a packet.
tweak operations play an important role in guiding the search algorithm towards the exploitation of the search land scape they modify existing solutions to generate new can didate solutions.
when defining tweak operations the char acteristics of the domain should be carefully considered.
forexample when a solution is represented in a binary format a tweak operation could just flip one or more bits.
however flipping bits on a complex data transmission file would likelyresult in meaningless or trivially wrong input data.
therefore one would need to exploit the information in the data modelsto automatically derive better tweak operations for exampleby applying model based data mutation.
in the context of robustness system testing one still wants to generate faulty input data but in a nontrivial way e.g.
byincluding multiple faults in a same test input .
however thereis a limit on the number of faults that can be included in asame input.
although having multiple mutations that affect asame test input might help stress the robustness of the system a number of faults that is too high might lead to inputs that aretrivially recognised and discarded by the system under test.
another very important aspect for the success of a search algorithm is the definition of a fitness function used to evaluate how close a solution is to optimising a chosen criterion.
sucha function is problem dependent and effort must be madeto design a proper one.
ideally one would like to exploit asmuch domain information as possible but this might lead tocomputationally expensive fitness functions.
the more timeconsuming the fitness function the fewer solutions can beevaluated by a search algorithm in the same amount of time.in the case of model based testing this tradeoff can be verycritical as fitness functions calculated on models can bemuch quicker to compute than ones calculated from test caseexecutions e.g.
using structural coverage .
finally in the presence of complex search spaces the quick identification of a proper solution is often aided bythe adoption of techniques that exploit knowledge about thesearch space to build solutions that improve the effectivenessof the search algorithm these are known as seeding techniques.
different seeding techniques have been adopted in the contextof software testing for instance a well known approach usedby techniques that generate inputs for unit testing is the reuseof constant values taken from the source code of the softwareunder test .
however smart seeding at the system level isnot as simple as that and poses new challenges.
iv .
a nevolutionary algorithm for robustness testing to address the problem of generating test cases to stress the robustness of software at the system level we propose anovel evolutionary algorithm based on an archive.
figure 3shows the algorithm.
in the context of this paper a solution to the search problem consists of a test suite that effectively tests the capability ofthe software to handle invalid data.
a test suite is a collectionof test inputs i.e.
data files conforming to a data model to beprocessed by the software under test.
during the search testinputs are generated and those are then aggregated to form afinal test suite to give as output to the user.
in our context thesolution to the search problem i.e.
the test suite cannot berepresented using a fixed size data structure because the sizeof the test suite i.e.
the number and size of the test cases itcontains cannot be known a priori.
we did not directly use a traditional search algorithm e.g.
a genetic algorithm or a hill climbing algorithm due 128require fd the field data used to generate test inputs require dm the data model used to drive tweaking require budget the maximum proportion of the search space that needs to be visited require pfield probability of sampling a new individual from the field data require pmutation probability of mutating an individual just after sampling it require pseeding probability of using seeding to sample from the field data require minsize the minimum size of a test input require maxsize the maximum size of a test input require maxmutations the maximum number of mutations for a same test input ensure archive the archive containing the minimised robustness test suite total while total budget do if archive.individualavailable maxmutations false or random p field then ind samplenew fd dm minsize maxsize p seeding ifrandom p mutation then mutate ind end if else ind archive.sampleacopy mutate dm ind end if if improving ind archive then archive.add ind forprev in archive do ifsubsume ind prev then archive.remove prev end if end for end if total total ind.size end while fig.
.
an evolutionary algorithm for robustness testing to the special characteristics of the addressed problem.
for example in system level testing each test case execution canbe computationally very expensive.
so a traditional geneticalgorithm that works on a population would likely be toocomputationally expensive to use.
furthermore special carewould be needed to design a crossover operator that generatesvalid offspring.
on the other hand hill climbing algorithms putemphasis on the exploitation of the search landscape this isachieved by using a tweak operation that iteratively improvesa single solution.
in our case it is hard to envision a singletweak operation that works at the test suite level and allows forthe building of a minimised test suite that contains test inputswith high diversity.
our customised evolutionary algorithm is based on the use of an archive of test cases initially empty.
archives have beenused in prior work related to multi objective search algorithms e.g.
.
the archive plays the important role ofguiding the algorithm towards the exploitation of the searchlandscape like hill climbing while maintaining at the sametime some characteristics of population based algorithms.
likehill climbing the algorithm improves only a single test input ateach iteration but uses the archive to keep a collection of thebest test inputs found so far i.e.
the test suite .
furthermore our algorithm keeps solutions in the archive that are differentfrom each other to maximise exploration like populationalgorithms.
the size of the archive can vary during the searchand the test suite is minimised by keeping in the archive onlythe best individuals that contribute to overall fitness of thewhole test suite i.e.
the archive .
finally the individuals inthe archive are tweaked one at a time thus exploiting thesearch landscape and creating new individuals that improvethe overall fitness of the test suite.
our novel algorithm addresses all the challenges presented in section iii.
the tradeoff between exploration and exploita tion is controlled by configuration parameters that regulate the probability of tweaking an individual from the archiveversus generating a completely new individual parameterp field in figure the probability of working with correct test inputs versus the use of test inputs that contain at leastone fault parameter p mutation in figure and the maximum number of data faults a test input may contain parameter maxmutations in figure .
tweaking operations are implemented by means of mutation operators describedin while specific fitness functions have been developedand are described in section vi.
the probability of seeding iscontrolled by the parameter p seeding further details are given in section vii .
at the first iteration the archive is empty and a new random individual needs to be sampled.
generating new inputdata completely at random would result almost certainly intrivially wrong data.
an alternative is to sample according tosome specific rules if those can be defined for the addressedproblem domain e.g.
a grammar in the testing of parsers .in our case we used a different approach that relies on thesampling of field data.
in the case of industrial data processingsystems we can have access to very large amounts of existingvalid field data.
if not already available a large field datapool can be constructed and then used by the evolutionaryalgorithm to sample from.
new individuals are sampled from the available field data by means of the function samplenew which randomly selects and returns a chunk of the available field data line .the function samplenew receives as input two integer values minsize and maxsize that indicate the minimum and maximum size of the data chunk to be sampled.
since in general an inputfor a data processing system does not have a size that is fixeda priori we leave it to the software engineers to decide therange of the input size according to their domain knowledge for example in the case of ses we choose the values and 500for the minimum and maximum values respectively wherethese values represent the number of vcdus .
the evolutionary algorithm incrementally builds a test suite by keeping only those individuals in the archive thatcontribute to improving the overall combined fitness of allthe currently stored test cases which will form the final testsuite.
the algorithm generates test inputs by applying thetechnique presented in that is by sampling chunks of datafrom the field data and by mutating these chunks to generatepossibly faulty inputs.
unlike in this paper we considerthe possibility of applying multiple mutations to a same testinput.
each test input is thus represented in terms of the offsetfrom the beginning of the original field data file the length ofthe sample and a list of the mutations that have been appliedto the sample.
the algorithm keeps exploring the search space until a given stopping condition is reached line .
since ouralgorithm focuses on the generation of test inputs for dataprocessing systems we express the stopping condition in termsof the amount of data processed to generate test cases that isthe sum of the size of all the test inputs generated during thesearch.
at each iteration the algorithm increments the counterof the data processed line .
in the specific case of ses wemeasure the size of a single test input in terms of the numberof vcdus that it contains but different measurement units meaningful for a given domain may be used for differentsystems.
at each iteration the algorithm works by tweaking an individual i.e.
a test input .
each individual is created by 129sampling either the field data or the archive this choice is driven by a probability value the parameter pfield in figure which indicates the probability of sampling a new individualfrom the field data see lines and .
if no individuals are available in the archive the algorithm samples the field data see the conditionarchive.individualavailable false in line .
this happens in two situations when the archive is empty i.e.
on the firstsearch iteration or when all the individuals in the archive havealready been mutated a maximum numbers of times.
softwareengineers can specify the maximum number of mutations thatcan be applied to the same test input.
this is done to avoidtrivially invalid inputs.
although in principle a test input canbe mutated an infinite number of times the presence of toomany mutations i.e.
data faults on the same test input mighttransform the test input into a trivially invalid input that iseasily detected by the data processing system and does nothelp to extensively test its robustness.
lines to show that the parameter p mutation regulates the probability of mutating an individual just after creating it that is the probability of working with individuals that containat least one data mutation.
lines and show that everytime the algorithm samples a copy of an individual from thearchive it applies a mutation to it.
this is done to create a copythat differs from the original one.
this tweaking operation isfurther described in section v. lines and show that an individual is added to the archive only if it improves the overall fitness of the archive.similarly the algorithm removes any individual already presentin the archive that is subsumed by the last one added seelines to .
individuals that are subsumed by new onescan be safely removed from the archive because they do notcontribute to the overall fitness of the archive.
section videscribes the assessment procedure adopted to measure howindividuals contribute to the fitness of the archive.
v. t weaking by means of data muta tion the search algorithm tweaks individuals by applying the data mutation operators described in .
there are six mu tation operators that can be applied to an individual classinstance duplication class instance removal class instancesswapping attribute replacement with random attributereplacement using boundary condition and attribute bitflipping.
to apply a mutation operator to an individual thealgorithm loads into memory the data chunk corresponding tothe test input as an instance of the data model.
according to each mutation operator can be applied only to a specific set of targets.
this is done to avoid makingmutations that will result in the generation of trivial datafaults and to ensure conformance with a domain specific faultmodel.
software engineers specify the targets of each mutationoperator by using appropriate stereotypes in the uml classdiagram data model .
these stereotypes indicate the elementsthat can be mutated and the operators that can be applied tothem.
to mutate an individual the algorithm randomly picks amutation operator identifies a possible target for the operatoron the current individual and applies the operator on thetarget.
for example during the generation of test inputs forses daq the algorithm may randomly choose the operatorattribute replacement with random.
it then selects one of theattributes that can be mutated according to that operator forexample the attribute sequencecount of class packet .
finally it identifies a specific instance to mutate which means that itchanges the value of the attribute sequencecount of one of the packet instances contained in the current test input.
in case a selected operator cannot be applied on a given individual another operator is randomly selected this canhappen for example if the algorithm selects the attributesequencecount for replacement with random but the current test input contains only idlepacketzones which according to figure does not contain any packet instances .
vi.
a ssessment procedure we identify four objectives that should be fulfilled to effectively stress the robustness of the software o1 include input data that covers all the classes ofthe data model o2 include data faults such that all the possible faultsof the fault model have been covered o3 cover all the clauses of the input output con straints o4 maximise code coverage.
each of the four objectives captures how well the test inputs cover some specific targets respectively the classes ofthe data model the faults of the fault model the clauses ofthe input output constraints and the code instructions.
eachobjective defines a set of targets e.g.
instructions in code coverage that the algorithm aims to cover.
a given objective isfully achieved by a test suite if each of its targets is covered byat least one test input of the test suite.
a portion of the targetsfor ses daq along with their coverage for three test inputs areshown in table i covered targets are marked with an x .
wedescribe below each objective and how fitness improvementsand subsumption can be defined in terms of these objectives.
objective o1 ensures that the test suite includes test inputs that cover all the classes of the data model.
each class of thedata model is univocally represented by an objective target.
atest input covers a class if it contains at least one instance ofthe class.
table i shows that input i3covers among others classes activepacketzone and idlepacketzone input i1covers only class activepacketzone there are no idle packets in i1 .
objective o2 ensures that an instance of each class and attribute has been mutated at least once by each mutationoperator that can be applied to it to generate test inputscovering all the faults of the fault model .
our algorithmgenerates faulty data i.e.
new test inputs by applying mutationoperators on instantiated field data objects.
since the attributesand classes of the data model can be mutated in differentways by applying different mutation operators for each testinput we keep track of which mutation operator has beenapplied to a specific class attribute.
table i for example shows that input i1 contains at least one instance of a vcdu whose vcframecount has been mutated with the operator attributereplacementwithrandom while input i3contains both a vcdu with a deleted packet operator classinstanceremoval is marked as being applied on an instance of class packet and a vcdu whose versionnumber has been replaced with a random value see operator attributereplacementwithrandom marked for the attribute versionnumber .
objective o3 ensures that every clause of the input output constraints has been exercised.
input output constraints areexpressed in the form of implications.
the left hand side ofthe implication captures the characteristics of the input under 130which a given output is expected.
the right hand side captures the characteristics of the expected output see figure .
to measure how well the test suite stresses the conditions under which a given output is generated it is enough to focuson the clauses contained on the left side of the implication i.e.clauses defined over the characteristics of the test input .
foreach clause we aim to have at least one test input that causesthe clause to be true and another that causes the clause to befalse.
each clause is thus associated to two different targetsfor objective o3 that trace whether the clause is true false atleast once in the input.
table i shows some targets derived forthe constraint in figure .
table i shows that input i1 has atleast one vcdu whose framecount does not correspond to the previousframecounter plus one this is the effect of the mutation operator attributereplacementwithrandom applied to the attribute vcframecount .
the same clause can be both true and false within the same test input.
this is the caseof input i1 that in addition to a vcdu with the invalidframecount also includes vcdus with a valid framecount i.e.
a framecount equal to previousframecount plus one .
it is noteworthy that by focusing on the input clauses we can measure objective o3 without the need to execute thesystem under test i.e.
without generating an output for a giventest input .
this makes the search algorithm scale even whenthe execution of the test cases is particularly time consuming.
objective o4 aims to maximise the structural coverage of the source code.
this is one of the means adopted by softwareengineers to ensure that all the implemented features havebeen tested at least once.
each instruction in the system is anobjective target.
in our implementation we measure coverageusing jacoco a toolkit for measuring java code coverage .
the main limitation of measuring the structural coverage of system test cases is that it requires the execution of the systemunder test.
this may slow down the overall search processconsiderably and prevent the generation of results in practicaltime.
furthermore in certain contexts for example systemsdeployed on dedicated hardware structural coverage might notbe easily calculated in practice.
for this reason the empiricalstudy presented in section ix aims also to determine to whichextent objective o4 is subsumed by other objectives.
our algorithm works with objectives that are not conflicting and aims to maximise the coverage of all targets.
therefore the algorithm does not rely upon the computation of paretofronts a solution adopted by others e.g.
.
our algorithm adds to the archive only test inputs that improve the overall fitness i.e.
a test input must cover at leastone target not covered by the other inputs in the archive .furthermore the algorithm removes from the archive any test inputs subsumed by new test inputs.
a test input i primesubsumes an input i prime primeif and only if i primecovers all the targets covered byi prime prime and either i primecovers at least one target not covered byi prime primeor the size of i primeis smaller.
for example given an archive that contains inputs i1and i2 our algorithm creates input i3by tweaking a copy of input i2 i.e.
by deleting a class instance .
i3is added to the archive because it covers target packet classinstanceremoval not covered by i1and i2 .
given that i3subsumes i2the algorithm will then remove i2from the archive thus minimising its size.
vii.
i nput seeding to further improve search results we developed a novel model driven seeding strategy that guides the search towardsthe identification of a diverse and complex set of test inputs.
to stress diversity in the data the algorithm aims to generate test inputs that cover all the available data types see objective o1 described in section vi .
given that some ofthese data types may occur rarely in the field data it might behighly improbable to cover these types by means of randomsampling.
to guarantee the coverage of all the data types itis enough to know the locations of the different data types.for example within a field data sample of ses daq we mayhave idle packets only in a very small number of the vcdusof the bytestream.
having the location information makes iteasier for the search algorithm to load multiple data chunkscontaining idle packets otherwise the chance of loading idlepackets is low when only resorting to random sampling.
to stress complexity our algorithm looks for test inputs that contain instances of two or more subclasses belongingto the same generalisation.
in the case of ses daq thiscorresponds to the case of an input containing a transitionbetween two alternate data types e.g.
from idle to active packetzone .
these complex inputs are interesting for robustnesstesting because one can assume that handling heterogeneousdata zones might be more prone to processing failures thanhomogeneous ones.
our seeding strategy works by first processing the field data to build a seeding pool that contains data chunks that are useful to stress both diversity and complexity.
to maximisediversity we identify for each class sthat is a subclass of a generalisation data chunks that contain at least one instance of the subclass s data chunks that contain only instances of the subclass s i.e.
data chunks that contain instances of sbut that do not contain instances of other classes belonging to the same hierarchy of s. to maximise complexity we identify for each pair of classes that belong to ageneralisation data chunks that contain at least one instance ofeach class in the pair.
in the case of ses daq this ensures thatwe test scenarios where two different data types are processed table i. a ssessment of three inputs of ses daq objective targetstest inputs i1 i2 i3objective o1transmission x x x sync x x x header x x x idlepacketzoneheader x x activepacketzoneheader x x x idlepacketzone x x activepacketzone x x x packet x x x ...objective o2header.versionnumber attributereplacementwithrandom x x header.vcframecount attributereplacementwithrandom x packet classinstanceremoval x packet classinstanceduplication packet classinstancesswapping ...objective o3true previousframecount x x x true framecount previousframecount x true previousframecount true framecount x x x ... false previousframecount false framecount previousframecount x x x false previousframecount x x x false framecount ...o4sesdaq.java line x x x sesdaq.java line x ... 131table ii.
l ist of the characteristics of the input da ta used to drive seeding for ses daq only idlepacketzoneheader instances are included at least one idlepacketzoneheader instance is included only activepacketzoneheader instances are included at least one activepacketzoneheader instance is included only idlepacketzone instances are included at least one idlepacketzone instance is included only activepacketzone instances are included at least one activepacketzone instance is included both idlepacketzone and activepacketzone are included both idlepacketzoneheader and activepacketzoneheader are included in sequence e.g.
idle and active packets .
table ii shows a list with the characteristics of the data chunks we identify for theses daq data model in figure .
the seeding pool can be used when a new individual is sampled.
when seeding is enabled the algorithm selects oneof the chunks in the seeding pool.
in the case of ses daq thisis done by first selecting one on the ten characteristics listedin table ii and then by loading a data chunk that presentssuch characteristics.
software engineers can tune the use ofseeding by means of a parameter for the search algorithm thatindicates the probability of applying seeding when sampling adata chunk from the field data p seeding in figure .
higher values of pseeding guarantee that all the characteristics are covered at the expense of a free exploration of thesearch space which may lead to the sampling of complex testinputs not identified by predefined seeding characteristics .
viii.
t esting automa tion the evolutionary algorithm generates a minimised robustness test suite that is kept in an archive.
the test suiteconsists of a set of test inputs that can be executed against thedaq system under test.
the oracle relies on the model basedautomated validation technique presented in and .
after the execution of a test case the oracle simply loads the test input and the test output as an instance of the datamodel and checks if the ocl constraints of the data modelare satisfied .
unsatisfied constraints indicate the presence of a failure i.e.
unexpected or missing output and are reportedto the software engineers.
similarly crashing executions arereported.
ix.
e mpirical ev alua tion we performed an empirical evaluation in order to respond to the following research questions rq1 how does the search algorithm compare with random and state of the art approaches?
rq2 how does fitness based on code coverage affect performance?
rq3 how does smart seeding affect performance?
rq4 what are the configuration parameters that affect performance?
rq5 what configuration should be used in practice?
a. subject of the study as subject of our study we considered the industrial sesdaq system.
ses daq is a good example of a data processingsystem dealing with complex input and output data written injava having bytecode instructions .
the data modelof ses daq includes classes with attributes and 1this is done by using the mdt uml2 library as an input for our approach we considered alarge transmission file containing field data provided by ses the same adopted for the empirical evaluation in .
the sizeof the transmission file is about gigabytes containing 1million vcdus belonging to four different virtual channels.
b. experimental settings to answer our research questions we carried out a series of experiments.
since our search algorithm depends on several parameters we evaluated several possible configurations.
the minsize and maxsize parameters i.e.
the minimum and the maximum size of a test input measured in vcdus werefixed to and respectively.
we used three different valuesforp field .
.
and .
.
for pmutation we considered the values .
.
and .
.
for maxmutations we used and .
for pseeding we used two values .0and0.
which means that in one case the seeding strategy was not used while in the other case the seeding was applied with a probability every time a new input was sampled.
in order to give the algorithm some degree of freedom when exploring the search space we do not consider the casein which inputs are selected exclusively according to the smartseeding strategy.
finally we considered cases with and withoutthe code coverage fitness function.
this led to different configurations.
the search budget in the case of ses daq the number of vcdus inspected when building new inputs during search might vary from project to project.
for this reason we eval uated each of the configurations of the algorithm on fivedifferent search budgets from to250 vcdus in steps of 50k .
vcdus correspond to one fourth of the transmission file used for building test inputs.
this led to108 different configurations of the search algorithm.
because search algorithms have a random component to take into account the effects of such randomness on the finalresults each of the experiments was repeated five times with adifferent random seed.
this led to a total of runs of the algorithm.
because each run could take betweenten and thirty five hours we used a large cluster of computersto run these experiments .
c. cost and effectiveness metrics we want to assess and compare cost effectiveness among automatically generated test suites.
code coverage is used as a measure of test effectiveness as it helps assess howcomplete the test suite is from a structural and functionalstandpoint.
we measure the code coverage in terms of thenumber of bytecode instructions covered by the test suiteby using jacoco.
maximising code coverage within timeconstraints is often an objective among system testers.
evensmall improvements in coverage can help exercise importantcorner cases.
for example in our case study additionallycovered instructions often turned out to be critical blocks ofcode including exception handling and critical scenarios.
wealso compare test suites with respect to their size because given two test suites having identical coverage one wouldprefer the smaller one entailing a lower testing and debuggingcost.
the size of a test suite is measured in terms of thenumber of test inputs in the test suite.
smaller test suites are ofpractical importance as in many systems system testing mustbe performed on actual deployment hardware or a dedicated realistic testing platform which requires some degree of tuning 132table iii.
c omparison between the best search algorithm configura tion and random search .
s eeding disabled .c ode coverage fitness enabled .
budget configuration coverage tests avg min max avg min max 50kbest r .
m n .
.
bo r .
m n .
.
rand r m n .
.
46100kbest r .
m n .
.
bo r .
m n .
.
rand r m n .
.
57150kbest r .
m n .
.
bo r .
m n .
.
rand r m n .
.
64200kbest r .
m .
n .
.
bo r .
m n .
.
rand r m n .
.
66250kbest r .
m n .
.
bo r .
m n .
.
rand r m n .
.
or simulation to run test cases.
access time to such platforms can also be limited.
d. rq1 how does the search algorithm compare with random and state of the art approaches?
the effectiveness of a search based algorithm highly depends on the nature of the problem to solve.
for example ifthe solution space of the problem is flat that is if the fitnessfunction does not provide any gradient then a search basedalgorithm might perform even worse than a random approach.
rq1 aims to evaluate the usefulness of the search based algorithm proposed in this paper by comparing it with arandom algorithm and with a simple model based algorithm.
however a direct comparison with a trivial random generation approach would not bring any useful result.
for example test inputs containing random data may be trivially invalid anduseless for extensively testing the system.
for this reason weuse as baseline for the comparison the random algorithm em ployed in previous work the algorithm samples a portion ofthe field transmission file randomly selects a mutation operatorand applies it to one of the elements where it can be applied.
the algorithm employed in does not support the generation of test inputs of variable size furthermore it does notminimise the generated test suite.
to address these limitationsand perform a fairer comparison we execute an improvedversion of the random algorithm employed in by using oursearch algorithm with a specific configuration minsize and maxsize are set to the same values as the search algorithm p field to generate a new test input at each iteration by sampling the field data and pmutation t oa l w a y s mutate the sampled test input like in .
the value chosenformaxmutations is irrelevant because we generate a new test input at each iteration because of p field .
to compare with a simple model based approach we consider the results achieved with the all possible targets apt approach proposed in .
the apt mutation strategy ensures that each class or attribute of the data model is mutatedat least once by each of the mutation operators that can beapplied to it.
table iii shows the comparison of the search algorithm with random search.
columns budget and configuration report the search budget and the best configurations found columncoverage reports the average minimum and maximum coverage achieved by the test suite and column tests reports the average minimum and maximum number of test inputs ineach test suite.
for each search budget we identified the bestconfiguration out of the configurations of the search algo rithm with seeding disabled best in table iii .
furthermore we identified the best configuration on average over all thesearch budgets bo in table iii .
the comparison with bo is fairer since the configuration indicated as bo is not optimised for a specific search budget but is a stable good overallconfiguration.
for each configuration we report the probabilityof random sampling r the probability of applying mutation when sampling m and the maximum number of allowedmutations in a test n .
the best values for maximum coverageand minimum test suite size appear in bold.
our comparisondid not include configurations with seeding because it is anoptimisation of the search algorithm.
the impact of seeding isaddressed in rq3.
the search algorithm presented in this paper provides better results than both the random approach and the apt algorithmpresented in .
apt achieves an average coverage of 23283instructions which is less than the coverage obtained withthe search and random approaches see table iii .
this ismostly because apt focuses on the coverage of the modeland stops after sampling many fewer vcdus at most 000vcdus in while the lowest search budget is .in summary the search algorithm presented in this papergenerates significantly less test inputs while achieving bettercoverage.
this mainly results from the adoption of fitnessfunctions that help minimise the test suites by keeping onlyuseful test inputs in the archive.
results also show that the search algorithm achieves better coverage than the random approach.
the difference in coverageranges between .
and instructions.
though the differencein coverage might not appear large as discussed earlier evensmall increases in coverage might exercise important cornercases.
once again the search algorithm generates significantlysmaller numbers of test inputs e.g.
.
versus on averagefor a 150k budget .
our conclusions hold even considering the random variation across runs which is small as shown by the min andmax values appearing in table iii.
e. rq2 how does fitness based on code coverage affect performance?
to answer rq2 table iv shows for each search budget the best configurations best with and without code coverage fitness code with seeding enabled and disabled seeding .furthermore table iv also reports the configuration thatperforms better on average over all the search budgets bo in table iv .
enabling code coverage fitness results in higher coverage but it comes however at the expense of a significantly largertest suite.
all configurations in table iv with higher coverageenable coverage fitness whereas all the ones with smallertest suites do not.
for small search budgets the differencein coverage when enabling coverage fitness is small thussuggesting that relying on model information is enough notrequiring test execution for generating test cases.
f .
rq3 how does smart seeding affect performance?
table iv shows that smart seeding has a positive effect on cost effectiveness when the search budget is above 150k.
in 133table iv .
c omparisons between best search algorithm configura tions based on whether code coverage is employed in the fitness ev alua tion column code and on whether smart seeding is activ a ted column seeding .
budget code seeding configuration coverage tests mut.50kfalse .
best r .
m n .
.
.
true .
best r .
m n .
.
.
false .
best r .
m n .
.
.
true .
best r .
m n .
.
.
true .
bo r .
m n .
.
.3100kfalse .
best r .
m n .
.
.
true .
best r .
m n .
.
.
false .
best r .
m n .
.
.
true .
best r .
m n .
.
.
true .
bo r .
m n .
.
.6150kfalse .
best r .
m n .
.
.
true .
best r .
m n .
.
.
false .
best r .
m n .
.
.
true .
best r .
m n .
.
.
true .
bo r .
m n .
.
.5200kfalse .
best r .
m n .
.
.
true .
best r .
m .
n .
.
.
false .
best r .
m n .
.
.
true .
best r .
m n .
.
.
true .
bo r .
m n .
.
.0250kfalse .
best r .
m n .
.
.
true .
best r .
m n .
.
.
false .
best r .
m n .
.
.
true .
best r .
m n .
.
.
true .
bo r .
m n .
.
.
these cases smart seeding is always part of the configurations that achieve the highest coverage or the lowest number of testcases.
g. rq4 what are the configuration parameters that affect performance?
for each of the configurations we calculated their average coverage over all the search budgets thus considering25 test suites for each configuration .
we ranked these config urations based on their average coverage not reported in thepaper due to space constraints .
a detailed analysis showedthat coverage fitness was enabled in the top configurations and never by the worst thus showing its importance toguide the search.
the effect of seeding is however much lessvisible as it depends on other parameters.
different parameters have different effects depending on the selected search budget.
for example table iv showsthat for small search budgets i.e.
search budgets includingat most vcdus search achieves better results whenmore focused on exploitation i.e.
having a low probabilityof random sampling like .
and .
.
table iv also showsthat with higher search budgets in the absence of coveragefitness or seeding putting more emphasis on the explorationof the search landscape i.e.
using a .
probability of randomsampling pays off.
this result is expected since with a highersearch budget random sampling allows for the sampling ofmuch of the field data transmission file roughly one fourth.
further table iv shows some interesting side effects.
for search budgets above or equal to using eitherseeding or code coverage decreases the need to explore thesearch landscape probability of random sampling decreasingfrom .
to .
.
if both are used even less exploration isneeded probability of random sampling equal to .
.
thisphenomenon can be easily explained for smart seeding as itdoes provide more diverse and useful samples in the searchtable v .
s ta tistical comparisons of best overall bo configura tion against best with no seeding best with no code coverage fitness function and random with code coverage .
budget configuration coverage a12 p value50kbo .
bo no seeding .
.
.
bo no code .
.
.676rand with code .
.
.530100kbo .
bo no seeding .
.
.835bo no code .
.
.037rand with code .
.
.022150kbo .
bo no seeding .
.
.403bo no code .
.
.012rand with code .
.
.027200kbo .
bo no seeding .
.
.144bo no code .
.
.012rand with code .
.
.012250kbo .
bo no seeding .
.
.144bo no code .
.
.012rand with code .
.
.
landscape.
in the case of code coverage this phenomenon occurs because some rare inputs contribute to code coveragebut not to other search objectives.
enabling code coveragefitness prevents the algorithm from discarding rare inputs thatcontribute to code coverage once they are found.
in the absenceof code coverage fitness such rare inputs can be easily missedif there is low variety in the test inputs stored in the archive.in the case of higher values of exploration there is going to behigher variety in the archive which increases the probabilityof having those rare inputs.
for completeness table iv reports also the average number of mutations per test input column mut.
.
although the presence of multiple mutations i.e.
data faults in a sameinput may trigger hard to detect complex failures it couldalso complicate debugging.
table iv shows that on averagethe number of mutations is low compared to the maximumallowed e.g.
.
versus for a budget of 250k thus makingeventual debugging operations easier .
h. rq5 what configuration should be used in practice?
the best overall configuration see table iv is using pfield .
small probability of sampling a new test data at random instead of reusing the ones already in the archive p mutation do not mutate new inputs immediately when sampled and maxmutations .
furthermore it does use seeding and the code coverage fitness function.
for each search budget we ran experiments only five times per configuration due to the high time cost of running them.on one hand this is useful to get a general picture of cost effectiveness trends among different parameter configurations.on the other hand it makes it harder to compare two specificconfigurations as the randomness of the algorithm does in troduce some degree of noise.
is the best found configurationreally better than the others?
to address this issue one couldrun more experiments just on a subset of configurations ofinterest.
for example in our case we are interested in what is 2to further simplify debugging our implementation keeps track of the list of mutations applied on each test input and a reference to the mutated element.
134the best overall configuration how it differs when seeding and code coverage fitness functions are or are not used and howit compares with random search.
however even with just fiveruns we obtained statistically significant results regarding ourresearch questions as reported in table v. following the guidelines in we used the wilcoxonmann whitney u test to check statistical difference quantifiedby the v argha delaney standardised effect size.
for largesearch budgets code fitness has the strongest effect e.g.
fora 250k budget bo is statistically significantly better than bo no code with an effect size of .
.
also for large budgets random yields statistically and practically worse results thansearch e.g.
for a 250k budget bo is statistically significantly better than rand with code with an effect size of .
.
but for low budgets no statistically significant results are visible which can be explained by low statistical power resultingfrom lower effect size less search and a small number ofobservations.
x. r ela ted work related work dealing with the automatic generation of faulty input data for system level testing focuses mostly onmodel based and mutation testing approaches while search based approaches are still in their infancy.
most model based testing techniques target the generation of valid data structures to be used in unit testing that are typically much simpler than the input files needed fortesting data processing systems.
existing model based techniques like threat modelling techniques and specification mutation testing generate test inputs that are less complex than those processed by dataprocessing systems.
threat modelling techniques use modelsthat capture the characteristics of typical invalid inputs thatshould be properly handled by the software under test.
modelslike attack trees uml state machines and transitionnets are used to generate sequences of illegal actions which are not relevant for testing data processing systemswhere the complexity of the testing process lies in the defini tion of the input data structures.
context free grammars insteadcan generate input data structures but do not allowfor the modelling of relationships among data fields.
specification based mutation testing techniques use mutation operators to seed faults into specification models e.g.a state machine to generate specification mutants .
ap proaches that generate mutated statecharts can only generateinputs that are sequences of system operations whileapproaches that mutate class diagrams have only been used totest model transformation systems in which the state diagramitself is the input .
data mutation approaches use mutation operators to generate new test inputs from existing ones .
like ourprevious approach the approaches based on data mutationfocus on a single objective e.g.
covering all the possible datafaults of a fault model but do not allow for covering themultiple objectives needed to perform effective robustness test ing.
the answer to rq1 in section ix shows that the searchbased approach presented in this paper performs better thanapproaches that focus on the coverage of a single objective.
most of the search based testing techniques have primarily focused on unit testing or the testing of non functionalproperties .
work on search based robustness testing per formed at the system level focuses either on the identificationof performance issues which are out of the scope of thispaper or functional faults caused by complex sequences of testinputs or by input signals .
ali et al.
exploit the information encoded into uml state machines and aspect oriented modelling to generate testcases that stress the robustness of a software system by gener ating complex invocations of function calls.
similarly fu andkone use finite state machines to generate robustness testcases for protocol testing.
in contrast with these techniques wefocus on systems for which it is important to generate complexdata structures thus instead of using behavioural models suchas state machines we rely upon exploit class diagrams.
in the context of embedded systems metaheuristic search is used for the generation of input signals satisfying somegiven properties such as requirements on signal shapes ortemporal constraints .
our work is complementary to theseapproaches since it addresses the problem of generating com plex data structures by innovatively combining metaheuristicsearch and data mutation.
xi.
c onclusion building a minimal robustness test suite for data processing systems with high fault revealing power is complicated bymultiple factors the complex structure of the test inputs thatpresent several constraints among their data fields the needfor generating a set of inputs that covers both the functionalspecifications and the data faults captured by a given faultmodel and the possibility to have multiple data faults in asame input.
we designed a novel evolutionary algorithm that addresses these challenges by generating complex test inputs by meansof data mutation relying upon model based and code based fit ness functions and identifying optimal test suites by managingthe tradeoff between the exploration and the exploitation of thesearch landscape thanks to a set of configuration parameters.the fitness functions capture aspects that are relevant forrobustness testing that is how well each input covers thestructure of the input data the fault model the functionalspecifications and the structure of the system.
empirical results obtained by applying our search based testing approach to test an industrial data processing systemshow that it outperforms previous approaches based on faultcoverage and random generation higher code coverage isachieved with smaller test suites.
furthermore we show that although a fitness function that includes code coverage is essential to maximise the coverageof the generated test suite fitness functions based on modelsalone can achieve good coverage results while significantlyreducing test suite size.
this is of practical importance as testgeneration is much quicker and often more practical whenno test execution is required.
finally we identified a bestconfiguration for our search algorithm that returns better resultsregardless of the search budget this configuration facilitatesthe application of our approach and includes smart seeding which turns out to be a key feature in improving search results.
a cknowledgment supported by the fonds national de la recherche luxembourg fnr p10 and fnr and an ses grant.
theauthors would like to thank raul gnaga vincent masquelier and tomislav nakic alfirevic for their valuable feedback andassistance.
135references yahoo!
stock market search engine.
peoplefinders people search service.
flightradar24 flight tracking web service.
google inc. google glasses wordlens camera translation application systems and software engineering vocabulary iso iec ieee e pp.
dec .
the register dow jones average collapses to .
.
theregister.co.uk dow jones average collapses .
d. di nardo f. pastore and l. briand generating complex and faulty test data through model based mutation analysis in 8th international conference on software testing v erification and v alidation icst .
ieee computer society .
s. luke essentials of metaheuristics 2nd ed.
lulu available at s. ali l. briand h. hemmati and r. panesar walawege a systematic review of the application and empirical investigation of search based test case generation software engineering ieee transactions on vol.
no.
pp.
nov .
w. afzal r. torkar and r. feldt a systematic review of search based testing for non functional system properties information and software technology vol.
no.
pp.
.
.
available m. iqbal a. arcuri and l. briand environment modeling and simulation for automated testing of soft real time embedded software software and systems modeling vol.
no.
pp.
.
.
available d. di nardo n. alshahwan l. briand e. fourneret t. naki c alfirevi c and v .
masquelier model based test validation and oracles for dataacquisition systems in automated software engineering ase ieee acm 28th international conference on nov pp.
.
the object management group the object constraint language council of the consultative committee for space data systems ccsds .
b aos space data link protocol .
g. fraser and a. arcuri the seed is strong seeding strategies in search based software testing in software testing v erification and v alidation icst ieee fifth international conference on april2012 pp.
.
g. t. parks and i. miller selective breeding in a multiobjective genetic algorithm in parallel problem solving from natureppsn v. springer berlin heidelberg .
j. knowles and d. corne the pareto archived evolution strategy a new baseline algorithm for pareto in proceedings of the congress on evolutionary computation cec99.
ieee computer society .
mountainminds jacoco library e. zitzler m. laumanns and l. thiele spea2 improving the strength pareto evolutionary algorithm for multiobjective optimization.
in proceedings of eurogen evolutionary methods for design optimisation and control with applications to industrial problems .
s. v arrette p .
bouvry h. cartiaux and f. georgatos management of an academic hpc cluster the ul experience in proc.
of the intl.
conf.
on high performance computing simulation hpcs .bologna italy ieee july pp.
.
a. arcuri and l. briand a hitchhiker s guide to statistical tests for assessing randomized algorithms in software engineering software testing v erification and reliability vol.
no.
pp.
.
c. boyapati s. khurshid and d. marinov korat automated testing based on java predicates in proceedings of the acm sigsoft international symposium on software testing and analysis ser.
issta .
new y ork ny usa acm pp.
.
v .
senni and f. fioravanti generation of test data structures using constraint logic programming in proceedings of the 6th internationalconference on tests and proofs ser.
tap .
berlin heidelberg springer v erlag pp.
.
s. khurshid and d. marinov testera specification based testing of java programs using sa t in automated software engineering vol.
no.
.
hingham ma usa kluwer academic publishers pp.
.
a. morais a. cavalli and e. martins a model based attack injection approach for security validation in proceedings of the 4th international conference on security of information and networks ser.
sin .new y ork ny usa acm pp.
.
m. hussein and m. zulkernine umlintr a uml profile for specifying intrusions in proceedings of the 13th annual ieee international symposium and workshop on engineering ofcomputer based systems ser.
ecbs .
washington dc usa ieee computer society pp.
.
.
available d. xu m. tu m. sanford l. thomas d. woodraska and w. xu automated security test generation with formal threat models ieee transactions of dependable and secure computing vol.
no.
pp.
jul.
.
d. hoffman h. y .
wang m. chang and d. ly gagnon grammar based testing of html injection vulnerabilities in rss feeds inproceedings of the testing academic and industrial conference practice and research techniques ser.
taic part .
washington dc usa ieee computer society pp.
.
s. zelenov and s. zelenova automated generation of positive and negative tests for parsers vol.
pp.
.
y .
jia and m. harman an analysis and survey of the development of mutation testing software engineering ieee transactions on vol.
no.
pp.
sept .
r. schlick w. herzner and e. j obstl fault based generation of test cases from uml models approach and some experiences inproceedings of the 30th international conference on computer safety reliability and security ser.
safecomp .
berlin heidelberg springer v erlag pp.
.
j. m. mottu b. baudry and y .
le traon mutation analysis testing for model transformations in proceedings of the 2nd european conference on model driven architecture f oundations and applications ecmda f a .
berlin heidelberg springer v erlag pp.
.
l. shan and h. zhu generating structurally complex test cases by data mutation comput.
j. vol.
no.
pp.
aug. .
.
available a. bertolino s. daoudagh f. lonetti e. marchetti f. martinelli and p .
mori testing of polpa based usage control systems software quality journal vol.
no.
pp.
.
m. de jonge and e. visser automated evaluation of syntax error recovery in proceedings of the 27th ieee acm international conference on automated software engineering ser.
ase .
newy ork ny usa acm pp.
.
p .
mcminn search based software test data generation a survey software testing v erification and reliability vol.
no.
pp.
.
.
available l. briand y .
labiche and m. shousha using genetic algorithms for early schedulability analysis and stress testing in real timesystems genetic programming and evolvable machines vol.
no.
pp.
.
.
available s. ali l. c. briand and h. hemmati modeling robustness behavior using aspect oriented modeling to support robustness testing of indus trial systems software and systems modeling vol.
no.
pp.
.
y .
fu and o. kone security and robustness by protocol testing systems journal vol.
no.
pp.
.
a. baresel h. pohlheim and s. sadeghipour structural and functional sequence test of dynamic and state based software with evolutionaryalgorithms in genetic and evolutionary computation ?
gecco ser.
lecture notes in computer science.
springer berlinheidelberg vol.
pp.
.
.
available b. wilmes and a. windisch considering signal constraints in searchbased testing of continuous systems in software testing v erification and v alidation workshops icstw third international conference on april pp.
.