learning a dual language vector space for domain specific cross lingual question retrieval guibin chen1 chunyang chen1 zhenchang xing1 and bowen xu2 1school of computer science and engineering nanyang technological university singapore 2college of computer science and technology zhejiang university china gbchen chen0966 zcxing ntu.edu.sg max xbw zju.edu.cn abstract the lingual barrier limits the ability of millions of nonenglish speaking developers to make e ective use of the tremendous knowledge in stack over ow which is archived in english.
for cross lingual question retrieval one may use translation based methods that rst translate the nonenglish queries into english and then perform monolingual question retrieval in english.
however translation based methods su er from semantic deviation due to inappropriate translation especially for domain speci c terms and lexical gap between queries and questions that share few words in common.
to overcome the above issues we propose a novel cross lingual question retrieval based on word embeddings and convolutional neural network cnn which are the state of the art deep learning techniques to capture wordand sentence level semantics.
the cnn model is trained with large amounts of examples from stack over ow duplicate questions and their corresponding translation by machine which guides the cnn to learn to capture informative word and sentence features to recognize and quantify semantic similarity in the presence of semantic deviations and lexical gaps.
a uniqueness of our approach is that the trained cnn can map documents in two languages e.g.
chinese queries and english questions in a dual language vector space and thus reduce the cross lingual question retrieval problem to a simple k nearest neighbors search problem in the dual language vector space where no query or question translation is required.
our evaluation shows that our approach signi cantly outperforms the translation based method and can be extended to dual language documents retrieval from di erent sources.
ccs concepts software and its engineering !software libraries and repositories information systems !multilingual and cross lingual retrieval keywords cross lingual question retrieval word embeddings convolutional neural network dual language vector space .
introduction question answering q a sites have become an important service for knowledge sharing and acquisition.
in the software engineering domain stack over ow is the most prominent q a site.
over the past few years it has accumulated a large amount of user generated content which makes it a valuable repository of software engineering knowledge.
the content in the q a sites is organized as questions and corresponding answers.
one key task for reusing content in such a site is nding questions that are similar to user queries as questions are the keys to accessing the knowledge in the site.
many techniques support monolingual question retrieval .
that is to retrieve english questions1in stack over ow user queries must also be in english.
however many developers are from non english speaking countries such as china.
according to stack over ow user and visit statistics the u.s has times more registered users than the china on stack over ow 2but the monthly visits from the u.s is only about times more than that from the china3.
this indicates that although not many chinese developers participate in q as much more of them do visit stack over ow to reuse its content.
the english skills of non english speaking developers could be good enough to read posts in english but are usually not su cient to express their questions in english queries.
this limits the ability of non english speaking developers to make e ective use of stack over ow content.
we thus need cross lingual question retrieval clqr technique that overcomes the language barrier and allows nonenglish speaking developers to issue queries in their native language e.g.
chinese to retrieve questions in english.
one possible way for cross lingual question retrieval to work is to rst translate the queries from the native language into english and then to use monolingual question retrieval to retrieve questions in english.
this approach assumes that query translation can preserve query semantics and question retrieval techniques can handle lexical gaps between 1several non english stack over ow sites have been launched.
however they are much less popular than english stack over ow.
ow.com research developer survey overview permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
queries and questions.
we argue that these assumptions are too simplistic.
semantic deviation is likely to be introduced in query translation especially for domain speci c technical terms.
for example has several common translations like examine censor or investigate by google translate.
however when we consider it in a speci c sentence within software engineering context it would be better translated into some domain speci c terms in english.
for example for the two chinese queries and the appropriate domain speci c translations would be code review tool and web element inspection .
that is should be translated as review and inspections in these two queries while google translate cannot make the appropriate domain speci c translation.
although the english words examine review and inspection are synonyms an inappropriate translation will introduce deviation from the original meanings of the queries and a ect the subsequent retrieval step.
in addition to term level semantic deviation across languages sentence level lexical gaps could further a ect the performance of the retrieval techniques.
lexical gap means that relevant queries and questions may not share many words in common.
as there are many ways to ask the same question a user might not be able to nd the answer if they are asking it a di erent way.
assume developers want to know how to read a text le.
some may describe their needs using the query read text le while others may use queries like ascii ascii le to string or access string content in le .
developers may also express errors of their program as queries like cannot load all strings .
stack over ow already has good answers to the question like read a plain text le .
although all the queries and the question share very similar meanings except the query read text le the other queries and the question share few words in common.
such sentence level lexical gap has become a major barricade for traditional information retrieval ir models e.g.
bm25 lda to determine query question similarity .
for cross lingual retrieval the issue would be more prevalent due to di erent norms and expressions across languages and semantic deviation in query translation.
in this paper we tackle the cross lingual question retrieval problem using deep learning techniques.
to overcome the above issues on semantic deviation and lexical gap we adopt word embeddings and convolutional neural networks cnn to recognize and quantify word and sentence level semantic similarity across lingual barrier and lexical gap.
we focus on chinese to english question retrieval because chinese is distant from english making it a more challenging task.
the key innovation of our approach is that we train the cnn using su cient examples of sentence pairs that are semantically equivalent but have semantic deviations due to inaccurate translation and lexical gaps.
these training examples are collected from the large amount of duplicate questions .
million in stack over ow and the corresponding chinese translations of these questions translated by machine google translate .
we design e ective loss functions to guide the cnn to learn to capture the most informative word and sentence features to determine semantic similarity in the presence of semantic deviations and lexical gaps.
after training our cnn can map both english and chipw i w i w i w i w i w w w w .... w i w i w i w i w i .... w n w n word sequencecontext window of w i inputoutput predictionfigure continuous skip gram model nese documents onto a dual language vector space in which terms and documents from both languages are represented as language independent vectors.
semantically close terms and documents from both languages are likewise close in the dual language vector space.
in this work the english documents are a knowledge base of english questions and the chinese documents are chinese queries.
in contrast to query translation followed by monolingual question retrieval our approach directly quanti es semantic similarity between chinese queries and english questions in terms of their distance in the dual language vector space in which no query or question translation is required.
given a query cross lingual question retrieval is then transfered to a simple k nearest neighbors search in the dual language vector space.
we compare our deep learning based approach with the baseline googletranslate lucene method.
the results show that our approach obtains slightly better results than the baseline method in terms of the rank of the rst relevant question retrieved.
however our approach signi cantly outperforms the baseline method in retrieving more relevant questions especially the questions that exhibit a lexical gap with the input queries which the baseline method usually fails to retrieve.
furthermore our approach is more robust to the semantic deviations across languages.
we also apply our cnn to a completely di erent data source english version and chinese version of a python tutorial website which demonstrate the generality of our approach.
the remainder of the paper is organized as follows.
section introduces background related to word embeddings and cnn.
section presents the technical details of our approach.
section reports the evaluation of our approach.
section reviews the related work.
section summarizes our work and outlines the future plans.
.
background this section introduces the basic concepts of the two key techniques i.e.
word embeddings and convolutional neural network that our approach relies on.
.
word embeddings word embeddings are dense low dimensional vector representations of words that are build on the assumption that words with similar meanings tend to be present in similar context.
studies show that word embeddings are able to capture rich semantic and syntactic properties of words compared with one hot word representation .
word embeddings are typically induced using neural language model which uses neural networks as the underlying predictive model.
figure shows the continuous skipgram model one of the two popular word to vector 745x1x2x3 xn x4 xn xn 2activation fucntion relu1 max pooling convolution operationcmax e.g.
filters of window size h n number of filtersconcatenated together to form a single feature vector h hk kfeature mapsfigure the architecture of a simple cnn word2vec neural language models proposed by mikolov et al.
the goal of the continuous skip gram model is to learn the word embeddings of a center word i.e.
wi that is good at predicting the surrounding words in a context window of 2t words t in this example .
more speci cally the objective function of the skip gram model is to maximize the sum of log probabilities of the surrounding context words conditioned on the center word nx i 1x t j t j6 0logp wi jjwi where widenotes the center word in a context window of length t and wi jdenotes the context word surroundingwiwithin the context window.
ndenotes the length of the word sequence.
the log p wi jjwi is the conditional probability de ned using the softmax function p wi jjwi exp v0t wi jvwi p w2wexp v0twvwi where vwandv0 ware respectively the input and output vectors of a word win the underlying a neural network and w is the vocabulary of all words.
intuitively p wi jjwi estimates the normalized probability of a word wi jappearing in the context of a center word wiover all words in the vocabulary.
this probability can be e ciently estimated by the negative sampling method .
the continuous skip gram model does not care about the input language as long as the sentences can be properly tokenized into a sequence of words.
given word sequences the model maps words onto a low dimensional real valued vector space.
word vectors are essentially feature extractors that encode semantic and syntactic features of words in their dimensions.
in this vector space semantically close words are likewise close in euclidean distance.
.
convolutional neural network a convolutional neural network cnn utilizes layers with convolution lters that are applied to local features of an object e.g.
an image or a sentence to produce a feature vector of the object .
some recent works have successfully applied cnns to model sentence and document level semantics for nlp tasks such as sentence classi cation and duplicate question detection .
figure shows the model architecture of a simple cnn for nlp tasks.
to apply the cnn to text words comprising a sentence need to be converted into vector representations to be used as input to the cnn.
a commonly used word vector representation is word embeddings as mentioned in the previous section.
letxi2rkbe the k dimensional word vector corresponding to the i th word in the sentence.
a sentence of length n is represented as x1 n x1 x2 xn where is the vector concatenation operator.
we can treat the sentence vector as an image and perform convolution on it via linear lters.
in text applications because each word is represented as a k dimensional vector it is reasonable to use lters with widths equal to the dimensionality of the word vectors i.e.
k .
thus we simply vary the window size orheight of the lter i.e.
the number of adjacent words considered jointly.
let xi i h 1refer to the concatenation vector of hadjacent words xi xi x i h .
a convolution operation involves a lter w2rhk a vector of h kdimensions and a bias term b2rh which is applied tohwords to produce a new value oi2r oi wt xi i h b where i n h and is the dot product between the lter vector and the word vector.
this lter is applied repeatedly to each possible window of hwords in the sentence i.e.
x1 h x2 h x n h n to produce an output sequence o2rn h i.e.
o .
we apply a non linear activation function fto each oito produce a feature map c2rn h 1where ci f oi .
a commonly used non linear activation function is relu relu oi max oi one may also specify multiple kinds of lters with di erent window sizes or use multiple lters for the same window size to learn complementary features from the same word windows.
in figure we illustrate n lters for the window size ofh .
the dimensionality of the feature map generated by each lter will vary as a function of the sentence length and the lter s window size.
thus a pooling function is then applied to each feature map to induce a xed length vector.
a common strategy is max pooling which extracts a scalar i.e.
a feature vector of length with the maximum value for each lter.
together the outputs from each lter can be concatenated into a feature vector for one layer of the cnn.
this feature vector can be fed into the next layer of the cnn for further convolution or be used as the output vector for di erent nlp tasks e.g sentence classi cation duplication question detection .
.
dual channel cnn for clqr we now describe our dual channel cnn that is designed for cross lingual question retrieval clqr .
.
approach overview this work focuses on chinese to english question retrieval because chinese is distant from english.
figure presents the main steps of our approach.
first we collect questions from stack over ow and two chinese q a sites segmentfault and v2ex as our dataset.
to mitigate the lack of 746english questions chinese questions augment data by google translateskip gram model skip gram modeldata collections preprocessing e.g tokenization ... dim chinese words... dim english wordsoutput preprocessing e.g tokenization processing a learning monolingual word embeddings dual language vector space chinese channelenglish channel java how to read file in java?
java how to read file in java ?
... dim chinese words ... dim english wordscosine loss sampling svm loss training dataloss function training cnn channel chinese sentence vector look up for corresponding word vectors preprocessing tokenization lowercasing etc.
english sentence vector b training the cnn k nearest neighborstrained cnn trained cnnjava ?
dual language vector space k return top k questions ...... k......reading a plain text file in java how to read data from a text file in java?english questions english channel java chinese channel... dim chinese wordslook up for corresponding word vectorspreprocessing ... dim english words preprocessing look up for word vectors c cross lingual question retrieval figure overview of main steps chinese questions for learning chinese word embeddings we augment the chinese question dataset with the chinese translations of stack over ow questions using google translate.
we learn monolingual word embeddings from a large number of english and chinese questions respectively.
these pre trained english and chinese word embeddings are used in two ways rst as word vector representations of english and chinese sentences for training the cnn and second as word vector representations of chinese queries andenglish questions for cross lingual retrieval.
word embeddings capture word level semantics for each language.
next we use cnns to quantify sentence level semantic similarity across language.
the application of cnns includes a training phase and a query phase.
in the training phase we generate a set of dual language sentence pairs i.e.
one sentence in english and the other in chinese from stack over ow duplicate questions and their corresponding chinese translations also obtained by google translate .
a dual channel cnn is designed one channel for each language respectively and trained by the dual language sentence pairs.
in the query phase the respective cnn channel can map chinese queries and a knowledge base of english questions onto the dual language vector space in which terms and sentences from both languages are represented as language independent vectors.
in this vector space semantically close terms and sentences from both languages are likewise close see examples in figure 7a .
given a query in chinese cross language question retrieval is reduced to ndk nearest english questions close to the query in the dual language vector space.
it is important to note that we use google translate to obtain chinese translations of stack over ow questions for learning chinese word embeddings and training the dualchannel cnn.
however cross language question retrieval is supported by the dual channel cnn in which no query or question translation is required.
.
learning monolingual word embeddings words are discrete symbols and cannot be fed directly to a neural network.
we need to map each word to a realvalued vector.
in this work we use word embeddings as input word vector representations because of the ability of word embeddings to capture latent semantic and syntactic features of words in their dimensions .
to learn word embeddings of a language we need large amounts of texts in that language.
for software engineering question retrieval texts should be domain speci c and cover the vocabulary of the questions people may ask.
therefore we collect question titles from stack over ow as a corpus of english software engineering texts for learning english word embeddings.
for learning chinese word embeddings we collect question titles from two chinese q a sites segmentfault and v2ex as a corpus of chinese software engineering texts.
because chinese q a sites have many fewer questions than stack over ow we augment chinese software engineering texts with the chinese translations of stack over ow question titles.
due to the sheer amount of texts needed human translation is impractical.
therefore we use machine translation google translate to obtain chinese translations of stack over ow question titles.
machine translation also intentionally injects some semantic deviations in between english and chinese texts so that the trained cnn can be more robust to semantic deviations between chinese queries and english questions.
each collected question title is considered as a sentence.
we preprocess english sentences by standard english text preprocessing steps like tokenization lowercasing etc.
for chinese sentences we tokenize them into chinese words using snownlp tool4 and lowercase any english words e.g.
tool names in the chinese sentences.
we use the continuous skip gram model the python implementation in 747gensim to learn monolingual word embeddings from the english and chinese software engineering text respectively.
the output is a dictionary of english words and their embeddings and a dictionary of chinese words and their embeddings which will be used to represent english questions and chinese queries respectively.
.
training the dual channel cnn this section describes the architecture of our cnn and how we generate training documents and train the cnn with these documents.
.
.
the architecture of the proposed cnn for cross lingual retrieval we design a dual channel cnn each channel can map sentences in one language into a duallanguage vector space in which sentence level semantic similarity can be quanti ed.
both channels have the same architecture as shown in figure .
the di erence is only that one takes english sentences as input while the other takes chinese sentences as input.
an input sentence is rst converted into a sentence vector by looking up the word embeddings dictionary and concatenating the word embeddings of the words comprising the sentence.
the sentence vector is then fed into the cnn as input.
our cnn consists of two layers each of which follows the architecture depicted in figure .
the rst layer of the cnn uses lters with three di erent window sizes i.e.
h .
that is we try to extract features of gram grams and grams in the input sentence.
for each window size we use n e.g.
lters to learn complementary features from the same word windows.
after convolution relu activation and max pooling the rst layer outputs an dimensional feature vector for each window size.
the three feature vectors output by the rst layer are intended to capture the most informative gram grams and grams in the input sentence and are used as the input vectors to the second layer.
the goal of the second layer is to extract the interactive information of these n grams for example syntactic or semantic dependence of di erent parts of a sentence.
the second layer uses lters with window size h .
for each feature vector output by the rst layer the second layer uses m e.g.
lters.
after convolution relu activation and max pooling the second layer outputs three m dimensional feature vectors.
finally the output layer also known as fully connected layer concatenates the feature vectors output by the second layer and performs a linear transformation to map the m dimensional feature vector into a h dimensional vector in the dual language vector space.
note that sentences from both languages are represented as languageindependent vectors in the dual language vector space in which semantically close sentences are likewise close.
.
.
generating dual language sentence pairs to train the proposed dual channel cnn we need a set of dual language sentence pairs each of which is a pair of semantically equivalent or di erent sentences one in english and the other in chinese.
in statistical machine translation research such dual language documents are usually obtained from human translated documents for example united nation documents for di erent languages .
in the software engineering domain such human translated dual language documents rarely exist except some textbooks link to duplicated questionsfigure an example of duplicate question or user manual of software tools.
however for the task of question retrieval textbook and user manual contents are not su cient to cover the ways people ask questions and the vocabulary of questions people ask.
furthermore the key design goal of our approach is to be able to quantify semantic similarity across query question lingual barrier and lexical gap.
to that end the training sentence pairs must include su cient examples of semantically equivalent chineseenglish sentence pairs with semantic deviation and lexical gap.
textbook and user manual contents cannot satisfy this need.
we exploit large amounts of stack over ow duplicate questions and machine translation to generate dual language sentence pairs for training our cnn.
on stack over ow large amounts of duplicate questions are kept except exact or nearly exact duplicates because stack over ow acknowledges that users could formulate the same question in di erent ways5 see figure for an example .
these duplicate questions are semantically equivalent as they can be answered by the same answer but often exhibit lexical gap.
we collect question titles of stack over ow duplicate questions as english sentences and translate them into chinese counterparts.
as training of a cnn requires large amounts of dual language sentence pairs human translation is impractical.
therefore we use google translate to generate the chinese translations of stack over ow question titles.
although machine translation will likely introduce semantic deviation from the original english questions there are usually correct translations in a group of duplicate questions.
furthermore non english speaking developers are likely to use similar machine translation services and thus introduce similar translation deviations.
third certain level of translation deviations in the training sentence pairs will make the cnn more robust to di erent ways to express semantically similar meanings across languages.
letegibe the ith group of english duplicate questions andcgibe the corresponding group of chinese duplicate questions translated by google translate.
denote esijas the jth english question in the ith english duplicate group and csijas the jth chinese question in the ith chinese duplicate group.
an english question and its corresponding chinese translation are indexed by the same ijin the corresponding english and chinese groups.
as illustrated in figure for a pair of egiandcgi we generate two sets of semantically equivalent sentence pairs whose semantic similarity is set to i.e.
sim .
the rst set includes the english question titleesij2egiand its corresponding chinese translation csij2cgifor all questions in the egiandcgi such ases112eg1andcs112cg1 es122eg1andcs122 ow.com help duplicates 748hfirst layer second layerdual language vector spaceinput output cnn channelchinese sentence vectorenglish sentence vectorchinese channelenglish channel convnet h k wvdim filters n convnet h k wvdim filters n convnet h k wvdim filters n gram grams grams convnet h k filters m convnet h k filters m convnet h k filters m n grams features linear transform semantic vectorglobal features sentence vectorwvdimfigure the structure of the proposed cnn channel duplicate groups cnduplicate groups ensemantically equivalent pairs sim direct correspondence within group correspondencesemantically different pairs sim group cs11 cs12es11 es12 cs13 es13 cs1i es1j... ... ... ...cross group group k...... ...... ...... ...... ...... ...... ............ ...... ...... ............ ...... ...... ......english questionschinese questionscg1 eg1 csk1 csk2esk1 esk2 csk3 esk3 cski eskj... ... ... ...cgk egk cs11 cg1 es11 eg1 cs12 cg1 es12 eg1 .... .... .... .... cskj cgk eskj egk cs11 cg1 es13 eg1 cs11 cg1 es1j eg1 .... .... .... .... cski cgk eskj egk note i j cs11 cg1 es26 eg2 cs13 cg1 esk2 egk .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... cski cgk esgj egg note k g figure generating dual language sentence pairs cg1 etc.
the number of entries in this set is the same as the number of duplicate questions in the group.
the second set include the english question title esij2egiand the chinese translation of a di erent english question title csik2cgi j6 k such as es132eg1andcs112cg1 es122eg1 cs112cg1 etc.
the entries in the second set are randomly selected within egiandcgi and the number of the entries in the second set is the same as that of the rst set.
both semantically equivalent sets would exhibit semantic deviations caused by machine translation and the second set would exhibit lexical gap between semantically equivalent questions.
we randomly select a set of semantically di erent sentence pairs each of which consists of an english question title from one group of english duplication questions egkand a chinese translation from a non corresponding group cgg k6 g such as es112eg1andcs262cg2.
the semantic similarity of these semantically di erent pairs is set to .
the number of semantically di erent sentence pairs across groups is the sum of the two sets of semantically equivalent pairs.
these semantically di erent sentence pairs contrast with the semantically equivalent pairs for training the cnn.
.
.
loss functions to train the cnn we feed the dual language question pairs to the cnn giving the english question to english cnn channel and the chinese question to chinese cnn channel.
let the kth dual language question pair be a tuple heskegkeid2egkeg cskcgkcid2cgkcg sim ki where keg andkcgare the group index of english duplicate questions and chinese duplicate questions respectively and keidand kcidare the index of the english question and the chinese question in the respective english and chinese questionsgroup.
if kegandkcgare the same i.e.
eskegkeidand cskcgkcidare from the corresponding english chinese groups simkis otherwise simk .
the respective cnn channel maps the english or chinese question as a vector in the dual language vector space.
we denote the two vectors as evkandcvk.
we use cosine similarity i.e.
cos evk cvk to measure the distance between the two vectors.
correspondingly mean square error is used as the loss function to quantify the agreement between the computed similarity and the expected similarity lcos ndsx k nds simk cos evk cvk where ndsis the number of dual language question pairs for training.
mean square error of cosine similarity i.e.
cosine loss will guide the cnn to map semantically equivalent questions around the same angle in the vector space thus cosine similarity would be close to but map semantically di erent questions at degrees thus cosine similarity would be close to .
meanwhile a svm is introduced for the two output vectorsevkandcvkrespectively.
in our case there are hundreds of thousands of duplicate questions groups.
such a large class classi cation problem will make the svm training time consuming.
since the svm here is used as an auxiliary driving force to guide the cnn to capture appropriate sentence features for separating semantically di erent groups performing a sampled classi cation is su cient similar to the negative sampling method used in training word embeddings .
the input of the sampling svm is the output vector from the cnn evk orcvk .
a linear layer is used to calculate the scores of the vector evk orcvk in di erent duplicate questions group j sj ut jevk bj where ujandbjare the weight and bias parameter of the svm.
then the sampling multiclass svm loss is de ned as lsv m evk x j2corrupt g max sj skeg where the corrupt g is a set of indexes of a small number e.g.
of sampled corrupted duplicate questions groups.
a corrupted group means the group other than the groundtruth group for the vector evk orcvk i.e.
keg orkcg .
thelsvm cvk can be de ned in the same way.
the overall loss function for the sampling svm is lsv m ndsx k nds lsv m evk lsv m cvk java python c c java functionpython errorandroid c object variable c argumentsource code encoding keywordstacknesteddictionarypath output differenttime valueserverdatabase application differenceurl text vectorcontrol constructorcharacter compareparse lengthconvert printsystemdetect passingmessageextract virtual constdeclarationbehaviorbytecalculateexpectedresourceredirectlogininstall build heightmac performancesending symbol compile structsorting entity enumwidth attributesdateswebsite a 2d visualization of semantically similar words java big o of this sorting algorithm quick sort worst case regex matching punctuation and roman in lowercaseregex trim multiple characters?accessing a file in java how do i require a file in java?null pointer exception null errorwhen to use interface and when to use abstract class b 2d visualization of semantically similar sentences figure words or sentences in the dual language space as we only sample a small number of corrupted groups for computing svm loss the computation time is dramatically reduced.
although the sampling svm provides only an approximation to the full svm our experiments show that when it cooperates with cosine loss function the sampling svm guides the cnn to cluster semantically equivalent sentences within small euclidean distance but separates semantically di erent sentences by large euclidean distance.
.
.
training process intuitively the training process makes the cnn learn to capture the most informative features of english and chinese sentences for quantifying semantic similarity across lingual barrier and lexical gap.
formally the training objective is to minimize the loss function overall the entire set dof dual language sentence pairs arg min w blcos lsv m r w where r w isl2 norm constraint which is used to avoid over tting problem.
the parameters w b to be estimated include the lters and bias terms in the rst and second layer of the cnn the linear transformation matrix of the output layer and the parameters of the svm classi ers.
the adam update algorithm is used instead of stochastic gradient decent sgd for a faster convergence.
.
cross lingual question retrieval after training the cnn we learn the parameters of each layer of the cnn.
it can then be used to map questions and queries in either language onto a dual language vector space.
figure 7b shows the visualization of several seman tically similar chinese queries and english questions that uses the t sne dimensional reduction technique .
figure 7a gives some most frequently used english words in stack over ow posts and their corresponding chinese translations in the two dimensional t sne visualization .
we can see that semantically close terms and sentences from both languages are close in the dual language vector space.
this shows the potential usefulness of our cnn for crosslingual information retrieval.
for chinese to english question retrieval we can collect a knowledge base of english questions di erent from those used to train the cnn .
these english questions will be converted into word vector representations using english word embeddings and then question vectors will be mapped onto the dual language vector space using the english cnn channel.
users can query these english questions using queries in chinese.
chinese queries do not need to be translated.
instead they are converted to word vector representations using chinese word embeddings and then query vectors will be mapped onto the same dual language vector space using the chinese cnn channel.
most relevant english questions can be retrieved by nding the k nearest questions close to the query in the vector space.
although this work focuses on chinese to english question retrieval our cnn can also support english to english chinese to chinese english to chinese retrieval.
that is queries in either language can retrieve questions in either language.
depending on the language of queries and questions the respective cnn channel can be used to map queries and questions onto the same vector space then question retrieval can be done by nding k nearest neighbors.
.
evaluation we perform a set of experiments to answer the following questions how do the hyperparameters of the cnn a ect the performance of the cnn?
can our approach outperform the query translationthen monolingual question retrieval method and in what cases?
can the cnn trained with stack over ow data be used to quantify semantic similarity of english chinese sentences from a di erent data source?
.
dataset the main data source of our study is stack over ow data dump of january .
we collect .
million duplicate questions from the data dump which form .
million duplicate questions groups.
we also crawl .
million chinese questions from two chinese q a sites segmentfault and v2ex .
we augment the chinese questions with the chinese translations of .
million randomly selected stack over ow questions.
in this work we use only question titles each of which is treated as a sentence.
the corpus of english sentences and chinese sentences is used to learn monolingual word embeddings to represent english and chinese words respectively.
we randomly divide the duplicate questions groups into three subsets i.e.
training validation and test .
the subset is used to generate .
.
.
.
.
.
.
wordembed dim wordembed dim wordembed dim wordembed dim wordembed dim 400lossi teration a loss convergence .
.
.
.
.
.
validation accuracye poch wordembed dim wordembed dim wordembed dim wordembed dim wordembed dim b validation accuracy figure impact of word embedding dimensionality dual language sentence pairs for training the cnn.
the rst subset is used to tune the cnn hyperparameters during training.
the second subset is used to compare our approach with the baseline method.
.
cnn hyperparameter optimization our cnn has two hyperparameters the dimensionalities of the word embeddings and the output feature vectors of each layer.
we conduct experiments to investigate the inuence of di erent parameter choices.
.
.
the dimensionality of word embeddings in this experiment we set the dimensionality of word embeddings at and .
we use the setting for the dimensionality of output feature vectors of the three layers in our cnn see section .
.
.
figure 8a plots the convergence of the loss function by the number of cnn training iterations for di erent dimensionalities.
the loss function converges at di erent speed for di erent dimensionalities.
the larger the dimensionality is the fewer iterations the loss function takes to converge.
however this does not mean that loss function takes less time to converge for larger dimensionality because for a larger dimensionality it will need more computation time per iteration.
figure 8b plots the validation accuracy by the number of epochs for di erent dimensionalities.
one epoch consists of one full training cycle on the training set.
given a pair of english chinese sentences in the validation data if the cosine similarity of the two sentences in the dual language vector space is above .
we consider the two sentences as semantically equivalent otherwise as semantically di erent.
validation accuracy is computed by the number of accurate predictions of semantically equivalent or di erent sentence pairs in the validation set.
as the validation dataset has an equal number of semantically equivalent or di erent sentence pairs the accuracy of a random prediction would be around .
.
figure 8b shows that the validation accuracy increases and converges at very similar rate for di erent dimensionalities and word embedding dimensionality does not have big impact on the accuracy of semanticequivalence di erence prediction.
.
.
the dimensionality of feature vectors the dimensionality of output feature vectors of cnn layers a ects the complexity of the cnn.
in our cnn the dimensionality of feature vectors in the rst layer is proportional to the number of lters n used for each window size.
the dimensionality of feature vectors in the second layer is proportional to the number of lters m used for each input vector.
the dimensionality of feature vector in the output layer is h. the search space of these cnn parameters is extremely large.
we experiment with three settings n m h .
.
.
.
.
.
.
lossi teration a loss convergence .
.
.
.
.
.
.
validation accuracye poch b validation accuracy figure impact of the cnn complexity with increasing complexity.
in this experiment we use dimensional word embeddings.
figure 9a and figure 9b plot the loss function convergence and validation accuracy for di erent cnn complexities.
we can see that the more complex the cnn is the fewer iterations the loss function takes to converge and the slightly higher the validation accuracy is.
.
comparison of clqr performance in this section the comparison experiments with the baseline methods are shown and discussed.
.
.
experiment setup ground truth for the duplicate question groups used as test data we compile all english questions as the knowledge base.
let egibe a group of english duplicate questions in the test data and cgibe the corresponding group of chinese duplicate questions see section .
.
.
we use each chinese question csij2cgias a query to retrieve english questions esik2egi k6 j .
that is all english questions in egi except the one corresponding to the query chinese question csij are used as ground truth to evaluate the performance of question retrieval.
baseline methods for the baseline method we use google translate to translate the chinese query into an english query and then use lucene to retrieve english questions for the translated english query.
in addition to googletranslate lucene we also directly use the english question corresponding to the query chinese question as the english query for lucene search.
this second method can be considered as the perfect translation of the chinese query for monolingual question retrieval.
our approach based on the cnn hyperparameters experiments we use dimensional word embeddings and the output feature vectors setting in this comparative study.
to gain insight into the impact of our loss functions on the cnn performance we train two cnns one with only cosine loss referred to as cnn only cos and the other with both cosine loss and svm loss referred to as cnn cos svm .
metrics given a chinese query let rebe the list of retrieved questions for the query and let gtbe the set of ground truth questions for the query.
we evaluate the question retrieval performance of our approach and the baseline methods using three metrics precision k pr k mean average precision map and mean reciprocal rank mrr .
precision k refers to the fraction of retrieved questions that are relevant in the top k retrieval results i.e.
jretgtj k. let the rank position of all relevant questions in the re for a query be r1 r2 rz.
we can compute pr k for k r1 r2 r z. the average precision for this query is de ned as the mean of the computed pr k. mean average precision map is the mean of the average precisions over 751all the queries.
let kbe the rank position of the rst relevant question in the refor a query then the reciprocal rank rr is de ned as1 k. mean reciprocal rank mrr is the mean of the rrs over all queries.
.
.
quantitative results table shows the performance results.
we can see that the performance of googletranslate lucene is worse than that of lucene only i.e.
simulating perfect translation .
this suggests that semantic deviations in query translation can degrade the performance of subsequent monolingual retrieval step.
in contrast both our cnn settings outperform googletranslate lucene baseline and our cnns achieve almost the same or better performance than lucene only baseline.
this suggests that directly quantifying semantic similarity between chinese queries and english questions in the dual language vector space is more robust than the separate query translation followed by monolingual retrieval.
cnn cos svm setting signi cantly outperform cnn only cos setting on all performance metrics.
this suggests that considering group information of semantically equivalent or di erent sentence pairs can help cnn capture richer semantic features of corresponding english and chinese sentences which in turn can improve the performance of question retrieval.
the pr and mrr of our cnn only cos setting the poorer performer in the two cnn settings are almost the same as those of lucene only baseline.
this suggests that our cnn only cos can achieve comparable performance as the traditional ir method in terms of the rank of the rst relevant question.
however the pr pr and map of our cnns both settings are signi cantly better than the corresponding metrics of the baseline methods.
our analysis of the retrieval results suggests that our cnns can retrieve more relevant questions and rank them higher especially those with lexical gaps to the query which traditional ir methods often fail to retrieve as discussed below.
.
.
qualitative analysis of retrieval results to gain insight into the capability of our cnn and compare results from two methods more objectively we randomly select queries for which the lucene only baseline has a better map than our cnn cos svm and another queries for which our cnn cos svm has a better map.
we manually compare the question retrieval results by our cnn cos svm with the lucene only baseline for each selected query.
table presents one query in which luceneonly baseline returns the better results and two queries in which cnn cos svm is better.
note that all queries are in chinese and the english translation is for non chinese readers as