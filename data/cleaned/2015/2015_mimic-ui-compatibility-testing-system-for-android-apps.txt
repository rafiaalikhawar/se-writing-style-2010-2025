mimic ui compatibility testing system for android apps taeyeon ki chang min park karthik dantu steven y .
ko lukasz ziarek department of computer science and engineering university at buffalo the state university of new york email tki cpark22 kdantu stevko lziarek buffalo.edu abstract this paper proposes mimic an automated ui compatibility testing system for android apps.
mimic is designed specifically for comparing the ui behavior of an app across different devices different android versions and different app versions.
this design choice stems from a common problem that android developers and researchers face how to test whether or not an app behaves consistently across different environments or internal changes.
mimic allows android app developers to easily perform backward and forward compatibility testing for their apps.
it also enables a clear comparison between a stable version of app and a newer version of app.
in so mimic allows multiple testing strategies to be used such as randomized or sequential testing.
finally mimic programming model allows such tests to be scripted with much less developer effort than other comparable systems.
additionally mimic allows parallel testing with multiple testing devices and thereby speeds up testing time.
to demonstrate these capabilities we perform extensive tests for each of the scenarios described above.
our results show that mimic is effective in detecting forward and backward compatibility issues and verify runtime behavior of apps.
our evaluation also shows that mimic significantly reduces the development burden for developers.
keywords mobile apps ui compatibility testing parallel testing programming model i. i ntroduction this paper proposes mimic an automated ui compatibility testing system for android.
it supports what we call followthe leader model of testing multiple testing devices are used in parallel but one device becomes a leader that performs a sequence of ui actions.
all other devices follow the leader and perform the same sequence of ui actions.
using this testing model mimic reports ui compatibility problems occurred during a testing run such as different ui paths taken different ui structures displayed different exceptions thrown differences in ui performance etc.
in essence the main focus of mimic isui compatibility testing .
this design choice of mimic stems from several testing needs and the lack of a practical testing system that meets those needs.
in particular there are four common testing scenarios that call for ui compatibility testing as we detail in section ii i version compatibility testing where developers test their apps on different android api versions ii device compatibility testing where developers test their apps on different android devices iii third party library testing where developers test new versions of third party libraries with their existing apps and iv instrumentation testing where mobile systems researchers test the correctnessof their bytecode instrumentation techniques by comparing instrumented apps to original apps.
all of these scenarios require ui compatibility testing i.e.
testers want to test and compare how the uis of apps display and behave across different environments.
we further detail each of these scenarios and the need for ui compatibility testing in section ii.
mobile testing has several unique challenges including ui version compatibility and consistency across devices app and os versions.
mimic addresses the above challenges by providing the following two main features an i easy to use programming model specifically designed for ui compatibility testing and ii a runtime that manages multiple devices and app or android versions.
as mentioned earlier it implements follow the leader testing model.
the runtime also captures visual differences of an app s ui across different versions or devices using image processing techniques.
to the best of our knowledge there is no previous work that focuses on ui compatibility testing for mobile apps.
as we discuss in section vii existing systems such as dynodroid a3e and others focus on uncovering bugs within an app or examining the security or performance aspects of an app rather than comparing how an app behaves across different versions or environments.
our evaluation shows that mimic is effective in finding ui compatibility problems in real android apps and ficfinder data set.
we have used mimic to test popular apps downloaded from google play on four different android platform versions we have also downloaded multiple versions of the same apps and tested them using mimic.
in various scenarios we have tested mimic has discovered that apps have backward and forward compatibility problems across different android versions including well known apps such as watchespn and yelp .
mimic has also discovered that apps throw different exceptions across different app versions.
with ficfinder data set mimic has detected compatibility problems including errors and distorted ui issues and performance problems such as four lagging ui problems two memory bloat problems and one battery drain problem.
section v discusses these and other findings in more detail.
ii.
m otiv ation prior research shows how ineffective humans are at manually exploring app uis.
humans can cover only .
ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
of the app screens and .
of the app methods according to the research.
this further argues for automated testing of app ui.
there is little prior work on ui compatibility testing despite the need.
for example the ficfinder work conducted an empirical study demonstrating compatibility bugs and published a data set with open source apps that could be used to reproduce such bugs.
another paper tests seven popular existing tools and it states that none of the tools can handle the ui compatibility issues correctly.
the paper mentions the need for a specific tool for ui compatibility testing.
unfortunately there is no existing system that supports compatibility testing of uis as discussed in section vii.
this section presents a set of scenarios under which specific ui testing functionality for mobile apps is required.
a. ui compatibility testing scenarios app developers and researchers frequently face the need for comparing app ui behavior across different app versions android versions and or devices.
here are four representative testing scenarios that require ui compatibility comparison.
in the following sections we define ui compatibility as consistent ui looks and behavior.
forward backward compatibility according to android dashboard from google android platform api versions from to are in active use as of july .
google has an aggressive release cycle of new android apis and has been releasing a new version once or twice a year .
app developers need to ensure correctness across all previous android versions backward compatibility as well as future releases forward compatibility .
this backward or forward compatibility testing requires ui compatibility testing across different android versions.
device heterogeneity there are many types of android devices ranging from smartphones to tablets with different screen sizes and resolutions.
in order for apps to be ui compatible with a wide set of devices developers need to test their apps on different types of devices.
library versioning most android apps rely on thirdparty libraries for value added functionality such as advertising google play services e.g.
google maps visualization image processing e.g.
opencv and others.
however these libraries evolve in parallel with newer features being added to them over time.
when these third party libraries release a new version developers need to test if their existing apps provide the same consistent user experience across different third party library versions.
this requires ui compatibility testing across different versions.
app instrumentation many researchers and companies are interested in using java bytecode instrumentation techniques that can modify existing apps without having any source code.
examples include improving energy efficiency for always on sensing providing mobile deep links or developing novel data management systems .
to test the correctness of such systems researchers automatically transform existing android apps with their instrumentationtechniques including the ui and compare the behavior of the instrumented apps to the uninstrumented original apps.
this requires ui compatibility testing.
b. requirements for ui compatibility testing configurability a basic requirement is the ability to easily run the same app on multiple devices using different app versions android versions and third party library versions for the purpose of comparing the ui across.
testers should be able to configure the set of devices they have and set them up as required for their testing.
they should also be able to install and configure the software platform on each of the devices to exactly set up the tests they would like to run.
comparing ui structures fundamental to ui compatibility testing is the ability to query ui structures and iterate through individual elements e.g.
buttons menu elements and text boxes.
with this ability a tester can ensure consistent ui experience by iterating through all elements interacting with each of them and comparing the experience with other runs of the same app in different environments.
testing modes some testers might want to thoroughly evaluate ui compatibility by iterating through all elements in a systematic manner while other testers might want to perform a quick check of a subset of ui elements to ensure that their recent change has not affected the ui adversely.
this advocates for both sequential and randomized ui testing.
visual inspection a challenge when dealing with different form factors of mobile devices e.g.
screen size resolution and orientation is the ability to accurately compare screens as viewed by the user.
thus a testing framework needs to provide some form of automated comparison of the uis displayed on different screens without requiring manual intervention in order to enable scalable testing.
interaction performance even if two runs of an app in different environments look similar visually there might be a difference in the performance such as greater lag between devices.
this is an important problem to identify as this breaks consistent ui experience across app runs.
a testing framework must be able to check for not just consistency of ui elements but to identify differences in performance parameters such as latency and memory use.
motivated by these challenges we have designed mimic.
the next section with describe the mimic design in detail.
iii.
m imic design figure shows the mimic architecture.
mimic takes as input a set of apps and a mimic script.
the mimic runtime that runs on a desktop that described in subsection iv a executes the script and runs specified tests on the set of devices connected.
mimic is designed to be able to scale the number of apps tested as well as the number of devices used.
this architecture is managed by the mimic programming model which provides methods for the setup of the runtime configuring the set of devices that will be used for testing specifying ui tests to be run and the specification of desirable properties of the test runs that need to be logged.
while it might be possible to authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
mimic runtimemimic script device setapk set fig.
.
mimic architecture 1from mimic import mimic key device type value tuple serial android version apk under test 4experiment settings leader 0361676e094dd2ed .
demo v1.
.apk follower 072dca96d0083871 .
demo v1.
.apk 8mimic experiment settings fig.
.
example mimic configuration write similar scripts by a tester who understands the nuances of android devices and tools mimic makes this easier with a small set of apis that provide high level abstractions and callbacks specifically designed for ui compatibility testing .
a. mimic programming model our programming model provides easy abstractions to configure and set up testing environments with multiple devices android versions and app versions.
it abstracts away this complexity by providing a single device single version illusion where testers write their testing logic as if they were using a single device and a single version of an app.
our implementation described in section iv handles the differences of multiple devices and versions.
further our programming model provides the ability to compose testing different aspects of android apps by providing an expressive callback based abstractions for writing tests.
abstractions for test configuration we provide a simple dictionary abstraction for testers to express their requirements.
figure shows a code snippet that initializes two devices to compare two different versions of an app.
in line the script configures a leader device by providing three parameters a device serial number an android version and an app file name.
in line the script configures a follower device also with three parameters.
in line the script loads the configuration and initializes the mimic runtime.
when the mimic runtime executes this script it finds two devices using the serial numbers installs android .
on both devices and installs the respective versions of the app on the appropriate devices for testing.
abstractions for testing mimic provides an event driven programming model and abstractions so that testers only need to handle important events related to ui compatibility testing.
these abstractions and the event driven programming model simplify the process of writing a test script.
table i shows the set of five callbacks that the mimic programming model1def onuitreechanged tree return tree.sort sorttype random 4def handlenextuiobject ui if ui.clickable ui.click.wait the unit is ms fig.
.
randomized testing example provides and table ii lists the set of methods for configuration and ui testing.
a tester can utilize these callbacks and interfaces for ui compatibility testing.
the most important abstraction is uitree when an app launches or a user action occurs on an already opened app the app shows a new app window or activity in android s terminology .
an activity is represented as a tree with three types of ui elements i the root which is the activity object itself ii the middle ui elements that are containers of other ui elements and iii the bottom most leaves of ui elements.
we capture this using our uitree abstraction.
it contains a list of ui elements within an activity.
we encapsulate each ui element with our uiobject abstraction and by default a uitree keeps all elements in an activity in its list in a random order.
however testers can modify the list membership and the ordering.
furthermore uitree provides useful helper methods and abstractions.
for example a tester can sort the order ofuiobject s using sort find a specific uiobject withselect and check if all uiobject s are tested with completedtest .
table iii shows the summary of uitree .
along with uitree we provide two callbacks that testers need to implement.
these callbacks are designed to allow testers to handle only important events relevant to ui compatibility testing.
primarily there are two categories of events that ui compatibility testing is interested in handling.
the first category is new ui tree events handling an event of this kind allows testers to perform specific actions when a new ui tree comes up i.e.
when a new activity comes up .
for example a tester might want to measure how long it has taken to make a transition from a previous activity to a new activity or another tester might want to analyze the ui elements within an activity and skip the testing for previouslytested ui elements.
our programming model defines a callback onuitreechanged to signal the appearance of a new ui tree.
testers can implement this callback and write their testing logic pertinent to an entire activity.
the second category of events is related to handling individual ui elements.
actual testing only happens when a tester interacts with a ui element and performs a certain action e.g.
a button click.
thus we provide a callback handlenextuiobject that gets invoked with the next ui element to process within uitree .
this is possible because uitree keeps a list of uiobject s and when an invocation of handlenextuiobject returns it gets called again with the next uiobject in the list.
figure and figure show two examples of how testers can use our programming model.
these are common strategies that testers use.
first figure shows a randomized testing strategy where a tester randomly sorts all ui elements in a new ui tree lines and performs a click on the first ui authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i callbacks of mimic programming model callback interface description uitree onuitreechanged uitree tree called when there is a new uitree event.
voidhandlenextuiobject uiobject ui called when there is a new uiobject to test.
a uiobject specifies one ui element such as button or text .
voidonbatterychanged battery battery called when the battery level of an app under test has changed.
a battery represents battery usage statistics for an app under test.
voidonheapusagechanged heap heap called when the heap usage of an app under test has updated.
a heap represents heap usage for an app under test.
voidontestingtimechanged testingtime time called when the total testing time has updated.
table ii mimic programming interface 1denotes system wide functionality and2denotes device specific functionality.
programming interface description voidterminate msg 1terminate all testing with the given message.
voiddetach serial 1remove the device of the given serial number from mimic.
voidattach serial 1add the device of the given serial number to mimic.
booldiffuitree threshold 1return true if the leader screen and one of follower screens are different over the given threshold.
deviceinfo info 2retrieve device information such as the screen width height rotation product name android version etc.
voidclick 2perform a button click.
voidlongclick 2perform a long click action on the object.
voiddrag x y 2drag the ui object to other point.
voidswipe direction 2perform a swipe action.
voidpress buttonname 2press the given button.
supported home back recent volumeup volumedown power etc.
voidwait time 2wait the given time for the next action.
boolcontains kwargs return ture if this object contains a mapping for the given keyworded variables.
it can be called with device uitree and uiobject .
supported keywords text classname description packagename resourceid etc.
voidscreenshot filename 2take a screenshot and save it with a given name.
table iii theuit ree abstraction attribute method description device device instance on which this uitree is running.
previousuitree the previous uitree of this uitree .
completedtest flag for weather all object s in this uitree are tested or not.
loadingtime load time of this uitree from performing a ui action to loading this uitree .
boolcontains kwargs return ture if this uitree contains a mapping for the given keyworded variables.
supported keywords text classname description packagename resourceid etc.
uiobject select kwargs return the uiobject to which the given keyword variables are mapped in uitree or none if the uitree contains no mapping for the given keyword variables.
uitree sort sorttype returns a new uitree with those uiobject s in sorted order based on the given sorttype.
1def onuitreechanged tree if tree.previousuitree.completedtest !
true tree.device.press back return tree.sort sorttype untested 6def handlenextuiobject ui if ui.clickable ui.click.wait the unit is ms fig.
.
sequential testing example element in the list for the ui tree lines .
it also shows an example use of wait which allows testers to wait a certain period of time for the next action.
second figure shows a sequential testing strategy where a tester tests every ui element on every activity.
if a ui element action makes the app to transition to a new activity then the testing script presses the back button to go back to the previous activity if the previous activity still contains more ui elements to test lines .
these examples show how concisely a tester can express testing logic.
in section v we show that simple randomized testing needs lines of code using our programming model while lines of code is needed using android ui automator.
the main difference comes from the fact that we provide highlevel abstractions in the form of callbacks and take care of all the plumbing work necessary.
using android ui automator 1def onheapusagechanged heap with open heap.device heapusage.log a as log log.write str mimic.currentuitree str mimic.currentuiobject str heap.total heap allocation n 7def onuitreechanged tree if tree.loadingtime the unit is ms mimic.terminate gui lagging return tree.sort untested fig.
.
performance testing example testers have the burden of checking if there is a new ui tree getting all the ui elements from the tree checking their properties e.g.
whether or not they are clickable etc.
this makes a significant difference in terms of development effort.
abstractions for performance monitoring previous research has shown that are three predominant types of performance bugs in android apps.
they are gui latency energy leaks and memory bloat.
mimic programming model provides mechanisms for monitoring the resources related to these bugs and provides callbacks so the test writer can easily specify testing actions to compare how their apps behave across different devices and versions.
specifically our programming model provides three callbacks onheapusagechanged onbatterychanged authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
mimic runtime ui selectordevice controllergraph generatorimage archivelog collector device set fig.
.
mimic runtime components andontestingtimechanged which the programmer can leverage to invoke testing.
onheapusagechanged is invoked when the heap usage of an app has changed.
this is configurable and by default it is triggered every 10kb.
onbatterychanged is called when the battery level changes.
ontestingtimechanged is invoked when the total testing time changes.
this is also configurable and by default it is triggered every second.
these methods allow the tester the ability to log these changes along with other state such as the ui tree to compare later.
figure shows an example where the code logs how much the heap usage has changed what is the state of the ui and what is the current ui object being interacted with.
helper methods for ui difference monitoring an essential aspect of ui compatibility testing is comparing how an app displays its ui across different versions and devices.
in order to support this our programming model provides two methods for capturing and comparing screenshots.
screenshot method takes a screenshot.
diffuitree method returns true if any of the followers is displaying a different ui from that of the leader.
diffuitree takes a threshold as an input parameter which indicates the percentage of difference.
for example diffuitree returns true when any of the follower s ui is more than different from the leader s. we use image processing to implement this as we describe in section iv b. iv .
m imic implementation this section describes the mimic runtime as well as how we enable visual inspection.
our prototype is roughly lines of python code.
a. mimic runtime the mimic runtime provides the environment for the execution of a mimic script.
it runs on a computer controls all android devices connected to it via usb and displays test results.
figure shows the components of the runtime.
the main component is device controller that executes a mimic script and controls the entire flow of testing.
while executing a script it interacts with testing devices using android tools mainly adb and ui automator and leverages other components.
this section describes the flow of execution and the components of the runtime.
initialization device controller first installs an android os images as well as testing apps on different devices andmain activity picture activity music activity main activity music activity a demo app .
b demo app .
fig.
.
two different versions of demo apps.
denotes the leader device.
packagename .mainactivity0 .mainactivity music2 .mainactivity pictures1 .music4 .pictures3 .music play15 .music stop16 .pictures show7 a graph for demo app .
.music stop2packagename .mainactivity0 .mainactivity music2 .mainactivity pictures1 .music3 .pictures .music play14 .music stop15 .music play2 .pictures show b graph for demo app .
package activity ui element not visited not available fig.
.
graph representations for demo apps in figure .
the numbers denote the visiting nodes order.
configures each device.
android image archive stores stock android os images our implementation currently supports ten images ranging from android .
.
to .
.
.
testing logic execution after initialization device controller launches testing apps on all devices.
it then periodically monitors each device for ui tree and resource status changes e.g.
battery level .
if any change is detected device controller invokes the appropriate callback provided by our programming model.
device controller monitors ui tree changes using android ui automator which enables the inspection of ui elements and their hierarchy.
device controller uses adb to monitor resource status changes.
our periodicity of monitoring is currently .
seconds and this is configurable.
when device controller detects a new ui tree i.e.
when a new activity comes up on a device it interacts with ui selector to filter out unnecessary ui elements.
we filter ui elements because there are many that are just containers for other ui elements and are not typically necessary for testing.
for example a simple button in android that turns wi fi on or off requires multiple ui elements e.g.
a text box an interactable button and a container that groups all ui elements together.
a ui testing system would only be interested in the button that is clickable.
the ui selector removes non leaf ui elements as well as non clickable ones from the ui hierarchy.
testing result generation after executing testing logic authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
anroid .
anroid .
anroid .
.
fig.
.
open gps tracker on different android versions.
denotes the leader device.
table iv ui d ifference inspection using three methods .
denotes methods that mimic uses.
sample testing image typecolor histogram template matchingfeature matching different ui color .
.
.
different ui position .
.
.
absence of ui .
.
.
device controller provides testing results in two forms.
first it uses log collector to capture all logs that testing apps generate.
device controller can display individual logs as well as deltas among the logs from different devices.
second device controller uses graph generator to display all ui transitions that occurred during testing.
this ui transition graph helps testers visualize how different app instances behaved.
figure and figure show visualizations of an example using a demo app we have developed.
there are two versions of this app.
the first version v1.
has three activities main picture and music.
however the second version v1.
only has two activities main and music and there are two more buttons in the music activity.
we intentionally inject one run time error that causes the app to crash when the show button on the picture activity is clicked.
figure shows an example run.
using our leader follower model both versions execute the exact same sequence of ui events.
for v1.
the unvisited node the show button is due to the run time crash error that we inject since the app crashes the button is not explored.
for v1.
the two unvisited nodes the second play button and the corresponding stop button are due to our leader follower model since the leader does not have those buttons they are not explored in the follower.
using these graphs testers can easily discover points of run time errors and compare ui differences across different versions.
b. enabling visual inspection as mentioned earlier our programming model provides screenshot anddiffuitree to take and compare screenshots.
in our implementation diffuitree computes a measure of change between the leader and the followers.
we do this by i taking a screenshot across testing devices ii masking the top status bar and the bottom navigation bar because each android version has the different size of status and navigation bars and iii calculating the difference between the screenshots using both color histogram difference and feature matching .table v statistics of a pps category o fa p p s example average app size lifestyle xfinity home .0m entertainment vimeo .2m travel tripadvisor .1m sports espn .0m personization paypal cash .9m education bookshelf .0m tools go security .9m photography open camera .4m total n a .4m to calculate the ui difference we have experimented with three popular methods from opencv histogram difference template matching and feature matching .
using original screen image of starbucks app we create three different sample image types that are possible due to version differences and we compare each sample image type with the original image using three methods mentioned.
table iv shows percentage differences from this comparison.
while results of template matching show an inaccuracy on all three sample image types color histogram difference and feature matching have given us satisfactory results either on color difference or on ui feature difference.
mimic takes higher percentage difference between color histogram difference and feature matching.
while we have used particular image processing mechanisms it is also easy to create other custom mechanisms to quantify visual difference between two screens.
figure shows an example of display differences across different android versions from .
to .
.
.
in this example the leader screenshot is about .
different from the image on android .
and about .
different from the image on android .
.
.
by using both diffuitree andscreenshot a tester can efficiently detect the display distortion.
v. e v aluation in this section we demonstrate mimic s capabilities in three ways.
to demonstrate effectiveness of our programming model we compare the lines of code necessary to implement different testing strategies using mimic and android ui automator.
second we evaluate mimic in different scenarios forward compatibility testing backward compatibility testing different app version testing and instrumentation testing.
for each testing scenario we have used separate android apps due to different aspects that we want to evaluate.
for each testing scenario we explain the details of how we have picked the apps.
table v shows the statistics of the total apps.
third we conduct an experiment to show that mimic correctly catches compatibility differences.
for this we use apps from the ficfinder data set which are real apps downloaded from open source repositories such as github with verified ui compatibility problems.
we have used android devices for our experiments three nexus s 512mb ram running android versions .
.
.
.
and .
.
two galaxy nexus 1gb ram running .
.
and .
.
four nexus 2gb ram running .
.
.
and .
and one galaxy s4 2gb ram running android .
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table vi as ummary of development effort .1denotes it runs on a single device independently 2means that it runs across all testing devices.
testing logic framework loc testing environment randomized testing1ui automator non automated sequential testing1ui automator non automated randomized testing2mimic automated sequential testing2mimic automated a. development effort we demonstrate the effectiveness of our programming model by comparing the lines of code necessary to implement randomized testing and sequential testing.
the randomized testing strategy we implement picks one clickable ui element e.g.
a button or a text input box performs an action on it e.g.
a button click and repeats it until either there is no clickable ui element or the app crashes.
the sequential testing strategy we implement tests each and every ui element one by one.
we have implemented both strategies using android ui automator and mimic.
mimic programming model enables compact descriptions of testing strategies our randomized testing implementation requires lines of code and our sequential testing implementation requires lines of code.
in contrast ui automator requires lines of code for randomized testing and lines of code for sequential testing which are 16x and 15x higher respectively.
even worse it does not have any support for device management environment initialization etc.
it requires testers to manually set up a testing environment.
table vi shows this result.
b. f orward compatibility testing to demonstrate that mimic is useful to test forward compatibility we have downloaded popular real apps from google play and chosen apps that target android api .
we then run the selected apps across four android api versions from android .
to android .
three times.
we have designated api as the leader and api api and api as followers.
we use three methods in our experiment base random mimic random and mimic sequence .
the base random method does not use mimic it implements the randomized testing strategy described earlier in subsection v a using ui automator.
since it does not use mimic randomization is done on each device independently without using our leaderfollower model.
the mimic random method uses mimic and implements the same randomized testing strategy.
however since it uses mimic all ui actions are synchronized across different devices.
the mimic sequence method uses mimic and implements the sequential testing strategy described earlier in subsection v a. during the experiment we have captured all logs that each app generates through our log collector.
we have collected log files and analyzed the log files.
table vii shows the summary of errors detected by our experiment.
all the errors reported in this section have caused the testing apps to crash.
and errors from nine appsare uncovered by the base random method the mimic random method and the mimic sequence method respectively.
apps out of apps have thrown at least one error.
more specifically yelp threw networkonmainthreadexception across four android api versions networkonmainthreadexception is thrown if an app performs a long task such as a networking operation on its ui thread also called the main thread .
the cause of the crashes from ip webcam and ccleaner is not properly handling new features such as runtime permissions .
three apps home exercise workouts weather kitty and livestream threw nosuchmethoderror at run time since they invoke methods that newer versions of android no longer support.
for example livestream app has crashed on api because this app uses apache httpclient which is deprecated effective api .
we make two observations.
first comparing the baserandom and the mimic random methods we observe that mimic s follow the leader model of testing is often more effective than independent testing.
this is because out of apps report more errors when using mimic and out of apps report the same number of errors.
second comparing the mimic random and the mimic sequence methods we observe that the mimic sequence method is more effective in all cases.
this is simply because the sequential testing tests every ui element one at a time and hence has a better chance of triggering an error.
c. backward compatibility testing to show that mimic is useful for backward compatibility as well we have run the same experiment as mentioned in subsection v b. only differences are that we have initialized the leader to use api android .
and followers to use api android .
api android .
and api android .
and selected apps that target api .
table viii shows the summary of errors detected by our experiment.
and errors from apps out of apps are found by the base random method the mimic random method and the mimic sequence method respectively.
the apps shown in table viii have experienced at least one error.
all of the errors caused the apps to crash since they are nullpointerexception andnosuchmethoderror .
once again nosuchmethoderror has been caused by the use of deprecated apis.
for example map of nyc subway app uses getdrawable which is not available on the android platform below api .
overall we can also see the similar patterns that we have observed for the forward compatibility experiment.
d. app v ersion compatibility testing in order to evaluate mimic s capability for app version compatibility testing we have selected apps that are top apps in each category from google play.
we have monitored these apps for two weeks and chosen out of apps that have at least two different versions.
for the simplicity of presentation we simply call earlier versions of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table vii thedetails of detected errors for forward compatibility .
denotes android api version on the leader.
app name android api version exception type base random mimic random mimic sequence yelp networkonmainthreadexception college sports live nullpointerexception home exercise workouts nosuchmethoderror fast cleaner nullpointerexception photo lab nullpointerexception weather kitty nosuchmethoderror ip webcam mishandling runtime permissions ccleaner mishandling runtime permissions livestream nosuchmethoderror table viii thedetails of detected errors for backward compatibility .
denotes android api version on the leader.
app name android api version exception type base random mimic random mimic sequence trutv nullpointerexception whitetail deer calls nosuchmethoderror cheapflights nosuchmethoderror map of nyc subway nosuchmethoderror watchespn nullpointerexception collage maker nosuchmethoderror table ix thedetails of detected errors .
both the stable and new apps are tested on android .
.
app name stable version new version exception type base random mimic random mimic sequence base random mimic random mimic sequence td jakes sermons inflateexception venmo illegalargumentexception v olume booster securityexception countdown timer stopwatch caller id nullpointerexception wonder buy nullpointerexception table x thedetails of detected errors from i nstrumented apps.
both versions of apps are tested on android .
.
app name non instrumented version instrumented version exception type mimic random mimic sequence mimic random mimic sequence gasbuddy classcastexception stamp and draw paint classcastexception big personality test assertionerror text to speech reader assertionerror gun vault assertionerror street art 3d nullpointerexception viaplay gamecenter f nullpointerexception q recharge nullpointerexception funny face photo camera nullpointerexception japan offline map hotels cars nullpointerexception apps as stable versions and later versions of apps as new versions in this experiment.
we have run the stable versions and new versions of apps on the same android version .
in order to show that mimic is effective in discovering errors that occur across different versions of apps.
table ix shows the result.
in total four types of errors have been found in five apps.
for td jakes sermons app the error happens during the initialization thus it is reported in all testing logic.
for wonder buy the error has been found in the stable version but not in the newer version which indicates that the developer has fixed the bug for the newer version.
we observe that the two methods using mimic the mimic random and the mimic sequence methods discover more errors across different versions than the base random method.
e. instrumentation testing to demonstrate mimic s ability to compare runtime behavior between non instrumented and instrumented versions we have used reptor and its data set.
reptor is a system wehave developed previously.
it is a bytecode instrumentation tool enabling api virtualization on android.
to demonstrate the capability of reptor it uses real apps instruments the apps and verifies the runtime behavior between the original and the instrumented version of apps using a randomized testing strategy.
this randomized strategy they use is in fact the same as the base random method described in subsection v b. for our experiment we have used the same apps and randomly selected apps of the and run the apps on android .
two times.
we have also used the result from the reptor paper for those apps and use it as the base random comparison point in this experiment.
in the first run we have run the apps with the mimic random method.
in the second run we have used the mimic sequence method.
we have also recorded heap allocation sizes using onheapusagechanged described in section iii in order to show heap usage comparison between the non instrumented and instrumented apps.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table xi thedetails of detected compatibility bugs.
the leader nexus running on android .
.
.
denotes that the bug are confirmed on multiple devices.
e and u stand for errors and ui distortion respectively.
app name category loc apk version no.
bug type results device android version open gps tracker travel local .1k .
.
u nexus .
.
connectbot communication .2k .
.
e galaxy nexus .
.
ankidroid education .2k .5alpha48 e galaxy s4 .
c geo entertainment .8k .
.
e nexus s .
.
anysoftkeyboard tools .0k .
.
e galaxy s4 .
qksms communication .4k .
.
e neuxs .
bankdroid finance .8k .
.
.
e galaxy s4 .
evercam tools .8k .
.
e nexus s .
.
chatsecure communication .2k .
.
u e galaxy nexus .
.
antennapod media video .9k .
.
.
u nexus .
brave android browser personalization .2k .
.
u nexus .
irssinotifier communication .5k .
.
e csipsimple communication .2k .
.
u bitcoin wallet finance .8k .
e table xii thedetails of detected performance bugs.
the leader nexus running on android .
.
.
denotes that the bug are confirmed on multiple devices.
g b and m stand for gui lagging battery leak and memory bloat respectively.
u stands for unknown due to not enough information given.
app name category loc apk version no.
bug type results device android version open gps tracker travel local .1k .
.
g galaxy nexus .
.
ankidroid education .6k .1beta6 m nexus s .
.
antennapod media video .7k .
.
.
g nexus .
csipsimple communication .2k .
.
b galaxy s4 .
brave android browser personalization .2k .
.
g m nexus .
k mail communication .8k .
g nexus .
wordpress social .1k .
.
u 20015202530354045505560total heap size mb audiobooks from audible audiobooks from audible 50152025303540total heap size mb flv video player flv video player fig.
.
heap usage for four selected apps instrumented apps .
the x axis shows testing time in seconds.
table x shows the details of the result.
mimic has caught errors that were not originally reported in the reptor paper using their randomized testing strategy i.e.
the base random method described in subsection v b which can miss some of the ui paths.
two errors are found in both non instrumented and instrumented versions.
eight errors are detected in only instrumented apps.
figure shows the heap usage results over testing time.
the results help app developers to understand memory utilization of their apps.
from the results we can see that mimic effectively verifies the runtime behavior in terms of error detection and performance comparison.
f .
ficfinder data set results compatibility problems the ficfinder data set consists of large scale open source android apps with various bugs and problems.
out of the apps apps are known to have ui compatibility problems and we have conducted an experiment to evaluate whether or not mimic can flag those apps as problematic.
we have used the randomized testing strategy and diffuitree function with threshold.
we have tested each of apps for minutes.table xi shows the results.
we have detected errors and distorted ui issues from apps.
for example c geo uses showcaseviewbuilder that does not work on api or below and it crashes on nexus s running api .
evercam also crashes on nexus s api nexus api etc.
due to a third party library uses called splunkmint .
the library has a compatibility problem with android versions from api to api .
four ui distortion issues have structural ui problems that do not render uis correctly across different android versions.
mimic did not flag three apps irssinotifier csipsimple and bitcoin wallet since their problems are specific to particular devices namely blackberry and zenfone .
since we have not tested on those devices mimic has not reported any problem.
performance problems we have also used the ficfinder data set to test mimic s capability to detect performance problems i.e.
lagging uis energy leaks and memory bloats .
out of apps apps are known to have performance problems and we have used those apps in our experiment to see if mimic flags those apps as problematic.
we have used the randomized testing strategy and also implemented onbatterychanged andonheapusagechanged to record battery and heap usage.
we report cases where the difference in battery or heap usage is more than .
to detect battery bloat we have run this experiment up to minutes.
table xii shows the results.
the results show that mimic flags apps out of apps as problematic.
mimic detects four lagging ui problems two memory bloat problems and one excessive battery drain problem.
we consider gui lagging if an app has a ui that cannot be loaded within seconds.
from our inspection of antennapod code and logs antennapod lags whenever it downloads new episodes.
ankidroid and brave android browser throw java.lang.outof254 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table xiii comparison to existing tools .
program.
hetero.
and no instr.
stand for programmability heterogeneity and no instrumentation required.
name type program.
hetero.
no instr.
mimic black monkeyrunner black caiipa black droidfuzzer black puma black robotest black robotium black espresso grey androidripper black a3e black dynodroid black evodroid white concolictest black varnasena black memoryerror .
csipsimple causes about battery drain for minutes on galaxy s4.
we could not find the root cause of the excessive battery drain but we realized that cpu user time of csipsimple is 965s 13ms from the logs that mimic captured.
mimic does not flag wordpress as problematic the wordpress issue tracker reports that wordpress .
.
version has gui lagging issue.
however there is not enough information to determine for certain that there is indeed a performance problem.
vi.
d iscussion in this section we discuss implications of our design choices as well as potential improvements to mimic.
mimic may not be suitable for all types of apps although our evaluation shows that mimic is effective in finding potential ui compatibility problems across different versions and devices it is not the case that mimic is suitable for all types of apps.
for example sensor centric apps such as games are not suitable for testing on mimic since mimic does not test sensor input.
for future work we are exploring how to enable automated testing for sensor centric apps.
mimic does not support system events some ui actions can be triggered by system events for example a calendar alarm pop up can be triggered by a system alarm event.
currently mimic does not support the testing of such ui events.
however supporting such ui events is potentially possible by extending our current implementation of mimic with the android activity manager available via amcommand we can send system events to an app.
however this could lead to a vast space to explore for testing as android has a large set of system events.
intelligently reducing the size of this space is also our ongoing work.
vii.
r elated work in this section we compare existing mobile app testing systems with mimic on a few important aspects and discuss how mimic occupies a unique position in the design space of mobile app testing.
in table xiii we compare mimic to other systems in four aspects testing type programmability support for heterogeneity and whether or not app instrumentation is required.
we discuss each aspect in this section.
testing type there are mainly three types of testing approaches depending on the availability of source code.
thefirst type is black box testing which does not require any source code.
fuzz testing and model based testing belong to this category but they focus on testing a single app.
mimic also belongs to this category but its focus is on using multiple devices and versions and testing ui compatibility.
the other two approaches are white box testing and grey box testing.
white box testing requires full access to source code.
grey box testing does not require source code but assumes some knowledge about apps being tested e.g.
ids of ui elements.
typical symbolic execution uses white box testing and there are other systems that take either a white box approach or a grey box approach .
since white box and grey box approaches require knowledge about apps being tested they are not suitable for certain scenarios such as testing instrumented apps.
programmability many existing systems only allow testers to use predefined methods of input e.g.
a monkey based random input generation method.
these systems do not provide any programmable api.
support for heterogeneity other systems do not provide support for handling multiple devices or versions.
in these systems testers need to manually set up and configure their testing environments if they want to leverage those systems for ui compatibility testing.
app instrumentation a few existing systems require app instrumentation to enable testing.
while instrumentation allows deep inspection of an app without requiring its source code it modifies the app code which might result in producing different behavior e.g.
heisenbugs .
viii.
c onclusions in this paper we have described a new ui compatibility testing system for android apps called mimic.
mimic supports follow the leader model of parallel testing where we designate one device to perform a sequence of ui actions all other devices follow this leader and perform the same sequence of ui actions.
this model is useful for ui compatibility testing across different android versions device types and app versions.
testing is made easy by mimic through a concise programming model for writing tests.
the programming model allows testers to quickly set up testing environments and to express their own testing logic.
after executing testing logic mimic reports ui compatibility problems such as different exceptions thrown different ui paths taken differences in resource usage etc.
our evaluation with a few hundred android apps downloaded from google play shows that mimic can effectively detect real errors and report ui compatibility problems across different android or app versions.
ix.
a cknowledgments we would like to thank the anonymous reviewers for their valuable feedback.
this work was supported in part by the generous funding from the national science foundation cns1350883 career and cns .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.