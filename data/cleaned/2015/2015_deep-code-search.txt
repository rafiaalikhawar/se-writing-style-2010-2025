deep code search xiaodong gu1 hongyu zhang2 and sunghun kim1 1the hong kong university of science and technology hong kong guxiaodong1987 .com hunkim cse.ust.hk 2the university of newcastle callaghan australia hongyu.zhang newcastle.edu.au 3clova ai research naver abstract toimplementaprogramfunctionality developerscanreusepreviouslywrittencodesnippetsbysearchingthroughalarge scale codebase.overtheyears manycodesearchtoolshavebeenproposedtohelpdevelopers.theexistingapproachesoftentreatsource code as textual documents and utilize information retrieval models to retrieve relevant code snippets that match a given query.
these approaches mainly rely on the textual similarity between source code and natural language query.
they lack a deep understanding of the semantics of queries and source code.
inthispaper weproposeanoveldeepneuralnetworknamed codenn code description embedding neural network .
instead of matching text similarity codenn jointly embeds code snippets and natural language descriptions into a high dimensional vector space in such a way that code snippet and its correspondingdescription have similar vectors.
using the unified vector repre sentation code snippets related to a natural language query can be retrieved according to their vectors.
semantically related words can also be recognized and irrelevant noisy keywords in queries can be handled.
as a proof of concept application we implement a code search toolnameddeepcsusingtheproposedcodennmodel.weempirically evaluate deepcs on a large scale codebase collected from github.theexperimentalresultsshowthatourapproachcaneffectively retrieve relevant code snippets and outperforms previous techniques.
ccs concepts software and its engineering reusability keywords code search deep learning joint embedding acm reference format xiaodong gu1 hongyu zhang2 and sunghun kim1 .
.
deep code search.
in icse icse 40th international conference on software engineering may june gothenburg sweden.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
icse may june gothenburg sweden copyright held by the owner author s .
publication rights licensed to the association for computing machinery.
acm isbn ... .
introduction code search is a very common activity in software development practices .toimplementacertainfunctionality forexample toparse xmlfiles developersusually searchand reusepreviously written code by performing free text queries over a large scale codebase.
many code search approaches have been proposed most of them being based on information retrieval ir techniques.forexample linsteadetal.
proposed sourcerer aninformationretrievalbasedcodesearchtoolthatcombines the textual content of a program with structural information.
mcmillanetal.
proposedportfolio whichreturnsachainof functions through keyword matching and pagerank.
lu et al.
expanded a query with synonyms obtained from wordnet and then performed keyword matching of method signatures.
lv etal.
proposed codehow which combines text similarity and api matching through an extended boolean model.
afundamentalproblemoftheir basedcodesearchisthemismatch between the high level intent reflected in the natural languagequeriesandlow levelimplementationdetailsinthesource code .
source code and natural language queries are heterogeneous.theymaynotsharecommonlexicaltokens synonyms orlanguagestructures.instead theymayonlybesemanticallyrelated.
forexample arelevantsnippetforthequery readanobjectfrom an xml could be as follows public static s s deserialize class c file xml try jaxbcontext context jaxbcontext .newinstance c unmarshaller unmarshaller context.createunmarshaller s deserialized s unmarshaller .unmarshal xml returndeserialized catch jaxbexception ex log.error error deserializing object from xml ex return null existing approaches may not be able to return this code snippet as it does not contain keywords such as readandobjector their synonymssuchas loadandinstance.therefore aneffectivecode searchenginerequiresahigher levelsemanticmappingbetween code and natural language queries.
furthermore the existing approacheshavedifficultiesinqueryunderstanding .they cannoteffectivelyhandleirrelevant noisykeywordsinqueries .
therefore an effective code search engine should also be able to understandthesemanticmeaningsofnaturallanguagequeriesand source code in order to improve the accuracy of code search.
inourpreviouswork weintroducedthedeepapiframework which is a deep learning based method that learns the semantics of queries and the corresponding api sequences.
however searching sourcecodeismuchmoredifficultthangeneratingapis because acm ieee 40th international conference on software engineering authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden xiaodong gu hongyu zhang and sunghun kim thesemanticsofcodesnippetsarerelatednotonlytotheapisequencesbutalsotoothersourcecodeaspectssuchastokensand methodnames.forexample deepapicouldreturnthesameapi imageio.write for the query save image as png andsave image as jpg.nevertheless theactualcodesnippetsforansweringthetwo queries are different in terms of source code tokens.
therefore the code search problem requires models that can exploit more aspects of the source code.
inthispaper weproposeanoveldeepneuralnetworknamedcodenn code descriptionembeddingneuralnetwork .tobridge thelexicalgapbetweenqueriesandsource code codennjointly embeds code snippets and natural language descriptions into a high dimensional vector space in such a way that code snippetand its corresponding description have similar vectors.
with the unifiedvectorrepresentation codesnippetssemanticallyrelated to a natural language query can be retrieved according to their vectors.
semantically related words can also be recognized and irrelevant noisy keywords in queries can be handled.
using codenn weimplement a code search tool deepcs as a proof of concept.
deepcs trains the codenn model on a corpus of .2millionjavacodesnippets intheformofcommentedmethods from github.
then it reads code snippets from a codebase and embedsthemintovectorsusingthetrainedcodennmodel.finally whenauserqueryarrives deepcsfindscodesnippetsthathave the nearest vectors to the query vector and return them.
toevaluatetheeffectivenessof deepcs weperformcodesearch on a search codebase using real world queries obtained from stackoverflow.ourresultsshowthatdeepcsreturnsmorerelevantcodesnippetsthanthetworelatedapproaches thatis codehow and a conventional lucene based code search tool .
on average the first relevant code snippet returned by deepcsis ranked .
while the first relevant results returned by code how and lucene are ranked .
and .
respectively.
for of the queries the relevant code snippets can be found within the top returned results.
the evaluation results confirm the effectiveness of deepcs.
toourknowledge wearethefirsttoproposedeeplearningbased code search.
the main contributions of our work are as follows weproposeanoveldeepneuralnetwork codenn tolearnaunifiedvectorrepresentationofbothsourcecodeandnatural language queries.
we develop deepcs a tool that utilizes codenn to retrieve relevant code snippets for given natural language queries.
weempiricallyevaluatedeepcsusingalargescalecodebase.
therestofthispaperisorganizedasfollows.section2describes the background of the deep learning based embedding models.section describes the proposed deep neural network for codesearch.
section describes the detailed design of our approach.section presents the evaluation results.
section discusses our work followed by section that presents the related work.
we conclude the paper in section .
background ourworkadoptsrecentadvancedtechniquesfromdeeplearning and natural language processing .
in this section we discuss the background of these techniques.hidden layeroutput layer input layer htht wt a rnn structureembedding h1 h2 h3 parse xml filew1 w2 w3 b rnn for sentence embedding figure illustration of the rnn sentence embedding .
embedding techniques embedding alsoknownasdistributedrepresentation is a techniquefor learningvector representationsof entitiessuch as words sentences and images in such a way that similar entities have vectors close to each other .
atypicalembeddingtechniqueiswordembedding whichrepresents words as fixed length vectors so that similar words are close toeachotherinthevectorspace .forexample supposethe wordexecuteisrepresentedas andtheword run isrepresentedas .fromtheirvectors wecanestimate their distance and identify their semantic relation.
wordembedding is usually realized using a model such as cbow and skip gram .thesemodelsbuildaneuralnetworkthatcaptures the relations between a word and its contextual words.
the vector representationsofwords asparametersofthenetwork aretrained with a text corpus .
likewise asentence i.e.
asequenceofwords canalsobeembedded as a vector .
a simple way of sentence embedding is for example to view it as a bag of words and add up all its word vectors .
.
rnn for sequence embedding wenowintroduceawidely useddeepneuralnetwork therecurrent neural networks rnn for the embedding of sequential data such as natural language sentences.
the recurrent neural network is a class of neural networks where hidden layers arerecurrently used for computation.
this creates an internal stateof the network to record dynamic temporal behavior.
figure 1a shows the basic structure of an rnn.
the neural network includes three layers an input layer which maps each input to a vector a recurrenthiddenlayer whichrecurrentlycomputesand updatesa hiddenstateafterreadingeachinput andanoutputlayerwhich utilizesthehiddenstateforspecifictasks.unliketraditionalfeedforward neural networks rnns can embed sequential inputs such as sentences using their internal memory .
consider a natural language sentence with a sequence of t wordss w1 ... wt rnn embeds it through the following computations it readswordsin thesentence oneby one andupdates a hiddenstateateachtimestep.eachword wtisfirstmappedtoa d dimensionalvector wt rdbyaone hotrepresentation or word embedding .
then the hidden state values in the hidden layer htisupdatedattime tbyconsideringtheinputword wtand the preceding hidden state ht ht tanh w t ... t authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deep code search icse may june gothenburg sweden h0h1h2h3max pooling with 1h4 window size7 figure illustration of max pooling where r2drepresents the concatenation of two vectors w r2d dis the matrix of trainable parameters in the rnn while tanhis a non linearity activation function of the rnn.
finally the embedding vector of the sentence is summarized from the hiddenstates h1 ... ht.atypicalwayistoselectthelasthidden statehtastheembeddingvector.theembeddingvectorcanalsobe summarizedusingothercomputationssuchasthemaxpooling s maxpooling maxpooling is an operation that selects the maximum value in each fixed size region over a matrix.
figure shows an example ofmaxpoolingoverasequenceofhiddenvectors h1 ... ht.each columnrepresentsahiddenvector.thewindowsizeofeachregion is set to tin this example.
the result is a fixed length vector whose elements are the maximum values of each row.
maxpooling cancapturethemostimportantfeature onewiththehighestvalue for each region and can transform sentences of variable lengths into a fixed length vector.
figure1bshowsanexampleofhowrnnembedsasentence e.g.
parse xml file into a vector.
to facilitate understanding we expand therecurrenthiddenlayerforeachtimestep.thernnreadswordsin the sentence one by one and records a hidden state at each time step.
when it reads the first word parse it maps the word into a vectorw1andcomputesthecurrenthiddenstate h1usingw1.then itreadsthesecondword xml embedsitinto w2 andupdatesthe hidden state h1toh2usingw2.
the procedure continuesuntil the rnnreceivesthelastword fileandgetsthefinalstate h3.thefinal stateh3can be used as the embedding cof the whole sentence.
the embedding of the sentence i.e.
the sentence vector can be used for specific applications.
for example one can build a languagemodelconditioningonthesentencevectorformachine translation .
one can also embed two sentences a question sentence and an answer sentence and compare their vectors for answer selection .
.
joint embedding of heterogeneous data suppose there are two heterogeneous data sets xandy.
we want to learn a correlation between them namely f x y for example suppose xis a set of images and yis a set of natural languagesentences fcanbethecorrelationbetweentheimages andthesentences i.e.
imagecaptioning .sincethetwodatasourcesareheterogeneous itisdifficulttodiscoverthecorrelation fdirectly.
thus we need a bridge to connect these two levels of information.
joint embedding also known as multi modal embedding is atechniquetojointlyembed correlateheterogeneousdataintoa unified vector space so that semantically similar concepts acrossthe two modalities occupy nearby regions of the space .
the joint embedding of xandycan be formulated as x vx j vx vy vy y read a text file line by line read an object from an xml file description query embedding code embeddingpublic voidreadtext string file bufferedreader br newbufferedreader newfileinputstream file string line null while line br.readline !
null system.out.println line br.close public s s deserialize class c file xml jaxbcontext context jaxbcontext.newinstance c unmarshaller unmarshaller context.createunmarshaller s deserialized s unmarshaller.unmarshal xml returndeserialized figure3 anexampleshowingtheideaofjointembeddingforcode and queries.
the yellow points represent query vectors while the blue points represent code vectors.
where x rdis an embedding function to map xinto a ddimensional vector space v y rdis an embedding function tomapyintothesamevectorspace v j isasimilaritymeasure e.g.
cosine toscorethematchingdegreesof vxandvyin order to learn the mapping functions.
through joint embedding heterogeneous data can be easily correlated through their vectors.
joint embedding has been used in many tasks .
for example in computervision karpathyand li use aconvolutional neural network cnn a deep neural network as the and an rnn as the to jointly embed both image and text into the same vector space for labeling images .
a deep neural network for code search inspired by existing joint embedding techniques weproposeanoveldeepneuralnetworknamedcodenn codedescription embedding neural network for the code search problem.
figure illustrates the key idea.
natural language queries and code snippets are heterogeneous and cannot be easily matched according to their lexical tokens.
to bridge the gap codenn jointly embeds code snippets and natural language descriptions into a unified vector space so that a query and the corresponding code snippets are embedded into nearby vectors and can be matched by measuring vector similarities.
.
architecture asintroducedinsection2.
ajointembeddingmodelrequiresthree components the embedding functions x rdand y rd as well as the similarity measure j .
codenn realizes these components with deep neural networks.
figure shows the overall architecture of codenn.
the neural network consists of three modules each corresponding to a component of joint embedding a code embedding network conn to embed source code into vectors.
a description embedding network denn to embed natural language descriptions into vectors.
a similarity module that measures the degree of similarity between code and descriptions.
the following subsections describe the detailed design of these modules.
.
.
code embedding network.
the code embedding network embedssourcecodeintovectors.sourcecodeisnotsimplyplain text.
it contains multiple aspects of information such as tokens controlflowsandapis .inourmodel weconsiderthreeaspects authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden xiaodong gu hongyu zhang and sunghun kim code embedding network conn description embedding network denn code description code vector description vector cosine similarity a overall architecturecode description rnnrnnrnnmax pooling read a text rnn filernn rnnmax pooling mlp text reader rnn rnn rnnmax pooling scanner.new scanner.next scanner.closemax pooling str buff close mlpfusion mlp method name api sequence tokens code vector description vector cosine similarity b detailed structure figure the structure of the code description embedding neural network ofsourcecode themethodname theapiinvocationsequence and the tokens contained in the source code.
they are commonly used in existing code search approaches .
for each codesnippet atthemethodlevel weextractthesethreeaspects of information.
each is embedded individually and then combined into a single vector representing the entire code.
consider an input code snippet c where m w1 ... wnmisthe methodnamerepresented asasequence of nmcamel split tokens a a1 ... anais the api sequence with naconsecutive api method invocations and 1 ... n is the set of tokensinthesnippet.theneuralnetworkembedsthethreeaspects asfollows forthemethodname m itembedsthesequenceofcamel split tokens using an rnn with maxpooling ht tanh wm t ... nm m maxpooling wherewt rdis the embedding vector of token wt r2drepresentstheconcatenationoftwovectors wm r2d disthematrix of trainable parameters in the rnn tanhis the activation function of thernn.
amethod name isthus embedded asa d dimensional vectorm.
likewise the api sequence ais embedded into a vector ausing an rnn with maxpooling ht tanh wa t ... na a maxpooling whereat rdis the embedding vector of api at wais the matrix of trainable parameters in the rnn.
for the tokens as they have no strict order in the source code theyaresimplyembeddedviaamultilayerperceptron mlp i.e.
the conventional fully connected layer hi tanh w i i ... n where i rdrepresents the embedded representation of the token i w is the matrix of trainable parameters in the mlp hi i ... n are the embedding vectors of all individual tokens.
the individual vectors are also summarized to a single vector tvia maxpooling t maxpooling finally thevectorsofthethreeaspectsarefusedintoonevector through a fully connected layer c tanh wc where represents the concatenation of three vectors wcis the matrix of trainable parameters in the mlp.
the output vector c represents the final embedding of the code snippet.
.
.
descriptionembeddingnetwork.
thedescriptionembeddingnetwork denn embedsnaturallanguagedescriptionsinto vectors.
consider a description d w1 ... wndcomprising a sequenceof ndwords.dennembedsitintoavector dusinganrnn with maxpooling ht tanh wd t ... nd d maxpooling wherewt rdrepresents the embedded representation of the descriptionword wt wdisthematrixoftrainableparametersinthe rnn ht t ...ndare the hidden states of the rnn.
.
.
similaritymodule.
wehavedescribedthetransformations that map the code and description into vectors i.e.
the candd .
since we want the vectors of code and description to be jointly embedded we measure the similarity between the two vectors.
we use the cosine similarity for the measurement which is defined as cos c d ctd bardblc bardbl bardbld bardbl wherecanddare the vectors of code and a description respectively.
the higher the similarity the more related the code is to the description.
overall codenntakesa angbracketleftcode description angbracketrightpairasinputand predicts their cosine similarity cos c d .
.
model training nowwepresenthowtotrainthecodennmodeltoembedboth code and descriptions into a unified vector space.
the high level goalofthejointembeddingis ifacodesnippetandadescription have similar semantics their embedded vectors should be close to eachother.inotherwords givenanarbitrarycodesnippet cand anarbitrarydescription d wewantittopredictahighsimilarity ifdis a correct description of c and a little similarity otherwise.
at training time we construct each training instance as a triple angbracketleftc d d angbracketright for each code snippet cthere is a positive description authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deep code search icse may june gothenburg sweden code vectorsrecommen ded code search codebase nearest vectors selectionquery query vectorembedding0101010 commented code snippets training instances trainingoffline training code snippets java methods offline embeddingaspect extraction aspect extraction codenn model natural language descriptions ction code snippets java methods figure the overall workflow of deepcs d acorrect descriptionof c aswellas anegative description an incorrectdescriptionof c d randomlychosenfromthepoolofall d s.when trainedonthe setof angbracketleftc d d angbracketrighttriples the codenn predicts the cosine similarities of both angbracketleftc d angbracketrightand angbracketleftc d angbracketrightpairs and minimizes the ranking loss l summationdisplay.
c d d pmax cos c d cos c d where denotes the model parameters pdenotes the training dataset is a constant margin.
c d andd are the embedded vectors of c d andd respectively.
a small fixed value of .
is used in all the experiments.
intuitively the ranking loss encourages the cosine similarity between a code snippet and its correct description to go up and the cosine similarities between a code snippet and incorrect descriptions to go down.
deepcs deep learning based code search in this section we describe deepcs a code search tool based on the proposed codenn model.
deepcs recommends top k most relevant code snippets for a given natural language query.
figure showstheoverallarchitecture.itincludesthreemainphases offline training offline code embedding and online code search.
we begin by collecting a large scale corpus of code snippets i.e.
java methods with corresponding descriptions.
we extract subelements including method names tokens and api sequences fromthemethods.then weusethecorpustotrainthecodenn model theofflinetrainingphase .foragivencodebasefromwhich users would like to search for code snippets deepcs extracts code elementsforeachjavamethodinthesearchcodebase andcomputesacodevectorusingtheconnmoduleofthetrainedcodennmodel theofflineembeddingphase .finally whenauserqueryarrives deepcsfirstcomputesthevectorrepresentationofthequeryusing the denn module of the codenn model and then returns codesnippets whose vectors are close to the query vector the online code search phase .
in theory our approach could search for source code written in anyprogramminglanguages.inthispaper welimitourscopeto the java code.
the following sections describe the detailed steps of our approach.
.
collecting training corpus asdescribedinsection3 thecodennmodelrequiresalarge scale training corpus that contains code elements and the correspond ing descriptions i.e.
the angbracketleftmethod name api sequence tokens description angbracketrighttuples.figure6 showsanexcerptofthe trainingcorpus.
we buildthe trainingtuples usingjava methodsthat havedocumentationcomments1fromopen sourceprojectsongithub .
foreach javamethod weuse themethoddeclarationas thecode elementandthe firstsentenceofitsdocumentation commentasits natural language description.
according to the javadoc guidance2 thefirstsentenceisusuallyasummaryofamethod.topreparethedata wedownloadjavaprojectsfromgithubcreatedfromaugust to june .
to remove toy or experimental programs we excludeanyprojectswithoutastar.weselectonlythejavamethods that have documentation comments from the downloaded projects.
finally we obtain a corpus comprising commented java methods.
havingcollectedthecorpusofcommentedcodesnippets weextract the angbracketleftmethod name api sequence tokens description angbracketrighttuples as follows method name extraction foreachjavamethod weextractits name and parse the name into a sequence of tokens accordingto camel case .
for example the method name listfileswill be parsed into the tokens listandfiles.
apisequenceextraction weextractanapisequencefromeach java method using the same procedures as described in deepapi parsing the ast using the eclipse jdt compiler and traversing the ast.
the api sequences are produced as follows for each constructor invocation new c we produce c.new and append it to the api sequence.
for each method call o.m whereois an instance of class c we produce c.mand append it to the api sequence.
for a method call passed as a parameter we append themethod before the calling method.
for example o1.m1 o2 .m2 o3.m3 we produce a sequence c2.m2 c3.m3 c1.m1 whereciis the class of the instance oi.
for a sequence of statements s1 s2 ... sn we extract the api sequence aifromeachstatement si concatenatethemtothe api sequence a1 a2 ... an.
for conditional statements such as if s1 s2 else s3 we createasequencefromallpossiblebranches thatis a1 a2 a3 whereaiistheapisequenceextractedfromthestatement si.
for loop statements such as while s1 s2 we produce a sequence a1 a2 wherea1anda2areapisequencesextracted from the statement s1ands2 respectively.
token extraction to collect tokens from a java method we tokenizethemethodbody spliteachtokenaccordingtocamelcase andremovetheduplicatedtokens.wealsoremovestopwords suchastheandin andjavakeywordsastheyfrequentlyoccurinsource code and are not discriminative.description extraction to extract the documentation comment weusetheeclipsejdtcompiler toparsetheastfromajava method and extract the javadoc comment from the ast.
1a documentation comment in java starts with slash asterisk asterisk and ends with asterisk slash authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden xiaodong gu hongyu zhang and sunghun kim method name api sequence tokens description english 1file reader inputstream.read outputstream.write input output stream write copy a file from an inputstream 2open url.new url.openconnection url open conn open a url 3test exists file.new file.exists file create exists test file exists figure an excerpt of training tuples converts a date into a calendar.
param date the date to convert to a calendar return the created calendar throws nullpointerexception if null is passed in since .
public static calendar tocalendar finaldate date finalcalendar c calendar .getinstance c.settime date returnc method name to calendar api sequence calendar.getinstance calendar.settime tokens calendar get instance set time date description converts a date into a calendar.
figure an example of extracting code elements from a java method dateutils.tocalendar3 figure7showsanexampleofcodeelementsanddocumentation commentsextractedfromajavamethod dateutils .tocalendar3in theapache commons lang library.
.
training codenn model we use the large scale corpus described in the previous section to train the codenn model following the method described in section .
.
thedetailedimplementationofthecodennmodelisasfollows weusethebi directionallstm astate of the artsubclassof rnn for the rnn implementation.
all lstms have hidden unitsineachdirection.wesetthedimensionofwordembedding to100.thecodennhastwotypesofmlps theembeddingmlp forembeddingindividualtokensandthefusionmlptocombine the embeddings of different aspects.
we set the number of hidden units as for the embedding mlp and for the fusion mlp.
the codenn model is trained via the mini batch adam algorithm .wesetthebatchsize i.e.
thenumberofinstances per batch as .
for training the neural networks we limit the size of the vocabulary to words that are most frequently used in the training dataset.
we build our model on keras and theano two opensource deep learning frameworks.
we train our models on a server withonenvidiak40gpu.thetraininglasts 50hourswith500 epochs.
.
searching code snippets givenauser sfree textquery deepcsreturnstherelevantcode snippets through the trained codenn model.
it first computes the codevectorforeachcodesnippet i.e.
ajavamethod inthesearch codebase.then itselectsandreturnsthecodesnippetsthathave the top k nearest vectors to the query vector.
morespecially beforeasearchstarts deepcsembedsallcode snippets in the codebase into vectors using the conn module of commons lang3 time dateutils.javacodenn in an off line manner.
during the on line search when adeveloperentersanatural languagequery deepcsfirstembeds the query into a vector using the denn module of codenn.
then it estimates the cosine similarities between the query vector and all code vectors using equation .
finally the top k code snippets whose vectors are most similar to the query vector are returned as the search results.
kis set to in our experiments.
evaluation in this section we evaluate deepcs through experiments.
we also compare deepcs with the related code search approaches.
.
experimental setup .
.
searchcodebase.
tobetterevaluatedeepcs ourexperiments are performed over a search codebase which is different fromthetrainingcorpus.codesnippetsthatmatchauserqueryareretrievedfromthesearchcodebase.inpractice thesearchcodebase could be an organization s local codebase or any codebase created from open source projects.
toconstructthesearchcodebase wechoosethejavaprojects thathaveatleast20starsingithub.differentfromthetrainingcor pus theyareconsideredinisolationandcontainallcode including thosedonothavejavadoccomments .thereare9 950projectsin total.weselectall16 602methodsfromtheseprojects.foreach javamethod weextracta angbracketleftmethodname apisequence tokens angbracketright triple to generate its code vector.
.
.
querysubjects.
toselectcodesearchqueriesfortheevaluation we adopt a systematic procedure used in .
we build abenchmarkofqueriesfromthetop50votedjavaprogramming questionsinstackoverflow.toachieveso webrowsethelistof java taggedquestionsinstackoverflowandsortthemaccordingto thevotesthateachonereceives5.wemanuallycheckthesortedlist sequentially andaddquestionsthatsatisfythefollowingconditions to the benchmark the question is a concrete java programming task.
we exclude questionsaboutproblems knowledge configurations experience andquestionswhosedescriptionsarevagueandabstract.forexample failed to load the jni library what is the difference between stringbuilderandstringbuffer?
and whydoesjavahavetransient fields?.
the accepted answer to the question contains a javacode snippet.
the question is not a duplicate of the previous questions.
we filter out questions that are tagged as duplicated .
the full list of the selected queries can be found in table .
for each query two developers manually inspect the top results returnedbydeepcsandlabeltheirrelevancetothequery.then theydiscusstheinconsistentlabelsandrelabelthem.theprocedure repeats until a consensus is reached.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deep code search icse may june gothenburg sweden .
.
performance measure.
we use four common metrics to measuretheeffectivenessofcodesearch namely frank successrate k precision k and mean reciprocal rank mrr .
they arewidelyusedmetricsininformationretrievalandcodesearch literature .
thefrank alsoknownas besthitrank istherankofthe first hit result in the result list .
it is important as users scan the results from top to bottom.
a smaller frank implies lowerinspection effort for finding the desired result.
we use frank to assess the effectiveness of a single code search query.
thesuccessrate k alsoknownas successpercentageatk measuresthepercentageofqueriesforwhichmorethanonecorrect result could exist in the top kranked results .
in our evaluations it is calculated as follows successrate k q q summationdisplay.
q 1 frankq k whereqis a set of queries is a function which returns if the input is true and otherwise.
successrate k is important because abettercodesearchengineshouldallowdeveloperstodiscoverthe neededcodebyinspectingfewerreturnedresults.thehigherthe metric value the better the code search performance.
theprecision k measuresthepercentageofrelevantresultsinthetop kreturnedresultsforeachquery.inourevaluations it is calculated as follows precision k relevant results in the top k results k precision k isimportantbecausedevelopersofteninspectmultiple resultsof differentusages tolearn from .
abetter codesearch engineshouldallowdeveloperstoinspectlessnoisyresults.the higherthemetricvalues thebetterthecodesearchperformance.
we evaluate successrate k andprecision k whenk s value is and .
these values reflect the typical sizes of results that users would inspect .
themrr istheaverageofthereciprocalranksofresults ofa setofqueries q.the reciprocalrankofa queryisthe inverse oftherankofthefirsthitresult .mrriscalculatedasfollows mrr q q summationdisplay.
q frankq the higher the mrr value the better the code search performance.
.
.
comparison methods.
we compare the effectiveness of ourapproachwithcodehow andaconventionallucene based code search tool .
codehowisastate of the artcodesearchengineproposedrecently.
it is an information retrieval based code search tool that incorporates an extended boolean model and api matching.
it first retrieves relevant apis to a query by matching the query with theapidocumentation.then itsearchescodebyconsideringbothplaincodeandtherelatedapis.likedeepcs codehowalsoconsiders multiple aspects of source code such as method name and apis.
itcombinesmultipleaspectsusinganextendedbooleanmodel .
the facts that codehow also considers apis and is also built for large scale code search make it an ideal baseline for our experiments.
lucene is a popular conventional text search engine behind many existing code search tools such as sourcerer .
sourcerer combines lucene with code properties such as fqn full qualifiedtable benchmark queries and evaluation results nf not found within the top returned results lc lucene ch codehow dcs deepcs no.question idqueryfrank lcchdcs convert an inputstream to a string create arraylist from array nfnf2 iterate through a hashmap nf41 generating random integers in a specific range nf62 converting string to int in java nf101 initialization of an array in one line nf41 how can i test if an array contains a certain value lookup enum by string value 1nf10 breaking out of nested loops in java nfnfnf how to declare an array nfnf4 how to generate a random alpha numeric string nf11 what is the simplest way to print a java array 6nf1 sort a map by values nf13 fastestwaytodetermineifaninteger ssquarerootisaninteger nfnfnf how can i concatenate two arrays in java nf11 how do i create a java string from the contents of a file 8nf5 how can i convert a stack trace to a string how do i compare strings in java how to split a string in java how to create a file and write to a file in java 21nf how can i initialise a static map iterating through a collection avoiding concurrentmodificationexception when removing in loop332 how can i generate an md5 hash get current stack trace in java sort arraylist of custom objects by property how to round a number to n decimal places in java how can i pad an integers with zeros on the left nf31 how to create a generic array in java nfnf3 reading a plain text file in java 4nf7 a for loop to iterate over enum in java nfnfnf check if at least two out of three booleans are true nfnfnf how do i convert from int to string 21nf how to convert a char to a string in java how do i check if a file exists in java java string to date conversion 6nf1 convert inputstream to byte array in java how to check if a string is numeric in java 1nf2 how do i copy an object in java how do i time a method s execution in java nfnf2 how to read a large text file line by line using java how to make a new list in java how to append text to an existing file in java converting iso compliant string to date what is the best way to filter a java collection nf92 removing whitespace from strings in java nf31 how do i split a string with any whitespace chars as delimiters in java what is the best way to determine the size of an object nfnfnf how do i invoke a java method when given the method name as a string312 how do i get a platform dependent new line character 1nf10 how to convert a map to list in java 6nf1 name of entities and code popularity to retrieve the code snippets.
in our implementation of the lucene based code search tool we considertheheuristicoffqn.wedidnotincludethecodepopularityheuristic computedusingpagerank asitdoesnotsignificantly improve the code search performance .
we use the same experimental setting for codehow and the lucene based tool as used for evaluating deepcs.
.
results table shows the evaluation results of deepcs and related approachesforeachqueryinthebenchmark.thecolumn questionid shows the original id of the question in stack overflow where the querycomesfrom.thecolumn frankshowsthefrankresultof eachapproach.thesymbol nf standsfor notfound whichmeans thatnorelevantresulthasbeenreturnedwithinthetop kresults k .
theresultsshowthatdeepcsproducesgenerallymorerelevant results than lucene and codehow.
figure 8a shows the statistical authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden xiaodong gu hongyu zhang and sunghun kim table overall accuracy of deepcs and the related approaches tool r 1r 5r 10p 1p 5p 10mrr lucene .
.
.
.
.
.
.
codehow .
.
.
.
.
.
.
deepcs .
.
.
.
.
.
.
summary of frank for the three approaches.
the symbol indicates the average frank value achieved by each approach.
we conservatively treatthe frank as11 for queries thatfail to obtain relevant results within the top returned results.
we observe that deepcsachievesmorerelevantresultswithanaveragefrankof .
whichissmallerthantheaveragefrankachievedbycodehow .
andlucene .
.thefrankvaluesof deepcsconcentrateon the range from to while codehow and lucene produce larger variance and many less relevant results.
figure 8b 8c and 8d show the statistics of precision k for the three approaches when kis 5and10 respectively.weobservethatdeepcsachievesbetter overall precision values than codehow and the lucene based tool.
totestthestatisticalsignificance weapplythewilcoxonsignedrank test p .
for the comparison of frank and precision k betweendeepcsandthetworelatedapproachesforallthequeries.
we conservatively treat the frank as for queries that fail to obtain relevant results within the top returned results.
the pvalues for the comparisons of deepcs with lucene and codehow are all less than .
indicating the statistical significance of the improvement of deepcs over the related approaches.
table2 shows the overall performanceof thethree approaches measuredin termsof successrate k precision k andmrr.
the columnsr r 5andr 10show the results of successrate k whenkis and respectively.
the columns p p 5and p 10show the results of the average precision k over all queries whenkis and respectively.
the column mrrshows the mrrvaluesofthethreeapproaches.theresultsshowthatdeepcs returns more relevant code snippets than codehow and lucene.
for example the r 5value is0.
whichmeans that for76 of the queries the relevant code snippets can be found within thetop5returnresults.the p 5valueis0.
whichmeansthat50 ofthetop5resultsaredeemedaccurate.forthesuccessrate k the improvements to codehow are and respectively.
for the precision k the improvements to codehow are and respectively.
for the mrr the improvement to codehow is .
overall our approach improves the accuracy of related techniques on all metrics.
.
examples of code search results we now provide concrete examples of code search results that demonstrate the advantages of deepcs.
figure9aand9bshowtheresultsfortwoqueries queueanevent to be run on the thread andrun an event on a thread queue.
the two queries have the same set of keywords with different wordsequences.
the keyword queuein the two queries have different meanings and it could be difficult for an ir based approach to distinguish.still deepcscanunderstandthemeaningofthetwo queriesand returnrelevant snippets.apparently deepcshas the ability to recognize query semantics.
theabilityofqueryunderstandingenablesdeepcstoperform a more robust code search.
its search results are less affected by 2lucenecodehowdeepc6 a frank 0lucenecodehowdeepc6 b precision 0lucenecodehowdeepc6 c precision 0lucenecodehowdeepc6 d precision figure8 thestatisticalcomparisonoffrankandprecison kfor three code search approaches irrelevantornoisykeywords.forexample thequery getthecontent of an input stream as a string using a specified character encodingcontains keywords.
codehow returns many snippets that arerelated to less relevant keywords such as specifiedandcharacter.
deepcs ontheotherhand cansuccessfullyidentifytheimportance ofdifferentkeywordsandunderstandthekeypointofthequery figure .
anotheradvantageof deepcsrelatestoassociativesearch.that is it not onlyseeks snippets with matched keywords but alsorecommendsthosewithoutmatchedkeywordsbutaresemantically related.
this is important because it significantly increases the search scope especially when the codebase is small.
besides developers need snippets of multiple usages .
the associative search provides more options of code snippets for developers to learn from.figure11ashowsthefirstresultofthequery readanobject from an xml file.
as discussed in section traditional ir based approachesmayonlymatchsnippetsthatcontainkeywordssuch asxml objectandread.however asshowninthefigure deepcs successfullyrecognizesthequerysemanticandreturnsresultsof xml deserialize even the keywords do not exist in the result.
bycontrast codehow only returns snippets containing read object andxml narrowing down the search scope.
the example indicates thatdeepcssearchescodebyunderstandingthesemanticsinstead of just matching keywords.
similarly the query initialization of an arraylist in one line in table returns snippets containing new arraylist angbracketleft angbracketright although the snippet does not include the keyword initialization.
figure 11b shows another example of the associative search.
when searching play a song.
deepcs not only returns snippets with matching keywords but also recommends results with semantically related words such as audioandvoice.
discussions .
why does deepcs work?
wehaveidentifiedthreeadvantagesof deepcsthatmayexplain its effectiveness in code search authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
deep code search icse may june gothenburg sweden public boolean enqueue eventhandler handler event event synchronized monitor handlers handler events event tail if handlers .length tail tail monitor.notify return true a the third result of the query queue an event to be run on the thread public void run while !stop dynamicmodelevent evt while evt eventqueue .poll !
null for dynamicmodellistener l listeners .toarray newdynamicmodellistener l.dynamicmodelchanged evt b the first result of the query run an event on the thread queue figure examples showing the query understanding public static string tostringwithencoding inputstream inputstream string encoding if inputstream null throw new illegalargumen texception inputstream should not be null char buffer new char stringbuffer stringbuffer newstringbuffer bufferedreader bufferedreader newbufferedreader newinputstreamreader inputstream encoding buffer size intcharacter returnstringbuffer .tostring figure an example showing the search robustness the first result of the query get the content of an input stream as a string using a specified character encoding public static s s deserialize class c file xml try jaxbcontext context jaxbcontext .newinstance c unmarshaller unmarshaller context.createunmarshaller s deserialized s unmarshaller .unmarshal xml returndeserialized catch jaxbexception ex log.error error deserializing object from xml ex return null a the first result of the query read an object from an xml file public void playvoice intclearedlines throwsexception intaudiosavailable audiolibrary .get clearedlines .size intaudioindex rand.nextint audiosavailable audiolibrary .get clearedlines .get audioindex .play b the second result of the query play a song figure examples showing the associative search a unified representation of heterogeneous data source code and natural language queries are heterogeneous.
by jointly embedding source code and natural language query into the same vector representation their similarities can be measured more accurately.
betterqueryunderstandingthroughdeeplearning unliketraditional techniques deepcs learns queries and source coderepresentationswithdeeplearning.characteristicsofqueries suchaspublic static byte generaterandom256 byte randomseed1 byteutils .longtobytes system .nanotime byte randomseed2 newsecurerandom .generateseed key size bytes byte bh1 byteutils .concatenate randomse ed1 randomseed2 thread.sleep 100l byte randomseed3 uuid.randomuuid .tostring .getbytes byte randomseed4 byteutils .longtobytes system .nanotime byte bh2 byteutils .concatenate rando mseed3 randomseed4 returnsimplehash256 byteutils .concatenate bh1 bh2 figure an example showing the inaccurate results the first result of the query generate md5 semanticallyrelatedwordsandwordorders areconsideredinthese models .therefore itcanrecognizethesemanticsofqueryand codebetter.forexample itcandistinguishthequery queueanevent to be run on the thread from the query run an event on the event queue.
clusteringsnippetsbynaturallanguagesemantics anadvantage of our approach is that it embeds semantically similar code snippets into vectors that are close to each other.
semanticallysimilar code snippets are grouped according to their semantics.
therefore in addition to the exact matching snippets deepcs also recommends the semantically related ones.
.
limitation of deepcs despitetheadvantagessuchasassociativesearch deepcscould still return inaccurate results.
it sometimes ranks partially relevant resultshigherthantheexactmatchingones.figure12showsthe result forthe query generate md5 .
theexactly matchingresultis ranked in the result list while partially related results such as generate checksum are recommended before the exact results.
this is because deepcs ranks results by just considering their semantic vectors.
in future work more code features such as programming context couldbeconsideredinourmodeltofurtheradjustthe results.
.
threats to validity ourgoalistoimprovetheperformanceofcodesearchovergithub thusbothtrainingandsearchareperformedovergithubcorpus.
there is a threat of overlap between the training and search codebases.
to mitigate this threat in our experiments the training and search codebasesare constructedto be significantlydifferent.
the training codebase only contains code that has corresponding descriptions while the search codebase is considered in isolation and containsallcode includingthosedonothavedescriptions .webe lievethethreatofoverfittingforthisoverlapisnotsignificantasour training codebase considers a vast majority of code in github.
the most important goal of our experiments is to evaluate deepcs in a real worldcodesearch scenario.forthat weused50real queries collected from stack overflow to test the effectiveness of deepcs.
these queries are not descriptions comments of java methods and are not used for training.
inourexperiments therelevancyofreturnedresultsweremanually graded and could suffer from subjectivity bias.
to mitigate thisthreat i themanualanalysiswasperformedindependentlybytwo developers and ii the developers performed an open discussionto resolve conflict grades for the questions.
in the future we willfurthermitigatethisthreatbyinvitingmoredevelopersforthe grading.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden xiaodong gu hongyu zhang and sunghun kim in the grading of relevancy we consider only the top results.
queriesthatfailareidenticallyassignedwithanfrankof11and could be biased from the real relevancy of code snippets.
however we believe that the setting is reasonable.
in real world code search developers usually inspect the top k results and ignore the remaining.
that means it does not make much difference if a code snippet appears at rank or if k is .
likerelatedwork e.g.
weevaluatedeepcswithpopular stack overflow questions.
so questions may not be representative to all possible queries for code search engines.
to mitigate this threat i deepcs is not trained on so questions but on large scale githubcorpus.
ii weselectthemostfrequentlyaskedquestions whichmightbealsocommonlyaskedbydevelopersinothersearch engines.
in the future we will extend the scale and scope of test queries.
related work .
code search in code search a line of work has investigated marrying stateof the art information retrieval and natural language processing techniques .muchoftheexisting workfocusesonqueryexpansionandreformulation .for example hilletal.
reformulatedquerieswithnaturallanguage phrasal representations of method signatures.
haiduc et al.
proposed to reformulate queries based on machine learning.
their method trains a machine learning model that automatically recommendsareformulationstrategybasedonthequeryproperties.luetal.
proposed to extend a query with synonyms generated from wordnet.
there is also much work that takes into account code characteristics.forexample mcmillanetal.
proposedportfolio acodesearchenginethatcombineskeywordmatchingwithpager anktoreturnachainoffunctions.lvetal.
proposedcodehow a code search tool that incorporates an extended boolean model andapimatching.ponzanellietal.
proposedanapproachthat automatically retrievespertinent discussions fromstack overflow givenacontextintheide.recentlylietal.
proposedracs a code search framework forjavascript that considers relationships e.g.
sequencing condition and callback relationships among the invoked api methods.
as described in section deepcs differs from existing code searchtechniquesinthatitdoesnotrelyoninformationretrieval techniques.itmeasuresthesimilaritybetweencodesnippetsand userqueriesthroughjointembeddinganddeeplearning.thus it can better understand code and query semantics.
as the keyword based approaches are inefficient on recognizing semantics researchers have drawn increasing attention on semanticsbasedcodesearch .forexample reiss proposed the semantics based code search which uses user specifications to characterizetherequirementandusestransformationstoadaptthe searchingresults.however reiss sapproachdifferssignificantly fromdeepcs.itdoesnotconsiderthesemanticsofnaturallanguage queries.
furthermore it requires users to provide not only naturallanguagequeriesbutalsootherspecificationssuchasmethod declarations and test cases.
besides code search there have been many other information retrievaltasksinsoftwareengineering such as bug localization feature localization traceability links recovery and community question answering .
ye et al.
proposed to embed words into vector representations to bridge the lexical gap between source code and natural language for se related text retrieval tasks.
different from deepcs the vector representations learned by their method are at thelevelofindividualwordsandtokensinsteadofthewholequery sentences.
their method is based on a bag of words assumption and word sequences are not considered.
.
deep learning for source code recently researchers have investigated possible applications of deeplearningtechniquestosourcecode .
atypicaluseofdeeplearningiscodegeneration .forexample mouetal.
proposedtogeneratecodefromnaturallanguage user intentions using an rnn encoder decoder model.
their resultsshowthefeasibilityofapplyingdeeplearningtechniquesto codegenerationfromahighlyhomogeneousdataset simpleprogramming assignments .
gu et al.
applies deep learning for apilearning thatis generatingapiusagesequencesforagiven naturallanguagequery.theyalsoapplydeeplearningtomigrate apisbetweendifferentprogramminglanguages .deeplearning isalsoappliedtocodecompletion .forexample whiteet al.
appliedthernnlanguagemodeltosourcecodefilesand showed its effectiveness in predicting software tokens.
recently white et al.
also applied deep learning to code clone detection.
their framework for automatically links patterns mined at the lexical level with patterns mined at the syntactic level.
in our work we explore the application of deep learning to code search.
conclusion in this paper we propose a novel deep neural network named codennforcodesearch.insteadofmatchingtextsimilarity codenn learns a unified vector representation of both source code and natural language queries so that code snippets semantically related toaquerycanberetrievedaccordingtotheirvectors.asaproofof conceptapplication weimplementacodesearchtooldeepcs basedontheproposedcodennmodel6.ourexperimentalstudy hasshownthattheproposedapproachiseffectiveandoutperforms the related approaches.
in the future we will investigate more aspects of source code suchascontrolstructurestobetterrepresenthigh levelsemanticsofsourcecode.thedeepneuralnetworkwedesignedmayalsobenefit other software engineering problems such as bug localization.
acknowledgment theauthorswouldliketothankdongmeizhangatmicrosoftresearchasiaforhersupportforthisprojectandinsightfulcomments on the paper.