theimpact offailing flaky and high failuretestsonthe numberofcrashreports associated with firefox builds mdtajmilurrahman pbsc urbansolutions longueuil qc canada trahman pbsc.competerc.
rigby departmentof computerscienceand software engineering concordiauniversity montreal qc canada peter.rigby concordia.ca abstract testingisanintegralpartofreleaseengineeringandcontinuous integration.intheory afailedtestonabuildindicatesaproblem thatshouldbefixedandthebuildshouldnotbereleased.inpractice tests decay and developers often release builds ignoring failing tests.inthispaper westudyingthelinkbetweenbuildswithfailing testsandthenumberofcrashreportsonthefirefoxwebbrowser.
buildswithalltestspassinghaveamedianofonlytwocrashreports.
incontrast buildswithoneormorefailingtestsareassociatedwith a median of and crash reports for beta and production builds respectively.
we further investigate the impact of flaky tests which can both pass and fail on the same build and find that they have a median of and crash reports for beta and productionbuilds.finally buildingonpreviousresearchthathas shown that tests that have failed frequently in the past will fail frequently inthe future we findthat builds with highfailuretests haveamedianof585and780crashreportsforbetaandproduction builds.unlikeothertypesoftestfailures highfailuretests havea larger impact on production releases than on beta builds and they haveamedianof2.7timesmorecrashesthanbuildswithnormal testfailures.
we conclude that ignoringtestfailures is related toa dramatic increaseinthe number ofcrashesreportedbyusers.
ccs concepts software andits engineering software testing keywords software testing usercrash reports builds flakytests acmreference format mdtajmilurrahmanandpeterc.rigby.
.theimpactoffailing flaky and high failure tests on the number of crash reports associated with firefox builds.
in proceedings of the 26th acm joint european software engineeringconferenceandsymposiumonthefoundationsofsoftwareengineering esec fse november lake buena vista fl usa.
acm newyork ny usa 6pages.
thisworkwas completed while rahmanwas a phd student at concordia permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
esec fse november lake buenavista fl usa associationfor computing machinery.
acm isbn ... .
introduction software builds are tested to ensure that the functionality of the systemisnotbrokenbyachange.developerswritetestcaseswhen they are developing new features or fixing bugs.
in a rapid release model fixedandshortenedreleaseschedulesreducethetimefor investigationofthetestregressions .weexaminetheimpactof ignoredfailingtests flakytests and highfailuretests onthebuild qualityasmeasuredbythenumberofusercrashreportsassociated withabuild.
we organize our research around the following questions rq1 numberofcrashes howmanycrashesarethere forbuilds on dev beta and production?
firefoxstagesitsdevelopmentintothreechannels.thedevelopment channel contains the current work being done by developers.
the beta channel is used by early testers and users.
the production channel is released to end users.
the stability of the code and the number of users increase as we movefromthedevtoproductionchannel.thisfirstresearch question quantifies the numberof crashes on each channel.
this basic information is important to put the remaining research questions into context as low use channelswill likely have fewcrashesbut maynot be of high quality.
rq2 test failures how many crashes are associated with builds that contain testfailures?
this researchquestion quantifies theimpactoftest failures on crashes.
our goal is to understand if ignored test failures leadto an increaseinend usercrash reports.
rq3 flaky tests how many crashes are associated with builds that contain flaky tests?
flaky tests fail non deterministically .
for example a test may both pass and fail on the same build.
as a result developers cannot trust a flaky test to determine software quality.ourgoalistounderstandifignoredflakytestfailures leadto an increaseinthe number of browsercrashes.
rq4 historicalfailures dofailuresofteststhathave failed many times in the past lead to an increase in crashes?
researchershaveshowntheteststhathavefailedinthepast tendtocontinuetofailathighlevels .thesehighfailuretestsallowedresearchestore ordertestsbasedontheir historical likelihood to fail .
we consider tests that historically fail of the time to be highfailuretests .
we investigatewhetherfailuresofthesetestsleadtoanincrease inthe number ofcrash reports.
857esec fse november4 lake buena vista fl usa mdtajmilurrahmanandpeterc.rigby thepaperisorganizedasfollows section 2providesthebackgroundonthefirefox project s build processand crash collection.
section3describes the research methodology.
this section also describesthedatausedinthiscasestudy.section 4discussesthe results for each research question.
section 5positions our work in the literature on build systems and testing.
section 6concludes the paper.
firefoxrelease process firefox is a popular open source modern web browser and has been funded by the mozilla corporation since november .
firefox s release process involves three channels development beta 1and production .
code remains on each channel for six weeks before transitioning to the next channel.
each channel has a differentlevel ofstability purpose and number of developers and userswhoexerciseit.mcintosh et al.
foundthatthenumber of users per channel is 100k for development 1m for beta and 100m for the release channel.
to create a release a continuous integration tool buildbot is usedthrough bootstrap automationscriptstobuildnewlycommitted features into a new release .
the buildbot master creates the build logs and manages the overall process.
each build has a report thatcontainsthelogsforeachbuildandincludesbasicinformation about the build setup environment test steps the test verdict and the overallbuildresult.
when the browser closes unexpectedly a dialogue box allows users to submit crash reports .
each submitted crash report contains a crash dump including the crashing page address user s local environment andthe firefox buildid.
methodologyand data ourgoalistoinvestigatetheimpactofignoredfailingtestsandflaky testsonthenumberofreportedenduserbrowsercrashes.wefollow a straight forward method for our study.
after loading the data intoadatabasewenormalizetheteststatusintothreecategories pass fail and flaky .
we calculate which tests are historically highfailuretests .wethenlinkthebuildandcrashreportsbasedon thebuildid.weusertoprovidestatisticalanswerstoourresearch questions.figure 1illustrates our researchmethodology.
.
data we collect the historical build logs and crash reports for mozilla firefoxspanningfromdecember2010todecember2012.weparse thebuildlogsandstoretheextractedinformationinadatabase.the top portion of the log file contains the basic build summary including information about the builder slave process start time pass or failverdict build id andsource coderevision number commit hash .
the test information is contained at the end of the build log file and includes the test status test path and a short description of the test.
we use the test path to uniquely identify each test.
the pathisaurl that islinkedto the test steps.
wethenparseandextractallthecrashreportsintothedatabase.
each crash report contains a crash signature url withan unique id build id operating system and other information that may be 1betawas originaldivided intotwochannels auroraand betausefultodevelopers.onceweloadthedataintothedatabase we remove incomplete data rows that have missing information such as the crasheswithnobuildid.
after extracting the build logs and crash reports we have two data sets containing the crash information and the build history.
the attributes are listed in table .
by joining the two data sets ofbuildsandcrashesweextractthebuildsthatcouldbemapped withoneormorecrashes.foreachbuildweextracttheteststeps fromthebuildlogandstorethemseparately.welinkthembased onbuild idand we found a total of .8k unique build ids that have bothcrashandtestinformation.associatedwiththesebuildsare 729k crashes.
table attributes forthe buildand crashdata builddata attr.
crash data attr.
build id build id build uid url revision uuid url start time crash date test info signature test description test name .
teststatus mapping we found six statuses that firefox developers use to label their tests.
since there is no formal definition for these test labels we examinedthecodeofthetestscripts .forthispaper wemap the test statuses into three categories pass fail and flaky tests.
the mapping between firefox statuses and the categories is found in table2along with the number of test runs associated with each status.
in table2the status pass maps to a normal test pass.
the fail unexpected pass and unexpected fail arecategorizedunderthe fail category.incontrast the pass expected random and known fail expected random are seen as failing and passing non deterministically and we consider them to be flakytests.
.
identifying flakytests flaky tests non deterministically lead to a pass or fail verdict.
lou et al.identifiedflakytestsintheirstudy bysearchingforthekeywords intermittent and flak within the commit history.
they used commit logs for identifying flaky tests because they were mostly interested in flaky tests that are already fixed.
however we donotusea keywordsearchto identifyflaky tests.we usetheexistingfirefoxclassificationinthebuildlog.inthetestlogsthetests thataremarkedas random weincludetheminthe flaky category which means tests that are labelled with the statuses pass expected random and known fail expected random are consideredto be the flakytests see table .
858the impactoffailing flaky andhigh failure tests on ... esec fse november4 lake buena vista fl usa figure steps inour researchmethodology table mappingbetween firefox teststatusandcategoriesused inthispaper firefox test status normalized categories numberoftestverdicts pass pass 120m pass expected random flaky 265k known fail expected random flaky 10k fail fail 2m unexpected pass fail 1k unexpected fail fail .
identifying highfailuretests the distribution of failures is not normal.certain highfailuretests accountforalargeproportionoftotalfailures.previousworkshave used this historical property to re prioritize tests so that those that have failed frequently in the past will be run first .
we investigateifbuildswith highfailuretests haveanincreaseinthe number ofcrash reports.
we define a highfailuretest to be one that has failed on ormoretestruns.asanexamplefromthefirefoxdataset atest brokenutf ran131k timesand of the total runs resulted with a pass while times it resulted as a fail .
we consider thistesttobea highfailuretests .incontrast thetest hiddenpaging which ran 275k times passed of the time with only failures.
this test would not be considered a highfailuretest even though it has failedonpastbuilds.
results .
rq1 numberofcrashes howmanycrashesarethereforbuildsondev beta andproduction?
thedistributionsinfigure 2aretheper buildnumberofcrashes on development beta and production channels.
a box plot is also containedwithinthedistributionwiththebottomandtopofthebox showing the 25th and 75th percentiles respectively.
the vertical lineshowsthe median.
in totaltheirare2.8k buildsassociatedwithone or morecrash reportsand3.8kbuildsthatdonothaveanycrashreports.although the dev channel contains experimental code and is likely not as stableastheotherchannels weseefewercrashesonthischannel.inthe median case development builds are associated with 0crashes andwith3crashesat the75thpercentile.the betachannelbuilds are associated with a median of crashes and the production builds are associatedto 233crashes.
sincebuildsonthedevelopmentchannelarenottypicallyrun bymain streamendusers thenumberoftotalusersislesslikely explaining thelimitednumber ofcrash reportsforbuildsonthis channel.asaresult wedonotconsiderdevelopmentchannelin the remainderofthis paper.
thepurposeofthebetachannelistostabilizecode.earlyadopters usethesebuildsandprovidecrashandbugreportstohelpdevelopers to stabilize the code.
despite having fewer users than the productionchannel buildsonthischannelhavethehighest number ofcrashes.
codethatreachestheproductionchannelhaspassedthrough variousstabilizationandbugfixingstageswhichareintendedto reduce the number end user crashes.
although there are many crash reports given the expanded number of users the production code does appear to be the moststable.
there are a median of and crashes for builds on thebetaandproductionchannels.despitehavingmoreend usersontheproductionchannel therearefewercrasheslikely indicating that production code has high stability.
859esec fse november4 lake buena vista fl usa mdtajmilurrahmanandpeterc.rigby dev beta productioncrash reports per build log scale figure numberofcrashesforeachchannel .
rq2 testfailures howmanycrashesareassociatedwithbuildsthatcontaintestfailures?
our conjecture is that when developer ignore quality assurance indicatorstherewillbemorecrashesonthesebuilds.inthisresearch question weexaminethenumberofignoredtestfailuresforbuilds and relate them with the number ofcrashes.
we expect that more browser crashes will be associated with builds that have failing tests i.e.failing builds compared to those that do not have failing tests i.e.passingorcleanbuilds .
firefox runs a large number of tests during each build.
in the median case 3m tests are run on each build with a maximum of 11m.
table 2shows the number of passing and failing tests 120m and slightly over 2m respectively.
we exclude random non deterministic tests examining theminthe nextsection.
figures3and4contrasts the number of crash reports for builds withatleastonefailingtestwithbuildsthathaveonlypassingtests.
forthebetachannel wefoundthatbuildsthathavefailingtests have a median of crash reports.
in contrast passing test builds have amedian of2crash reports with5at the 75thpercentile and amaximum of25.
intheproductionchannelbuildswithfailingtestshaveamedian of crashes.
in contrast passing build are associated with a median of2crash reports with13 at the 75thpercentile.
inthemediancase buildsthathavefailingtestsareassociated with and crash reports for the beta and production channels.incontrast buildswithpassingtestshaveamedianof only two crash reports with some outlier pass production builds withmanycrashes.
.
rq3 flakytests howmanycrashesareassociatedwithbuildsthatcontainflakytests?
flaky tests fail in a non deterministic manner and potentially hidebugs.forexample ifaflakytestfailsfrequently developers tendto ignorethe failuresand couldmissthe realbugs.
we investigatewhetherignoringflakytestsisapotentialreasonforincreased browsercrashes.weusethefirefoxtestoutcomelabelsthatcontain random todeterminewhichtestsareflaky seethemethodology section 3for more details on the process of classification .
in table2 we see that 275k flaky test runs are labelled with the random verdict acrossallbuilds.
in the median case each production channel build that contains atleastoneflaky testfailure isassociated with234crashes.there isahighdegreeofvariationwith585crashesatthe75thpercentile and a maximum of 93k crash reports.
production builds with flaky tests are associated with almost the same number of crashes as thosewithregularfailingtests.
forthebetachannelinfigure 3weobservedasimilarpatterwith a514 .2k 21kcrashreportsforthemedian 75thpercentileand maximum respectively.
beta builds with flaky tests are associated with almost the same number of crashes as those with regular failingtests.
a wilcoxon test comparing the crashes for flaky builds shows a statistically significant difference between the beta and production channelswiththep value p .
.whilefutureworkisnecessary weconjecturethatdevelopersaremoreconservativewithreleasing productionbuildswithknownfailingandflakyteststhanwithbeta builds.
in the median case builds with failing flaky tests we associatedwith514and234crashreportsforbetaandproduction respectively.
.
rq4 historically highfailuretests do failures of tests that have failed many times in the past lead to an increase in crashes?
insoftwaresystems problemsclusterarounddefectivecodeand teststhat havefailed frequentlyin thepastare likelyto failin the future .toinvestigatethesetests weclassifytestthatfail in10 ormoreoftheirtotalrunsashistorically highfailuretests .
in the last distribution in figures 3and4 we show that builds that have a failing test that is classified as highfailuretest lead to lower qualitybuildsasmeasuredbyanincreaseinreportedcrasheson both the production andbetachannels.
the crash distribution for production builds that have one or morefailingteststhatarecategorizedas highfailuretests showa medium 75th percentile and maximum of .4k and 21k crash reports respectively.productionbuildswith highfailuretests are associated with .
times more crashes than builds with regular failingtests and390times higher thanthe passingbuilds.
forbetabuildsthecorrespondingvaluesare585 .5k and92k crashes reports for the median 75th percentile and maximum respectively.betachannelbuildswith highfailuretests areassociated 860the impactoffailing flaky andhigh failure tests on ... esec fse november4 lake buena vista fl usa pass fail flaky high failcrash reports per beta build log scale figure crashes pertype forbetabuilds pass fail flaky high failcrash reports per production build log scale figure crashes pertype forproductionbuilds with1.3timesmorecrashesthanbuildswithregularfailingtests and292times higher thanthe passingbuilds.
a wilcoxon test comparing highfailuretests on the beta and production channels shows a statistically significant difference betweenthe crashesonthesetwochannels withthep value p .
.unliketheflakyteststhatarelabeledbyfirefoxdevelopers highfailuretests arenotdifferentiatedfromothertypesoffailingtestsby the developers.
this is especially problematic on production builds astheseignoringthese highfailuretests leadtomanycrashreports.
firefoxdevelopersmightbenefitfromidentifyingandmonitoring this classification oftests.
in the median case builds that contain failing historically highfailuretests are associated with and crashes for betaandproductionrespectively.
highfailuretests areparticularly problematic with production builds where there are .
times more crash reports when compared to builds with normal test failures.
related work we divide the related work into testing and build maintenance and quality.weareunawareofanyworkthathasstudiedtheimpact oftestingonfield crash reports.
testing and flaky tests.
labuschagne et al.studied the cost of regression testing in practice .
they found that of the total testsuiteexecutionsfail.moreinterestingly ofthesefailures areflaky.ofthenon flakyfailures only74 werecausedbyabugin the system under test and the remaining were due to incorrect orobsoletetests.theyalsofoundthatinthefailedbuilds only0.
ofthetestcaseexecutionsfailedand64 offailedbuildscontaining more than one failedtest.
this study illustrates the importance of dealingwiththeflakyteststoimprovethequalityoftheregression testing.
our study addsto this knowledge by studying theimpact oftest failures onfield crashes.
recent works on flaky tests identified the root cause of the flakiness.forexample eloussiidentifiedflakytestsfromtestresults inherdoctoralresearchwheresheproposesthreeimprovements forthebasictechniquetoidentifyflakinessoftests.bymanually examining the flaky test eloussi divided the tests into three types non burstly burstly and state dependent burstly.
another study on flaky test by memon et al.
provides a detail post mortem of flaky tests that provides actionable information about avoiding detecting and fixing these types of non deterministic tests.
they inspectthe test code by analyzingthe code commits thatlikelyfix flaky tests.
they also identified the root causes of flakiness but not theimpactafterrelease.inourwork wemeasuredtheimpactof flakytestsoncrashes.futureworkcould usethecrashreportson flaky teststo validate thecauses identified by eloussi and memon.
generalbuildsystemstudies.
xinet al.performedanempirical studyonbugsinbuildsystems .theycategorizedbugsbasedon theirtypeandseveritiesandfoundthatthethirdhighestpercentage of bugs belong to the build configuration category.
they examined the association between the bugs and the build configurations while we associate browser crashes with the failing tests in a build.
mcintoshempiricallystudiedbuildsystemsinhisdissertation .
his publications include a build maintenance study of the effort spent on maintaining the build process and the ownership of thesebuildscripts .hefoundthatmaintainingthebuildsystem required significant effort with an overhead of on source code 861esec fse november4 lake buena vista fl usa mdtajmilurrahmanandpeterc.rigby developmentand44 ontestdevelopment.ourworkaddsevidence thatbuildmaintenanceisanimportantproblembyshowingthat ignoringbuildandtestproblemsleadtosubstantiallymorecrashes increasing developer effortandimpact onend users.
conclusion and futurework in this paper we investigate the association between builds and browsercrashesonbetaandtheproductionchannelsofthefirefox web browser.
we study the impact of ignoring failing flaky and highfailuretests onthe number ofcrashesfor abuild.
weobservethatignoringfailingtestsmakesthefirefoxbuilds much more crash prone compared to the builds that do not have any failing tests passing builds .
passing builds have a median of crashesforbothbetaandproduction.incontrast buildswithfailing tests have and crashes in the median case respectively.
flaky tests non deterministically pass or fail reducing developer confidence in the test.
in the median case builds with failing flaky testshad514and234crashesforthebetaandproductionchannels respectively.
previous works have shown that tests that have failed in the past are more likely to fail in the future .
we quantified highfailuretests as those thathave failed in of past runs.
in the median case builds with failing highfailuretests have 585and780crashesfor betaandproduction.
our results show that ignoring failing and flaky tests results in morecrashesinbetathanproduction.however ignoring highfailureteststests leads to more crashes on the production than beta channel.ignored highfailuretests wereassociatedwithamedian of crashes on the production channel.
this is the most crashes associated with any type of failing test and channel.
in the median case there are .
times more crashes per build than builds with normaltestfailures.afailing highfailuretests clearlywarrantsa detailedinvestigation before release byfirefox developers.
we hope that our work will inspire developers to understand the high risk of ignoring failing tests.
we hope that researchers willextendourworkbyexaminingboth therootcausesoffailing andflakytests andby contributingadvanced statisticalmodelsto enhance our understanding ofthe associatedrisks.