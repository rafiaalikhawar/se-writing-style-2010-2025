see discussions st ats and author pr ofiles f or this public ation at .researchgate.ne t public ation how can i use this method?
conf erence paper may .
ic se.
.
citations 150reads author s including laur a mor eno the univ ersity of t exas at dallas publica tions citations see profile massimiliano di p enta univ ersity of sannio publica tions citations see profile rocco oliv eto univ ersity of molise publica tions citations see profile andrian mar cus geor ge mason univ ersity publica tions citations see profile all c ontent f ollo wing this p age was uplo aded b y laur a mor eno on june .
the user has r equest ed enhanc ement of the do wnlo aded file.how can i use this method?
laura moreno gabriele bavota massimiliano di penta rocco oliveto and andrian marcus the university of texas at dallas usa free university of bozen bolzano italy university of sannio italy university of molise italy abstract code examples are small source code fragments whose purpose is to illustrate how a programming language construct an api or a specific function method works.
since code examples are not always available in the software documentation researchers have proposed techniques to automatically extract them from existing software or to mine them from developer discussions.
in this paper we propose muse method usage examples an approach for mining and ranking actual code examples that show how to use a specific method.
muse combines static slicing to simplify examples with clone detection to group similar examples and uses heuristics to select and rank the best examples in terms of reusability understandability and popularity.
muse has been empirically evaluated using examples mined from six libraries by performing three studies involving a total of developers to i evaluate the selection and ranking heuristics ii provide their perception on the usefulness of the selected examples and iii perform specific programming tasks using the muse examples.
the results indicate that muse selects and ranks examples close to how humans do most of the code examples are perceived as useful and they actually help when performing programming tasks.
i. i ntroduction developers frequently need to use methods they are not familiar with or they do not remember how to use.
to learn about such methods developers usually resort to reference manuals e.g.
javadoc questions and answers q a forums e.g.
stack overflow or other information sources.
often these resources provide little more than generic explanations of the method usage syntax or focus on the method s technical details.
in such cases developers could benefit from short code fragments i.e.
code examples presenting actual practical uses of the method.
the goal of our work is to automatically generate such code examples given a specific method.
our work adds to and complements existing approaches that have been developed to obtain code examples of one sort or another.
some of them focus on matching a query onto an existing code base with the aim of identifying generic code examples .
other approaches try to match user queries or developer s code context onto discussions containing examples in q a forums .
recently a few approaches have focused on providing abstract code examples for a given api either as a whole or focusing on specific methods .
abstract code examples are fragments of code showing usage patterns of a code element e.g.
an api or one of its methods .
even when there are several ways of using a code element an abstract example presents a synthesized and generic use of a code element.
conversely concrete code examples are working fragments of code showing actual existing usage scenarios of a code element.
our conjecture is that providing developers with severalconcrete method usages would augment abstract code examples and result in better understanding of the method usage.
for this reason we focus on the still open problem of mining relevant concrete code examples for a given method.
specifically we aim at answering the following question given a specific method needed to perform a task what are the necessary steps to use it?
for instance once a developer has understood the purpose of an api and has gained an idea of what the various methods do e.g.
through a reference manual she wants to know what are the typical invocation scenarios for a given method say copyinputstreamtofile .
to this aim she needs to find one or more examples that have the necessary steps to invoke this method such as invoking other methods of the api or manipulating the method s parameters.
such a method usage example see fig.
shows that in order to use the desired method line two arguments are required e.g.
zip.getinputstream entry andfile .
the inline comments lines provide information about each argument the former is an inputstream instance whose content will be copied and the latter is a non directory file instance where the content will be written to.
the comments also inform that neither of them should be null.
in addition the usage example shows how all elements involved in the invocation are obtained lines .
for example the inputstream argument is obtained from a zipfile object instantiated in line and given a zipentry object instantiated in line .
we propose muse method usage examples an approach that automatically finds extracts and documents concrete usage examples of a given method like the one in fig.
.
muse employs state of the art static analysis techniques which ensure its precision usability and set it apart from existing work.
it parses existing applications to collect method usages and computes a static backward slice for each usage statement to detect the sequence of relevant steps to invoke the method and prune out code that is less relevant.
since different applications might use the same method in a similar way muse identifies similar examples through clone detection.
the detected clones are used as a measure of file source file target zipfile zip new zipfile source enumeration ?
extends zipentry entries zip.entries while entries.hasmoreelements zipentry entry entries.nextelement file file new file target entry.getname zip.getinputstream entry the inputstream to copy bytes from must not be null file the non directory file to write bytes to possibly overwriting must not be null fileutils.
copyinputstreamtofile zip.getinputstream entry file fig.
.
example generated by muse .
ieee acm 37th ieee international conference on software engineering .
ieee ieee acm 37th ieee international conference on software engineering .
ieee ieee acm 37th ieee international conference on software engineering .
ieee ieee acm 37th ieee international conference on software engineering .
ieee popularity of the code examples which is in turn used to rank the groups of similar examples i.e.
code clones .
finally muse selects one representative example for each group based on its readability and estimated reusability effort i.e.
how much code from the client applications needs to be imported to reuse the example .
we implemented muse and evaluated it through three empirical studies.
the first study was aimed at assessing the selection and ranking heuristics used by muse and involved nine industrial developers.
the results indicate that muse selects and ranks examples much like human developers do.
in the second study ten code examples were evaluated by surveyed developers out of which were contributors of the projects for which the examples were produced and the remaining ones worked on projects relying on those libraries.
to the best of our knowledge this is the largest study evaluating the perceived usefulness of automatically generated code examples human judgments .
the results indicate that of the evaluated code examples are perceived as useful by the study participants.
we also performed an extrinsic evaluation aimed at investigating whether muse supports developers in their implementation tasks.
this is another notable contribution of this paper as we involved twelve industrial developers to assess the usefulness of automatically generated code examples during software development.
we considered a control group relying on the web to retrieve documentation and or examples of the methods to be used.
the experiment revealed that the developers using muse s examples achieved significantly more complete implementations.
all the material used to run our three studies as well as the raw results are publicly available in a comprehensive replication package all urls last verified on .
ii.
muse overview when a user needs method usage examples from a project release of interest pi she provides pi s source code and javadoc if available and a list of client projects using pi as library.
with no other input required muse proceeds as follows the clients downloader downloads the source code of the client projects.
the example extractor parsespi s source code to extract its public methods and analyzes each of the client projects looking for calls to such methods.
from each of these calls muse computes a backward slice each one representing a raw usage example.
the example evaluator ranks the raw usage examples and selects the most representative ones.
the example injector adds informative comments to each example.
also for each method covered by at least one code example the example injector creates an html page containing all the selected examples as ranked by the example evaluator and injects such a page in the official html javadoc documentation.a.
clients download for each client project of pito consider the user provides a link to a compressed file containing its source code.
there is no minimum number of clients required by muse .
clearly the more clients are given the higher the likelihood of identifying method usage examples.
the generation of the list of possible clients can be automated.
in our studies for example we used a script to automatically download the client projects using the library of interest i.e.
pi from its maven page.
a projectpiindexed in maven includes in its web page a list of client projects using it e.g.
see the maven page of apache commons io .
at we did not integrate such a script in muse since we are working on a more general web crawler able to collect client projects without the maven infrastructure.
starting from the compressed file the clients downloader automatically retrieves the source code of each client and builds it by using the eclipse java development tools jdt in conjunction with the maven eclipse plugin when appropriate i.e.
when the code of a client includes a pom.xml file .
note that building the client projects is required since the static slicer used by muse during the examples extraction can only be used on compilable code.
the downloaded clients that cannot be successfully built are discarded.
thus the output of this component is the set of built client projects.
b. example extraction selection and ranking the example extractor parses the source code of the set of built client projects and searches for invocations of pi s public methods.
then the example extractor uses the jdeodorant java static slicer to compute an intra procedural backward slice for each identified method invocation.
at this point every method in piwill have a number of slices each one representing a raw code example.
if none of the clients uses a given method then no examples will be extracted for it.
there are cases however when the example extractor finds many examples for a given method.
on one hand having a large number of usage examples for a method is desirable as different examples might show alternative usages of the method and some of them might be easier to adapt than others to the developer s task.
on the other hand providing developers with a large set of code examples which might contain several similar elements would result in information overload.
muse tries to avoid such a situation by identifying groups of similar examples and reporting just one representative example for each group.
also muse provides aranking of the selected examples indicating to the developer those that are more likely to be useful.
we describe in more details how the ranking and selection are performed.
ranking the example evaluator relies on type clone detection.
given a set of slices i.e.
raw examples e ex1 ex ... ex n extracted for a method mj pi theexample evaluator uses the simian clone detector to identify examples in ethat are type clones i.e.
identical code fragments except for variations in comments identifiers literals types and whitespace .
we chose to use simian since 881client a code example client b code example client a code example 3client a clones client a clonesclient b clone client a cloneclient b cloneclient c cloners 10rs 6rs fig.
.
rsassigned to three code examples all having three clones.
i it is able to detect type and type clones ii it is very efficient in terms of cpu and memory usage and iii it is freely available for research purposes .
while there are techniques able to detect also type i.e.
clones with added removed and or modified statements and type i.e.
clones with similar semantics but different implementation clones we are not interested in capturing them since they would likely represent different usage examples.
the output of simian consists of all pairs of code examples exk ex l e eidentified as type clones.
based on this information the example evaluator collects the clones of each code example exi eand uses them to compute a ranking score based on their popularity across the mined clients.
the conjecture inspired by previous work by keivanloo et al.
is that code examples found several times in the clients i.e.
having a high number of type clones represent good usage patterns of the analyzed method.
thus the more type clones a code example has the higher the ranking score assigned by muse to the code example is.
however the example evaluator makes a distinction between clones found in the same client and clones spread across different clients considering the latter as more important.
clones present within the same client could be method usages implemented by the same developer possibly as result of copied and pasted source code.
thus they are considered less indicative of popularity of a code example compared to clones present in different clients.
formalizing the ranking score rs for a code example exiis computed as follows rs exi clients2 clones clients where clones represents the total number of exi s clones identified across the clients the added unit represents exi itself and clients represents the number of different client projects in which exiand its clones were found.
to better understand how the ranking works fig.
shows the rs see the black rectangles computed for three code examples each one having three clones in the clients.
as it can be noticed the most important factor in the ranking is the clients .
based on their ranking score the code examples are listed in descending order to form a ranked list where each item consists of a group of code examples gi represented by a code example and followed by its clones.
referring back to the scenario depicted in fig.
example and its clones would be ranked in the first position example in the second position and example in the third position.
this ranking is used by muse to present the code examples of a method to the developer the higher the ranking is the higher the estimated usefulness of a code example is.
note that theranking depends on the choice of clients.
some potentially useful examples might be ranked low if the client applications rarely use such examples and vice versa a given example may not be particularly useful outside the set of chosen examples.
selection as mentioned before each item in the ranked list consists of a group giof similar code examples i.e.
type2 code clones .
we conjecture that showing to the developer a set of type clone examples would result in information overload.
hence muse shows the developer just one representative example for each group i.e.
the code example in gi easier to read and reuse .
muse uses the readability metric proposed by buse and weimer to evaluate the readability of a given example.
this metric combines a set of low level code features e.g.
identifiers length number of loops etc.
and has been shown to be effective in predicting developers readability judgments.
we used the authors implementation of such a metric which is available at given a source code fragment a code example in our case the readability metric takes values between lowest readability and maximum readability .
regarding the ease of reuse given a code example exi w e define and measure its reusability as follows reuse exi braceleftbigg javaobjectt ypes objectt ypesif objecttypes .
otherwise where javaobjecttypes is the number of object types used by exiand belonging to the java framework and objecttypes is the total number of different object types used byexi.
the reuse metric is in the range where indicates that all object types in the code example are custom objects low reusability and indicates that all object types in the code example belong to the java framework or that no object types are present in the code example high reusability .
the intuition behind this metric is that reusing a code example that makes use of custom object types requires importing those objects into the code under development which implies an extra task during reuse.
thus the higher the number of custom object types used by a code example is the lower its reuse value is hence it is more difficult to reuse the example.
since the readability and reuse metrics are defined in the range we linearly combine them to obtain the following selection score ss ss exi .
readability exi .
reuse exi indicating the overall score of the code example exi.
note that we are assigning the same importance to both metrics when computing the overall selection score as we do not prefer readability over easiness of reuse orvice versa .
clearly the weights can vary if needed.
muse selects from each group giof type clone examples in the ranked list the one having the highest ssvalue.
thus after the selection process each method in piis associated with a list of ranked and diverse code examples.
882c.
example documentation and injection the ranked list of representative code examples for each method is provided to the example injector .
this component extracts information from the javadoc documentation of each methodmj piand includes it in the code examples.
currently muse extracts the textual descriptions of mj s parameters identified with the tag param and includes them as inline comments to explain the arguments passed tomjinvocation right where it occurs.
the generated examples look like the one shown in fig.
at page for thecopyinputstreamtofile method of the apache commons io project.
finally every ranked list of representative code examples is presented as an html page which is injected into the official javadoc documentation if provided by the user .
an example of a javadoc page augmented by muse with the generated examples can be found at check the orange links .
iii.
s tudy i v alidation of ranking and selection study i is a survey with software developers with the goal of evaluating muse s example ranking and selection.
a. research questions and context selection this study aims at providing a human assessment of the ranking and selection heuristics.
thus we formulate the following research questions rq does muse s example ranking reflect developers judgment of code examples representing groups of clones?
rq does muse s example selection reflect developers judgment of similar code examples?
the study was conducted through an online survey.
we invited industrial developers from our professional networks.
each participant completed a pre questionnaire and then performed the evaluation of both the ranking and the selection of code examples for six randomly selected methods i.e.
one from each object system .
the pre questionnaire aims at assessing the participants experience and expertise.
it consists of the following questions f or how many years have you developed software?
any numeric value equal or higher than zero was accepted.
do you have any industrial experience as software developer?
if yes how long?
any numeric value equal or higher than zero was accepted.
how often do you make use of third party libraries in your software projects?
possible answers fall in a fourpoint likert scale never rarely i use third party libraries in less than of my projects occasionally i use third party libraries in more than but less than of my projects and frequently i use third party libraries in more than of my projects.
how often do you use code examples found on the internet to check how to use the api of a library?
possible answers adopt a four point likert scale similar to previous one never rarely i use code examples fortable i apache libraries used in the empirical studies .
library public clients methods with mean examples version methods examples per method commons io .
.
commons lang3 .
.
httpclient .
.
.
poi .
.
tika .
.
xerces2 j .
.
.
less than of the apis i use occasionally i use code examples for more than but less than of the apis i use and frequently i use code examples for more than of the apis i use.
the objects of this study are i the libraries for which muse mines examples and ii the clients from which examples are extracted.
in terms of libraries we considered six open source apache projects including two generic widelyadopted libraries commons io andcommons lang3 and four libraries that serve to more specific tasks httpclient poi tika and xerces2 j .
the characteristics of the projects are reported in table i including the number of clients muse used in each case the number of methods for which muse found examples and the mean number of examples found for each method.
b. study design and analysis method in order to evaluate the ranking and the selection of code examples we adopted the following generic procedure.
suppose that we have extracted the code examples e1 ... e and that they were grouped in the following clone groups g1 e1 e2 g2 e3 e4 andg3 e5 e6 e7 .
assume also that for each clone group muse selected based on their readability and reusability e1 e4 ande6 respectively as the representative examples.
finally assume that muse has ranked these examples as follows e6 e1 and e4.
we asked the participants to perform two tasks to assess both ranking and selection.
first we showed each participant the representative examples in our case e6 e1 ande4 ordered randomly.
for each example the participants were asked to assign a score using a likert scale not useful at all slightly useful useful and v ery useful .
the expectation here is that the higher the representative example is in the rank the greater the score provided by the developer to that example should be.
after that for one randomly selected clone group among g1 g2 andg3 sayg3 we showed the examples belonging to that group in this case e5 e6 ande7 in a random order.
just as before we asked the participants to evaluate each example using the likert scale.
in this case the expectation is that the score provided by the developer to the example selected as representative should be greater than the score provided to the other examples in the clone group.
to address rq we compute and report the spearman s rank correlation between the participants evaluation in likert scale and the example ranking according to equation for each evaluated example.
our expectation is that if our ranking heuristic is adequate then the evaluation should be negatively correlated with the ranking.
we report descriptive 883not useful at allslightly usefulusefulvery useful selected not selected fig.
.
evaluation of selected and not selected examples.
statistics showing the evaluation distribution for examples having different ranking.
to address rq we compare the rating of each example we selected e.g.
e5for clone group g3 against the rating provided to all other examples using the mann whitney test .
our expectation is that if the selection heuristic is adequate then the evaluation of the example selected by the heuristic according to equation should be significantly higher than of all the other examples.
the results are considered as statistically significant at .
.
c. analysis of the results nine participants completed the survey.
they declared on average .
years of development experience median and .
years of industrial experience median .
all claimed to frequently using third party libraries in their projects and eight of them reported also frequently using code examples found on the web to check how to use apis.
by analyzing the ranking results rq we obtained a statistically significant p value .
spearman rank correlation with .
.
as expected the correlation is negative although small.
the congruence between the participants assessment and muse s ranking is however confirmed by the ratings assigned by participants to the code examples having different rankings.
in particular the top code examples received a .
score on average followed by those in 2nd and 3rd position .
and to those in position 4th 5th and 6th .
.
note that none of the methods of this evaluation had more than six different code examples.
in summary we can answer rq 1by stating that the muse s ranking heuristic properly reflects the developers evaluation.
turning to rq the results of the mann whitney test did not show a statistically significant difference between the ratings assigned by participants to the examples selected as representative and those assigned to all other examples pvalue .
.
looking into the data we noticed that for some of the methods of this study the code examples showed to the participants were extremely similar thus pushing participants to assign flat ratings to the presented examples e.g.
assign useful to all clones .
however looking at the ratings distributions shown in fig.
for selected and not selected code examples it is clear that the selected examples were generally preferred by participants thus supporting our selection heuristic.
we can therefore answer rq 2by saying that the selected examples are considered more useful by the developers than the not selected ones.
iv .
s tudy ii p erceived usefulness of examples study ii is a survey with software developers with the goal of investigating their perceived usefulness of the codeexamples generated by muse .
the context consists of i objects i.e.
code examples extracted by muse for methods of six open source systems and ii participants i.e.
open source and professional developers providing their opinions on muse s code examples.
a. research question and context we aim at investigating whether developers consider the generated code examples useful for understanding the usage of methods.
thus we formulate the following research question rq are muse s usage examples considered useful by developers?
we answer this research question by asking open source and professional developers to assess through an online questionnaire their perceived usefulness of muse s usage examples.
the objects i.e.
code examples of our study are generated by muse for the same six systems adopted in study i. in particular we randomly selected ten methods from each system for which muse generated at least one usage example.
we asked the study participants to evaluate the usefulness of the top ranked examples i.e.
the best ones as selected by muse sexamples evaluator for each of the selected methods via an online survey described below.
as participants of this study we targeted i developers of the six object systems referred to as library developers and ii developers of open source projects using any of the object systems as libraries referred to as client developers .
the list oflibrary developers was extracted from the official apache committers page which reports the committers for each apache project.
while such a page only reports the committers id a sort of nickname their email can be obtained by adding apache.org to the committer s id.
in total we identified library developers .
the process for extracting the client developers emails required building an ad hoc crawler for visiting the maven page of each identified client project and looking for a developers table see e.g.
using such a crawler we identified client developers .
our choice of participants was driven by the goal of evaluating the usefulness of the method usage examples from the perspective of developers familiar with the libraries.
we consider the library developers and the client developers uniquely qualified to assess the difficulties encountered in understanding the usage of the methods hence also able to assess whether muse s examples can help in such a process.
each developer received an email with i instructions on how to participate in our study and ii a link to the survey for the specific object system for which the participant is a library orclient developer.
b. study design and analysis method the survey is composed of two parts.
the first one aims at gathering information about the developers background and contains the same questions as the study i pre questionnaire see section iii b with the addition of the following question have you contributed to pi s implementation?
a yes no 884g g g g g g g1.
.
.
.
developing industryhow often do you make use of... libraries code examplesneverrarelyoccasionallyfrequentlynumber of years in...system developers industrial experience library developers commons io commons lang http client poi tika xerces j total57 fig.
.
study ii participants data question aimed at verifying if the developer was a contributor i.e.
library developer t opi.
the second part of the survey consists of the evaluation of ten code examples covering ten different api methods of each object library.
for each code example to evaluate we showed to the participants the full path of the method that the code example refers to i.e.
package.class.method parameters and the best code example generated by muse as selected by the example evaluator .
then we asked the participants how useful is the code example in understanding the usage of the aforementioned method?
this question was answered by providing a score on a four point likert scale not useful at all slightly useful useful and v ery useful .
also the participants had the opportunity to justify their score in a free text form shown for each evaluated code example and they were encouraged to leave any additional comment about the evaluated code examples.
the survey of each object system was hosted on a web application that allows participants to complete the questionnaire in multiple rounds e.g.
answering a few questions on one day and the others later .
the developers had days to respond.
at the end of this period we collected complete questionnaires distributed among the object systems as reported in the top part of fig.
.
we analyze the results using box plots.
then we rely on developers comments to qualitatively discuss muse s strengths and weaknesses highlighted by the evaluation.
c. analysis of the results we first analyze the developers background summarized in fig.
.
the developers involved in our study have on average years of developing experience median .
the developers with industrial experience spent on average years in industry median see box plots in the left bottom corner of fig.
.
almost all developers out of frequently use third party libraries in their projects while frequently use code examples to check how to use the api of a library and just four developers rarely or never use them see box plots in the right bottom corner of fig.
.
fifteen of developers have contributed to the development of the object systems.not useful at allslightly usefulusefulvery useful 0commons io g g not useful at allslightly usefulusefulvery useful 0commons lang gg g g gg g g not useful at allslightly usefulusefulvery useful 0http client g gg not useful at allslightly usefulusefulvery useful 0poi gg gg g g not useful at allslightly usefulusefulvery useful 0tika g g gg g g not useful at allslightly usefulusefulvery useful 0xerces fig.
.
study ii answers to the question how useful is the code example in understanding the usage of the aforementioned method?
to answer rq fig.
aggregates the answers provided by developers when evaluating the usefulness of the ten code examples numbered with ids from to showed for each system.
a global view across the assessed examples shows that of them examples have been appreciated by developers as very useful or useful median scores .
only of the code examples generated by muse and evaluated by participants were found slightly useful median score .
none of the examples were considered as not useful at all median .
also the perceived usefulness of muse s code examples seems to be independent from i the system on which it is applied and ii the number of client projects parsed to extract such examples see table i for details 885about the number of analyzed clients for each system .
we also analyzed the participants rationale justifying their scores.
positive scores were seldom justified by developers and mostly included exclamations like nice!
this is a very good one!
or excellent example!
we also received positive feedback about the feature that generates documentation and inserts it in the code examples e.g.
you have an immediate example of how to use the api and a description of the parameters this way you don t have to endlessly switch between code and docs.
the most frequent reason for negative scores was the unnecessary complexity of some of the examples.
this happens when the examples generated by muse include code statements that even if needed to invoke the method and thus included in the slicing step do not directly relate to the method usage.
an instance of such a case is the third code example in the commons io survey which received several low scores median slightly useful .
this code example refers to the copydirectory file file filefilter method in the fileutils class.
such a method copies the contents of an existing directory into a target directory by applying an optional filtering e.g.
only copying files having a specific extension .
the generated code example consists of lines of code out of which the first are aimed at instantiating objects used later to create the two file s representing the existing and the target directories.
thus just the last six lines of the example were strictly related to the method invocation.
this aspect has been highlighted in several developers comments e.g.
the example should contain only the lines after otherwise the code will seem too complex event if it is not like in this case .
developers also pointed out that they would have expected a description of the returned value in some of the examples.
for instance several developers pointed out this issue for the code example related to the getlevenshteindistance charsequence charsequence method of the commons lang s stringutils class e.g.
it would be fantastic to also report the output of the api invocation when possible .
this feature is not currently implemented by muse but we plan to integrate it in the future.
another situation highlighted in some comments was the lack of alternative usage scenarios for a given method.
our design choice for this study was to limit the time needed to complete the survey as much as possible thus favoring a high response rate.
for this reason we asked developers to evaluate only one of the code examples generated by muse the top ranked one for each method.
therefore while we did not show alternative usage scenarios it is possible that by presenting muse s entire ranked list of examples for each method this issue would have not being reported.
the open comments at the end of the questionnaire confirmed muse s usefulness.
there were enthusiastic comments such as the purpose of this work seems to be appealing overall good sometimes too complex but if they are automatically generated is interesting or i would say ofthem is useful.
some contain statements that are not needed to use the method .
there were also constructive suggestions helpful to plan our future work e.g.
the code examples that are most useful at the method level are unit tests.
they express the behavior and usage of the code through example inputs and outputs the comments with argument descriptions are very useful however api examples need to be focused on use of the api call and not incorporate extra code .
in conclusion we answer rq 3stating that of muse s code examples have been considered either useful or very useful by developers with consistent results achieved across the six object systems.
v. s tudy iii e xtrinsic ev aluation study iii is a controlled experiment with the goal of evaluating how useful are muse s code examples to developers during a programming task.
the quality focus is the completeness of the task a developer can perform in a limited time frame e.g.
because of a hard deadline.
the context consists of industrial developers as participants and of development tasks performed by using apis of specific libraries as objects .
a. research questions and context we aim at addressing the following research question rq do muse s examples help developers to complete their programming tasks?
to answer rq 4we asked the twelve developers to perform two programming tasks involving the use of specific libraries one with the availability of muse s code examples and another one without the examples.
the two tasks are t1 create a java program that given the url of a pdf document available on the internet downloads the pdf and prints in the console all its metadata and textual content.
the task consisted of three subtasks i download the pdf using the httpclient library ii extract the pdf metadata using the tika library and iii extract the pdf textual content using the tika library.
t2 for this task we created a directory containing in different subdirectories files having different extensions including .csv storing portions of the openflights airports database1.
then we asked participants to create a java program that given the local path of the directory described above i creates an output directory dand an output .csv filef ii retrieves all .csv files related to the airports database by using the commons io library iii checks the completeness of the data reported in each .csv file by verifying that its rows each one containing data about one airport contain the correct number of columns by using the commons lang3 library iv copies the correct files into dby using the commons io library and v prints in fthe first rows of each correct file by using the commons lang library.
each task involved at least two libraries for which muse generated code examples as described in the previous studies.
886note that while part of the implementation of both tasks is possible using alternate libraries or none we required the participants to use specific libraries in each step.
otherwise it would not have been possible to observe the usefulness of the muse s examples when using the libraries.
in order to assess the quality of the implementations and answer rq we measure the task completeness as the dependent variable in our experiment as done in previous similar studies see e.g.
.
for a given task tiwith subtasks ti j w e define the completeness of tias the sum of the completeness of its subtasks ti j. the completeness score of the subtasks is proportional to its difficulty and complexity t1 t1 t1 t2 t2 t2 t2 t2 .
if a subtask ti jis implemented correctly then it receives its maximum score otherwise it receives zero .
hence the completeness score for a task ti ranges from to .
since this is difficult to automatically evaluate we asked two independent developers to serve as evaluators and measure the completeness by performing code reviews on each task implemented by each participant.
the evaluators did not know the goal of the study nor which tasks were performed with or without muse s code examples.
the evaluators compared the tasks independently and conducted a discussion when their scores diverged.
this happened on out of the evaluated tasks i.e.
tasks for each of the participants and in each case the evaluators reached an agreement quickly by performing an additional code inspection.
the main factor and independent variable of this study is the availability of the code examples generated by muse .
specifically such a factor has two values i.e.
code examples available ce or not nce .
participants were allowed to use any resource they want to complete the tasks including material available on the internet note that in both treatments we provided the official javadoc documentation of the libraries to use augmented in ce with the code examples generated by muse .
this was done to simulate a real development context.
also note that while other techniques have been proposed in the literature to generated code examples e.g.
we chose to use as control group i.e.
nce the current state of the practice.
indeed the wide availability of q a websites e.g.
stack overflow forums and dedicated websites from which developers can grab code examples makes the internet the most natural benchmark for our approach.
factors that could influence the results are i the possible different difficulty of the two tasks ii the different level of fatigue in the two sessions iii the participants development experience and iv their knowledge of the libraries.
b. study design and analysis method the study design shown in table ii is a classical paired design for experiments with one factor and two treatments.
the design is conceived in such a way that i each participant worked with both ce and nce ii each participant performed different tasks t1 and t2 across the two sessions iii different participants worked with ce and nce in different ordering as well as on the two different tasks t1table ii study iii d esign .
session group a group b group c group d t1 nce t1 ce t2 nce t2 ce t2 ce t2 nce t1 ce t1 nce and t2.
overall this means partitioning participants into four groups receiving different treatments in the two lab sessions.
first we conducted a pre experiment briefing where we illustrated in detail the experiment procedure.
we made sure not to reveal the study research questions.
after that participants filled in a pre questionnaire including all questions present in the pre questionnaire of study i see section iii a plus a question requiring a self assessment of their knowledge of the four libraries used in the two tasks on a four point likert scale going from no knowledge to high knowledge .
then the participants had to perform the study in two sessions of minutes each.
in other words participants had a maximum of minutes to complete each of the required tasks.
each participant received the instructions for the task to perform in the first session.
after minutes each participant provided the implemented code for the required task.
a minutes break was given before starting the second session to avoid fatigue effects.
during the break participants did not have the chance to exchange information among them.
after the break each participant received the instructions for the second task and minutes later they provided the implemented code for the required task.
finally once the study was completed we asked participants to also evaluate the usefulness of the code examples by providing and justifying a score on a four point likert scale not useful at all slightly useful useful and v ery useful .
as for the analysis method we present box plots of the completeness achieved by participants with the two treatments i.e.
ce and nce .
also we statistically compare the two distributions of completeness by using the mann whitney test .
the results are intended as statistically significant at .
.
we also estimate the magnitude of the difference between the two different distributions by using the cliff s delta ord a non parametric effect size measure for ordinal data.
we followed the guidelines by grissom and kim to interpret the effect size values small for d .
positive as well as negative values medium for .
d .
and large for d .
.
finally to check the influence of the various co factors i.e.
task lab session development experience industrial experience and knowledge of the used libraries from a statistical standpoint and their interaction with the main factor treatment we use the permutation test a non parametric alternative to analysis of variance anov a .
c. analysis of the results the pre questionnaires provided the following information about the background of the participants involved in our study on average they have five years of development experience median .
out of which two have been spent in industry median .
the majority of developers often orvery 887gggg gg overall nce0 overall cet1 ncet1 cet2 ncet2 ce fig.
.
rq4 completeness achieved by participants.
often use third party libraries in their projects and very often use code examples found on the internet to understand how to use apis.
their knowledge of the four object libraries required for the two tasks is generally limited i.e.
median .
forcommons io median for commons lang median forhttpclient and median for tika .
fig.
reports the box plots of completeness achieved by participants with ce and without nce muse s code examples.
results are shown when considering both tasks as a single dataset overall and separately.
on average participants achieved completeness in nce against the in ce.
such a difference is statistically significant pvalue .
with a medium effect size d .
.
as it can be noticed from fig.
the difference in completeness in favor of the ce treatment is present in both tasks.
the difference is in task t i.e.
vs68 while it grows up to in task t i.e.
vs38 .
only the participants completeness in task t 2has a significant difference between the two treatments p value .
with a large effect size d .
.
such a difference can be justified by considering that task t 2required to use more methods from the object libraries than task t .
when asking in the post questionnaire about the usefulness of the code examples nine participants rated them as useful two as slightly useful and one as not useful at all .
the latter justified his answer by explaining that while the examples can help in understanding the method usage i still prefer surfing on the internet for more rich discussions on how to use methods.
finally the statistical analysis of the co factors influence has highlighted that only the knowledge of the used libraries has a significant effect on the completeness p value .
although it does not interact with the main factor.
in other words people with higher knowledge of the used libraries perform better independently of the availability of the code examples generated by muse .
summarizing our experiment allows us to answer rq 4by stating that the use of muse s code examples increased the developers work quality i.e.
completeness by up to .
vi.
t hreats to validity threats to construct validity concern the measurements performed to address our research questions.
both study i and ii rely on a subjective assessment through a likert scale .
study iii aims at overcoming this limitation by assessing the examples usefulness in an actual development task.
concerning study factors that can influence our results internal validity we assessed and reported the participants background in all our studies.
furthermore in study iii we also analyzed whether this and other confounding factors e.g.
task ordering could influence the results.
last but not least in study i and ii we randomized the ordering in which the examples were shown to the participants to mitigate any sort of learning or fatigue effect.
as detailed in the analysis method of the three studies we used appropriate statistical tests correlation and effect size measures as needed to support our results and to mitigate threats related to conclusion validity .
finally it is possible that the selected examples used in the studies are not fully representative of the readability reusability in study i and usefulness of the examples generated by muse in study ii and study iii .
this may limit the generality of our results external validity .
however we tried to involve at least in study i and ii a relatively large set of examples from six different projects to mitigate this problem.
study iii is smaller but still it involved examples generated for four different projects and as explained above it assesses the examples usefulness better than just a questionnaire.
vii.
r elated work api usage examples are a common output of code search approaches based on text matching.
chatterjee et al.
and keivanloo et al.
for instance use textual similarity to return a ranked list of abstract examples relevant to a natural language nl query formulated by the user and expressing her task at hand.
similar to muse chatterjee et al.
s approach sniff documents clusters and ranks the extracted code snippets.
the clustering and ranking criteria however are based on the textual content and the frequency of the extracted patterns within the data set respectively.
keivanloo et al.
s approach combines textual similarity and clone detection techniques to find relevant code examples and ranks them according to i their textual similarity to the query and ii the completeness and popularity of their encoded patterns.
in a similar token but generating a ranked list of concrete api usage examples the structural semantic indexing ssi proposed by bajracharya et al.
combines heuristics based on structural and textual aspects of the code based on the assumption that code entities containing similar api usages are also similar from a functional point of view.
other tools such as strathcona and prompter automatically generate queries from the developer s code context.
in strathcona the code snippets relevant to such queries are identified through six structural heuristics based on inheritance links method calls and used types.
the resulting examples are ranked according to their frequency in the final set.
prompter matches the generated query with stack overflow entries to automatically push discussions relevant to the developers task at hand.
unlike muse task based code search techniques rely heavily on textual analysis to extract cluster and rank code elements.
our approach builds code snippets based on static program analysis specifically code slicing .
moreover task based api usage examples are not attached to a single api they show how different apis can be used for the same 888goal i.e.
the task at hand .
muse s examples are meant to help developers to reuse a particular method once they know it might help with the task at hand.
in other words we envision muse to be used by developers for getting more precise information about the methods they want to use after using one of the tools mentioned above.
more related to our approach is the research on extraction of code examples showing the usage of specific apis.
mapo proposed by xie and pei and extended by zhong et al.
sets the foundations on mining abstract usage examples of a given api method.
mapo analyzes code snippets retrieved by code search engines to extract all the call sequences involving the desired api method.
a subset of sequences covering all method calls is identified and then clustered into similar usage scenarios according to heuristics based on method names class names and called api methods composing each sequence.
for each cluster mapo identifies usage patterns based on frequent call sequences and finally it ranks the patterns based on their similarity with the developer s code context.
muse builds on the same idea of extracting examples for a specific method but it differs from mapo in several ways i to find the usage of a method muse considers its complete signature whereas mapo uses only the method name which can lead to mismatch between methods with the same name but different signatures i.e.
polymorphic methods ii muse uses slicing for locating the statements associated to the analyzed usage as opposed to control flow based heuristics iii for clustering similar examples muse applies widely used clone detection techniques while mapo uses similarity heuristics iv muse s ranking is independent of the developers context and considers the readability and easiness of reuse of the examples instead of the similarity measures used by mapo which depend on the development context and finally v muse s output is a list of concrete usage examples patterns as opposed to abstract examples that do not explicitly offer control flow and instantiation steps.
up miner is a variation of mapo that removes the redundancy in the resulting example list.
to this end upminer clusters the extracted method sequences based on ngrams and discovers a pattern for each cluster by applying a frequent sequence mining algorithm.
as these patterns might be similar up miner executes another clustering round on them.
the resulting patterns are ranked according to their frequency and presented as probabilistic graphs.
similar to mapo but different from muse up miner produces abstract examples.
moreover muse makes use of clone detection techniques to cluster similar code examples instead of the probabilistic language model used by up miner.
other approaches focus on generating code examples for an entire api or a set of apis.
acharya et al.
detect patterns on the conjunct usage of different apis by mining frequent partial orders from common api usage scenarios.
given a set of apis inter procedural static traces are produced to construct and cluster partial orders from api sequential patterns which are in turn mined to extract ordering rules between apis.
differently from muse this approach considers how a set ofapis is usually combined across different systems.
more recently buse and weimer proposed to generate documented abstract api usages by extracting and synthesizing code examples of a particular api data type.
their approach mines examples by identifying and ordering i code instantiations of the given data type and ii the statements relevant to those instantiations as defined by previously extracted path predicates computed from intra procedural static traces.
the examples are then clustered based on their statement ordering and data type usage.
for each cluster an abstract example i.e.
a usage pattern is formed by merging its code examples and finally documented according to predefined heuristics that depend on the kind of statement and most frequent names in the mined code.
as with the code search approaches muse could be used to complement these abstract api usage patterns with specifics on the methods of interest.
also related to our work is the study conducted by ying and robillard aimed at investigating how humans produce code examples summarizing them from existing code.
one of their findings is that developers rarely copy the code verbatim rather they try to abstract the relevant parts.
muse s main innovation is the use of code slicing which aims at removing unnecessary code from the examples emulating the developers behavior highlighted by ying and robillard s study.
viii.
c onclusion and future work muse extracts and documents code examples from the code of applications that make use of the specific method.
among related approaches muse is novel as it extracts concrete method usage examples as opposed to abstract usage patterns by using static code slicing and documents them with comments that help to understand the method s parameters.
muse has been empirically evaluated through three different empirical studies involving in total opensource and professional developers and conducted on six java libraries.
the first study a survey showed that muse ranks and selects code examples close to how developers do.
the second also a survey revealed that of muse s examples are perceived as useful or very useful by developers.
the third study a controlled experiment showed that developers improve the quality of their implementations when using muse s examples.
future work will focus on improving i the examples documentation e.g.
by commenting the output of the method invocation and ii the filtering of less important statements.