how bad can a bug get?
an empirical analysis of software failures in the openstack cloud computing platform domenico cotroneo federico ii university of naples italy cotroneo unina.itluigi de simone federico ii university of naples italy luigi.desimone unina.it pietro liguori federico ii university of naples italy pietro.liguori unina.itroberto natella federico ii university of naples italy roberto.natella unina.itnematollah bidokhti futurewei technologies inc. usa nbidokht futurewei.com abstract cloud management systems provide abstractions and apis for programmatically configuring cloud infrastructures.
unfortunately residual software bugs in these systems can potentially lead to highseverity failures such as prolonged outages and data losses.
in this paper we investigate the impact of failures in the context widespread openstack cloud management system by performing fault injection and by analyzing the impact of the resulting failures in terms of fail stop behavior failure detection through logging and failure propagation across components.
the analysis points out that most of the failures are not timely detected and notified moreover many of these failures can silently propagate over time and through components of the cloud management system which call for more thorough run time checks and fault containment.
ccs concepts software and its engineering software fault tolerance softwaretestinganddebugging softwarereliability computer systems organization cloud computing .
keywords bug analysis fault injection openstack acm reference format d. cotroneo l. de simone p. liguori r. natella n. bidokhti.
.
how bad can a bug get?
an empirical analysis of software failures in the openstack cloud computing platform.
in proceedings of the 27th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august tallinn estonia.
acm new york ny usa pages.
introduction cloud management systems such as openstack are a fundamental element of cloud computing infrastructures.
they provide abstractions and apis for programmatically creating destroying and snapshotting virtual machine instances attaching and detaching permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august tallinn estonia association for computing machinery.
acm isbn .
.
.
.
and ip addresses configuring security network topology and load balancing settings and many other services to cloud infrastructure consumers.
it is very difficult to avoid software bugs when implementing such a rich set of services at the time of writing the openstack project codebase consists of more than million lines of code loc which implies thousands of residual software bugs even under the most optimistic assumptions on the bugs per loc density .
as a result of these bugs many high severity failures have been occurring in cloud infrastructures of popular providers causing outages of several hours and the unrecoverable loss of user data .
in order to prevent severe failures software developers invest efforts in mitigating the consequences of residual bugs.
examples are defensive programming practices such as assertion checking and logging to timely detect an incorrect state of the system and for providing to system operators useful information for quick troubleshooting .
another important approach to mitigate failures is to implement fault containment strategies.
examples are i interrupting a service as soon as a failure occurs i.e.
a fail stop behavior by turning high severity failures such as data losses into lower severity api exceptions that can be gracefully be handled ii notifying the cloud management system and operators about the failures through error logs so that they can diagnose issues and undertake recovery actions such as restoring a previous state checkpoint or backup iii separating system components across different domains to prevent cascading failures across components .
in this paper we aim to empirically analyze the impact of highseverity failures in the context of a large scale industry applied case study to pave the way for failure mitigation strategies in cloud management systems.
in particular we analyze the openstack project which is the basis for many commercial cloud management products and is widespread among public cloud providers and private users .
moreover openstack is a representative real world large software system which includes several sub systems for managing instances nova volumes cinder virtual networks neutron etc.
and orchestrates them to deliver rich cloud computing services.
we adopt software fault injection to accelerate the occurrence of failures caused by software bugs our approach deliberately injects bugs in one of the system components and analyzes the reaction of the cloud system in terms of fail stop behavior failure reporting through error logs and failure propagation across components.
we based fault injection on information on software bugsarxiv .04055v1 jul 2019esec fse august tallinn estonia d. cotroneo l. de simone p. liguori r. natella n. bidokhti reported by openstack developers and users in order to characterize frequent bug patterns occurring in this project.
then we performed a large fault injection campaign on the three major subsystems of openstack i.e.
nova cinder and neutron for a total of experiments.
the analysis of fault injections pointed out the impact of the injected bugs on the end users e.g.
service unavailability and resource inconsistencies and on the ability of the system to recover and to report about the failure e.g.
the contents of log files and the error notifications raised by the openstack service api .
results of the experimental campaign revealed the following findings in the majority of the experiments .
openstack failures were not mitigated by a fail stop behavior leaving resources in an inconsistent state e.g.
instances were not active volumes were not attached unbeknownst to the user in the .
of these failures the problem was never notified to the user through exceptions the others were only notified after a long delay longer than minutes on average .
this behavior threatens data integrity during the period between the occurrence of the failure and its notification if any and hinders failure recovery actions.
in a small fraction of the experiments .
there was no indication of the failure in the logs.
these cases represent a high risk for system operators since they lack clues for understanding the failure and restoring the availability of services and resources in most of the failures .
the injected bugs propagated across several openstack components.
indeed .
of these failures were notified by a different component from the injected one.
moreover there were relevant cases of failures that caused subtle residual effects on openstack .
even after removing the injected bug from openstack cleaning up all virtual resources and restarting the workload on a set of new resources the openstack services were still experiencing a failure that could only be recovered by fully restarting the openstack platform and restoring its internal database from a backup.
these results point out the risk that failures are not timely detected and notified and that they can silently propagate through the system.
based on this analysis we identify a set of directions towards more reliable cloud management system.
to support future research in this field we share an artifact for configuring our fault injection environment inside a virtual machine and our dataset of failures which includes the injected faults the workload the effects of the failures both the user side impact and our own in depth correctness checks and the error logs produced by openstack.
in the following section elaborates on the research problem section describes our methodology section presents experimental results section discusses related work section includes links to the artifacts to support future research section concludes the paper.
overview on the research problem mitigating the severity of software failures caused by residual bugs is a relevant issue for high reliability systems yet it still represents an open research challenge.
ideally in the case that a fault occurs a service should be able to mask the fault or recover from it in a transparent way to the user such as by leveraging redundancy.
however this is often not possible in the case of software bugs.
since software bugs are human mistakes in the source code the traditional fault tolerance strategies for hardware and network faults often do not apply.
for example if a service is broken because of a regression bug then retrying to execute the service api with the same inputs would result again in a failure a retrial would only succeed in thecase that the software bug is triggered by a transient condition such as a race condition .
if recovery is not possible the failed operation must be necessarily aborted and the user should be notified so that the failure can be handled at a higher level of the business logic.
for example the end user can skip the failed operation or put on hold the workflow until the bug is fixed.
if the failure does not immediately generate an exception from the os or from the programming language run time the service may continue its faulty execution until it corrupts in subtle ways the results or the state of resources.
such cases need to be mitigated by architecting the software into small de coupled components for fault containment in order to limit the scope of failure e.g.
the bulkhead pattern and by applying defensive programming practices to perform redundant checks on the correctness of a service e.g.
pre and post conditions to check that a resource has indeed been allocated or updated .
in this way the system can enforce a fail stop behavior of the service e.g.
interrupting an api call that experiences a failure and generating an exception so that it can avoid data corruption and limit the outage to a small part of the system e.g.
an individual service call .
in this work we study the extent of this problem in the context of a cloud management system.
applying software fault tolerance principles in such a large distributed system is difficult since its design and implementation is a trade off between several objectives including performance backward compatibility programming convenience etc.
which opens to the possibility of failure propagation beyond fault containment limits.
we investigate this problem from three perspectives by addressing the following three perspectives.
in the case that service experiences a failure is it able to exhibit a fail stop behavior?
if a service request could not be completed because of a failure the service api should return an exception to inform about the issue.
therefore we experimentally evaluate whether the service indeed halts on failure and whether the failure is explicitly notified to the user.
in the worst case the service api neither halts nor raises an exception and the state of resources is inconsistent with respect to what the user is expecting e.g.
a vm instance was not actually created or is indefinitely in the building state .
areerrorreportingmechanismsabletopointouttheoccurrence of a failure?
error logs are a valuable source of information for automated recovery mechanisms and system operators to detect failures and restore service availability and for developers to investigate the root cause of the failure.
however there can be gaps between failures and log messages.
we analyze the cases in which the logs do not record any anomalous event related to a failure since the software may lack checks to detect the anomalous events.
are failures propagated across the services of the cloud management system?
to mitigate the severity of failures it is desirable that failure is limited to the specific service api that is affected by a software bug.
if the failure impacts other services beyond the buggy one e.g.
the incorrect initialization of a vm instance also causes the failure of subsequent operations on the instance it is more difficult to identify the root cause of the problem and to recover from the failure.
similarly the failure may cause lasting effects on the cloud infrastructures e.g.
the virtual resources allocated for a failed instance cannot be reclaimed or interfere with other resource allocations that are difficult to debug and to recover from.
therefore we analyze whether failures can spread across different components of the system and across several service calls.empirical analysis of software failures in the openstack ... esec fse august tallinn estonia apidictsqlrpcsystemagent plugin figure distribution of bug types.
methodology our approach is to inject software bugs .
.
in order to obtain failure data from openstack .
.
then we analyze whether the system could gracefully mitigate the impact of the failures .
.
.
bug analysis a key aspect to perform software fault injection experiments is to inject representative software bugs .
since the body of knowledge on bugs in python software the programming language of openstack is relatively smaller compared to other languages we seek for more insights about bugs in the openstack project.
therefore we analyzed the openstack issue tracker on the launchpad portal by looking for bug fixes at the source code level in order to identify bug patterns for this project.
from these patterns we defined a set of bug types to be injected.
we went through the problem reports and inspected the related source code.
we looked for reports where i the root cause of the problem was a software bug excluding build packaging and installation issues ii the problem had been marked with the highest severity level i.e.
the problem has a strong impact on openstack services iii the problem was fixed and the bug fix was linked to the discussion.
we manually analyzed a sample of problem reports from the launchpad focusing on entries with importance set to critical and with status set to fix committed or fix released such that the problem report also includes a final solution shipped in openstack .
of these problem reports we identified reports that met all of the three criteria.
we shared the full set of bug reports see section .
the bugs encompass several areas of openstack including bugs that affected the service apis exposed to users e.g.
nova api bugs that affected dictionaries and arrays such as a wrong key used in image bugs that affected sql queries e.g.
database queries for information about instances in nova bugs that affected rpc calls between openstack subsystems e.g.
rpc.cast was omitted or had a wrong topic or contents bugs that affected calls to external system software such as iptables anddsnmasq bugs that affected pluggable modules in openstack such as network protocol plugins and agents in neutron.
figure shows statistics about the bug types that we identified from the problem reports and their bug fixes.
the five most frequent bug types include the following ones.
wrong parameters value the bug was an incorrect method call inside openstack where a wrong variable was passed to the method call.
for example this was the case of the nova bug which was fixed in by changing the exit codes passed through the parameter check exit code .
missing parameters a method call was invoked with omitted parameters e.g.
the method used a default parameter instead of the correct one .
for example this was the case of the nova bug which was fixed in by adding the parameter read deleted yes when calling the sql alchemy apis .
missing function call a method call was entirely omitted.
for example this was the case of the nova bug launchpad.net nova bug whichwasfixedin openstack.org c by adding and calling the new method trigger security group members refresh .
wrong return value a method returned an incorrect value e.g.
none instead of a python object .
for example this was the case of the nova bug which was fixed in by returning an object allocated through allocate fixed ip .
missing exception handlers a method call lacks exception handling.
for example this was the case of the nova bug which was fixed in by adding an exception handler for exception.instancenotfound .
.
fault injection in this study we perform software fault injection to analyze the impact of software bugs .
this approach deliberately introduces programming mistakes in the source code by replacing parts of the original source code with faulty code.
for example in figure the injected bug emulates a missing optional parameter a port number to a function call which may cause failure under certain conditions e.g.
a vm instance may not be reachable through an intended port .
this approach is based on previous empirical studies which observed that the injection of code changes can realistically emulate software faults in the sense that code changes produce runtime errors that are similar to the ones produced by real software faults .
this approach is motivated by the high efforts that would be needed for experimenting with hand crafted bugs or with real past bugs in these cases every bug would require to carefully craft the specific conditions that trigger it i.e.
the topology of the infrastructure the software configuration and the hardware devices under which the bug surfaces .
to achieve a match between injected and real bugs we focus the injection on the most frequent five types found by the bug analysis.
these bug types allow us to cover all of the main areas of openstack api sql etc.
and suffice to generate a large and diverse set of faults over the codebase of openstack.
we emulate the bug types by mutating the existing code of openstack.
the figure shows the steps of a fault injection experiment.esec fse august tallinn estonia d. cotroneo l. de simone p. liguori r. natella n. bidokhti faulty roundfault free round iface name self.get interface name network port ifbug trigger true iface name self.get interface name network else iface name self.get interface name network port originalpython codeinjectedpython code .
deploy start the target system3.the buggy execution path is initially disabled .runthe workloadfor the first time .testbedclean up before next test .runthe workload for the second timecorrectexecution path the same method call of the original python script buggyexecution path missing parameter in method call .the buggy execution path is enabledat this point1.
scan the source code to identify a fault injection point2.
mutate the source code to inject the bug7.the buggy execution path is disabledagaintimelineoffonoff figure overview of a fault injection experiment we developed a tool to automate the bug injection process in python code.
the tool uses the astpython module to generate an abstract syntax tree ast representation of the source code then it scans the ast by looking for relevant elements function calls expressions etc.
where the bug types could be injected it modifies the ast by removing or replacing the nodes to introduce the bug finally it rewrites the modified ast into python code using the astunparse python module.
to inject the bug types of section .
we modify or remove method calls and their parameters.
we targeted method calls related to the bugs that we analyzed by targeting calls to internal apis for managing instances volumes and networks e.g.
which are denoted by specific keywords such as instance andnova for the methods of the nova subsystem .
wrong input and parameters are injected by wrapping the target expression into a function call which returns at run time a corrupted version of the expression based on its data type e.g.
a null reference in place of an object reference or a negative value in place of an integer .
exceptions are raised on method calls according to a pre defined list of exception types.
the tool inserts fault injected statements into an ifblock together with the original version of the same statements but in a different branch as in step in figure .
the execution of the faultinjected code is controlled by a trigger variable which is stored in a shared memory area that is writable from an external program.
this approach has been adopted for controlling the occurrence of failures during the tests.
in the first phase round we enable the fault injected code and we run a workload that exercises the service apis of the cloud management system.
during this phase the fault injected code could generate run time errors inside the system which will potentially lead to user perceived failures.
afterward in a second phase round we disable the injected bug and we execute the workload for a second time.
this fault free execution points out whether the scope of run time errors generated by the first phase is limited to the service api invocations that triggered the buggy code e.g.
the bug only impacts on local session data .
if failures still occur during the second phase then the system has not able to handle the run time errors of the first phase.
such failures point out the propagation of effects across the cloud management system see .
we implemented a workload generator to automatically exercise the service apis of the main openstack sub systems.
the workload has been designed to cover several sub systems of openstack and several types of virtual resources in a similar way to integration test cases from the openstack project .
the workload creates vm instances along with key pairs and a security group attaches the instances to volumes creates a virtual network with virtual routers and assigns floating ips to connect the instances to the virtual network.
having a comprehensive workload allows us to point out propagation effects across sub systems caused by bugs.table assertion check failures.
name description failure image active the created image does not transit into the active state failure instance active the created instance does not transit into the active state failure ssh it is impossible to establish a sshsession to the created instance failure keypair the creation of a keypair fails failure security group the creation of a security group andrules fails failure volume created the creation of a volume fails failure volume attached attaching a volume to an instance fails failure floating ip created the creation of a floating ip fails failure floating ip added adding a floating ip to an instance fails failure private network activethe created network resource does not transit into the active state failure private subnet createdthe creation of a subnet fails failure router active the created router resource does not transit into the active state failure router interface createdthe creation of a router interface fails the experimental workflow is repeated several times.
every experiment injects a different fault and only one fault is injected per experiment.
before a new experiment we clean up any potential residual effect from the previous experiment in order to be able to relate failure to the specific bug that caused it.
the clean up redeploys openstack removes all temporary files and processes and restores the database to its initial state.
however we do not perform these clean up operations between the two workload rounds i.e.
no clean up between the steps and of figure since we want to assess the impact of residual side effects caused by the bug.
.
failure data collection during the execution of the workload we record inputs and outputs of service api calls of openstack.
any exception generated from the call api errors is also recorded.
in between calls to service apis the workload also performs assertion checks on the status of the virtual resources in order to point out failures of the cloud management system.
in the context of our methodology assertion checks serve as ground truth about the occurrence of failures during the experiments.
these checks are valuable to point out the cases in which a fault causes an error but the system does not generate an api error i.e.
the system is unaware of the failure state .
our assertion checks are similar to the ones performed by the integration tests as test oracles they assess the connectivity of the instances through ssh and query the openstack api to check that the status of the instances volumes and network is consistent with the expectation of the test cases.
the assertion checks are performed by our workload generator.
for example after invoking the api for creating a volume the workload queries the volume status to check if it is available volume created assertion .
these checks are useful to find failures not notified through the api errors.
table describes the assertion checks.
if an api call generates an error the workload is aborted as no further operation is possible on the resources affected by the failureempirical analysis of software failures in the openstack ... esec fse august tallinn estonia e.g.
no volume could be attached if the instance could not be created .
in the case that the system fails without raising an exception i.e.
an assertion check highlights a failure but the system does not generate an api error the workload continues the execution as a hypothetical end user being unaware of the failure would do regardless of failed assertion check s .
the workload generator records the outcomes of both the api calls and of the assertion checks.
moreover we collect all the log files generated by the cloud management system.
this data is later analyzed for understanding the behavior of the system under failure.
.
failure analysis we analyze fault injection experiments according to three perspectives discussed in section .
the first perspective classifies the experiments with respect to the type of failure that the system experiences .
the possible cases are the following ones.
api error in these cases the workload was not able to correctly execute due to an exception raised by a service api call.
in these cases the cloud management system has been able to handle the failure in a fail stop way since the user is informed by the exception that the virtual resources could not be used and it can perform recovery actions to address the failure.
in our experiments the workload stops on the occurrence of an exception as discussed before.
assertion failure in these cases the failure was not pointed out by an exception raised by a service api.
the failure was detected by the assertion checks made by the workload in between api calls which found an incorrect state of virtual resources.
in these cases the execution of the workload was not interrupted as no exception was raised by the service apis during the whole experiment and the service api did apparently work from the perspective of the user.
these cases point out non fail stop behavior.
assertion failure s followed by an api error in these cases the failure was initially detected by assertion checks which found an incorrect state of virtual resources in between api calls.
after the assertion check detected the failure the workload continued the execution by performing further service api calls until an api error occurred in a later api call.
these cases also point out issues at handling the failure since the user is unaware of the failure state and cannot perform recovery actions.
no failure the injected bug did not cause a failure that could be perceived by the user neither by api exceptions nor by assertion checks .
it is possible that the effects of the bug were tolerated by the system e.g.
the system switched to an alternative execution path to provide the service or the injected source code was harmless e.g.
an uninitialized variable is later assigned before use .
since no failure occurred these experiments are not further analyzed as they do not allow to draw conclusions on the failure behavior of the system.
failed executions are further classified according to a second perspective with respect to the execution round in which the system experienced a failure .
the possible cases are the following ones.
failure in the faulty round only in these cases a failure occurred in the first faulty execution round figure in which a bug has been injected and no failure is observed during the second faultfree execution round in which the injected bug is disabled and in which the workload operates on a new set of resources.
this behavior is the likely outcome of an experiment since we are deliberately forcing a service failure only in the first round through the injected bug.
tracedebuginfoauditwarningerrorcriticalnumber of occurencesseveritycindernovaneutronglanceheatkeystonehorizonswiftfigure distribution of log messages severity during a fault free execution of the workload.
failure in the fault free round despite the faulty round these cases are concerns for fault containment since the system is still experiencing failures despite the bug is disabled after the first round and the workload operates on a new set of resources.
this behavior is due to residual effects of the bug that propagated through session state persistent data or other shared resources.
finally the experiments with failures are classified from the perspective of whether they generated logs able to indicate the failure .
in order to make more resilient a system we are interested in whether it produces information for detecting failures and for triggering recovery actions.
in practice developers are conservative at logging information for post mortem analysis by recording high volumes of low quality log messages that bury the truly important information among many trivial logs of similar severity and contents making it difficult to locate issues .
therefore we cannot simply rely on the presence of logs to conclude that a failure was detected.
to clarify the issue figure shows the distribution of the number of log messages in openstack across severity levels trace to critical during the execution of our workload generator and without any failure.
we can notice that all openstack components generate a large number of messages with severity warning info anddebug even when there is no failure.
instead there are no messages of severity error orcritical .
therefore even if a failure is logged with severity warning or lower such log messages cannot be adopted for automated detection and recovery of the failure as it is difficult to distinguish between informative messages and actual issues.
therefore to evaluate the ability of the system to support recovery and troubleshooting through logs we classify failures according to the presence of one or more high severity message i.e.
critical orerror recorded in the log files logged failures or no such message non logged failures .
experimental results in this work we present the analysis of openstack version .
.
release pike which was the latest version of openstack when we started this work.
we injected bugs into the most fundamental services of openstack i the nova subsystem which provides services for provisioning instances vms and handling their life cycle ii the cinder subsystem which provides services for managing block storage for instances and iii the neutron subsystem which provides services for provisioning virtual networks for instances including resources such as floating ips ports and subnets .
each subsystem includes several components e.g.
the nova sub system includes nova api nova compute etc.
which interact through message queues internally to openstack.
the nova cinder and neutron sub systems provide external rest api interfaces to cloud users.
figure shows the testbed used for the experimental analysis of openstack.
we adopted an all in one virtualized deployment of openstack in which the openstack services run on the same vm esec fse august tallinn estonia d. cotroneo l. de simone p. liguori r. natella n. bidokhti mysqlserverrabbitmqservernovanova apinova computenova schedulerneutronneutron serverneutron dhcp agentneutron ovs agentcindercontroller code mutationworkflow logs ......cinder apicinder schedulercinder volumenova conducorneutron dbworkloadgeneratoropenstackapi callsstart workload keystoneglanceopenstacktestbedinjectoragent figure openstack testbed architecture.
for the following reasons to prevent interferences on the tests from transient issues in the physical network e.g.
sporadic network faults network delays caused by other user traffic in our local data center etc.
to parallelize a high number of tests on several physical machines by using the packstack installation utility to have a reproducible installation of openstack across the vms to efficiently revert any persistent effect of a fault injection test on the openstack deployment e.g.
file system issues in order to assure independence among the tests.
moreover the all in one virtualized deployment is a common solution for performing tests on openstack .
the hardware and vm configuration for the testbed includes virtual intel xeon cpus e5 2630l v3 .80ghz 16gb ram gb storage linux centos v7.
.
in addition to the core services of openstack e.g.
nova neutron cinder etc.
the testbed also includes our own components to automate fault injection tests.
the injector agent is the component that analyzes and instruments the source code of openstack.
the injector agent can i scan the source code to identify injectable locations i.e.
source code statements where the bug types discussed in .
can be applied ii instrument the source code by introducing logging statements in every injectable location in order to get a profile of which locations are covered during the execution of the workload coverage analysis iii instrument the source code to introduce a bug into an individual injectable location.
the controller orchestrates the experimental workflow.
it first commands the injector agent to perform a preliminary coverage analysis by instrumenting the source code with logging statements restarting the openstack services and launching the workload generator but without injecting any fault.
the workload generator issues a sequence of api calls in order to stimulate openstack services.
the controller retrieves the list of injectable locations and their coverage from the injector agent .
then it iterates over the list of injectable locations that are covered and issues commands for the injector agent to perform fault injection tests.
for each test the injector agent introduces an individual bug by mutating the source code restarts the openstack services starts the workload and triggers the injected bug as discussed in .
.
the injector agent collects the logs files from all openstack subsystems and from the workload generator which are sent to the controller for later analysis .
.
we performed a full scan of injectable locations in the source code of nova cinder and neutron for a total of analyzed source code files.
we identified injectable faults that were covered by the workload.
figure shows the number of faults per sub system and per type of fault.
the number of faults for each type and sub system depends on the number of calls to the target functions and on their input and output parameters as discussed in .
.
we executed one the test per injectable location by injecting one fault at a time.
figure number of fault injection tests.
figure distribution of openstack failures.
after executing the tests we found failures respectively in .
out of tests .
out of tests and out of tests of tests in nova cinder and neutron for a total of .
in the remaining .
of the tests out of tests instead there were neither an api error nor assertion failures in these cases the fault was not activated even if the faulty code was covered by the workload or there was no error propagation to the component interface.
the occurrence of tests not causing failures is a typical phenomenon that occurs with code mutations which may not infect the state even when the faulty code is executed .
yet the injections provided us a large and diverse set of failures for our analysis.
.
does openstack show a fail stop behavior?
we first analyze the impact of failures on the service interface apis provided by openstack.
the workload generator which impersonates a user of the cloud management system invokes these apis looks for errors returned by the apis and performs assertion checks between api calls.
a fail stop behavior occurs when an api returns an error before any failed assertion check.
in such cases the workload generator stops on the occurrence of the api error.
instead it is possible that an api invocation terminates without returning any error but leaving the internal resources of the infrastructure instances volumes etc.
in a failed state which is reported by assertion checks.
these cases represent violations of the fail stop hypothesis and represent a risk for the users as they are unaware of the failure.
to investigate this aspect we initially focus on the faulty round of each test in which fault injection is enabled figure .
figure shows the number of tests that experienced failures divided into api error only assertion failure only and assertion failure s followed by an api error .
the figure shows the data divided with respect to the subsystem where the bug was injected respectively in nova cinder and neutron moreover figure shows the distribution across all fault injection tests.
we can see that the cases in which the system does not exhibit a fail stop behavior i.e.
the categories assertion failure only andassertion failure followed by an api error represent the majority of the failures.empirical analysis of software failures in the openstack ... esec fse august tallinn estonia figure distribution of assertion check failures.
figure distribution of api errors.
figure shows a detailed perspective on the failures of assertion checks.
notice that the number of assertion is greater than the number of tests classified in the assertion failure category i.e.
assertion failure only andassertion failure followed by an api error since a test can generate multiple assertion failures.
the most common case has been one of the instances not active because the instance creation failed i.e.
it did not move into the active state .
in other cases the instance could not be reached through the network or could not be attached to a volume even if in the active state.
a further common case is the failure of the volume creation but only the faults injected in the cinder sub system caused this assertion failure.
these cases point out that openstack lacks redundant checks to assure that the state of the virtual resources after a service call is in the expected state e.g.
newly created instances are active .
such redundant checks would assess the state of the virtual resources before and after a service invocation and would raise an error if the state does not comply with the expectation such as a new instance could not be activated .
however these redundant checks are seldom adopted most likely due to the performance penalty they would incur and because of the additional engineering efforts to design and implement them.
nevertheless the cloud management system is exposed to the risk that residual bugs can lead to non fail stop behaviors where failures are notified with a delay or not notified at all.
this makes not trivial to prevent data losses and to automate recovery actions.
figure provides another perspective on api errors.
it shows the number of tests in which each api returned an error focusing on out of apis that failed at least one time.
the api with the highest number of api errors is the one for adding a volume to an instance openstack server add volume provided by the cinder sub system.
this api generated errors even when faults were injected in nova time s .
.
.
.81probabilitynova cinder neutronfigure cumulative distribution of api error latency.
instance management and neutron virtual networking .
this behavior means that the effects of fault injection propagated from other sub systems to cinder e.g.
if an instance is in an incorrect state other apis on that resource are also exposed to failures .
on the one hand this behavior is an opportunity for detecting failures even if in a later stage.
on the other hand it also represents the possibility of a failure to spread across sub systems thus defeating fault containment and exacerbating the severity of the failure.
we will analyze fault propagation in more detail in section .
.
to understand the extent of non fail stop behaviors we also analyze the period of time latency between the execution of the injected bug and the resulting api error.
it is desirable that this latency is as low as possible.
otherwise the longer the latency the more difficult is to relate an api error with its root cause i.e.
an api call invoked much earlier on a different sub system or virtual resource and the more difficult to perform troubleshooting and recovery actions.
to track the execution of the injected bug we instrumented the injected code with logging statements to record the timestamp of its execution.
if the injected code is executed several times before a failure e.g.
in the body of a loop we conservatively consider the last timestamp.
we consider separately the cases where the api error is preceded by assertion check failures i.e.
the injected bug was triggered by an api different from the one affected by the bug from the cases without any assertion check failure e.g.
the api error arises from the same api affected by the injected bug .
figure shows the distributions of latency for api errors that occurred after assertion check failures respectively for the injections in nova cinder and neutron.
table summarizes the average the 50th and the 90thpercentiles of the latency distributions.
we note that in the first category api errors after assertion checks all sub systems exhibit a median api error latency longer than seconds with cases longer than several minutes.
this latency should be considered too long for cloud services with high availability slas e.g.
four nines or more which can only afford few minutes of monthly outage.
this behavior points out that the api errors are due to a reactive behavior of openstack which does not actively perform any redundant check on the integrity of virtual resources but only reacts to the inconsistent state of the resources once they are requested in a later service invocation.
thus openstack experiences a long api error latency when a bug leaves a virtual resource in an inconsistent state.
this result suggests the need for improved error checking mechanisms inside openstack to prevent these failures.
in the case of failures that are notified by api errors without any preceding assertion check failure the second category in table the latency of the api errors was relatively small less than one second in the majority of cases.
nevertheless there were few cases with an api error latency higher than one minute.
in particular theseesec fse august tallinn estonia d. cotroneo l. de simone p. liguori r. natella n. bidokhti table statistics on api error latency.
subsys.
avg 50th ile 90th ile api errors after an assertion failurenova .
.
.
cinder .
.
.
neutron .
.
.
api errors onlynova .
.
.
cinder .
.
.
neutron .
.
.
cases happened when bugs were injected in nova but the api error was raised by a different sub system cinder .
in these cases the high latency was caused by the propagation of the bug s effects across different api calls.
these cases are further discussed in .
.
.
is openstack able to log failures?
since failures can be notified to the end user with a long delay or even not at all it becomes important for system operators to get additional information to troubleshoot these failures.
in particular we here consider log messages produced by openstack sub systems.
we computed the percentage logging coverage of failed tests which produced at least one high severity log message see also .
.
table provides the logging coverage for different subsets of failures by dividing them with respect to the injected subsystem and to the type of failure.
from these results we can see that openstack logged at least one high severity message i.e.
with severity level error orcritical in most of the cases.
the cinder subsystem shows the best results since logging covered almost all of the failures caused by fault injection.
however in the case of nova and neutron logs missed some of the failures.
in particular the failures without api errors i.e.
assertion failure only exhibited the lowest logging coverage.
this behavior can be problematic for recovery and troubleshooting since the failures without api errors lack an explicit error notification.
these failures are also the ones in need of complementary sources of information such as logs.
to identify opportunities to improve logging in openstack we analyzed the failures without any high severity log across with respect to the bug types injected in these tests.
we found that missing function call andwrong return value represent the .
of the bug types that lead to non logged failures .
and .
respectively .
the wrong return value faults are the easiest opportunity for improving logging and failure detection since the callers of a function could perform additional checks on the returned value and record anomalies in the logs.
for example one of the injected bugs introduced a wrong return value in calls to a database api called by the nova sub system to update the information linked to a new instance.
the bug forced the function to return anone instance object.
the bug caused an assertion check failure but openstack did not log any high severity message.
by manually analyzing the logs we could only find one suspicious message with the only warning severity and with little information about the problem as this message was not related to database management.
the non logged failures caused by a missing function call emphasize the need for redundant end to end checks to identify inconsistencies in the state of the virtual resources.
for example in one of these experiments we injected a missing function call in the libvirtdriver class in the nova subsystem which allows openstack to interact with the libvirt virtualization apis .
because of the injected bug the nova driver omits to attach a volume to an instance but the nova sub system does not perform checks that the volume is indeed attached to the instance.
this kind of end to endtable logging coverage of high severity log messages.
logging coverage subsystemapi errors onlyassertion failure onlyassertion failure and api errors nova .
.
cinder neutron .
.
checks could be introduced at the service api interface of openstack e.g.
in nova api to test the availability of the virtual resources at the end of api service invocations e.g.
by pinging them .
.
do failures propagate across openstack?
we analyze failure propagation across sub systems to identify more opportunities to reduce their severity.
we consider failures of both the faulty and the fault free rounds respectively figure .
in the faulty round we are interested in whether the injected bug impacted on sub systems beyond the injected one.
to this aim we divide the api errors with respect to the api that raised the error e.g.
an api exposed by nova neutron or cinder .
similarly we divide the assertion check failures with respect to the sub system that manages the virtual resource checked by the assertion.
there is aspatial fault propagation across the components if an injection on a sub system say nova causes an assertion check failure or an api error on a different sub system say cinder or neutron .
figure 10a shows a graph with of events occurred during the faulty round of the tests with a failure.
the nodes on the top of the graph represent the sub systems where bugs were injected the nodes on the middle represent assertion check failures the nodes on the bottom represent api errors.
the edges that originate from the nodes on the top represent the number of injections that were followed by an assertion check failure or an api error.
moreover the edges between the middle and the bottom nodes represent the number of tests where an assertion check failure was followed by an api error.
the most numerous cases are emphasized with proportionally thicker edges and annotated with the number of occurrences.
we used different shades to differentiate the cases with respect to the injected sub system.
the failures exhibited a propagation across openstack services in a significant amount of cases .
of the failures .
in many cases the propagation initiated from an injection in nova which caused a failure at activating a new instance as discussed in the previous subsections the unavailability of the instance was detected in a later stage such as when the user attaches a volume to the instance using the cinder api.
even worse there are some cases of propagation from neutron across nova and cinder.
these failures represent a severe issue for fault containment since an injection in neutron not only caused a failure of their apis but also impacted on virtual resources that were not managed by these sub systems.
therefore the failures are not necessarily limited to the virtual resources managed by the sub system invoked at the time of the failure but also to other related virtual resources.
therefore end to end checks on api invocations should also include resources that are indirectly related to the api such as checking the availability of an instance after attaching a volume .
for as concerns cinder instead there are no cases of error propagation from this sub system across nova and neutron.
we further analyze the propagation of failures by considering what happens during the fault free round of execution.
the faultfree round invokes the service apis after the buggy execution path is disabled as dead code.
moreover the fault free round executes on new virtual resources i.e.
instances networks routers etc.
areempirical analysis of software failures in the openstack ... esec fse august tallinn estonia a during faulty round.
b after removing the injected fault fault free round .
figure fault propagation during fault injection tests.
created from scratch .
therefore it is reasonable to expect and it is indeed the case that the fault free round executes without experiencing any failure.
however we still observe a subset of failures .
that propagate their effects to the fault free round.
these failures must be considered critical since they are affecting service requests that are supposed to be independent but are still exposed to temporal failure propagation through shared state and resources.
we remark that the failures in the fault free round are caused by the injection in the faulty round.
indeed we assured that previous injections do not impact on the subsequent experiments by restoring all the persistent state of openstack before every experiment.
figure 10b shows the propagation graph for the fault free round.
the most cases the nova sub system was unable to create new instances even after the injected bug is removed from nova.
a similar persistent issue happens for a subset of failures caused by injections in neutron.
these sub systems both manage a relational database which holds information on the virtual instances and networks and we found that the persistent issues are solved only after that the databases are reverted to the state before fault injection.
this recovery action can be very costly since it can take a significant amount of time during which the cloud infrastructure may become unavailable.
for this reason we remark the need for detecting failures as soon as they occur such as using end to end checks at the end of service api calls.
such detection would support quicker recovery actions such as to revert the database changes performed by an individual transaction.
.
discussion and lessons learned the experimental analysis pointed out that software bugs often cause erratic behavior of the cloud management system hindering detection and recovery of failures.
we found failures that were notified to the user only after a long delay when it is more difficult to trace back the root cause of the failure and recovery actions are more costly e.g.
reverting the database or the failures were not notified at all.
moreover our analysis suggests the following practical strategies to mitigate these failures.
need for deeper run time verification of virtual resources.
fault injections pointed out openstack apis that leaked resources on failures or left them in an inconsistent state due to missing or incorrect error handlers.
for example the server create api failed without creating a new vm but it did not deallocate virtual resources e.g.
instances in dead state unused virtual nics created before the failure.
these failures can be prevented through fault injection.
moreover residual faults should be detected and handled by means of run time verification strategies which perform redundant endto end checks after a service api call to assert whether the virtual resources are in the expected state.
for example these checks can be specified using temporal logic and synthesized in a run time monitor e.g.
a logical predicate for a traditional os can assert that a thread suspended on a semaphore leads to the activation of another thread .
in the context of cloud management the predicates should test at run time the availability of virtual resources e.g.
volumes and connectivity similarly to our assertion checks .
increasing the logging coverage.
the logging mechanisms in openstack reported high severity error messages for many of the failures.
however there were failures with late or no api errors that would benefit from logs to diagnose the failure but such logs were missing.
in particular fault injection identified function call sites in openstack where the injected wrong return values were ignored by the caller.
these cases are opportunities for developers to add logging statements and to improve the coverage of logs e.g.
by checking the outputs produced by the faulty function calls .
moreover the logs can be complemented with the run time verification checks.
preventing corruptions of persistent data and shared state.
the experiments showed that undetected failures can propagate across several virtual resources and sub systems.
moreover we found that these propagated failures can impact on shared state and persistent data such as databases causing permanent issues.
fault injection identified failures that were detected much later after their initial occurrence i.e.
with high api error latency or no api errors at all .
in these cases it is very difficult for operators to diagnose which parts of the system have been corrupted thus increasing the cost of recovery.
therefore in addition to timely failure detection using deeper run time verification techniques as discussed above it becomes important to address the corruptions as soon as the failure is detected since the scope of recovery actions can be smaller i.e.
the impact of the failure is limited specific resources involved by the failed service api call .
one potential direction of research is on selectively un recent changes to the shared state and persistent data of the cloud management system .
.
threats to validity the injection of software bugs is still a challenging and open research problem.
we addressed this issue by using code mutations to generate realistic run time errors.
this technique is widespread in the field of mutation testing to devise test cases moreover it is also commonly adopted by studies on software dependability and on assessing bug finding tools .
in our context bug injection is meant to anticipate the potential consequences of bugs on service availability and resource integrity.
to strengthen the connection between the real and the experimental failures we based our selection of code mutations on past software bugs in openstack.
the injected bug types were consistent with code mutations typically adopted for mutation testing and fault injection e.g.
the omission of statements .
moreover the analysis of openstack bugs gave us insights on where to apply the injections e.g.
onesec fse august tallinn estonia d. cotroneo l. de simone p. liguori r. natella n. bidokhti method calls for controlling nova for performing sql queries etc.
.
even if some categories of failures may have been over or underrepresented e.g.
the percentages for failures that were not detected or that propagated our goal is to point out the existence of potential critical classes of failures despite possible errors in the estimates of the percentages.
in our experiments these classes were large enough to be considered a threat to cloud management platforms.
related work analysisofbugsandfailuresofcloudsystems.
previous studies on the nature of outages in cloud systems analyzed the failure symptoms reported by users and developers and the bugs in the source code that caused these failures.
among these studies li et al.
analyzed failures of amazon elastic compute cloud apis and other cloud platforms by looking at failure reports on discussion forums of these platforms.
they proposed a new taxonomy to categorize both failures content late timing halt and erratic failures and bugs development interaction and resource faults .
one of the major findings is that the majority of the failures exhibit misleading content and erratic behavior.
moreover the work emphasizes the need for counteracting development faults i.e.
bugs through semantic checks of reasonableness of the data returned by the cloud system.
musavi et al.
focused on api issues in the openstack project by looking at the history of source code revisions and bugfixes of the project.
they found that most of the changes to api are meanttofixapiissuesandthatmostoftheissuesaredueto programming faults .
gunawi et al.
analyzed outage failures of cloud services by inspecting headline news and public post mortem reports pointing out that software bugs are one of the major causes of the failures.
in a subsequent study gunawi et al.
analyzed software bugs of popular open source cloud systems by inspecting their bug repositories.
the bug study pointed out the existence of many killer bugs that are able to cause cascades of failures in subtle ways across multiple nodes or entire clusters and that software bugs exhibit a large variety where logic specific bugs represent the most frequent class.
most importantly the study remarks that cloud systems tend to favor availability over correctness that is the systems attempt to continue running despite the bugs cause data inconsistencies corruptions or low level failures are detected in order to avoid that users could perceive outages but putting at risk the correctness of the service.
these studies give insights into the nature of failures in cloud systems and point out that software bugs are a predominant cause of failures.
while these studies rely on evidence that was collected after the fact e.g.
the failure symptoms reported by the users we analyze failures in a controlled environment through fault injection to get more detailed information on the impact on the integrity of virtual resources error logs failure propagation and api errors.
fault injection in cloud systems.
the fault injection is widely used for evaluating fault tolerant cloud computing systems.
wellknown solutions in this field include fate and its successor prefail for testing cloud software such as cassandra zookeeper and hdfs against faults from the environment by emulating at api level the unavailability of network and storage resources and crashes of remote processes.
similarly ju et al.
and chaosmonkey test the resilience of cloud infrastructures by injecting crashes e.g.
by killing vms or service processes network partitions by disabling communication between two subnets and network traffic latency and losses.
other fault models for fault injection include hardware induced cpu and memory corruptions and resource leaks e.g.
induced by misbehaving guests .
cloudval and cerveira et al.
applied these fault models to test the isolation among hypervisors and vms.
pham et al.
applied fault injection on openstack to create signatures of the failures in order to support problem diagnosis when the same failures happen in production.
the fault model is the main difference that distinguishes our work from previous studies.
most of them assess software robustness with respect to external events e.g.
a faulty cpu disk or network .
in other studies fault injection has been simulating software failures through process crashes and api errors but this is a simplistic form of software bugs which can cause generate more subtle effects such as incorrect logic and data corruptions as pointed out by bug studies .
in this work we injected software bugs inside components by mutating their source code to deliberately force their failure and to assess what happens in the worst case that a bug eludes the qa process and gets into the deployed software.
we remark that previous work on mutation testing also adopted code mutation but with a different perspective than ours since we leverage mutations for evaluating software fault tolerance.
our work contributes to this research field by showing new forms of analysis based on the injection of software faults fail stop behavior logging failure propagation .
the same approach is also suitable to other systems of similar size and complexity of openstack e.g.
where the need for coordination among large subsystems raises the risk for non fail stop behavior and failure propagation .
experimental artifacts we release the following artifacts to support future research on mitigating the impact of software bugs i the analysis of openstack bug reports ii raw logs produced by the experiments figshare.
and iii tools for reproducing our experimental environment in a virtual machine figshare.
.
conclusion in this work we proposed a methodology to assess the severity of failures caused by software bugs through the deliberate injection of software bugs.
we applied this methodology in the context of the openstack cloud management system.
the experiments pointed out that the behavior of openstack under failure is not amenable to automated detection and recovery.
in particular the system often exhibits a non fail stop behavior in which it continues to execute despite inconsistencies in the state of the virtual resources without notifying the user about the failure and without producing logs for aiding system operators.
moreover we found that the failures can spread across several sub systems before being notified and that they can cause persistent effects that are difficult to recover.
finally we point out areas for future research to mitigate these issues including run time verification techniques to detect subtle failures in a more timely fashion and to prevent persistent corruptions.