automated multi objective control for self adaptive software design antonio filieri university of stuttgart stuttgart germany filieri informatik.uni stuttgart.dehenry hoffmann university of chicago chicago usa hankhoffmann cs.uchicago.edumartina maggio lund university lund sweden martina control.lth.se abstract while software is becoming more complex everyday the requirements on its behavior are not getting any easier to satisfy.
an application should o er a certain quality of service adapt to the current environmental conditions and withstand runtime variations that were simply unpredictable during the design phase.
to tackle this complexity control theory has been proposed as a technique for managing software s dynamic behavior obviating the need for human intervention.
control theoretical solutions however are either tailored for the specific application or do not handle the complexity of multiple interacting components and multiple goals.
in this paper we develop an automated control synthesis methodology that takes as input the configurable software components or knobs and the goals to be achieved.
our approach automatically constructs a control system that manages the specified knobs and guarantees the goals are met.
these claims are backed up by experimental studies on three di erent software applications where we show how the proposed automated approach handles the complexity of multiple knobs and objectives.
categories and subject descriptors d. .
design methodologies i. .
model development modeling methodologies general terms design experimentation theory performance reliability keywords adaptive software control theory dynamic systems nonfunctional requirements run time verification.
.
introduction self adaptation is a first class property of modern software systems.
adaptive software incorporates monitoring decisionmaking and actuation to maintain reliable behavior despite sudden unpredictable changes like application workload fluctuations and hardware failures.
adaptive software detects such changes inthe operating environment determines how to respond and implements the response with an actuation phase.
many methodologies have arisen for designing and implementing such adaptive orautonomic software systems .
dynamic feedback driven adaptation has been studied in control theory for decades where rigorous engineering techniques adapt physical systems while providing formal guarantees of their behavior e.g.
cruise control in an automobile .
recent research has applied control theory to adaptive software systems ensuring software behaves predictably in dynamic environments for examples see the survey .
control theoretic solutions are particularly attractive for software systems with strict requirements in unpredictable environments because control techniques permit formal analysis of a system s dynamic behavior.
the drawback of using control theory in software is that software engineers must master both their application domains and control systems.
indeed using control theory requires understanding all the actuators or software components that can be changed during runtime developing a suitable mathematical model relating these components to observable feedback and implementing a control strategy based on the model and the desired behavior .
while the first point is typically easy for the software developer the other two items require deep background in control science and might be not applicable for all the software adaptation problems .
recent work proposed addressing points and by fully automating the construction of a mathematical model and the synthesis of a suitable controller for software systems putting control techniques in the hands of non experts .
that approach addresses systems having a single quantitative goal and a single actuator.
it produces a controller using two phases learning andsynthesis.
the former conducts a systematic exploration of the actuator and builds a mathematical model relating this actuator to measurable changes in the software.
the synthesis phase uses the model to construct a controller that is formally guaranteed to meet the goal if feasible.
additional enhancements make this approach robust in the face of unpredictable external changes e.g.
those in thirdparty components or services.
despite the variety of problems that can be tackled with this approach its guarantees are limited to only a single quantitative goal.
software systems requiring guarantees in multiple dimensions e.g.
energy and performance cannot make use of this approach.
automatic synthesis of controllers for multiple goals is usually challenging and requires human intervention.
a general approach handling multiple goals and multiple actuators has been proposed .
to deal with conflicting goals this approach uses a cascade control schema where higher priority goals are pursued before lower priority ones.
this approach assumes perfect knowledge of this is the author s version of the work.
it is posted here for your personal use.
not for redistribution.
the definitive version was published in the following publication esec fse august september bergamo italy c acm.
... 13the system at design time and requires continuous knobs to be discretized possibly leading to an overwhelming number of control actions.
to find the best configuration within a large search space an optimization problem is formulated but the approach relies on heuristic solutions to avoid scalability problems .
recent work however has shown that the e ectiveness of such heuristics is system dependent i.e.
they are not general and not portable across systems .
we address this need to generalize support for multiple goals by extending our previous automated framework to synthesize controllers to manage requirements for multiple non functional quantitative properties simultaneously by synthesizing cascade controller systems given a prioritization of the control goals.
we address shortcomings in prior work by handling both discrete and continuous actuators and by finding exact solutions to optimization problems that translate control signals into knob settings.
the methodology we propose assumes the software system is a equipped with multiple actuators and is required to satisfy multiple quantitative goals.
each actuator is associated with a control knob or control variable that may a ect multiple quantitative dimensions at the same time.
after an initial possibly rough discretization of continuous knobs the controller dynamically refines the discretization between relevant values to increase control accuracy at runtime.
no constraints are imposed on the actual unknown relationship between knob and goal which can be linear or nonlinear.
multiple actuators a ecting multiple dimensions create an exponential explosion of possible software configurations which might limit the scalability of prior work.
the key insight of our approach is to overcome this explosion by exploiting the ranking of the nonfunctional properties or goals under control to design an e cient control allocation where multiple controllers operate in cascade on subsequently smaller decision spaces.
to this end we partition the knobs depending on the goals they a ect and decide on each partition separately whenever possible.
this can reduce the number of knobs configurations involved in each separate decision.
furthermore we replace the heuristic solution of the internal optimization problems with an exact solution based on the analysis of the dual problem.
this analysis allows to e ciently reduce the number of configurations that has to be considered for finding an optimal solution by several orders of magnitude.
for example in a radar processing case study see sec.
.
this analysis reduces the possible number of actuator combinations to consider from over million to just under thousand three orders of magnitude reduction.
in the proposed approach the controllers are dynamically synthesized based on the control actions selected for higher ranked goals.
the only requirement is that at least one problem dimension must be free i.e.
it will be minimized or maximized without any guarantees on its value.
our methodology supports two ranking schemes.
the first is user defined users explicitly define the priority of di erent goals.
this schema resembles the elicitation of primary mandatory requirements and secondary desirable requirements.
if some goals cannot be achieved the controllers first meet the highest ranked one and then the subsequents coming as close as possible on infeasible goals.
the second scheme is an automatic ranking where the control system orders the goals to maximize the number which can be achieved.
as in prior work continuous learning mechanisms are applied to keep the system model updated at runtime and maintain guarantees despite possible changes.
the methodology in this paper greatly extends the generality of prior approaches to automated control design moving existing ef fort much closer to the requirements necessary for deployment in a real system with multiple constraints all of which must be met despite unpredictable and uncertain execution environments.
at the same time the proposed approach requires no prior knowledge of the software under control nor special mathematical skills making control theoretic solutions available to non experts who must guarantee multiple aspects of a programs behavior.
we implement the proposed methodology and obtain results in three case studies.
in the first we manage performance security and energy for encrypted communications on a mobile device.
in the second we manage hardware resources and software configuration to achieve accurate and timely target localization in a cyberphysical radar system.
in the third we design a dynamic binding mechanism for a service oriented system to guarantee reliability response time and cost e ectiveness.
these examples demonstrate the broad applicability of our proposed automated methodology.
the rest of the paper is organized as follows.
section discusses prior work.
section presents some control background.
the proposed methodology is discussed in section and evaluated in section .
section concludes the paper.
.
related work many modern software systems are self adaptive i.e.
they select at runtime the best configuration to achieve specific nonfunctional requirements like reliability or response time.
there are many examples from compiler based support for alternative implementations to the exploitation of dynamic knobs for power and energy management .
hardware architectures can be dynamically adjusted to target execution speed and much more .
in high performance computing it is common to adapt a running application e.g.
tuning ffts for graphics processing units .
considerable e ort has been devoted to mapreduce which exposes many configurable parameters .
self adaptive techniques are also prominent in industry.
for example companies like ibm developed the ibm touchpoint simulator and the k42 operating system .
oracle produced the automatic workload repository and intel the ras technologies for enterprise .
formal methods are often used to build self adaptive systems because of their ability of providing mathematical guarantees on both the e ectiveness and the dependability of the adaptation mechanism .
among those methods control theory has been recognized by the software engineering community as a solution to meet quality of service requirements despite unpredictable changes in the execution environment.
several recent surveys capture the current state of the art applying control theory to software applications as well as highlighting the main criticisms of early approaches .
examples control delays for web servers manage data centers allocate resources tune operating systems minimize energy and coordinate across the system stack .
these strategies adapt tunable knobs that can be identified either o ine or at runtime .
the use of control theory in software engineering however is still in a preliminary stage.
it is di cult to develop accurate control models for software because strong mathematical skills are needed to deal with the complex non linear dynamics of real systems .
these di culties result in control strategies that solve a particular problem and operate in particular conditions but do not generalize.
this paper s goal is to increase the generality of control solutions by introducing a methodology that automatically constructs control systems for software adaptation.
the proposed methodology requires little prior knowledge instead adopting a push button approach that maintains the formal guarantees 14o ered by traditional control approaches but requires little mathematical background.
the first general and automated solution was recently proposed .
the solution was based on very simple qualitative equation based models.
while complex and precise quality models have been used in the past to enable design time optimizations such complexity is a drawback for runtime adaptation due to its overhead .
despite its generality with respect to problems and environments the solution proposed in su ers from restrictions and limitations.
this methodology is in fact only applicable to single input single output systems where the application developer must identify one single actuator to be changed and the methodology guarantees only a single non functional requirement e.g.
response time.
in this paper we further generalize this previous work to obtain an automated solution capable of dealing with multiple objectives and actuators simultaneously.
.
background this section connects software engineering and control theoretic terminology to provide the necessary background for following the methodology presented in sec.
.
applying control theory requires the ability to measure the quantitative property under control and some desired values for these properties.
these quantitative properties are referred to as the goals of the system and are related to non functional requirements like energy consumption response time and reliability.
for example the measured response time of a web server can be of 500ms while its desired value is below 1s.
in this paper we assume it is possible to measure every objective that the control system should fix and that the system developer assigned some desired values for the quantities under control.
the desired value in control terms is called the setpoint .
a software systems will have multiple nonfunctional requirements which we refer to as the dimensions of software behavior or simply dimensions for brevity.
the current measurement of the system status in a particular dimension is a feedback signal .
control also requires adjustable system components called controlknobs oractuators .
these actuators should a ect the measured quantitative properties and allow the system to reach the desired setpoints.
the software can have multiple knobs and multiple setpoints.
however the methodology proposed here requires the number of knobs to be greater than or equal to the number of dimensions under control.
the controller chooses the knobs settings to drive the feedback signal to the goals a process called setpoint tracking .
if the controller has to counteract external factors that could interfere with behavior it is disturbance rejection .
if it has to act also in presence of unreliable measurement e.g.
noisy feedback this calls forrobustness to inaccuracies.
these requirements for the control system can themselves be mapped into specific quantifiable properties.
specifically stability refers to convergence to an equilibrium with a well designed controller the setpoint.
also one can decide how the system reaches the equilibrium point.
overshooting means that the measured feedback may be higher than the goal for some time.
avoiding overhsoots avoids penalties e.g.
violating user requirements or service level agreements.
the time required to reach the equilibrium point is the settling time .
the system can also be robust in control theoretical terms and converge to the setpoint despite quantifiable errors in the control model.
it is obviously desirable to design a stable system that avoids overshooting has low settling time and high robustness.
the main advantage of a control theoretical approach is that these propertiesare formally guaranteed on the system s model.
thus it is possible to determine when the system behaves as expected and when it does not how it will converge and what the values of the measured quantities will be.
.
methodology this section presents our methodology for controlling multiple dimensions of software behavior using multiple actuators.
we assume no prior knowledge about the e ect of these actuators on the system.
we do assume that each knob has a nominal value which is the knob s value when it is not used to control any dimension.
when all the knobs are set to their nominal value the i th dimension ditakes value bi.
the term configuration denotes a specific set of values for the available knobs.
for example given the set of knobs k fk1 k2 k3g a possible configuration is fk1 k3 0g where k2is not assigned and can be set to any of its feasible values.
.
control strategy we propose a generalized feedback control strategy to guarantee the behavior of the software system in multiple dimensions.
the new approach builds on prior work that controlled performance power and application accuracy by extending the control strategy to any set of non functional quantitative goals automating the controller s design solving internal optimization problems exactly and incorporating continuous control knobs.
such multi dimensional control is a di cult problem because decisions made to adjust behavior in one dimension will likely affect others.
to overcome these complications we impose an ordering on dimensions of control.
given an ordering our approach applies control based on rank where higher ranking dimensions are controlled before lower ranking ones.
for example if performance is ranked highest and energy is lower ranked the controller will first tune knobs to satisfy the performance goal then determine a set of knobs that a ect energy but not performance and finally tune those knobs to meet the energy goal.
we support two di erent ordering schemes.
the first allows user specified priorities .
a user may be concerned more about software s performance than its accuracy.
performance will be higher ranked and the control strategy will adjust for performance first using any knob.
knobs which do not a ect performance are used to control accuracy.
the second ordering scheme supports feasibility .
in this scheme the controller automatically ranks dimensions based on available actuators to achieve as many goals as possible.
for example if three of the ten available actuators are necessary to constrain the performance behavior and five of them would be needed to address accuracy goals the controller chooses performance as a primary dimension to leave as many actuators as possible free to control accuracy.
the ordering scheme whether priority or feasibility based is the key insight to our approach.
at runtime the control strategy selects the actuators in the order of dimension ranking.
after applying the control values for one dimension we estimate the e ects on the subsequent dimensions remove the already constrained knobs from the pool of available ones and execute the controller to optimize for the new dimension.
adapting the control strategy during runtime based on decisions made for higher ranking dimensions allows us to account for the dependence between dimensions.
the estimation strategy bases the decision on actual data about how the system is reacting to changes in the previously controlled dimensions adjusting prior knowledge about the mentioned dependence.
figure shows an example of the proposed feedback control strategy managing two dimensions accuracy and performance.
in 15this example the highest ranking dimension is accuracy performance is lower ranking.
at runtime our approach first collects the current goals ga t and gp t and feedback measurements fa t and fp t in the accuracy a and performance p dimensions at time t. accuracy controlleraccuracy translatorga t ea t ua t performance controllerperformance translatorgp t ep t up t software systemka t kp t fa t fp t figure block diagram for controlling accuracy and performance.
the controller then computes the errors in both dimensions the di erences between the goals and current values ea t and ep t .
the control system produces ua t a signal indicating how to change accuracy to compensate for the measured error.
the control signal ua t is passed to a translator which determines the set of possible knobs values that achieves the desired accuracy and among those the ones that allow for optimal performance.
the predicted performance of this configuration is used to modify a second control system that produces up t a signal that drives the performance error ep t to zero.
this signal is then translated to a disjoint set of knobs that achieves up t with minimal interference on any other dimension.
at the next time step feedback and goals are measured and new control signals are calculated.
.
controller synthesis this section presents a formal description of the proposed controller.
for clarity we first present an approach for goals in two dimensions and then extend it to handle more.
this approach is based on classical control synthesis techniques but extends them to handle a general set of problems with a general set of knobs.
in classical control synthesis the knobs and the goals are known in advance and the synthesis is carried out to find a suitable controller for the specific case and the specific system model to be used.
our approach instead focuses on achieving generality and tackles the problem of having a set of knobs and goals to match.
the approach has five phases.
the first selects the lead dimension.
for user specified priorities this phase is skipped and user priorities become input for the second phase.
subsequent phases are the control of the lead dimension its translation into knob settings the control of the subordinate dimension and its translation.
if more dimensions are added the last two steps are repeated for each additional dimension.
we first focus on discrete or discretized knobs and discuss the dynamic refinement continuous knob discretization later in this section.
.
.
selecting the lead dimension if the controller does not receive user specified priorities the first step is to select the lead dimension.
given a set of ndimensionsd fd1 d2 dng the lead dimension is the one that can be controlled using the fewest actuators.
a set of mknobs k fk1 k2 kmgdoes not a ect dimension diif c2c di c bi wherec fk1 k k mg kawith a2f1 mgthe domain of the a th knob or the set of values that the a th knob can assume di c is the value of dimension didue to the e ect of the configuration c biis a baseline measurement of dimension di.
in other words for any possible combination of actuator values the cartesian product of the set of admissible values for each knob the behavior is unchanged.
with two dimensions we would like to find dlead the primary dimension and dsub the secondary one.
we start by finding the setk1of knobs a ecting the first dimension d1and the setk2 a ecting the second one d2.
from these two sets we select the lead dimension as the one which is influenced by a smaller number of knobs.
k1 fkjd1 c b1g k2 fkjd2 c b2g dlead argmindijkij dsub argmaxdijkij eqns.
and determine the set of knobs a ecting the goal dimensions while eqn.
chooses the lead dimension as the set with minimal cardinality.
the remaining one is the secondary dimension.
to control the lead dimension fewer actuators are used.
this means that more knobs are available for subsequent dimensions.
.
.
controlling the lead dimension letglead t and flead t denote the goal and feedback in the lead dimension at time t. control for the lead dimension eliminates the error elead t glead t flead t by computing a control signal ulead t based on its prediction of the next feedback measurement flead t flead t blead ulead t dist lead t where bleadis the baseline behavior in the lead dimension i.e.
its behavior with all the knobs configurations set to their nominal value.
dist lead t represents a transient disturbance.
for example if the lead dimension is performance bleadis the performance with the software in its default configuration.
flead t represents the measured performance at time t.ulead t represents the speedup over baseline the translator should achieve at time t and dist lead t represents a momentary disruption in performance e.g.
due to a page fault .
given elead t our methodology finds ulead t using a deadbeat controller based on the model defined by eqn.
.
the approach followed is the same as proposed in using zero as value for the pole i.e.
imposing that the controller is as fast as possible in making the measured value converge to its specified goal.
the control signal ulead t then becomes ulead t ulead t elead t blead the disturbance dist lead t disappears from eqn.
.
this is not surprising since dist lead t models any fast transient change that is not under control and a ects flead.
the fast transient disturbances cannot be controlled in any way unless a precise model of their action exists.
when the disturbance does not immediately disappear its action is slowly included into the model via a change in the baseline bleadthat is kept updated at runtime.
thus a temporary page fault receives no correction but a sudden lack of memory that persists for some time is compensated for due the feedback signal itself.
for now we assume to know how bivaries for each dimension during the execution of the application.
this assumption will be relaxed in sec.
.
.
.
.
translating the lead dimension once the control signal ulead t is computed it must also be actuated.
to this end we must select specific settings for the knobs in the setklead eqns.
to obtain the desired control signal.
to convert the continuous control signal into a configuration for the available knobs our approach schedules configurations for a window of time units with su cient for the next feedback signal to reflect the e ects of the schedule.
the schedule is a list lead of couples cw w where cwrepresents a specific configuration and wdenotes a time to spend in that configuration.
the configurations in the list are sequentially applied until the end of the window .
configurations are chosen so that the average behavior over the time window is equal to the control signal and the e ect of the schedule on the subordinate dimension is minimized.
the resulting optimization problem is optimize leadx cw w leaddsub cw w subject to x cw w lead w dlead cw blead ulead t x cw w lead w cw w lead ki2 cw ki klead here the word optimize stands for either minimize or maximize depending on the specific dimension.
for example optimizing power consumption minimizes it while optimizing reliability maximizes it.
this formulation assures the subordinate dimension is optimized eqn.
the control signal for the lead dimension is realized eqn.
and the total time does not exceed the time before the next feedback measurement eqn.
.
it also guarantees that every configuration included in the schedule uses only knobs belonging toklead eqn.
.
while mathematical optimization problems are in general expensive to solve the particular structure of this problem lends itself to a cost e ective solution.
this problem has two non trivial constraints eqns.
and and the other constraints confine solutions to have non negative components.
the geometric structure of this problem implies that the optimal solution will have at most two configurations in lead i.e.
only two of the coe cients win eqn.
will be nonzero .
a simple solution that returns the true optimal then would divide the set of cleadof all possible configurations of the knobs in klead the cartesian product of all sets of potential values for the selected knobs into two subsets co fc2cleadjdlead c ulead t g cu fc2cleadjdlead c ulead t g eqn.
finds the set of all the configurations that exceed ulead while eqn.
selects the configurations that approach the desired control signal from below.
given these two sets we can search over all pairs where one entry in the pair is drawn from coand one entry is drawn fromcu.
this algorithm is guaranteed to return an optimal solution to the problem but it executes in o jc2j time.
this exhaustive search can be straightforwardly parallelized e.g.
in with a map reduce strategy .
for very large configuration spaces it is possible to further reduce the complexity by creating a table of buckets for each dimension essentially a hash table where each bucket represents a range of behavior in that dimension.
we employ this approach for largec confining the search to the small number of configurationsthat map to the same bucket requiring o operations when the table is large enough to avoid bucket collisions.
finally an analysis of the dual problem of the mixed integer linear optimization in eqns.
can lead to a dramatic reduction of the search space by pruning out the configurations dominated by others toward the identification of the optimal point .
a deeper discussion of the possible optimization strategies is beyond the scope of this paper.
notably state of the art optimization tools have a variety of them built in and can be used o the shelf provided the user knows how to structure the problem.
the lead dimension is now controlled with a set of knobs that is as small as possible leaving the other knobs free for other dimensions.
.
.
controlling the subordinate dimension having selected coand cu the proposed approach calculates bsub t an estimate of how the choice for the lead dimension will a ect the subordinate one bsub t bsub dsub co co dsub cu cu we use eqn.
and the integral control law to calculate a control signal that eliminates the error esub t gsub t fsub t in the subordinate dimension.
usub t usub t esub t bsub t the value of usub t is then passed to the translator to obtain the remaining knob configuration.
.
.
translating the subordinate dimension for stability usub t must be translated into knob settings without a ecting the behavior of dlead.
from eqns.
we know the two sets of knobskleadandksuba ecting the lead and subordinate dimensions.
the controller then computes the set kvalid a ecting only the subordinate one.
if kvalid is the empty set the problem is not feasible and the system reports that it is not possible to achieve the subordinate dimension s goal without compromising that of the lead dimension.
kvalid knk lead k sub letdf reebe the dimension for which there is no goal.
then the signalusub t is translated to knob configurations that optimize df ree by computing a schedule subof couples cs s solving the following optimization problem optimize subx cs s subdf ree cs s subject to x cs s sub s sub dsub cs bsub usub t x cs s sub s sub cs s sub ki2 cs ki kvalid where subrepresents the sampling time of the subordinate loop which can be di erent than the lead one.
eqn.
optimizes the free dimension eqn.
ensures the control signal is realized eqn.
ensures the time window is respected and eqn.
ensures that configurations come from eqn.
.
this optimization problem is solved using the same optimal algorithm as presented previously for the lead dimension.
17extension to more dimensions the process above can be extended to an arbitrary set of dimensions.
instead of a lead and subordinate dimension the methodology ranks dimensions.
the highest rank dimension is equivalent to the lead dimension in the above.
for each subsequent dimension the process described in the last two steps is applied substituting the set of all the used knobs kleadin eqn.
.
this set is computed as the union of kleadwith all the already prescribed ksub.
in the hash table implementation of the above control for each dimension takes o time.
therefore applying this process to multiple dimensions takes o n time where nis the number of dimensions under control.
discretization refinement the controller schema introduced in this section identifies a sequence of discrete configurations ciapproximating the enforcement of a continuous reference uiprovided by a deadbeat abstract controller.
whenever a continuous knob kcassumes di erent values between two consecutive configurations in the sequence the transition between them is linearly smoothed over one or more steps for the continuous knob kc while the discrete knobs follow the original plan.
in practice assuming at time ithe plan requires a transition between the two configurations c iand ci a one step smoothing of kcwould replace this step change with a subsequence c i ci ci where the value of kcat time iiskic ki 1c ki 1c .
this refinement explore a new configuration for knob kc smoothing the discretization level.
this linear smoothing can be also extended over more steps.
however since no assumptions have been made on the actual function that is being discretized it is possible that the new values for kcmake the control plan deviate from its expected behavior e.g.
in presence of a nonlinear behavior around those values.
for this reason the length of the smoothed transition should be kept short when nonlinear behaviors are expected.
nonetheless the possible deviations are only transitory for the current actuation steps while providing additional information for the next control decision.
to avoid an unnecessary growth of the discretized configuration space for control knobs it is a good practice to introduce a maximum resolution threshold such that smoothing is only enforced when the di erence between two subsequent values of a continuous knob kcis larger than such threshold.
this threshold also bounds the error incurred through discretization of continuous knobs.
.
learning and runtime adaptation we have assumed the values di c for each dimension iand each valid configuration care known.
however in many cases these values might be unknown or subject to runtime changes.
in control theory updating the model parameters is called system identification.
we review some existing techniques and propose guidelines to find the best identification method for a specific problem.
a naive solution applies only a statistical estimator to learn and update each value di c depending on c. several such approaches have been proposed both in control theory and in software engineering .
despite its simplicity this approach is realistic only for small numbers of configurations because of the prohibitively large number of samples needed to guarantee convergence .
surrogate models approximate software behavior as a function of knob configurations.
examples are radial basis function spline models or gaussian processes such as the popular kriging models .
these models may require fewer samples for a suitable approximation the software behavior but their increased computational complexity may reduce the reaction time of the controllers.
whenever possible a computationally e cient parametric model is preferred.
such models define families of possible behav iors the objective of learning then reduces to finding the best parameter assignment to describe the behavior of the system with respect to each non functional requirement dimension.
several parametric models are already used in software engineering especially to reason about reliability and performance .
furthermore parametric models are usually easy to keep updated with online tracking mechanisms such as bayesian estimation kalman filtering or other techniques for statistical learning .
this has a twofold benefit the model tracks changes in the system and the quality of the estimates is continuously improved while the system is running overcoming possible inaccuracies in the information collected during an initial learning phase.
in the third case study on quality driven dynamic binding we fit and continuously update a parametric model based on incremental estimation and quasi montecarlo sampling.
.
discussion and formal assessment our methodology uses a control theoretic runtime decision engine to adapt a running application in response to unpredictable events.
this control theoretic approach allows formal analytical assessment.
such analysis is based on the assumption that software behavior is bounded i.e.
an uncontrolled application cannot continually increase or decrease its performance power consumption or output accuracy.
bounded also implies that every situation can be recovered.
this is not always the case with control strategy where a signal could indefinitively grow therefore leading to instability that cannot be addressed.
having bounded inputs and outputs simplifies the analysis and the formal assessment of the system.
we analyze here the properties mentioned in sec.
stability overshooting and settling time.
stability to study the convergence of the system the timebased quantities can be converted to their frequency domain counterparts using the z transform a frequency domain representation of a discrete time control signal.
in particular to assess the system stability we consider the closed loop system z transform and determine if the poles of said z transform lie in the unit circle .
the controlled system is composed of multiple cascade loops one for each dimension under control .
the first closed loop system controlling the lead dimension is stable by design.
in fact given the controller synthesis method we generate a deadbeat controller in control terms this means generating the controller that is less robust to noise but brings the desired signals as close as possible to their setpoints as fast as possible its z transform is z therefore there is only one pole at zero.
the analysis of the subsequent subordinate loop is more complicated since some of the signals depend on previous loops and are thus time varying.
in principle these loops should be analyzed as a switching system where some signals are rapidly changed from one value to another.
however it is possible to make the simplifying assumption that the loops corresponding to the dimensions controlled beforehand i.e.
the lead and the subsidiaries with higher priority have already stabilized to their goals.
in this case the analysis becomes straightforward since again the deadbeat nature of the control strategy guarantees a closed loop z transform of z. contains an exhaustive description of deadbeat controllers stability properties.
to guarantee that the loops for higher ranked dimensions are already stabilized the time constant subthat appears in eqn.
should be long enough.
more precisely sub where is the one used in eqn.
.
for each additional subordinate dimension the sampling time of the controller should be increased to twice the value of the previous loop.
this guarantees that the values set by the previous loop have already settled to their regime values.
the control signal of such systems is seen as a disturbance from the 18subsequent ones in the chain.
to guarantee the stability in face of disturbance one could do a robustness analysis and verify what is the maximum amount of change that the subordinate dimensions could tolerate.
the z transform function of the closed loop augmented with the disturbance has the same poles as the original one so the stability property is preserved whenever the goals are feasible.
the augmented settling time is therefore a su cient condition.
in principle it could be relaxed with a switching system analysis.
such analysis however is system dependent and unsuitable for an automated control strategy.
it is common in practice to select the sampling period of each loop multiples of one another so that the su cient condition for system stability is fulfilled.
overshoot the deadbeat controller converges to the set point without overshoot if the system operates under perfect information.
however short overshoots are expected because of transient disturbances that cannot be canceled without additional knowledge on the system s behavior e.g.
the e ects of an outlier reported by the monitors will be reduced by the integral action of the control though it may be too large to be fully compensated .
however the fast controller reaction the z transform has only one pole at zero guarantees overshoots are quickly acted upon and canceled by the control strategy .
indeed the experimental results in the following section show that the system can be found in overshoot conditions but they are promptly lowered by the controller action.
settling time the settling time of the closed loop system is by definition the settling time of the slowest loop in the system.
assuming the sampling strategy indicated above is employed to guarantee the system s stability sub the settling time of the overall system is given by 2n where nis the number of dimensions under control.
clearly one can select to be as small as possible for faster convergence.
however the choice of is limited by the fact that the control action taken at time tshould have a measurable e ect at time t .
this last assumption is unavoidably application dependent.
for example changing the distribution policy for a load balancer can be enforced in fraction of seconds while starting a virtual machine in the cloud may take a potentially unpredictable time.
has to be greater than the maximum actuation time in the application and may limit the applicability of the proposed control approach when faster reactions are desired.
including an online estimation procedure also may extend the time that the system needs to settle since the estimator settling time should also be taken into account before correct information about the system become available.
for every loop one should consider the time that the corresponding estimation strategy takes to converge as part of the total convergence time.
optimality of the translation the optimality of the translation from the continuous reference value to a sequence of discrete configurations is subject to several assumptions.
if the knobs are all discrete the only way to guarantee the optimality of the translation is to know the e ects of every possible configuration.
the exhaustive exploration of such finite configuration space is often infeasible and replaced by a systematic or randomized exploration of a smaller subspace as described in section .
.
this introduces an approximation of the optimal solution that should be taken into account when implementing a specific application.
this observation extends naturally to discretized continuous functions where the quality of the finite discretization may not capture all possible nonlinearities in the approximated continuous function.
these lack of information may lead to sub optimal plans though the stability of the system is not compromised since only known configurations will be enforced whose e ect are known.
the problem of discrete knowledge can be overcame when an analytical model of the system is available.
in such case the op timization problems can be straightforwardly restated taking into account the actual function relating knobs to goals however this esulates from the scope of this paper.
the use of online learning techniques introduce additional uncertainty.
indeed until the estimators converged the decisions might be transitory biased.
as side e ect a configuration might be not enforced on the base of wrong knowledge preventing the gathering of additional information and in turn the slower convergence of the estimators.
the initial learning phase can be leveraged to reduce this risk while at runtime it is possible force the periodic exploration of configurations that have not been visited for a while.
if a parametric model of the system is available the goal of learning and online updating moves to the estimation of unknown model parameters see sections .
.
in such case the risk of outdated or not converged estimates is compensated by the additional knowledge of the model structure.
knobs finally besides the possible feasibility limitations due to a bad prioritization schema discussed in section .
another limitation of the proposed strategy is that the number of controlled dimensions can never exceed the number of knobs available in the system.
notice that the controlled dimensions do not include the free one or possibly more than one if several dimensions can evaluated through a utility function that is used for the optimization in eqn.
.
.
experimental evaluation we present three case studies that evaluate our methodology and its ability to automatically manage multiple quantitative nonfunctional properties for a software system.
in our first study we create an adaptive encryption system for mobile communication.
in the second we present a cyberphysical managing a radar infrastructure required to provide timely and accurate target localization.
the final study deals with an adaptive web infrastructure which balances load between di erent heterogeneous servers o ering the same service.
for each study we describe the knobs used to control the system the dimensions managed and the results of deploying the adaptive software system to respond to dynamic events.
the secure mobile system responds to changes in user goals controlling the encryption algorithm settings the radar case manages both the hardware and the application to achieve the target localization quality and the load balancer operates at application level responding to changes in server reliability performance and cost.
the first case study has only discrete knobs while the others both continuous and discrete ones.
.
secure mobile communication mobile systems are a natural match for our methodology.
they are limited by battery life making energy is a primary concern.
further their applications are primarily interactive so predictable performance is essential.
in this scenario we add an additional non functional property security as users might want a certain level of privacy for their communication.
to accommodate such users we apply our methodology to create an adaptive system managing performance energy and security on a mobile device using the advanced encryption standard aes for privacy.
aes encodes bit data blocks using keys of size or bits.
we create an adaptive encryption system which dynamically selects the block size and processor frequency to manage performance energy and security.
controllable dimensions performance bit blocks encoded per second.
we measure this directly from the application.
.
.
.
.
.
time performance .
.
.
.
.
.
time energygoal adaptive software time average securityfigure secure mobile communication results normalized performance and energy consumption and average security over time.
energy joules per block.
energy is measured using the hardware performance counter on our test platform.
security average key size over blocks.
this value is read directly from the application.
knobs we manage two di erent knobs for this adaptive software application.
the first is based on the hardware and controls the energy and performance tradeo .
the second is based on the aes application and controls the performance and security tradeo .
the mobile system is a sony v aio svt11226cxb tablet with a dual core intel haswell processor.
the processor supports eleven clock speed settings ranging from 600mhz to 501ghz.
each system configuration shows di erent tradeo s between performance and power consumption.
here therefore the clock speed a ects performance and energy simultaneously.
our application is based on openaes .
it supports key sizes each with a di erent tradeo between security and performance.
the bit key size is the most secure and slowest.
the bit key size increases performance by and the bit key size is faster.
adapting to changing users needs we demonstrate how the security application constructed with our methodology responds to changing user goals.
fig.
shows the performance energy and security for our adaptive application over time where time is measured in the application progress i.e.
encoded blocks .
the performance and energy are normalized to the default setting i.e.
when the actuators have their default values.
we show the system reacting to a change in user goals.
this change represents a shift from operating on wall power where energy is not an issue to battery power where energy consumption becomes paramount .
initially the goal is to obtain real time performance represented by in the first plot of fig.
and maximum security which means using a key size of bits .
at time we switch from wall power to battery and set new goals the application should still maintain real time performance but reduce energy consumption to of the default value increasing battery life by .
the areas corresponding to the two di erent goals are marked in the plot by a white background a striped pattern background.
notice that we are using two knobs to control two dimensions while optimizing in the third one.
the initial goals are highest security and real time performance and they switch to the same performance with less energy consumption.
as shown in the figure the performance goal is maintained throughout the application s execution with some minor disturbances due to random system activity.
when at block the goals change the control system makes energy consumption immediately drop to the desired value.
after a minor degradation performance quickly returns to real time by reducing security.
.
radar signal processing radar signal processing is another example application which must balance the competing demands of multiple quantitative nonfunctional properties this time in a cyberphysical system.
signal processing applications are composed of individual kernels each of which might support a tradeo between the computational complexity of the kernel and the accuracy of the result.
the accuracy of the radar i.e.
its ability to detect targets in noise will be a function of the composition of the accuracy of these individual kernels.
as with our previous examples energy consumption is a major concern in many platforms.
for example in an autonomous vehicle the vehicle s range will be determined by its energy consumption.
a fielded radar system will have goals in all of these dimensions performance ensures that targets are detected in time accuracy ensures targets are detected in noise and energy determines mission lifetime.
this case study demonstrates the methodology proposed in this paper but here accuracy performance tradeo s are determined by a combination of continuous and discrete knobs.
controllable dimensions performance radar pulses processed per second.
this value is measured directly from the application.
power watts per pulse.
this is measured using a hardware power meter available on our test platform.
accuracy signal to noise ratio for detected targets.
the application reports these values.
knobs we use our methodology to control several di erent knobs at both the system and application level.
our hardware platform is a core intel xeon e5 processor running linux .
.
.
the processor supports hyper threading and has di erent speeds including turboboost.
we read total system power from a wattsup power meter.
like our prior example there are three system level knobs that alter the performance and energy tradeo s. the first uses the cpufrequtils package to control clockspeed the highest setting actually turns control over to the hardware by enabling turboboost .
higher clockspeeds increase performance at the cost of increased power consumption.
the second adaptation uses thread a nity to reduce the number of cores actively performing computation.
the processor supports power gating so reducing core usage will reduce both power consumption and performance.
the final adaptation is to idle the processor to take advantage of the low power idle state.
this adaptation supports racing to idle where the system attempts to complete work as quickly as possible and maximize idle time.
idling will not increase power consumption but can decrease it for a decrease in performance.
the total number of system configurations is counting the use of idle states the number of configurations is e ectively infinite .
.
.
.
time pulse latency s .
.
.
.
.
time normalized signal to noise ratioproposed approach application only system only time power watts figure radar results performance energy consumption and accuracy over time.
we use an existing radar benchmark which represents a generic processing chain for a phased array sensor .
the radar supports application level tradeo s that can reduce signal to noise ratio accuracy in exchange for increased performance.
there are a number of parameters a ecting the radar s accuracy performance tradeo s. two of these are discrete parameters the strength of an initial lowpass filter which has settings and the number of beams number of directions to search simultaneously which can be set from to in multiples of .
the third parameter is e ectively continuous.
it is the number of range bins the resolution in distance from the radar .
this value can be anywhere from to .
while it is possible to model this as a discrete variable with settings it is impractical so we treat it as a continuous variable.
the total number of possible configurations considering both application and system is greater than billion.
it is simply impossible to build empirical models of every possible combination of knob settings.
thus this case study further tests our methodology s ability to approximate the e ect control of one property will have on another learning is indeed limited to a grid sampling of million configurations at runtime thanks to the analysis of the dual optimization problem described in the search space is reduced to just thousand reducing the runtime computational overhead .
adapting to changing mission requirements to demonstrate the benefit of our proposed approach we consider an autonomous vehicle in the field performing radar processing.
during its mission the vehicle gets a change in requirements.
initially the system was deployed with a latency and accuracy goal it was to meet a target latency 20th of a second per radar pulse and maximize the signal to noise ratio.
halfway through this mission the vehicle receives a new set of goals it must still maintain the same latency but now has to increase its mission lifetime reducing power to w .
to demonstrate the power of this paper s proposed approach we compare to two other approaches.
the first adapts only the application.
the second adapts only the system.
we measure the performance as latency power and accuracy for each of these three approaches.
the results are shown in fig.
.
the left chart shows latency the middle shows power and the right shows accuracy.
the x axis of each chart shows time measured in radar pulses.
the results demonstrate the power of our technique.
the systemlevel approach reduces power but exceeds the target latency by almost .
the application level approach meets the target latency and power but sacrifices accuracy reducing signal to noise ratio to just of the default.
the approach in this paper provides the best outcome it meets the latency and power goals but when the mission changes it reduces signal to noise ratio to of the default a considerable savings over the the application level approach.
.
multi objective service dynamic binding our final example is the problem of multi objective dynamic binding in the context of service oriented architecture soa .
dynamic binding flexibly assigns abstract service interfaces to concrete implementations possibly provided by third parties and is one of the principal means to adapt the behavior of soas .
in this example an abstract service interface can delegate an incoming request to one out of three third party services.
although functionally equivalent each of the three services provides a distinct reliability and performance depending on the paid service level agreement.
the measurable performance depends on the service level and external factors including the underlying communication and execution infrastructure and the workloads of the three services.
the controller selects which service should process any incoming request and for each service the service level for each time step.
the controller is synthesized to achieve the desired reliability and performance while minimizing the cost due the selection of higher service levels.
we considered the problem of dynamic binding in previous work controlling only a single objective with a single knob .
here we provide a solution to the multi objective problem.
controllable dimensions reliability the probability of processing an request without exceptions.
we estimate this from the counts of forwarded requests and thrown exceptions each time step.
performance average response time.
the binder measures its perceived end to end service time for each forwarded request and averages over a time step.
cost depending on the service level requested for the three services.
each service reports this through an api.
knobs the controller can decide a service level from to for each of the three services and the distribution of the incoming requests by setting two probabilities p1andp2.
in particular the probability of selecting the first service is p1 while the second service has probability p1 p2 and consequently the third service is selected with probability p1 p2 .
the domain of both p1andp2is continuous and incrementally discretized up to a maximum resolution of leading to at most configurations to be potentially explored .
each service si has a measurable reliability ri performance coe cient ti and cost coe cient ci.
also the controller can decide a service level li.
for every request a fair coin is flipped in our implementation to decide whether it will raise an exception or not according to pi the response time for each request is sampled from an exponential distribution with mean ti l2 i i.e.
the time required to process the request is an inverse quadratic function of the service level the cost of processing an incoming request is ci li.
.
.
.
.
.
time reliability time performancegoal adaptive software time costfigure service dynamic binding results reliability performance and cost over time.
the nominal values ri ti and ciare not known by the controller which can only measure the time it takes to process a request it forwards to si how many of such requests are successful or failed and how much it cost to process each request.
in order to quantify the values dj c needed to decide the control actions the controller undergoes an initial learning phase where the configuration space is explored to fit a parametric model of the system behavior.
the two problems to undertake are thus two what could be a suitable parametric model to fit and how to sample the configuration space since its exhaustive exploration may be unpractical.
for the first problem by reasoning on the structure of the system we can identify a simple polynomial parametric family of models to describe the possible behaviors of the system p1 q1 p1 p2 q2 p2 q3 .
the values of p1andp2are set by the controller and therefore known while qiis a placeholder for the estimators of the values dj jin reliability performance cost when the service operates at level li.
fitting this parametric model requires only values to be estimated for each service three dimensions times five service levels for a total of estimates.
the exploration of the configuration space is performed in two phases.
in the first phase the baseline configuration is thoroughly assessed.
in the second phase a systematic exploration of the configuration space is performed through a quasi montecarlo sampling based on the halton sequence a low discrepancy sequence with demonstrated e ectiveness for multidimensional spaces the exploration of the extreme values for each knob s domain are manually added to the sample set to span the learning over the full control domain .
the use of the halton sequence may increase the convergence of the estimators for the qualities qiup to be linear in the number of samples.
in practice with only samples reasonably good initial estimates for dj c have been achieved.
to overcome possible inaccuracy of the initial estimation and to deal with changes of the services behavior during runtime the initial estimates are kept updated during the control phase i.e.
at each time step a new sample is pushed to the estimators and used to continuously refine the estimate.
the estimator we use here has been proposed in and allows for the incremental estimation of both the mean and the variance of each parameter qi.
each estimator requires only three floating point values to be stored and a few arithmetic operations to update the estimate after each new sample.
furthermore given the variance of the parameters is estimated too a simple change point detection mechanism can be implemented based on the frequency of outliers in the new samples e.g.
those lying further than ntimes the standard deviation away from the mean .
when such frequency gets too high it is possible that the system undergone an abrupt change which invalidates the current model and requires a new learning phase .the results of this experiment are reported in fig.
.
the setting for these experiment is r1 t1 c1 r2 t2 c2 r3 t3 and c3 .
on the three plots the setpoint for the reliability leading dimension and the performance subsidiary are represented by a dashed black line while the obtained quality is in a red continuous line.
the initial learning phase is marked with a striped pattern.
the control starts from time and achieves both the goals though it takes some e ort to keep performance to the setpoint given the required reliability.
at time the goal for reliability is reduced both the goals are still feasible but the performance can be achieved more smoothly.
at time the setpoint for reliability is changed to an infeasible goal the controller therefore approaches the goal being as close as possible performance is instead achievable.
at time the required performance is stressed more the goal is still achievable though at an higher cost i.e.
higher service levels are required .
finally at time the goal for reliability is raised to a higher value both reliability and performance are achievable though the cost is much higher than before.
.
conclusion and future work in this paper we proposed an automated control strategy to achieve multiple objectives using the many knobs available in production software systems.
to take advantage of the formal guarantees that the proposed methodology o ers users need only identify a set of knobs and the maximum timescale of all the knobs that belong to the set e.g.
the time it takes to change the processor frequency .
with this information our methodology automatically devises a control strategy composed of multiple connected loops which guarantees whenever feasible all the chosen dimensions and optimizes the last free one.
we have shown three case studies introducing the features of our system in order of increasing complexity.
the fist case study assumes perfect knowledge of the actions that can be taken on the system and all the involved dimensions while more uncertainty is introduced in the following two.
in the last case study we have shown how the system deals with infeasible goals.
this work advances the state of the art on automated control strategies for software systems moving existing e orts much closer to what is necessary in a real system.
.