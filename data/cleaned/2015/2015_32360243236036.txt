complementing global and local contexts in representing api descriptions to improve api retrieval tasks thanh nguyen iowa state univ.
usa thanhng iastate.edungoc tran univ.
of texas dallas usa ngoctran utdallas.eduhung phan iowa state univ.
usa hungphd iastate.edutrong nguyen iowa state univ.
usa trong iastate.edu linh truong univ.
of texas dallas usa linh.h.truong utdallas.eduanh tuan nguyen axon corp usa ntanhbk44 gmail.comhoan anh nguyen iowa state univ.
usa hoan iastate.edutien n. nguyen univ.
of texas dallas usa tnn160630 utdallas.edu abstract when being trained on api documentation and tutorials word 2vec produces vector representations to estimate the relevance between texts and api elements.
however existing word 2vec based approaches to measure document similarities aggregate word 2vec vectors of individual words or apis to build the representation of a document as if the words are independent.
thus the semantics of api descriptions or code fragments are not well represented.
in this work we introduce d2vec a new model that fits with api documentation better than word 2vec.d2vec is a neural network model that considers two complementary contexts to better capture the semantics of api documentation.
we first connect the global context of the current api topic under description to all text phrases within the description of that api.
second the local orders of words and apis in the text phrases are maintained in computing the vector representations for the apis.
we conducted an experiment to verify two intrinsic properties of d2vec s vectors similar words and relevant api elements are projected into nearby locations and some vector operations carry semantics.
we demonstrate the usefulness and good performance of d2vec in three applications api code search text to code retrieval api tutorial fragment search code to text retrieval and mining api mappings between software libraries code to code retrieval .
finally we provide actionable insights and implications for researchers in using our model in other applications with other types of documents.
ccs concepts software and its engineering software maintenance tools keywords word 2vec big code api documents code search api mappings acm reference format thanh nguyen ngoc tran hung phan trong nguyen linh truong anh tuan nguyen hoan anh nguyen and tien n. nguyen.
.
complementing global and local contexts in representing api descriptions to improve permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa association for computing machinery.
acm isbn .
.
.
.
retrieval tasks.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
introduction an important class of software engineering se tasks is centered around the relevance between text descriptions and source code .
an se application is text to code retrieval e.g.
helping developers find code examples from a given textual query .
another se application in the other direction with code to text retrieval is to automatically search relevant textual fragments in an api tutorial for a given api query .
in general a wide range of other se applications also involves text and code relevance including bug localization software traceability code summarization bug triaging documentation processing and naming convention .
the advances in natural language processing nlp have been applied to se documents and have achieved much success .
one of the advanced techniques that have gained much popularity is word 2vec .
after being trained on a large corpus of texts it is capable of projecting terms into a continuous vector space in which the semantically related terms are mapped to nearby locations thus it can capture the relevance between texts and apis .
specifically word 2vec is run on api documentation and tutorials of software libraries to build the vector representations called embeddings for the english terms and api elements.
since documentation and tutorials contain both texts and api elements word 2vec is able to project terms and api elements in a shared vector space .
for example the api string.substring is mapped to a nearby location with the terms used to describe that api.
then semantic similarities between two documents are estimated via a weighted sum of the similarities between the words in those documents.
such similarity is used to improve accuracy in code retrieval from a text and in bug localization between bug reports and files .
despite the successes in those se applications those retrieval approaches with word 2vec based similarity feature face a key issue in using the word 2vec vectors of individual words in the documents to estimate document similarities .
the vectors of the words are aggregated as if the words in the same document are treated as independent.
in source code the program dependencies among api elements are important as different dependencies might result in different semantics of a code fragment.
in textual documents treating their words as independent without considering esec fse november lake buena vista fl usa nguyen tran phan nguyen truong nguyen nguyen and nguyen word orders will not capture well the semantics of the documents leading to imprecise estimation of document similarities.
in those approaches different sentences can have exactly the same representation if the same words are used.
moreover with the continuous bags of words cbow architecture of word 2vec on api documentation an api is represented by a vector that is encoded with the bags of surrounding words being used in the description of the api.
however in many cases e.g.
for the closely semantically related apis in the same class we need a representation that can precisely capture the sentences in the api s description so that the functionality of each api can be better represented.
in this paper we conjecture that we need a new model that best fits with api documentation and se documents.
we present d2vec a neural network model that considers two complementary contexts to better capture the semantics of api descriptions.
first of all instead of using bags of words we realize the local context in which our unsupervised algorithm learns the vector for an api by training on the description of the api to predict the next word given its previous words and the api topic under description .
that treatment allows the model to consider the local orders of words and related apis in the description.
second at the same time the topic of the api under description plays the role of the global context that repeatedly connects the api topic to each of the text phrases in the description in the training windows and the topic serves as a memory of the current topic for all the phrases.
in comparison in the existing approaches the word 2vec vectors are computed in a separate step and then aggregated to compute document similarity in which no word order is considered.
moreover while existing models represent an api by the vector corresponding to the api within the text descriptions in our model the vector for the api is the vector for the api topic which is aimed to capture the entire description of the api.
our model is trained to maximize the likelihood of seeing all text phrases in the description of that api as the api topic is attached to all those phrases during training.
we conducted an experiment to verify the intrinsic properties of the vectors produced by d2vec .
similar to word 2vec d2vec s vectors have the following properties similar words and relevant api elements e.g.
wordutils.capitalize and stringutils.uppercase are projected into nearby locations in the vector space some vector operations carry meaning.
for instance the vector v append v stringwriter.append v string.concat is closest to the vector of the word concatenate where v denotes a vector and the and signs denote the vector subtraction and addition respectively.
the interpretation of this operation is that append explains the functionality of the api stringwriter.append thus concatenate can be used to describe the functionality of the api string.concat .
we also showed the performance of d2vec on three se applications.
the first one is api example code search text to code retrieval in which our goal is to retrieve the api code examples given an english query.
the retrieval accuracy on a dataset of code examples from kodejava a tutorial site is .
.
with one to five suggestions.
compared to word 2vec d2vec achieves .
and .
higher for top and top accuracies respectively.
another se application is code to text retrieval in which a fragment of an api tutorial is retrieved for a given api as a query.
our experiments on five publicly open tutorials show that with d2vec the retrieval is improved over word 2vec by up to in f score.
... ...wtwtwt c wt cwt c wt ccbow skip gramoutputinput inputoutputfigure word 2vec with two different architectures we also evaluate d2vec s in the application of mining the mappings between apis of two libraries jdk and apache code tocode retrieval .
for this we rely on the principle that two corresponding apis with the same similar functionalities in two libraries are explained in similar descriptions.
thus they are mapped to nearby locations in the vector space.
that is d2vec s vectors can be used to relate two apis in two libraries since they capture the respective descriptions.
in deriving the mappings for most common jdk apis the accuracy with a single suggestion is .
finally we provide actionable insights on using d2vec in the context of other documents e.g.
source code in other applications.
in this paper we make the following key contributions d2vec that is adapted to fit with api documentation to produce better representations for apis than existing approaches the idea of using a topic keyword imposing on all phrases that are semantically relevant to that topic to represent better the content of those phrases and its applications in other se tasks three applications of d2vec in text to code retrieval api example code search code to text retrieval api tutorial search and code to code retrieval for the api mappings in two libraries.
word 2vec for api documentation .
background on word 2vec among the models proposed to estimate the vector representations of the words in natural language texts word 2vec has recently attracted much attention.
mikolov et al.
introduce two models named continuous bag of words cbow and skipgram figure to learn the vector representations for the words from text corpus.
generally cbow predicts a center word given other words in a context via a neural network that takes the context words as input and produces prediction probabilities for every target word in the vocabulary v. to do that cbow represents each input word as a n dimensional vector via a matrix w v nsuch that the k th row of w corresponds to the vector representation of k th word in v. from the input vectors the hidden layer output hcan be computed by summing these vectors and another matrix w n v in the second layer is used to compute the ultimate probabilities in the output.
for skip gram its input is the center word of a window and the output is the surrounding ones.
the task can be considered as predicting the context given a word.
in addition more distant words are given less weight by randomly sampling.
552complementing global and local contexts in representing ... esec fse november lake buena vista fl usa word 2vec model is trained with stochastic gradient descent over a large collection of texts.
the learned vector representations have two key properties.
first words that are semantically similar have vectors closer to each other in the embedding space.
for example powerful and strong are closer than each to the vector of paris .
additionally the difference between word vectors also contains the information on semantic relations.
for example v king v man v queen v woman where vis word2vec and the minus sign denotes vector substraction operation.
.
word 2vec for embedded api elements yeet al.
apply word 2vec with skip gram architecture on api documentation and tutorials to learn the vector representations of english terms and api elements including api classes and methods.
they treat the api elements embedded within the texts of documentation as special words.
the vectors computed for these special words after training are used as the vectors for api elements .
furthermore for api documentation they add the fully qualified name of each api to the beginning of the description for that api and then run word 2vec on all the descriptions.
the relevance between api elements and regular words can be considered as parts of the semantic relations among two words which is successfully captured by word 2vec.
relevant api elements and words often occur with similar words and related api elements called surrounding contexts in the texts of the api documentation.
those words and code elements are expected to have nearby representations.
let us take as an example of the api stringbuilder.substring int int in figure .
the documentation concisely describes the functionality of stringbuilder.substring .
the sentences and the phrases subsequence of characters substring contained in this sequence etc.
in the description explain the functionality of stringbuilder.substring as well as the relation to the api string.string .
a typical characteristic of api documentation that makes the word 2vec model appealing to our problem is the mixture of api elements and english words in the api descriptions.
the fully qualified name for the api java.lang.stringbuilder.substring is placed at the start of its description and word 2vec model is trained on all the api descriptions to build the embeddings vectors for the words and the apis embedded within the texts .
the semantic similarities between two documents are then estimated via a weighted sum of similarities between their words in which no word order is considered .
3d2vec model for api documentation .
the d2vec model ind2vec for each api in an api documentation we aim to build a vector for the textual description of that api.
the vector for its description is used to represent the api itself.
we aim to capture the semantics of the description of the api as well as keep the word orders in that resulting vector to reflect the description better than the bags of surrounding words.
at the same time we want to associate the api under description with the semantics of its actual description.
that is we aim to have the current api under description as a global topic for all the phrases within that description.
to achieve that we use two complementary types of context in our model.
first we use the api topic as the global context for all the phrases within the description of the api.
specifically we add stringbuilder.substringreturns a new string.string that contains a subsequence of characters currently contained in this sequence.
the substring begins at the specified start and extends to the character at index end .
api topic keywordembedded api code api documentationfigure documentation of jdk api stringbuilder.substring an id for an api to each of the windows of texts in the description of that api see figure .
the ids are uniquely assigned for all the apis of interest in the api documentation in the corpus.
the id token is a special word that acts as memory remembering what is missing from the current context window of words i.e.
thetopic of the description .
the api id vector is shared across all context windows generated from the description for the same api but not across the descriptions for other apis.
after training the vector corresponding to an api id is used as the vector embedding for the api itself.
in other words d2vec learns vector representations from contextual information in sentences as well as incorporating thetopic of the description which is the current api element under description .
the topic which is the api under description is always connected to all the phrases windows in the description and we use it to enrich the representations.
that topic connects those words in the same description with the same semantic which is the api under description.
thus the topic is expected to help the model capture the semantics of the api description .
second we use the previous words in the description as the local context to preserve word orders.
while the semantics of a word in a description is captured in its vector as in word 2vec an api is represented by the vector for its description considering the word order in small windows thus preserving word orders locally .
the vectors for the words in the descriptions are computed as regular embeddings in the traditional word 2vec.
the embedded apis are replaced with the corresponding api ids.
both the topic as the global context and the previous words as the local context provide the clues for the prediction of the next word in all the windows in the description of the current api.
that is the api id vector and the vectors of the words apis in the descriptions are combined to predict the next word in a context window.
the vector for the api id representing its entire description can be used as features for the api in retrieval algorithms.
.
architecture figure displays the architecture.
mathematically given a descriptionw1 w2 .. wtof the api a we maximize the log probability l t t c i clogp wi wi c wi c ... wi a the conditional probability p wi wi c ... wi a and the prediction task are modeled via a multiclass classifier with the network in figure and a softmax function.
the word prediction task considers the given local context and the global topic on which this context covers.
each of yifor each output word iis un normalized 553esec fse november lake buena vista fl usa nguyen tran phan nguyen truong nguyen nguyen and nguyen ...concatenate input classifier context words in a window api topic idthe next word wi a wi c wi c wi wi figure d2vec model to learn vector representations for words and api elements from api documentation log probability and is computed as y b uh a a wi c ... wi w where uandbare the softmax parameters.
we use wto denote the word matrix being formed by putting together all word vectors as its columns.
ais called the api id matrix that is formed by putting together all unique vectors for all the api ids.
the hfunction is constructed by a concatenation of the word vectors extracted from the matrix wand the api id vectors extracted from the matrix a. note that the word orders in wiare considered.
training for d2vec is done with stochastic gradient descent similarly to pvdm s and word 2vec s .
at each step of stochastic gradient descent a fixed length context is randomly sampled from a description.
we then compute the error gradient from the network and use the gradient to update the model s parameter.
at prediction time for a new text the inference step is also done via gradient descent where the rest of the parameters of the model e.g.
all the word vectors and the softmax weights are fixed.
d2vec learns vector representations with two key advantages it enriches information of the api elements that do not occur within the documentation descriptions of other api elements the api elements with similar descriptions are likely to be close to each other in the vector space.
properties from word 2vec such as the vector offsetting still hold for d2vec will be explained later .
.
approach applicability and implications in this work the topic is the api under description and the training data is textual documentation.
however the direction that we put forth here is applicable to other software engineering tasks related to textual documentation and source code.
let us explain how we would extend the idea of topic for other se tasks.
.
.
representation for a code fragment with an addition of a topic keyword.
if we apply our idea to source code and inline comments we could view the text comments as the topics for the source code fragments relevant to those comments .
if we extract keywords or topic words from the comments and use d2vec on source code we could learn the vector representations that capture the entire code fragments as well as the relations among the words in the comments and code elements.
for example in figure we could use the inline comment converts back the list into array object and prints the new values as the text relevant to the code below it.
the training data is the set of source files that have high quality comments for source code.
we could first extract the keywords in thecomment such as convert list array object print value and use them as the topics for the code elements in the code fragment below the comment.
we train a model to predict the next code token or api in that code fragment.
as a consequence the model will be able to capture the sequence of code or api elements that are relevant to the topics keywords.
in brief in this representation the topics are english keywords to describe the semantics for the sequence of code tokens or api elements in a code fragment.
an application of such a model is api code search section a user gives a query or a keyword the model will rank the list of code fragments depending on their relevancy levels to the query.
.
.
applying d2vec on bug localization with lda.
let us discuss d2vec in the context of bug localization in which the text in a bug report is given and the model will rank the source files relevant to the report for the debugging purpose.
in bug localization the topics could be built from the bug reports by extracting keywords or using topic models such as lda .
we could use the bug reports and the corresponding fixed files as training data.
finally the other similar se tasks include software traceability from texts to code and code summarization summarizing source code with texts .
.
.
database of vector representations for apis as building blocks.
for software libraries their apis could be described in many sources api documentation tutorials online discussions inline comments and their source code.
for each kind of documents we could train d2vec to produce the representations for all the apis of the libraries of interest.
for example we could produce the vectors for string.substring by running d2vec on jdk documentation for that api running on kodejava tutorial and on its source code in jdk.
an interesting research question is to compare and combine such the distributed vector representations of the apis to achieve a better retrieval mechanism for se applications.
for example the vectors produced by running on api documentation would have different characteristics with the vectors on source code.
however they both capture the semantics relevance of the api of interest to the text description and its implementation source code.
one could expect to exploit those two types of vector spaces to support the se tasks related to linking api documents and source code.
research questions and dataset .
research questions we conducted several experiments to evaluate d2vec in se applications that involve text to code retrieval code to text retrieval and code to code mappings.
we focus on the following questions rq1.
how does d2vec compare or help improve a baseline retrieval model in the task of api code search text to code retrieval ?
rq2.
how well does d2vec perform in the task of searching tutorial fragments relevant to a given api code to text retrieval ?
rq3.
how well does d2vec perform in an application of finding the mappings for the apis in two libraries code to code retrieval ?
.
training corpus we collected a dataset of the api documentation of java jdk core and apache commons libraries we call them jdk and apache for short to train the models to learn the vector representations of words and api elements.
table shows the dataset s statistics.
554complementing global and local contexts in representing ... esec fse november lake buena vista fl usa table jdk and apache training dataset number of distinct api elements number of distinct english words occurrence frequency of apis .
occurrence frequency of english words .
vocabulary size number of apis embedded within a documentation .
in addition to getting textual descriptions a non trivial task of building training data is to identify and annotate api code embedded within the descriptions.
fortunately jdk and apache opensource files are available and javadoc style comments can be parsed using the java development toolkit jdt to accomplish the task.
the procedure for data collection was as follows.
we first collected java source files of both jdk and apache libraries.
then we parsed each source file with jdt to obtain the declarations i.e.
api fields methods classes and javadoc style comments.
these documentation comments are stored in ast nodes that were accordingly traversed using some specific tags such as code param return to identify api elements.
after that we annotated the identified api code elements with their packages and class names e.g.
java.lang.stringbuilder.append if fully qualified names are available from traversing the ast nodes otherwise we utilized a dictionary of jdk and apache apis.
we considered all annotated apis as special api ids in the documentation and added them to a vocabulary for training the models.
for d2vec we also labeled the descriptions with their own apis i.e.
the topic words .
we removed stopwords and stemmed the remaining words.
note that for the traditional word 2vec the fully qualified name of an api is added to the beginning of its text description and the model was run on all the descriptions.
for d2vec an api id is added to all the context windows for all phrases of the description of the api.
finally we have the api documentation corpus with distinct apis and distinct english words .
on average each api occurs .
times in the entire corpus and each documentation contains .
api elements.
the occurrence frequency for english word is .
.
compared with the english dictionary the number of words in this dataset is much smaller.
thus this relatively high regularity suggests that the dataset would be helpful in learning vector representations via neural network based models.
rq1.
api example code search this section presents an evaluation of our model on an important text to code retrieval application api example code search given an english query a tool returns the code examples in a codebase that are semantically relevant the most to the query.
we used the dataset in table to train d2vec to obtain the vectors for the apis.
for an example we extracted the sequence of api elements and used the trained model to build the vector for that sequence to represent the code example.
for the query we used our trained model to run on the words of the query to produce the vector representation for it.
then we used cosine similarity scores between two documents to rank the api code examples for a given query.
we also compared our model against the word 2vec skip gram model used in the work in ye et al.
and the revised vector space model rvsm .
we chose the first one because it is atable kodejava test dataset for retrieval number of api code examples number of unique words number of unique apis word occurrence frequency .
api occurrence frequency .
average length of queries .
average length of code examples .
create s an array of integ er value and print s the origina l value s. integer numbers new integ er syste m.out.println origina l numbers arrays.t ostring number s create s an arrayl ist object and add the entire conten t of number s array into the lis t. we use the add in dex elemen t to add elemen t at index .
list integer list new arrayli st list.add all arrays.aslist n umbe rs list.add convert s back the list into array object and prints the new values.
number s list.toarray new integer syste m.out.println afte r ins e r t arrays.to string num bers title how can i insert an element in array at a given position?
integer .integer integer .integer arrays.tostring printstream.println list.listarrays.aslist list.addalllist.add list.size integer .integer list.toarray arrays.tostring printstream.printlntitle code example extracted apis figure an api code example with query from kodejava state of the art neural network based model while the second one is an advanced vector representation model that has been shown to outperform other traditional information retrieval models .
for word 2vec the vectors for a query or a code example are computed by aggregating the vectors of all the words in the query or all the apis in the example in the same way as explained in their paper.
for rvsm a query and a code example are represented by the tf idf vectors considering the words and the apis in the example.
.
data collection .
.
training data for retrieval.
for this experiment we built training data to include only the documentation of jdk apis in table .
finally we have the dataset with unique api elements and english words.
the occurrence frequencies for an api and a word are .
and .
respectively.
.
.
test data for retrieval.
to evaluate the performance of our model we collected a parallel dataset of queries and api code examples from a java tutorial website named kodejava .
each kodejava post contains a title that describes a programming task and the jdk api code fragment s to fulfill that task.
for code retrieval we utilized the title and the code fragment with the highest score as a ground truth.
an example is shown in figure in which a developer places a query on how to insert an element in an array given a specified position.
for retrieval we processed code examples to extract api elements using the jdt parser figure .
finally our dataset contains pairs of titles and corresponding sequences of api code elements from the corresponding examples.
.
metrics and setting ford2vec and word 2vec we set the context window size c and the dimensionality n 200to learn the vectors.
for each query we rank all the code examples.
if the correct example occurs in the 555esec fse november lake buena vista fl usa nguyen tran phan nguyen truong nguyen nguyen and nguyen table accuracy comparison on api code search top top top top top word2vec .
.
.
.
.
d2vec .
.
.
.
.
revised vector space model rvsm .
.
.
.
.
rvsm word2vec .
.
.
.
.
rvsm d2vec .
.
.
.
.
table example results of api code search and ranks q question d2vec d2vec rvsm a expected apis rvsm q breaks a text or sentence into words?
a locale breakiterator.getwordinst... breakiterator.settext q insert an element in array at a given position?
a list.list arrays.aslist list.addall list.add list.toarray... q get the maximum number of concurrent connections?
a connection.getmetadata database...getmaxconnections q launch user default web browser?
a uri.create desktop.getdesktop desktop.browse... top klist of the retrieved examples we count it as correct otherwise it is a miss.
top kaccuracy for api code search is computed as the ratio of the number of hits over the total number of cases.
.
empirical results and analysis table shows the top ranked accuracies on code retrieval for different approaches.
first the top and top accuracies of word 2vec andd2vec are .
.
and .
.
respectively.
as seen d2vec improves over word 2vec .
relatively and .
.
relatively in top kaccuracies.
to further study d2vec s ability to connect queries and code examples that are lexically mismatched we combine it with an advanced vector space model rvsm .
the relevance score is a linear combination of the two scores from d2vec and rvsm rvsm q c d2vec q c .
we adjust the parameter based on the jaccard similarity between the query qand the code example c. if jaccard score is greater than the threshold of .
we use favoring rvsm .
otherwise 0since we expect d2vec to complement well for the lexical mismatch cases.
all scores are normalized over all examples.
as seen in table the combined approach improves .
.
top and .
.
top accuracies over the individual approach in rvsm and d2vec .
in comparison to rvsm d2vec performs slightly better because it handles better the cases of lexical mismatches between the queries and source code.
table shows a few examples with lexical mismatches.
for instance for the query insert an element in array at a given position d2vec andd2vec rvsm retrieve the correct code example at high positions.
rvsm ranked it at the 22ndplace due to the mismatch of insert and list.add .
among examples that were ranked first by d2vec cases were not retrieved by rvsm in the first results due to low textual similarities between queries and code examples.
thus d2vec complements well to rvsm in the cases with low textual similarities between a query and source code.
finally the combined model rvsm d2vec outperforms rvsm word 2vec.
thus d2vec helps improve better the baseline model rvsm than word 2vec based similarity measure in ye et al.
.
timefile datecolorrequest locale.localecolo r.color countryscreen headerzip httpcompress file.file degreezipfile.zipfile read radian logarithm locale.getdisplaycountryurl.openconnection pixelurlconnection.urlconnection responsecolo r.getredmath.log math.todegrees math.toradians robot.getpixelcolor urlconnection.getheaderfieldszipentr y.getcompressedsize zipfile.entries .
.
.
.
.
.
.
.
.
.
.
.
.
.
.8y xfigure d visualization of api and word vectors via pca table example of relations via vector offsets in jdk r1.
get the next element stringtokenizer.hasmoretokens stringtokenizer.nexttoken scanner.hasnext scanner.next hashmap.containskey hashmap.get stack.isempty stack.pop r2.
check the size of a collection before getting an element java.util.arraylist.size java.util.arraylist.get java.util.map.size java.util.map.get r3.
insert an element java.util.vector java.util.vector.add java.util.treemap java.util.treemap.put r4.
check before adding an element linkedlist.contains linkedlist.add hashtable.containskey hashtable.put set.contains set.add r5.
write to and flush a stream fileoutputstream.write fileoutputstream.flush printwriter.write printwriter.flush .
properties of vector representations .
.
nearby vectors of words and related api.
we conducted another experiment to examine the arrangements of the vectors for english words and related api elements.
we selected english words and api elements from six retrieval ground truth pairs of queries and code examples from kodejava dataset .
then we projected their vectors onto the dimensional space for visualization with principal component analysis pca figure .
as seen english words and api elements that are grouped into nearby locations in the vector space belong to the same ground truth.
for example the words zip compress and file are clustered more closely to file.file zipfile.entries zipentry.getcompressedsizethan to the other elements.
similarly radian and degree are relatively close to math.toradians and math.todegrees .
in brief d2vec projects the words and relevant api elements into nearby locations in the vector space.
thus it allows our tool to highly rank the code examples with the apis relevant to the query s terms.
.
.
nearby vectors of related apis.
we observed that the apis with related functionalities are also projected to nearby locations.
those apis are often described surrounded by similar words thus 556complementing global and local contexts in representing ... esec fse november lake buena vista fl usa they have nearby vectors.
for example outputstreamwriter write printstream printf bufferedwriter flush have nearby vectors.
as an implication we could use the vector space to find relevant apis.
.
.
vector offsets and relations between words and api elements.
vector offset operations hold true to the d2vec vector representation.
for instance v read v bufferedreader.read v bufferedwriter.write becomes a vector closest to the vector representation of the word write .
the interpretation of this operation is that the word read explains the functionality of bufferedreader.read thus write could be used to describe bufferedrwriter.write .
another example is v append v stringbuilder.append v concatenate v string.concat .
that is this operation captures a word being used to describe the behavior of an api element .
.
.
vector offsets and relations between apis.
we observed that vector offsets could explain the relations between apis.
for example v scanner.hasnext v scanner.scanner v stringtokenizer.stringtokenizer is close to the vector of stringtokenizer.nexttoken .
that is using hasnext on a scanner has the same meaning as using nexttoken on a stringtokenizer .
using our knowledge on jdk apis and our pca visualization not shown we have observed several relations via the vector offset operation .
for example some pairs have different names for the same function r4 in table .
rq2.
searching relevant fragments in api tutorials in this experiment we evaluate d2vec on a code to text retrieval task retrieving a text fragment in a tutorial of a library that is relevant to a given api.
learning to properly use api is important.
a practical way is to learn from a tutorial.
however such a tutorial is usually lengthy and mixed with irrelevant information.
it is tedious for developers to peruse a full tutorial for an unfamiliar api .
a challenging question is to determine which fragments are relevant i.e.
containing explanatory information to developers.
a fragment of text in a tutorial might contain the names of the other related apis but not explanatory information on api usages .
non explanatory sentences contain the apis for an overview of an entire api class or an enumeration of related apis.
specifically this problem is stated as follows.
a user provides the name of an api in which s he is interested.
the task is to rank the fragments in a tutorial with respect to containing explanatory information relevant to a given api as a query.
a tutorial contains several fragments paragraphs of texts with mixture of apis .
figure shows an example of a fragment in android graphics tutorial.
there are four apis that appear in the fragment canvas bitmap surfaceholder and surfaceview .
according to the manual annotation canvas and bitmap are relevant to this fragment whereas surfaceholder and surfaceview are not since the fragment is not about those types of objects.
several approaches have aimed to address this problem using information retrieval .
.
data collection for a comparative study we chose to use the state of the art tool fraft with a dataset and the ground truth used in its experiments.
we used this dataset of five libraries as test data .
to train the models to build vectors we downloaded the source when you re writing an application in which you would like to perform specialized drawing and or control the animation of graphics you should do so by drawing through a canvas .
a canvas works for you as a pretense or interface to the actual surface upon which your graphics will be drawn.
it holds all of your draw calls.
via the canvas your drawing is actually performed upon an underlying bitmap which is placed into the window.
in the event that you re drawing within theondraw callback method the canvas is provided for you... you can also acquire a canvas from surfaceholder .lockcanvas when dealing with a surfaceview object.
however if you need to create a new canvas then you must define the bitmap upon which drawing will actually be performed.
the bitmap is always required for a canvas .
you can set up a new canvas ... figure a fragment in android graphics tutorial table tutorial corpus tutorial apis explanatory non expla.
frag with frag w o fragment fragment apis apis joda time math lib col. official col. jenkov smack code including the javadoc of those libraries.
we parsed the code and javadoc to produce the documentation for the apis in those libraries which was used to train the models to build the vectors.
.
approach using d2vec let us explain how we use d2vec for this problem.
the intuition of using d2vec is that the fragment on a specific api needs to be semantically relevant to a high degree to the description of the api in api documentation.
they might not be exactly matched but must semantically relevant.
specifically we used d2vec to compute the relevance score sim a f between an api aand a fragment f. to compute the score we trained d2vec with the javadoc api documentation of the libraries included in their corresponding source code as explained earlier.
after this step we have the vectors vapis for all the apis in those five libraries.
for example in figure the vector for the paragraph on the right side is the vector for the api sringbuilder.substring .
the vector vffor each fragment fin a given api tutorial is computed during that process.
the relevance score sim a f is computed as the cosine similarity between the vector vaof the api aafter training and the vector vffor the fragment f. .
metrics and setting in this experiment we compared d2vec against word 2vec and fraft which is a state of the art information retrieval approach fraft contains two parts fragmenting the tutorials and searching relevant tutorial fragments to which we compared .
because we focus only on comparing the searching part we used the fragments provided as part of the ground truth from fraft s authors .
we used each of the three techniques word 2vec fraft andd2vec to rank the fragments and measured the accuracy.
we used the same setting dataset oracle parameters and metrics as in fraft .
specifically precision is the ratio between the number of correctly predicted relevant fragment api pairs over all the retrieved pairs.
recall is the ratio between the number of the correctly predicted relevant fragment api pairs over all the pairs.
f score is the harmonic mean between precision and recall.
557esec fse november lake buena vista fl usa nguyen tran phan nguyen truong nguyen nguyen and nguyen table accuracy in ranking relevant tutorial fragments for apis precision recall f score tutorial fraft word2vec d2vec fraft word2vec d2vec fraft word2vec d2vec joda time .
.
.
.
.
.
.
.
math lib .
.
.
.
.
.
.
.
.
col. official .
.
.
.
.
.
.
.
.
col. jenkov .
.
.
.
.
.
.
.
.
smack .
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
table difference between frapt s and d2vec s results tutorial fraft correct both correct d2vec correct jodatime math lib col. official col. jenkov smack total correct cases as percentage .
.
.
empirical results as seen the results in table d2vec achieves precision recall and f score of .
.
and .
respectively.
comparing with word 2vec used in ye et al.
d2vec hasmuch better precision .
versus .
while maintaining similar recall .
thus that resulted in an improvement of nearly in f score.
since d2vec takes into consideration both the orders of words and context of the api s description as a whole the generated vectors are under more constraints to represent the apis comparing to the word 2vec vectors.
since d2vec has both local and global contexts it can predict better leading to much increase in precision.
d2vec also achieves slightly better results than fraft.
in particular d2vec increases recall by .
while loosing only precision leading to an overall increasing f score of .
we further conducted another study to investigate how many cases that d2vec correctly predicted while fraft did not and vice versa.
as seen in table d2vec can correctly predict .
of the cases that fraft did not.
meanwhile fraft can correctly predict only .
of the cases that d2vec did not.
in brief there is difference in results between the two tools.
therefore two approaches complement well to each other leading to a promising direction of combining them to achieve better accuracy.
rq3.
mapping between api libraries inspiring by the properties of d2vec s vectors in sections .
.
and .
.
we present a code to code retrieval application of d2vec in library migration deriving single mappings for equivalent apis between the two libraries jdk and apache.
in software development an application could appear in multiple platforms.
each platform might require the use of different software libraries.
while companies like to develop their software in multiple platforms with different libraries at the same time they often develop software in one environment and then migrate it toanother using a different library.
this process is called library migration.
for migration an important task is to find the respective apis with the same similar functionality in two libraries.
for example one uses the jdk api string.touppercase to capitalize the characters of a string.
s he could also use stringutils.capitalize in apache.
the process of manually defining api mappings not only requires much effort but also is error prone and incomplete .
to reduce manual effort several approaches to automatically map api elements have been proposed .
traditional approaches use heuristics to map the api classes and methods in two libraries based on the similarities of apis names parameters or calling structures .
however apis with same functionality in two libraries generally could have different names different parameter types and even different syntactic types e.g.
a method call is mapped to a field access or array access .
instead of heuristics gokhale et al.
use the execution traces of two similar gui applications to derive the mappings.
it is limited to gui libraries.
ibm model with statistical learning was proposed to learn the mappings from the corresponding sequences of apis in the client code of two libraries.
however collecting a parallel corpus of client code is not trivial.
nguyen et al.
propose an approach to mapping apis using word 2vec.
nonetheless it requires a training set of already mapped pairs between two libraries.
.
api to api mapping with d2vec our intuition for api mapping problem is that if in two libraries two apis have the equivalent functionality i.e.
they are the mapped api of each other they are likely to be described with similar words in their respective descriptions in api documentation .
section .
shows that d2vec can capture the relevance among words apis and related ones via the distances and the offsets of their representation vectors in the shared vector space.
thus it motivates us to evaluate how welld2vec can represent the mapping between two api elements in two different libraries via their respective vectors when they are equivalent in two libraries.
that is given the api mapping pair of aanda in two libraries lsandlt the vector of ais closer to the vector of a than others.
thus d2vec could help derive api mapping pairs based on the relative vector distances.
.
evaluation on api mappings .
.
data collection.
to train d2vec to produce the vectors we used the same training data with both api documentations of java jdk core and apache commons libraries listed in table .
as for testing we used the oracle api mappings provided in .
558complementing global and local contexts in representing ... esec fse november lake buena vista fl usa figure accuracy comparison in deriving api mappings table api mappings between jdk and apache by d2vec jdk api apache api rank string.touppercasewordutils.capitalize wordutils.capitalizefully stringutils.uppercase string.tolowercasestringutils.lowercase stringutils.uncapitalize wordutils.uncapitalize integer.parseint numberutils.createinteger math.max fastmath.max enumeration.nextelement iteratorenumeration.hasmoreelements inputstream.readautocloseinputstream.read ioutils.read teeinputstream.read pattern.matcherstrmatcher.charsetmatcher strmatcher.trimmatcher matcher.matcher bufferedreader.readline crlflinereader.readline map.containskey abstracthashedmap.containskey set.containstreebidimap.containskey singletonmap.isequalkey string.replaceallstringutils.replacepattern strbuilder.replaceall strbuilder.replacefirst stringutils.replaceeachrepeatedly file.deletefiledeletestrategy.deletequietly filedeletestrategy.dodelete .
.
metrics and setting.
we used the trained model to derive api mappings as follows.
given an jdk api we computed its vector via d2vec .
then we computed the list of apache apis that have the closest vectors to the vector of the jdk api.
the apache apis are ranked based on their cosine distances to the vector of the jdk api.
note that we derived only the mappings between api methods in jdk and apache since the functionality of their methods are well defined and more verifiable as equivalent than classes the classes in those two libraries often contain extra functionality .
between java jdk core and apache commons libraries an api in one library can be mapped to one or multiple alternatives in the other library.
for example jdk api method java.io.file.delete is used to delete the file or folder.
this task can also be achieved by using either apache api deletequietly ordodelete of the class org.apache.commons.io.filedeletestrategy .
thus if one of the alternatives is found we still consider it as a correct suggestion.
we measured top kaccuracy in deriving api mappings.
if the candidate list of apache apis with kelements contains at least one apache api that can be used to replace the given jdk api according to the usage documentation in two libraries we consider the case string.touppercase math.minfiles.copy map.entryset fastmath.minioutils.readstring.trimautocloseinputstream.read inputstream.readfileutils.copyfile stringbuffe r.appendwordutils.capitalize wordutils.capitalizefully abstractdualbidimap.entrysetstringbuilderwrite r.append abstr actreferencemap.en trysetstringutils.uppercasefileutils.copyfil etodirectory mathutils.minstringutils.tri m .
.
.
.
.
.
.
.
.
.
.
.
.
.8y xfigure d visualization of jdk and apache api vectors as a hit.
otherwise it is a miss.
top kaccuracy is computed as the ratio between the number of hits over the total number of cases.
accuracy shows how likely one would find the correct mapped api in the resulting list of kapache api results.
.
.
empirical results.
as seen in figure d2vec can derive correct answers for out of cases with only one suggestion.
in half of the cases the correct mappings are in the list of suggestions.
note that in many cases our approach can map more than one correct answers so the total number of derived mappings for jdk apis is .
however to calculate the top kaccuracy we only use the best suggestions.
as compared to word 2vec d2vec outperforms with at top and top accuracies.
table shows some examples.
some mappings have different names e.g.
string.tolowercase and stringutils.uncapitalize and integer.parseint and numberutils.createinteger .
we also reported the rank of the correct api s in the ranked list.
as seen d2vec is able to rank very high the correct apache apis for the given jdk apis.
.
.
a visualization study.
to further study d2vec s vectors for jdk s and apache s apis we first selected most commonly used jdk apis and its well known respective apache apis.
then we projected their vectors into a dimensional space for visualization.
figure shows the geometric arrangements of their d2vec s vectors in the d space.
the visualization indicates that jdk api and apache alternatives are mapped to closer locations than other elements.
for example math.min in jdk is close to mathutils.min and fastmath.min in apache files.copy and fileutils.copyfile fileutils.copyfiletodirectory are close to each other.
interestingly the pairs with different names are also captured such as string.touppercase in jdk and wordutils.capitalize and wordutils.capitalizefully in apache.
this result shows the reason that d2vec performs well in deriving api mappings between jdk and apache as it overcomes the textual mismatch issue between the names of the apis in two libraries.
threats to validity for the study on the properties of the vector representations we do not have a benchmark to verify against.
we observed the nearby and vector offset phenomena through vector operations and d projection with pca.
in the experiments for code search application our kodejava dataset might not be representative.
however they 559esec fse november lake buena vista fl usa nguyen tran phan nguyen truong nguyen nguyen and nguyen were used in a prior code search research .
the combination between two approaches is a simple linear combination.
however our goal is to compare with the traditional word 2vec to see how word 2vec and d2vec improve an baseline model in rvsm.
for api mapping application our dataset on api mapping pairs might not be representative.
however we picked only the most commonly used apis among jdk and apache.
in the tutorial search application the dataset of five libraries might not be representative.
however we used the same dataset as in the fraft paper .
related work d2vec is related to pvdm which aims to represent phrases sentences paragraphs or documents with vectors via associating ids to paragraphs.
first d2vec aims to combine global context api topic and local context n gram to represent api elements and the words in api descriptions which was inspired by the ngram topic model .
inn gram topic model the probability that a token cappears at a position is estimated simultaneously based on the global topic kand the local sequence of n 1previous tokens.
while it relies on bayesian network d2vec uses neural network.
second d2vec s architecture and computation are different.
in d2vec an api topic word is associated with each context window in api documentation and that api word could also occur in the same or different window.
that is the surrounding words of the apis in api documentation are also considered.
thus both global context topic and local context n gram are combined.
in pvdm a paragraph id is an id for a phrase sentence paragraph.
it is uniquely assigned to identify paragraphs and does not appear in texts of any paragraph.
if paragraph ids are used to represent apis pvdm will not have the local contexts for those apis.
third to assign paragraph ids pvdm faces the problem of splitting up phrases sentences paragraphs while d2vec does not need such notions.
to empirically compare with pvdm we need to solve that non trivial splitting task due to the embedded apis.
fourth d2vec focuses on combining global and local contexts instead of focusing on only word ordering which is a derivation of local context via n grams.
finally the idea of combining topic modeling and local contexts set forth by d2vec is applicable to other se tasks.
yeet al.
use word 2vec to project words and code elements into the same space to improve retrieval accuracy in bug localization problem.
nguyen et al.
use word 2vec on api documentation for code retrieval in the same manner.
allamanis et al.
propose a neural probabilistic language model to suggest method class names.
code tokens with statistical co occurrences are projected into a continuous space together with the text tokens from the names.
they also introduce a jointly probabilistic model short utterances and code snippets .
the bimodal modeling treats texts and code in two distinct spaces.
we use one single space.
guerrouj et al.
use the context of the words that surrounds a code element in stackoverflow posts to summarize its use and purpose.
they follow the spirit of n gram language model by calculating the frequencies of all the phrases from grams grams up to n grams with certain nvalue.
then they use the k means clustering to select words to include in the summary of an identifier.
in comparison they follow a counting based approach while in d2vec we use a neural network model.information retrieval ir is a very common approach in linking code and texts and searching api usage examples from texts.
some ir based approaches match queries against the names of components program elements sourcerer gridle graphs of api elements and relations among api elements .
recodoc links code like terms in documentation to their source code elements by using regular expressions to extract them from free form text and uses partial program analysis for extracting from code.
baker is a tool that incorporates links to the api documentation into the code snippets that use the api.
yang and tan propose swordnet to automatically infer semantically related words in software by leveraging the context of words in comments and code.
swordnet does not aim to handle api or program elements.
it focuses on the relations between words in order to expand queries with semantically related words.
statistical nlp approaches have been used in se.
maddison and tarlow present a generative model for source code that is based on ast structures.
tbcnn also uses trees for suggest next code tokens.
other se applications of statistical nlp include code suggestion code convention name suggestion api suggestions code mining type resolution etc.
statistical nlp approaches were used to generate code from text.
swim first uses ibm model with word translation to produce code elements.
it then uses syntactic rules on those elements to build code sequences close to the query.
deepapi uses rnn to generate api sequences for a given text by using deep learning to relate apis.
desai et al.
synthesize domain specific languages from english.
anycode uses a probabilistic cfg with trees for java constructs and api calls to synthesize small java expressions.
t2api uses graph based api synthesis algorithm that generates a graph representing an api usage from a large corpus.
conclusion in this work we conjecture that we need a new model that fits with api documentation better than word 2vec.
we present d2vec a neural network model that considers two complementary contexts to better capture semantics of api descriptions.
first we connect the global context of the current api topic under description to all the text phrases within the description of that api.
second the local orders of words and api elements in the text phrases are maintained in computing the vector representations for the apis.
we demonstrate the usefulness and good performance of d2vec in three applications api code search text to code retrieval tutorial fragment search code to text retrieval and mining api mappings between jdk and apache code to code retrieval .
finally inspired by d2vec s success in those applications we provide actionable insights on how to use d2vec in the context of other se documents including source code and in other tasks e.g.
building a representation for a code fragment with an addition of a topic applying d2vec on bug localization with lda and building database of vector representations for apis as building blocks.