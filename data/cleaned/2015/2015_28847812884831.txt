how does the degree of variability affect bug finding?
jean melo claus brabrand andrzej w asowski it university of copenhagen denmark jeanmelo brabrand wasowski itu.dk abstract software projects embrace variability to increase adaptability and to lower cost however others blame variability for increasing complexity and making reasoning about programs more di cult.
we carry out a controlled experiment to quantify the impact of variability on debugging of preprocessorbased programs.
we measure speed and precision for bug nding tasks de ned at three di erent degrees of variability on several subject programs derived from real systems.
the results show that the speed of bug nding decreases linearly with the degree of variability while e ectiveness of nding bugs is relatively independent of the degree of variability.
still identifying the set of con gurations in which the bug manifests itself is di cult already for a low degree of variability.
surprisingly identifying the exact set of a ected con gurations appears to be harder than nding the bug in the rst place.
the di culty in reasoning about several con gurations is a likely reason why the variability bugs are actually introduced in con gurable programs.
we hope that the detailed ndings presented here will inspire the creation of programmer support tools addressing the challenges faced by developers when reasoning about con gurations contributing to more e ective debugging and ultimately fewer bugs in highly con gurable systems.
categories and subject descriptors d. .
testing and debugging keywords variability preprocessors bug finding .
introduction a recent study reports that the global cost of debugging software has risen to billion dollars annually and that on average software developers spend half of their programming time nding and xing bugs .
this is particularly permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may austin tx usa acm.
isbn .
.
.
.
in the context of variability.
software projects embrace variability hoping to increase exibility at lower cost to better control system resources to extend portability across di erent hardware to meet requirements of various market segments .
however multiple research indicate that variability might also amplify maintenance problems.
recent literature on variability is riddled with claims to that end.
we list a few examples bug nding is a time consuming and tedious task in the presence of variability managing variability can become complex variability speci cations and realizations tend to erode in the sense that they become overly complex.
understandability and maintainability may be negatively a ected .
however reasonable there is little to no hard evidence for these claims.
speci cally how are maintenance tasks bug nding in particular a ected by variability?
how much harder is it to debug a program as variability increases?
does variability a ect speed or also quality of debugging?
in this paper we set o to understand such issues using a controlled experiment designed to quantify the impact of the degree of variability in program code on bug nding.
in the experiment we use simpli cations of real bugs extracted from linux busybox and bestlap .
we de ne three degrees of variability no low and high corresponding to zero one and three features respectively.
given a program and a degree of variability we ask the participants to debug the programs.
in summary we learn that the time needed for nding bugs increases only linearly with the amount of features with the time becoming less predictable with more features.
the di erences in e ectiveness of various programmers are ampli ed by the increase of variability.
most developers correctly identify bugs partial correctness yet many fail to identify the set of a ected con gurations complete correctness .
this is consistent with earlier hypotheses that programmers introduce errors because it is di cult to reason about all the executions involved via con guration choices.
interestingly this ability does not seem to improve with increasing level of education while general non variability related bug nding skills do seem to improve.
challenges with reasoning about con gurations are already measurable for low degrees of variability.
the intended audience of this work are designers of variability management tools of bug nding tools and of other supporting methods such as variability aware software architectures aiming at improving e ciency and correctness in ieee acm 38th ieee international conference on software engineering a no variability zero features .
b low variability one feature .
c high er variability three features .
figure a program with an uninitiliazed variable error with progressively increasing degrees of variability.
development of variability intensive systems.
we hope that the insights provided will in uence new tools and methods that can help avoiding bugs like the one presented in the next section or help debugging programs that contain them.
we also hope to inspire software architecture researchers to further understand the trade o s between increasing variability and development cost by also including potential debugging costs.
.
motiv ating scenario today variability intensive software systems include both large industrial product lines and open source systems of various sizes up to the linux kernel with more than con gurable features .
a multitude of technologies can be used to implement con gurable systems object oriented patterns aspects domain speci c languages and code generation plugin mechanisms and so on.
among these the c preprocessor cpp is one of the oldest the simplest and the most popular mechanisms in use especially in the systems domain.
for these reasons we use the preprocessor in our study.
the results do not generalize to other mechanisms but provide a good indication given that the other solutions are more complex and require much more additional code to handle variability.
figure 1presents an example extracted from the netpoll module of linux kernel slightly adapted to java syntax using coloured lines instead of preprocessor .
netpoll is an api that provides a means to implement udp clients and servers in the kernel independently of the main networking stack.
these can be used in unusual situations like failure monitoring crash recovery or debugging of the kernel.
the original function called netpoll setup in c is used to initialize the module.
it is about lines long and involves one optional feature.
historic versions of the function contained anerror .1if the feature is disabled the function returns the value of an uninitialized variable err intended to hold an error value in case of unexpected situations.
we illustrate how the task of debugging becomes more complex as the numbers of features increase.
figure 1a shows a version of the bug as a conventional program without variability.
it is fairly easy to establish that the function returns the value of an uninitialized variable in line .
in line the variable flag is assigned false which means that ?id e39363a9def53dd4086be107dc8b3ebca09f045dthe conditional statement in line is not executed hence the variable errwhich was declared and uninitialized in line is never assigned a value.
now since the value of the variable ipv4 istrue line the conditional statement in line returns the value of the uninitialized variable err.
figure 1bcontains the same program but now involving one feature shown in light gray background color.
a feature such as light gray in this example can be con gured either asenabled ordisabled .
features are used in compile time conditional directives ifdef s to control whether to include orexclude code fragments in a program.
we use the conventions of cide which assign colors to features and show conditional statements using background colors rather than ifdef s. we discuss implications of this di erence in section .
a colored statement is to be included in a program if and only if the corresponding feature color is enabled .
obviously a feature thus gives rise to twopossible con gurations a program with the light gray statement and a program without it.
in general nfeatures will give rise to 2ncon gurations i.e.
2nprograms.
now programming errors conditionally depend on con gurations.
indeed the error in figure 1boccurs only whenever weenable the light gray feature.
if the light gray statement in line is included the variable erris not initialized in line .
if we disable the light gray feature the error no longer occurs errwould then be initialized in line .
ultimately this leads to a notion of partial correctness for debugging.
a developer can correctly diagnose the error nd the bug but incorrectly misdiagnose the exact set of con gurations the combinations of features producing the bug .
in the case of a single feature partial correctness misdiagnosis means mixing up enabled and disabled.
the problems and di culties escalate when we consider more features i.e.
higher degrees of variability.
figure 1c shows the same programs as before but now with three features light gray gray and dark gray.
the three features yield eight con gurations.
in debugging the program the developer must somehow consider all con gurations.
determining the erroneous products becomes a non trivial combinatorial problem which as we shall see in section is di cult.
indeed for our program in figure 1c the error now occurs in exactly three out of eight con gurations.
.
experiment in this section we explain our experimental design and setup.
a p2.
b p3.
figure programs p2and p3with hivariability degree p1is a larger version of fig.
1c.
prg origin filename bug type loc mth p1 linux netpoll.cuninitialized21 2variable p2 busybox http.cnull pointer29 3dereference p3 bestlap gamescreen.javaassertion31 4violationprg degree jfj scattering tangling vcc p1 no lo hi p2 no lo hi p3 no lo hi figure characteristics of our three benchmark programs p1 p2 and p3.
.
objective our experiment aims to analyze the impact of variability on bug nding.
we want to understand exactly how much harder does the debugging task become as the degree of variability in a program increases?
speci cally we aim to answer the following research questions rq1 how does the degree of variability a ect thetime of bug nding?
rq2 how does the degree of variability a ect theaccuracy of bug nding?
to address these research questions we perform a range of classic nd the bug experiments and measure the time and accuracy of the bug nding task.
we expose developers to programs with di erent degrees of variability while controlling for noise factors such as learning developer competence and program complexity.
we measure accuracy as the number of correct vs incorrect identi cations of bugs.
we are particularly interested in the time and accuracy of bug nding as functions of the degree of variability.
.
treatments in order to study debugging as a function of variability we expose each participant to programs with di erent degrees of variability so programs using di erent amounts of conditional compilation blocks.
we settled on using three distinct degrees of variability.
let fdenote the set of features conditional compilation symbols used in a program .
first to establisha baseline we consider programs with no variability degree no f .
then we consider programs that use one feature degree lo jfj and programs with three features degree hi jfj .
the number of con gurations grows from one for degree noprograms two for loprograms to eight for hi.
this should make any performance di erences manifest themselves clearly.
even though it would be interesting to study higher degrees the limitation to three features has one important advantage it leaves us with programs su ciently small to be used in a time delimited controlled experiment.
we derive programs of lower degrees by taking an erroneous program with three features see below and appropriately x features as either enabled ordisabled retaining the original error.
we thus obtain three versions of each program no lo and hi much like in fig.
.
.
subjects we now turn to the subjects of the experiment which are a ected by the treatments the participants and the programs .
participants.
we performed the experiment with n participants m.sc.
students ph.d. students and post docs.
the m.sc.
students came from two courses at the it university of copenhagen interactive web services using java and xml and system architecture and security .
the ph.d. students and post docs came from three danish universities it university of copenhagen itu university of copenhagen ku and technical university of denmark dtu .
we informed all participants that they could stop 681participating at any time.
all participants had programming experience especially injava and around half of the participants had industrial experience ranging from a few months to several years.
programs.
for the robustness of our experiment in order to minimize risks of speci c e ects from particular programs and bug types we took three programs with di erent kinds of errors.
we based our programs on realvariability errors from three highly con gurable systems linux busybox2 and bestlap .
these are qualitatively di erent systems in terms of size architecture purpose variability and complexity.
linux is an operating system and is likely the largest highly con gurable open source system with more than mloc and features.
busybox is an opensource highly con gurable system with kloc and about features that provides several essential unix tools in a single executable le.
bestlap is a commercial highlycon gurable race game with about kloc.
the kinds of errors we consider are also di erent an uninitialized variable anull pointer dereference and an assertion violation .
we simpli ed the error in each system down to an erroneous program that would t on a screen without scrolling lines yet involve exactly three features.
figure 2shows the programs p2andp3with hivariability degree p1is a larger version of fig.
1c .
the loand no variability programs are obtained from the hivariants by selecting a feature and con guration that preserves the bug and in uences the program size as little as possible so that it can still be comparable to the hivariant.
in the following we describe the bugs used in the experiment of hivariability degree informing the type and erroneous con gurations of each bug.
bug description of p1.p1has one only method which contains conditional statements and an integer local variable called err as shown in fig.
1c.
the three features yield eight possible con gurations.
in debugging the program the developer must somehow consider all con gurations.
to accomplish the task the developer needs to identify that the variable errisuninitialized in exactly three out of eight con gurations.
indeed for our program in figure 1c the error occurs in the following con gurations by enabling only i dark gray ii light gray and dark gray iii light gray gray and dark gray.
in these erroneous con gurations the variable erris not initialized in line .
bug description of p2.p2contains two methods to handle incoming http requests see figure 2a .
it also has a variable subject that may be null and is dereferenced for certain con gurations.
so to identify the bug the developer should realize that this happens in three con gurations.
in fact the error in figure 2aoccurs only when we disable the green feature.
thus the erroneous con gurations are when weenable only i blue ii yellow iii blue and yellow.
if the sendheaders method is called by either the yellow or blue features the variable subject will be null whenever the green feature is disabled and in line there is an access to subject which may cause null pointer exception.
bug description of p3.p3has three methods responsible for computing the score as can be seen in fig.
2b.
according to a user requirement the game should also compute negative scores.
but the method setscore contains a condition 5cd6461b6fb51e8cf297a49074fce825e1960774prohibiting negative scores.
we encode the requirement using assertions.
to nd the bug the developer should consider all con gurations and somehow see that the variable totalscore is always equal to zero in the end of setscore computation when passing negative values to the method which is revealed through an assertion violation in the code line .
for our program in figure 2b the presence condition of the error is blue yellow .
thus the assertion error occurs in exactly two con gurations i blue and yellow ii blue yellow and green.
figure 3lists various characteristics for each of our programs.
the left hand side gives the basic characteristics of the programs their origins project and le name bug type number of lines of code excluding whitespace and comments and number of methods.
the right hand side lists variability metrics for each of the degrees feature scattering feature tangling and variational cyclometric complexity vcc .
we show metrics for each of the degrees using a slash separated notation no lo hi .
for each feature scattering counts the number of conditional compilation colored blocks based on the cdc metric .
we give accumulated numbers for all features involved.
the feature in p2 lo for example is scattered over three locations in the source code.
for each feature tangling in turn counts the number of switches between regular code and feature code through the controlow of the program based on the cdloc metric .
again the feature in p2 lo for instance requires a concern scope change between the code base and the feature code four times through the program.
for vcc we use the cyclomatic complexity metric on the variability programs treating ifdef s colored lines as ordinary ifs.
notice that the programs are all quite di erent yet in terms of complexity they are hierarchically ordered from p1 simplest to p3 most complex .
.
design we present rst a fully randomized experiment design point out a problem and address it using a well known technique.
in terms of debugging tasks we have nine in total three programs each at three degrees of variability.
however a developer cannot be assigned to perform allnine debugging tasks.
clearly we can only have a developer nd bugs in a given program once otherwise there would be a learning e ect on subsequent attempts.
for similarly obvious reasons we can only have a developer consider a variability degree once.
abiding by these constraints we can have each developer debug three di erent programs each at a di erent variability degree.
however there might still be learning e ects lurking due to sequencing of assigned tasks.
presumably debugging loafter hiis not the same as debugging hiafter lo even for di erent programs.
similarly debugging an easy program after a hard one is not the same as in the reverse order.
fully randomized design.
to counter these e ects we need to randomize the order developers consider the tasks.
aside from learning e ect we also need to control for other subject related noise factors confounding factors such as di erences in developer competence and program complexity.
after all a competent developer debugging an easy program will obviously not produce the same result as an incompetent developer debugging a hard program.
again randomization may be used to control for these e ects.
for larger samples these e ects will diminish.
thus one solution would be for each developer to assign 682programs p1 p2 p3d evelopersd1 no lo hi d2 lo hi no d3 hi no lo figure latin square .
programs and degrees completely at random without ever reusing a program or a degree.
this does control for confounding factors.
however statistically we could get vastly di erent number of data points for the nine debugging tasks.
in particular we could get a low number of data points for certain debugging tasks e.g.
p1 no .
obviously this could compromize the quality of subsequent statistical analysis.
latin square design.
however there is a better solution.
an nlatin square is an n nmatrix with ndistinct values as entries with the property that no row or column contains the same value twice.
latin squares present a common solution to the above statistical problem in many experimental setups .
figure 4depicts a latin square applied to our context.
the columns are labelled with the three subject programs p1 p2 p3 .
the rows are labelled with names of three subject developers d1 d2 d3 .
the nine squares in the center contain the three treatments no lo hi .
now with this design each developer receives all three treatments listed in his row for all three subject programs listed in the headers of the corresponding columns.
we apply the latin square design to ensure the same number of data points for all debugging tasks without compromising control over the confounding factors.
there are distinct latin squares modulo swapping rows and swapping columns.
for each three developers we randomly pick one of these latin squares and randomize also the assignment of developers and programs to rows and columns.
the result is the same number of data points for all debugging tasks without compromising control over the confounding factors such as developer personal competence or program complexity.
still each combination of treatments and programs is equally probable for each developer in any order of programs and in any order of treatments .
for n participants each performing three out of nine tasks we get exactly data points for each of the nine debugging tasks.
technically our experiment is an instance of the so called within group design in which all subjects are exposed to every treatment.
we apply our treatments independent variable variability with three levels no lo and hi to the subjects programs and developers .
in addition we distinguish the tasks for each program since the programs are not equivalent.
we measure our dependent variables time and number of correct and incorrect answers for bug nding tasks.
data analysis.
we used anova to test signi cance of di erences between di erent treatments.
anova is heavily used in controlled experiments and useful in comparing three or more means for statistical signi cance.
anova requires a normal distribution variance homogeneity and model additivity of the samples.
we check these assumptions using thebox cox bartlett and tukey tests respectively.we conventionally consider a factor as signi cant when a p value .
.
.
execution before the actual large scale experiment on subjects we executed a pilot study with a small group of local students to assess our design and tasks.
the results of the pilot are not considered in our analysis and the number is excluding the pilot study.
based on the pilot feedback we changed mainly the presentation of the tasks.
before the subject developers were confronted with their tasks we presented a simple tutorial on the basics of variability in particular features con gurations and compile time conditional statements.
also we demonstrated how to solve a small warm up task to demonstrate the nature of the tasks and what kind of answers are expected.
the warm up task was inspired by figure in a paper by liebig et al.
.
we randomly generated latin squares as described above.
we then used the latin squares to compile a task description sheet for each participant with their three relevant debugging tasks e.g.
rst p2 lo second p1 hi third p3no .
every participant then performs the debugging task i.e.
nd the bug for the three given programs in sequence.
all task description sheets contained instructions and a link to an online form for completing the task.
we ensured that each program ts in a single screen to avoid participants scrolling up and down to see the entire code.
we prepared all tasks using google forms to avoid heterogeneous environments and installing software on di erent machines.
that is we provided to the participants only a static window i.e.
no ides no tools no navigation.
we recorded timestamps for each of the participants when they start and nish allowing us to calculate the duration of their debugging.
in addition to time after the experiment we calculated the number of correct incorrect and partially correct answers.
we eliminated participants who left the experiment early without completing the task and returned their latin square assignments to the pool of the rows available for further random allocation.
the eliminated unmotivated participants are not included in the gure.
no other deviations happened during execution.
in addition to the quantitative results we conducted semistructured interviews after the experiments.
this was to get qualitative feedback on how the participants approached the debugging tasks particularly the ones involving a hidegree.
we asked two questions i how did you go about nding thehibug?
and ii what were the di culties?
.
results we now present the results of our experiment and discuss the implications.
we make eight observations addressing the research questions the impact of variability on the time and accuracy of bug nding.
before proceeding we stress that the observations should not be generalized far beyond the degree of variability for which we ran the experiment i.e.
jfj .
we will elaborate on external validity in sect.
.
.
all experiment materials are available online at dk people jeam variability experiment including data programs task descriptions and statistical processing scripts .
.
how does the degree of variability affect the time of bug finding?
rq1 683figure mean bug nding time along the y axis in minutes as a function of the degree of variability x axis .
we consider the rst research question now.
observation mean bug nding time appears to increase linearly with the degree of variability.
figure 5plots the mean bug nding times in minutes along they axis for each of our three benchmark programs.
each dot depicts the mean time to nd the bug for a particular program p1 p2andp3 for a particular degree of variability i.e.
no jfj lo jfj and hi jfj .
for instance the fastest mean bug nding time is about 2minutes for program p1with novariability whereas the slowest mean bug nding time is a bit less than minutes for program p3with a hivariability degree of jfj .
for each program we t a regression line to its respective points.
the lines suggests that the mean bug nding time increases linearly with the degree of variability.
according to an anova test the di erence between bug nding times for distinct degrees of variability is statistically signi cant with p value .
also bug nding time is a linear function of programs and degrees with p value by f test for regression.
recall that the number of variant programs to be considered by a participant grows exponentially with the degree of variability i.e.
jkj 2jfj assuming all variants constitute valid programs .
clearly a developer has to somehow consider each of the 2jfjvariants in order to make an accurate diagnosis of the bug.
afterall each of the variants may or may not harbour a bug.
one might then in fact suspect that bug nding time ought to increase exponentially with the degree of variability.
the post treatment interviews provide qualitative insights into how the participants approached the problem and what di culties they faced in understanding programs with a hivariability degree.
the participants agreed that nding bugs in the noprograms so without variability required less e ort than in programs with hidegree of variability.
one participant explains i tried to keep all di erent paths in mind but it was especially di cult with multiple colors .
along the same lines another participant says with more variability you need to build up exponentially more traces in your head.
the participants analyze programs as one unit despite varifigure the distribution of bug nding time.
ability.
they do notsplit the task into analysis of exponentially many independent programs one variant at a time.
an unconscious use of brute force would yield a 2jfjfactor slow down in overall bug nding time.
hick s law from psychology based on so called choicereaction time experiments explains that the amount of time for a human response increases logarithmically with the number of possible choices.
compared to a baseline program with novariability programs with higher degrees of variability involve exponentially more choices to be made.
obviously composing an exponential function with a logarithmic one yields a linear function.
we thus hypothesize that the seemingly linear increase in bug nding time in spite of the exponential blow up can be attributed to hick s law.
presumably the more complex the variability of a program the more time it would take to nd bugs in that program.
indeed fig.
5is consistent with this expectation the slopes of the lines are ordered according to the complexity of the respective programs.
recall from figure 1that programs p1 p2 and p3were increasingly complex both in terms of variability unaware and variability aware characteristics.
also we remark that even if we exclude participants that failed to correctly identify the bug we see a picture similar to that of fig.
.
in summary the rst observation indicates that an increase in variability e.g.
by adding features complicates bug nding but not dramatically and not prohibitively so.
this is a very positive nding that is consistent with existence of software products with hundreds even thousands of features testifying that developers in the trenches areable to deal with variability.
observation the variance of bug nding time appears to be ampli ed by the degree of variability.
figure we plot the distribution of bug nding times for each program and variability degree.
each box encapsulates the middle data points.
the lower and upper limit of the box respectively represent the lower and upper quartiles the and percentiles .
the upper and lower whiskers represent the data above and below the middle half of the data.
the horizontal line within the box draws up the median 3the diagram can be found in the accompanying materials.
684no lo hi no lo hi no lo hi p1 p2 p3ph.d.
m.sc.
figure ranking of fastest at the bottom to slowest top participants according to their educational level i.e.
m.sc.
vs. ph.d. students .
of the data points.
finally the circles above the boxes visualize outliers.
for instance for program p3 the three rightmost boxes the middle half of the participants spent between 3and minutes to nd the bug with novariability whereas for hivariability the middle half spent from about to 2minutes.
again considering only participants that found the bug yields a similar diagram consistent with the above.
ampli cation of variance is a predictable consequence of our rst nding.
for the variance of a stochastic variable x multiplied by a constant factor c depending on the degree of variability we have that var c x c2var x .
in popular terms this observation means that di erences in bug nding competences are ampli ed when working with variability.
ultimately this means that getting talented developers on such projects is important.
observation ph.d. students appear to not be faster at nding variability bugs than m.sc.
students.
figure 7shows the ranking of bug nding from the fastest participants towards the bottom to the slowest participants towards the top abstracting away the actual time they spent debugging.
m.sc.
students are shown in light gray ph.d. students in black.
there appears to be no pattern of one group of students being faster than the other even for higher degrees of variability.
in the late 1980es oman et al.
compared debugging abilities of novice intermediate and skilled student programmers using two programs written in pascal.
among other things they found that experienced programmers nd errors faster than less experienced programmers.
we in turn do not notice any di erence in terms of bug nding time between ph.d. and m.sc.
students.
this can be explained by the level of subjects in that study.
they considered only undergraduate students separating them according to the amount of computer science courses taken whereas we test with graduate students who would likely be considered as skilled programmers in their setup.
furthermore we study a di erent phenomenon which is variability that might be challenging independently of education level.
.
how does the degree of variability affect the accuracy of bug finding?
rq2 we now turn to our second research question rq2 on the accuracy of bug nding observation most developers correctly identify bugs in programs regardless of the degree of 4see the accompanying website for more information.
figure ratio of incorrectly vs.correctly identifying a bug.
variability.
figure 8shows shows what percentage of developers were able to nd the bugs correctly.
the incorrect answers are black and the correct ones are gray.
the data is presented for each degree of variability separately.
the frequency of incorrect answers is consistently low with around a fth being the incorrect answers.
for programs with novariability of subjects out of did not nd the bug.
even for the hi variability programs only of the subjects out of answered incorrectly.
generally developers seem to be good at nding bugs in programs and in programs with variability at least up to three features .
interestingly more than half out of of the participants correctly identi ed the bug in allthree tasks.
on average if we disregard the variability degrees of the participants were able to correctly nd the bug.
all in all we conclude that nding bugs in programs seems to not be signi cantly a ected by the degree of variability at least forjfj .
observation many developers fail to exactly identify the set of erroneous con gurations already for a low degree of variability.
we now look a little closer at accuracy and split the correct answers in two sets.
if the participant got the set of erroneous con gurations exactly right we classify her answer as fully correct.
similarly we classify answers as partially correct if the developer has correctly identi ed the bug but failed to correctly specify the set of con gurations in which the error occurred missing some con gurations or listing too many .
we ignore incorrectly identi ed bugs for this part of the analysis as it is hard to interpret the identi cation of con gurations for them.
for instance program p3with hivariability contains an assertion error that occurs in two out of eight con gurations.
for this task some participants found only one of the erroneous con gurations and others listed extra con gurations for which the error does not occur.
figure presents the numbers of fully and partially correct answers at di erent levels of variability.
obviously partial correctness does not make sense for programs without variability for nowe have only one possible con guration .
already for lovariability one feature we see that the number of partially correct answers quickly rises to out of .
for hivariability this number escalates to almost out of .
identifying the exact set of erroneous con gurations seems to become di cult already for jfj hivariability .
this requires understanding the combinations of features that enable the incriminated execution paths a form combinatorial reasoning which apparently becomes di cult fast.
such problems are notoriously hard for humans.
for realistic systems where a feature model additionally shapes the set of legal con gurations this task would presumably be even harder as one needs to reason about feature model 685figure ratio of partially correctly versus fully correctly identifying a bug.
constraints in addition .
from a prior qualitative study we know that programming errors related to variability appear due to inability of programmers to correctly reason about all variations of the program that they are modifying.
those ndings are consistent with the above it is plausible that developers mis identify the sets of con gurations during programming tasks and during debugging tasks for the same reasons.
to the best of our knowledge this study presents the rst quantitative con rmation that indeed reasoning about multiple con gurations is a challenge even for relatively small sets.
observation for higher degrees of variability it appears to be more di cult to correctly identify the set of erroneous con gurations than to nd the bug in the rst place.
forhivariability we saw that out of did not nd the bug see figure .
among the ones that did a staggering out of erred on set of erroneous con gurations cf.
figure .
although the participants were only asked to nd the bugs not also xthem we nd that our results are consistent with studies of creating and xing bugs.
yin and coauthors report that in general bug xers may forget to x all the buggy regions with the same root cause .
.
our earlier study also reports that bugs are introduced because the programmers do not realize the complexity of all the con gurations in which their code will run.
observation ph.d. students appear to be more accurate at nding variability bugs than m.sc.
students.
figure 10acompares the ratio of correct to incorrect answers according to educational level separating m.sc.
students and ph.d. students.
we see that the number of incorrect answers for ph.d. students are consistently low.
in fact even forhivariability only of the ph.d. students answered incorrectly.
for m.sc.
students the numbers are consistently higher.
for hivariability the number of incorrect answers are more than three times higher at .
on average the ph.d. students found bugs three times more accurately than m.sc.
students for all degrees of variability.
presumably ph.d. students having more education are more careful and meticulous when debugging than m.sc.
students.
interestingly ph.d. students prevail only as far as identifying the actual bug is concerned but they are not better in identifying the relevant set of con gurations.
for both m.sc.
and ph.d. students the percentage of fully correct answers seems to not signi cantly be impacted by variability observation identifying the exact set of erroneous con gurations is hard regardless of education both for m.sc.
and ph.d. students .
figure 10bshows the frequency of partially correct answers for m.sc.
versus ph.d. students.
for hivariability forinstance we see that of m.sc.
students answered partially incorrect versus for that of ph.d. students.
the numbers testify that the combinatorial task of identifying the exact combination of features provoking an error is di cult regardless of educational level.
we see the frequency ofpartially correct answers double from lotohidegree in both groups of students.
.
threats to v alidity .
internal validity choice of variability degrees?
we chose zero one and three features for pragmatic reasons.
if we instead had chosen for instance two four and sixfeatures the experiments would have required much longer time discouraging and tiring participants.
in fact the mean time for program p3with three features is almost ten minutes.
also ve participants spent more than twenty minutes to nd the bug in programs involving three features.
choice of language?
in this experiment we adopted java as our programming language.
this is because we want to run the experiment with as many students as possible andjava is well known among students in denmark.
we only admitted students who had experience with java.
use of background color?
researchers have shown that background colors may improve program comprehension and subjects favor background colors .
additionally the bene ts of colors compared to text based annotations is that one can clearly distinguish background colors feature code from base code and humans are able to recognize colors faster than text .
thus we decide to use background colors instead of preprocessor directives.
choice of colors?
before the experiment we check for color blindness among the participants.
we also take care of choosing colors that are clearly distinguishable blue green and yellow .
aside from this we do not believe that the exact choice of colors matter so much 5for our experiment.
selection bias?
to minimize selection bias we randomly assign participants degrees and programs into the latin squares.
so we control our confounding factors via latin square design and randomization.
every participant takes all treatments including all the three programs.
we did have few unmotivated students who left the experiment early without completing the tasks.
as mentioned earlier we eliminated them from the study and returned their latin square assignments to the pool of the rows available for further random allocation.
participation incentives?
we o ered a chocolate bar as a participation incentive.
aside from that only pride prohibits participants from deliberately performing poorly.
we found no indications of participants giving deliberately silly answers.
.
conclusion validity statistical tests?
in this experiment we use anova including veri cation of its assumptions to verify whether or not the means of our test groups are equal by analyzing variance.
in fact anova is heavily used in controlled experiments and useful in comparing three or more means for statistical signi cance.
besides that we are comparing 5the fashion industry may disagree.
m.sc.
ph.d. a ratios of incorrect vscorrect answers for m.sc.
students above and ph.d. students below .
m.sc.
ph.d. b ratios of partially correct vsfully correct answers for m.sc.
students above and ph.d. students below .
figure accuracy of bug nding according to educational level m.sc.
students vs ph.d. students .
group means against each other not speci c subjects which decreases the impact of developer competence.
time measured?
the time measured to complete a test involves both thinking as well as writing down the answer.
we asked the participants to write down the bug kind line and a list of erroneous con gurations a con guration is written as a list of enabled features .
obviously this might interfere in the measured time due to variations in writing speed.
however we observed that the task of writing usually took tens of seconds whereas thinking took on the order of minutes.
hence the speed of the task of writing is dwarfed by the thinking.
note that we exclude the time of reading the tasks as we only start the timer once the participants have read each task description i.e.
when they see the programs .
.
construct validity do participants know what to do?
before exposure to the programs with variability we explained the basics of variability including features compile time conditional statements and con gurations .
in addition we performed a warm up task with di erent degrees of variability with them in order to demonstrate what they need to do and what they need to answer including the format of answers .
in summary we essentially taught them how to accomplish the tasks and ll in the forms.
disregarding incorrect answers?
when analyzing response times we decided not to exclude wrong answers because we wanted to measure the time it takes to debug a program whether correct or not.
note however that we do check that our observations still hold are stable when disregarding incorrect answers.6recall that the number of incorrect answers were consistently low regardless of the degree of variability cf.
observation figure 8and10a .
.
external validity beyond c ideand preprocessors?
our experiment and entire study is dedicated and tailored to a particular technique for dealing with variability preprocessor.
our observations generalize to ifdef s instead of background colors because of their close relationship after all cide is based on ifdef s and known results shows that a judicious use of colours instead of ifdefs can only simplify the task .
generalization to other variability techniques is not intended.
beyond university students ?
the main question is whether our study is also relevant to the industry?
our n participants were predominantly m.sc.
and ph.d. students from three di erent danish universities.
all had java 6see the accompanying website for more information.programming experience and around half of them had industrial experience few months to several years .
note that we had participants from africa asia incl.
the middle east europe north and south america.
in addition previous research has established that graduate students make good proxies for industry developers .
all of this contributes to representativity and generalization to real world industrial developers.
beyond our buggy programs?
we based our programs p1 p1 and p3 on real variability bugs from real highlycon gurable systems linux busybox 7andbestlap respectively precisely to minimize the risks of introducing and studying arti cial problems.
also the programs were qualitatively di erent cf.
figure .
further our bug nding tasks present three qualitatively di erent types of bugs uninitialized variable null pointer dereference and assertion violation .
in fact these are common variability bugs in bug reports .
for these reasons we expect the results should transfer to other smaller programs.
of course there may be additional e ects unaccounted for when debugging programs beyond lines.
beyond lab settings?
more programs larger programs higher degrees realistic programs and tools all extend the task duration beyond one hour and make it signi cantly harder to attract anywhere near participants.
for these reasons we optimized for internal validity and quantitative observations in lab conditions recognizing that there is an inherent tradeo between internal and external validity in experiment design .
we ensured that each program ts in a single screen to avoid participants scrolling up and down to see the entire code.
additionally we prepared all tasks using google forms to avoid heterogeneous environments and installing software on di erent machines.
that is we provided to the participants only a static window i.e.
no ides no tools no navigation.
beyond three features?
it is entirely likely that the linear relationship we observed in observation breaks down at some point for some higher degrees of variability.
presumably at some point developers will be unable to simply cope with the exponentially many combinations of features.
however this is beyond the scope of our study.
.
related work variability bugs.
previous studies have shown negative aspects of preprocessor usage such as code pollution no separation of concerns and error proneness 5cd6461b6fb51e8cf297a49074fce825e1960774 .
these studies are predominantly artifact based so based on studying programs not investigating human abilities to work with the code.
recently medeiros and co authors interviewed developers to study their perceptions of the c preprocessor .
the developers assess that preprocessor related bugs are easier to introduce harder to x and more critical than other bugs.
many admit that they check only a few con gurations of the source code in practice when testing their implementations.
our experiment con rms these qualitative insights and complements them with quantitative data.
medeiros et al.
investigated syntactic errors in preprocessor based systems.
they noticed that developers introduce syntax errors when changing existing code and adding preprocessor directives and that some of the relevant errors survive in the life cycle all the way to the release stage.
in this paper we work with human developers not artifacts which allows us to quantify the e ort of bug nding.
also we are concerned not with syntactic but with semantic errors.
ribeiro et al.
conducted a controlled experiment to evaluate whether emergent interfaces reduce e ort and number of errors during code change tasks involving feature code dependencies.
in general they found a decrease in codechange e ort and number of errors when using their tool support.
emergent interfaces are an example of tooling that attempts to simplify reasoning about variability.
our experiment con rms the need for more research on such tools.
bug finding.
oman et al.
compared debugging abilities of novice intermediate and skilled student programmers using two pascal programs.
among other things they found that programmers ability to nd errors increases with general programming experience they become faster and make fewer mistakes.
comparing to our study we did not design the experiment to directly compare novices versus experts even though we discussed some indications.
our observations suggest that ph.d. students are not faster at nding variability bugs than m.sc.
students.
but the former appear to be more careful and meticulous when debugging than the latter.
furthermore we studied a di erent phenomenon which is variability trying to measure the impact of the degree of variability on debugging.
program comprehension.
feigenspan et al.
in a series of controlled experiments show that use of distinct background colors improves comprehension of ifdefs .
this is one important reason why are we using colours in the experiment instead of preprocessor directives.
in accordance with their work the bug nding with actual ifdef directives should likely be slower than with colours.
another controlled experiment applied functional magnetic resonance imaging fmri to measure program comprehension .
they found that ve di erent brain regions associated with working memory attention and language processing become activated for comprehending source code.
however variability was not in their focus.
we in turn focused on quantifying the e ect of the degree of variability on debugging.
schulze et al.
studied the in uence of the discipline of preprocessor annotations on program comprehension.
they found that the discipline of annotations has no in uence at all.
our observations agree in that nding bugs with variability is time consuming and di cult.
we are however able to quantify the increase of di culties when the degree of variability grows.
.
conclusion we have presented a controlled experiment quantifying the impact of variability on the time andaccuracy of bug nding in highly con gurable systems.
we observe that bug nding time appears to increase linearly with the degree of variability.
this conclusion is both positive and negative.
an increase in variability complicates bug nding negative but not dramatically so positive if developers reasoned about each of the variants separately we would have observed an exponential not linear growth.
the practical implication is that it is bene cial to introduce variation points into programs from the debugging perspective it is bene cial to pay a linear price for bug nding if the alternative is to maintain a super linear set of variants at least up to three variations in a le .
however there might be bene ts in selecting designs architectures and algorithms that require less variability if possible.
somewhat expectedly the variance in bug nding time is ampli ed by variability.
in other words di erences in bugnding competences of developers appear to be ampli ed when working on software projects with variability.
getting talented developers for such projects might be important.
we also nd that most participants correctly identify bugs in programs with accuracy that is independent of the degree of variability.
however developers often fail to exactly identify the set of erroneous con gurations and this happens already for a rather low number of features and gets worse with the degree of variability increasing.
clearly reasoning about multiple con gurations is a challenge.
this is consistent with earlier qualitative indications that variability bugs appear when developers unintentionally ignore an execution that is enabled by an unexpected for them con guration of features.
in fact our study suggests that for higher degrees of variability it is more di cult to correctly identify the set of erroneous con gurations than to nd the bug in the rst place.
this is rather unexpected given that to understand the bug one needs to reason about control ow a temporal non local phenomenon that is not obviously simpler than combinatorics.
this means that it is bene cial to work on support tools that help developers to navigate the con guration space on top of ow oriented bug nders .
the future follow up on this work is expected to design tools exploiting the results of the study in particular indicating the sets of con gurations impacted by a program change in order to simplify reasoning about all ows that a change participates in cf.
observations and .
additionally further research like replicating this experiment with more programs larger programs more subjects higher degrees realistic programs and tools is required to confront our observations and to draw new ones.
it would be also interesting to replicate our study using fmri or eye tracking to better explain the impact of variability on debugging.
with this it would be possible to actually see how developers approach programs with di erent degrees of variability.