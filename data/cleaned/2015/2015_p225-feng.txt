test report prioritization to assist crowdsourced testing y ang feng1 zhenyu chen1 james a. jones2 chunrong fang1 baowen xu1 1state key laboratory for novel software technology nanjing university nanjing china 2department of informatics university of california irvine usa zychen software.nju.edu.cn yang.feng jajones uci.edu abstract in crowdsourced testing users can be incentivized to perform testing tasks and report their results and because crowdsourced workers are often paid per task there is a nancial incentive to complete tasks quickly rather than well.
these reports of the crowdsourced testing tasks are called test reports and are composed of simple natural language and screenshots.
back at the software development organization developers must manually inspect the test reports to judge their value for revealing faults.
due to the nature of crowdsourced work the number of test reports are often di cult to comprehensively inspect and process.
in order to help with this daunting task we created the rst technique of its kind to the best of our knowledge to prioritize test reports for manual inspection.
our technique utilizes two key strategies a diversity strategy to help developers inspect a wide variety of test reports and to avoid duplicates and wasted e ort on falsely classi ed faulty behavior and a risk strategy to help developers identify test reports that may be more likely to be fault revealing based on past observations.
together these strategies form our divrisk strategy to prioritize test reports in crowdsourced testing.
three industrial projects have been used to evaluate the effectiveness of these methods.
the results of the empirical study show that divrisk can signi cantly outperform random prioritization divrisk can approximate the best theoretical result for a real world industrial mobile application.
in addition we provide some practical guidelines of test report prioritization for crowdsourced testing based on the empirical study and our experiences.
categories and subject descriptors d. .
testing and debugging keywords crowdsourcing testing test report prioritization natural language processing test diversity1.
introduction the idea of crowdsourced testing has gained recent attention in the software engineering community e.g.
.
in crowdsourced testing crowdsourced workers are given testing tasks to perform and the workers choose their tasks perform them and submit a test report of the behavior of the system.
in this way the crowdsourced workers help the centralized developers reveal faults.
in order to attract workers testing tasks are often encoded in interesting tasks such as games or testing tasks can be nancially compensated.
in this paradigm the workers perform the tasks and then submit their test reports which are simple and informal descriptions of the behavior of the software system.
these test reports are composed of natural language descriptions sometimes accompanied with screenshots and an assessment as to whether the worker believes that the software behaved correctly i.e.
passed in testing parlance or behaved incorrectly i.e.
failed in the latter case the test report can serve as an informal bug report.
at the software development organization developers and testers receive the crowdsourced test reports and subsequently judge their merit attempt to recreate failures and debug any true faults.
naturally this task can be timeconsuming and tedious even in traditional testing scenarios but moreover in a crowdsourced setting the number test reports can be prohibitive.
further crowdsourced workers may choose to perform many smaller and less impactful tasks.
in practice it is often impossible to manually inspect all test reports in a limited time.
past research has produced test case prioritization techniques e.g.
in which test cases from a regression test suite are prioritized so that they execute in an order that reveals faults earlier.
although such techniques do not address the concept of prioritizing test reports these techniques motivating principles provide inspiration for our approach to prioritizing test reports namely diversi cation of test behavior to help identify multiple faults.
another body of existing research attempts to automatically classify bug reports in a bug reporting system by their level of severity e.g.
.
although such techniques do not address the concept of prioritizing test reports which are less structured and report both passing and failing behavior they inspire another motivating principle of our approach namely recognizing patterns of risk factors that may foretell reports that reveal faults.
in this paper we propose a strategy to prioritize test reports for use in crowdsourced testing.
we adopt natural permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
copyright is held by the owner author s .
publication rights licensed to acm.
esec fse august september bergamo italy acm.
... .
225language processing nlp techniques word segmentation and synonym replacement to extract keywords from test reports.
these keywords are used to predict failure risks of tests and calculate distances of test reports.
we design two single prioritization strategies the risk strategy risk and the diversity strategy div .
the risk strategy is designed to dynamically select the most risky test report for inspection in each iteration.
as such its purpose is to prioritize test reports most likely to reveal faults.
the diversity strategy is designed to select the diversi ed test reports for inspection by maximizing the distances to already inspected test reports.
as such its purpose is to prioritize test reports most unlike already inspected reports to avoid redundant work and reveal di erent faults.
finally in order to reveal as many faults as possible as early as possible we combine the two strategies to a hybrid prioritization strategy divrisk .
in we conducted three projects that used crowdsourced testing with our industry partner baidu1 .
the three projects were used to evaluate the e ectiveness of testreport prioritization methods.
the average percentage of faults detected apfd and the fault detection rate were introduced to compare four test report prioritization methods random risk div anddivrisk .
the best and the worst theoretical results of test report prioritization were computed to discover the room for improvement of our prioritization methods.
the results of empirical study indicate that divrisk can outperform the random prioritization technique signi cantly .
.
improvement in terms of the apfd metric divrisk can approximate the best theoretical result the gap is only .
in terms of apfd of crowdsourced testing for the mobile application testing.
the main contributions of this paper are as follows.
to the best of our knowledge this is the rst work to identify the problem of test report prioritization for crowdsourced testing.
our practical experiences lead us to believe that this is an important problem in crowdsourced testing.
a novel feature of our test report prioritization method divrisk is to combine the risk strategy and the diversity strategy.
three industrial projects of crowdsourced testing are used to evaluate test report prioritization methods.
some practical guidelines of test report prioritization in crowdsourced testing are summarized.
.
background motivation in this section we describe some background of crowdsourced testing to motivate the prioritization of test reports and describe our experience that motivates our prioritization strategies.
we also introduce an example test report which is used to demonstrate how our prioritization techniques work in next section.
crowdsourced testing study.
in we conducted three projects of crowdsourced testing with baidu .
figure shows the procedure of crowdsourced testing in our projects.
testers in baidu prepared packages for crowdsourced testing software under test and testing tasks.
testing tasks were divided into some sub tasks.
the packages were distributed online and workers bid on testing tasks.
1baidu is the largest chinese search service provider.
figure the procedure of crowdsourced testing workers were required to complete tasks in a limited time days in our projects .
then workers submitted test reports online.
workers submitted thousands of test reports due to nancial incentive and other motivations.
these test reports had many false positive results in our projects i.e.
a test report marked as failed that actually described correct behavior.
test reports also contained many redundant behaviors because workers preferred to reveal simple faults instead of complex faults.
testers manually inspected all test reports to judge the workers performance i.e.
their values for revealing faults.
this was a time consuming and tedious process nearly days in our projects .
hence it motivated us to prioritize test reports to improve the e ectiveness of inspection in crowdsourced testing.
the three software systems on which we conducted crowdsourced testing are as follows p1 the rst project is baidu input2on android which can support several input methods.
testers in baidu provide functionality sets.
one crowd worker can select one functionality set and each functionality set can be selected by at most two crowd workers who use di erent mobile phones and di erent versions of android.
p2 the second project is baidu browser3 which is a web browser.
testers in baidu provide seven functionality sets for regression testing.
one crowd worker can select three functionality sets.
p3 the third project is baidu player4 which is a multimedia player.
testers in baidu provide three performance testing scenarios.
one crowd worker should cover all of these three scenarios.
workers can report other problems such as usability and compatibility problems in test reports.
example test report.
figure shows a test report committed by a worker in p25.
workers are required to commit test reports in a same format to facilitate processing them by testers.
a test report is a four tuple fe i o dg in which e i o anddare as follows.
eis test environment including hardware and software con guration etc.
5test reports are written in chinese in our projects.
in figure we translate them into english to facilitate understanding.
226environment e input i output o description d operating system windows sp1 os version no ms windows .
.
system language chinese screen resolution 1920x1080select menu!options in the browser set ad block closed in the security page open the link default.aspx and oating ads or ads around the edge of the web page are found select menu!options in the browser set ad block enhanced in the security page open the link to check switch the browser mode refresh the page to check again.screenshots when the blocking mode is switched the number of blocked ads is not consistent with the previous one.
figure example test report from p2 iis test input including input data and operation steps.
ois test output some screenshots.
dis test result description any information for understanding faults.
experience and lessons.
the crowdsourced workers submitted over test reports.
of these submitted test reports were labeled as failed and as such were gathered for manual inspection.
upon manual inspection of all test reports that were labeled as failed of the failed test reports were false positives.
in other words out of test reports described behavior that was either correct behavior or behavior that was considered outside the behavior of the studied software system e.g.
external problems such as advertisements .
as an example the test report in figure provides an example of an actual submitted test report from the study.
the test report describes the problem of some inconsistent advertisements in di erent modes.
the test report is false positive because it is not a fault of baidu browser but instead of the advertisement host site.
through informal and extensive discussions with professional test engineers at baidu a number of observations and lessons were learned .
the number of test reports submitted by crowdsourced workers quickly became challenging to manually inspect.
a larger crowdsourced testing session would have produced prohibitively many reports to manually inspect.
.
the number of false positives were more numerous than would have been expected and presented challenges for inspection.
.
many of the true positives and false positives were duplicates of the same underlying behavior.
.
many crowdsourced workers performed many easy tasks and reported shallow bugs presumably due to the incentive structures that reward quantity of submitted reports.
.
the word choice among the true positive and false positive test reports were su ciently consistent when accounting for word variations and synonyms.
based on these observations by test engineers at baidu and informed by our discussions with them we attempted to assist with the processing and inspection of test reports particularly for the scenario of crowdsourced testing for which the plethora of reports would be even greater.
lessons and simply motivate the need for some automated assistance.
lessons and motivate the need for looking for diversity in test reports test reports that are duplicate whethertrue positives or false positives present the opportunity for wasted inspection e ort and delayed identi cation of new true faults.
lesson motivates the use of natural language techniques to categorize test reports in an e ort to automatically infer duplicate test reports.
as such our experiences and interactions with our industrial partners motivate us to use natural language techniques i.e.
nlp to cluster test reports.
lessons and have motivated the need to prioritize these clusters to account for diversity i.e.
ourdiv strategy .
however because the goal of such prioritization is to reveal as many faults as early as possible we have incorporated an additional strategy that we are calling risk .
the risk strategy learns from already inspected test reports that were manually assessed as true positive i.e.
true failures that revealed true faults in the software system under test.
as such the risk strategy guides the prioritization order toward other test report clusters that include similar words.
finally we note and recognize that the motivations for divandrisk are in a way at odds divseeks to nd the next test report cluster most dissimilar from the already inspected ones whereas risk seeks to nd the next test report cluster most similar to already inspected true positives.
to account for these contrasting motivations we created a hybrid strategy divrisk that incorporates both div and risk to both maximize the distance from inspected test reports and thus reduce inspection of duplicates and false positives and guide the search toward riskier software behavior and thus increase discovery of new true positives .
.
methodology in this section we present our test report prioritization methods in detail.
figure shows the framework of test report prioritization which mainly contains four steps test report collection test report processing keyword vector modeling and prioritizing test reports.
.
test report collection in our crowdsourcing projects all test reports were committed online by workers in excel les.
we prede ned the format of excel les such that these test reports strictly contained four parts e i o andd.ianddwere used to inform keywords for use by the nlp techniques.
eando were additionally used to assist testers in test report inspection.
running example.
in order to demonstrate our methods we sample seven test reports iandd in p2 as shown in table .
tr1 tr5 tr6 and tr7 are false positive test reports.
that is workers mark them as failed test reports but testers inspect them and judge that they 227test reportstest reportstest reportstest reportsword segmentationsynonymdictionarysynonym replacementbuild keyword dictionarybuild keyword vectorprioritize test reportsdevelopercrowdsourced workerswordscleaned wordskeyworddictionarykeywordvector modelfigure the framework of test report prioritization are not.
tr2 and tr4 reveal the same fault denoted by fault1 .
tr3 reveals another fault denoted by fault2 .
tr7 is the example in figure .
please note that all test reports are written in chinese and our implementation is written to handle chinese test reports.
in order to facilitate understanding we translate them into english in the paper as shown in table .
.
test report processing as shown in figure test report processing contains two steps word segmentation and synonym replacement.
word segmentation.
word segmentation is a basic nlp task.
there are many e cient tools of word segmentation for di erent languages .
we adopted ictclas6for word segmentation which is a widely used chinese nlp platform.
ianddof test reports were segmented into words marked with their part of speech pos in the context and then the pos tagging was applied.
hidden markov models were used in the pos tagging .
finally the bi gram model was introduced to count the classes of words.
synonym replacement.
in crowdsourced testing test reports are committed by part time workers or self identi ed volunteers who are often from di erent workplaces.
workers have di erent preferences of words and di erent habits of expression.
some words in test reports are meaningless for revealing faults.
hence we ltered out these useless words often referred to as stop words in the nlp literature .
prior studies show that verbs and nouns are most important to re ect the content of a document .
hence we retained only verbs and nouns as candidate keywords of test reports and ltered out other words.
also workers often use di erent words to express the same concept.
for example thumb keyboard and nine grids keyboard refer to the same layout of keyboard in chinese.
we introduced the synonym replacement technique in nlp to alleviate this problem.
in our method we adopted the synonym library of language technology platform ltp which is largely considered as one of the best cloud based chinese nlp platforms.
example.
in our example keywords are extracted from test reports shown in table .
for example compatibility indicates that tr1 tr2 and tr4 may report some compatibility problems menu indicates that tr6 and tr7 may report some problems related to menu options.
.
keyword vector modeling the next step is to build the keyword vector model kv.
we then create the risk vector rvand the distance matrix dm based onkv.
dictionary.
keywords extracted from test reports play an important role in test report prioritization.
in order to summarize the information contained within the keywords we count the frequencies i.e.
the number of occurrence of keywords.
in practice we set a threshold to remove some keywords with low frequency to improve the e ectiveness.
as a result a keyword dictionary is built.
example.
table shows the keyword dictionary of the test reports.
in the example i.e.
all keywords with frequency in table are removed to produce table .
keyword vector.
based on the keyword dictionary we construct a keyword vector for each test report tri ei ei ei m in which mis the number of keywords in keyword dictionary.
we compute that ei j if theith test report contains the jth keyword in keyword dictionary and ei j otherwise.
example.
table shows the keyword vector model kvof the seven test reports in which the ith row is the keyword vector of tr i i.e.
kv i tri.kvis ann mmatrix forntest reports and mkeywords in keyword dictionary.
risk vector.
keywords in a test report re ect their values of revealing faults to some extent.
for example the most frequent word is click in table .
however we cannot claim that click is the most important one for revealing faults because click is a common operation in a browser.
we can simply count the number of s in the keyword vector as the risk value of test report denoted by rv i pm j 1ei j. rvis ann vector for ntest reports as shown in table .
distance matrix.
based on the keyword vector matrix kv we can calculate the distances of each pair of test reports.
in this work we adopt the hamming distance.
that is for two keyword vectors triandtrk we count the number of di erent ei jandek jin the corresponding position j as the distanced tri trk .
the inverse distance indicates the similarity of test reports.
example.
as a result we construct an n ndistance matrix for ntest reports.
for example the distance matrix of the seven test reports is shown in table .
d tr1 tr2 for tr1 and tr2 have di erent keywords d tr1 tr7 for tr1 and tr7 have di erent keywords in the keyword dictionary.
.
prioritization strategy in this subsection we present three prioritization strategies risk div anddivrisk based on the risk vector rv and the distance matrix dm which are calculated based on the keyword vector model kv.
228table keyword vector model kv no tr1 tr2 tr3 tr4 tr5 tr6 tr7 table seven test reports from p2 no.
i d result tr1 login renren.com in compatibility mode click on personal homepage or send a gift to friends then after clicking the back button click the forward button.the page content is not consistent before clicking the back button with the content after clicking the forward button.nonfault tr2 enter compatibility mode login renren.com click one of the friend links then click the personal homepage button click the back button after loading.it can go back to friend page only after clicking the back button twice.fault1 tr3 open the browser select tools!options!security set ad block enhanced input in the address bar.ads on the lower right of the page are not blocked successfully.fault2 tr4 in the input box in baidu homepage search group buying in compatibility mode.
next search ice cream and the red bull double click the back button and then click the forward button once.the page content is not consistent before clicking the back button with the content after clicking the forward button .fault1 tr5 open the browser in maximized mode wait for the program to load and then switch the program which means rapid and continual full screen switch.sometimes the bug appears when taskbar at the bottom of the system do not disappear especially when open other browser simultaneously .
when the system is busy the bug is more likely to occur.
move the cursor onto some task and after the appearance of the task the system operates correctly.nonfault tr6 select menu!options in the browser set ad block closed in the security page open the link and pop up ads are found while loading select menu !options in the browser set ad block enhanced in the security page open the link to check again.ads blocking failed.
nonfault tr7 select menu!options in the browser set ad block closed in the security page open the link and floating ads or ads around the edge of the web page are found select menu!options in the browser set ad block enhanced in the security page open the link to check switch the browser mode refresh the page to check again.when the blocking mode is switched the number of blocked ads is not consistent with the previous one.nonfault risk.
in order to reveal faults as early as possible it is natural to give the top priority to inspect the most risky test report i.e.
the test report tr iwith the highest risk valuerv i .
if multiple test reports share the highest risk value one of them is selected for inspection.
let qtr be theordered set of already inspected test reports.
example.
based on the risk values in table tr7 rv is rst selected for inspection.
then tr6 rv is selected for inspection followed by tr2 rv .
at this point of processing qtr ftr7 tr6 tr2g.
we adopt a dynamic prioritization strategy based on the risk values and the inspection results.
that is if tr kis inspected and determined to be a true failure all keywords of trkinkvare increased by in our projects .
the algorithm of updating kv is shown in algorithm .
based on the new kv the risk values in rvare updated.table keywords from test reports no.
keywords tr1 compatibility n mode n login v click v person n homepage n friend n gift n back v button n forwad v page n content n tr2 enter v compatibility n mode n login v click v friend n link n person n homepage n button n load v back v page n tr3 open v browser n tool n options n security n ads n block v select v address n input v page n corner n not v tr4 compatibility n mode n input v groupon v click n search v button n icecream n redbull n back v forward v result n tr5 open v browser n maximize v condition n wait v program n load v nish v do v switch v fullscreen n appear v system n task n miss v possibility n mouse n thumbnail n restore v tr6 browser n click n menu n options n security n page n ads n block v close v open v link n load v nd v strength n check v fail v tr7 browser n click n menu n options n security n page n ads n block v closed v open v link n appear v oating v strength n check v switch v mode n refresh v button n select v change v number n table keyword dictionary no.
word freq.
no.
word freq.
k1 button k2 strength k3 homepage k4 input k5 person k6 switch k7 browser k8 friend k9 options k10 login k11 check k12 back k13 mode k14 block k17 click k18 ads k19 load k20 menu k21 security k22 select k23 link k24 page k25 forward k26 compatibility that is for each i rv i pm j 1kv i j .
example.
because tr2 is determined to be a true failure the risk values of tr1 tr3 tr4 and tr5 are updated to .
.
.
.
.
and .
.
respectively.
that is for tr1 tr3 tr4 and tr5 there are and same keywords as tr2 respectively.
in this way we can get the nal prioritization result of test reports qtr ftr7 tr6 tr2 tr1 tr3 tr4 tr5g.
div.
thediv strategy is based on the diversity principle of test selection .
we prefer to select the test report triwith the maximal distance to qtr .
without confusion qtr is also used to denote the set of keyword vectors ftrig of already inspected test reports.
the distance of trand qtr denoted byd tr qtr is de ned by the maximum distance between trand eachtriinqtr i.e.d tr qtr max tri2qtrfd tr tr i g. example.
we use the seven test reports to demonstrate div based on the distance matrix in table .
initially the most risky test report tr7 is selected thus qtr ftr7g.
for the next test report since the maximum distance isd tr1 qtr tr1 is selected.
thus 229table distance matrix dmand risk vector rv dm tr1 tr2 tr3 tr4 tr5 tr6 tr7 rv tr1 tr2 tr3 tr4 tr5 tr6 tr7 algorithm updatekv kv k for alljdo ifkv k j 0then for allido kv i j kv i j end for end if end for returnkv qtr ftr7 tr1g.
and then tr5 is selected because d tr5 qtr is the maximum distance for the remained test reports.
in this way we can get the nal prioritization result of test reports qtr ftr7 tr1 tr5 tr3 tr4 tr6 tr2 g. divrisk.
in order to reveal faults as early as possible and as many as possible risk anddivare combined to a hybrid strategy divrisk .
the algorithm of divrisk is shown in algorithm .
the risk value vector rvand distance matrix dm can be calculated based on kv line .
initially the most risky test report is selected for inspection line .
then a candidate set ctr is constructed by selectingnctest reports with maximum distance s d tri qtr line .
the most risky test report in ctr is selected for inspection line .
if the inspected test report is a failed one and the keyword vector kvwill be updated by algorithm and the risk value vector rvwill also be updated line .
finally the prioritization result qtr is returned.
algorithm divrisk kv nc for eachi j dm i j d kv i kv j for eachi rv i pm j 1kv i j trf1 ng nis the number of rows in kv qtr ftrkg trkwith the highest risk value rv k intr qtr qtr ftrkg tr tr ftrkg whilejtrj6 0do ctr selectncreports tr iwith the largest distances d tri qtr select the test report tr kwith the highest risk value in ctr for inspection qtr qtr ftrkg tr tr ftrkg iftrkis a failed test report by inspection and 0then kv updatekv kv k for eachi rv i pm j 1kv i j end if end while returnqtr example.
we use the seven test reports to demonstrate divrisk .
initially tr7 is selected for inspection for it is most risky.
qtr ftr7g.
since the number of test reports is small in this example we set nc to facilitate demonstration.
the candidate set ctr ftr2 tr1g ford tr2 qtr andd tr1 qtr are the twotable summary of test reports project p1 p2 p3 report f report f report .
.
.
fault largest ones.
tr2 is selected for inspection for tr2 is more risky than tr1 i.e.
rv rv .
in this way we can get the nal prioritization result qtr ftr7 tr2 tr3 tr4 tr6 tr1 tr5 g. the hybrid strategy divrisk will be reduced to the risk strategy risk ifnc jtrj and it will be reduced to the diversity strategy div ifnc .
hence we need to set a modest number to nc nc in our projects for a reasonable hybrid result.
.
experiment design in this study we evaluated our test report prioritization methods risk divanddivrisk with three crowdsourced testing projects.
in our projects andnc as described above.
.
comparison baseline in order to verify the e ectiveness of our prioritization methods three baselines for comparison are selected.
the rst baseline of comparison was the random strategy which is widely used in software testing.
given a set of nite number of test reports all possible orderings of test reports could be enumerated in theory.
supposing that we know which test reports are truly fault revealing in advance the best and the worst prioritization results could be determined.
for example ftr2 tr3 tr4 tr1 tr5 tr6 tr7 gis one of the best prioritization results and ftr7 tr1 tr5 tr6 tr2 tr4 tr3 gis one of the worst prioritization results.
in order to fairly compare these prioritization methods the experiment was repeated times to collect experimental data.
.
test report in our projects all test reports were manually inspected by testers without any prioritization method.
we carefully checked the inspection results again and get the nal inspection results as summarized in table .
in table report is the number of test reports marked as failed by workers.
these test reports were collected in the test report bucket.
testers inspected these test reports to judge whether they could reveal faults.
freport and f report are the number and the percentage of test reports judged as failed ones by testers respectively.
in practice some tests may reveal same faults.
fault is the number of faults revealed by these test reports.
.
research question in the experiment we investigated the following research questions.
rq1 can our prioritization methods improve the effectiveness of test report inspection?
if we have no prioritization method on hand testers will inspect test reports in a random order.
that means testers would be motivated to adopt a prioritization method only if it can outperform the random strategy.
rq1 evaluates 230the e ectiveness of our prioritization methods risk div anddivrisk .
rq2 how large is the gap between our prioritization methods and best ?
in practice it is di cult to design one method that can work well in all cases.
hence it is valuable to know the gap between the on hand methods and the best one in theory.
rq2 evaluates the room for improvement of our prioritization methods.
.
evaluation metric in order to measure the e ectiveness of prioritization methods we adopt the apfd average percentage of fault detected which is a widely used evaluation metric in test case prioritization .
for each fault we mark the index of the rst test report which reveals it.
based on the order of the test reports and information about which test reports revealed which faults we can calculate the apfd values to measure the e ectiveness of the prioritization methods.
a higher apfd value indicates a better prioritization result.
that is it can reveal more faults earlier than the other methods do.
apfd is formalized as follows.
apfd tf1 tf2 tfm n m n in which ndenotes the number of test reports and mdenotes the total number of faults revealed by all test reports.
tfiis the index of the rst test report that reveals fault i. apfd indicates the fault detection rate of all test reports.
however testers cannot inspect a large number of test reports in limited time.
in practice testers will stop inspecting test reports when the limited resource is used up.
at that time testers may only inspect or test reports.
therefore we should evaluate how apfd varies for permutations of the same set of test reports .
we use the linear interpolation as follows.
mdenotes the total number of faults revealed by all test reports.
p2f25 g the percentage used in our experiment.
q m p which is the number of faults corresponding to a percentage.
let int q andfrac q be the integer part and fractional part of q respectively.
if frac q the linear interpolation is needed.
i jare the indexes of reports that reveal at least q and q faults respectively.
the linear interpolation is calculated as i j i frac q the linear interpolation value indicates the cost of testing to detect the given number of faults.
hence a lower value of linear interpolation indicates a better prioritization result.
.
result analysis in this section we analyze the experimental results to answer rq1 and rq2.
the results of all prioritization methods are shown in figure .
figure a c and e shows the boxplots of apfd results of the three projects p1 p3 for the experimental runs.
the prioritization methods are shown on the horizontal axis and the apfd values are shown on the vertical axis.
the blue horizontal line in figure a c and e denotes the best apfd value in theory for thattable bonferroni means separation tests method apfd improvement gap meansx random randombest x x p1 f .
p value divrisk .
.
.
div .
.
.
risk .
.
.
random .
.
best .
.
p2 f .
p value divrisk .
.
.
div .
.
.
risk .
.
.
random .
.
best .
.
p3 f .
p value divrisk .
.
.
risk .
.
.
div .
.
.
random .
.
best .
.
subject.
figure b d and f shows the average growth curves of the three projects p1 p3 .
the percentage of the inspected test reports is shown on the horizontal axis the the percentage of revealed faults is shown on the vertical axis.
.
addressing rq1 rq1 can our prioritization methods improve the e ectiveness of test report inspection?
based on the results shown in figure a c and e we can nd that all of our prioritization methods outperform random .
in particular divrisk can outperform random signi cantly.
the hybrid strategy divrisk can also improve the single strategies risk anddiv.
moreover the box plots show that our methods are substantially more stable than random .
figure b d and f show the average growth curves.
the line charts in figure b and d show thatdivrisk presents smooth curves to the top best .
in order to further investigate our test report prioritization methods we do bonferroni means separation tests for all results in table .
all f values are very large and the all p values are much smaller than .
in table .
compared with the random strategy the percentage of improvement ofdivrisk ranges .
.
.
in summary the experimental results are encouraging for the use of the hybrid divrisk strategy in practice.
in summary we nd that our prioritization methods can improve the e ectiveness of test report inspection.
.
addressing rq2 rq2 how large is the gap between our prioritization methods and best ?
figure shows that the hybrid strategy divrisk provides the best approximation of the best result in p1 and p2.
for p3 divrisk provides one of the best results but there is a larger gap between its results and the best result than we found for p1 and p2.
for more details we can observe the growth curves in figure .
the curves of best grow very fast.
the curves of divrisk reach the curves of best when we have inspected nearly test reports in p1 p2 and nearly test reports in p3.
table shows the gaps between our prioritization methods and best .
the gap between divrisk andbest on p1 is small .
.
please recall that the results of best are a apfd on p1 b average fault detection rates on p1 c apfd on p2 d average fault detection rates on p2 e apfd on p3 f average fault detection rates on p3 figure experimental results times 232table linear interpolation the average number of inspected test reports pro.
tech.
p1random .
.
.
.
risk .
.
.
.
div .
.
.
.
divrisk .
.
.
.
best .
.
.
.
p2random .
.
.
.
risk .
.
.
.
div .
.
.
.
divrisk .
.
.
.
best .
.
.
.
p3random .
.
.
.
risk .
.
.
.
div .
.
.
.
divrisk .
.
.
.
best .
.
.
.
purely hypothetical and based on an unrealistically omniscient best case analysis.
hence the result of divrisk may be or at least approximate the best one in practice.
the gaps on p2 and p3 may be thus acceptable .
and .
in practice and moreover do improve the ordering of unordered or random ordering.
in order to explain the results more clearly we calculate the linear interpolations shown in table .
table shows the average numbers of inspected test reports in the cases of detecting and faults.
if we need to reveal or faults divrisk is near to best .
however if we need to reveal more faults there may be room for additional improvement.
a strange phenomenon is worthy of attention risk outperforms divrisk for the level of inspected faults faults for p2 and the level of inspected faults for p3.
this result may be due to the heuristic nature of these methods and will be a subject of additional investigation in the future.
in summary we nd that our prioritization methods can provide a reasonable approximation for the theoretical best result for some software subjects and for other subjects provide some of the smallest gaps.
in all cases that we studied it provided better than the unordered or random ordered test reports.
.
discussion method selection.
the idea of prioritization is widely used in software engineering especially in software testing.
crowdsourced testing is usually conducted in rapidly iterative software development.
in this situation we can only inspect a subset of test reports for revealing and xing faults before software release.
hence test report prioritization plays a key role for a cost e ective result of crowdsourced testing.
our prioritization methods contain two key parts the risk strategy risk and the diversity strategy div .
in software development we need to reveal as many faults as possible i.e.
div.
in contrast we need to inspect the most probable true failure test reports early i.e.
risk .
these two requirements of crowdsourced testing drive us to design a hybrid prioritization method divrisk by combiningrisk anddiv.
therefore it is not surprising that divrisk can outperform the random prioritization technique signi cantly.
mobile application testing.
divrisk shows di erent e ectiveness in di erent crowdsourcing projects.
the p1 project involves mobile application testing.
the e ectiveness of divrisk in p1 was very encouraging and approxi mated best .
we reviewed the test reports in p1 and discussed with testers in baidu.
since workers used di erent mobile phones and di erent versions of android they reported many compatibility problems of the application under test.
the compatibility problems were easier to identi ed than other problems for mobile applications.
moreover part time workers crowd workers here preferred to select testing tasks of mobile applications because it could be done anywhere and any time.
therefore it is not surprising that the prioritization results of p1 as shown in table were more e ective than on p2 and p3.
workers committed more test reports and revealed more faults on p1 than on p2 and p3.
the percentage of useful test reports i.e.
f report is .
which was better than p2 .
and p3 .
.
the high quality test reports can help our test report prioritization methods because our methods rely on keywords from test reports.
as such such crowdsourced testing and prioritization methods may be a good t for mobile application testing.
cost and scalability.
the total cost of test report processing in our projects is less than minutes.
please note that our prioritization algorithms only involve numerical calculation on kv rvanddm.
hence the cost of test report prioritization methods may be negligible.
the divrisk algorithm is exible.
for example we can set in algorithm and as a result the dynamic prioritization strategy are reduced to a static prioritization strategy.
the static prioritization strategy does not rely on inspection.
hence it can be fully automated and be more e cient although the results may be worse.
moreover divrisk does not rely on the languages of test reports.
divrisk can also be used for test reports written in other languages by using other nlp tools for other natural languages.
for example we can adopt stanford corenlp7for word segmentation and wordnet8for synonym replacement to process english test reports and build keyword vector model kv.
based onkv we can use the divrisk algorithm for english test report prioritization.
.
threats to validity there are some general threats to validity in our empirical study.
for example we need more projects and di erent parameter values to reduce the threat to validity.
subject selection bias.
these three crowdsourced testing projects are from industry.
the software products are widely used on the internet and were not especially designed for our study.
due to the limited cost we only required the industry partner to provide crowdsourced testing tasks that could be nished in days.
the cost of conducting our empirical study was very expensive involving more than people so we have only three projects in our empirical study.
this may threaten the generalization of our conclusions.
however the software products used in our crowdsourced testing projects are diverse.
this may reduce the threat to some extent.
crowd worker relation.
crowdsourcing often requires workers from a large pool of individuals that one has no direct relationship with the others .
in our experiment the students play the roles of crowd worker which means 233our crowd workers have certain social relations and we have only nearly crowd workers.
the results may be di erent if the crowd workers are from internet with open calls.
however salman et al.
found that that if a technique is new to both students and professionals similar performance can be expected to be observed.
as such we believe that this may not be a key point for our test report prioritization techniques.
data quality.
the materials of crowdsourced testing are prepared and distributed by the industrial testers.
all test reports are committed by workers online directly.
we checked all data and participated in the discussions of the nal inspection results.
in summary all data used in this paper are from industry and the results were checked carefully by professional testers of baidu.
this may reduce the threat to the validity of data quality.
.
related work in this section we discuss three areas related to our work test case prioritization failure report classi cation and crowdsourced testing.
test case prioritization.
test case prioritization has been intensively studied in the past decades .
we only discuss some test case prioritization techniques using distance.
w. dickinson et al.
studied cluster ltering techniques and proposed an adaptive sampling strategy to select all tests in a cluster when a failed test is inspected.
yan et al.
proposed esbs inspired by the intuitions of fault localization that uses spectra information in clustered test selection.
jiang et al.
studied test case prioritization in regression testing and proposed a new family of coveragebased adaptive random testing techniques to replace traditional random testing.
fang et al.
introduced ordered sequences of program entities to improve the e ectiveness of test case prioritization and proposed several novel similaritybased test case prioritization techniques based on the edit distances of ordered sequences.
ledru et al.
proposed techniques to prioritize test cases based on the text of test data rather than code or speci cations and provided empirical results on di erent distances.
yoo et al.
proposed a cluster based test case prioritization technique incorporating expert knowledge to reduce the cost of pair wise comparisons.
test case prioritization techniques rely on surrogates for fault detection such as statement coverage and hope that satisfying these surrogates earlier will lead to an increasing fault detection rate .
most of test case prioritization techniques use execution pro les whereas our prioritization techniques use test reports in natural language.
test case prioritization increases fault detection rate by executing test cases.
test report prioritization increases fault detection rate by inspecting test reports.
failure report classi cation.
failure report classi cation is also related to test report prioritization.
runeson et al.
investigated using nlp techniques to support the identi cation of duplicated failure reports.
they took the words in the failure reports in plain english processed the text and then used the statistics on the occurrences of the words to identify similar reports.
lo et al.
addressed software reliability issues by proposing a method to classify software behaviors based on past history or runs.
bowringet al.
classi ed program behavior using execution data.
dhaliwal et al.
reduced the bug xing time by using the stack traces and runtime information to group the crash reports triggered in di erent usage scenarios.
wang et al.
proposed a technique combining natural language and execution information to detect duplicate failure reports.
these techniques take some information whether natural language or runtime information about failures and attempt to classify them.
test report prioritization in contrast attempts to not only nd duplicates but to order the test cases in a way that facilitates faster inspection by testers.
crowdsourced testing.
crowdsourced testing is already popular in industry and it is a fairly new trend in software engineering research community.
crowdsourcing is the act of taking a job traditionally performed by a designated agent usually an employee and outsourcing it to an unde ned generally large group of people in the form of an open call .
liu et al.
applied crowdsourced testing in usability testing.
they studied both methodological di erences for crowdsourcing usability testing and empirical contrasts to results from more traditional face to face usability testing.
pastore et al.
studied whether it is possible to exploit crowdsourcing to solve the oracle problem generated test input depends on a test that oracle requires human input in one form or another.
dolstra et al.
used crowdsourced testing to accomplish the expensive tasks on gui testing.
they use virtual machines to run the system under test and enable semi automated gui testing by crowd workers.
nebeling et al.
evaluated the usability of web sites and web based services with crowdsourcing data where they showed that crowdsourcing data could provide an e ciently and e ective testing method to the web interfaces.
all the studies above use crowdsourced testing to solve some problems in traditional software testing activities.
however we propose test report prioritization methods to solve the problem of crowdsourced testing.
.
conclusions in this paper we proposed a novel test report prioritization method divrisk to reduce the cost of inspection in crowdsourced testing.
the keywords are extracted from test reports by using nlp techniques.
these keywords construct a keyword vector model kv.
we calculate the risk vectorrvbased onkv to predict failure risk of tests.
we construct the distance matrix dm based onkv to design the diversity strategy for prioritization.
the risk strategy and the diversity strategy are combined to a hybrid strategy divrisk to ful ll e ective test report prioritization.
three crowdsourced testing projects from industry have been used to evaluate the e ectiveness of test report prioritization methods.
the results of empirical study encourage us to use divrisk for test report prioritization in practice especially for mobile application testing.
we also provide guidelines to extend our prioritization methods to deal with test reports written in other languages.