better test cases for better automated program repair jinqiu yang alexey zhikhartsev yuefei liu lin tan j223yang azhikhar yuefei.liu lintan uwaterloo.ca electrical and computer engineering university of waterloo waterloo on canada abstract automated generate and validate program repair techniques g v techniques suffer from generating many overfitted patches due to in capabilities of test cases.
such overfitted patches are incorrect patches which only make all given test cases pass but fail to fix the bugs.
in this work we propose an overfitted patch detection framework named opad overfitted patchdetection .
opad helps improve g v techniques by enhancing existing test cases to filter out overfitted patches.
to enhance test cases opad uses fuzz testing to generate new test cases and employs two test oracles crash and memory safety to enhance validity checking of automatically generated patches.
opad also uses a novel metric named o measure for deciding whether automatically generated patches overfit.
evaluated on bugs from large systems the same benchmark used by genprog and spr opad filters out .
overfitted patches generated by genprog ae kali and spr.
in addition opad guides spr to generate correct patches for one more bug the original spr generates correct patches for bugs .
our analysis also shows that up to of such automatically generated test cases may further improve g v techniques if empowered with better test oracles in addition to crash and memory safety oracles employed by opad .
ccs concepts software and its engineering software testing and debugging keywords overfitting in automated program repair patch validation testing acm reference format jinqiu yang alexey zhikhartsev yuefei liu lin tan.
.
better test cases for better automated program repair.
in proceedings of 11th joint meeting of the european software engineering conference and the acm sigsoft symposium on the foundations of software engineering paderborn germany september esec fse pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse september paderborn germany copyright held by the owner author s .
publication rights licensed to association for computing machinery.
acm isbn .
.
.
.
introduction automated generate and validate program repair techniques g v techniques show promising results to reduce manual quality assurance efforts and to improve software reliability.
g v techniques automatically generate patches to repair buggy programs with the guidance of test cases and validate the correctness of the generated patches using the same set of test cases.
despite the great potential g v techniques suffer from generating incorrect patches due to in capabilities of test suites .
qi et al.
pointed out that of the patches that are generated by genprog are incorrect.
a large portion of such incorrect patches are equivalent to deletion of buggy functionalities.
these incorrect patches make test cases pass after the entire buggy code is removed simply because the test cases do not cover the expected correct behaviors of the buggy code.
for example for the bug libtiff 1ba75 an arithmetic bug genprog generates incorrect patches that remove an integer overflow check to make these given test cases pass because they do not expose the integer overflows.
following previous work we call such incorrect patches overfitted patches since they are overfitted to pass only the given tests but fail to fix the bugs.
overfitted patches prevent g v techniques from generating correct patches.
the terminating condition of g v techniques is to make all given test cases pass thus once an overfitted patch is generated g v techniques often stop exploring other patch candidates.
this happens when the failing test case cannot well define the bug and or if the original passing test cases fail to define all correct behaviors of the software.
thus we need to improve test cases to precisely decide whether the generated patches overfit and make g v techniques continue to generate correct patches.
there are limited prior efforts to enhance test cases for large and complex systems to further improve g v techniques.
previous studies focus on illustrating the impacts of low quality test suites on the quality of automatically generated patches.
recent work on this direction demonstrates the challenges of using automated test generation to improve g v techniques in a realworld setting.
xin et al.
design a new test generation technique to cover the generated patches by g v techniques.
however it requires correctly patched programs to get oracles e.g.
expected outputs which is difficult to obtain in practice.
in this work we propose an overfitted patchdetection framework opad that combines automated test generation two oracles crash and memory safety and a novel overfitness metric to detect overfitted patches.
first opad improves existing test suites to better define bugs and preserve the desired functionalities from two angles generating new test cases automatically and leveraging additional oracles i.e.
memory safety oracles to improve validity checking of automatically generated patches.
for opad applies 831esec fse september paderborn germany jinqiu yang alexey zhikhartsev yuefei liu lin tan fuzz strategies on input from existing test cases to automatically generates new test cases .
for in addition to crash oracles for automatically generated tests and manual oracles for existing test cases e.g.
expected output from developers opad obtains memory safety oracles using valgrind a well known memory bug detection tool to ensure program memory safety.
this approach is analogous to the treatment of sickness to determine if a patient has recovered analogous to whether a bug has been fixed by a patch in addition to checking if symptoms have been improved doctors often order laboratory tests such as blood tests analogous to generating new tests and check if medical metrics such as white blood cell counts have been improved compared to those when a patient is sick analogous to the improved validity checking .
second opad leverages a novel metric the overfitness measure o measure in short to assist the improved test suite in detecting overfitted patches.
the proposed o measure is shown to be an effective approximation of the ideal metric that can best distinguish a correct patch from an overfitted patch.
a prior study shows that deciding whether a patch is overfitted using whether the patched version fails on any of the additional tests is imprecise in distinguishing overfitted from correct patches.
different from this prior study our o measure is built based on the assumption that a correctly patched program should not behave worse than the corresponding buggy program e.g.
fail on more test cases .
third we investigate how many of opad s automatically generated test cases have the potential to filter out more overfitted patches if empowered with better test oracles in addition to crash and memory safety through a post mortem manual analysis.
the usefulness of automatically generated test cases is limited by weak oracles.
thus we believe that there exist some automatically generated test cases that although they currently do not contribute to identifying overfitted patches due to the limitation of oracles have the potential to filter out more overfitted patches.
we call such test cases weakly relevant to the target bug.
we apply opad to improve four g v techniques genprog ae kali and spr in generating patches for bugs.
opad automatically generates between to new test cases per bug which include both passing and failing test cases.
our evaluation shows that opad filters out a significant portion .
of overfitted patches generated by the four g v techniques.
with opad genprog ae kali and spr generate correct patches for and bugs respectively.
by filtering out overfitted patches opad helps spr generate a correct patch for one additional bug libtiffd13be ccadf compared to the original spr vanilla spr generates correct patches for bugs .
in other words without our approach spr fails to generate this correct patch.
although many overfitted patches are filtered out opad does not always lead to the generation of more correct patches since to generate the correct patch alloverfitted patches that precede the correct one in the search space must be filtered and the search space must contain the correct patch.
our relevance analysis shows that for each bug up to of automatically generated test cases are relevant to the target bug.
the result indicates that a large portion of the automatically generated test cases may filter out more overfitted patches if empowered with better test oracles.
in summary this paper makes the following contributions we enhance test suites for improving g v techniques.
we formulate the ideal and theoretical metric for determining if a generated patch is overfitted and propose a novel practical metric for it.
we explore and identify a scalable and practical approach to enhance existing test suites by generating new test cases and leveraging two oracles crash and memory safety .
we evaluate the proposed approach by applying it to improve four g v techniques to repair large and complex systems.
we conduct a relevance analysis on automatically generated test cases we identify promising test cases that can filter out more overfitted patches if empowered with better test oracles in addition to crash and memory safety .
availability .
we make the data from this work available at background on automated g v program repair we briefly describe how g v techniques genprog kali ae and spr automatically generate patches given a buggy version and given both failing and passing test cases.
figure shows the typical structure of g v techniques at the top and the structure of our patch validation framework at the bottom .
g v techniques start with a buggy program i.e.
code that contains a target bug and a set of failing and passing test cases to define the target bug.
then g v techniques utilize spectrumbased fault localization techniques to narrow down the scope of the faulty source code.
after that g v techniques use specific approaches to construct a search space of patch candidates genprog ae rsrepair and kali leverage different template based operators to generate patch candidates i.e.
fixes and spr uses parameterized templates.
after constructing the search space g v techniques iterate the patches in the search space until they find a patch that can pass the patch validation i.e.
whether the patch can make the same set of test cases pass .
despite their differences g v techniques share the same techniques of fault localization and patch validation.
imperfect patch validation e.g.
using the same test cases for both patch generation and validation may lead to overfitted patches .
overfitting hinders the effectiveness of g v techniques when there is a correct patch in the search space g v techniques risk failing to present it to the developer due to the correct patch being preceded by overfitted patches.
in this paper we propose an approach that filters out overfitted patches that precede the correct patch and helps g v techniques generate more correct patches.
832better test cases for better automated program repair esec fse september paderborn germany figure overview of the proposed overfitted patch detection framework opad and how opad is integrated with g v techniques.
approach overview.
figure shows an overview of the proposed overfitted patchdetection framework opad for validating the correctness of automatically generated patches.
we also show how opad can be used to improve g v techniques.
opad employs automatic test generation two test oracles crash and memory safety and a metric overfitness measure o measure to assess the correctness of automatically generated patches.
first to generate new test cases opad leverages fuzz testing and uses existing test suites as fuzzing seeds.
second for all test cases including automatically generated test cases and developers original test cases opad employs two additional oracles a crash and a memory safety oracle e.g.
buffer overflows uninitialized variables and memory leaks to improve validity checking of automatically generated patches.
third based on the validity results for each automatically generated patch opad uses o measure to decide whether a patch is overfitted.
figure shows how opad complements g v techniques by deciding whether a generated patch is overfitted.
opad guides g v techniques to continue choosing the next patch candidate in the search space if a patch is identified as overfitted.
challenges.
there are two main challenges in designing an overfitted patch detection framework based on automatically generated new test cases for large and complex systems.
the first challenge is how to leverage the generated tests and bug detection tools to determine if a patch is overfitted.
a naive approach is that if a patch causes any automatically generated test to fail the improved validity checking e.g.
the patched version contains a memory bug as reported by a bug detection tool then we consider the patch overfitted.
however this approach is likely to filter out correct patches because there are other irrelevant bugs i.e.
bugs are not related to the target bug in the program.
a correct patch may correctly fix the target bug but fail to fix other irrelevant bugs in the program i.e.
bugs that are not targeted by the g v tool current g v approaches are designed to fix only the target bug as defined by developer failing test cases .
to address such irrelevant bugs opad uses o measure section .
that only considers a patch to be overfitted if the patched program performs worse than the buggy program under the same set of tests.
our assumption is that a correctly patched version should not behave worse than the buggyversion e.g.
the patched version should not fail on the test cases on which the buggy version passes.
section .
presents how and why we define o measure.
the second challenge is the lack of test oracles developer written tests usually contain manually defined test oracles e.g.
assert statements that compare the expected output of a program with the actual output however it is an open challenge to automatically generate such test oracles .
to address this challenge we leverage two oracles crash and memory safety to help ensure the correctness of the patches.
these two oracles are correct because programs should not crash under any circumstances i.e.
a crash is a definite indication of a bug in the program and should not violate memory safety e.g.
memory leaks .
by adding new test cases the memory safety oracles can guarantee memory safety of more code execution paths in the program by patched g v techniques.
.
generating new test cases using fuzz testing in order to generate new test cases we use fuzz testing a well established bug finding technique that feeds the program under test with randomly generated input.
we choose fuzz testing due to the following constraints when improving g v techniques on large and complex systems.
first fuzz testing is scalable to large and complex systems i.e.
programs of millions of lines of code .
currently many other advanced automatic test generation techniques do not work for programs of such scale.
second fuzz testing can be applied to a wide spectrum of software from image manipulation programs to interpreters .
finally our benchmark consists of c programs for which there are limited tools available unlike other languages there are well established tools e.g.
randoop and evosuite for java .
primitive fuzzing techniques rarely find errors deep within programs control flow because the randomly generated input is usually rejected at early stages of error checking.
to mitigate this issue mutation based fuzzing was proposed .
mutation based fuzzers perform random mutations on well formed input which allows mutated input to pass initial sanity checks and trigger the bugs that lie deeper in the program.
in this work we use american fuzz lop afl a coverage guided fuzz testing tool to generate new test cases for the bugs in the evaluated benchmark.
afl is a mature mutation based fuzz testing tool that detects significant vulnerabilities in mature c projects .
afl works by applying mutation rules on input by selecting the new input that explores new paths to achieve higher coverage and by continually mutating the newly created input until all inputs are explored or afl is terminated manually.
.
generating memory safety oracles opad employs memory safety oracles on both newly automaticallygenerated test cases and developer test cases to improve validity checking of automatically generated patches.
weak oracles e.g.
checking only whether a program crashes are not sufficient to guarantee program correctness.
this is true for both developer test cases and automatically generated test cases.
to mitigate this opad enhances validity checking of patches by inspecting the quality of memory management and ensuring memory safety.
to validate large and complex systems we need a practical and scalable memory safety checker.
we chose dynamic analysis over 833esec fse september paderborn germany jinqiu yang alexey zhikhartsev yuefei liu lin tan figure sets of failing test cases on the buggy version b the versions with overfitted patches ovfp and the version with correct patch corrp .
static analysis since static analysis tools may generate too many false positives.
this makes static analysis unsuitable for our purpose since false positives might erroneously prune overfitted patches not due to the defect in the patch in addition false positives are likely to prune correct patches as well.
opad leverages valgrind i.e.
memcheck for memory safety oracles.
specifically opad applies valgrind with each test case i.e.
either from a developer or automatically generated and records the detection results from valgrind i.e.
memory errors and leaked memory bytes .
valgrind inspects memory safety by instrumenting the program under test keeping track of validity of all unallocated allocated memory and reporting errors once memory safety is violated.
valgrind can detect various memory related problems such as using undefined values accessing already freed memory and memory leaks.
.
measuring the overfitness of a patch using an overfitness metric o measure in this subsection we present our definition of o measure .opad uses o measure to determine whether automatically generated patches overfit.
then we provide a justification about why our proposed definition of o measure works best under both theoretical and practical constraints for g v techniques.
.
.
defining o measure.
we propose a metric o measure to identify overfitted patches.
the proposed o measure is calculated based on the results of executing test cases both developer and automatically generated against two oracles crash and memorysafety .
we present the definition of o measure and how to use omeasure to decide overfitness of patches below.
definition .
.
given a test suite t b the set of test cases that make the buggy version fail b t b the set of test cases that make the buggy version pass b t p the set of test cases that make the patched version fail p t .
o measure is defined as the size of b p. definition .
.
a patch is overfitted if it has a non zero o measure and not overfitted otherwise.
.
.
calculating o measure.
opad executes each test case on both versions the buggy and the patched versions and records the oracle related execution results i.e.
whether the program crashes and memory safety detection results .
based on the results opad calculates o measure to determine the overfitness of patches.
if o measure is non zero for a patch opad determines the patch to be overfitted and not overfitted otherwise.
it is straightforward to calculate o measure for test cases with crash oracles.
for memory safety oracles opad decides whether a test case contributes to o measure b p by checking whether the patched version exposes more memory issues than the buggy version.
different from crash oracles for which the result is a binary value whether the program crashes memory safety oracles produce comprehensive memory detection results.
thus simply using whether memory safety is violated for deciding failure is not sufficient.
instead we calculate o measure by checking whether the patched version exposes more memory issues than the buggy version.
for example valgrind reports memory errors e.g.
use of uninitialized values and the number of bytes leaked definitely indirectly possibly lost .
if for a test case the patched version contains extra memory errors or extra leaked bytes of the three types above mentioned the value of o measure of this patch is incremented by one.
.
.
reasons behind our choice of o measure.
the proposed definition of o measure definition .
is merely one possible way to define overfitness of patches.
we illustrate why we propose this o measure definition from both theoretical and practical aspects.
the ideal overfitness measure o measure .
we define the ideal o measure as the o measure that can perfectly distinguish overfitted patches from correct patches.
figure demonstrates the relationship among the sets of failing test cases on the buggy version annotated as b on the correctly patched version corrp and on the overfittedly patched version ovf p .
we use b corrp and ovf p to annotate the sets of passing test cases on the buggy version the correctly patched version and the overfittedly patched version.
in figure the five regions are highlighted r1isb ovf p corrp r2isb ovf p corrp r3isb ovf p corrp r4is b ovf p corrp and r5isovf p corrp .
the ideal o measure should be able to differentiate between correct and overfitted patches.
this means that there exists at least one test case that shows different behaviors i.e.
fail or pass on the oracle on the two versions i.e.
the one with the correct patch and the one with an overfitted patch .
so the ideal o measure for deciding overfitness is the size of the set ovf p corrp ovf p corrp r1 r4 r5in figure .
if the ideal o measure of a patch is nonzero this patch is overfitted as it has different behaviors from the correct patch on at least one test case.
from the ideal o measure to our definition of o measure definition .
.
the ideal o measure is annotated as r1 r4 r5 figure .
in the context of automated program repair the correct patch is not available.
this means that r5 which is a subset of corrp is hard to approximate in practice.
thus we take the firststepapproximation of the ideal o measure using r1 r4 a subset of ovf p .
however r1 r4 a subset of ovf p still cannot be directly computed due to the unavailability of corrp r2andr3cannot 834better test cases for better automated program repair esec fse september paderborn germany be excluded precisely.
we take the second step approximation usingr3 r4in figure our o measure definition definition .
to approximate r1 r4by excluding r1and including r3.r1is b ovf p corrp and r3isb ovf p corrp .
the inclusion ofr3in inevitable to approximate r4due to the unavailability of correct patch.
below we illustrate why the exclusion of r1is a reasonable choice in the context of using opad to improve g v techniques.
first we prove that for a particular type of bugs and their corresponding overfitted patches r1is empty in theory.
if r1is empty r1 r4 the first step approximation described above equals to r4.
thus for these cases using r3 r4 our o measure a superset ofr4 will identify all overfitted patches that can be identified by the first step approximation of the ideal o measure .
we describe the proof in proving the emptiness of b ovf p corrp for specific cases below.
we manually investigate how many bugs and their corresponding overfitted patches genprog benchmark that we use for evaluation fall into this particular pattern that we prove.
r1is empty for of the bugs bugs for which there is at least one overfitted patch from the four g v techniques and their corresponding overfitted patches.
second r1has to be approximated using r1 r2 b ovf p .
such approximation introduces the inclusion of r2.
since r2is part ofcorrp and is not part of the ideal o measure the inclusion of r2causes two risks ineffectiveness in filtering out overfitted patches especially if r2 b ovf p and incorrectly filtering out correct patches.
empirically we find that both of the two risks are true in the evaluation the bugs in the genprog benchmark the patches from the four g v techniques genprog ae kali and spr and automatically generated test cases by opad .
particularly for of the overfitted patches in the evaluation r2 b ovf p. this shows that using b ovf pto approximate r1is ineffective to filter out overfitted patches since for most cases r1is empty.
in addition b corrp i.e.
r1 r2when an automatically generated patch is correct instead of overfitted is not empty for correct patches of of the bugs.
this shows that using r1 r2as o measure or part of o measure would incorrectly filter out correct patches for of the evaluated bugs.
this echoes with previous work which shows that using ovf pas o measure is ineffective.
in summary we choose the definition of o measure definition .
due to both theoretical and practical concerns.
the intuition behind our o measure is that the patched program should not behave worse than the buggy program.
proving the emptiness of b ovf p corrp for specific cases .this proof is to show that for a particular type of bugs and their corresponding overfitted patches the proposed o measure is the most reasonable metric to distinguish between correct and overfitted patches.
note that the proposed o measure is not tied to this particular type of bugs and it also applies to other bugs as shown in the evaluation .
we first describe the particular type of bugs and its corresponding overfitted patches and then show that for these bugs and patches there do not exist test cases in r1 b ovf p corrp in figure .
first the code structure of this particular type of bugs is if cond s1 elses2 wheres1ands2are code statements.
second this particular type of bugs and their corresponding overfitted patches satisfy the following conditions which constitute of the studied benchmark a1 irepresents the entire input space i i1 i2andi1 i2 .
a2 on a buggy program b for every input iini s1is always executed.
we use b i s1 to represent that on a buggy program s1is executed for every input in i. in the context of g v techniques there must exist at least one test case i.e.
a pair of an input iand an oracle so that b i s1 leads to a failure as the oracle is not satisfied.
this proof should cover all possible test cases in theory.
it is unnecessary and unrealistic to obtain the result of executing every possible test case because the proof is generalizable for both cases b i s1 leads either a failure or a pass.
a3 overfitted patches modify conditions to redirect every input in ito execute s2.
a4 correct patch makes the failing test case pass by redirecting every input in i1to execute s2 while keeping every input ini2to execute s1.
a5 both overfitted and correct patches change program executions by only modifying cond .
thus such patches have no side effects on other parts of the program other than that the execution flow is changed e.g.
from executing s1 tos2.
this means for example for the same input i the results of executing b i s1 corrp i s1 and ovfp i s1 are the same as long as they all execute s1.
proof of emptiness of b ovf p corrp .we start by inferring the following facts from the conditions f1 corrp i1 s2 .
from a4.
f2 ovfp i1 s2 .
from a1anda3.
f3 corrp i1 s2 ovfp i1 s2 .
from f1 f2anda5.
this means that corrp i1 s2 and ovfp i1 s2 have the same result i.e.
either both failure or both pass.
f4 corrp i2 s1 .
from a4.
f5 b i2 s1 .
from a1anda2.
f6 corrp i2 s1 b i2 s1 .
from f4 f5 and a5.
similar to f3.
we prove by contradiction.
if b ovf p corrp is not empty there exists at least one test case that satisfies all three conditions fails on b denoted as condition fails on ovfp condition and passes on corrp condition .
from a1 the input of such test case must be either i1ori2 if the input is i1 based on f3 corrp i1 s2 and ovfp i1 s2 should have the same result either both failures or both passes.
this means that condition 2andcondition 3cannot be satisfied at the same time and if the input is i2 based on f6 corrp i2 s1 andb i2 s1 should have the same result thus condition 1and condition 3cannot be satisfied at the same time.
835esec fse september paderborn germany jinqiu yang alexey zhikhartsev yuefei liu lin tan thus such test case that satisfies all the three conditions does not exist which means b ovf p corrp is empty.
the proof above shows that the proposed o measure is the most reasonable one for this particular type of bugs.
in addition the omeasure also works well for other bugs as shown in the evaluation .
.
an optimized setting of opad opad calculates o measure based on running test cases against test oracles.
since opad uses o measure by only asserting whether it is zero or not opad can be optimized by deciding a patch is overfitted as soon as o measure becomes non zero.
for example for a patch from g v techniques once a test case new or developer test case against test oracles i.e.
crash or memory safety fails on the patched version but not on the buggy version opad decides this patch is overfitted.
furthermore when examining the next patch from the search space of g v techniques opad can prioritize running the test cases with oracles that have contributed to filtering out overfitted patches before.
in our evaluation we evaluated opad without this optimization to get a full understanding of the effectiveness of o measure unless specified.
however we find that by using this optimization we can significantly speed up opad e.g.
from over to less than minutes for opad to guide spr to generate a correct patch for libtiff d13be ccadf a loose condition bug .
evaluation in this section we present the experimental setup and the three research questions we answer.
experimental setup.
we evaluate opad on the same set of bugs evaluated by previous work genprog ae kali and spr .
particularly we select all bugs for which at least one of the four repair tools have generated at least one patch.
in total we apply our approach on bugs from systems and corresponding patches both overfitted and correct ones that are generated by g v techniques.
to generate new test cases we feed afl section .
with input from non crashing developer test cases i.e.
test cases that do not make the program crash .
such non crashing test cases include all passing test cases and some failing test cases if the failures are observed by non crash oracles e.g.
defined expected output .
the reason is that afl by its design does not mutate crashing test cases in order to avoid focusing on the exact same crash.
we terminate afl when no new paths are explored within two hours since afl may keep running without manual interruption.
afl leverages coverage to guide the mutation for better performance and the coverage is obtained by running executables from the program under test.
for some evaluated systems that contain more than one executable we only apply afl on the executables that are identified to expose the target bug by developer test cases.
we run each automatically generated test case against crash oracles ten times to mitigate possible non determinism.
this number was chosen as an acceptable trade off between efficiency of running test cases and efficacy of mitigating non determinism.
the experiment is primarily conducted in the virtual machine image released by le goues et al.
except for spr s patches that are obtainedfrom the spr virtual machine .
we host the virtual machines on computers with 16g ram and .
ghz intel i5 cpu.
rq1 how many overfitted patches does opad filter out?
motivation.
identifying overfitted patches is crucial for g v techniques since it allows them to continue exploring the search space to eventually find the correct patch.
note that it is not realistic for g v techniques to iterate the entire search space to find all patches that make the test cases pass.
as stated in a recent study there can be up to thousands of overfitted patches per search space.
so stopping at the first patch that makes all the test cases pass is a reasonable design choice for g v techniques.
even if one generates all patches that make the test cases pass filtering out overfitted patches could still save developers time in selecting the correct one as often a few correct patches are hidden among many overfitted patches .
approach.
we evaluate opad on four automated g v techniques genprog ae kali and spr to study whether our approach can correctly filter out overfitted patches while preserving correct patches.
specifically for genprog ae and kali the generated patches are publicly available.
so we apply our approach on the released patches and report how many overfitted patches are successfully pruned byopad .
we obtained the ground truth for correct and overfitted patches from qi et al.
.
a patch is correct if it fixes the bug.
conversely a patch is overfitted if it merely causes the test cases to pass and does not fix the bug.
for the bugs that we evaluate the correct patches from g v techniques are semantically equivalent to developer patches.
results.
in total opad filters out .
overfitted patches from the four automated g v techniques.
table shows the overall result of filtering out overfitted patches.
table contains all the bugs in total from the genprog benchmark for which at least one of the four g v techniques can generate patches i.e.
overfitted or correct .
since ae is an adaptive version of genprog based on a different search algorithm we merge genprog and ae into one column.
to show the improvement from different components of opad we show the number of pruned patches in four settings using crash oracles on new test cases from fuzz testing column crash fuzz using memory safety oracles on developer test cases column mem.
dev.
and using memory safety oracles on new test cases from fuzz testing column mem.
fuzz using the combination of all the above opad column all .
genprog ae often generate several patches for a bug so we show the total number of patches per bug in column genprog ae total .
kali generates one patch per bug which is a total of overfitted patches from kali we omit the column that shows the total number of patches for kali .
for some bugs that spr has correct patches in the search space we set spr to continue exploring the search space until a patch is accepted by opad .
therefore the number of patches from spr that are filtered out by opad may be more than one.
column spr total shows the total number of overfitted patches from spr that are evaluated by opad .
the three components of opad mostly complement each other using crash oracles on fuzz test cases filters out overfitted 836better test cases for better automated program repair esec fse september paderborn germany table the results of using opad to filter out overfitted patches from genprog ae kali and spr.
total is the number of overfitted patches evaluated by opad for kali this number is always one unless a kali s patch is correct .
check symbol means that genprog ae kali or spr find the correct patch and these correct patches are not incorrectly pruned by our approaches.
double check symbol means that opad guides spr to generate a correct patch original spr does not generate this correct patch .
bug genprog ae kali spr total crash mem.
mem.
all crash mem.
mem.
all total crash mem.
mem.
all fuzz dev.
fuzz fuzz dev.
fuzz fuzz dev.
fuzz gzip 3fe0 39a3 gzip a1d3 f17c libtiff 1ba75 libtiff 5b021 3dfb3 libtiff 90d13 4c666 libtiff d13be ccadf libtiff ee2ce b5691 lighttpd lighttpd lighttpd lighttpd lighttpd lighttpd python python python python python python wireshark wireshark wireshark wireshark php php php php php php php php php php php php php php php php php php php php gmp gmp sum patches crash fuzz using memory safety oracles on developer test cases filters out overfitted patches mem dev.
and using memory safety oracles on fuzz test cases filters out overfitted patches mem fuzz .opad does not filter out correct patches incorrectly for ten bugs.
we use or in table to annotate the bugs that opad preserves the correct patches for them.
opad filters out three correct patches libtiff 5b021 3dfb3 php and php because the correctly patched programs behave worse than 837esec fse september paderborn germany jinqiu yang alexey zhikhartsev yuefei liu lin tan table results of using opad to improve spr spr opad on the bugs from the genprog benchmark.
each cell contains two symbols.
the first symbol shows whether spr opad generates a correct patch y or n and the second symbol shows how opad contributes in the patch generation process filtering out overfitted patches not filtering out patches neither overfitted nor correct b filtering out both overfitted and correct patches and filtering out correct patches only.
bug id crash mem.
mem.
all bug id crash mem.
mem.
all fuzz dev.
fuzz fuzz dev.
fuzz gzip a1d3d f17cb n n n n php n n n n libtiff 5b021 3dfb3 n b n n n b php y y y y libtiff d13be ccadf y n n y php y y y y libtiff ee2ce b5691 y y y y php n n n n python n n n n php y y y y php y y n n php n n n n php y y n n php y y y y php y y y y php y y y y php n n n n gmp y y y y php y y y y 1inttiffwritedirectorytagcheckedrational double value ... 2assert value .
failed assertion the earliest version if value uint32 value ... else if value .
... if value .
the current version ... ... figure a bug hidden in the buggy version of libtiff 5b0213dfb3.
the buggy programs based on o measure i.e.
either crash or fail the memory safety oracles on some test cases while the buggy program does not .
this happens because there are some hidden bugs in the buggy version and such hidden bugs are exposed after the patches are applied.
such hidden bugs are exposed in the patched version once the patch changes the control flow of the program some of these hidden bugs are later fixed by the developers .
we show an example of a hidden bug.
in libtiff 5b021 3dfb3 a hidden bug that is caused by a failed assertion is newly exposed by a correct patch line in figure .
we reported the bug to libtiff developers and the bug has been fixed1.
this failed assertion should have been removed after the functionality had been changed.
the earlier version of this function contains the assertion to abort the program if value is invalid.
however this function was later modified to capture an invalid value in theif branch line and the assertion became obsolete.
the buggy version exits before reaching the assertion due to the nature of the target bug but the correctly patched version continues the execution until the assertion fails this results in a non zero o measure for the correct patch.
nonetheless the generated test cases and our approach should help developers fix the new bugs in the patched version and ultimately improve the quality of the software.
rq2 can opad guide spr to generate correct patches for more bugs?
motivation.
we want to evaluate whether opad can improve automated g v techniques in terms of generating correct patches for more bugs.
in this evaluation we focus on spr because spr if nstrips buggy if nstrips developer if nstrips overfitted compression compression none stripbytecount !
stripbytecount tiffwarning wrong field ignoring and calculating from imagelength if estimate tif ... gotobad figure patches for libtiff d13be ccadf a loose condition bug .
is shown to have great potential there are many correct patches in the spr search space that are not discovered because they are blocked by overfitted patches.
a prior study shows that there are no more correct patches in the search space of genprog ae and kali to be discovered for the bugs in this evaluation.
therefore although our approach filters out many overfitted patches generated by genprog ae continuing running genprog kali will not generate more correct patches.
in contrast spr has correct patches for eight more bugs in its search space but fails to generate correct patches due to having too many overfitted patches .
approach.
we integrate opad with spr to see if opad can guide spr to generate correct patches for more bugs see the integration in figure .
particularly whenever spr generates one patch that makes existing test cases pass that patch will be validated by opad by calculating o measure based on new test cases and two oracles .
if this patch is determined as overfitted by opad o measure is nonzero spr will continue exploring the search space until it finds the next patch that can pass both the original validation developer test cases and opad .
results.
our approach guides spr to generate a correct patch for one additional bug spr previously fixed bugs .
table shows the results of applying spr opad on the bugs from the genprog benchmark for which there are correct patches in spr s search space.
we use y to annotate the case that opad helps spr generate the correct patch.
for libtiff d13be ccadf a loose condition bug spr cannot generate a correct patch without opad our approach prunes overfitted patches that block the correct one.
finding the correct patch .
we describe how opad exposes the flaws in overfitted patches for libtiff d13be ccadf a loose condition bug filters them out and finds the correct patch.
the bug is in the image reading routine simplified code is presented in figure .
the function estimate at line should only be called when input images are ill formed.
however since the condition at line is incorrect estimate is called for some well formed images as well.
the correct patch fixes the condition so that estimate is only called when it should be.
however an overfitted patch removes the entire branch line by adding 0to the condition.
thus estimate is never called even for ill formed images.
when some automatically generated test cases exercise the overfitted patch with ill formed images one garbage item which should otherwise be cleaned up in the estimate routine is used as an array index and this leads to a segmentation violation.
all the overfitted patches that precede the correct one has the described flaw and therefore have a non zero o measure and are filtered by opad .
thus with the help of automatically generated test cases opad successfully guides spr to generate the correct patch.
838better test cases for better automated program repair esec fse september paderborn germany rq3 how many of opad s automaticallygenerated test cases may filter out more overfitted patches if empowered with better oracles?
motivation.
the usefulness of test cases in filtering out overfitted patches is limited by oracles i.e.
oracles might fail to precisely define correct behaviors.
generating effective oracles is an open challenge in software testing .
even if an automatically generated test case explores relevant program paths which is required to identify an overfitted patch the test case cannot filter out overfitted patches without sufficient oracles.
this does not mean such a test case is useless in identifying overfitted patches.
if with better oracles e.g.
manually defined oracles automatically generated regression oracles orassert statements to distinguish different program states such test cases may filter out more overfitted patches.
we call such test cases weakly relevant to the target bug that we want to repair.
approach.
we first define weakly relevant test cases then we describe how we perform relevance analysis manually.
a test case is weakly relevant if it exposes the target bug inexplicitlyby showing differences in program states.
a weakly relevant test case usually does not expose the target bug explicitly due to the limitations of its oracle.
instead running weakly relevant test cases shows the differences in the program states between the buggy version and the correctly patched version.
this means that if empowered with better oracles weakly relevant test cases can explicitly tell the differences between the buggy version and the correctlypatched version.
for example libtiff 1ba75 an arithmetic bug is a bug in a check for an integer overflow due to a developer s mistake many benign inputs that do not contain an integer overflow are rejected.
for this specific bug we say that the test case essentially the input of the test case is weakly relevant if it is rejected by the buggy version and accepted by the developer version.
another example is php a bug is in the savehtml routine of the domdocument class.
invoking savehtml with an optional parameter generates empty results this behavior is incorrect .
we define a test case to be weakly relevant if it contains a call to thesavehtml with a parameter.
for each bug we manually investigate the root cause of the target bug and create methodologies that can identify weakly relevant from all the automatically generated test cases.
then we instrument the buggy version and the correctly patched version at the points of interest run each test case on both versions and use the collected data to determine the relevance of the test case.
for example for the bug libtiff 1ba75 an arithmetic bug described above the root cause of the bug is that many benign inputs are erroneously rejected.
to find weakly relevant test cases we instrument the buggy version and the correctly patched version using gcov coverage instrumentation and observe the number of times the input was rejected by each version.
if the buggy version rejects the input from a test case mtimes the correctly patched version rejects the same input ntimes and m n then we say that this particular test case is weakly relevant.
the detailed methodology of weakly relevance of each bug is on our project website.
results.
our relevance analysis shows that up to of the automatically generated test cases are weakly relevant totable weakly relevant test cases from opad through manual analysis.
bug id weakly total bug id weakly total relevant relevant gzip 3fe0c 39a36 php gzip a1d3d f17cb php libtiff 1ba75 php libtiff 5b021 3dfb3 php libtiff 90d13 4c666 php libtiff d13be ccadf php libtiff ee2ce b5691 php lighttpd php lighttpd php lighttpd php lighttpd php lighttpd php lighttpd php python php python php python php python php python php python php wireshark php wireshark wireshark wireshark the target bug and they may filter out more overfitted patches if empowered with better oracles.
table shows the results of our manual relevance analysis on the automatically generated test cases.
the weakly relevant test cases that are identified by relevance analysis could filter out more overfitted patches if empowered with better oracles.
the column weakly relevant presents the number ofweakly relevant test cases from all the automatically generated test cases in opad .
the column total shows the total number of automatically generated test case by fuzz testing for a particular bug.
for example in the case of the bug libtiff 5b021 3dfb3 we discovered weakly relevant test cases out of .
this shows that these test cases can indeed identify the differences between the buggy version and the correctly patched version from program states i.e.
behaviors however these test cases currently cannot do so due to limitations of the oracles.
in summary for bugs there exist automatically generated test cases that can potentially identify more overfitted patches if with better oracles for each bug up to of test cases have such a potential .
these test cases can observe different behaviors between the buggy version and the correctly patched version.
those differences in behaviors can be encoded in oracles e.g.
assert or manually defined expected output to allow weakly relevant test cases to identify more overfitted patches.
threats to validity non determinism .
some studied programs show non deterministic behaviors during the execution of the automatically generated test cases.
for example a program may only crash one out of times given the same input e.g.
due to address space layout randomization .
to mitigate this issue we execute each automaticallygenerated test case times which reduces the risk of getting spurious results and erroneously filtering out a patch.
hidden bugs .
in some cases a correct patch can have a non zero o measure because of hidden bugs.
such correct patches change the control flow of a program and reveal the bugs that were hidden in the buggy version this leads to more crashes in the patched 839esec fse september paderborn germany jinqiu yang alexey zhikhartsev yuefei liu lin tan version compared to the buggy version and results in a non zero omeasure as described in section for libtiff 5b021 3dfb3 .
although the correct patch would not be accepted by opad in this scenario the test cases that we generated would help developers fix such hidden bug manually which remains as future work.
limitations of fuzz testing .
fuzz testing has a limitation of targeting initial levels of input parsing in programs under test .
if a particular patch correctly fixes a bug in deeper levels of programs but the program contains other bugs in initial levels then the fuzz test cases would crash the program without even reaching the patched code.
our definition of o measure eliminates this issue by a comparison with the buggy version both versions will fail on such a test case and the correct patch will not be filtered out.
related work automated program repair .
researchers have been working on various g v techniques.
genprog is the pioneer work in this area followed by par rsrepair kali spr relifix and nopol .
the above mentioned techniques differ from genprog in terms of either search space and or search algorithm.
par uses hard coded patch templates to construct search space.
rsrepair employs a random search algorithm instead of genetic programming which is used by genprog .
kali uses a restricted search space emphasizing on deleting operations and an exhaustive search strategy.
spr which outperforms the previous work constructs search space based on predefined transformation schemas and leverages a targeted search algorithm.
the constructed search space contains more useful patches and provides a larger set of fix templates than that of par .
as an alternative to g v techniques semantic based automatic repair tools are proposed e.g.
directfix angelix .
semantic based automatic repair uses symbolic execution and constraint solvers to synthesize a patch that by design passes all the developer test cases.
our current approach focuses solely on g v systems.
recently innovative approaches are proposed on top of existing automated program repair e.g.
genprog ae and spr .
fan et al.
apply probability model to improve the ranking of patches in search space so that the correct patches can be selected first.
le et al.
propose to prioritizefix candidates based on frequent fix patterns that are mined from software fix history.
tan et al.
use anti patterns to identify overfitted patches e.g.
one anti pattern is to eliminate patches that only contain deletions.
different from prior work we focus on filtering out overfitted patches and generating correct ones from a new angle improving test cases.
using testing to improve g v techniques.
xin et al.
propose techniques to guide test generation techniques to cover patches by g v techniques with an assumption that perfect oracles are already available.
our work shows that basic oracles can improve g v techniques.
yu et al.
aim to leverage test generation to guide g v techniques to generate patches that are less overfitted.
differently we use automatic test generation to improve g v techniques by filtering out overfitted patches and then continuing g v techniques to generate correct patches.
liu et al.
propose a novel technique which leverages the similarity of execution traces to heuristically determine the correctness of the generated patches by g v techniques.
differently we use new test inputs and oracles directly to detect overfitted patches.empirical study on program repair .
barr et al.
study what percentage of fixes can be constructed from historical fixes.
martinez et al.
study the possibility of constructing fixes using the code from the same version i.e.
buggy version .
these papers focus on the theoretical correctness of the fundamental assumption of search based automated program repair whether fixes can be constructed given a search space.
differently our work empirically studies the impacts of the quality of test cases on g v techniques on bugs from systems.
smith et al.
compare the quality of automatically generated patches with developers patches.
in terms of quality of patches they use the passing ratio of an independent set of test cases from white box test generation.
this work focuses on empirically studying whether patches of g v techniques have lower quality or not.
differently our work focuses on ways to distinguish overfitted patches from correct ones.
automated test generation.
there exist different types of automated test generation.
random test generation techniques and fuzz testing tools scale to large systems but lack of direction.
dynamic symbolic execution and concolic testing tools aim to generate test cases which achieve high coverage.
search based test generation techniques integrate search algorithms to guide unit test generation to achieve high coverage.
all the above techniques use crashes as oracles.
alternatively regression oracles are automatically generated by recording variable values during running black box test cases written by developers.
in this work we use crash which is a widely accepted oracle and memory safety oracles to improve validity checking of patches.
empirical study on test effectiveness.
previous work on test suite effectiveness shows that coverage is not strongly correlated with test suite effectiveness and mutation detection is strongly correlated with real fault detection .
zhang et al.
show assertions are strongly correlated with test suite effectiveness.
in this work we study the effectiveness of test cases in the context of automated program repair.
conclusions in conclusion we experiment with ways to improve existing test cases in order to improve g v techniques.
we propose an approach to filter out incorrect patches by augmenting existing test cases.
opad improves existing test cases from two angles better validity checking by employing memory safety oracles and new test cases from fuzz testing.
we propose o measure to filter out overfitted patches based on the new test cases and oracles.
our evaluation on bugs from systems shows that opad filters out .
of the overfitted patches.
more importantly opad helps spr generate the correct patch for one additional bug original spr can only generate correct patches for bugs .
in addition we identify how many of the automatically generated test cases can filter out more overfitted patches if used with better test oracles.
our findings highlight promising research directions on improving g v techniques.