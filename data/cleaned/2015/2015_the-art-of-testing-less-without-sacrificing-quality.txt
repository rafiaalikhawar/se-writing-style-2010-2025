the art of testing less without sacrificing quality kim herzigi kimh microsoft.com michaela greilerii mgreiler microsoft.com jacek czerwonkaii jacekcz microsoft.com brendan murphyi bmurphy microsoft.com i microsoft research united kingdom ii micros oft corporation redmond united states abstract testing is a key element of software development processes for the manage ment and assess ment of product quality.
in most development environments the software engineers are responsible for ensuring the functional correct ness of code .
however for large complex software products there is an additional need to check that changes do not negatively impact other parts of the software and they compl y with system constraints such as backward compatibility perfo rmance security etc.
ensuring these system constraints may require complex verification infrastructure and test procedures .
although such tests are time consuming and expensive and rarely find defect s they act as an insurance process to ensure the softwar e is compli ant.
however long lasting tests increasingly conflict with strategic aims to shorten release cycles.
to decrease production costs and to improve development agility we created a generic test selection strategy called theo that accelerate s test processes without sacrificing product quality.
theo is based on a cost model which dynamically s kips tests when the expected cost of running the test exceeds the expected cost of removing it.
we replayed past development periods of three major microsoft products resulting in a reduction of of test executions saving millions of dollars per year while maintaining product quality.
index terms measurement cost estimation test improvement .
i. introduction software testing is a key element of software dev elopment processe s. the purpose of testing is to ensure that code changes applied to a software product do not compromise product quality.
often testing is associated with checking for functional correctness.
however for large complex software systems i t is also important to verify system constraints such as backward compatibility performance security etc.
complex s ystems like microsoft windows and office are developed by thousands of engineers that simultaneous ly apply code changes which may interfer e with each other.
in such environments testing may be seen as an insurance process verifying that the software product complies with all necessary system constraints at all times.
by nature system and compliance tests are complex and time consuming although they rarely find a defect .
large complex software products tend to run on millions of configuration in the field and emulating these configurations requires multiple test infrastructure s and procedures that are expensive to run in terms of cost and time.
making tests faster is desirable but usually requires enormous development efforts.
simply removing tests increases the risk of expensive bugs being shipped as part of the theo stands for test effectiveness optimization using historic data.
final product.
this is a generic issue for developing large complex software sys tems .
at the same time long running test processes increasingly conflict with the need to deliver software products in shorter periods of time while maintaining or incre asing product quality and reliability.
increasing productivity through running les s tests is desirable but threatens product quality as code defects may remain undetected.
the goal of this work is to develop a cost based test selection strategy called theo1 to improve development processes .
theo is a dynamic self adaptive test selection strategy which does not sacrific e product quality .
theo automatically s kips test executions when the expected cost of running a test exceeds the expected cost of not ru nning it.
we designed theo to ensure that all tests will execute on all code changes at least on ce before shipping the software product.
executing all tests at least once ensures that we eventually find all code defects .
thus theo does not sacrifice product quality but may delay defect detection to later development phases.
theo and its underlying cost model are based on historic test measurements to estimate future test execution costs and causes no test runtime overhead .
additional dynamic or static te st analysis data such as coverage or dependency graphs are not required.
we evaluated the effects of theo by simulating historic development processes for three major microsoft products windows office and dynamics.
combined our simulation results cove r more than month of industrial product development and more than million test executions.
we make the following contributions in this paper we develop a cost model for test executions based on historic test execution results that causes no test execution runtime overhead and is capable of readjusting its cost estimations based on execution contexts e.g.
configuration s of the test environment .
we develop theo a self adapting test selection strategy to accelerate test processes without s acrifici ng code quality .
we evaluate theo by simulating its impact on historic windows office and dynamics developments .
we briefly discuss expected secondary improvements such as developer satisfaction .
in section ii we provide background information about development and verification processes to explain the context of this work motivat ion and methodology are described in section iii.
we define the historic data feeds in section iv.
based on this input we derive failure probability functions section v used in our cost model section vi .
section vii contains a description on simul ating theo on past development processes .
sections viii and ix discuss simulation measurements and results.
secondary improvements are briefly discussed in section x. we close the paper with threats to validity section xi related work section xii and a conclusion section xiii .
ii.
development and verification processes in this section we provide a brief overview of the complex subject o f software development methodologies.
the process for developing large software products is predominantly through developing a single component across multiple code branches e.g.
microsoft windo ws or through developing independent components e.g.
microsoft office which form the product.
when developing code across multiple code branches a code branch is a forked version of the code base that allow s parallel modifications without interferen ce for more details we refer to bird and zimmermann and murphy et al.
.
the alternative is to design the product into multiple independent components.
each component can be develop ed independently on a single code branch .
independent from the development methodology development teams are often responsible for verify ing functional correctness of any code change.
where system constraints exist on the pr oduct additional test infrastructure is required to ensure all code meets those constraints.
since product constraints are system properties they often need to be verified at system level.
the complexity of the verification requirements is dependent upon system size and the number of system constraints.
for example windows uses multiple development branches that integrate into a single trunk branch see fig.
.
.
developer commit their code changes to development branches bra nch b1 in fig.
.
and integrate these code changes through multiple integration branches b2 and b3 in fig.
.
into the trunk branch which contains the current stable version of the next windows release.
each integration path between any two branches is guarded by so called quality gates checking for various constraints e.g.
backward compatibility requirements both in terms of the hardware it runs on and in terms of supported applicati ons.
to verify that windows meets these constraints requires the emulation of millions of different execution setups at each branch level .
the lower the level of branches a quality gate fails the higher the number of affected engineers and the more expens ive a defect becomes.
at the same time any complex test infrastructure evolves over time.
new test are added older tests might get less important or even deprecated.
maintaining tests and preventing test infrastructures from decay can be an enormous effo rt.
for older products there may be a lack of ownership for some of the older tests.
the se tests can affect development speed as the more tests are executed and the longer the runtime of tests the longer the verification process and the slower the development speed e.g.
the time required to integrate code changes into the trunk branch .
additionally verification time depends on the number of test failures.
most test failures require human effort to inspect and to fix the underlying issue.
note that test failures may be due to code defects or due to test reliability issues and that running and analyzing t hese failures is a time consuming task especially at system and integration level.
system tests can run for minutes or hours as they often require entire systems to be set up and tore down.
at the same time developing large complex software systems usually implies large development teams developing code changes in parallel that need to be tested also in parallel .
consequently increasing the effecti veness and efficiency of test processes has an immediate effect on product development.
running less tests might help to improve test performance .
however reducing testing imposes the risk of elapsing defects to later development stages and unnecessary in volving or affecting more developers.
iii.
motivation methodology the goal of this work is to decrease development costs and to increase productivity without sacrificing product quality .
as we discusse d in section ii achieving this goal requires a careful balance between conservative test strategies to minimize the number of defects elapsing into later development stages and reducing test time to allow faster integration processes and higher productivity .
any test optimization strategies should not compromise the quality of the code shipped to customers.
it may be acceptable to find defects later than possible but it remains unacceptable to weaken overall product quality .
our optimization strategy ensures that all test scenario s are executed at least once for each code change before integrating the code change into the main product code base e.g.
the trunk branch.
from the example shown in fig.
.
it is acceptable to skip a test scenario on branch b if and only if the very same test scenario will be executed on branch b3 for the very same code change.
the basic assumption behind this work and most other test optimization and test selection approaches is that for given t2branch b branch b branch b time trunkt1 t6t4 t5 t3 fig.
.
integration path example.
the integration path of change c applied to branch b1 lists the branches b1 b2 b3 and trunk including the timestamps of the corresponding successful merge operations.
scenarios not all tests are equally well suited.
some tests are more effective than others are .
however deciding the effectiveness of tests and when to execute them is not trivial.
the approach presented in this paper uses historic test execution data and development process cost factors to perform test selection.
each test execution is considered an investment and the expected test result considered as return of investment.
using a cost model our goal is to create a dynamic self adapt ive test selection strategy which readjusts its cost estimations based on test execution context .
to provide an actionable solution that fits into the microsoft development processes we considered only solutions that cause no runtime overhead a nd that required no additional data collection.
theo only use s historic test data collected by standard testing frameworks and excludes code coverage data as collecting code coverage can increase the runtime overhead.
later we w ill show that theo ensures all tests are executed at least once for each code change .
therefore it does not affect the overall code coverage of the test process .
iv.
historic test performance data feeds the process collects the results of prior test executions this data is already collected by most test execution frameworks.
the individual data sources we use to select test cases are depict in table i. .
the main da ta collection categories are a. general test execution information the name of the executed test testname and the unique identifier of the test execution instance testexecid are collected .
this data allows us to bind and group test execution results to the according test case.
b. test runtime we use the time taken for the test to run i.e.
the test execution time testexecduration for each test execution as recorded by the test framework.
c. test results further we collect the results of all tests being run within the development process.
a test fail ure is where the expected result of a test could not be produced i.e.
assertion failed or the whole test execution terminated with an error.
usually we assume that a failing test indicates a code defect caused by introducing a defect e.g.
through side effects when merging multiple parallel developed code changes.
however it might also be that the test case reporting the test failure is not reliable.
we call test failures due to test reliabi lity issues false alarms .
categorizing the test result as passing or failing is implicitly given by the testing framework.
however it is also important to further distinguish test failures into code defects and false alarms .
to do so we need additional information.
using links between test failures and bug reports we can distinguish test failures due to code defects from false alarms .
if the failure led to a bug report that was later fixed by applying a code change we ma rk it as a code defect .
otherwise the failed test execution is marked as a false alarm .
to identify their cause test failures always need to be manually inspected to either fix the problem or identify the test failure as false alarm .
due to resource restrictions not all test failures can be investigated.
therefore test failures that were not manually investigated are marked as undecided and ignored as their cause is indeterminable .
d. execution context modern software systems tend to be multi platform applications running on different process or architectures e.g.
x64 and arm different machines and different configurations.
we define an execution context as a set of properties used to distinguish between different test environments.
in this paper we use the execution context properties buildtype architecture language branch see table i. for detailed description .
however the concept of execution contexts is variable.
adding or removing properties will influence the number of different execution contexts but requires no modification of the general approach.
this is a crucial point as a test may show different execution behaviors for different execution contexts.
for example a test might find more issues on code of one branch than anoth er depending on the type of changes performed on that branch.
for example tests cases testing core functionality might find more defects on a branch containing kernel changes than on a branch managing media changes.
thus our approach will not only dif ferentiate between test cases but also bind historic defect detection capabilities of a test to its execution context.
v. test failure probabilities given a planned test execution and given the corresponding execution context we can use past test executions of the same test in the same execution context and derive the number of reported defects and the number of false test alarm that the test reported.
from t hese past observations we can derive two failure probabilities ptp as the probability that the combination of test and execution context will detect a defect true positive and pfp as the probability that the combination of test and executio n context will report a false alarm false positive .
these probabilities are defined as ptp t c detected defects t c executions t c pfp t c false alarms t c execution s t c table i. test execution data u sed for test selecti on.
data point descr iption general test execution information testname unique name of test case executed.
testexecid unique identifier of test case execution.
test runtime information testexecduration number of seconds the recorded test case execution lasted.
test resu lt information testexecresult the result of the test execution.
possible value passed code defect false alarm undecided.
requires interpretation as described in section iv.c execution context information buildtype the bu ild type of the binaries on which the test was executed.
possible value debug release .
architecture architecture information of the binaries under test.
possible values x86 x64 arm.
language language information of the binaries under test e.g.
en us.
especially media and gui tests depend on it.
branch unique identifier of the source code branch on which the test execution was performed.
where the tuple t c is a combination of test t and execution context c where detected defects t c represents the number of defects reported by t when executed in c executions t c represents the number of time s t has been executed in c and where false alarms t c represents the number of false test alarms caused by t when executed in c. for example consider a test t executed times in an execution context c e.g.
on build type release architecture x64 branch b and language en us which report ed false alarms and defects then pfp t c .
and ptp t c .
.
both probability measurements consider the entire history from the beginning of monitoring until th e moment the test is about to be executed.
consequently probability measures get more stable and more reliable the more historic information we gathered for the corresponding test.
note that test failure probabilities enclose code coverage like informatio n. for tests not covering a changed code area and for test covering the code area but not checking any execution results the defect detection probability ptp will be zero.
vi.
cost modelling theory the decision of when to execute or skip a test case in a given execution context is solely based on cost.
the idea is to estimate the cost of executing or not executing a test in a given executio n context beforehand and to choose the less expensive option.
a cost model was developed to deliver the cost factors for the test selection strategy using the data feeds and test failure probabilities discussed in section iv.
it is sensitive to the history of a test case as it considers past test executions in the same context to assess the expected cost values.
for each scheduled test execution the cost model considers two different scenarios executing the scheduled test and not executing it.
for both scenarios and the given execution context we estimate the corresponding expected costs and decide for the scenario which is expected to be less expensive .
thus if the estimated cost of not executing the test cost skip is lower than the cost executing it cost exec theo will skip the test execution not selecting the test to be executed.
for both execution scenarios the contributing cost factors must be considered.
executing a test raises both the computa tional cost section vi.a and the cost of inspecting the test result section vi.b if necessary.
executing tests that fail without detecting a real code defect will trigger unnecessary failure in spections performed by engineers.
on the other hand not executing a test might lead to undetected defects that will escape to later development stages and therefore impact more engineers than necessary section iv.c in this p aper cost factors which are described with positive values express the expected cost to be paid.
we could also have described cost saving values with negative amounts and cost increasing values with positive amounts but we explicitly wanted to avoid an y up front judgment on these figures.
a. base cost of test executions a major cost facto r is the time shared cost of infrastructure necessary to execute a test on all required execution contexts.
price for azure a7 high memory vm as of feb. .
per hour the constant cost mac hine is a constant representing the per minute infrastructure cost.
multiplied with the execution time per test we get the total infrastructure cost of running a test.
for the microsoft development environment we computed cost machine to have a value of .
min.
the cost factor corresponds roughly to the cost of a memory intense azure windows virtual machine2 and includes power and hardware consumption as well as maintenance.
for example consider we executed a test times in a given execution context and that each execution took minutes.
the total machine cost required to run the test in that context accumulates to .
min .
b. cost of test inspections all test failure s require human inspec tion effort but inspecting failing tests due to any thing other than code defects is unnecessary and should be avoided .
the cost of a test inspection equals the amount of time required to conduct the inspection times the salary of the engineer conducting the inspection .
the cost constant cost inspect represents the average cost rate of test failure inspect ions at microsoft.
it conside rs the size of the test inspection teams the number of inspections performed and the average sal ary of engineers on th e team .
the average cost per test inspection is .
.
although t his cost may vary from case to case for simplicity reasons we use the average cost of a test inspection in ou r model.
note that this cost reflects only the time spent by inspecting engineer s. additional cost factors such as waiting time or the need to run extra tests is not included.
c. cost of escaped defects code defects escaping a test run can be expensive.
the longer a defect remains hidden the more people can potentially be affected and the more expensive the escaped defect become.
defects closer to release dates tend to be more expensive and increased time f rom defect introduction to its detection increases cost due to aggravated root cause analysis more changes have been applied since then .
understanding and fixing an older change is more difficult.
additionally the greater the numbe r of engineers affected by a defect the more expensive disrupt ions will be while fixing the defect .
defects usually imply some sort of development freeze e.g.
no check ins until issue resolved.
the constant cost escaped represents the average cost of an escaped defect.
this cost depends on the number of people that will be affected by the escaped defect and the time duration the defect remains undetected.
we used a value of .
per developer and hour of delay for cost escaped .
this value represents the average cost of a bug elapsing within microsoft.
depending on the time the defect remains undetected and the number of additional engineers affected elapsing a defect from a developme nt branch into the main trunk branch in windows can cost tens of thousands of dollars.
we do not model defect severity explicitly .
there are two main reasons for this.
first the severity of a defect cannot be determined prior to its occurrence we would ne ed to predict defect severity which will not be reliable or actionable .
second all defects breaking a system and integration test and causing development activity to freeze on the corresponding branch must be considered severe.
vice versa system constra ints and properties whose violations are not considered severe will not be tested during system and integration testing .
such defects are caught by pre check in verification processes or dog food and manual testing procedure s. for example breaking look an d feel properties may not cause a development freeze .
the impact of the defect on the overall system is too low to cause a sever disruption of the overall development process.
d. final cost function finally we combine the individual cost components into two cost functions the expected cost of executing a test cost exec and the expected cost for not executing a test cost skip .
cost exec represents the expected cost if we decide to execute the test which depends on the machine cost cost mac hine the probability that the executed test will fail due to any other reason as a defect pfp and the cost of conducting an unnecessary test failure inspecting cost inspect cost exec costmac hine pfp cost inspect cost skip represents the expected cost of not executing the test which depends on the cost of escaped defects cost escaped and the number of additionally affected engin eers engineers and the time the defect remains undetected time delay cost skip ptp cost escaped time delay engineers the number of engineers engineers is a static prop erty of the engineering system and can be determined by counting the number of engineers whose code changes passed the current code branch.
the time delay is the average time span required to fix historic defects on the corresponding code branc h. both properties are easy to measure and reliable we verified these values with the corresponding product teams .
for tests that found no defects in the given execution context ptp and cost skip is zero and the test is skipped .
the sa me test for a different execution context e.g.
different branch is likely to have a different ptp value and thus might remain enabled.
note that we skip tests only if we know that the code change will be tested by the same execution context later again e.g.
on a lower branch level as shown in fig.
.
vii.
improvement strategy simulation applying theo to a live development environment without a period of thorough evaluation is too risky as it can directly impact product qua lity.
the result presented are based on simulations of theo on past development periods.
this section discusses the process of simulating the execution of the test s and also simulating the impact of any failures that would propagate as test s are removed.
simulating the test selection process also allows us to compare our results with actual test and code quality behavior .
a. simulating test case executions to simulate the behavior and impact of our test selection strategy we replayed test execu tions as they occurred in past development periods.
tests executions and their test results failed or passed are recorded in databases by the test execution framework s at microsoft.
using these database s we know the test suite and test case executions the execution context s these tests were executed and the order in which these tests run.
this information is sufficient for the simulation.
our simulation process follow s the following basic steps described in fig.
.
step using the databases contain ing the test executions and their corresponding test execution results failed or passed we order these historic test executions by their executi on timestamp .
thus our test optimization strateg y is fed with test executions in the order they were applied .
step each historic test execution and the corresponding test execution context definition is fed into a simulation process running an implementation of our test selection strategy.
the test s election process then returns a binary decision indicating whether the test case received as input is selected to be executed by the test selection strategy.
step depending on the binary result the originally executed test is marked as skipped or execu ted in a separate simulation table .
skipped t est executions represent those test executions that would not be executed when using our test selection strategy .
our simulation process does not execute test cases.
it only makes decisions on wheth er a test case would have been executed depending on the cost balance tightly influence by its defect finding capabilities .
the result of this test case simulation is a simple list of test case execution s that would have been prevented when using our test selection strategy.
this list can be used to compare against the original set of executed tests.
b. simulating defect detection removing test execution s may impact code quality.
defects detected by test executions that have been removed by theo time test selection simulatorexecute ?recorded historic test executions mark execution as skipped mark execution as executedno yesnext fig.
.
flow chart illustrating the test execution simulation process.
for each test execution and execution context ordered by time we decide which tests should be executed .
would remain undetected at least for some time.
disabling test executions is likely to have impact on code changes and developer behavior which cannot be simulated.
thus we have to estimate how undetected defects would propagate through the development process and w hen t hey would be detected .
the heuristic estimating when and where escaped defects would have been recaptures requires more data about the actual project specific development process .
this data is needed to gain further insight in how code changes and thus defects propagate through the development process.
for this part of our simulator we collect the following project specific datasets and make some basic assumptions about code defects and test behavior .
integration paths of code changes an d defects assuming a base code branch usually called trunk the integration path of a code change is a sequence of branches and timestamps the corresponding code change was applied to before the code change was merged into trunk.
for projects using a sing le development branch e.g.
microsoft office the integration path of a code change is a single entry identifying the name of the single branch and the commit timestamp.
for projects using multiple code branches e.g.
microsoft windows the integration pat h usually contains multiple entries.
for example consider the example shown in fig.
.
the integration path of change c lists the branches b1 b2 b3 and trunk including the timestamps of the corresponding merge operation that applied c to the code base in each branch .
for each change originally applied to the version control system we compute their corresponding integration paths tracing code changes through the version control system .
for more details on that procedure we refer to murphy et al.
.
basic defect and test behavior assumptions we make the following two assumptions with respect to code issues and test ca ses detecting these code issues .
a combination of test case and execution context3 that detected and reported a defect at time ti will also detect and report the same defect at any time tk if and only if tk ti and if tk is executed on an integration path of the defect.
this assumption disregards that even though unlikely the code defect might hav e be suppressed but not fixed by other code changes applied to the c ode base.
.
the code issue can be replicated by re running the test in the corresponding execution context.
we discussed these assumptions with various microsoft product teams who verified and confirmed the ir validity.
product teams also confirmed that in few cases code defects escape their original branches and are re captured on higher level branches or later on the same branch.
c. assigning defects to simulated test executions the i ntegration path of code cha nges is assumed to correspond to the propagation path of undetected defects and for defect propagation purposes we ignore all branch specific execution context informa tion as long as the change containing the defect was integrated into the branch.
can be used to estimate which test execution would have re captured an escaped defect.
in the example s hown in fig.
.
a defect in change c is caught by test t2 and the code does not progress past branch b2.
a bug fix is created and verified by tests t4 t5 and t6.
if theo skips test t2 the defect is assumed to be immediately merged into branch b3.
theo assumes t he defect is caught by test t6 as it r uns the same tests as t2.
in this scenario the bug fix would be applied in branch b1 after running test t6 and theo would assume that the cost of fixing the defect is now higher than its original cost .
while the original association between defect and test execution is stored in the test execution framework database our simulator returns a modified version of the original association s reflecting simulation results.
for each test that is executed during simulation we assign all original code issues detected during test execution.
additionally we assign all escaped defects to the test execution that would have been ca ught given the heuristics above .
as a result the number of defects associated with a test execution equals the number of defects during the actual execution of the test plus an additional set of escaped defects .
d. executing all test cases at least once the goal of this work is to optimize testing processes without sacrificing product quality.
this implies that we ensure that all escaped defects are eventually caught before releasing the product to customers.
to satisfy this condition we ensure all originally executed combinations of tests and execution contexts for all code changes applied to the cod e base are executed at least once .
to ensure this happens we use t wo separate criteria depending on the development process option for single branch development processes e.g.
microsoft office we enforce each test to execute at least every third day4.
since all code changes are applied to the same branch re execution of each test for each execution context periodically ensures that each code change has to go through the same verification procedures as performed originally.
option for multi branch development processes e.g.
microsoft windows we enforce to exe cute a combination of test and execution context on the branch closest to trunk on which the test had been executed originally .
thus theo can only skip test executions if the criteria described above allow a test to be skipped .
otherwise theo s decision to skip a test in a given execution context will be ignored by the simulator and the test will be executed.
e. training phase as the underlying cost model depends on risk factors extracted from historic data these risk factors will be unknown and unreliable in the early stages of the simulation process in which no historic data is known.
to compensate each test and execution context combination has to go through a training this value is the result of a complex analysis we conducted with the office product team and reflects the optimal solution for the system.
due to confidential reasons we cannot share the details of this process.
phase of executions before the simulator will allow theo to disable the correspon ding test in the given execution context.
viii.
simulation evaluation a. test execution reduction the first e valuation identifies the number of test executions that were skipped during the simulation of the test selection strategy.
to retrieve this number we count the number of originally recorded test executions and subtract the number of test executions that our test strategy would have executed during simulation.
we report the relative test reduction rate as the number of skipped test execution divided by the total number of originally executed test execution during development.
considering the execution time of individual test executions see section iv.b we can translate the relative test reduction rate into relative test executi on time improvements the total execution time of all skipped test executions divided by the total test execution time of all tests executed originally.
we further show reduction rates over time.
the number of skipped test executions and their summed execut ion duration determines how much machine cost cost mac hine has been saved.
b. test result inspection as discussed in section vi.b test failures require human effort for inspect ion in order to decide what action to take .
skipping test cases that would have caused unnecessary test inspections false alarms is an improvement.
relating these suppressed false alarms with the corresponding cost factor for test failure inspections cost inspect identifies the relative improvement with respect to test insp ection time and the associated development cost improvements.
c. escaped defects while the reduction in test executions section viii.a and the reduction in test inspections section viii.b relate to productivity and cost improvements the number of defects that escaped due to skipped test execution s reflects the negative aspects of theo and relates to development cost increases.
we report the number of escaped defects relative to the number of all co de defects reported by test executions.
the simulator is pessimistic assuming d efects can only be found by the same test cases that are ignored comp aring testname and testexecid in reality they may be caught earlier by other test cases .
d. cost improvemen t skipping tests may save development time.
at the same time it imposes risk of escaped defects temporarily compromis ing code quality and thus to increase development cost finding defects later in the development tends to be more expensive .
to validate if the estimated development cost improvements predominate the estimated developme nt cost increases we report the total cost balance as the balance of cost reductions .
we add the cost reductions due to reduced test time and du to less test failure inspections and sub tract the extra cost of escaped bugs .
a positive balance reflects cases in which the improvements predominate.
negative balances refer to cases in which our optimization strategy theo does not payoff and would have caused additional development costs rather than lowering them.
e. evaluation subject we evaluated the effects of theo on three major microsoft products windows office and dynamics.
combined our simulation results cover months of product development more than million test executions.
table ii.
contains information about the duration of simulated development periods and the number of simulated test cases per project.
results presented for windows reflect the entire windows .
development period months .
fo r windows we simulated more than million test executions.
simulation results for office cover development activities and test executions covering three mont hs a total of more than .
million test executions.
for dynamics we simulated a development pe riod of months with more than .
million test executions.
while windows and dynamics use multi branch development setups office uses a single collaboration branch.
thus for simulations for windows and dynamics we enforce to execute a combination of test and execution context on the branch closest to trunk option discussed in section vii.d while for office we simulate using the time based test execution policy option discussed in section vii.d .
ix.
simulation results in this section we present and discuss the results of our simulation experiments as described in section vii.
we will restrict the discussion in this section to measurable improvements.
section x contains a discussion on secondar y improvements that cannot be measured directly.
similar to the cost model the overall cost improvement depends on the constant cost estimations as presented in section viii.
the results are summarized in table iii.
a. test execution and test time reduction theo would have skipped .
of all windows test executions across all branches.
considering the runtime of these tests and relating it to the total runtime of all executed tests theo would have saved .
of the total test execution time.
this is a significant improvement .
in simulation submitting a code change to a development branch in windows could have been integrated into trunk in only of the original integration time.
note that this m easurement considers only test execution time but does not consider other development or human factors.
thus the test time improvement of may not translate to a increase in integration time but it certainly lowers the lower bound of integrat ion time.
multiplying the test time improvement with the cost factor for test execution cost mac hine we yield a cost improvement of over .
million .
note that the test time cost improvement figures table ii.
evaluation subject de tails .
windows office dynamics simulated period months months months test executions million .
million .
million branches consider only the time of not executing the skipped tests.
it does not include potential cost improvements due to skipping test setups test teardowns removing entire dedicated test machines from a branch etc.
thus the test time cost improvement must be seen as a lower bound of the actual cost improvement.
the average test execution reduction rate for dynamics i s above .
this m eans that theo would have prevented more than half of the originally executed test executions and saved of test execution time.
in theory code could have been moved nearly faster into the trunk branch a significant improvement.
although the test execution and test time reduction rate exceeds the values achieved for windows the test machine cost improvements that correlated with the reduced test time are two orders of magnitude lower than for windows.
this is due to the fact that tests executed f or dynamics terminate much faster than windows tests.
thus the reduction rate is translating into less computational time and thus is less lucrative.
the same is true for office tests which also execute much faster and therefore the savings on reduction of test execution time are lower than for windows.
the cost savings for office are further less significant as we only simulate a three month period and test executions on one branch.
nevertheless theo would have skipped a significant number of .
of all performed test executions and saved .
of the total test execution time b. test result inspection .
as discussed for the cost modelling theory section vi theo specifically targets unnecessary test inspections caused by test failures due to other reasons than code defects false test alarms .
suppressing such test failures implies reduction of unnecessary test result inspections which translate s into cost savings.
row test result inspection of table iii.
contains the relative number of spared test inspection s for all three products .
for windows and dynamics the reduction r ate lies around one third of originally carried out test result inspections were unnecessar y. for office theo would suppress .
of all false positives.
interestingly the associated cost improvements for windows 61k and office 104k are again two orders of magnitudes lower when compared to dynamics .3m .
the reason for the difference is again the different absolute number of test failures suppressed.
c. escaped defect s while removed test executions and reduced test inspections determines a positive cost savings the number of temporarily escaped defects increases development costs.
in our windows simulation .
of all defects escaped at least one test execut ion.
as shown in table iv.
of these escaped defects escaped only one branch and were found in the corresponding next merge branch.
of escaped windows defects escaped two branches and escaped even branches.
importa ntly none of the defects escaped into the trunk branch.
on dynamics theo would have elapsed .
of all defects a much higher escape rate as for windows.
the vast majority of these escaped dynamics defects were caught on the direct consecutive me rge branch.
the remaining escaped two branch levels.
for both windows and dynamics the extra cost caused by escaped bugs is significant.
however in both cases these new extra costs are orders of magnitudes lower than the highest cost savings achieved by removed test executions and inspections.
for office the results are a bit different.
whereby the percentage of bugs that escaped is .
which is comparable to dynamics the costs are 75k higher in relation to the cost savings.
this is due to an ad ditional cost of manual testing work that we added as a penalty for office in case the bugs was not found within days.
the rationale behind this lies in the way the office test and development is performed.
nevertheless approximately of bugs escape d would have been found already in the next scheduled build and test.
d. cost improvement .
looking at the overall cost balance we see cost savings for all three evaluation subjects.
for windows we estimate a total cost saving of .
m for dynamics a total cost saving of approximate .
m and for office a cost saving of approximate 100k .
these values may seem small considering the total development budgets of projects of their scale.
howe ver this only identifies test sav ings.
considering the actual test execution reduction rates of to puts these numbers into perspective.
the actual values are secondary it is important that the achieved productivity increase though faster integ rations .
e. variable performance over time fig.
.
shows the relative test execution reduction over time measured in development days for windows .
the dark area correspond s to originally executed tests that were removed during simulation.
as shown theo requires an initial training table iii.
simulation results for microsoft windows office and dynamics .
windows office dynamics measurement rel.
improvement cost improvement rel.
improvement cost impr ovement rel.
improvement cost improvement test executions .
.
.
test time .
.
.
.
.
.
test result inspection .
.
.
.
.
.
escaped defec ts .
.
.
.
.
.
total cost balance .
.
.
table iv.
distribution of escap ed defects over number of escaped branches for windows and dynamics .
number of escaped branch levels windows dynamics phase see section vii in which we observe the current testing process to estimates risk factors before applying any test selection.
once theo starts skipping test executions the ratio of removed executions converge s to an almost stable state.
the plot shows fluctuations in the relative number of reduced test executions e.g.
a sharp drop from nearly to midway through the plotted timespan .
the reason for the se fluctuations in test execution reduction rates are natural fluctuations in code quality.
the quality of submitted code changes is not constant a drop in overall code quality causes more test failures and directly influences risk factors.
changes in risk factors can cause previously skipped tests to be re enabled.
reduction rates for office and dynamics look similar.
due to space reason we abstain from showing these plots.
x. secondary improvements the measu rable improvements discussed in section ix are likely to have further secondary consequences.
although these improvements are not directly measurable they are an important part of the improvement of the development processes .
a. code velocity reducing the number of executions and consequently th e overall required test time may have positive effects on code velocity.
executing fewer tests implies that code changes have to spend less time in verification and changes can be integrated faster freeing up engineering time that may have been spent evaluating false positives.
however the immediate impact on code velocity is hard to measure.
code velocity is determined by many different asp ects including human behavior which is not possible to simulate .
thus it is hard to predict how theo would affect actual development speed.
it might well be that the bottleneck of current development processes is not only testing.
nevertheless the number of executed tests represents a lower bound to code velocity as the consecutive time necessary to pass all required tests is the minimal time required to integrate code change s. by lowering the number of executed tests theo lowers the lower bound for cod e velocity .
b. developer satisfaction a very important but also hard to measure factor of every development process is developer satisfaction.
reducing the time for testing and the number of required test inspections is likely to increase developer satisfacti on.
it should help to increase the confidence in test results and decisions based on testing .
increasing the speed of the development process will itself also impact the developer experience.
the ability to merge integrate and share code changes faster c an reduce the number of merge conflicts and is likely to support collaboration.
xi.
threats to validity like most empirical studies the presented study has threats to validity.
we identified three main groups of threats.
a. generalizability we investigated test executions specific to three microsoft products and their development processes.
even though some terminology might be unique to microsoft the execution of tests during software development and the impact of test execution time on development speed a re generalizable.
the estimated costs presented in this paper are specific to microsoft.
the cost model considers several independent but development process spe cific aspects and cost factors are likely to vary across releases and projects e.g.
cost factors for machines and engineers .
replicating this study for different projects or releases requires detailed review s and adjustments .
b. construct validity the approach to estimate the impact of escaped defects and to identify those test case s that would eventually re capture these defects is a heuristic and associated cost values must be considered approximations.
however our data was derived through investigations discuss ions and fine tuning with the corresponding product teams.
we consider the approximations of th ese heuristic s as fair a nd realistic.
cost factors used in this study are based on average microsoft development figures and numbers e.g.
average salary and work hours per year .
these numbers vary and might not consider all possible aspects.
execution conte xt information might be particular important for system and integration tests while other tests e.g.
unit tests might be more independent from them.
however the presented concept is generic .
reducing the number of execution co ntexts to one or further increasing the number of execution contexts does not threat the validity of the overall approach.
c. internal threats to validity the simulator may contain defects .
to conquer this threat we implemented multiple test cases and performed manual inspec tions.
through validating with product teams we are confident that the data collected and analyzed reflects the development process es accurately.
xii.
related work in this section we give an overview of related st udies.
a. test selection prioritization and reduction measuring the absolute effectiveness of testing is generally not possible but comparison between effectiveness of tests is .
based on this concept basili and selby presented one of the first studies comparing the effectiveness and cost of testing fig.
.
relative test execution reduction rate for windows over time.
the area shows the relative number of tests skipped by theo.
development days windows relative test time improvementstrategies showing that changing or choosing different test strategies might impact test effectiveness.
many of the following research studies focus ed on the area of test case selection prioritization and reduction.
in yoo and harman presented a comprehensive survey of research studies showing that many techniques assume access either to code execution traces or some sort of model to derive selection criteria i.e.
based on code coverage .
in contrast the strategy discussed in this paper does not ma ke any such assumption .
we treat tests entirely as black boxes.
lately anderson et al.
used software repositories to measure historic test perfo rmances.
this study was carried out in parallel to this work and does not provide any dynamic test selection solution but rather assess the effectiveness of tests .
schroeder and korel used input output analysis to construct test input to reduce the number of required black box test executions without lowering test effectiveness.
contrary to their study theo measures test effectiveness based on a cost model and uses the context and frequency of execution s to drive efficiency improvement.
altering input s for tests would not be actionable at microsoft .
goradia used the fault exposing potential of a test as the selection criteria whereby he relies on mutation analysis i.e.
modification of the original program to determine the probabilities of a test to reveal a fault.
more generally a number of empirical case studies and extensive literature reviews compared and identify test tools most likely to yield optimal test effectiveness .
with respect to test executions costs valles pir and herbert used machine and inspection costs for individual unit tests to conclude that the number found defects is low compared to the relatively high cost of unit tests.
however the cost calculation presented in are based on three samples and are rather vague and seem not applicable for development process es nor for system and integration tests at microsoft .
b. cost aware improvement strategies usin g cost aware test improvem ent strategies is not unique .
yoo and harman and alspaugh et al.
used time aware techniques selecting a subset of test cases that can be executed in a given time budget.
do et al.
assessed the effect of time constraints on the cost and benefits of prioritization techniques.
additionally empirical studies exist that consider the impact of testing strategies on the cost effectivenes s in the wider context of the overall software lifecycle .
lately li and boehm proposed a value based test prioritization strategy ranking tests by their risk exposure coverag e and the relative cost of tests .
gustafson applied cost factors to software flow graphs to define areas that require additional testing.
the difference between these s tudies and theo is that theo is solely based on a dynamic empirically derived cost model which directly impacts the strategy decisions.
our model uses not only the cost and ris k of executing or skipping a test but relates both values to each other.
empirical evaluations and application of testing techniques at industrial settings remain limited .
all those studies focus on regression test suites.
to the best of our knowledge this study is the first that evaluates system and integration test effectiveness based on execution contexts such as branching structures and archi tectures .
c. merge conflicts and awareness even though vers ion control systems allow parallel development activities and avoid conflicts several studies showed that merge conflicts occur frequently whereby most of the studies focused on facilitating collabo ration effectiveness.
one of the first studies that showed how frequent merge conflicts occur has been performed by zimmermann .
he showed that between and of integrations could not be automatically resolved and res ulted in conflicts.
in a recent study brun et al.
showed that merge conflicts are frequent and persistent.
in summary they showed that of merges that were reported to contain no textual conflicts by the version contr ol system in fact contained higher order conflicts which manifested themselves as a build or a test failure.
also perry et al.
showed significant correlations between the degree of parallel work and the number of quality problems .
the effect of merge conflicts on quality has also been studied by bird and zimmermann .
those studies aimed at quantifying the degree of merge conflicts or at establishing collaborative awareness for merge confli cts we focus on quantifying and improving the effectiveness and efficiency of tests.
xiii.
conclusion we presented a novel cost based test selection strategy theo which skips test executions where the expected cost of running the test exceeds the expected cos t of not running it.
our strategy is dynamic and self adaptive and only uses historic al test data which is already collected by most test frameworks.
theo was verified through simulat ing its impact on the microsoft windows office and dynamics developmen ts.
theo would have reduced the number of test executions by up to cutting down test time by up to .
at the same time product quality was not sacrifice d as the process ensures that all tests are ran at least once on all code changes .
removing tests would result in between .
and of defects being caught later in the development process thus increasing the cost of fixing th ose defect s. nevertheless simulation shows that theo produced an overall cost reduction of up to million per development year per product.
through reduc ing the overall test tim e theo would also have other impact s on the product development process such as increasing code velocity and productivity.
these improvements are hard to quantify but are likely to increase the cos t savings estimated in this paper.
the technique and results described in this paper have convinced an increasing number of product teams within microsoft to provide dedicate resources to explore ways to integrate theo into their actual live production test environments .
acknowledgment we thank the windows office and dynamics development teams for their help and feedback.
this work is based on data extracted from varies development repositories by the codemine process managed by microsoft s tools for s oftware engineers group .
our special thanks go to jason means blerim kuliqi alex gorischek alain zariffa wayne roseberry adrian marius marin craig campbell and alan back.