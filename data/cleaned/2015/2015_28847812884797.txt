automated test suite generation for time continuous simulink models reza matinnejad shiva nejati lionel c. briand snt centre university of luxembourg luxembourg reza.matinnejad shiva.nejati lionel.briand uni.luthomas bruckmann delphi automotive systems luxembourg thomas.bruckmann delphi.com abstract all engineering disciplines are founded and rely on models although they may differ on purposes and usages of modeling.
inter disciplinary domains such as cyber physical systems cpss seekapproaches that incorporate different modeling needs and usages.
specifically the simulink modeling platform greatly appeals to cps engineers due to its seamless support for simulation and codegeneration.
in this paper we propose a test generation approachthat is applicable to simulink models built for both purposes of simulation and code generation.
we define test inputs and outputs as signals that capture evolution of values over time.
our test gener ation approach is implemented as a meta heuristic search algorithmand is guided to produce test outputs with diverse shapes accordingto our proposed notion of diversity.
our evaluation performed on industrial and public domain models demonstrates that in contrast to the existing tools for testing simulink models that are onlyapplicable to a subset of code generation models our approach isapplicable to both code generation and simulation simulink models.
our new notion of diversity for output signals outperforms random baseline testing and an existing notion of signal diversity inrevealing faults in simulink models.
the fault revealing abilityof our test generation approach outperforms that of the simulinkdesign v erifier the only testing toolbox for simulink.
ccs concepts software and its engineering software testing keywords simulink models software testing time continuous behaviors search based software testing output diversity signal features structural coverage simulink design v erifier sldv .
introduction modeling has a long tradition in software engineering.
software models are particularly used to create abstract descriptionsof software systems from which concrete implementations are produced .
software development using models also referred to as model driven engineering mde is largely focused around theidea of models for code generation or models for test generation .
code or test generation although important is not the permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full cita tion on the first page.
copyrights for components of this work owned by others thanacm must be honored.
abstracting with credit is permitted.
to copy otherwise or re publish to post on servers or to redistribute to lists requires prior specific permissionand or a fee.
request permissions from permissions acm.org.
icse may austin tx usa c circlecopyrt acm.
isbn .
.
.
.
reason for software modeling when software development occurs in tandem with control engineering.
in domains where software closely interacts with physical processes and objects such ascyber physical systems cpss one main driving force of model ing is simulation i.e.
design time testing of system models.
simulation aims to identify defects by testing models in early stages and before the system has been implemented and deployed.
in the cps domain models built for simulation have major differences from those from which code can be generated.
simulationmodels are heterogeneous encompassing software network and physical parts and are meant to represent as accurately as possi ble the real world and its continuous dynamics.
these models havetime continuous behaviors described using differential equations since they are expected to capture and continuously interact with the physical world .
code generation models on the other hand capture software parts only and have discrete time behavior described using some form of discrete logic or discrete state ma chines .
this is because the generated code will run on platforms that support discrete computations and further the code will receive input data as discrete sequences of events.
when simulation models are available testing starts very early typically by running those models for a number of selected scenar ios.
early testing of simulation models pursues among others two main objectives ensuring correctness of simulation models so that these models can act as oracle.
this can significantly reduceengineers reliance on expensive and late testing and measurementson the final hardware.
obtaining an initial test suite with high fault revealing power to be used for testing software code or at later stages for testing the system on its final hardware platform.
our goal is to provide automated techniques to generate effective test suites for simulink models .
simulink is an advancedplatform for developing both simulation and code generation models in the cps domain.
the existing approaches to testing and verifying simulink models almost entirely focus on models withtime discrete behavior i.e.
code generation models.
these ap proaches generate discrete test inputs for simulink models with the goal of reaching runtime errors to reveal faults violating assertions inserted into simulink models based on someformal specification and achieving high structural cov erage .
discrete test inputs however are seldom sufficientfor testing simulink models in particular for those models with time continuous behaviors.
many faults may not lead to runtime crashes.
formal specifications are rather rare in practice and fur ther are not amenable to capturing continuous dynamics of cpss.finally effectiveness of structural coverage criteria has yet to be ascertained and empirically evaluated for simulink model testing.
in this paper we provide test generation techniques for simulink models with time continuous behaviors.
we generate test inputs ieee acm 38th ieee international conference on software engineering ascontinuous signals and do not rely on the existence of implicit oracles e.g.
runtime failures or formal specifications.
assuming that automated oracles are not available we focus on providing test generation algorithms that develop small test suites with high faultrevealing power effectively reducing the cost of human test ora cles .
instead of focusing on increasing structural coverage we propose and evaluate a test data generation approach that aims to maximize diversity in output signals of simulink models.
output signals provide a useful source of information for detecting faultsin simulink models as they not only show the values of model out puts at particular time instants but also they show how these values change over time.
by inspecting output signals one can determine whether the model output reaches appropriate values at the righttimes whether the time period that the model takes to change itsvalues is within acceptable limits and whether the signal shape isfree of erratic and unexpected changes that violate continuous dynamics of physical processes or objects.
our intuition is that test cases that yield diverse output signals may likely reveal different types of faults in simulink models.
the key here is the definition of output diversity.
in our earlier work weproposed a notion of output diversity based on the euclidean dis tance between signal vector outputs of mixed discrete continuousstateflows .
stateflow is a subset of simulink for capturingstate based behaviors.
in this work we introduce a new notion ofdiversity for signals that is defined based on a set of representative and discriminating signal feature shapes.
we refer to the former as vector based and to the latter as feature based diversity objectives.
we develop a meta heuristic search algorithm that generates testsuites with diversified output signals where the diversity objective can be either vector based or feature based.
our algorithm uses the whole test suite generation approach .
further our algorithmadapts a single state search optimizer to generate continuousinput signals and proposes a novel way to dynamically increasevariations in test input signals based on the amount of structural coverage achieved by the generated test suites.
we evaluate our algorithm using four industrial and public domain simulink models.
our contributions are as follows we identify the problem of testing simulation models and argue that though simulation models are essential in the cps domain few systematic testing verification techniques exist for them.
we propose a new notion of diversity for output signals and develop a novel algorithm based on this no tion for generating test suites for both simulation and code genera tion simulink models.
we show that our new notion of diversity for output signals outperforms random baseline testing and an existing notion of signal output diversity in revealing faults in simulinkmodels.
we compare our test generation approach that diversifies test outputs with the simulink design v erifier sldv the only simulink testing toolbox.
sldv automatically generates test suites for a subset of simulink models with the goal of achieving highstructural coverage.
we argue that while our approach is applica ble to the entire simulink sldv supports a subset.
we show that when considering the sldv compatible subset our output diversity approach is able to reveal significantly more faults compared to sldv and further it subsumes sldv in revealing faults anyfault identified by sldv is also identified by our approach.
.
motiv ation and background in this section we provide examples of simulation and code generation models.
we then introduce simulink design v erifier sldv and motivate our output diversity test generation approach.
example.
we motivate our work using a simplified fuel level controller flc which is an automotive software component used in cars fuel level management systems.
flc computes the fuelvolume in a tank using the continuous resistance signal that it receives from a fuel level sensor mounted on the fuel tank.
the sensor data however cannot be easily converted into an accurate estima tion of the available fuel volume in a tank.
this is because the relationship between the sensor data and the actual fuel volume is impacted by the irregular shape of the fuel tank dynamic conditionsof the vehicle e.g.
accelerations and braking and the oscillationsof the indication provided by the sensors.
hence flc has to rely on complex filtering algorithms involving algebraic and differential equations to accurately compute the actual fuel volume .
simulation models.
in the automotive industry engineers build simulation models prior to any software coding and often at the time of system level design and engineering.
simulation models most often contain time continuous mathematical operations .for example figure a shows a very simplified flc simulinkmodel which is adopted from and includes a time continuous integral operator integraltext .
we refer to the model in figure a as a simulation model of flc.
the input of this model is the resistance signal from the sensor and its output shows the fuel volume.
thesimulink model in figure a is executable.
engineers can run themodel for any desired input signal and inspect the output.
automotive engineers often rely on their knowledge of mechanics and control theory to design simulation models.
these models however need to be verified or systematically tested as they are complex andmay include several hundreds of blocks.
code generation models.
figure b shows an example flc code generation model i.e.
the model from which software code can be automatically generated .
the code generation model is discrete the time continuous integrator block integraltext in the simulation model is replaced by a time discrete integrator sum in the code generationmodel.
the behavior of code generation models may deviate fromthat of simulation models since the latter often has time continuousoperations while the former is purely discrete.
typically some degree of deviations between simulation and code generation model outputs are acceptable.
the level of acceptable deviations how ever have to be determined by domain experts.
simulation model outputs vs. code generation model outputs.
figure c shows an example continuous input signal for flc over a10sec time period.
the signal represents the resistance values received from the tank sensor.
this input signal can be applied toboth simulation and code generation models of flc.
note that a time continuous model has to be provided with a continuous input signal i.e.
a continuous function over time .
a time discretemodel on the other hand only requires single values at discretetime steps which can be extracted from continuous signals as well.models in figures a and b respectively produce the outputs in figures d and e once they are provided with the input in figure c .
as shown in figures d and e the percentages of fuelvolume in the continuous output signal differ from those in the dis crete output signal.
for example after one second the output of the simulation model is .
while that of the code generation model is88.
.
these deviations are due to the differences between the continuous and discrete integrals.
for example the grey area in the left part of figure f shows the value computed by integraltext after three seconds while the value computed by the discretized integral operator after three seconds is the grey area in the right part of figure f .
clearly these two operators and hence the two models infigures a and b generate different values for the same input.
simulation models as oracles for code.
in the cps domain timecontinuous simulation models play a crucial role.
not only that they are the blueprints for code generation models and later thesoftware code but also they serve as oracle generators for test596 d flc simulation model output fuel level c flc model input fuel level sensor 0fuel level sensor fuel level e flc code generation model output fuel level f continuous vs. discrete integral 0fuel level 0time s time s time s time s time s time s time s fuel level g faulty flc model output test input h faulty flc model output test input 250fuel level sensor150 ba 60708090fuel level a flc simulation model b flc code generation model faulty model output correct model outputfaulty model output correct model output figure a fuel level controller flc example a a simulation model of flc b a code generation model of flc c an input to flc d output of a when given c as input e output of b when given c as input f comparing outputs of the blocks integraltext andsum from models a and b respectively g a test output of a faulty version of a and h another test output a faulty version of a .
ing software code on various platforms.
while oracles obtained based on formal specifications or runtime errors are often preciseand deterministic those obtained based on simulation out puts are inexact as some deviations from oracles are acceptable.
these oracles provide engineers with a reasonable and yet valuableassessment of the code behavior.
further the oracle informationthat is obtained from simulation models is not readily available inother artifacts e.g.
requirements and specifications.
therefore it is important to develop accurate and high quality simulation models.
hence our goal in this paper is to provide systematic algorithms tohelp with testing of time continuous simulation models.
simulink design verifier.
simulink design v erifier sldv is a product of mathworks and a simulink toolbox.
it is the only simulink toolbox that is dedicated to test generation.
it automat ically generates test input signals for simulink models using con straint solving and model checking techniques .
sldv provides two usage modes generating test suites to achieve some form of structural coverage and generating test scenarios counter examples indicating violation of some given properties assertions .in the first usage mode sldv creates a test suite satisfying a givenstructural coverage criterion .
in the second usage mode sldv tries to prove that assertions inserted into simulink models cannot be reached or otherwise it generates inputs triggering the asser tions hence disproving the desired properties.
in this paper we compare the fault revealing ability of our algorithm with that of the first usage mode of sldv i.e.
test generationguided by structural coverage.
note that the second usage mode ofsldv requires exact oracles which is out of the scope of this paper.we chose to compare our work with sldv as it is distributed bythe mathworks and is among the most well known tools for testing and verification of simulink models.
other existing tools in that category e.g.
reactis rely on formal techniques as well.based on our experience working with sldv and according to themathworks white papers sldv has the following practical limitations model compatibility.
sldv supports a subset of the simulink language i.e.
discrete fragment of simulink and is not applicable to time continuous blocks of simulink suchas the continuous integrator in figure a .
specifically sldv isapplicable to the model in figure b but not to that in figure a .
further there are a number of blocks that are acceptable by the simulink code generator but are not yet compatible with sldv inparticular the s function block which provides access to system functions and custom c code from simulink models.
scalability.
the lack of scalability of sldv is recognized as an issue by mathworks .
further as the models get larger and morecomplicated it is more likely that they contain blocks that are notsupported by sldv .
so for sldv the problem of compatibilityoften precedes and overrides the problem of lack of scale .
coverage vs. output diversity.
in contrast to sldv which aims to maximize structural coverage we propose a test generation algorithm that tries to diversify output signals.
we illustrate the differences between test generation based on structural coverage and based on output diversity using a faulty version of the simulationmodel in figure a .
suppose the line connected to point a inthis model is mistakenly connected to point b. figures g and h show two different output signals obtained from this faulty model along with the expected outputs.
the faulty output is shown by a solid line and the correct one oracle is shown by a dashed line.the faulty output in figure g almost matches the oracle whilethe one in figure h drastically deviates from the oracle.
given that small deviations from oracle are acceptable engineers are unlikely to identify any fault when provided with the output in fig ure g .
when the goal is high structural coverage the test inputsyielding the two outputs in figures g and h are equally de sirable.
indeed for the flc model one test input is sufficient to achieve full structural coverage.
if this test input happens to produce the output in figure g the fault goes unnoticed.
in contrast our approach attempts to generate test cases that yield diverse out put signals to increase the probability of generating outputs that noticeably diverge from the expected result.
.
test generation algorithms we propose a search based whole test suite generation algorithm for simulink models.
our test generation algorithm aims to maximize diversity among output signals generated by a test suite.
wedefine two notions of diversity among output signals vector based and feature based.
we first fix a notation and will then describe our notions of output diversity and our test generation algorithm.
notation.
let m i o be a simulink model where i i ... i n is the set of input variables and ois the output variable ofm.
each input output variable of m is a signal i.e.
a 597function of time.
irrespective of m being a simulation or a code generation model each input or output of m is stored as a vector whose elements are indexed by time.
assuming that the simulation time is t the simulation interval is divided into small equal time steps denoted by t. we define a signal sgas a functionsg t t ... k t r where t is the simulation time step kis the number of observed simulation steps and ris the signal range.
the signal range ris bounded by its min and max values denoted minrandmaxr respectively.
for the example in figure we have t 0s t 1s andk .
note that in that example to better illustrate the input and output signals t is chosen to be larger than normal.
in one of our experiments forexample we have t .001s t 2s andk .
our goal is to generate a test suite ts i ... i q .
each test input ijis a vector sgi1 ... sg in of signals for the input variables i1toinofm.
by simulating m using each test input ij we obtain an output signal sgofor the output variable oofm.
all the input signals sgijand the output signal sgoshare the same simulation time interval and simulation time steps i.e.
the valuesof t t andkare the same for all of the signals.
test inputs.
in simulink every variable even those representing discrete events are described using signals.
in this context testinput generation is essentially signal generation.
each test inputis a vector sg i1 ... sg in of signals.
each signal sgijis also a vector with a few thousands of elements and each element cantake an arbitrary value from the signal range.
to specify an in put signal however engineers never define a few thousands ofvalues individually.
instead they specify a signal by defining asequence of signal segments .
to formalize input signals generated in practice we characterize each input signal sg with a set k v1 ... kp vp of points where k1tokpare the simulation steps s.t.
k1 k1 k2 ... kp andv1tovpare the values that sgtakes at simulation steps k1tokp respectively.
the set k1 v1 ... kp vp specifies a signal sgwithpsegments.
the firstp 1segments of sgare defined as follows for j p each pair kj vj and kj vj specifies a signal segment s.t.
l kj l k j sg l t vj sg kj t vj .
the last segment of sgis a constant signal that starts at kp vp and ends at k v p wherekis the maximum number of simulation steps.
for example v specifies a constant signal at v i.e.
one segment p .
a step signal going from v0tov1and stepped at k primeis specified by v0 k prime v1 i.e.
two segments p .
input signals with fewer segments are easier to generate but they may fail to cover a large part of the underlying simulink model.by increasing the number of segments in input signals structuralcoverage increases but the output generated by such test inputs becomes more complex and engineers may not be able to determine expected outputs oracle .
furthermore highly segmented in put signals may not be reproducible on hardware platforms as theymay violate physical constraints of embedded devices.
for each input variable engineers often have a good knowledge on the maximum number of segments that a signal value for that variable maypossibly contain and still remains feasible.
in our test generationalgorithm discussed at the end of this section we ensure that foreach input variable the generated input signals achieve high structural coverage while their segment numbers remain lower than the limits provided by domain experts.
vector based output diversity.
this diversity notion is defined directly over output signal vectors.
let sg oandsg prime obe two signals generated for output variable oby two different test inputs of m.i n our earlier work we defined the vector based diversity measure between sgoandsg prime oas the normalized euclidean distancebetween these two signals.
we denote the vector based diversity betweensgoandsg prime oby dist sgo sg prime o .
our vector based notion however has two drawbacks it is computationally expensive since it is defined over signal vectors with a few thousands of elements.
using it in a search algorithmamounts to computing the euclidean distance between many pairs of output signals at every iteration of the search.
a searchdriven by vector based distance may generate several signals withsimilar shapes whose vectors happen to yield a high euclidean distance value.
for example for two constant signals sg oandsg prime o dist sgo sg prime o is relatively large when sgois constant at the maximum of the signal range while sg prime ois constant at the minimum of the signal range.
a test suite that generates several output signals with similar shapes may not help with fault finding.
feature based output diversity.
in machine learning a feature is an individual measurable and non redundant property of a phe nomenon being observed .
features serve as a proxy for largeinput data that is too expensive to be directly processed and further is suspected to be highly redundant.
in our work we define a set of basic features characterizing distinguishable signal shapes.
we then describe output signals in terms of our proposed signal fea tures effectively replacing signal vectors by feature vectors.
feature vectors are expected to contain relevant information from signals so that the desired analysis can be performed on them instead of the original signal vectors.
to generate a diversified set of outputsignals instead of processing the actual signal vectors with thou sands of elements we maximize the distance between their corre sponding feature vectors with tens of elements.
figure a shows our proposed signal feature classification.
our classification captures the typical basic and common signal pat terns described in the signal processing literature e.g.
constant decrease increase local optimum and step .
the classification in figure a identifies three abstract signal features value derivative and second derivative.
the abstract features are italicized.
thevalue feature is extended into instant value and constant value fea tures that are respectively parameterized by v and n v .
the former indicates signals that cross a specific value vat some point and the latter indicates signals that remain constant at vfornconsecutive time steps.
these features can be instantiated by assigningconcrete values to norv.
specifically the constant value n v feature can be instantiated as the one step constant value v and always constant value v features by assigning nto one and k i.e.
the simulation length respectively.
similarly specific values for v are zero and max and min of signal ranges i.e.
max randminr .
the derivative feature is extended into sign derivative and extremederivative features.
the sign derivative feature is parameterized by s n wheresis the sign of the signal derivative and nis the number of consecutive time steps during which the sign of the signal derivative is s. the sign scan be zero positive or negative resulting in constant n increasing n and decreasing n features respectively.
as before specific values of nare one and k. the extreme derivatives feature is non parameterized and is extendedinto one sided discontinuity one sided discontinuity with local op timum one sided discontinuity with strict local optimum discon tinuity and discontinuity with strict local optimum features.
the second derivative feature is extended into more specific features similar to the derivative feature.
due to space limit we have notshown those extensions in figure a .
figures b to e respectively illustrate the instant value v the increasing n the one sided continuity with local optimum and the discontinuity with strict local optimum features.
specifically the signal in figure b takes value vat point a. the signal in figure c is increasing for nsteps from b to c. the signal in 598increasing n decreasing n constant value n v signal features derivative second derivative sign derivative s n extreme derivatives sided discontinuity discontinuity sided continuity with local optimum1 sided continuity with strict local optimum one step constant value v all step constant value v value instant value v constant n instant value ...... instant value minrminr ...... ...always constant1 step constant...... a features classification b instant value v ... d sided continuity with local optimum e discontinuity with strict local optimumv k. t k. t 0k prime.
t k prime n .
t k. t k. t 0discontinuity with strict local optimum c increasing n v 0v v 0v v 0v ...... ...k prime.
tabc d e v maxr v maxr v minr v minr n 1n n k n k n 1n n k n kn 1n n 1n s 0s s 1s s s figure signal features a our signal feature classification and b e examples of signal features from the classification in a .
figure d is right continuous but discontinuous from left at point d and further the signal value at d is not less than the values at its adjacent point hence making d a local optimum.
finally the signal in figure e is discontinuous from both left and right atpoint e which is a strict local optimum point as well.
we define a function f ffor each non abstract feature fin figure a .
we refer to ffasfeature function.
the output of functionffwhen given signal sgas input is a value that quantifies the similarity between shapes of sgandf.
more specifically ffdetermines whether any part of sgis similar to feature f. for example suppose functions lds sg i and rds sg i respectively compute the left and right derivative signs of sgat simulation step i. specifically they generate and0if the derivative value is positive negative and zero respectively.
we define fffor the feature in figure d as follows ff sg kmax i derivative sg i localopt sg i such that derivative sg i sg i t sg i t tand localopt sg i braceleftbigg lds sg i negationslash rds sg i otherwise function ff sg computes the largest left or right derivative of sg that occurs at a local optimum point i.e.
the largest one sidedderivative that occurs at a point isuch that the derivative of sg changes its sign at i. the higher ff sg the more similar sgis to the feature in figure d .
our complete signal feature classifica tion and the corresponding feature functions are available at .
having defined features and feature functions we now describe how we employ these functions to provide a measure of diversitybetween output signals sg oandsg prime o. letf1 ... f mbemfeatures that we choose to include in our diversity measure.
we com pute feature vectors f v sgo ff1 sgo ... f fm sgo and fv sg prime o ff1 sg prime o ... f fm sg prime o corresponding to signals sgoandsg prime o respectively.
since the ranges of the feature function values may vary widely we standardize these vectors beforecomparing them.
specifically we use feature scaling which is acommon standardization method for data processing .
having obtained standardized feature vectors f v sgo and fv sg prime o corresponding to signals sgoandsg prime o we compute the euclidean distance between these two vectors i.e.
dist fv sgo fv sg prime o as the measure of feature based diversity between signals sgoand sg prime o. below we discuss how our diversity notions are used to generate test suites for simulink models.
whole test suite generation based on output diversity.
we propose a meta heuristic search algorithm to generate a test suite ts i1 ... i q for a given model m i o to diversify the set of output signals generated by ts .
we denote by tso sg1 ... sg q the set of output signals generated by ts .
we capture the degree of diversity among output signals in tsousing objective functions ovandofthat correspond to vectorbased and feature based notions of diversity respectively ov tso q summationtext i 1min sg tso sg i dist sgi sg of tso q summationtext i 1min sg tso sg i dist fv sgi fv sg function ovcomputes the sum of the minimum distances of each output signal vector sgifrom the other output signal vectors in tso .
similarly ofcomputes the sum of the minimum distances of each feature vector fv sgi from feature vectors of the other output signals in tso .
our test generation algorithm aims to maximize functions ovandofto increase diversity among the signal vectors and feature vectors of the output signals respectively.
our algorithm adapts the whole test suite generation approach by generating an entire test suite at each iteration and evolving ateach iteration every test input in the test suite.
the whole test suitegeneration approach is a recent and preferred technique for test data generation specially when similar to o vandof objective functions are defined over the entire test suite and aggregate all testing goals.
another benefit of this approach for our work is that it allowsus to optimize our test objectives while fixing the test suite size at a small value due to the cost of manual test oracles.
our algorithm implements a single state search optimizer that only keeps one candidate solution i.e one test suite at a time as opposed to population based algorithms that keep a set of candi dates at each iteration.
this is because our objective functions are computationally expensive as they require to simulate the underlying simulink model and compute distance functions between ev ery test input pair.
when objective functions are time consuming population based search may become less scalable as it may have to re compute objective functions for several new or modified members of the population at each iteration.
figure shows our output diversity test generation algorithm for simulink models.
we refer to it as od.
the core of od is a hill climbing search procedure .
specifically the algorithm generates an initial solution lines iteratively tweaks this solution line and selects a new solution whenever its objective functionis higher than the current best solution lines .
the objectivefunctionoin od is applied to the output signals in tso that are obtained from test suites.
the objective function can be either o f orov respectively generating test suites that are optimized based on feature based and vector based diversity notions.
while being a hill climbing search in essence od proposes two novel adaptations it initially generates input signals that contain a small number of signal segments p. it then increases ponly when it is needed while ensuring that pis never more than the limit provided by the domain expert pmax .
recall that on one hand increasing segments of input signals makes the output more difficultto analyse but that on the other hand input signals with few seg599algorithm.
the test generation algorithm applied to a simulink model m. .p initial number of signal segments for test inputs .ts genera te initial test suite p .bestfound o tso .pmax maximum number of signal segments permitted in test inputs .tso signal outputs obtained by simulating mfor every test input in ts .
whole test suite coverage coverage achieved by ts overm .
initial coverage whole test suite coverage .
accumulative coverage initial coverage .
let exploration and exploitation be the max and min tweak parameters respectively.
.
exploitation tweak parameter .
repeat .
newts t weak ts p generating new candidate solution .
tso signal outputs obtained by simulating mfor every test input in newts .
whole test suite coverage coverage achieved by newts overm .
accumulative coverage accumulative coverage whole test suite coverage .
ifo tso highestfound .highestfound o tso .ts newts .
ifaccumulative coverage has reached a plateau at a value less than andp p max .p p .
reduce proportionally as accumulative coverage increases over initial coverage .
until maximum resources spent .
returnts figure our output diversity od test generation algorithm for simulink models.
ments may not reach high model coverage.
in od we initially generate test inputs with psegments lines .
the tweak operator does not change the number of segments either line .
we in creaseponly when the accumulative structural coverage achieved by the existing generated test suites reaches a plateau at a valueless than i.e.
remains constant for some consecutive iterations of the algorithm lines .
further although not shown in the algorithm we do not increase pif the last increase in phas not improved the accumulative coverage.
the tweak operator in od line is explorative at the beginning and becomes more exploitative as the search progresses.
our tweak is similar to the one used in ea algorithm .
at each iteration we shift every signal sg ts denoted by k v1 ... kp vp as follows we add values xi respectively yi to every vi respectively ki for1 i p. thexi respectively yi values are selected from a normal distribution with mean and variance maxr minr respectively k where ris the signal range and kis the number of simulation steps.
we control the degree of exploration and exploitation of our search us ing .
given that the search space of input signals is very large if we start by a purely exploitative search i.e.
.
our result will be biased by the initially randomly selected solution.
toreduce this bias we start by performing a more explorative search i.e.
.
.
however if we let the search remain explorative it may reduce to a random search.
hence we reduce iteratively in od such that the amount of reduction in is proportional to the increase in the accumulative structural coverage obtained by thegenerated test suites line .
finally we note that the tweak operator takes the signal segments pas an input line and in case the number of signal segments has increased from the previous it eration it ensures to increase the number of segments in signal sg.
we note that our input signal generation algorithm described above is geared towards the specific needs of the automotive domain where the dynamic behavior of the system is tested using sequences of step signals as input.
this is sufficient for capturingautomotive environment events which are largely aperiodic e.g.
driver s commands.
for other domains e.g.
the communicationdomain our test generation algorithm may need to be adapted to effectively capture highly frequent periodic signals.
.
experiment setup in this section we present the research questions.
we further describe our study subjects our metrics to measure the fault revealingability of test generation algorithms and the way we approximate their oracle cost.
we finally provide our experiment design.
.
research questions rq1 sanity check .
how does the fault revealing ability of the od algorithm compare with that of a random test generation strategy?
we investigate whether od is able to perform better than random testing which is a baseline of comparison.
we compare the fault revealing ability of the test suites generated by od when usedwith each of the o vandofobjective functions with that of the test suites generated by a random test generation algorithm.
rq2 comparing ovandof .how does the ofdiversity objective perform compared to the ovdiversity objective?
we compare the ability of the test suites generated by od with ovandofin revealing faults in time continuous simulink models.
in particular we are interested to know if irrespective of the size of the generated test suites any of these two diversity objectives is able toconsistently reveal more faults across different study subjects anddifferent fault types than the other.
rq3 comparison with sldv .
how does the fault revealing ability of the od algorithm compare with that of sldv?
with this question we compare an output diversity approach od with an approach based on structural coverage sldv in generating effective test suites for simulink models.
this question further enables us to provide evidence that our approach is able to outperform themost widely used industry strength simulink model testing tool.finally in contrast to rq1 and rq2 where we applied od to timecontinuous simulink models this question has to focus on discretemodels because sldv is only applicable to time discrete simulinkmodels.
hence this question allows us to investigate the capabili ties of od in finding faults for discrete simulink models as well.
.
study subjects we use four simulink models in our experiments two industrial models clutch position controller cpc and flap position controller fpc from delphi and two public domain models cruise controller cc and clutch lockup controller clc from the mathworks website .
table shows key character istics of these models.
cpc and cc include stateflows and fpcand clc are simulink models without stateflows.
fpc and cpc are time continuous models and incompatible with sldv .
the cc model which is the largest model from the sldv tutorial exam ples is compatible with sldv .
since the other tutorial examplesof sldv were small we modified the clc model from the mathworks website to become compatible with sldv by replacing the time continuous and other sldv incompatible blocks with theirequivalent or approximating discrete blocks.
we have made themodified version of clc available at .
note that we werenot able to make cpc fpc or any other delphi simulink models compatible with sldv since they contained complex s function blocks and hence they have to be almost reimplemented beforesldv can be applied to them.
the coverage criterion used by bothsldv and od in figure is decision coverage also known as branch coverage that aims to ensure that each one of the possi ble branches from each decision point is executed at least once andthereby ensuring that all reachable blocks are executed.
we chosebranch coverage as it is the predominant coverage criterion in theliterature .
table reports the total number of decision points in our study subjects.
in addition we report the total number of simulink blocks and stateflow states as well as input variables andconfiguration parameters for each model.
cpc and fpc are rep resentative models from the automotive domain with many input 600table characteristics of our study subject simulink models.
publicly availablenameno.
inputsno.
blocks states cpc no 590no.
decision points fpc cc clcno yesno.
configs yes12 variables and blocks.
in order to compare od with sldv we use cc from the mathworks website and the modified version of clcas both models are reasonably large and complex and yet compat ible with sldv .
.
measuring fault revealing ability we need test oracles to automatically assess the fault revealing ability of generated test suites in our experimental setting.
as dis cussed earlier in our work test oracles depend on manual inspection of output signals and on engineers estimates of acceptable deviations from the expected results.
to measure the fault revealing ability of a test suite we use a quantitative notion of oracle de fined as the largest normalized distance among all output signals between test results and the ground truth oracle .
for the pur pose of experimentation we use fault free versions of our subject models to produce the ground truth oracle.
let ts be a test suite generated by either od or sldv for a given faulty model m let o sg ... sg q be the set of output signals obtained by runningmfor the test inputs in ts and letg g1 ... g q be the corresponding ground truth oracle signals.
we define our quantitative oracle qo as follows qo m ts max i q dist sgi gi .
we use a threshold value thr to translate the quantitive oracle qo into a boolean fault revealing measure denoted by fr .
specifically fr returns true i.e qo m ts thr if there is at least one test input in ts for which the output of msufficiently deviates from the ground truth oracle such that a manual tester conclusively detects a failure.
otherwise fr returns false.
in our work we set thr to0.
.
we arrived at this value for thr based on our experience and discussions with domain experts.
in our experiments inaddition we obtained and evaluated the results for thr .
andths .
and showed that our results were not sensitive to such small changes in thr .
.
test oracle cost estimation since we assume that test oracles are evaluated manually to compare the fault revealing ability of od and sldv rq3 weneed to ensure that the test suites used for comparison have thesame oracle cost.
the oracle cost of a test suite depends on thesize of the test suite and the complexity of input data.
the latter in our work is determined by the number of signal segments p o f each input signal.
more precisely test suites ts i ...i q1 andts prime i prime ...i prime q2 have roughly the same oracle cost if they have the same size q q2 and the input signals in ts andts primehave the same number of segments.
that is for every test input ii sg1 ... sgn ints respectively ts prime there exists some test input ij sg prime ... sg primen ints prime respectively ts such that sgkandsg prime k for1 k n have the same number of segments.
in our experiments described in section .
we ensure that the test suites used for comparison of different test generation algorithms satisfy the above two conditions and hence can be usedas a proper basis to compare algorithms.
.
experiment design we developed a comprehensive list of simulink fault patterns and have made it available at .
examples of fault patterns in clude incorrect signal data type incorrect math operation and in correct transition condition.
we identified these patterns throughour discussions with senior delphi engineers and by reviewing theexisting literature on mutation operators for simulink models .
we have developed an automated fault seeding program to automatically generate faulty versions of cpc faulty ver sions of fpc faulty versions of cc and faulty versions ofclc one fault per each faulty model .
in order to achieve diversity in terms of the location and the types of faults our automation seeded faults of different types and in different parts of the mod els.
we ensured that every faulty model remains executable i.e.
no syntax error .
having generated the fault seeded models we performed two sets of experiments exp i and exp ii described below.
exp i focuses on answering rq1 and rq2 using the faulty versions of the time continuous models from table i.e.
cpc and fpc.
we ran the od algorithm in figure with vector based o v and feature based of objective functions.
for each faulty model and each objective function we ran od for sec and created a test suite of size qwhereqtook the following values and10.
we chose to examine the fault revealing ability of small test suites to emulate current practice where test suites are small sothat the test results can be inspected manually.
we repeated od 20times to account for its randomness.
specifically we sampled 444different test suites and repeated each sampling times i.e.
in total different test suites were generated for exp i .
overall exp i took about hours in execution time on a notebook with a .4ghz i7 cpu gb ram and gb ssd.
exp ii answers rq3 and is performed on the sldv compatible subject models from table i.e.
cc and clc.
to answer rq3 we compare the fault revealing ability of the test suites generated by sldv with that of the test suites generated by od.
we givesldv and od the same execution time budget sec in our experiment .
this time budget was sufficient for sldv to achieve a high level of structural coverage over the subject models.
further we ensure that the generated test suites have the same test oraclecost.
specifically for each faulty model m we first use sldv to generate a test suite ts mbased on the decision coverage criterion within the time allotted sec .
we then apply od to mto generate a test suite ts prime msuch that ts mandts prime mhave the same test oracle cost see section .
.
we have implemented a matlabscript that enables us to extract the size of the test suites as well as the number of input signal segments for each individual test input ofts m. further we have slightly modified the od algorithm in figure so that it receives as input the desired number pof signal segments for each input signal and it does not modify pduring search.
finally we note that while sldv is deterministic and isexecuted once per input model od is randomized and hence wererun it times for each faulty model.
.
results and discussions this section provides responses based on our experiment design for research questions rq1 torq3 described in section .
rq1 sanity .
to answer rq1 we ran exp i and further in order to compare with random testing for each faulty version werandomly generated test suites with size and .
we ensuredthat each test suite generated by random testing has the same ora cle cost as the corresponding test suites generated by the od algorithm.
moreover similar to od we reran random testing 20times.
figures a to c compare the fault revealing ability of random testing and od with the objective functions o fandov.
each distribution in figures a to c contains points.
each point relates to one faulty model and represents either the average quantitive ora601fr thr .
qo fr thr .
fr thr .
.
.
.
fr thr .
r0.
.
.5qo fr thr .
.
.
.
fr thr .
od o f od o v .
.
.5rod o f od o v rod o f od o v .
.
.
rod o f od o v rod o f od o v rod o f od o v .
.
.5rod o f od o v rod o f od o v .
.
.
.
.
.
a average qo and fr values for q for faulty models b average qo and fr values for q for faulty models c average qo and fr values for q for faulty models fr thr .
qo fr thr .
fr thr .
.
.
.
rod o f od o v rod o f od o v rod o f od o v rod o f od o v .
.
.
.
.
.
.
.
.
figure boxplots comparing average quantitative oracle values qo and fault revealing measures fr of od with both diversity objectives and random test suites for different thresholds and different test suite sizes.
cleqo or the average fault revealing measure fr over different test suites with a fixed size and obtained by applying a test generation algorithm to that faulty model.
note that the fr values are computed based on three different thresholds thr of0.
.
and0.
.
for example a point with x r and y .
in theqo plot of figure a indicates that the different random test suites with size generated for one faulty model achieved anaverageqo of0.
.
similarly a point with x od o f and y .
in any of the fr plots of figure b indicates that among the different test suites with size generated by applying odwith objective function o fto one faulty model test suites were able to reveal the fault i.e.
fr and could not reveal that fault i.e.
fr .
to statistically compare the qo andfr values we performed the non parametric pairwise wilcoxon pairs signed ranks test and calculated the effect size using cohen s d .
the level of significance was set to .
and following standard practice dwas labeled small for .
d .
medium for .
d .
and high for d .
.
testing differences in the average qo andfr distributions for all the three thresholds and with all the three test suite sizes showsthat od with both objective functions o fandovperforms significantly better than random test generation.
in addition for allthe comparisons between od and random the effect size is con sistently high for od with o fand medium for od with ov.
to summarize the fault revealing ability of od outperforms that of random testing.
rq2 comparing ofwithov .
the results in figure compare the average qo andfr values for the feature based od o f and the vector based od ov output diversity algorithms.
as for the qo distributions the statistical test results indicate that od o f performs significantly better than od o v for test suite sizes and with a small effect size.
for test suite size there is no statistically significant difference but od o f achieves higher mean and median qo values compared to od o v .
as for the fr distributions the improvements of od o f over od ov are not statistically significant.
however for all the three thresholds and with all the test suite sizes od o f consistently achieves higher mean and medianfr values compared to od o v .
specifically with threshold0.
the average fr is .
.
and .
for od o f and .
.
.
.
.
fr thr .
sldv od0.
.
.5qo sldv od fr thr .
.
.
.
sldv od fr thr .
.
.
.
sldv od figure boxplots comparing quantitative oracle values qo and fault revealing abilities of od and sldv for different thresholds.
and .
for od o v for test suite sizes and10 respectively.
that is across all the faults and with all test suite sizes the average probability of detecting a fault is about higher when we use od o f instead of od o v .to summarize the fault revealing ability of the od algorithm with the feature based diversity objec tive is higher than that of the od algorithm with the vector based diversity objective.
rq3 comparison with sldv .
to answer rq3 we used the better diversity objective from rq2 i.e.
od with the feature based diversity objective and performed exp ii on30faulty models of cc and clc.
we evaluate the fault revealing ability of sldv and od by comparing the quantitative oracle qo and the fault revealing measure fr values obtained over these faulty models.
in addition we investigate if any of sldv and od subsumes the other technique fault revealing subsumption .
that is we determine if any of od and sldv does not find any additional faults missedby the other technique.
finally we compare the structural cover age achieved by each of sldv and od over these faulty models.we report coverage results for two reasons we confirm our earlier claim that with the timeout of sec used in exp ii sldv has been able to achieve high structural coverage.
we provide evidence that achieving higher structural coverage does not neces sarily lead to better fault revealing ability.
comparing fault revealing ability.
we computed qo andfr with three thr values of .
.
and0.
over the test suites generated by od and sldv .
figure compares the distributionsobtained for the faulty models of cc and clc.
recall that sldv is deterministic and od is randomized.
so in figure each point in distributions related to sldv shows the value of qo orfr obtained for one and the only one test suite generated by sldv for one faulty model.
in contrast each point in distributions related to od shows the average value ofqo orfr for different test suites generated by od for each faulty model.
further for each faulty model sldv yields a fr value of one or zero respectively indicating whether sldv reveals the fault or not.
however for od we compute an average fr value over different runs which is between zero and one indicating an estimated probabilityfor od to reveal a fault.
as shown in figure the qo andfr values obtained for sldv are very small compared to those obtained by od .
testing differences in qo andfr distributions with all the three thresholds shows that od performs significantly better than sldv .
in addi tion for all the four comparisons depicted in figure the effectsize is high .
fault revealing subsumption.
tables and compare performance of od and sldv for individual faults.
specifically table 2shows for each faulty model the distribution of qo values obtained by od and the single value of qo obtained by sldv .
note that xshows the mean and q q2 andq3refer to the three quartiles of the qo distribution obtained by different runs of od i.e.
q1 q2 andq3are the 25th 50th and 75th percentiles respectively .
in addition we report as denoted by pin table the percentages of od runs that achieve a qo value greater than or equal to that obtained by sldv .
for example for faults 602table quantitative oracle qo distributions for od and singleqo values for sldv per each faulty model.
od sldv1 x q1 q2 q30.
.
.
.
.013od sldv2 00od sldv3 od sldv4 od sldv5 .012od sldv6 od sldv7 od sldv8 od sldv9 x q1 q2 q3od sldv10 od sldv11 od sldv12 od sldv13 od sldv14 od sldv15 od sldv16 od sldv17 x q1 q2 q3od sldv18 od sldv19 od sldv20 od sldv21 od sldv22 od sldv23 od sldv24 od sldv25 x q1 q2 q3od sldv26 od sldv27 od sldv28 od sldv29 od sldv300.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.4450p p p p1.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table the number of fault revealing runs of od out of for our faulty models and the fault s that sldv is able to find with a threshold thr o f0.
.
faults sldvod and in table and30 of the od runs respectively yield qo values that are not worse than those generated by sldv .
table shows for each faulty model and when consideringfr with threshold .
whether sldv is able to identify the fault or not.
we depict detection of a fault by sldv with squaresolid.
further the table shows out of the runs how many times od is able tofind the fault.
note that the results for the thresholds .
and0.
were similar to those in table .
based on table six faults go undetected by both sldv and od irrespective of the threshold value i.e.
for six faults we haveqo for both sldv and all od runs .
there is no fault that sldv can possibly detect qo but od cannot.
for faults all runs of od yield results that are at least as good as those ofsldv p .
for all the faults the average of qo obtained by od denoted by x is higher than the value of qo obtained by sldv .
finally sldv totally fails to detect faults and while some runs of od are able to identify these seven faults.
based on table sldv identifies only one fault with athreshold of .
and that particular fault is also detected by all the runs of od.
the results for thresholds .
and0.
are similar.
comparing coverage.
figure compares the structural coverage percentages i.e.
decision coverage achieved by test suites gener ated by od and sldv over the faulty models of cc and clc.
asbefore the distribution for sldv shows the percentages of structural coverage achieved by individual test suites generated for individual faulty models while the distribution for od shows theaverage of structural coverage percentages obtained by differentruns of od for each faulty model.
as shown in the figure sldv was able to achieve on average a coverage of for cc models and85 for clc models.
in contrast od achieved on average coveragecc clc sldv od84 sldv od75 figure the percentages of branch decision coverage achieved by od and sldv over the faulty versions of cc andclc subject models.
a od input example c sldv input example .
.
.
.
.
.
.
.
.
.
.
.
.
b od output example time s time s time s time s d sldv output exampleset input set input throttle outputthrottle output500 faulty model output ground truth oracle figure examples of test inputs and output signals generated by sldv and od algorithm.
a coverage of for cc and for clc.
in addition sldv has been able to cover out of the fault seeded blocks and od covered out of the fault seeded blocks.
this shows thatwithin sec sldv had sufficient time to cover the structureof the faulty models only one fault seeded block was missed .
indeed for out of faults sldv terminated before the timeout period elapsed.
hence by increasing the execution time it isunlikely that sldv s fault revealing ability would be impacted.
in summary our comparison of sldv and od shows that for both studied models od is able to reveal significantly more faultscompared to sldv .
od subsumes sldv in revealing faults any fault identified by sldv is also identified by od.
sldvwas able to cover a large part of the underlying models within the given timeout period i.e.
out of the fault seeded blocks and further it achieved slightly higher decision coverage over studysubjects compared to od.
however covering a fault does not nec essarily lead to detecting that fault.
in particular sldv was ableto reveal only one out of the faults that it could cover.
finally our results on comparing sldv and od are not impacted by small modifications in the threshold values used to compute thefault revealing measure fr .
discussion.
why does sldv perform poorly compared to od?
our results in rq3 show that compared to the output diversity od algorithm sldv is less effective in revealing faults in simulinkmodels.
in our experiment even though test suites generated bysldv cover most faulty parts of the simulink models the outputs produced by these test suites either do not deviate or only slightly deviate from the ground truth oracle hence yielding very small qo values.
in contrast od generates test suites with output signalsthat are more distinct from the ground truth oracle.
note that as discussed in section any deviation should exceed some threshold to be conclusively deemed a failure.
for example figures b and d show two output signals solid lines of a faulty model together with the oracle signals dashed lines generated by od and sldv respectively.
note that the range of the y axis in figure b is times larger than that in figure d .
hence the deviation from theoracle in figure b is much larger than that in figure d .
inparticular the signals in figures b and d respectively produceqo values of .
and0.
.
therefore the output in figure b is more fault revealing that the one in figure d .
since sldv is commercial and its technical approach description is not publicly available we cannot precisely determine thereasons for its poor performance.
we conjecture however that the reason for sldv s poor fault finding lies in its input signal generation strategy.
specifically all value changes in the input signalsgenerated by sldv typically occur during the very first simulationsteps and then the signals remain constant for the most part anduntil the end of the simulation time.
in contrast changes in the input signals generated by od can occur at any time during the entire simulation period.
for example figures a and c show two ex amples of input signals generated by od and sldv respectively.in both case the signal value changes from one to zero.
however for the signal in figure a the change occurs almost in the middleof the simulation at 6sec while in figure c the change occurs after the first step at .
sec .
signals in figures a and c happen to cover exactly the same branches of the underlying model.
however they respectively yield the outputs in figures b and d with drastically different fault revealing ability.
.
related work modeling is by no means new to the testing and verification community and has already been the cornerstone of a number of well studied techniques.
in particular two well known techniques model based testing and model checking have been previously applied to test and verify simulink models.
model based testing relies on models to generate test scenarios and oracles for implementation level artifacts.
a number of model based testing techniques have been applied to simulink models with the aim of achieving high structural coverage or detecting a large number of mutants.
for ex ample search based approaches reachability analysis guided random testing and a combination of thesetechniques have been previously applied to simulink models to generate coverage adequate test suites.
alternatively various search based and bounded reachabilityanalysis techniques have been used to generate mutant killingtest suites from simulink models.
these techniques aim to generate test suites as well as oracles from models that are considered to be correct.
in reality however simulink models might contain faults.hence in our work we propose techniques to help testing complexsimulink models for which automated and precise test oracles arenot available.
further even though in simulink every variable is described using signals unlike our work none of the above techniques generate test inputs in terms of signals.
model checking is an exhaustive verification technique and has a long history of application in software and hardware verification .
it has been previously used to detect faults in simulink models by showing that a path leading to an error e.g.
an assertion or a runtime error is reachable or by maximizingstructural coverage e.g.
by executing as many paths as possible ina model .
to solve the reachability problem or to achieve high coverage these techniques often extract constraints from the underlying simulink models and feed the constraints into some constraintsolver or sa t solver.
some alternative techniques trans late simulink models into code and use existing code analysis tools such as java pathfinder or klee to detect faults.
allthese approaches only work for code generation models with linear behavior and fail to test or verify simulation models with time continuous behavior.
our approach however is applicable to bothsimulation and code generation simulink models.
recent work in the intersection of simulink testing and signal processing has focused on test input signal generation using evolu tionary search methods .
these techniques however assume automated oracles e.g.
assertions are provided.
since test oracles are automated they do not pose any restriction on the shape of test inputs.
in our work however we restrict variations in inputsignal shapes as more complex inputs increase the oracle cost.
sim ilar to our work the work of proposes a set of signal features.these features are viewed as basic constructs which can be composed to specify test oracles.
in our work since oracle descriptions do not exist we use features to improve test suite effectiveness bydiversifying feature occurrences in test outputs.
our algorithm uses whole test suite generation that was proposed for testing software code.
this approach evolves an entire test suite instead of individual test cases with the aim of covering all structural coverage goals at the same time.
our algorithm in stead attempts to diversify test outputs by taking into account allthe signal features see figure at the same time.
the notion of output diversity in our work is inspired by the output uniqueness criterion .
as noted in effectiveness of this criterion de pends on the definition of output difference and differs from one context to another.
while in output differences are described in terms of the textual or structural aspects of html code in our work output differences are characterized by signal shape features.
.
conclusions simulink is a prevalent modeling language for cyber physical systems cpss and supports the two main cps modeling goals automated code generation and simulation i.e.
design time testing.
in this paper we distinguished simulink simulation and code generation models and illustrated differences in their behaviors us ing examples.
in contrast to the existing testing approaches that areonly applicable to code generation simulink models we proposed a testing approach for both kinds of simulink models based on our notion of feature based output diversity for signals.
our testing ap proach is implemented using a meta heuristic search algorithm thatis guided to produce test outputs exhibiting a diverse set of signalfeatures.
our evaluation is performed using two industrial and two public domain simulink models and shows that our approach significantly outperforms random test generation.
our algo rithm when used with the feature based notion of output diversitygenerates higher fault revealing rates compared to when output diversity is measured based on the euclidean distance between signalvectors.
our approach is able to reveal significantly more faults compared to simulink design v erifier sldv and further it sub sumes sldv as it is able to find the faults identified by sldv witha probability.
in this paper we showed that the fault revealing ability of our output diversity algorithm significantly outperforms that of sldvwhen used with the decision coverage branch coverage criterion.we expect no notable changes in our results if we use sldv with a stronger coverage criterion e.g.
mc dc since the limitations of sldv is due to its test generation strategy and not in its ability tocover simulink models.
nevertheless in future we plan to repeatour experiments with mc dc test suites generated by sldv .