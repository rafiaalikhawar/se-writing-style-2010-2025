revisiting the impact of classification techniques on the performance of defect prediction models baljinder ghotra shane mcintosh ahmed e. hassan software analysis and intelligence lab sail school of computing queen s university canada ghotra mcintosh ahmed cs.queensu.ca abstract defect prediction models help software quality assurance teams to effectively allocate their limited resources to the most defect prone software modules.
a variety of classification techniques have been used to build defect prediction models ranging from simple e.g.
logistic regression to advanced techniques e.g.
multivariate adaptive regression splines mars .
surprisingly recent research on the nasa dataset suggests that the performance of a defect prediction model is not significantly impacted by the classification technique that is used to train it.
however the dataset that is used in the prior study is both a noisy i.e.
contains erroneous entries and b biased i.e.
only contains software developed in one setting.
hence we set out to replicate this prior study in two experimental settings.
first we apply the replicated procedure to the same known to be noisy nasa dataset where we derive similar results to the prior study i.e.
the impact that classification techniques have appear to be minimal.
next we apply the replicated procedure to two new datasets a the cleaned version of the nasa dataset and b the promise dataset which contains open source software developed in a variety of settings e.g.
apache gnu .
the results in these new datasets show a clear statistically distinct separation of groups of techniques i.e.
the choice of classification technique has an impact on the performance of defect prediction models.
indeed contrary to earlier research our results suggest that some classification techniques tend to produce defect prediction models that outperform others.
i. i ntroduction a disproportionate amount of the cost of developing software is spent on maintenance .
fixing defects is a central software maintenance activity.
however before defects can be fixed they must be detected.
software quality assurance sqa teams are dedicated to this task of defect detection.
defect prediction models can be used to assist sqa teams with defect detection.
broadly speaking defect prediction models are trained using software metrics e.g.
size and complexity metrics to predict whether software modules will be defective or not in the future.
knowing which software modules are likely to be defect prone before a system has been deployed allows practitioners to more efficiently and effectively allocate sqa effort.
to perform defect prediction studies researchers have explored the use of various classification techniques to train defect prediction models .
for example in early studies researchers used simple techniques like logistic regression and linear regression to train defect prediction models.
in more recent work researchers have used more advanced techniques like multivariate adaptive regres sion splines mars personalized change classification pcc and logistic model trees lmt .
ensemble methods that combine different machine learning techniques have also been explored .
moreover researchers have started to develop context sensitive techniques that are aware of the peculiarities of software defects .
despite recent advances in machine learning studies suggest that the classification technique used to train defect prediction models has little impact on its performance .
for example lessmann et al.
conducted a study comparing the performance of different classification techniques on the nasa corpus.
their results show that the performance of of the classification techniques are statistically indistinguishable from each other.
however the nasa corpus that was used in the prior work is noisy and biased.
indeed shepperd et al.
found that the original nasa corpus contains several erroneous and implausible entries.
this noise in the studied corpus may have led prior studies to draw incorrect conclusions.
furthermore the studied corpus only contains proprietary software developed within one organization.
software developed in a different setting e.g.
open source may lead to different conclusions.
therefore we set out to revisit the findings of lessmann et al.
both in the original known to be noisy setting section iv and in two additional settings section v .
we find that we can indeed replicate the initial results of lessmann et al.
when the procedure is reapplied to the knownto be noisy nasa corpus.
on the other hand the results of our experiment in two additional settings seem to contradict lessmann et al.
s early results.
we structure our extended replication by addressing the following two research questions rq1 do the defect prediction models of different classification techniques still perform similarly when trained using the cleaned nasa corpus?
no we find a clear separation of classification techniques when models are trained using the cleaned nasa corpus .
the scott knott statistical test groups the classification techniques into four statistically distinct ranks.
our results show that models trained using lmt and statistical techniques like simple logistic tend to outperform models trained using clustering rule based support vector machines svm nearest neighbour and neural network techniques.rq2 do the defect prediction models of different classification techniques still perform similarly when trained using a corpus of open source systems?
no scott knott test results show that similar to rq1 classification techniques like lmt simple logistic and their combination with ensemble methods tend to produce defect prediction models that outperform models trained using clustering rule based svm nearest neighbour and neural network techniques.
in summary contrary to prior results our study shows that there are statistically significant differences in the performance of defect prediction models that are trained using different classification techniques.
we observe that classification techniques are grouped into largely consistently ranks in both the cleaned nasa and promise corpora.
indeed some techniques tend to produce defect prediction models that outperform models that are trained using other techniques.
given the ease of access to such advanced techniques nowadays in the toolboxes of researchers e.g.
randweka we encourage researchers to explore the various techniques available in such toolboxes instead of relying on prior findings to guide their choice of classification technique.
paper organization the remainder of the paper is organized as follows.
section ii describes the classification techniques that we used in our study.
section iii discusses the design of our case study.
section iv discusses the results of our replication study on the original known to be noisy nasa corpus while section v presents the results of our two research questions.
section vi discloses the threats to the validity of our study.
section vii surveys related work.
finally section viii draws conclusions and describes promising directions for future work.
ii.
c lassification techniques in this section we briefly explain the eight families of classification techniques that are used in our study.
table i provides an overview of each technique.
a. statistical techniques statistical techniques are based on a probability model .
these techniques are used to find patterns in datasets and build diverse predictive models .
instead of simple classification statistical techniques report the probability of an instance belonging to each individual class i.e.
defective or not .
in this paper we study the naive bayes and simple logistic statistical techniques.
naive bayes is a probability based technique that assumes that all of the predictors are independent of each other.
simple logistic is a generalized linear regression model that uses a logit link function.
b. clustering techniques clustering techniques divide the training data into small groups such that the similarity within groups is more than across the groups .
clustering techniques use distance andsimilarity measures to find the similarity between two objects to group them together.
in this paper we study the k means and expectation maximization clustering techniques.
k means divides the data into kclusters and centroids are chosen randomly in an iterative manner .
the value of kimpacts the performance of the technique .
we experiment with four different kvalues i.e.
and and found that k tends to perform the best.
we also study the expectation maximization em technique which automatically splits a dataset into an approximately optimal number of clusters .
c. rule based techniques rule based techniques transcribe decision trees using a set of rules for classification.
the transcription is performed by creating a separate rule for each individual path starting from the root and ending at each leaf of the decision tree .
in this paper we study the repeated incremental pruning to produce error reduction ripper and ripple down rules ridor rule based techniques.
ripper is an inductive rulebased technique that creates series of rules with pruning to remove rules that lead to lower classification performance .
ridor is a rule based decision tree technique where the decision tree consists of four nodes a classification a predicate function a true branch and a false branch.
each instance of the testing data is pushed down the tree following the true and false branches at each node using predicate functions.
the final outcome is given by the majority class of the leaf node .
d. neural networks neural networks are systems that change their structure according to the flow of information through the network during training .
neural network techniques are repeatedly run on training instances to find a classification vector that is correct for each training set .
in this paper we study the radial basis functions neural network technique.
radial basis functions consists of three different layers an input layer which consists of independent variables output layer which consists of the dependent variable and the layer which connects the input and output layer to build a model .
e. nearest neighbour nearest neighbour a.k.a.
lazy learning techniques are another category of statistical techniques.
nearest neighbour learners take more time in the testing phase while taking less time than techniques like decision trees neural networks and bayesian networks during the training phase .
in this paper we study the knn nearest neighbour technique.
knn considers the kmost similar training examples to classify an instance.
knn computes the euclidean distance to measure the distance between instances .
we findk to be the best performing kvalue of the five tested options i.e.
and .table i overview of the studied classification techniques .
family technique abbreviation statistical techniquesnaive bayes nb simple logistic sl clustering techniquesk means k means expectation maximizationem rule based techniquesrepeated incremental pruning to produce error reductionripper ripple down rulesridor neural networks radial basis functionsrbfs nearest neighbourk nearest neighbourknn support vector machinessequential minimal optimizationsmo decision treesj48 j48 logistic model tree using logistic regressionlmt ensemble methods using lmt nb sl smo and j48bagging bag lmt bag nb bag sl bag smo and bag j48 adaboost ad lmt ad nb ad sl ad smo and ad j48 rotation forest rf lmt rf nb rf sl rf smo and rf j48 random subspacersub lmt rsub nb rsub sl rsub smo and rsub j48 f .
support vector machines support vector machines svms use a hyperplane to separate two classes i.e.
defective or not .
the number of features in the training data does not affect the complexity of an svm model which makes svm a good fit for experiments where there are fewer training instances than features .
in this paper we study the sequential minimal optimization smo svm technique.
smo analytically solves the large quadratic programming qp optimization problem which occurs in svm training by dividing the problem into a series of possible qp problems .g.
decision trees decision trees use feature values for the classification of instances.
a feature in an instance that has to be classified is represented by each node of the decision tree while the assumption values taken by each node is represented by each branch.
the classification of instances is performed by following a path through the tree from root to leaf nodes by checking feature values against rules at each node.
the root node is the node that best divides the training data .
in this paper we study the logistic model tree lmt and j48 decision tree techniques.
similar to model trees i.e.
regression trees with regression functions lmt is a decision tree like structure with logistic regression functions at the leaves .
j48 is a c4.
based technique that uses information entropy to build the decision tree.
at each node of the decision tree a rule is chosen by c4.
such that it divides the set of training samples into subsets effectively .
h. ensemble methods ensemble methods combine different base learners together to solve one problem.
models trained using ensemble methods typically generalize better than those trained using the standalone techniques .
in this paper we study the bagging adaboost rotation forest and random subspace ensemble methods.
bagging bootstrap aggregating is designed to improve the stability and accuracy of machine learning algorithms.
bagging predicts an outcome multiple times from different training sets that are combined together either by uniform averaging or with voting .
adaboost performs multiple iterations each time with different example weights and gives a final prediction through combined voting of techniques .
rotation forest applies a feature extraction method to subsets of instances to reconstruct a full feature set for each technique in the ensemble.
random subspace creates a random forest of multiple decision trees using a random selection attribute approach.
a subset of instances is chosen randomly from the selected attributes and assigned to the learning technique .
iii.
s tudy design in this section we discuss the studied corpora and our approach to constructing and evaluating defect prediction models to address our research questions.
figure provides an overview of the steps in our approach.
a. studied corpora in order to replicate the case study of lessmann et al.
we use the original known to be noisy nasa corpus.
moreover in order to perform our extended study we use the cleaned version of the nasa corpus as provided by shepperd et al.
rq1 and the promise corpus1 rq2 .
an overview of the studied corpora is shown in table ii.
the studied corpora that we use in our study are available online enabling further replication and extension of our results.
iv noisy nasa corpus rq1 clean nasa corpus rq2 promise corpus b fold generation training corpus testing corpus90 c model construction clustering models d model evaluation repeat times x fold cross validation results a studied corpora other models from table istatistical models... fig.
.
an overview of our model construction and evaluation approach.
table ii an overview of the studied corpora .
corpus dataset no.
of modules defective defective noisy nasa section iv cm1 .
jm1 .
kc1 .
kc3 .
kc4 .
mw1 .
pc1 .
pc2 .
pc3 .
pc4 .2clean nasa rq1 cm1 .
jm1 .
kc1 .
kc3 .
mw1 pc1 .
pc2 .
pc3 .
pc4 .8promise rq2 ant .
.
camel .
.
ivy .
.
jedit .
log4j .
lucene .
.
poi .
tomcat .
xalan .
.
xerces .
.
b. fold generation the defect prediction models are trained using the fold cross validation approach which divides an input dataset into folds of equal size.
of the folds are allocated to the training corpus and fold is set aside for testing .
the training corpus is used to train the models using different classification techniques whereas the testing corpus is used to analyze model performance.
this process is repeated ten times using each fold as the testing corpus once.
to further validate our results we repeat the entire fold process ten times total iterations .c.
model construction in order to train our defect prediction models we use implementations of the classification techniques of section ii provided by the weka machine learning toolkit.
the process of training defect prediction models using clustering techniques is different than for the other techniques.
specifically we adopt the approach of bettenburg et al.
in order to train models using clustering techniques .
the training corpus is divided into clusters and a classification model is trained within each cluster using the naive bayes technique.
to determine which model to use in order to classify any particular row in the testing corpus we use the euclidean distance between each testing data point and the various clusters.
the model of the cluster that is the minimum distance away is used to predict the final outcome for that row.
d. model evaluation to compare the performance of defect prediction models we use the area under the receiver operating characteristic curve auc which plots the false positive rate i.e.
fp fp tn against the true positive rate i.e.
tp fn tp .
larger auc values indicate better performance.
auc values above .
indicate that the model outperforms random guessing.
scott knott test .
we use the scott knott test to group classification techniques into statistically distinct ranks .
.
the scott knott test uses hierarchical cluster analysis to partition the classification techniques into ranks.
the scottknott test starts by dividing the classification techniques into two ranks on the basis of mean auc values i.e.
mean auc of ten fold runs for each classification technique .
if the divided ranks are statistically significantly different then scott knott recursively executes again within each rank to further divide the ranks.
the test terminates when ranks can no longer be divided into statistically distinct ranks .
we used the scott knott test to overcome the confounding issue of overlapping groups that are produced by several other post hoc tests such as nemenyi s test which was used by the original study.
nemenyi s test produces overlapping groups of classification techniques implying that there exists noscott knott test 2nd run project scott knott test 1st run ... mean auc valuetechnique mean auc valuetechnique mean auc valuetechnique 110x mean auc valuetechnique mean auc valuetechnique mean auc valuetechnique 210x mean auc valuetechnique n mean auc valuetechnique n mean auc valuetechnique n10x t2 t5 t7technique rank t1 t10 t3 t4 t6 t8 t9 4t2 t5technique rank t1 t7 t10 t3 t4 t6 t8 t9 4project scott knott test 1st run ... mean auc valuetechnique mean auc valuetechnique mean auc valuetechnique 110x mean auc valuetechnique mean auc valuetechnique mean auc valuetechnique 210x mean auc valuetechnique n mean auc valuetechnique n mean auc valuetechnique n10x t3 t7 t8technique rank t2 t10 t1 t4 t6 t5 t9 project m scott knott test 1st run ... mean auc valuetechnique mean auc valuetechnique mean auc valuetechnique 110x mean auc valuetechnique mean auc valuetechnique mean auc valuetechnique 210x mean auc valuetechnique n mean auc valuetechnique n mean auc valuetechnique n10x t2 t10technique rank t1 t7 t8 t3 t4 t6 t5 t9 ... fig.
.
our approach for ranking classification techniques using a double scott knott test.
table iii metrics used to train defect prediction models in the nasa corpus .
category metric definition rationale mccabe software metricscyclomatic complexity cyclomatic density design complexity essential complexity and pathological complexitymeasures of the branching complexity of the software module.complex software modules may be more prone to defects .
halstead attributescontent difficulty effort error est length level prog time volume num operands num operators num unique operands num unique operatorsestimates of the complexity of reading a software module based on the vocabulary used e.g.
number of operators and operands .software modules that are complex to understand may increase the likelihood of incorrect maintenance and thus increase defect proneness .
loc counts loc total loc blank loc comment loc code and comment loc executable and number oflinesmeasures of the size of a software module.larger software modules may be difficult to understand entirely and thus may be more prone to defects .
miscellaneous branch count call pairs condition count decision count decision density design density edge count essential density parameter count maintenance severity modified condition count multiple condition count global data density global data complexity percent comments normalized cyclomatic complexity and node countmetrics that are not well defined metrics in the mdp database .n a statistically significant difference among the defect prediction models trained using many different classification techniques.
to address our research questions figure provides an overview of our scott knott test approach.
to find statistically distinct ranks of classification techniques we performed a double scott knott test.
the first run of the scott knott test is run over each individual project.
we input the mean auc values of the different fold cross validation runs of each classification technique to the scott knott test to find the statistical distinct ranks of classification techniques within each project.
in the second run for each classificationtechnique we have ten different scott knott ranks i.e.
one from each project which we input into another scott knott test to find the final statistically distinct ranks of techniques.
our approach of using a double scott knott test ensures that our analysis recognizes techniques that perform well across projects independent of their actual auc value.
for example it might be the case that one classification technique achieves an auc of .
for project a and an auc of .
for project b. at first glance it might seem that such a technique is not a strong performing technique.
however if an auc of .
is the best auc value of the studied techniques for project a andtable iv the studied techniques ranked according to our double scott knott test on the known to be noisy nasa corpus .
overall rankclassification techniquemedian rankaverage rankstandard deviation 1ad nb em rbfs ad sl rf nb rsub nb ad smo k means knn nb sl bag nb rsub sl ad j48 rsub j48 ad lmt lmt rf sl bag sl rf j48 rsub lmt bag j48 rf lmt and bag lmt3.
.
.
2rsub smo smo rf smo ridor bag smo ripper and j488 .
.
auc of .
is the best auc for project b then that particular classification technique is the top performing technique.
the first scott knott test creates the project level ranking for each technique using the auc values then the second scott knott test creates a global ranking of techniques using their projectlevel rankings.
iv.
r eplication study in this section we discuss the results of our replication study on the known to be noisy nasa corpus.
the nasa corpus provides several software metrics that can be used to predict defect prone modules.
table iii describes the metrics present in the nasa corpus.
results .similar to prior work our replication using the known to be noisy nasa data yields a pattern of two statistically distinct groups of classification techniques .
table iv shows that of the studied classification techniques appear in the first scott knott group.
figure shows the auc values for each of the studied techniques.
figure highlights the importance of our double scottknott approach.
looking at project jm1 and project pc4 or kc4 many of the best techniques for project jm1 perform worse than some of the worst performing techniques on the pc4 orkc4 project.
our double scott knott approach is able to account for such peculiarities by considering the rank in the second scott knott iteration.
our replication yields a similar pattern of classification techniques as was found in the previous nasa study i.e.
of the studied techniques end up in the same statistical group.
most classification techniques produce defect prediction models that have statistically indistinguishable performance from one another.
v. e xtended case study in this section we present the results of the extended lessmann et al.
replication with respect to our two research questions.
generally speaking our goal is to study whether the previously reported results of lessmann et al.
and confirmed by us in section iv still hold in two new ad j48ad lmtad nbad slad smobag j48bag lmtbag nbbag slbag smoemj48k meansknnlmtnbrbfsrf j48rf lmtrf nbrf slrf smoridorripperrsub j48rsub lmtrsub nbrsub slrsub smoslsmo .
.
.
.
.9fig.
.
auc values for the studied classification techniques on the knownto be noisy nasa corpus.
cm1 jm1 kc1 kc3 kc4 mw1 pc1 pc2 pc3 pc40.
.
.
.
.
fig.
.
auc values of all classification techniques that are applied to the different projects in the known to be noisy nasa corpus.
experimental settings which we formulate as our two research questions.
below we discuss our approach for addressing each research question and the results.table v differences in the noisy and clean nasa corpora .
dataset instances before cleaninginstances after cleaninginstances removed by cleaning cm1 jm1 kc1 kc3 mw1 pc1 pc2 pc3 pc4 table vi cleaning criteria applied to the noisy nasa corpus by shepperd et al.
.
criterion data quality category explanation identical cases instances that have identical values for all metrics including class label.
inconsistent cases instances that satisfy all conditions of case but where class labels differ.
cases with missing valuesinstances that contain one or more missing observations.
cases with conflicting feature valuesinstances that have or more metric values that violate some referential integrity constraint.
for example loc total is less than commented loc.
however commented loc is a subset of loc total.
cases with implausible valuesinstances that violate some integrity constraint.
for example value of loc .
.
rq1 do the defect prediction models of different classification techniques still perform similarly when trained using the cleaned nasa corpus?
approach .
our extended study starts with using the cleaned version of nine of the nasa projects that were used by lessmann et al.
.
the purpose of using the cleaned version is to find whether the removal of noisy instances has an impact on the performance of defect prediction models produced using the studied classification techniques.
an overview of the cleaned version of the nasa dataset is shown in table ii.
we are only able to use of the nasa projects because the kc4 dataset does not contain any instances after cleaning.
the differences in the noisy and clean nasa corpora are highlighted in table v. the preprocessing criteria that shepperd et al.
applied to the noisy nasa corpus are presented in table vi.
results .we find four statistically distinct ranks of classification techniques when models are trained using datasets from the cleaned nasa corpus.
table vii shows that there is a clear separation of classification techniques into four scottknott ranks in the cleaned nasa corpus.
this is in stark contrast with the two groups of classification techniques that lessmann et al.
found and that section iv confirmed .
the techniques are divided into four distinct ranks in whichtable vii the studied techniques ranked according to the double scott knott test on the clean nasa corpus .
overall rankclassification techniquemedian rankaverage rankstandard deviation 1bag j48 lmt rf j48 sl bag sl rf sl rsub sl rf lmt rsub lmt and bag lmt1.
.
.
2rbfs ad nb knn ad sl rf nb rsub nb nb bag nb ad smo ad j48 ad lmt and rsub j482.
.
.
ripper em j48 and kmeans5.
.
.
4rsub smo rf smo smo ridor and bag smo7.
.
.
ensemble techniques i.e.
rf a base learner decision trees i.e.
lmt statistical techniques i.e.
simple logistic and naive bayes neural networks i.e.
rbfs and nearest neighbour i.e.
knn outperformed models trained using rule based techniques i.e.
ripper and ridor clustering techniques i.e.
k means and em and svm i.e.
smo .
our results in table vii show that the prediction models trained using ensemble techniques i.e.
rf a base learner decision trees i.e.
lmt and statistical techniques i.e.
simple logistic also outperformed models trained using nearest neighbour i.e.
knn and neural networks i.e.
rbfs .
furthermore we find that prediction models trained using the combination of lmt and simple logistic with ensemble methods like bagging rotation forest and random subspace produce defect prediction models that perform the best i.e.
achieve the highest average scott knott rank when compared to other combinations i.e naive bayes j48 and smo with ensemble methods.
since four statistically distinct ranks of classification techniques emerge we conclude that the used classification technique has a statistically significant impact on the performance of defect prediction models trained using the datasets from the cleaned nasa corpus.
rq2 do the defect prediction models of different classification techniques still perform similarly when trained using a corpus of open source systems?
approach .
in an effort to further generalize our extended study results we replicate the lessmann et al.
study using another ten datasets from the promise corpus.
the selected promise datasets were used in several previous studies .
the promise datasets provide different metrics than the nasa corpus.
table ii provides an overview of the promise corpus.
table viii describes the metrics provided by the promise corpus.
results .we find that similar to our results of rq1 a pattern of four statistically distinct ranks of classification techniques emerges in the promise corpus as well .table viii metrics used to train defect prediction models in the promise corpus .
category metric definition rationale size loc measures of the size of a software module.
larger software modules may be difficult to understand entirely and thus may be more prone to defects .
ck wmc dit cbo noc lcom rfc ic cbm amc lcom3measures of the complexity of a class within an object oriented system design.more complex classes may be more prone to defects .
mccabe s complexitymax cc and avg cc measures of the branching complexity of the software module.complex software modules may be more prone to defects .
qmood dam moa mfa cam npm measures of the behavioural and structural design properties of classes objects and the relations between them .higher qmood metrics imply higher class quality.
on the other hand lower qmood metrics imply lower class quality2 which may lead to defects.
martin s metrics ca ce measures of the relationship between software components including calls as well as number of instances .higher values indicate low encapsulation reusability and maintainability which may lead to defects.
ad j48ad lmtad nbad slad smobag j48bag lmtbag nbbag slbag smoemj48k meansknnlmtnbrbfsrf j48rf lmtrf nbrf slrf smoridorripperrsub j48rsub lmtrsub nbrsub slrsub smoslsmo .
.
.
.8fig.
.
auc values of the classification techniques on the promise corpus.
table ix shows the four statistically distinct ranks generated by applying the double scott knott test.
the results show that prediction models trained using decision trees statistical techniques k nearest neighbour and neural networks outperform models trained using clustering techniques rule based techniques and svm.
furthermore we find that when ensemble methods are used to combine lmt and simple logistic the best performance is achieved when compared to the other classification techniques.
figure shows the mean auc values of techniques on the promise dataset.
we observe that the mean auc value of the best rf lmt and the worst rf smo classification techniques in the promise corpus have higher values when compared to best bag lmt and worst rsub smo classification techniques in the cleaned nasa corpus.
figure again emphasizes the importance of using the ant camel ivy jedit log4j lucene poi tomcat xalan xerces0.
.
.
.
fig.
.
auc values of all classification techniques that are applied to the different projects in the promise corpus.
double scott knott approach in our analysis.
the best performing technique for the camel project is well below the worst performing technique for other projects such as poi andxalan .
we observed some common patterns in the ranks of both the nasa and promise corpora.
our results shows that lmt when combined with ensemble methods i.e.
bagging random subspace and rotation forest achieve top rank performance in both corpora.
also ensembles of simple logistic along with stand alone lmt and simple logistic appear in the top scottknott rank in both corpora.
furthermore clustering techniques i.e.
em and k means rule based techniques ripper and ridor and svm i.e.
smo appear in the lowest scott knott rank in both corpora.table ix the studied techniques ranked according to the double scott knott test on the promise corpus .
overall rankclassification techniquemedian rankaverage rankstandard deviation 1rsub j48 sl rsub sl bag sl lmt rf sl rf j48 bag lmt rsub lmt and rf lmt1.
.
.
2rbfs bag j48 ad sl knn rf nb ad lmt nb rsub nb and bag nb2.
.
.
3ripper em j48 ad nb bag smo ad j48 ad smo and k means5.
.
.
4rf smo ridor smo and rsub smo6.
.
.
complementing our results of the cleaned nasa study we again find four statistically distinct ranks of classification techniques which leads us to conclude that the used classification technique has a statistically significant impact on the performance of defect prediction models trained using the datasets of the promise corpus as well.
vi.
t hreats to validity in this section we discuss the threats to the validity of our case study.
a. construct validity the datasets that we analyze are part of two corpora i.e.
nasa and promise which each provide a different set of predictor metrics.
since the metrics vary this is another point of variation between the studied corpora which may explain some of the differences that we observe when drawing comparisons across the corpora.
nevertheless our main findings still hold i.e.
there are statistically significant differences between the performance of defect prediction models trained using various classification techniques within the cleaned nasa dataset and the promise one as well.
b. internal validity the tuning of the parameters of the k means k and knn k techniques that we used may influence our results.
for example this tuning may impact the performance of models trained using these classification techniques when compared to default parameters.
to combat this bias we experimented with several configurations k and and k and that achieved similar results.
nonetheless a more carefully controlled experiment of the influence of configuration parameters is needed.
c. external validity we performed our experiments on datasets which may threaten the generalization of the results of our study.
however these datasets are part of the popular nasa and promisecorpora which has been used by several studies that benchmark defect prediction models in the past .
the consistency of our results are based on two studied data corpora.
the results may vary in other corpora that we have not analyzed.
on the other hand the two corpora contain software systems that are developed in both proprietary and open source paradigms which helps to counter potential bias.
nonetheless additional replication studies may prove fruitful.
vii.
r elated work in order to train defect prediction models defective software modules are predicted using software metrics such as object oriented metrics e.g.
chidamber and kemerer ck metrics process metrics e.g.
number of changes recent activity product metrics e.g.
lines of code and complexity historical metrics e.g.
code change and structural metrics e.g.
cyclomatic complexity or coupling .
in this paper we used several types of metrics based on their availability in the studied corpora.
a promising direction for future research would be to explore the impact that different metrics have on our findings e.g.
perhaps our findings are sensitive to the type of metrics that are available for training our models .
there exists several evaluation metrics to measure the performance of classification models such as accuracy auc error sum median error error variance and correlation .
in this paper we use the auc because it is threshold independent.
in addition to lessmann et al.
there have been several studies that compare the performance of classification techniques across projects in search of the top performing classification techniques.
panichella et al.
conducted a study to compare the performance of statistical neural networks and decision trees classification techniques using the promise corpus.
bettenburg et al.
conducted a study using the promise corpus in order to compare the performance of classification models that are trained using clustering and statistical techniques reporting that clustering techniques tend to perform better than models that are trained using statistical techniques.
however lemon found contradictory results to bettenburg et al.
when building classification models on nasa datasets using clustering and statistical techniques.
the key differences between such prior work and this paper are the following to the best of our knowledge this is the first study to replicate a prior analysis using the cleaned nasa corpus.
most prior studies use either the nasa or the promise corpus they rarely combine both corpora together for a single analysis hence the generalization of prior results is hindered.
most prior studies use a post hoc statistical tests to compare techniques.
however the results of such tests are not as clear cut as the scott knott tests which produces non overlapping ranks.
to the best of our knowledge this study is the first to extrapolate project specific analyses to a corpus level byranking classification techniques at the project level and lifting it using a double scott knott test.
to the best of our knowledge this is the first study to use several recent classification techniques e.g.
lmt while ensuring that we capture techniques from a variety of families.
viii.
c onclusions a previous study on the nasa corpus by lessmann et al.
showed that the performance of defect prediction models that are trained using various classification techniques do not vary much implying that the use of any technique should be sufficient when predicting defects.
in this paper we revisit this very impactful finding since it has influenced a large amount of follow up literature after its publication many researchers opt not to investigate the performance of various classification techniques on their datasets.
our replication of lessmann et al.
yields the same results even when using a more advanced statistical ranking technique i.e.
the scott knott test .
however when using a cleaned version of the nasa corpus and when using another corpus the promise dataset which provides different metrics than the nasa corpus does we found that the results differ from those of lessmann et al.
i.e.
classification techniques produce defect prediction models with significantly different performance.
like any empirical study our study has several limitations cf.section vi .
however we would like to emphasize that we do not seek to claim the generalization of our results.
instead the key message of our study is that there are datasets where there are statistically significant differences between the performance of models that are trained using various classification techniques.
hence we recommend that software engineering researchers experiment with the various available techniques instead of relying on specific techniques assuming that other techniques are not likely to lead to statistically significant improvements in their reported results.
given the ubiquity of advanced techniques in commonly used analysis toolkits e.g.
randweka we believe that our suggestion is a rather simple and low cost suggestion to follow.