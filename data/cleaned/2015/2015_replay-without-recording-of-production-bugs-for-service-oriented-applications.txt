replay without recording of production bugs for service oriented applications nipun arora dropbox new york ny usa nipun dropbox.comjonathan bell george mason university fairfax va usa bellj gmu.edufranjo ivan i google new york ny usa ivancic google.com gail kaiser columbia university new york ny usa kaiser cs.columbia.edubaishakhi ray columbia university new york ny usa rayb cs.columbia.edu abstract short time to localize and time to fix for production bugs is extremely important for any 24x7 service oriented application soa .
debuggingbuggybehaviorindeployedapplicationsishard asitrequirescarefulreproductionofasimilarenvironmentandworkload.priorapproachesforautomaticallyreproducingproductionfailuresdonotscaletolargesoasystems.ourkeyinsightisthatformany failures in soa systems e.g.
many semantic and performance bugs a failure can automatically be reproduced solely by relaying network packets to replicas of suspect services an insight that we validatedthroughamanualstudyof16realbugsacrossfivedifferent systems.
this paper presents parikshan an application monitoring framework that leverages user space virtualization and network proxytechnologiestoprovideasandbox debug environment.in this debug environment developers are free to attach debuggers andanalysistoolswithoutimpactingperformanceorcorrectnessofthe production environment.
in comparison to existing monitoring solutionsthatcanslowdownproductionapplications parikshan allows application monitoring at significantly lower overhead.
ccs concepts software and its engineering software testing and debugging keywords fault reproduction live debugging acm reference format nipun arora jonathan bell franjo ivan i gail kaiser and baishakhi ray.
.
replay without recording of production bugs for service oriented applications.
in proceedings of the 33rd acm ieee international conference on automated software engineering ase september montpellier france.
acm new york ny usa 12pages.https permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
ase september montpellier france copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
introduction modern daydevicesrelyoninteractiveandresponsiveapplications that provide a rich interface to end users.
behind the scenes of theseapplicationsareoftenseveralservice orientedapplications working in concert to provide the final service.
such services include storage compute queuing synchronization and applicationlayerfunctionality.applicationsfollowingsuchservice oriented architectures soa requiretheorchestrationofavarietyofcomponents.rapidresolutionofincident error alert management in soa isextremelyimportant asfailureofoneservice canleadtocascadingfailureofthewholesystem.thelargescale ofsuchsystemsmeansthatanydowntimehassignificantimpact on the user experience a product s image and a company s brand and potentially revenue .
debuggingproductionbugsinsoaisnotoriouslychallengingbecause itrequirescarefulreproductionofasimilarlyorchestratedenvironmentandworkloadsothatdeveloperscanidentifytheroot cause and bugsneedtoberesolvedasaptoensureminimum downtime.
worse still a single observed failure in one component might in fact be due to several latent bugs in other components.
thus localizing a single bug might require understanding complex interactionsacrossmultiplecomponentsrunningondifferenthosts.
debuggingbecomesevenmorefrustratingfor non crashing bugs suchasperformancebugs semanticbugs andresourceleaks which tendtoariseduetoaccumulatedstate makingthemparticularly complicated to reproduce in testing environments.
most debugging techniques are not suitable for on the fly debugging of soa production bugs.
for instance approaches likerecord and replay r r typically collect execution trace infor mation from the production environment and use that trace to reproduce the bug in a testing environment where developers can usetraditionaldebuggingtools.whiler rcanbeveryeffectivein reproducing a bug if sufficient execution traces can be captured to allowtheentireapplicationexecutiontobefaithfullyreproduced inadebuggingenvironment thiscancausesignificant performanceoverhead.despitemuchworktowardsoptimizingthetrace data captured overheads imposed by such tracing can still be unacceptable for soa production use the overhead can balloon up to 10x overhead .
in contrast some monitoring systems capture only very minimal high level information for instance collecting existing log authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france nipun arora jonathan bell franjo ivan i gail kaiser and baishakhi ray information and from it building a model of the system and its irregularities .
while these systems impose almost no overhead on the production system being debugged since they simplycollectloginformationalreadybeingcollected orhavelightweight monitoring they can not automatically reproduce all bugs and hence may be limited in their utility.
thus for on the fly debugging of soa production bugs we require a solution which allows developers to observe instrument and debug the system components in parallel with the production.
inthispaper weproposealivedebuggingenvironmentforsoa whichallowsdebuggersafreereigntodebug withoutimpacting theuser facingapplication.bymanuallystudying220realworld soa production bugs we observe that it is possible to successfully replay a soa bug solely by capturing network transmissions.
for these bugs conventional r r capture of very low level sources of non determinism e.g.
threadscheduling generalsystemcalls is unnecessary to automatically reproduce the buggy execution.
guided by this insight we have developed parikshan1 a live debugging architecture that supports online debugging of production soa applications withoutdegrading access to the app during debugging.
our approachleverages technologies commonly used insoasystems suchaslightweightcontainers toautomatically createsandboxeddebuggingenvironmentsthatmirrortheirproduction environments.
each replica is kept isolated so developers can modifyitwithoutfearofimpactingtheproductionsystem.
parikshanreplicates all network inputs flowing to the corresponding productioncontainer bufferingandfeedingthem withoutblocking to the debugging container.
within the debug environment developersarefreetouseheavyweightinstrumentation thatwould not be suitable in a production environment to diagnose the fault.
this approach could be used offline recording rather than replicatingtrafficandstoringtheclonedreplicasfor later but parikshan focuses on helping developers debug faults online as they occur in production systems.
the key benefits of parikshan are verylowoverheadinproduction parikshandoes imposeashort pausewhendebugenvironmentsarelaunched butthendevelopersare free to use very high overhead debugging tools e.g.
gdb in the debugenvironment yetthe productionenvironment continuesto service requests at near native speed.captures large scale context parikshan captures the context of largescale longrunningproductionsystemsbycloningservices in situ creating sandbox environments.
capturing such state isextremely difficult in conventional testing environments as they wouldneedlong runningtestinputsequencesandlargetest clusters.
weevaluate parikshan bysuccessfullyreplaying16real world bugs finding that parikshan imposed very low overhead.
manually reproducing real world bugs is a very time intensive process and hence to lend additional validity to this evaluation of bugs we categorized additional bugs from three applications finding that most were similar in nature to the that we reproduced.
motivation to better understand what kinds of bugs occur in production soa systemsandhowtheycanbestbedebugged westudied220realworldproductionbugsfromthreesoaapplications apache mysql 1parikshan is thesanskritword for testing.table survey and classification of bugs category apache mysql hdfs total performance 19semantic 173concurrency resource leak total and hdfs.
we searched issue trackers for each project ignoring bugsinnon productioncomponents.wealsofilteredbugreports that were feature requests or did not include a triggering test our goalwastofocusonlyonbugsthataroseduringproductionscenarios.
to understand the nature of these bugs we classified them e.g.basedondescriptionandfix intothefollowingcategories performance semantic concurrency and resource leak as shown in table1.completedetailsonhowweselectedandcategorizedthese bugs alongwithalistingofthebugsthemselves areavailablein the full version of this work .
one of the key insights from this study is that most of the bugs we examined are deterministic in nature everything but concurrencybugs andinfact mostaresemanticbugs .for manyofthem theapplicationbehaviorisincorrect e.g.itprovides the wrong output to the user but there is no error or warning generated in the system log s .
to trigger these bugs we only need tocapture thestateofthe systemandthe inputthatresultsin the bug and notallnon deterministicevents e.g.threadscheduling .
wecapturethestateofthesystemthroughlivecloning replicating the entire state of each production container that is relevant to the bug.
to capture the inputs to the system that result in the bug we replicate all network inputs that enter the production containers.
.
sample scenario consider the complex multi tier service oriented system shown infigure whichcontainsseveralinteractingserviceseachrunning in its own container segregating components in separatecontainers isgenerally considereda best practice .
operators might observe unusual memory usage in the glassfish application server causing error logs to be generated in the nginx web server.
operatorssurmisethereisapotentialmemoryleak allocationproblem.
however with monitoring restricted to avoid performance cloned test contain ers network duplicationdebug outputtier tier tier tier tier tiers n user observes error creates sandbox production system online debug system figure1 workflowof parikshan inamulti tiersystemwith interacting services.
when the administrator observes errors in two of the tiers she can create a sandboxed debugenvironment.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
replay without recording of production bugs for service oriented applications ase september montpellier france asynchronous forwarderdownstream components buffer managerproduction container debug containerpass through forwarder upstream componentspass through forwarder asynchronous forwardernetwork duplicator network aggregator buffer manager dummy reader dummy readerlive clonedclients user requests etc.storage datab ase services etc.
process p1process p2 process p3process p4process p1 process p2 process p3process p4 legend duplicatorlegend aggregatorclone manager figure2 highlevelarchitectureof parikshan showingthemaincomponents networkduplicator networkaggregator and cloningmanager.thereplica debugcontainer iskeptinsyncwiththemaster productioncontainer throughnetwork levelrecord and replay.
in our evaluation we found that this light weight procedure was sufficient to reproduce many real bugs.
penalties on production they can only go so far.
extensive trace collectionintheproductionenvironmentforreliabler rdebugging is also not feasible as that will hurt the system s performance.
thus trouble tickets are typically generated for such problems to be debugged offline.
however repr oducing similar scenario offline involving so many soa applications is challenging.
weobservethatitispossibletoreplaythebugfaithfully without hurting the performance of the system by simply cloning the potentiallybuggycontainersandthensendingthesamenetwork inputsastheproductioncontainerstothesereplicas.wedesigna faultreproductionframework parikshan basedonthisobservation.
based on the erroneous behavior the administrators can choose the nginx and glassfish containers for cloning and debugging askingparikshan tocreatethenew nginx debug andglassfish debug containers.
parikshan s network duplication mechanism ensures thatthedebugreplicasreceivethesameinputsastheproduction containers and that the production containers continue service withoutfurtherinterruption.oncethedebugenvironmentiscreated parikshan canbeusedwithanyexistingautomatedormanual debugging tools that developers may wish to use.
this separation ofproductionanddebuggingenvironmentallowsthedevelopers touseheavierdynamicinstrumentationfordeeperdiagnosisinthedebug containers without fearof disrupting production.
since eachreplicaisclonedfromitsoriginal buggy productioncontainer itexhibitsthesamepersistentmemoryleaksand orlogicalerrors.note thatweprimarilyenvision parikshan beingappliedtoreproduce application bugs and not to reproduce security attacks.
debugcontainerscanbecreatedandrecreatedatanytime either at the start of execution or at any point during execution allowing post facto analysis of the bugs.
within debug replicas analysistools that slow down the buggy execution may be used without impacting production performance.
hence developers can use any oftheirpreferreddebuggingapproachesinthesereplicasinorder to determine the cause of the failure.
design sinceparikshan isbuiltforon the flydebuggingofsoaproduction bugs its design is guided by the following principles.
real time insights observing application behavior as a bug presents itself will allow for quick insights and shorter timetodebug.developersshouldbeabletomonitorsystemstatus as they debug.
sanityandcorrectness ifdebuggingistobedoneinarunningapplicationwithrealusers itshouldbedonewithout impactingtheoutcomeoftheprogram.theframeworkmust ensurethatanychangestotheapplication sstateorto the environment does not impact the user facing production application.
language application agnostic the mechanisms presented should be applicable to any language and any service orientedapplication ourscopeislimitedtosoaarchitectures .
performanceimpact theenduserofasystemthatisbeing debugged should not observe any noticeable performance degradation.debuggingmustbeunobtrusivetotheenduser bothintermsoffunctionalityandanyconfigurationorsetup in addition to performance.
service interruption since we are focusing our efforts on service oriented systems any solution should ensure that thereisnoimpactontheservice andtheuserfacingservice should not be interrupted.
figure2shows the architecture of parikshan when applied to a single mid tier application server.
parikshan consists of modules clone manager manages live cloning between the production containersandthedebugreplicas.livecloningallowsdevelopers todecidetocreatenewdebugenvironmentsatanytimewhilean applicationisrunning.
networkduplicator managesnetwork trafficduplicationfromdownstreamserverstoboththeproductionanddebugcontainers.
networkaggregator managesnetwork communicationfromtheproductionanddebugcontainerstoupstreamservers.thenetworkduplicatoralsoperformstheimportant task of ensuring that the production and debug container executions do not diverge.
the duplicator and aggregator can be used to target multiple connected tiers of a system by duplicating traffic at thebeginningandendofaworkflow.
parikshan candynamically detectwhichportsanapplicationusesandpromptthedeveloperto choose if traffic on each port should be aggregated or duplicated.
.
clone manager parikshan uses live cloning a variant of live migration tospawndebugcontainersthatexactlymirrorthecorresponding production services without disconnecting any clients or stopping authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france nipun arora jonathan bell franjo ivan i gail kaiser and baishakhi ray any processes incurring a negligible suspend time.
the challenge here is to manage two containers with the same identities in the networkandapplicationdomain.thisisimportantastheoperating systemandtheapplicationprocessesrunninginitmaybeconfigured with ip addresses that cannot be changed on the fly.
hence the same network identifier should map to two separate addresses and enable communication with no problems or slowdowns.
parikshan supports two high level modes of live cloning internal cloning inthismode weallocatetheproductionand debugcontainerstothesamephysicalhost.thismodetakesless time to perform the initial clone since the container does not need tobetransferredoverthenetwork andmaybemorecost effective since it does not require additional machines.
however co hosting the debug and production containers could potentially decrease performanceoftheproductioncontainerduetoresourcecontention.networkidentitiesinthismodearemanagedbyencapsulatingeach container in separate network namespaces .
this allows both containers to have the same ip address with different interfaces.
theduplicatoristhenabletocommunicatetoboththesecontainers with no networking conflict.
external cloning inthismode weprovisionanexternalserver as the host of our debug container this server can host more than one debug container .
while this mechanism can have a higher overheadintermsofsuspendtimeandrequiresprovisioninganadditional host the advantage of this mechanism is that once cloned the debug container is totally separate and will not impact theperformance of the production container.
network identities inexternal mode are managed using nat network address trans lation in both host machines.
hence both containers can have thesameaddresswithoutanyconflict.currently weassumethat each production container has at most a single debug replica inthefuturewewillconsidersupportingmultiplereplicasforeach production container which might make it easier for developers to apply multiple debugging techniques simultaneously.
the suspend time of cloning depends on the operations happeningbetweenstep2andstep4 thefirstandthesecondrsync moreoperationswillresultinmoremodifiedpagesofmemory impacting the amount of memory that needs to be later copied.
this suspend time can be viewed as an amortized cost in lieu of instrumentation overhead.
we evaluate the performance of live cloning in .
.
.
network duplicator and aggregator oncethedebugcontainerisprovisioned parikshan keepsthedebug container in sync with the production container by duplicating networktrafficintothecontainer.thenetworkproxyduplicator andaggregatorarecomposedofthefollowinginternalcomponents synchronouspassthrough thesynchronouspassthroughtakes input from a source port and forwards it to a destination port.
thepassthroughisusedforcommunicationfromtheproduction container out to other components which are not duplicated .
asynchronousforwarder theasynchronousforwardertakes input from a source port and forwards it to both a destination portandtoaninternalbuffer.forwardingtothebufferisdonein a non blocking manner so as to not delay network forwarding.
buffermanager managesafifoqueuefordatakeptinternally in the proxy for the debug container.
it records the incoming data and forwards it to a destination port.
dummy reader this is a standalone daemon that reads and drops packets from a source port.
proxy network duplicator all requests inbound to the productioncontainerareduplicatedandforwardedtothedebugcontainer.
asimplenetworkproxyorportmirrorwouldduplicatealltraffic from the production container to the debug container but would not be able tocope with the different execution speeds of the two containers and would not be able to correctly filter responses from thedebugcontainerbacktotheclient whichshouldonlyreceive responses from the production container .
our solution is a customized tcp level proxy.
this proxy duplicatesnetworktraffictothedebugcontainerwhilemaintaining thetcpsessionandstatewiththeproductioncontainer.sinceit works at the tcp ip layer applications are completely oblivious to it.
figure 2shows how our proxy works each incoming connection isforwarded toboth theproduction container andthe debug container.
this is a multi process job involving parallel processes p1 p4 in p1 the asynchronous forwarder sends data from client totheproductionservice whilesimultaneouslysendingittothe buffermanagerinanon blockingsend.thisensuresthatthereisnodelayintheflowtotheproductioncontainerbecauseofslow down in the debug container.
in p2 the pass through forwarder readsdata from the production and sends it to the client downstreamcomponent .
process p3 then sends data from buffer manager to thedebugcontainer andprocessp4usesadummyreadertoread from the production container and drops all the packets.
the above strategyallows for non blocking packetforwarding and enables a key feature of parikshan whereby it avoids slowdownsinthedebugcontainertoimpacttheproductioncontainer using an in memory buffer discussed further in .
.
proxynetworkaggregator whilethenetworkduplicatorduplicatesincomingrequests thenetworkaggregatormanagesincoming responses for requests sent from the debug container.
in addition to dropping duplicate responses to clients the network aggregatormust also drop duplicate requests to backend servers.
for instance processing a request in a mid tier server might require insertingor deleting data from a backend database since both the productionanddebugcontainerswillprocessthisrequest therewillbe duplicate requests sent to these backend services leading to an inconsistent state.
the proxy aggregator module stubs the requests fromaduplicatedebugcontainerbyreplayingtheresponsessent totheproductioncontainertothedebugcontaineranddropping all packets sent from it to upstream clients.
asshowninfigure whenanincomingrequestcomestothe aggregator it first checks if the connection is from the production containerordebugcontainer.inprocessp1 theaggregatorforwards the packets to the upstream component using the pass through forwarder.
in p2 the asynchronous forwarder sends the responses from the upstream component to the production container and sends theresponse in anon blockingmanner to theinternal queue in the buffer manager.
once again this ensures no slow down in theresponsessenttotheproductioncontainer.thebuffermanager thenforwardstheresponsestothedebugcontainer processp3 .
finally in process p4 a dummy reader reads all the responses from the debug container and discards them.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
replay without recording of production bugs for service oriented applications ase september montpellier france weassumethattheproductionandthedebugcontainerarein the same state and are sending the same requests.
hence sending the corresponding responses from the fifo queue instead of the backend ensures a all communications to and from the debug container are isolated from the rest of the network b the debug container gets a logical response for all it s outgoing requests making forward progress possible and c .
similar to the proxy duplicator thecommunicationsfromtheproxytointernalbufferis non blocking to ensure no overhead on the production container.
.
debug window parikshan s asynchronous forwarder uses an internal buffer to ensure that incoming requests proceed directly to the production container without any latency regardless of the speed at whichthe debug replica processes requests.
the incoming request ratetothebufferisdependentontheuser andislimitedbyhowfast the production container manages the requests i.e.
the production containeristherate limiter .theoutgoingratefromthebufferis dependent on how fast the debug container processes the requests.
as instrumentation overhead increases the incoming rate of requests may eventually exceed the transaction processing rate in the debug container leading to a buffer overflow.
we call the time perioduntilbufferoverflowhappensthe debugwindow.oncethe bufferhasoverflowed thedebugcontainermaybeoutofsyncwithproduction andafreshdebugcontainerwouldneedtobelaunched.
the debug window size depends on the size of the buffer the incoming request rate the overhead of any debugging activitiesand the application behavior in particular how it launches tcp connections.
parikshan generatesapipebufferforeachtcpconnect call and the number of pipes are limited to the maximum number of connections allowed in the application.
hence buffer overflows happen only if the requests being sent in the same connectionoverflow the queue.
for webservers and application servers the debugging window size is generally not a problem as each request isanew connection.
thisenables parikshan totoleratesignificant instrumentation overhead without a buffer overflow.
on the other hand database and other session based services usually have small requestsizes butmultiplerequestscanbesentinonesessionwhich isinitiatedbyauser.insuchcases foraserverreceivingaheavy workload the number of calls in a single session may eventually have a cumulative effect and cause overflows.
to further increase the debug window parikshan could load balancedebugginginstrumentationoverheadacrossmultipledebug containers each of which can get a duplicate copy of the incoming data.
for instance debug container could have of the instrumentation andthe restcould occurin debugcontainer .such a strategy would significantly reduce the chance of a buffer overflow in cases where heavy instrumentation is needed.
.
divergence checking itispossiblethatnon deterministicbehavior discussedin in the containers or instrumentation could cause the production and debugcontainertodivergeovertime.tounderstandandcapture thisdivergence wecomparethecorrespondingnetworkoutputs received by the proxy providing a black box mechanism to check thefidelityofthereplicabasedonitscommunicationwithexternal components.
we use a hash of each data packet which is collectedandstoredinmemoryforthedurationthateachpacket sconnection is active.
the degree of acceptable divergence is dependent on the application behavior and the operator s wishes.
for example an application that includes timestamps in each of its messages i.e.
is expectedtohavesomenon determinism couldperhapsbeexpected to have a much higher degree of acceptable divergence than an applicationthatshouldnormallybereturningdeterministicresults.
.
implementation parikshan ispubliclyavailableunderthemitopensourcelicense ongithub .theclonemanagerandthelivecloningutilityare built on top of the user space container virtualization software openvz .parikshan extendsvzctl4.
live migration facility to provide support for online cloning.
the network isolationfortheproductioncontainerwasdoneusinglinuxnetwork namespaces and nat.
while parikshan is based on lightweightcontainers webelieveitcanalsobeappliedtotraditional virtualization software where live migration has been further optimized .
thenetworkproxyduplicatorandthenetworkaggregatorare implemented in c c .
the forwarding in the proxy is done by forkingoffmultipleprocesseseachhandlingonesend orreceive a connection in a loop from a source port to a destination port.
data from processes handling communication with the production container is transferred to those handling communication with the debug containers using linux pipes .
pipe buffer size is a configurable input based on user specifications.
evaluation to evaluated parikshan through the following research questions rq1 howlongdoesittaketocreatealivecloneofaproduction container and what is its impact on the performance of the production container?rq2 what is the impact of parikshan on the throughput and latency of the production application?rq3 whatisthesizeofthedebuggingwindow andhowdoesit depend on resource constraints?rq4 canparikshan successfully reproduce real bugs?
wecompared parikshan stwocloningmodes internalandexternal .
our internal cloning mode was evaluated using two identical vm swithaninteli7cpu with4cores and16gbrameachinthe samephysicalhost oneeachforproductionanddebugcontainers .
weevaluatedtheexternalcloningmodeontwoidenticalhostnodes withintelcore2duoprocessor 8gbofram.allevaluationswere performed on centos .
.
apart from cloning performance evaluationinrq1 otherevaluationsuseexternalmode i.e.different identical machines for debug and production containers .
.
rq1 live cloning performance as explained in a short suspend time during live cloning is necessary to ensure that both containers are in the exact same system state.
we measure this overhead on both real and synthetic workloads andseparatethesuspendtimeintoitsfourcomponents suspend dump time taken to pause and dump the container pcopyaftersuspend timerequiredtocompletersyncoperation copy dump file time taken to copy an initial dump file and authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france nipun arora jonathan bell franjo ivan i gail kaiser and baishakhi ray basicapache thttpd tradebeanstradesoappetstore0510time seconds suspend dump pcopy after suspend copy dump file undump resume figure3 suspendtimeforlivecloning whenrunningarepresentativebenchmark.here basic indicatesbaselinewith out any services running in the container.
undump resume time taken to resume the containers.
we used both micro and macro benchmarks to evaluate live cloning.
real worldapplicationsandworkloads first weevaluated parikshan ssuspendtimeusingfivewell knownapplicationsworkloads.
we ran the httperf benchmark on apache and thttpdto compute max throughput of the web servers by sending a large number of concurrent requests.
tradebeansand tradesoap are realistic workloads running a multi tier stock trading application andarepartofthedacapo benchmark daytrader application.
petstore is also a well known jee reference application.
we deployed petstore in a tier system with jboss mysql and apacheservers andclonedtheapp server.theinputworkloadwas a random set of transactions which were repeated for the duration of the cloning process.
as shown in figure for apache and thttpd the container suspend time ranged between seconds.
however in more memory intensiveapplicationserverssuchaspetstoreanddaytrader the total suspend time was higher seconds .
nevertheless wedid not experience any timeouts or errors for the requests in the workload .
we felt that these relatively fast temporary app suspensions were a reasonable price to pay to launch an otherwise overhead free debug replica.microbenchmark themainfactorthatimpactssuspendtimeis thenumberof dirtypages recentlymodified inthesuspendphase that have not been copied over in the pre copy rsync operation see .
.hence tofurthercharacterizethesuspendtimeimposed bythecloningphaseof parikshan wecreatedamicrobenchmark thatcontrolsthisvariable.weusedthe fioutility togradually increasethenumberofi ooperationswhile ranfiotoreadandwritesofrandomvalueswithacontrolledi o 2incaseofpacketdrops requestsareresentbothatthetcplayer andtheapplication layer.
this slows down the requests for the user but does not drop them10310451015 i o ops kbps time secs read internalmode read externalmode write internalmode write externalmode figure live cloning suspend time with increasing amounts of i o operations bandwidth.
we ensured that the i o workload being processed by fiowas long enough to last through the cloning process.
asshowninfigure readoperationshaveamuchsmallerimpact on suspend time of live cloning compared to write operations.
this can be attributed to the increase of dirty pages in write operations whereasforread thediskimageremainslargelythesame.
theinternalmodeismuchfasterthantheexternalmode asboth theproductionanddebugcontainerarehostedinthesamephysical device.
for higher i o operations with a large amount of dirty pages network bandwidth becomes a bottleneck leading to longer suspendtimes.overallinourexperiments theinternalmodeisabletomanagewriteoperationupto10mbps withatotalsuspend time of approx seconds whereas the external mode is only able to manage up to mbps fora5s ecsuspend time.
to answer rq1 live cloning introduces a short suspend timeintheproductioncontainerdependentontheworkload.writeintensiveworkloadswillleadtolongersuspend times while read intensive workloads will take much less.
suspendtimesinrealworkloadonreal worldsystemsvary from2 3secondsforwebserverworkloadsto10 11seconds forapplication databaseserverworkloads.comparedto external mode internal mode hada shorter suspend time.
a production quality implementation could reduce suspend time further by rate limiting incoming requests in theproxy orusing copy on writemechanismsandfaster shared file system storage devices already available in several existing live migration solutions.
.
rq2 impact on production performance we measured the impact of parikshan on a running production application after the debug environment was created in terms of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
replay without recording of production bugs for service oriented applications ase september montpellier france table first four latencies of get post requests from wikipedia traces.
the third and fourth column show overhead of proxy compared to native and overhead of duplication compared to proxy mode native proxy duplicationproxy overheadduplication overhead .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
throughputandlatency.tounderstandtheimpactofduplication on network throughput we ran a microbenchmark using iperf inthreedifferentmodes native justtheclientandserver proxy clientcommunicatestoserverthroughaproxy andduplication clientcommunicatestooriginalserver andaclonevia parikshan s duplicator .
we observed that while the native and proxy com munication had no discernible difference with both transferring at941mb s theduplicationmodewasonanaverage0.
slower thantheothertwo.webelievethisdifferenceisnegligibleformost practicalapplicationsandwillnotimpactapplicationend to end performance.
to measure the impact on latency we created a scaled down versionofwikipedia mediawiki wherewepopulateddata from data dumps available through wikibench .
we used a sampleworkloadofrequestsfrom2008 andcomparedthelatenciesofabout500httprequestsinthesamethreedeployments native proxy and duplicate .
table 2shows a snapshot of such requests andtheirlatenciesandoverheadsindifferentmodes.wefoundthat theproxywasgenerallyslowerthanthenativeconnection with the slowdown ranging from .
more importantly we found that whencomparingthelatenciesintheduplicationmodetoourproxy mode the overhead was negligible due to caching .
these experiments are described in greater detail in the full version of this work .
to answer rq2 we found that duplication of traffic has minimalimpact onthroughput .
andno discernible impact on network latency.
.
rq3 debug window size the network level proxies are responsible for buffering communicationto fromthedebugcontainer s allowingtheproduction application to operate without slowing down to account for any overheadsinthedebugapplication.hence thesizeofthisbufferdirectlyimpactshowfarthedebugenvironmentisabletofallbehind production.
we refer to this time window where debugging can call behind production as the debug window and evaluated how differentsizebuffersimpacttheabilityofdeveloperstodebugin both real world experiments and also in controlled simulations.experimental results to evaluate the approximate size of the debugwindow wesentrequeststobothaproductionanddebug mysqlcontainerviaournetworkduplicator.eachworkloadran for about minutes select from table queries with varying request workloads.
we also profiled the server and foundtable approximate debug window sizes for a mysql request workload input rate debug window pipe size slowdown bps rq s .8x bps rq s sec 3x bps rq s sec 3x pois.
rq s sec 8x pois.
rq s sec 5x pois.
7r q s .2x pois.
rq s sec .2x thatisabletoprocessamaxof27req s3inasingleuserconnect session.foreachofourexperiments wevarythebuffersizestoget anideaofdebugwindow.wegeneratedaslowdownbymodeling the time taken by mysql to process requests req s or 17req s and putting an approximate sleep in the request handler.
initially we created a connection and sent requests at the maximumrequestratetheserverwasabletohandle 27req s .wefound that for overheads up to .8x approx we experienced no buffer overflows.
for higher overheads the debug window decreased primarily dependent on buffer size request size and slowdown.
next we mimic user behavior to generate a realistic workload.
wesendpacketsusingapoissonprocesswithanaveragerequest rateof17requestspersecondtoourproxy.thisvariestheinterrequestarrivaltime andletsthedebugcontainercatchupwiththe productioncontainerduringidleperiodsbetweenrequestbursts.
we observed that compared to earlier experiments there was more slack in the system allowing it to tolerate a much higher overhead .2x with no buffer overflows.
simulation results in our next set of experiments we simulate packet arrival and service processing for a buffered queue in soa applications.
we use a discrete event simulation based on an mm1 queue whichisaclassicqueuingmodelbasedonkendall snotation andisoftenusedtomodelsoaapplicationswithasingle buffer basedqueue.
essentially weare sendingand processingrequests based on a poisson distribution with a finite buffer capacity.
in our simulations see figure we kept a constant buffer size of 64gb anditerativelyincreasedtheoverheadofinstrumentation thereby decreasing the service processing time.
each series set of experiments starts with an arrival rate approximately times less thantheserviceprocessingtime.thismeansthatat400 overhead the system would be running at full capacity for stable systems soa applications generally operate at much less than system capacity .
each simulation instance was run for seconds .7hours .wegraduallyincreasedtheinstrumentationby10 each time and observed the hitting time of the buffer time it takes for the buffer to overflow for the first time .
as shown there is no bufferoverflowinanyofthesimulationsuntiltheoverheadreaches around420 beyondthisthedebugwindowdecreasesexponentially.sincebeyond400 overhead thesystemisover capacity the queue will start filling up fairly quickly.
this clarifies the behaviorweobservedinourexperiments whereforloweroverheads 3not the same as bandwidth req s is the maximum rate of sequential requests mysql server is able to handle for a user session authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france nipun arora jonathan bell franjo ivan i gail kaiser and baishakhi ray .
.2x we did not observe any overflow but beyond a certain point we observed that the buffer would overflow fairly quickly.
alsoasshowninthesystem sincethebuffersizeissignificantly larger than the packet arrival rate it takes some time for the buffer to overflow several hours .
we believe that while most systems willrun significantlyunder capacity largebuffer sizescan ensure that our debug container may be able to handle short bursts in the workload.
however a system running continuously at capacity is unlikely to tolerate significant instrumentation overhead.
to answer rq3 we found that the debug container can stay in a stable state without any buffer overflows as long as the instrumentation does not cause the service times to becomemorethantherequestarrivalrate.furthermore a large buffer will allow handling of short bursts in the workload until the system returns back to a stable state.
thedebugwindowcanallowfor asignificantslowdown which means that many existing dynamic analysis techniques aswellasmostfine grainedtracing can be applied on the debug container without leading to an incorrect state.
.
rq4 reproducing real bugs one of our core insights is that for most soa systems production bugscanhencebetriggeredbynetworkreplayalone.tovalidate thisinsight weselectedsixteenreal worldbugs applied parikshan reproduced them in a production container and observed whether theywerealsosimultaneouslyreproducedinthereplica.foreachofthe sixteen bugs that we triggered in the production environments parikshan faithfully reproduced them in the replica.
weselectedourbugsfromthoseexaminedinpreviousstudies focusing on bugs that involved performance resourceleaks semantics concurrency and configuration.
we have further categorized these bugs whether they lead to a crash or not andif they can be deterministically reproduced.
table 4presents an overhead in percentage debug window in hours arrival 4mb s arrival 2mb s arrival 1mb s figure simulation results for debug window size buffer 64gb .
each series has a constant arrival rate.overview of the bugs that we studied.
a thorough description of each bug the steps that we took to reproduce it with parikshan and description of the debugging experience is available in the full version of this work in section .
.
semantic bugs we recreated semantic bugs from redis queuing system and cassandra a nosql database .
for instance redis 761isanintegeroverflowerror.thiserroristriggered when the client tries to insert and store a very large number.
this leads to an unmanaged exception which crashes the production system.otherssuchasredis 487resultedinexpiredkeysstillbeing retainedin redis because ofanunchecked edgecondition.
while this error does not lead to any exception or any error report inapplication logs it gives the user a wrong output.
in the case ofsuch logical errors the application keeps processing but the in ternal state can stay incorrect.
in our experiments we were able to clone the input of the production in the debug containers and easily replayed both these errors.
performance bugs wereplayed3mysqlproductionbugs.for example imysql reported that some of the user requests which were dealing with complex scripts chinese japanese were runningsignificantlyslowerthanothers.toevaluate parikshan we re created a two tier client server setup with the server container running a buggy mysql server and sent queries to the production container with complex scripts chinese .
these queries were asynchronously replicated in thedebug container.to furtherinvestigatethebug diagnosisprocess wealsoturnedonexecution tracing in the debug container using systemtap .
this gives us the added advantage of being able to profile and identify the functions responsible for the slow down without the tracing having any impact on production.resource leaks parikshan successfully reproduced resource leak bugs in redis.
let us take redis for instance here we had a redis master and slave set up for both production and debug container.
we then triggered the bug by running concurrent requests through the client which can trigger the memory leak.
the memoryleakwaseasilyreplayedinthedebugcontainerbyturning on debug tracing which showed a growing memory usage.concurrency bugs one of the most subtle bugs in production systemsiscausedduetoconcurrencyerrors.thesebugsarehard toreproduce astheyarenon deterministic andmayormaynot happeninagivenexecution.although parikshan cannotguarantee thereplayofconcurrencybugs inourexperimentwecouldsuccess fullyreproducealltheconcurrencybugsshowingintable .given thatthedebugcontainerisalive cloneoftheproductioncontainer and that it replicates the state of the production container entirely we believethat the chancesof replaying thenon deterministic concurrencybuginthedebugcontainerarequitehigh asevidentby our experiments.
additionally the debug container is a useful tracingutilitytotrackthreadlockandunlocksequences togetanidea of the concurrency bug.configuration bugs configuration errorsare usuallycausedby wrongly configured parameters i.e.
they are not bugs in the application butbugsintheinput configuration .thesebugsusuallygettriggered at scale or for certain edge cases making them extremelydifficulttocatch.asimpleexampleofsuchabugisredis here theslaveisunabletosyncwiththemaster.theconnectionwith theslavetimesoutandit sunabletosyncbecauseofthelargedata.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
replay without recording of production bugs for service oriented applications ase september montpellier france table list of real world production bugs studied with parikshan bug type bug id application symptom causedeterministiccrash steps to reproduce semanticredis redis .
.
keys command duplicate or omits keys yes no setkeystoexpire executespecific reqs cassandra cassandra .
.
missing columns from wide row yes no fetch columns from cassandra cassandra cassandra .
.
deleted columns become available after flush yes no insert delete and flush columns redis redis .
.
crash with large integer input yes yes query for input of large integer performancemysql mysql .
.
bug caused due to multiple calls in a loop yes no repeated insert into tablemysql mysql .
.
load data is slow in a partitioned table yes no createtablewithpartitionandload data mysql mysql .
.
calculation of hash values inefficient yes no mysql client select requests concurrencyapache httpd .
.
per child buffer management not thread safe no no continuous concurrent requests apache 21287httpd .
.
php .
.1dangling pointer due to atomicity violation no yes continuous concurrent request mysql mysql .
data race leading to crash no yes concurrent select queries mysql mysql .
race condition leading to out of order logging no no delete and insert requests mysql mysql .
race visible in logging no no concurrentflushlogandinsertrequests resource leakredis redis .
.
master slave not replicated correctly yes no setup replication push and pop some elements redis redis .
.
memory leak in master yes no concurrent key set requests configurationredis redis .
.
slave cannot sync with master yes no load a very large db hdfs hdfs .
.
create a directory in wrong location yes no create new directory while the bug is partially a semantic bug as it could potentially have checks and balances in the code.
the root cause itself is a lower output buffer limit.
once again it was easily replayed in our debugcontainersthattheslavewasnotsynced andinvestigated further by the debugger.
to answer rq4 we found that parikshan s approach of capturing the network traffic and replaying it in an offline environmentisefficienttoreproducerealproductionbugs.
applications of live debugging statisticaltesting onewell knowntechniquefordebuggingproductionapplicationsisstatisticaltesting.thisisachievedbyhaving predicate profiles from both successful and failing runs of a programandapplyingstatisticaltechniquestopinpointthecauseof the failure.
the core advantage of statistical testing is that the sampling frequency of the instrumentation can be decreased to reduce theinstrumentationoverhead.
however theinstrumentationfrequency for such testing to be successful needs to be statistically significant.
unfortunately overhead concerns in the production environment limit the frequency of instrumentation.
in parikshan the buffer utilization can be used to control the frequency of such statisticalinstrumentationinthedebugcontainer.thiswouldallow the user to utilize the slack available in the debug container for instrumentation toit s maximum without leading toan overflow.
thereby improving the efficiency of statistical testing.
record and replay record and replay techniques have been proposed to replay production site bugs.
however they are not yet used in practice as they can impose unacceptable overheads in theserviceprocessingtime.
parikshan replicascanbeusedtodorecordingatamuchfinergranularity higheroverhead allowingforeasy and fast replays offline.
similar to existing mechanisms the system can be replayed can then be used for offline debugging without imposing any recording overhead to the production container.
patchtesting bug fixes and patches to resolve errors often need to undergo testing in the offline environment and are not guar anteed to perform correctly.
patches can be made to the replica instead.thefixcanbetracedandobservedifitiscorrectlyworking before moving it to the production container.
this is similar in naturetoab tesing whichisappliedtofindifanewfixisuseful or works .
limitations and threats to validity there may be several threats to the validity of our findings.
for instance the bugsthat weselectedto studymay notbetruly representativeofabroadrangeofdifferentfaults.perhaps parikshan s low overhead network replay approach is less suitable to some classes of bugs.
to alleviate this concern we selected from several establishedbugcategories andfurther evaluated parikshan with bugsthathadalreadybeenstudiedinotherliterature toalleviatea risk of selection bias.
we further strengthened this by categorizing bug reports from three real world applications finding that mostweresemanticinnature andveryfewwerenon deterministic withsimilarcharacteristicstothe16thatwedemonstrated parikshancan reproduce.
therearealsoseveralunderlyinglimitationsandassumptions regarding parikshan s applicability non determinism non determinism can be attributed to three mainsources systemconfiguration applicationinput and authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france nipun arora jonathan bell franjo ivan i gail kaiser and baishakhi ray orderinginconcurrentthreads.livecloningoftheapplicationstate of a service container ensures that both the production and debug services are in the same system state and have the same configurationparametersforitselfandalldependencies.
parikshan s networkproxyensuresthatallinputsreceivedintheproduction container are also forwarded to the debug container.
however any non determinism from other sources e.g.
thread interleaving random numbers reliance on timing may limit parikshan s ability to faithfullyreproduceanexecution.whileourcurrentprototypedoes nothandle these webelieve thereare severalexistingtechniques that can be applied to tackle this problem in the context of live debugging such as deterministic scheduling .parikshan allows significanttracingofsynchronizationpoints oftenrequiredforconstraint solvers to go explore all synchronization orderings tofindconcurrencyerrors.wehavealsotriedtoalleviatethisproblemusingourdivergencechecker section .
.and aswasseenin our case studies even in the face of limited non determinism bugs will often still be triggered in the replica.
distributed services large scale distributed systems are often comprised of several interacting services such as storage ntp backup services controllers and resource managers.
parikshan can be used for multiple communicating services where any given servicemaybecloned turnedofforcontinueasis dependingonits nature.forexample storageservicessupportingareplicashould be cloned or turned off depending on debugging environment astheycouldpropagatechangesfromthedebugcontainertothe production containers.
services like ntp can be allowed to continuewithoutcloningastheirpublishsubscribebroadcastcannotbeimpactedbycloningofotherservicesanyway.furthermore instrumentationinsertedinareplicawillnotnecessarilyslowdownall itsservices e.g.
addinginstrumentationtoamysqlqueryhandler will not slow down file sharing or ntp services running in the same container.dataprivacy thisisalimitationthat parikshan shareswithmost existing record replay systems .parikshan clones incoming traffictherebyanydebuggerhavingaccesstothedebugmachine can potentially look at the user data depending on the kind of instrumentationtheyuse .currentlywedonotproposeanywayto address this issue inour system and leave it to thedebugger and thedataaccesscontrolpoliciesoftheirproductiondeployments as to how this can be addressed.
related work record and replay systems record and replay hasbeenanactiveareaofresearchintheacademic community for many years.
these systems offer highly faithful re execution but incur performance overhead on the production application for instance odr reports .6x slowdown and rr .2x scribe reduces to .
for server applications and for desktop applications.
parikshan avoids recording overhead entirely butitscloningsuspendtimemaybeviewedasanamortized cost in comparison to the overhead in record replay systems.
amongrecordandreplaysystems theworkweknowofmost closelyrelatedtooursisaftersight .aftersightrecordsaproductionsystemandreplaysitconcurrentlyalongsideinanothervm.while aftersight intends like parikshan to enable nearly real time diagnosisfacility aftersightsuffersfromrecordingoverheadinthe production vm.
the average slow down in aftersight is and canballoonupto2.6xinworst casescenarios.varan isannversionexecutionmonitorthatmanagessimultaneousexecutions ofaproductionapplication checkingamongthemfordivergence.
varaneffectivelyreplicatesapplications atthesystemcalllevel butparikshan s lower overhead mechanism does not impact the performance of the master production application.
parikshan also toleratesgreaterdivergencefromtheproductionexecution i.e.
a debugreplicacontinuestorunevenifitsexecutionpathismodifiedby the analysis instrumentation.
varan has recently been appliedto run multiple incompatible dynamic analyses in parallel i t would be interesting to use parikshan for this application as well.
real time diagnosis techniques chaos monkey injects faultsintoproductionsystemstoconductfault tolerancetesting randomly introducing time outs resource hogs etc.
this allows netflixtotesttherobustnessoftheirsystematscale andavoidlarge scalesystemcrashes.abtesting probabilisticallytestsupdates or beta releases on some percentage of users while letting the majorityoftheuserscontinueworkingwiththeoriginalapplication.
ab testing allows the developer to understand user response to anynewadditionstothesoftware whichcouldbeusedtodetect bugs as well as feature problems.
unlike parikshan this kind of approach directly impacts some users.
live migration cloning live migration of virtual machines facilitates fault management load balancing and low level system maintenance for the administrator.
most existing approaches use a pre copyapproach that copies the memory state over several iterations and then copies the process state.
this includes hypervisors such as vmware xen and kvm .
vm cloning on the other hand is usually done offline by taking a snapshot of a suspended shutdown vm and restarting it on another machine.cloning is helpful for scaling out applications using multiple in stances of the same server.
live cloning such as sun et al.
uses copy on write mechanisms to create a duplicate of the targetvmwithoutshutting downtheoriginal.otherwork uses live cloning to do cluster expansion.
conclusion future work parikshan isanovelframeworkforlivedebuggingofproduction soa applications.
we show that in combination with existing bug diagnosistechniques parikshan successfullylocalizesseveralrealworld production bugs that would be hard to find otherwise.
comparedtoexistingmonitoringsolutionsthatfocusonreducinginstrumentationoverhead ourapproachenablesminimalperformance slowdown while at the same time allowing heavyweight debugginginstrumentation.the parikshan prototypeispubliclyavailable under the mit open source license on github .