whencodecompletionfails casestudyon real world completions vincent j. hellendoorn department of computer science uc davis davis usa vhellendoorn ucdavis.edusebastian proksch department of informatics university of zurich z rich switzerland proskch ifi.uzh.chharald c. gall department of informatics university of zurich z rich switzerland gall ifi.uzh.chalberto bacchelli department of informatics university of zurich z rich switzerland bacchelli ifi.uzh.ch abstract code completion is commonly used by software developers and is integrated into all major ide s. good completion tools can not only save time and effort but may also help avoid incorrect api usage.
many proposed completion tools have shown promising results on synthetic benchmarks but these benchmarks make no claims about the realism of the completions they test.
this lack of grounding in real world data could hinder our scientific understanding of developer needs and of the efficacy of completion models.
this paper presents a case study on code completions that were applied by real developers which we study and contrast with artificial completions to inform future research and tools in this area.
we find that synthetic benchmarks misrepresent many aspects of real world completions tested completion tools were far less accurate on real world data.
worse on the few completions that consumed most of the developers time prediction accuracy was less than an effect that is invisible in synthetic benchmarks.
our findings have ramifications for future benchmarks tool design and real world efficacy benchmarks must account for completions that developers use most such as intra project apis models should be designed to be amenable to intra project data and real world developer trials are essential to quantifying performance on the least predictable completions which are both most time consuming and far more typical than artificial data suggests.
we publicly release our preprint and replication data and materials index t erms code completion benchmarks language models i. i ntroduction numerous studies have developed tools that can assist programmers in common but taxing tasks such as fault localization program patching type annotation and api recommendation .
one of the most clear cut ways in which tools can reduce the effort of software development for programmers is through code completion.
many tools that suggest plausible completions of the current token statement or even function have been proposed and are part of most integrated development environments ides .
nearly without exception code completion tools are evaluated against synthetic benchmarks typically produced by adding holes to existing code e.g.
removing an identifier and tasking the model with predicting the correct expression for that location using the remaining context .
nevertheless there is reason to be suspicious of synthetic benchmarks proksch et al.
show that developmentthat happens locally takes place in a very different order and context from what is eventually committed so that simulations in published code probably do not reflect the developer s working context .
code completion tool design could greatly benefit from real world insights robbes lanza demonstrated this by using observations from a developer study to improve their code completion tool .
an empirical foundation is needed to establish the characteristics of real world code completion usage and the objectives that code completion tools should meet to have real world efficacy.
our case study provides the first step towards such a foundation by analyzing over completions that were applied by developers in visual studio in a pre existing dataset.
we compare these completions with artificial ones first in terms of their characteristics and then by replicating them with state of the art code completion models including structured recommenders recurrent neural networks and dynamic n gram models.
we answer the following questions rq1 how do code completions in synthetic benchmarks compare to applied real world completions?
we find that real world completions are different from typical artificial ones on both trivial factors such as primarily addressing tokens with longer names and local variables and more complex ones such as often focusing on method invocations and field accesses from within the same project.
rq2 do these different characteristics affect code completion models?
we show that state of the art models have a much harder time completing real world queries than those on randomly sampled tokens with similar characteristics.
we outline promising factors and outstanding challenges e.g.
models that dynamically integrate local data fare much better than static ones but intraproject queries remain the hardest category.
rq3 what characteristics of real completions are overlooked in code completion models?
inter alia our analysis shows that most completions are both applied quickly and fairly predictable apparently geared at reducing typing effort but the small remainder typically involving apis consume most of the developers time.
existing models perform very poorly on the second category suggest a ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ing that they are optimized for repeating typical patterns but may not provide novel insights.
our findings inform concrete recommendations for improving code completion benchmarks tool design and realworld efficacy.
benchmarks should first and foremost aim to address completions following their frequency of use which we characterize showing for example that intra project api completions are surprisingly relevant.
code completion tools should similarly aim to be flexible able to integrate local information and be aware of contexts in which they can be im precise.
finally real world efficacy is increased precisely by focusing on the least accurate predictions in synthetic data these are far more common and time consuming in realworld data.
crucially our work shows that we need more benchmarks of real code completion engines and developer trials to promote impactful research in this field.
ii.
e xperiment al setup synthetic code completion benchmarks are typically created by taking a complete program and removing a random token such as an identifier or method invocation .
we want to contrast such synthetic completions with real code completions.
to this end we base our case study on a public dataset containing code completions invoked in visual studio and simulate artificial completions on the same data.
this section details our data collection and modeling setup.
our processed and replicable code completion data as well as our model implementations are publicly available .
a. code completion data several tools have been developed to track developer as they interact with the integrated development environment ide such as blaze dflow and feedbag .
the latter is most appropriate for our purposes feedbag captures event streams in visual studio that include invocations of the code completion tool as well as events for other actions such as opening new projects using version control and editing a file.
change related events include snapshots of the file in which the edit completion took place stored in the form of simplified syntax trees sst an ast with fully qualified type annotations.
we produce our benchmark data by processing a public dataset of developer interactions that were recorded with this tool .
in addition we use a large dataset of c repositories as the training data .
both datasets have been cleaned and pre processed as described below to allow replication of a variety of models including deep learning models cache based models and ast based models all of which we include in this work.
a full description of the datasets is available in the original papers.
benchmark data our benchmark dataset contains event streams from developers of which had reproducible completions with varying levels of experience as they interact with c projects in visualstudio.
this dataset includes many code completion events and general edit events.
we extract all completion events and their accompanying code contexts which contain a syntax tree of the state of the file in which thedeveloper triggered the completion with a placeholder at the current completion location.
we extract the prefix the characters that were already written towards the completion and the ultimately selected correct completion.
to be compatible with all the models in this study we only aim to predict the identifier portion e.g.
the method name of the completion.
across these developers we find nearly completion events.
of these ca.
were terminated by filtering meaning the developer narrowed down the prefix by typing more characters were canceled and were ultimately applied.
this latter category is most interesting to us as it presents reproducible completion events.
we cannot make strong claims about the reasons for the cancellation events but it is safe to say that they include at least some cases in which visual studio was unable to provide a useful completion we discuss the ramifications of this in section iv a .
not all completion events contain all the information needed to create reproducible completions.
among the applied cases ca.
did not include locality information about where the completion took place and did not store the selected completion but rather a generic lookupobject that could not be decoded into the corresponding token after the fact.
accounting for overlap between these categories we extract valid applied completions belonging to developers for which we can fully replicate the context.
for each completion event we extract all the surrounding tokens from the stored syntax tree.
ideally we would also have access to the tokens in the other files in the project but these are understandably not stored in the dataset.
we do have access to any file in which an edit event or completion event takes place so we approximate the content of the surrounding project by storing the latest snapshots of edited files incrementally thus creating a restricted view of the project for each developer that improves over time.
in addition to using the recorded completions as a benchmark we also simulate a synthetic benchmark on the same data instead of completing the real missing token we remove another token at random from the file in which the completion takes place and ask each model to infer it.
to allow a more fine grained comparison we run this simulation many times per file and store the randomly selected token s characteristics e.g.
token length syntactic type this allows us to approximate several forms of synthetic benchmarks after the fact specifically completing all tokens completing just identifiers and completing only api calls.
several other types of synthetic data creation have been proposed e.g.
removing sets of related api calls and may be studied similarly we selected options representative of most work on code completion.
training data we use a public dataset of c github repositories comprising ca.
42m lines of code to train the different completion tools .
this dataset is released with the benchmark data and its code is stored in the same representation simplified syntax trees .
we use a built in method 1we have ca.
two continuous weeks of observations for each developer demarcated by sessions of ide use authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
that comes with the dataset to transform it back into c with small adaptations to inline previously nested expressions again and reverse the flattened representations of the simplified syntax trees.
this made it easy for us to extract lexical token sequences corresponding to the syntax trees.
the resulting token sequences can be used by lexical tokenbased models.
to make training with deep learners see section ii b feasible in reasonable time we select of the files in this dataset at random to create a corpus of 16m tokens.
a static vocabulary is estimated on the training data and all events seen less than times are treated as a generic unknown token to produce a vocabulary of .
both the corpus and the vocabulary are comparable to prior work using deep learners on source code and we use them to train both the n gram models and deep learners.
b. selected code completion models our aim is not to compare and rank different completion engines but we are interested in understanding how the discrepancies between real and artificial completion data impact real code completion models and tools.
thus we want to select representative and accurate models from diverse backgrounds.
approaches to perform code completions span a wide spectrum of techniques but virtually all intelligent code completion models attempt to relate the context at the site of a required completion to a context that has been observed in some large training corpus.
completions that were applied in similar contexts in the training data can then be recommended in the present context.
the difference between approaches often comes down to two aspects the type of context that is extracted and the way this context is related to contexts seen during training.
one demarcation relates to the way information is extracted on one side are tools that focus on extracting rich structural features from the surrounding source code such as types that are being used close by.
approaches on the other side do not explicitly select structural features at all but instead exploit the naturally recurring patterns in source code using text mining techniques from which useful features can naturally emerge .
we select performant representatives from both ends of this spectrum summarized in table i. a grey area exists in between these extremes including approaches that select specific structural features of interest but use text mining techniques to infer a model over these models e.g.
bielik et al.
develop a domain specific language over allowed contexts and learn a statistical model to predict which context to use at test time .
other models have been proposed both using natural language models e.g.
and traditional feature selection e.g.
we leave the analysis of such models to future work and provide our dataset to support such efforts.
structural feature selection a traditional approach to code completion especially api recommendation is to describe the current editing context through a set of feature types such as the surrounding method invocations.
these feature types are manually designed by leveraging domainknowledge about source code.
the vocabulary of available t able i models included in this study with characteristics how they represent code whether they can dynamicall y learn new p a tterns and wha t types of completions they address .
model format dynamic?
completions bmn ast no members rnn lexical limited all n gram lexical y es all we extended the implementation to support member completion only unqualified