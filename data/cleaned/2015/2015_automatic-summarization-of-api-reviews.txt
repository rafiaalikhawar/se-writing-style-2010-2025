automatic summarization of api reviews gias uddin school of computer science mcgill university montr eal qc canada gias cs.mcgill.cafoutse khomh swat lab polytechnique montr eal qc canada foutse.khomh polymtl.ca abstract with the proliferation of online developer forums as informal documentation developers often share their opinions about the apis they use.
however given the plethora of opinions available for an api in various online developer forums it can be challenging for a developer to make informed decisions about the apis.
while automatic summarization of opinions have been explored for other domains e.g.
cameras cars we found little research that investigates the benefits of summaries of public api reviews.
in this paper we present two algorithms statistical and aspect based to summarize opinions about apis.
to investigate the usefulness of the techniques we developed opiner an online opinion summarization engine that presents summaries of opinions using both our proposed techniques and existing six off theshelf techniques.
we investigated the usefulness of opiner using two case studies both involving professional software engineers.
we found that developers were interested to use our proposed summaries much more frequently than other summaries daily vs once a year and that while combined with stack overflow opiner helped developers to make the right decision with more accuracy and confidence and in less time.
index terms opinion mining api informal documentation opinion summaries study summary quality.
i. i ntroduction apis application programming interfaces offer interfaces to reusable software components.
modern day rapid software development is often facilitated by the plethora of open source apis available for any given development task.
the online development portal github now hosts more than million public repositories.
we can observe a radical increase from the .
million active repositories hosted in github in .
while developer forums serve as communication channels for discussing the implementation of the api features they also enable the exchange of opinions or sentiments expressed on numerous apis their features and aspects.
in fact we observed that more than of stack overflow posts that are tagged java and json contain at least one positive or negative sentiment.
most of these posts also do not contain any code examples.
the number of numerous apis available and the sheer volume of opinions about any given api scattered across many different posts though pose a significant challenge to gain quick and digestible insights.
due to the diversity of opinions in online forums about different products e.g.
camera cars mining and summarization of opinions for products have become an important but challenging research area .
we are aware of no summarization techniques applied to api reviews.
given the plethora of reviews available about an api in developer forums it fig.
.
screenshots of opiner api review search engine can be extremely beneficial to produce an informative but concise representation of the summaries so that quick but useful insights can be gathered.
we developed an online api review summarization engine opiner that given as input all the opinionated sentences about an api produces summaries of the reviews using our two proposed techniques statistical and aspect based summarization.
in figure we present a screenshot of opiner.
opiner is developed as a search engine where developers can search opinions for an api .
upon clicking on api developers can see all the reviews collected about the api both in summarized and original formats .
a developer can also search for an api aspect e.g.
performance in opiner to find the most popular apis based on the aspect .
we investigated the usefulness of opiner based on two research questions rq1 how informative are the various api opinion summarization techniques to the developers?
we investigated the informativeness of our proposed summaries against the summaries produced by six off the shelf summarization techniques by conducting a study involving professional software engineers.
the developers rated the summaries of four different apis using five development .
c ieeease urbana champaign il usa technical research159 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
scenarios e.g.
selection of an api etc.
.
we found that developers strongly favored our proposed summaries against other summaries more than vs less than ratings .
rq2 how useful is an api opinion summarization engine to support development decisions?
we conducted another study where we provided access to our tool to professional software engineers to help them in their selection of an api for two development tasks.
we observed that while developers correctly picked the right api with accuracy while just using stack overflow they had accuracy while they used opiner and stack overflow together.
this paper makes the following contributions opiner we present opiner our online api opinion summarization and search engine where developers can search for opinions about apis.
evaluation we evaluated the effectiveness of the summaries and opiner using two case studies.
ii.
a utomatic summarization of api r eviews we investigated opinion summarization algorithms from the following major categories aspect based positive and negative opinions are grouped around aspects related to the entity e.g.
picture quality .
aspects can be pre defined or dynamically inferred using algorithms such as topic modeling.
contrastive contrastive viewpoints are grouped.
extractive a subset of the opinions are extracted.
abstractive an abstraction of the opinions is produced.
statistical overall polarity is transformed into numerical rating e.g.
star ratings .
we present algorithms to produce aspect based and statistical summaries of api reviews see sections ii b ii c .
we leveraged off the shelf algorithms to produce extractive abstractive and topic based summaries and implemented the algorithm proposed by kim and zhai to produce contrastive summaries see section ii d .
in section ii a we introduce the api review dataset that we used to produce the summaries.
we then leverage the dataset to describe our summarization techniques.
a. dataset our api review dataset was produced by collecting all the opinionated sentences for each java api mentioned in the stack overflow threads tagged as java json i.e.
the threads contained discussions and opinions related to the json based software development tasks using java apis.
we selected java apis because we observed that java is the most popular object oriented language in stack overflow.
as of april there were more than .
million threads in stack overflow behind only javascript .
million .
we used json based threads for the following reasons competing apis.
due to the increasing popularity in json based techniques e.g.
rest based architectures microservices etc.
we observed a large number of competing apis in the threads offering json based features in java.
diverse opinions.table i statistics of the dataset a a nswers c c omments threads posts a c sentences words users .7k .9k .8k 87k .08m .5k average .
.
.
.
.
.
we observed diverse opinions associated to the competing opinions from the different stakeholders both api users and authors .
diverse development scenarios.
jsonbased techniques can be used to support diverse development scenarios such as serialization lightweight communication between server and clients and among interconnected software modules and growing support of json based messaging over http using encryption techniques and on the fly conversion of language based objects to json formats and vice versa.
in table i we show descriptive statistics of the dataset.
there were posts from threads with scores greater than zero.
we did not consider any post with a negative score because such posts are considered as not helpful by the developers in stack overflow.
the last column users show the total number of distinct users that posted at least one answer comment question in those threads.
to identify uniqueness of a user we used the user id as found in the stack overflow database.
on average around four users participated in one thread and more than one user participated in threads .
and a maximum of distinct users participated in one thread .
from this corpus we identified all of the java apis that were mentioned in the posts.
our api database consists of the java official apis and the open source java apis listed in the two software portals ohloh and maven central .1we crawled the javadocs of five official java apis se and ee and collected information about packages and types.
we consider an official java package as an api in the absence of any guidelines available to consider otherwise.
in total our api database contains distinct java apis.
all of the apis hosted in maven central are for java.
from ohloh we only included the java apis out of the total crawled projects .
we considered a project in ohloh as a java api if its main programming language was java.
we collected the opinionated sentences about apis using a technique previously developed by uddin an khomh .
the technique consisted of the following steps loading and preprocessing of stack overflow posts.
detection of opinionated sentences.
we used a rule based algorithm based on a combination of sentistrength and the sentiment orientation so algorithm .
detection of api names and hyperlinks in the forum texts and the association of apis to opinionated sentences based on a set of heuristics.
in table ii we present summary statistics of the opinionated sentences detected in the dataset.
overall distinct apis were found.
while the average number of opinionated sentences per api was .
it was for the top five most 1we crawled maven in march and ohloh in dec .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii distribution of opinionated sentences across api s overall top five api total pos neg total pos neg average .
.
.
.
.
reviewed apis.
in fact the top five apis contained .
of all the opinionated sentences in the posts.
the apis are jackson google gson spring framework jersey and org.json.
intuitively the summarization of opinions will be more helpful for the top reviewed apis.
b. statistical summaries of api reviews an approach to statistical summaries is the basic sentiment summarization by simply counting and reporting the number of positive and negative opinions .
in figure shows such a summary for api jackson.
we propose the following statistical summaries for api reviews star rating.
provides an overall sentiment representation towards an api using a rating on a five star scale.
we present a technique to compute a five star rating for an api based on sentiments.
sentiment trends.
because apis can undergo different versions and bug fixes the sentiment towards the api can also change over time.
co reviewed apis.
we visualize other apis that were mentioned in the same post where an api was reviewed.
such insights can help find competing apis.
star rating the determination of a statistical rating can be based on the input star ratings of the users.
for example in amazon product search the rating is computed using a weighted average of all star ratings.
thus if there are inputs with five stars two four stars four three stars one two stars and three one stars the overall rating can be computed as .
.
given as input the positive and negative opinionated sentences of an api we computed an overall rating of the api in a five star scale by computing the proportion of all opinionated sentences that are positive as follows r positives positives negatives.
for example for the api jackson with positive sentences and negative sentences the rating would be .
.
in figure shows the calcuated five star rating for jackson.
our approach is similar to blair goldensohn et al.
at google research except that we do not penalize any score that is below a manually tuned threshold.
in our future work we will investigate the impact of sentiment scores and popularity metrics available in developer forums.
sentiment trend we produce monthly aggregated sentiments for each api by grouping total positive and negative opinions towards the api.
in of figure the line charts show an overview of the summary by months.
we produced the summary as follows we assign a timestamp to each opinionated sentence the same as the creation timestamp of the corresponding post where the sentence was found.
we group the timestamps into yearly buckets and then into fig.
.
statistical summarization of opinions for api jackson.
monthly buckets.
we place all the opinionated sentences in the buckets based on their timestamp.
co reviewed apis in figure shows the other apis that were reviewed in the same posts where the api jackson was discussed.
we computed this as follows for each opinionated sentence related to the given api we identify the corresponding posts where the sentence was found.
we detect all the other api mentions in the same post.
for each of the other apis in the post we detect the opinionated sentences related to the apis.
for each api we group the positive and negative opinions in each post.
we then calculate a ratio of negativity vs positivity by taking the count of total positive opinions for the given api vs total negative opinions for each of the other apis.
if the ratio is greater than one we say that the given api is more negatively reviewed around the other api.
for each of the other apis we count the number of times the other api is more positively reviewed around the given api.
c. aspect based summaries of api reviews aspect based summarization involves generating summaries of opinions around a set of aspects each aspect corresponding to specific attributes features of an entity about which the opinion was provided.
for example for a camera picture quality can be an aspect.
aspects can be different depending on the domains.
thus the detection of domain specific aspects is the first step towards aspect based opinion summarization .
in this section we describe the components of our system that identifies the aspects of an api about which developers provided opinions.
this includes finding authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the corresponding sentences that mention these aspects.
our approach contains the following steps each supported by our development infrastructure static aspect detection we leverage the fact that similar to other domains we observed a zipfian distribution for api reviews i.e.
some aspects are more likely to be discussed across the apis e.g.
performance usability .
these are called static aspects and we present techniques to automatically detect those in section ii c1.
dynamic aspect detection we observe that certain aspects can be more common in an api or a group of apis e.g.
object conversion for json parsing apis vs applet size for apis to design user interfaces .
the detection of these dynamic aspects requires techniques different from the detection of static aspects see section ii c2 summarizing opinions for each aspect for each api we produce a consolidated view by grouping the reviews under the aspects and by presenting different summarized views of the reviews under each aspect see section ii c3 .
static aspect detection in a previous study uddin et al.
surveyed software developers and found that developers prefer to see opinions about the following api aspects in the forum posts performance how well does the api perform?
usability how usable is the api?
security how secure is the api?
documentation how good is the documentation?
compatibility does the usage depends on other api?
portability can the api be used in different platforms?
community how is the support around the community?
legal what are licensing requirements?
bug is the api buggy?
only sentiment opinions without specifying any aspect.
others opinions about other aspects.
we developed a supervised classifier to detect each aspect to account for the fact that more than one aspect can be discussed in one opinionated sentence.
we used four performance measures to assess the performance of the classifiers precision p recall r f measure f1 and accuracy a .
p t p t p fp r t p t p fn f1 p r p r a t p t n t p fp t n fn t p nb.
of true positives and fn nb.
false negatives.
we report the performance of our aspect detection component using a dataset previously labeled by uddin and khomh .
the benchmark consisted of manually labeled sentences from stack overflow posts.
the threads were selected from tags representing nine distinct domains two tags for each domain .
candidate classifiers.
because the detection of the aspects requires the analysis of textual contents we selected two supervised algorithms that have shown better performance for text labeling in both software engineering and other domains svm and logistic regression.
we used the stochastic gradient descent sgd discriminative learner approach for the two algorithms.
for svm linear kernel we used the libsvm implementation.
both sgd and libsbm offered more flexibilitytable iii performance of static aspect detectors precision recall f1 score accuracy aspect n a s a s a s a s perf ormance b .
.
.
.
.
.
.
.
usability b .
.
.
.
.
.
.
.
security u .
.
.
.
.
.
.
.
community u .
.
.
.
.
.
.
.
compatibility t .
.
.
.
.
.
.
.
p ortability u .
.
.
.
.
.
.
.
documentation b .
.
.
.
.
.
.
.
bug u .
.
.
.
.
.
.
.
legal u .
.
.
.
.
.
.
.
onlysentiment b .
.
.
.
.
.
.
.
others u .
.
.
.
.
.
.
.
n ngram u unigram b bigram t trigram a average s stdev for performance tuning i.e.
hyper parameters and both are recommended for large scale learning.
we applied the svm based classification steps as recommended by hsu et al.
who observed an increase in performance based on their reported steps.
the steps also included the tuning of hyper parameters.
intuitively the opinions about api performance issues can be very different from the opinions about legal aspects e.g.
licensing of apis.
due to the diversity in such representation of the aspects we hypothesized each as denoting a sub domain within the general domain of api usage and tuned the hyper parameters of classifiers for each aspect.3as recommended by chawla to train and test classifiers on imbalanced dataset we set lower weight to classes with over representation.
in our supervised classifiers to set the class weight for each aspect depending on the relative size of the target values we used the setting as balanced which automatically adjusts the weights of each class as inversely proportional to class frequencies.
picking the best classifiers.
to train and test the performance of the classifiers we applied fold cross validation on the benchmark for each aspect as follows we put a target value of for a sentence labeled as the aspect and otherwise.
we tokenized and vectorized the dataset into ngrams.
we used n for ngrams i.e.
unigrams one word as a feature to trigrams three consecutive words .
we investigated the ngrams due to the previously reported accuracy improvement of bigram based classifiers over unigram based classifiers .
as recommended by hsu et al.
we normalized the ngrams by applying standard tf idf with the optimal hyper parameter e.g.
minimum support of an ngram to be considered as a feature .
for each ngramvectorized dataset we then did a fold cross validation of the classifier using the optimal parameter.
for the folds we used stratified sampling which keeps the ratio of target values similar across the folds.
we took the average of the precision recall f1 score accuracy of the folds.
thus for each aspect we ran our cross validation nine times three times for each candidate classifier and once for each of the ngrams.
we picked the best performing classifier as the one with the best f1 score among the nine runs.
2we used the sgdclassifier of scikit 3we computed hyper parameters using the gridsearchcv algorithm of scikit learn authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
the distribution of dynamic aspects in the dataset .in table iii we report the performance of the final classifiers for each aspect.
except for one aspect security svmbased classifiers were found to be the best.
unigram based features were better suited for most of the aspects five aspects and others followed by bigrams four aspects and trigrams compatibility .
the diversity in ngram selection can be attributed to the underlying composition of words that denote the presence of the corresponding aspect.
for example performance based aspects can be recognized through the use of bigrams e.g.
thread safe memory footprint .
legal aspects can be recognized through singular words e.g.
free commercial .
in contrast compatibility based features require sequences of words to realize the underlying context.
analysis of misclassifications.
while the precisions of nine out of the detectors are at least .
it is only .
for the aspect community .
while the detection of the aspect compatibility shows an average precision of .
there is a high degree of diversity in the detection results.
for example in second column under precision of table iii we show the standard deviation of the precisions across the folds and it is .
for compatibility .
this happened because the detection of this aspect showed more than accuracy for half of the folds and close to zero for the others.
we observed two primary reasons for the misclassifications both related to the underlying contexts required to detect an aspect implicit when a sentence was labeled based on the nature of its surrounding sentences.
consider the following two sentences jboss is much more popular .
.
.
it is easier to find someone .
.
.
and sometimes this is more important .
.
.
.
the second sentence was labeled as community because it was a continuation of the opinion started in the first sentence which was about community support towards the api jboss.
unseen when the features i.e.
vocabularies corresponding to the particular sentence were not present in the training dataset.
in the future we will investigate this issue with different settings e.g.
using more labeled data.
in figure shows the distribution of the static aspects in our dataset.
some aspects are much less represented due to the specific nature of the domain.
for example security is less of a concern in json parsing than it is in network based tasks.
the aspect compatibility because apis offering json parsing in java can be applied irrespective of the underlyingoperating system.
the aspect usability accounted for almost of the opinions followed by others .
the sentences belonging to the others category contain opinions about api aspects not covered by the static aspects.
we apply our dynamic aspect detection on the opinions labeled as others .
dynamic aspect detection we detect dynamic aspects on the of the sentences in our dataset that were labeled exclusively as others .
our dynamic aspect detection algorithm is adopted from similar techniques used in other domains e.g.
electronic products and local service reviews e.g.
hotels restaurants .
our approach consists of the following three steps keyword identification we find representative keywords and phrases in the sentences labeled as others .
pruning we discard keywords phrases based on two filters.
the remaining items are considered as dynamic aspects.
assignment we assign each sentence to a dynamic aspect.
the remaining keywords are considered as dynamic aspects.
we discuss the steps with examples below.
keyword identification the first step in dynamic aspect detection is to find keywords phrases that are most likely to be a representation of the underlying domain.
unlike hu et al.
and blairgoldensohn et al.
who used frequent itemset mining we use the keyphrase detection process used in the textrank algorithm.
textrank uses the google page rank algorithm to find representative keyphrases in a given dataset.
the page rank algorithm has been successfully applied in software engineering research .
we detect the keyphrases as followed we tokenize the sentences we remove the stopwords we detect parts of speech we discard any words that are not nouns we create a graph of the words by putting the words as nodes and creating an edge between the words whenever they appeared together in the dataset.
on this network of words we then applied the page rank algorithm to find the ranking of each words.
from this ranking we keep the top words as keywords.
in our future work we will investigate the algorithm with different percentages of keywords.
we then merge two or more keywords into one if they appeared together in the sentences.
pruning hu et al.
and blairgoldensohn et al.
observed that the frequent keywords by themselves can still be too many to produce a concise list of aspects.
we thus apply two filters on the keywords to remove the keywords that are not widely found across different apis.
both of the filters were originally proposed by hu et al.
we then compute the tfidf score of the remaining keywords and phrases in all the sentences labeled as others and rank those based on score i.e.
a keyword with a higher score is more representative .
assignment we assign each sentence in the others dataset to a keyword as identified in the previous step.
if a sentence is not labeled as any of the keywords phrases we consider it as a general comment .
we label each such sentence as discussing about an api feature .
for a sentence labeled as more than one dynamic aspect we assign it to the 4we used the page rank implementation from python networkx library authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
screenshot of the aspect based summarizer .
one with the highest tf idf score.
in figure 2shows the distribution of the dynamic aspects except the general comments in the dataset.
aspect based summarizing of opinions we produce the following summaries based on the aspects overview nested aspect views and comparative by aspect.
we explain the techniques behind each summaries below overview for a given api we produce a consolidated overview of the aspects detected in its reviews.
the overview contains the rating of each aspect for the given api and the most recent positive and negative opinion.
in figure shows the overview page.
the aspects are ranked based on their recency i.e.
the aspect with the most recent positive or negative opinion is placed at the top.
we further take cues from statistical summaries and visualize sentiment trends per aspect for the api.
such trends can be useful to compare how its different aspects have been reviewed over time.
nested views given as input all the positive or negative opinions of an api we group the opinions by the detected aspects.
we rank the aspects based on their recency i.e.
the aspect with the most opinion is placed at the top.
we observe that even after this grouping some of the aspects contain hundreds of opinions e.g.
usability .
to address this we further categorize the opinions under each aspect into subcategories.
we denote those as nested aspect.
we detect the nested aspects as follows.
a we collect all the stack overflow tags associated to the threads in the dataset.
b for each sentence under a given aspect of an api we label each opinion as a tag if the opinion contains at least one word matching the tag.
c for an opinion labeled as more than one tag we assign the opinion to the tag that covers the most number of opinions within the aspect for the api.
d for an opinion that can not be labeled as any of the tags we label it as general .
in figure the second circle sho ws a screenshot of the nested views of the negative opinions of the api jackson.
by clicking the details link the user is taken to the corresponding post in stack overflow where the opinion was provided.
comparative by aspect while all the above three sum marizations take as input the opinions and aspects of a given api they do not offer a global viewpoint of how a given aspect is reviewed across the apis.
such an insight can be useful to compare competing apis given an aspect e.g.
performance .
we show three types metrics popularitypa na pc nc positi vitypa pc and ne gativityna nc.pais the total number of positive opinions about an api and pcis the total number of positive opinions about all apis.
in figure the third circle sho ws a screenshot of the comparative by aspect views for the aspect performance and shows that the most popular api for jsonbased features in java is jackson.
a user can click the api name and then it will show the most recent three positive and three negative sentences one after another.
if the user is interested to explore further a link is provided after those six opinions that will take the user to all the opinions and summaries of the reviews of the api.
d. summarization algorithms adopted for api reviews in this section we explain how we adopt currently available summarization algorithms to produce extractive abstractive contrastive and topic based summaries of api reviews.
topic based summarization in a topic based opinion summarization topic modeling is applied on the opinions to discover underlying themes as topics.
there are two primary components in a topic based opinion summarizer a title name of the produced topics and a summary description of the topic.
three types of summaries are investigated word level phrase level and sentence level .
for each api in our dataset we summarize the reviews by applying lda latent dirichlet allocation once for the positive opinions and once for the negative opinions.
we apply standard practices e.g.
optimal number of topic detection we use the technique proposed by arun et al.
.
to determine the representativeness of topics we use the topic coherence measure as proposed by r oder et al.
.
for each topic we produce a description by taking the top ten words of the topic.
contrastive viewpoint summary in contrastive summaries contrastive viewpoints are grouped together e.g.
the object conversion in gson is easy to use vs the object conversion in gson is slow .
in the absence of any off the shelf api available to produce such summaries we implemented the technique proposed by kim and zhai .
extractive summarization with the extractive summarization techniques a summary of the input documents is made by selecting representative sentences from the original documents.
extractive summaries are the most prevalent for text summarization across domains e.g.
news blogs .
for each api we apply three extractive summarization algorithms luhn lexrank and textrank .
luhn is the oldest summarization algorithms while texrank and lexrank are among the most recent.
using each algorithm we produce two summaries for each api one for positives and one for negatives.
for each api we produce summaries containing of the inputs or opinionated sentences whicever yields the most number of sentences in the summary .
each sentence in authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
our dataset has words on average.
the most reviewed api in our dataset is jackson with sentences with a threshold i.e.
words .
for sentences the summaries would have words.
previous research considered lengths of extractive summaries to be between and words .
abstractive summarization abstractive summaries produce an abstraction of the documents by generating concise sentences.
a limitation of extractive summarization for opinions is that a limit in the summary size may force the algorithm to remove important sentences.
we produce abstractive summary for an api once for positive sentences and once for negative sentences using opinosis a domain independent abstractive opinion summarization engine.
iii.
i nformativeness of the summaries rq1 because the goal of the automatic summary of the reviews of an api is to provide developers with a quick overview of the major properties of the api that can be easily read and digested we performed a study involving potential users in a manner similar to previous evaluation efforts on software artifact summarization .
in this section we describe the design and results of the study.
a. study design our goal was to judge the usefulness of a given summary.
theobjects were the different summaries produced for a given api and the subjects were the participants who rated each summary.
the contexts were five development tasks.
the five tasks were designed based on a preliminary survey of software developers of their preference to use summaries of api reviews in assisting development tasks .
each task was described using a hypothetical development scenario where the participant was asked to judge the summaries through the lens of the software engineering professionals.
persona based usability studies have been proven effective both in the academia and industry .
we briefly describe the tasks below.
t1.
selection.
can the summaries help you to select this api?
the persona was a software architect who was tasked with making a decision on a given api based on the provided summaries of the api.
the decision criteria were c1 rightness contains the right and all the useful information.
c2 relevance is relevant to the selection.
c3 usable different viewpoints can be found.
t2.
documentation.
can the summaries help to create documentation for the api?
the persona was a technical writer who was tasked with writing a software documentation of the api to highlight the strengths and weaknesses of a given api based on the reviews in stack overflow.
the decision criteria were c1 completeness complete yet presentable c2 readable easy to read and navigate.
t3.
presentation.
can the summaries help you to justify your selection of the api?
the persona was a development team lead who was tasked with creating a short presentation of a given api to other teams with a view to promote or discourage the usage of the api across the company.the decision criteria were c1 conciseness is concise yet complete representation c2 recency shows the progression of opinions about the different viewpoints.
t4.
awareness.
can the summaries help you to be aware of the changes in the api?
the persona was a software developer who needed to stay aware of the changes to this api because she used the api in her daily development tasks.
the decision criteria were c1 diversity provides a comprehensive but quick overview of the diverse nature of opinions.
c2 recency shows the most recent opinions first.
t5.
authoring.
can the summaries help you to author an api to improve its features?
the persona was an api author who wanted to assess the feasibility of creating a new api to improve the features offered by the given api.
the decision criteria were c1 strength and weakness highlighter shows the overall strengths and weakness while presenting the most recent opinions first.
c2 theme identification presents the different viewpoints about the api.
we assessed the ratings of the three tasks selection documentation presentation using a point likert scale the summary does not miss any info misses some info misses all the info .
for the task authoring we used a point scale full helpful partially helpful partially unhelpful fully unhelpful .
for the task awareness we asked participants how frequently they would like to use the summary never once a year every month every week every day .
each of the decision criteria under a task was ranked using a five point likert scale completely agree completely disagree .
each participant rated the summaries of two apis.
we collected ratings for four different apis.
the four apis jackson gson org.json and jersey were the four most reviewed apis in our dataset offering json based features in java.
the four apis differed from each other on a number of aspects.
for example jackson differentiates itself by providing annotation based mixin to facilitate faster processing of json files.
gson focuses more on the usability of the overall usage org.json is the oldest yet the natively supported json api in android and jersey is a framework i.e.
it offers other features besides providing json features .
in addition to collecting rates for each summary we also collected demographic information about the participants i.e.
by collecting their experience and current roles in their development teams.
we analyzed the responses using descriptive statistics.
b. participants we hired participants from the online professional social network site freelancer.com .
each freelancer had at least a .
star rating the maximum possible star rating being .
sites like amazon mechnical turks and freelancer.com have been gaining popularity to conduct studies in empirical software engineering research due to the availability of efficient knowledgable and experienced software engineers.
we only hired a software engineer if he used at least one of the four apis in the past.
each participant was remunerated between which is a modest sum given the volume of the work.
each participant was allowed to complete a study only once.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
textranklexrankluhnabstractivecontrastivetopicstatisticalaspect 50never once a year every month every week every dayfig.
.
developers preference of usage of summaries table iv impact of the summaries based on the scenarios task option s a t c b l p u selection nm ms ma documentation nm ms ma presentation nm ms ma authoring fh ph pu fu nt nm not misses any info ms misses some info nt neutral ma misses all info s statistical summary a aspect fh ph full partially helpful fu pu full partially unhelpful t topic based b abstractive l lexrank p texrank u luhn to ensure future traceability we collected the name and email address of each participant.
the study was conducted using google doc form.
each participant was given an access to the opiner online tool and provided a short demo of the tool to ensure that they understood the tool properly.
in addition a coding guide was linked to the study questionnaire to explain all the summaries in details.
each reference to a summary of a given api was also linked to the corresponding web page in opiner.
the participants were allowed to only complete the study when they completed reading the coding guide.
all of the participants were actively involved in software development and had software development experience ranging from three to more than years.
the occupation of the participants varied among software developers team leads and architects.
c. results in table iv we show the percentage of the ratings for each summary for the four tasks selection documentation presentation authoring .
in figure we show the percentage of participants showing their preference towards the usage of the summaries to stay aware awareness task .
the aspectbased summary was considered as the most useful followed by statistical summaries.
among the other summaries the topicbased summaries were the most favored.
in table v we showtable v rationale based on the scenarios only completely agreed and agreed statistics are presented for brevity t criteria agree s a t c b l p u s rightness e l relevant e c usable t d complete o c readable u p concise r e recency s a diversity w a recency r a highlight u t theme h completely agree u luhn l lexrank s statistics a aspect t topic b abstract p textrank the percentages of the ratings for each criteria under each development task.
we discuss the rationales below.
selection while most participants completely agreed the most for aspect based summaries when we combined the ratings of completely agreed and agreed statistical summaries were the most favored.
when the participants were asked to write a short summary of their ratings based on the provided criteria participant r6 summed it up well first i used the statistics summarization to get an overview on the api whether i should go on and check it s details.
then i headed to aspect based summarization to check each aspect of the api.
these gave me a good picture of the api and whether we should use it or not.
documentation the aspect based summary was favored followed by statistical summaries.
among the other summaries topic based summaries were the most favored showing the needs of grouping opinions by themes as well as offering visualizations.
according to a participant aspectbased summarization s documentation part was a huge help making the decision.
all the other summarization were quite useless regarding deciding about documentation.
presentation while aspect and statistical summaries were again strongly favored contrastive summaries were the most favored among the rest showing a similarity with the selection task.
this inclination could mainly be due to the fact that both tasks asked them to provide justification of their selection usage of the api.
according to one participant statistical is ideal for presentation aspect covers every thing trends positive and negative response response summary .
awareness while the aspect and statistical summaries were the most favored based on each criteria topic based was authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
considered as the most helpful among others.
intuitively the same trends were observed for the task documentation and we note both were focused towards helping developers while selection and presentation were mainly for software developers who are not involved in daily programming activities.
authoring aspect based summaries were still favored the most because of the ratings provided to each aspect in the overview page aspect based summary helped a lot with it s rating system where i could see exactly what are the weak spots of the api so what should i concentrate on if i decide to make a new better api.
iv.
e ffectiveness analysis of opiner rq3 because the purpose of developing opiner was to add benefits over the amazing usefulness stack overflow provides to the developers we sought to seek the usefulness of opiner by conducting another study of professional software developers who completed two tasks using stack overflow and opiner.
a. study design thegoal of this study is to analyze the usefulness of opiner to assist in a development task.
the objects of this study are the apis and their summaries in opiner and stack overflow.
the subjects are the participants completing the tasks.
our study consisted of two parts p1 assessment of opiner s usefulness in a development setting p2 opportunities of industrial adoption of opiner.
to conduct the study we developed an online data collection tool with the following features login features for each user passive logging of user activities storage of user inputs in a relational database.
p1.
usefulness analysis.
we designed two tasks both corresponding to the selection of an api from a pool of two apis.
the two tasks corresponded to two different sets of apis.
t1 the participants were asked to make a selection out of two apis gson and org.json based on two criteria a usability and b licensing usage.
the right answer was gson.
the licensing agreement of the api org.json is generally considered not safe for industrial usage .
t2 the participants were asked to make a selection out of two apis jackson and json lib based on two criteria a performance and b pre installed base in leading frameworks e.g.
spring restlet etc.
the correct answer was jackson which is the pre packaged json api in the major frameworks.
we asked each developer to complete a task in two settings so only complete only using stack overflow so opiner complete using stack overflow opiner for a task in a each setting each developer was asked to provide following answers selection their choice of api confidence how confident they were while making the selection.
we used a five point scale fully confident value fully unsure .
rationale the reason of their selection in one or more sentences.
in addition we logged the time it took for each developer to complete a task under each setting.
p2.
industrial adoption after a developer completed the tasks we asked her to share her experience of using opiner table vi progression of learning from stack overflow to opiner t tool correctness conversion confidence time so .
.
.
so opiner .
.
so .
.
.
so opiner .
.
usage would you use opiner in your development tasks?
usability how usable is opiner?
improvement how would you like opiner to be improved?
the developers were asked to write as much as they can.
b. participants we invited nine professional developers from a software company and two developers from freelancer.com to participate in the study.
the experience of the developers ranged between year and years.
the developers carried on different roles ranging from software developer to architect to team lead.
the team leads were also actively involved in software development.
except one developer from the company all others participated in the study.
we first provided an online demo of opiner to them within the company during the lunch time.
the freelancers were provided the demo by sharing the screen over skype.
after the demo each developer was given access to our data collection tool.
c. study data analysis we analyzed the responses along the following dimensions correctness how precise the participants were while making a selection in the two settings.
confidence how confident they were making the selection?
time how much time did the developers spend per task?
in addition we computed a conversion rate as the ratio of developers who made a wrong selection using stack overflow but made the right selection while using both stack overflow and opiner.
d. results usefulness analysis.
nine developers completed task using the two different settings using stack overflow one of them could not continue further due to a sudden deadline at work .
five developers completed task using both of the settings nine using stack overflow four of them could not complete the task.
in table vi we present the impact of opiner while completing the tasks.
to compute the conversion rate we only considered the developers that completed a task in both setting.
for task only of the developers using stack overflow only selected the right api but all of those who later used opiner picked the right api i.e.
conversion rate .
for task only .
of the developers using stack overflow only selected the right api but all of them picked the right api when they used both opiner and stack overflow conversion rate .
the developers using the setting so opiner on average took only .
minutes total to complete t1 and .
minutes to complete t2.
when the same developers used so alone they took on authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
average .
minutes to complete t1 and .
minutes to complete t2.
the developers also reported higher confidence levels when making the selection using opiner with stack overflow.
for task the confidence increased from .
to .
and for task it increased from .
partially unsure to .
almost fully confident .
for task one developer did not select any api at all while using stack overflow and wrote not a lot of actual opinions on the stackoverflow search for net.sf.json lib and same for com.fasterxml.jackson.
most stack overflow questions are related to technical questions .
one of the developers with years of experience spent more than minutes for task while using stack overflow only and made the wrong selection but then he picked the right api while using opiner by citing that he found all the information in the summary com.google.code.gson has an open source license while i was only able to find a snippet citing bogus license for org.json .
.
.
the developers found the information about license while using aspect based summaries.
industrial adoption.
all developers answered to the three questions.
we summarize major themes below.
would you use opiner in your daily development tasks nine out of the developers mentioned that they would like to use opiner.
the participants wanted to use opiner as a starting point yes i would use opiner.
the summaries were a great way to form a first opinion which could then be improved with specific fact checking.
if i was not constrained by today s exercise to only use stack overflow then i would then challenge the my first opinion with general searches.
for one developer the usage may not be daily yes perhaps not daily .
.
.
the value is the increased confidence.
the comentioned apis are extremely valuable.
how usable is opiner the participants were divided about the usability of opiner.
half of the participants considered it usable enough it is very user friendly to use.
got used to it quickly.
.
for another participant the ui is simple.
but is a bit confusing to go through the tabs to find out exactly what the user needs.
the search bar is a big help.
how would you improve opiner moving forward the participants offered a number of features that would like to see in opiner enhanced contrastive focus on contrastive view .
.
.
instead of its technical qualities e.g.
why use one vs another .
documentation a brief description regarding what aspects contrastive extractive abstractive mean would be helpful sorting for aspect based summaries i think the categories need to be in alphabetical order.
it makes it hard when i was comparing to api to another because i had to ctrl f to find category.
opinion from multiple sources another thing that could improve opiner is if it mined from other websites too.
i think it would give an even better opinion and i would be tempted to use it more than google then.
v. t hreats to validity construct validity threats concern the relation between theory and observations.
in this study they could be due to measurement errors.
in fact the accuracy of the evaluation of api aspects and mentions is subject to our ability to correctlydetect and label each such entities in the forum posts we investigated.
we relied on the manual labelling of aspects .
to assess the performance of participants we use time and the percentages of correct answer which are two measures that could be affected by external factors such as fatigue.
reliability validity threats concern the possibility of replicating this study.
we attempted to provide all the necessary details to replicate our study.
the anonymized survey responses are provided in our online appendix .
nevertheless generalizing the results of this study to other domains requires an in depth analysis of the diverse nature of ambiguities each domain can present namely reasoning about the similarities and contrasts between the ambiguities in the detection of api aspects mentioned in forum posts.
vi.
r elated work related work can broadly be divided into analysis of developer forums and summarization of software artifacts.
analysis of developer forums.
online developer forums have been studied extensively to find dominant discussion topics to analyze the quality of posts to analyze developer profiles or to determine the influence of badges in stack overflow .
several tools have been developed such as autocomment assistance collaborative problem solving and tag prediction .
summarization in software engineering.
natural language summaries have been investigated for software documentation and source code .
murphy proposed two techniques to produce structural summary of source code.
storey et al.
analyzed the impact of tags and annotations in source code.
the summarization of source code element names types methods has been investigated .
a more recent work proposed automatic documentation via the source code summarization of method context .
the selection and presentation of source code summaries have been explored through developer interviews and eye tracking experiment .
our findings confirm that developers also require api review summaries.
vii.
s ummary opinions can shape the perception and decisions of developers related to the selection and usage of apis.
the plethora of open source apis and the advent of developer forums have influenced developers to publicly discuss their experiences and share opinions about apis.
in this paper we have presented opiner our summarization engine for api reviews.
using opiner developrs can gather quick concise yet complete insights about an api.
in two studies involving opiner we observed that our proposed summaries resonated with the needs of the professional developers for various tasks.
in our future work we plan to extend opiner to integrate opinions from multiple forums and to integrate continuous learning modules into the summaries to learn and improve from developers feedbacks.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.