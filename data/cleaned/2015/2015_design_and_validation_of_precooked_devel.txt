design and validation ofprecooked developerdashboards vladimirivanov innopolis university innopolis russia v.ivanov innopolis.ruvladislavpischulin innopolis university innopolis russia dalv6666 gmail.comalan rogers innopolis university innopolis russia a.rogers innopolis.ru giancarlosucci innopolis university innopolis russia g.succi innopolis.rujooyongyi innopolis university innopolis russia j.yi innopolis.ruvasilii zorin innopolis university innopolis russia v.zorin innopolis.ru abstract despiteincreasingpopularityofdeveloperdashboards theeffectiveness of dashboards is still in question.
in order to design a dashboardthatiseffectiveandusefulfordevelopers itisimportant to know a what information developers need to see in a dashboard and b howdeveloperswanttouseadashboardwiththat necessaryinformation.toanswerthesequestions weconducted two series of face to face individual interviews with developers.
in thefirststepweanalyzedanswers buildagoal question metric model and designeda precookeddeveloper dashboard.then duringthesecondseparateseriesofinterviews wevalidatedthegqm and derived feedback on the designed dashboard.
given that the costofdashboardcustomizationpreventsdevelopersfromutilizing dashboards webelievethatourfindingscanprovideasolidstarting point to build precooked developer dashboards that can be readily utilizedbysoftware companies.
ccs concepts softwareanditsengineering softwaredevelopmentprocess management keywords developer dashboards interviewswithdevelopers gqmmethod acmreference format vladimirivanov vladislavpischulin alanrogers giancarlosucci jooyong yi andvasiliizorin.
.designandvalidationofprecookeddeveloper dashboards.in proceedingsofthe26thacmjointeuropeansoftwareengineering conference and symposium on the foundations of software engineering esec fse november 4 9 lake buena vista fl usa.
acm newyork ny usa 6pages.
introduction there is increasing interest and use of developer dashboards in the software engineering industry .
developer dashboards permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
esec fse november 4 9 lake buenavista fl usa associationfor computing machinery.
acm isbn ... .
assumptionhereisthatvisualizationhelpsmanagers developersbe awareoftheoverallstatusoftheprojectstheyareworkingon and makepropercollaborative individualdecisionswhiledeveloping software which will improve the overall productivity of a developmentteam .however this promisingassumption ofa developer dashboard would hold only when a dashboard displays informationneededbytheusers withoutdistractingtheuserswith unwantedinformation.
what kinds of information do developers want to see in a dashboard?whilethere would be no single answertothese questions weseektofindgeneralanswersfromsoftwareengineersinthefield.
ourfindingscanbeusedtoconstructausefuldefaultdashboard.althoughmostmoderndashboardsarecustomizable developersoften usethedefaultdashboardduetothecostentailedbycustomization forexample developersoftendonotwanttoreadthroughthefull functionalities the dashboard provides and consequently do not customize the dashboard as reported in .
the same paper reports that managers often do not customize dashboards either andcallsforabetterdefaultdashboard.ourworktacklesthisniche.
weconductface to faceindividual interviews withrepresentatives of different companies.
we designedour questionnaire sessions takingintoaccountthegoal question metric gqm method .
our key contributions in this paper are to the best of our knowledge weforthefirsttimeidentifymetricsdeveloperswanttosee in dashboards by employing an observational study provide agqmmodeltounderstandwhytheidentifiedmetricsmatterforthe developers and identifyhowdeveloperswanttousedashboards in their development processes.
given that the cost of dashboard customizationpreventsdevelopersfromutilizingdashboards our participants expressed similar responses we believe that our findingscanprovideasolidstartingpointforsoftwaredevelopment organizationswho seekto improve team productivitybyutilizing developerdashboards.lastbutnotleast ourvalidationprocesshas revealedanadditionalaspectinrequirementsrepresentationand visualization.
backgroundand related work thetaskofbuildingadashboardinvolvesselectingtheappropriate metrics for measuring the software product as well as the minimum required set of functionality in the dashboard.
one of the approachestoselectingthenecessarymetricsisthecollectionofesec fse november4 lake buena vista fl usa v.ivanov v.pischulin a.rogers g.succi j. yi andv.zorin allpossiblemeasurements.theeffectivenessofthismethodisquestionable.
first data collection is a time consumingprocess.
second the question of interpreting the results remains open.
another option for metrics selection is based on the goal question metric approach.thisapproachdeliberatelyforcesthescreeningofmetricsbybindingthemtopre establishedgoals.thisconceptprovides interpretationof thecollecteddata andis the defacto standardin buildingadashboard .thus metricselectionshouldbeguided bytheproperselectionandprioritizationofgoalsofdashboard s users.inthissectionwefirstreviewrelatedworkaboutwhatisthe purposeofdashboardsinsoftwareengineeringandthenanalyze work that describes howto apply gqmindashboardsdesign.
.
dashboards forsoftwaredevelopment dashboardsincreasecollaborationandmakethecommunication process more transparent since they spread information intheorganization helpingtocontrolprojectstatus findingbottlenecks increasingperipheralawareness andcomparingteams.
dashboardshavealsobeenfoundusefulindistributedteams .in essence properlydesigneddashboardscanbeusedasanextremely effectivetoolforvisualcontrolforthedevelopmentofhigh quality software products .
substantiallytherearethreebasictypesofdashboards strategic operational and analytical .
in the following two basic interaction modes of a dashboard is listed pull and push.
in the pull the user wants to use a dashboard in order to get some specificinformation.inthepush adashboardisusedlikeanotification panel which pushes highly important information to a user triggeringalmostpredefinedfollowup actions.
the types of and mode of interactions with dashboards are strongly connected with their specific purposes.
yigitbasioglu and velcu listthefollowingfourbasicpurposesforcreatingadashboard performancemonitoring measurementconsistency communication and planning.
lastly to collect the data for the dashboard there are mainly two approaches manual and automatic non invasive .
.
designing dashboards andgqm at the end of this analysis it should be clear that creating an effective and useful dashboard is not easy stated simplistically it requires one to select the right data and the right visualization techniques.
the concept of right is indeed hard to define.
a step toward the identification of such data and visualization techniques can be performed using thegoal question metricsapproach .this approachallowscompaniestodefinewhichdatashouldbecollected and how it can be interpreted.
it provides a goal based framework for software measurement.
in the authors formed a gqm model for the dashboard whichfocusesonthesize complexity planning costandquality of the product being created and validated it using a survey of industryprofessionalsusingthelikertscale.respondentsratedthe following metrics lines of code loc function points mccabe s cyclomaticcomplexity.asaresult ofrespondentsconsider these metrics to be the basis for software product measurements noticingfor the fact that locisthe leastsignificant metric.in theauthoraimstodevelopasetofmetricsforsoftware development groups to find out what software development teams should measure to effectively monitor their progress.
the work was conducted in the infrastructure of a real company with agileteams through the collection of requirements joint discussions of requirements for thedashboardfunctionality and thesubsequent use andevaluation ofthe createddashboardinreal work.
the result of the work is measurements in different spheres covering the most important needs of agile team of software developers for effective monitoring of their progress.
there are number ofopendefects numberofestimatedandremaining storypoints number of planned successfully executed tests integration speed.
generalstructureofthe study the studyreported inthis articlehas two main stages .we startwithbuildinga typical gqmneededfordashboarddesign by running a series of interviews with industry.
then we validate theresultusingthesameinstrumentwithanothersetofcompanies.
figure1shows the key stagesofthework.this diagram depicts mainstages gqmmodelgeneration and gqmmodelvalidation .
the paper isstructuredinasimilar way.
figure structure ofthe study .
building a typical gqm through the first step we collected information from software engineers and developers to build a typical gqm from which to derive the dashboard.
there is one proviso we did not pose explicit questions on company goals and strategies to avoid being exposed alsounintentionally tosensitiveinformation rather we remained at a more generic level that allowed us to preserve full confidentiality while stillcollecting everything practicallyneeded.
designinganeffectivequestionnaireforourinterviewisnoteasy since the way questions are defined organized and laid down may significantly influence the overall result.
therefore we followed the rules defined in .
we selected a closed questionnaire usingthetechniquesproposedbyfurnham andofpodsakoff et al.
to minimize the biases in the responses including redundancyandreplication.moreover weemployedthegqmitselfto designthequestionnaire .inotherwords weusedthegqm approach to design a questionnaire aimed at defining a typical gqm .
taking this approach the goalof the investigation was to try to build a typical gqm used in software organizations or a significant subset of it and the typical visualization to use effectivelyin several companiesnothavingtheefforttobuilda fully fledgeddashboard.designandvalidation ofprecookeddeveloperdashboards esec fse november4 lake buena vista fl usa .
validationofgqm afterbuildingthegqmitneedtobevalidated.theresultswere validatedusing aface to faceinterviewswith anotherset ofcompanies.
itconsists of4main parts part1 verifyingtheestablishedrelationshipsbetweenthe indicators andthe goal similar goals dashboardfeatures part2 checking availability ofmetrics incompanies part3 reidentifyingthe goalsofthe developmentprocess part4 collecting demographicdata.
relationshipsbetweenmetricsandgoals inordertoverify therelationshipofmetricstothegoal aprototypeofthedashboard andsyntheticscenarioswereused oneforeachgoal.dashboard isimplemented in aformofan htmlpagewith javascriptinsets for dynamic content changes .
the page contains metrics selected for the study.
each metric is placed in a tile.
three metrics arerepresentedasagraph 10metricsarerepresentedasnumerical values.on thepagethere isa sliderthat isresponsiblefor theday of the iteration.
changing its position changes the state of metrics.
this is necessary to keep track of the change history.
each tile has a color indication to indicate the acceptability of the state of the containedmetric.
this prototype simulates the software development situation byateamof5peopleusingthescrummethodology 10iterations were completed each iteration consists of days.
the task of this prototype is to find out whether the relationship between the metrics andthegoal isobvious toadeveloper and alsotounderstand whichmetrics are the mostrepresentative for this relationship.
figure dashboard interface forinterview features of a dashboard after a personal immersion in the interactionwiththedashboardandtheendoftheanswerstothe questions of the first scenario the respondent was given the opportunity to comment on his her experience of interaction.
the respondent was asked to comment on the interaction point out theextraelementsandwhatfunctionalityhe shelacked.afterthis thetransition to thesecond scenariowas carried outandafterthe completion of which the respondent was asked about the desire to supplementhisthoughtsonthe dashboard.
metrics availability the main source of metrics is the tools used in companies.
the project management tool pmt is one of themainsuchsupplierofquantitativeindicatorsofthedevelopment process.here we askedwhichcompany managementtoolisused.goalsofthedevelopmentprocess to reidentify the existing goals thatareusuallyrelatedtocommonissuesofthedevelopment process weasked thisquestiondirectly.questionsabout thecurrentdevelopmentcycle theplaceofwork thecommunicationteam were asked earlier.
we also asked questions about the validation of already identifiedgoals.
demographics the fourth part is devoted to demographic issues ithelpsto understandhowthe results can be generalized.
.
administrationoftheinstrument to increase the external validity of our findings we have followed thebestpracticessuggestedintheliterature .inparticular we drew interview participants from multiple different companies located in a high tech city and conducted a face to face interviewwitheachparticipant.morespecifically thequestionnairewas firstsenttotheparticipants thenamemberoftheresearchteam interviewed them face to face recorded the results in a written document shared the written document back to the participants to ensure that theright information wascaptured andcorrectedthe resultsifrequired.eachinterviewlastedaboutonehourandthe overallpreparationforitandfollowupcheckrequiredabouttwo hours.
building typical gqmand developer s dashboard .
demographics whenbuildinga typical gqmwehaveinterviewed44companies.
the working experience of respondents ranged from to years withamedianvalueof7 figure a .majorityofthem hadat least a graduate degree.
figure b hold a master degree haveaphd arebachelors and5 havenouniversityeducation.
a workingexperienceoftheparticipants b profiles of the participants figure informationaboutparticipants overall theparticipantswerehomogeneouslydistributedacross the participating companies and they appear a reasonable representation of the overall population of the developers and software engineers present inthe area andaround the world .
.
resulting typical goals themajority answeredthateffortestimationisthebiggest obstacle.whiletheeffectivenessofeffortestimation thatis how effective the estimated effort is can be measured with a metricesec fse november4 lake buena vista fl usa v.ivanov v.pischulin a.rogers g.succi j. yi andv.zorin g1 more effective effort estimationg2 more efficient use of resourcesg3 better software quality and development process q1 what is the level of collaboration with customers?q2 how adherent is the project with the original plan?q3 how many features have been implemented so far?q4 what is the quality of the project?q5 is the consumption of resources adherent to the budget?
m1 progress state of the projectm2 speed of the work performedm3 status of testingm4 status of software qualitym5 effectiveness of effort estimation iteration burndown chart release burndown chart cumulative flow diagram of feature cut team velocity average cycle time schedule variance amount of tasks unit of time lead time code coverage of passed tests defect removal efficiency of detected defects of unresolved defects defect density coupling class method length system spoilage effort estimation accuracy figure the resulting typical gqm model where strong labelsandhigh frequencymetrics are depicted inboldface indeed ourgqmmodelcontainseffortestimationaccuracyasone ofitshigh frequencymetrics effortestimationitselfiscurrently conducted in a rather ad hoc way relying on the experience of the developers.
given this situation we find strong need from the industry for more systematic data based effort prediction further research on this area is warranted.
overall we summarize our first step results as follows based on our results we have extracted a typical gqm model shown in figure consisting of three goals more effective effort estimation g1 more efficient use of resources g2 better software quality and development process g3 five questions five metric categories.
we have alsonoticedthatourinformantsshowlessinterestinsoftwarequalitythan otherkindsofmetrics.meanwhile ourinformantsshowedthestrongest interestin iterationburndown charts .
.
questions we have identified the following typical questions three strong questions and two moderate questions.
we consider a question strong ifthequestionisidentifiedbasedonahigh frequencyanswer.
q1.whatisthelevelofcollaborationwithcustomers?
this question mainly reflects the importance of customer satisfaction .thisquestionalsocoversnumberofchangerequests andteam velocity taskswiththe biggestcycletime .
q2.how adherent is the project with the original plan?
this question reflects the importance of delivering a product on time percentage of implemented features and amount of unimplemented tasks or user stories .
this question also covers schedule variance and number of overdue tasks andteam velocity .
q3.how many features have been implemented so far?
this question reflects the importance of implementation of features percentageofimplementedfeatures andamount of unimplemented tasks or user stories most critical defects .
q4.what isthequalityoftheproject?
thisquestioncovers quality relatedresponsessuchasmeetingqualityandsafetystandards qualitymetrics andmostcriticaldefects .
this question also covers reports of crashes and other problems percentageofpassedtests andtaskswiththebiggest cycletime .q5.is the consumption of resources adherent to the budget?it covers budget related responses such as staying within the budget supported over budgeted tasks cost performance index costvariance andteam velocity .
.
metrics we have identified the five distinct kinds of metrics metrics to show the current progress state of the project metrics to show the speed of workperformed metrics to show thestatus oftesting metricstoshowthestatusofsoftwarequality and metricstoshowtheeffectivenessofeffortestimation.wehavealso connected these metrics with the goals developers want to achieve throughagqm model .
we have observedthat ingeneral developersaremoreconcernedaboutmonitoringwhetherthedevelopment process is on track than monitoring software quality.
webelievethatwiththiscategorization softwareengineerswillbe abletounderstandmoreeasilywhatkindsofmetricsothersoftware engineersaretypicallyinterestedin thanwhentheysimplyread throughthe listofmetrics.
m1.progressstateoftheproject thismetriccategorymainly representsiterationburndowncharts oftable .italsocovers release burndown charts and cumulative flow diagram oftable1 and percentageoffeatures cutduringtheproject oftable3.
m2.speed of theworkperformed this metric category concernsthedynamicaspectofthesoftwaredevelopmentprocess that is howfastprogressismadeovertime.thiscategorymainlyrepresents team velocity of table .
other metrics that can be includedinthiscategoryare averagecycletime oftable andschedulevariance amountoftasksperunitoftime andleadtime oftable .
m3.status of testing thismetriccategorymainlyrepresents code coverage of table .
other metrics that can be included inthiscategoryare percentageofpassedtests oftable and defectremovalefficiency of table .
m4.statusofsoftwarequality thesemetricsincludenumber ofdetecteddefects andnumberofunresolveddefects of table1 defectdensity coupling andmethodorclass length oftable andsystemspoilage of table .
m5.effectivenessofeffortestimation thismetriccategory covers effort estimation accuracy of table .
the previous metric categories do not sufficiently capture this high frequency metric and we separate this single item category from the rest.
developerswantadashboardtobeabletonotifythemwhenthe developmentprocessisonthewrongtrack.however wehavealso spotted the difficulty of supporting this notification effectively it is difficult to picture the right track when developers are currently having difficulties in estimating the effort necessary to complete a task.
for this reason we argue that research on effort prediction deserves strongattention.
validation ofgqmand dashboard .
demographics asaresultoftheinterviews responsesfrom30differentcompanies werecollected.mostofrespondents haveabachelor sdegree designandvalidation ofprecookeddeveloperdashboards esec fse november4 lake buena vista fl usa table metrics forsoftware development process metric frequency iterationburndowncharts team velocity number ofdetecteddefects number ofunresolveddefects percentageofpassedtests release burndowncharts code coverage cumulative flowdiagram leadtime averagecycletime table metrics forsoftware quality metric frequency code coverage defectdensity coupling method class length lack ofcohesion ofmethods lcom table metrics forprocess quality metric frequency effortestimationaccuracy schedulevariance amountofcompletedtasksper unitoftime defectremovalefficiency systemspoilage percentageoffeatures cut duringthe project hold master s degree and a are phds.
work experience differsfrom1to30yearswiththemedian5years.themajorityare men arewomen.fiftyfivepercentofrespondentsworkin micro companies less than employees in large companies morethan500employees inmedium sizedcompanies 250to 500employees and11 insmallcompaniesemployees from10to .
percentage of respondents that apply a methodology based on agileprinciplesis65 while19 ofrespondentsworkonscrum methodology and only use waterfall methodology use othermethodologies.almostahalf ofrespondentsworkin ateamoflessthan5 inateamof6to10people and9 ina teamof11to15people.sincetherespondentcancombineposts the distribution of the roles ofthe respondentsis asfollows of respondents are developers analysts technical leaders and13 one ofthe scrum master managerortester.
.
typical gqmvalidation in the first part of the survey we were checking the relationship between metrics and goals.
validation of goal g2did not give enoughresultsfor analysisduetorespondents lackof awarenessofthecompany sbudget.therefore wedonotconsiderthegoal g2inthis section.
forthegoal g178 ofrespondentsrecognizedtheproblemwith the assessment of efforts.
the key metric for of respondents was the iteration burndown chart mentioned the effort estimationaccuracy focusedonthe teamvelocitychart .
amongthosewhorecognizedtheproblem79 confirmedthatthey experiencedasimilarsituationinthelast5iterations.thisconfirms theimportanceoftheestimationofeffortinthegqmandinthe corresponding dashboard.
relevant metricsfor this goal are based on the iteration burndown effort estimation accuracy and team velocity listedinthe order of importance .
forg3we got the following result.
during the interviews ofrespondentsrecognizedtheproblemwiththequalityofthecode.
thus all respondents confirmed the importance of the goal related to the quality of the code and the corresponding problem.
the distributionofakey metric isthe following passedtests code coverage unresolveddefects class methodlength iterationburndownchart defectremoval detecteddefects anddefectdensity .
amongrespondentswhorecognizedtheproblem60 confirmed that they had experienced similar situation in last iterations.
the averagelead timeand averagecycle timemetricswerenottaken into account due to the impossibility of their interpretation by the respondents.
in the second part of the interview we were checking theavailabilityofmetrics inreal world environment.
indeed validationofthetypicalgqmdependsontheavailabilityofmetrics.
respondentswereaskedaboutthepmtandaboutsoftwaresolutionsforqualitycontrolofthecode.developers usethefollowing toolsformetricscollection jira youtrack trello team foundation server and of respondents were using othersoftwareproducts.forcodequalitytrackingrespondentsuse sonar coverage.py differentproducts and11 ofrespondentsuse notools.
.
dashboard validation inthefirstpartoftheinterview therespondentsinteractedwith theprototypeofthedashboard seefigure .itdisplayed13metrics iteration burndownchart team velocity release burndown chart averagecycletime averageleadtime codecoverage passedtests defect removal efficiency defected density class method length effortestimationaccuracy.eachmetricwasplacedonaseparate tile.afterinteractionwiththedashboard respondentwereasked thequestion whatwouldyouliketoaddtothefunctionalityof the dashboardandwhy?
.the answers were as follows history andforecastedchangeinthe metric ability to view metrics for an individual team member possibility offiltering andaggregating metrics explanationofpossiblereasonsforchangingthemetric indicationofthe number of completedtasks indicationofthe remaining time of the iteration displayingahintaboutactionsinthecurrentsituation esec fse november4 lake buena vista fl usa v.ivanov v.pischulin a.rogers g.succi j. yi andv.zorin the respondents also reported that the function of displaying critical changes implemented in the prototype was most useful during the decision making process.
thus the functionality of thedashboard shouldhave the following objectives coherentand complementary to the outcomes ofthe firstpart ofthis research viewchanging metric predict the changeinthe metric displaycriticalvaluesofmetrics filterandaggregate metrics recommendaction onthe situation displayprogressofwork.
validityofresults inthecontextandwiththeintrinsiclimitationsofanobservational study run with an empirical constructivist approach we claim that itisreasonabletogeneralize somehow theresultsofthisstudy beyondour study.
such a claim indeed is loose since all the work focuses mostlyonobservational analysis anddoes not employ anyinferential statistics to assess the significance of the results problem of internal validity using a questionnaire with closed answers exposes us to the risk of the respondent bias problem of construct validity the respondents were self selected and no randomization process occurred problem of external validity however wedothinkthatourfindingshavethepotentialtobewidespread to a wider population since we took precautions to mitigate the above three classes of threats to validity even if it is not possible toeliminatethem.ourworkisafirstobservationalstudynotemploying inferential statistics thus the issue of the significance of thefindingsisimportantbutnotessential.still onlyreplications to this study would make our results stronger and for this purpose we are ready to share our instrument to any interestedresearcher.
in particular we conducted the validation of the gqm and the precookeddashboard.
conclusions inthisstudy wehaveintervieweddevelopersfromvariouscompanies inanattempttoobtaininformationnecessarytobuildan effective developer dashboards.
we have identified five distinct kindsofmetrics metricstoshowthecurrentprogressstateof the project metricsto show the speedofwork performed metrics to show the status of testing metrics to show the status of software quality and metrics to show the effectiveness of effort estimation.
we have observed that in general developers are more concerned about monitoring whether the development processisontrackthanmonitoringsoftwarequality.developers want a dashboard to be able to notify them when the development process is on the wrong track.
we have connected these metrics with the goals developers want to achieve through a gqm model.
finally we have validated thetypical gqm using interviews with aseparate setofrespondents.
acknowledgment the authors thank innopolis university for funding this study and victor basili for providing very valuable feedback on an earlier versionofthis manuscript.