automated trainability evaluation for smart software functions ilias gerostathopoulos stefan kugele christoph segler tomas bures and alois knoll department of informatics technical university of munich garching b. m nchen germany email ilias.gerostathopoulos stefan.kugele tum.de knoll in.tum.de bmw group research new technologies innovations garching b. m nchen germany email christoph.segler bmwgroup.com charles university in prague czech republic email bures d3s.mff.cuni.cz abstract more and more software intensive systems employ machine learning and runtime optimization to improve their functionality by providing advanced features e. g. personal driving assistants or recommendation engines .
such systems incorporate a number of smart software functions ssfs which gradually learn and adapt to the users preferences.
a key property of ssfs is their ability to learn based on data resulting from the interaction with the user implicit and explicit feedback which we call trainability .
newly developed and enhanced features in a ssf must be evaluated based on their effect on the trainability of the system.
despite recent approaches for continuous deployment of machine learning systems trainability evaluation is not yet part of continuous integration and deployment cid pipelines.
in this paper we describe the different facets of trainability for the development of ssfs.
we also present our approach for automated trainability evaluation within an automotive cid framework which proposes to use automated quality gates for the continuous evaluation of machine learning models.
the results from our indicative evaluation based on real data from eight bmw cars highlight the importance of continuous and rigorous trainability evaluation in the development of ssfs.
index terms trainability smart software functions continuous deployment i. i ntroduction more and more computing systems feature advanced software enabled functionalities that make them be perceived as smart by their users.
smartness is achieved via various machine learning and runtime optimization capabilities that aim to increase the value delivered to the users.
indeed seen as the ability to learn and improve over time smartness is increasingly a must have property of many modern systems.
in an attempt to understand and aid the development of smart systems we coin the term trainability to describe the key quality property of smart systems the ability of a system to accurately quickly cost effectively and robustly learn users behaviors and preferences with the goal to maximize the value delivered to them.
in a way trainability is the opposite notion oflearnability which focuses on the ability of users to learn to use a system .
as an example from the automotive domain consider a windscreen wiper that supports two modes of operation the research leading to these results has received funding from the ecsel joint undertaking ju under grant agreement no .
i interval based i. e. there are three levels with fixed intervals between wipes and ii automatic i. e. the intervals are computed based on rain intensity.
in practice customers perform in either case additional manual wipes.
a smart windscreen wiper would try e. g. via reinforcement learning to adapt to the driver s preferences and minimize the number of manual interventions.
its trainability depends on how quickly accurately and effectively it learns such preferences.
considering trainability as an important quality property of smart systems along with traditional properties of software systems such as availability maintainability and performance two important questions arise first how can we quantify trainability so that different versions of a system can be compared on their effect on this property?
second how can we assess the effect of software development and evolution integration of new or enhanced features on the trainability of a smart system?
here we are particularly interested in automation that allows for quickly and reliably providing feedback to developers regarding the effect of their software changes on the trainability of the system under development.
to answer the above questions in this paper we contribute by i elaborating on five key facets of trainability solution quality convergence overhead robustness and effect and associated metrics for their evaluation ii reporting on our on going attempt for embedding trainability evaluation in the development of smart systems in the automotive domain.
ii.
t rainability facets and metrics in this work we propose five score trainability facets that are of particular interest and importance during the development of smart systems solution quality i. e. what the quality of the learned model or optimized function is convergence i. e. how efficient the learning is in terms of time or algorithmic steps to reach its goal overhead i. e. what the cost of learning and execution is in terms of resources e. g. memory cpu robustness i. e. how stable the quality of the learned model or optimized function is when trained with different input data or under different system conditions 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
t able i metrics of solution quality trainability facet technique metrics stochastic search arrowhookleft single objective distance from global optimum distance from best known solution ... arrowhookleft multi objective hypervolume inverted generational distance generated spread euclidean distance from ideal solution ... supervised learning arrowhookleft classification accuracy balanced accuracy precision recall f1 score area under the roc .
.
.
arrowhookleft regression root mean squared error rmse mean absolute error coefficient of determination r2 ... unsupervised learning arrowhookleft clustering internal silhouette coefficient dunn s index .
.
.
external adjusted rand index jaccard index .
.
.
effect i. e. what the effect of the learning is on the actual customer satisfaction and adoption.
solution quality can be quantified via various existing metrics tab.
i .
the selection of one or more metrics depends on the specific technique used to develop a smart system.
for instance in systems employing single objective stochastic optimization the distance of the reported solution to the optimal one or to the best so far known solution can be measured .
similarly in multi objective optimization different quality indicators can be used e. g. hypervolume indicator .
v arious well known metrics exist also for the evaluation of classification e. g. accuracy f1 score and regression e. g. rmse algorithms.
finally clustering algorithms are typically evaluated using internal e. g. silhouette coefficient or external e. g. jaccard index metrics depending on the availability of ground truth .
convergence refers to the time to reach an acceptable potentially optimal solution and can be quantified either via the number of steps or iterations an algorithm needs to reach this solution or via the computation time which is implementation specific.
similarly overhead refers to the computational or memory resources that are needed for the solution to be executed and is highly implementation specific since specific implementations of algorithms incur more overhead than others.
robustness refers to the differences in solution quality when training a machine learning model with different input data or optimizing a function under different operational conditions with higher robustness leading to smaller differences.
robustness can be measured via evaluating the quality of multiple runs of the smart system.
the challenge is how to generate different input datasets or ensure that the system goes through diverse operational conditions.
finally effect refers to the value delivered to the users as a result of enhanced functionality.
though this alone is an important property of any function we consider it also an important facet of trainability since it is particularly difficult to assess how smart a system should be in order to be accepted by the potential users.
effect can be measured only by deploying the smart system to real cars and collecting feedback from users e. g. usage rates ratings .
the score facets and their analysis above show thatqg2 qg0 s sfu i o a qg2 qg0 s ?
sfu i o b qg2 s sf i o c fig.
.
reference architecture of a ssf with user u actions predicted user action inputi and output result o a training phase b test phase the component ?compares the user action with the predicted action .
c the ssf sis active and controls the sf.
t local testqg0 integ ration testqg1 deplo yt o employeesqg2 deplo y var.
to empl.deplo yt o customersqg3 push fig.
.
trainability evaluation framework for automotive cid.
trainability can be considered as a universal property of smart systems that employ machine learning and self optimization.
iii.
t rainability ev alua tion framework in our initial attempt to assess the effect of software development on the trainability of a smart system we focused onsmart software functions ssfs in the automotive domain.
in automotive ssfs are meant to enhance drivers and passengers comfort by learning and automating certain manual tasks such as turning on off seat heating opening closing the driver s window and changing the driving mode to sport comfort.
fig.
depicts the reference architecture of a ssf in three different phases.
in the training phase cf.
fig.
1a the user u is interacting with the classic non smart software function sf .
based on these user actions the smart learning component denoted as s trains a classifier to predict when to apply a user action based on vehicle data e. g. predict start seat heating when the temperature is low .
in the testing phase cf.
fig.
1b the architecture changes since snow outputs predicted user actions that are compared to the actual user actions which are still used for controlling the sf.
based on the predicted and actual actions we can measure certain trainability facets without interfering with the actual system.
in the active phase cf.
fig.
1c the ssf is completely active and the predictions from sare now used for controlling sf.
given the above reference architecture and phases we propose to evaluate the trainability of different revisions of ssfs using a continuous integration and deployment cid approach that provides fast and reliable feedback to the developers on the effect of their changes on the score trainability facets cf.
fig.
.
in particular the following four quality gates qg ensure that only revisions satisfying certain trainability criteria proceed to the next stages.
qg0 in this step the developer follows guidelines that prescribe which trainability criteria should be satisfied before pushing a revision.
for example a criterion could be that the classification accuracy of sshould be at least and not authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
lower than the accuracy of the previous revision.
obviously the validity of such checks depends on the availability of data that can be accessed locally by the developer.
in general we assume that the developer has access to a subset of historical vehicle data in order to develop their ssfs.
qg1 this qg gets activated once a revision is pushed to the cid pipeline.
in this step the ssf can only run in the training and test phase due to the execution on recorded datasets without any user in place yet cf.
figs.
1a and 1b .
here apart from the solution quality of s its robustness and overhead are also tested.
for instance robustness can be checked by measuring the standard deviation of a solution quality metric such as accuracy.
overhead can also be realistically measured in terms of cpu cycles or ram used in training and testing since hardware in the loop testing environments are present.
concretely listing shows a possible specification of the checks to be performed in qg1.
first the specification defines the type and size of input data to be used in the training and testing phases of s. data can be retrieved belonging to either users or cars type and over a prescribed duration.
for example listing specifies that datasets from users driving their car for days should be used.
each dataset should be split into folds and training should be performed using all the possible combinations of consecutive folds fold validation and tested on the remaining fold.
this creates tests.
here the metric of solution quality is balanced accuracy which can measure the accuracy even in imbalanced datasets that are common for ssfs as some functions are only rarely used.
this value has to be larger than for each test to pass.
for the solution quality facet to be satisfied more than half of the total tests should pass and the number of passing tests should not be less than the equivalent number in the previous revision.
the method shown in this example is just a simple way to identify trainability regressions also more sophisticated methods e. g. t tests can be easily specified.
in this case the specification also denotes that in each test the size of the classification model should not exceed mb in order to satisfy the overhead criterion.
finally as a criterion for robustness the standard deviation of balanced accuracy values from all tests should be less than .
.
note that convergence cannot be evaluated at this step since this can only be done with an increasing amount of data and for the effect an interaction with a real user is necessary.
qg2 once the revision passes from qg1 it gets deployed on company cars that are driven by employees of the automotive company which can be considered as beta testers in this setting.
we distinguish between deployment of a single revision and deployment of two or more revisions that contain variants of a ssf that should be compared against each other via a b testing .
for the latter case we assume that an online experimentation module decides which variants are deployed on each car and is responsible for gathering data for each deployment variant.
at this step along with the other facets it becomes possible to evaluate the convergence and effect facets of a revision.
also it now becomes possible to test not only sbut the whole ssf in the following way first theinput type user count recorded time days train test split fold solution quality balanced accuracy each value .
all passed count .
total count all passed count passed count previous revision convergence not applicable for qg1 overhead model size each value mb all passed count total count robustness balanced accuracy all standard deviation .
effect not applicable for qg1 listing .
definition of qg1.
input type user count evaluation days train test split active days filter department company model company solution quality inherited from qg1 convergence training time each evaluations value evaluations overhead inherited from qg1 robustness inherited from qg1 effect percentage used each value .
all passed count .
total count listing .
definition of qg2.
ssf is in passive mode cf.
figs.
1a and 1b and goes through a number of evaluations where sis first trained for a certain amount of time and then tested.
after a number of evaluations sconverges to the acceptable level of solution quality.
at this point ssf changes to active mode cf.
fig.
1c whereby sis used as input for user actions and the effect of the revision is evaluated.
all evaluation results are sent to the company cid servers to allow for testing the metrics of qg2.
concretely listing specifies the case where a revision should be tested with employees driving a specific car model and working in a specific department.
here a single evaluation takes days and is split in days of training time and day of testing time.
the convergence constraint is that sshould reach the solution quality criterion of as specified in the previous qg within at least days and at most days.
after this the system should stay in active mode where the effect can be measured for days.
in this case the effect is measured by the percentage of time ssf is on and not manually deactivated by the user for more than for at least of the users .
qg3 in the last step a single variant if many of the ssf revision is deployed to customer cars.
the approach to trainability assessment is very similar to the previous step since the ssf should again undergo a train test active cycle to make the evaluation of all the score facets possible.
here though there is also the opportunity for longer active phases and continuous monitoring of effect metrics.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
iv .
d emonstra tion and indica tive ev alua tion in this section we demonstrate our approach on the development of an automatic window opener awo .
awo predicts when to open or close the driver s window based on the user s actions in certain situations delineated by car data such as inside outside temperature road type or location of the car key.
we assume that the development of awo goes through two revisions in the first revision sis trained using a support v ector machine svm classifier on all the available car data as input in the second sis trained with the same classifier but in this case based on inputs from the car data selected by a feature selection algorithm here fisher score .
for our demonstration we configured qg1 as specified in listing .
we assume that the developer has access to a single user dataset when developing and testing locally qg0 while qg1 has access to user datasets.
these datasets originate from eight bmw cars that have been equipped with hardware loggers collecting all messages which were sent over the internal communication networks.
as specified in listing each dataset contains vehicle data recorded over days.
when the first revision svm without feature selection is pushed to the cid pipeline out of tests passed the solution quality criterion of balanced accuracy above and only out of tests passed the overhead criterion of model size below mb thereby the revision does not pass qg1.
awo is then refined second revision and tested locally qg0 .
after passing qg0 this revision is pushed to the cid pipeline.
this revision now passes out of tests and hence satisfies the solution quality criterion.
at the same time all runs pass the overhead criterion and the robustness checks since the model size of each run is below mb and the standard deviation of the balanced accuracy values is .
which is below .
as specified in the robustness.
since all facets are satisfied the revision passes qg1.
our indicative evaluation shows that the different steps and qgs in the proposed cid pipeline were able to quickly detect the poorly performing first revision and prevent it from being deployed to real cars.
we also show that it is feasible to capture all the score facets in our framework and evaluate each one of them when and where it is most applicable.
v. r ela ted work to the best of our knowledge there is only little work on cid pipelines in the automotive domain.
v st and wagner report on a test selection approach supporting ci.
the authors consider non smart sfs in particular the adaptive cruise control and show a reduction of executed tests.
their approach covers qg1 for non smart functions.
knaus et al.
and van der v alk et al.
highlight the challenges of ci pipelines in the automotive development process.
they underline the importance of tools and automation especially in the building and integration stages going along with the presented cid approach of fig.
.
moreover they highlight the importance of contracts within an automotive ecosystem.
the proposed notion of trainability can be seen in contrast to the learnability notion in software quality characteristics.in particular iso iec defines learnability as the degree to which a product or system can be used by specified users to achieve specified goals of learning to use the product or system with effectiveness efficiency freedom from risk and satisfaction in a specified context of use .
likewise we consider trainability as a quality attribute of a ssf.
however trainability features the ability of a ssf to learn user preferences and not the ability to be learned.
similarly to renggli et al.
we use the y aml file format to specify quality gates facilitating an embedding in commonly used ci cd systems such as travis ci.
in comparison we do not only focus on machine learning models but generalize to a variety of solution quality trainability facets holistically.
moreover the proposed cid approach does not only consider early phases but spans the whole product lifecycle from development to deployment of ssfs to customers.
vi.
c onclusion in this paper we proposed the notion of score solution quality convergence overhead robustness effect to define the different facets of trainability for the development of smart software functions.
based on these trainability facets we presented our approach for automated trainability evaluation within an automotive continuous integration and deployment framework.
we demonstrated the applicability of our approach on the development of a smart function in the automotive domain using real vehicle data.
in future work we would like to introduce more metrics in each facet of trainability and evaluate qg2 and qg3 on real users.