what parts of your apps are loved by users?
xiaodong gu and sunghun kim department of computer science and engineering the hong kong university of science and technology hong kong fxguaa hunkimg cse.ust.hk abstract recently begel et al.
found that one of the most important questions software developers ask is what parts of software are used loved by users.
user reviews provide an effective channel to address this question.
however most existing review summarization tools treat reviews as bags of words i.e.
mixed review categories and are limited to extract software aspects and user preferences.
we present a novel review summarization framework surminer.
instead of a bags of words assumption it classifies reviews into five categories and extracts aspects in sentences which include evaluation of aspect using a pattern based parser.
then surminer visualizes the summaries using two interactive diagrams.
our evaluation on popular apps shows that sur miner summarizes more accurate and clearer aspects than state of theart techniques with an average f1 score of .
significantly greater than that of reviewspotlight .
and guzmans method .
.
feedback from developers shows that developers agreed with the usefulness of the summaries from sur miner.
index terms review summarization user feedback sentiment analysis data mining i. introduction often software developers are eager to know what parts of their software is used loved by users.
according to a survey covering microsoft engineers the question what parts aspects of a software product are most used and or loved by customers?
ranks the second among the top questions developers asked .
this question requires developers to analyze preferences for and opinions toward different software aspects.
user reviews are an important channel for software developers to understand users requirements preferences and complaints .
through analyzing user reviews developers can evaluate their products identify users preference and improve software maintenance and evolution tasks .
yet understanding software reviews is very challenging and tedious.
first the volume of user reviews is too large to be checked manually.
developers receive hundreds or thousands of reviews every day .
given the large number of reviews they need to read and manually classify the reviews into complaints or new feature requests .
such processes are extremely time consuming and tedious.
on the other hand user reviews fall into too many varieties that need to be distinguished .
they can be new feature requests bug reports praises or complaints.
different types of reviews target different tasks and developers .
for example a praising review may not be valuable for software testing but can be 1this question is from a study by begel et al.
at microsoft essential for product evaluation.
a review reporting a bug is not important for requirements analysis but can be crucial for software testing.
given millions of reviews developers must first categorize them manually .
a few tools are proposed for software user review summarization.
for example chen et al.
filter non informative reviews by a classification technique and apply latent dirichlet allocation lda to summarize topics of the informative reviews.
fu et al.
filter rating inconsistent reviews which have sentiments different from their rating by a regression model.
they also apply lda to summarize topics in the remaining reviews and show rating trends for different topics.
iacob et al.
filter reviews that request new features by linguistic rules and summarize key words of the requests with lda.
these tools summarize informative and reliable reviews.
however the lda model that they used is based on a bag of word assumption without considering sentence structures and semantics.
such assumption may be problematic for software reviews which exhibit multiple purposes e.g.
aspect evaluation and feature request and sentiments.
since these tools mix up aspects and opinions and mix topics related to different categories they are not effective to gauge users sentiments toward each aspect.
to address these limitations we propose software user review miner sur miner a framework that can summarize users sentiments and opinions toward corresponding software aspects.
instead of treating reviews as bags of words surminer makes full use of the monotonous structure and semantics of software user reviews and directly parses aspectopinion pairs from review sentences based on pre defined sentence patterns.
it then analyzes sentiments for each review sentence and associate sentiments with aspect opinion pairs in the same sentence.
finally it summarizes software aspects by clustering aspect opinion pairs with the same aspects.
we empirically evaluate the performance of sur miner on recent user reviews of android apps such as swiftkey camera360 wechat and templerun2.
we measure the performance of key processes i.e.
classification aspect opinion extraction and sentiment analysis by f1 score which is a common accuracy measure in the text mining literature .
results show that the sur miner produces reliable summaries with average f1 scores of .
.
and .
for review classification aspect opinion extraction and sentiment analysis respectively.
the final aspects from sur miner are significantly more accurate and clearer than state of the art techniques with an f1 score of .
greater than that of reviewspotlight .
and guzmans method .
.as a proof of concept application we design two interactive diagrams aspect heat map and aspect trend map using the summaries from sur miner to help developers grasp users preferences and typical opinions towards each software aspect.
feedback from corresponding app developers is also encouraging with of respondents agreeing that the summaries of sur miner are useful indicating that sur miner helps developers understand users preferences for different aspects in practice.
overall our study makes the following contributions we leverage a classification technique in which we designed text features to distinguish five review categories such as bug reports and new feature requests.
we propose a pattern based parsing technique which can parse complex app review sentences and extract aspects and corresponding opinions.
we design novel interactive visualizations to present summaries efficiently for app developers and managers.
we conduct an empirical evaluation of sur miner to investigate its usefulness.
the rest of this paper is organized as follows.
section ii presents the related work.
section iii presents the detailed design of our framework.
section iv presents the evaluation.
section v discusses the threats to validation and section vi concludes the paper.
ii.
related work a. app review filtering app review filtering has drawn increasing attention in the software engineering community.
chen et al.
filter noninformative reviews and rank the user review by significance.
their framework trains a classifier and categorizes reviews into two classes namely informative and non informative.
fu et al.
filter rating inconsistent reviews reviews that have sentiments different from their rating by a regression model on the review vocabulary.
these tools can partially select informative reviews.
however they do not define clearly under which circumstances reviews are informative since different developers need different types of reviews .
to a further step of their work we aim at distinguishing different review purposes categories and selecting reviews from a specific category to extract and summarize software aspects.
recent work by sorbo et al.
proposes a similar idea to classify development emails according to their purposes.
they also design a classification approach using natural language parsing techniques.
while their technique could also be applied for app review classification it does not support aspect summarization within each category.
b. aspect extraction from app reviews aspect extraction also has been widely investigated in software engineering.
chen et al.
use lda to extract topics of reviews.
hu et al.
propose a method for web review mining.
their method extracts frequent words as aspects and link corresponding adjective words as opinions.
fu address the problem of mining users negative feedback.
theyapply lda topic model to mine topics from negative feedback and rank the summarized problems for each release.
galvis et al.
mine requirement changes by adapting a topic model named aspect and sentiment unification model asum .
they also extract common topics and present users opinions toward those topics.
however their approaches differ significantly from ours.
they applied frequent items mining or topic models which are based on a bag of word assumption without considering sentence structures and semantics.
that means they can distinguish neither review categories praising feature requests bug reports shortcomings nor aspects and user opinions which could result in inaccuracy and confusion.
for example a topic word prediction extracted by lda may mean that users appreciate the prediction feature or alternatively users wish for a new prediction feature.
in such case developers cannot efficiently interpret the topics.
recent work by sarro et al.
extracts features from app descriptions using natural language processing .
our work differs to theirs in that we extract features from app reviews.
besides we aim at summarizing app features while their goal is to investigate feature lifecycles .
to our best knowledge there is only one previous work that are closely related to ours.
guzman and maalej proposed to extract software features and analyze their sentiments.
our work differs from theirs in three major aspects.
first our approach aims not only to identify features but also to distinguish feature evaluations and feature requests.
second our approach can identify complex and novel features since it parses review sentences with semantic patterns while their techniques are based on frequent item mining and topic model as traditional approaches did.
finally we propose interactive visualizations to help app managers and developers grasp the feature evaluations and sentiment trends.
c. review mining in other marketplaces user review mining is also an attractive topic in other marketplaces e.g.
commodity goods movies .
yatani et al.
proposed a review summarization tool called reviewspotlight which extracts aspect opinion pairs by identifying adjectivenoun word pairs from review sentences.
huang et al.
adopted a similar idea and designed revminer an extractive interface for summarizing restaurant reviews.
nichols et al.
proposed recloud which parses review sentences with nlp techniques.
zhuang et al.
studied movie review summarization.
their approach integrates multiple knowledge including wordnet statistical analysis and movie knowledge.
however these techniques can hardly be applied to app reviews directly.
app reviews are quite different from reviews in other marketplaces .
they have different lexicons and formats that existing tools can hardly parse.
the reviewspotlight presents a word cloud by extracting adjective noun word pairs.
likewise the revminer extracts word pairs using a bootstrapping algorithm.
the recloud takes semantic into consideration it also presents a word cloud but with a spatial layout reflecting the nlp context.
however app reviews cannot simply be represented by word clouds or word pairs.
for example case i love the fact that we can change themes raw reviewsaspect evaluation praises bug reportsfunction requests othersaspect opinion pairs 2classification3aspect opinion extraction 4sentiment analysis aspect opinion sentiment pairsreview sentences1pre processing 5aspect clustering aspect groups6visualization aspect heat map aspect trend mapfig.
overview of the proposed sur miner framework the reviewspotlight cannot output anything since there is no adjective.
the revminer and recloud may present some meaningless word pairs.
in contrast sur miner can present the correct pairhwe can change themes love ias it considers semantics and app review patterns.
in addition app reviews contain multiple purposes that target different developers .
none of existing tools can distinguish such categories.
consider the following cases case the blue screen after clicking the ok button is annoying.
case a simple ui would be better.
from developers perspective they are just a bug report and a feature request and should not be considered as users opinions toward screen and ui .
such cases account for a large proportion in app reviews .
while all these tools still output word pairs cloud such as hbutton annoyingiand hui simplei sur miner can distinguish the above cases as it leverages a classification technique.
iii.
sur miner this section introduces the generic architecture of surminer.
as illustrated in figure our framework takes user reviews including texts and ratings as inputs and outputs the main opinions and sentiments toward different aspects of the app.
the whole procedure consists of six main steps for raw reviews that need to be summarized we first split them into sentences step .
then we classify each sentence into five categories namely aspect evaluation praises feature requests bug reports andothers step .
then we only select sentences in the aspect evaluation category and filter out other types of sentences.
we then extract aspects and corresponding opinions and sentiments from the set of aspect evaluation sentences step .
the resulting aspect opinion sentiment pairs are clustered and visualized with two interactive diagrams step .
each step is explained in detail herein below.
a. step preprocessing the raw user review needs preprocessing.
it often consists of more than one sentences with different purposes.
for example a raw review the ui is ugly.
i want a beautiful ui consists of two sentences.
the first sentence is an evaluation of an aspect ui while the second is a request for improvement in the aspect ui.
they have different purposes and sentiments.
therefore it would be desirable to separate these sentences for analysis.
furthermore user reviews have many typos andcontractions which make it hard to understand the meaning automatically.
to address these two issues we split the raw review text into sentences using the stanford corenlp tool .
each review sentence is time stamped and assigned rating to be the same as in its raw review.
we also correct common typos contractions and repetitions such as u !you coz!because !and plz!please soooo!so and thx!thanks .
we collected such typos and contractions and replaced them with regular expressions2.
b. step review classification as discussed in section i review sentences may have different categories .
different categories target different tasks and developers .
it is very tedious and time consuming for developers to manually classify them and select appropriate sentences for aspect evaluation.
in the review classification step we aim to automatically classify and select review sentences which contain aspect evaluation.
we define five review categories including aspect evaluation bug reports feature requests praise andothers .
pagano et al.
found categories topics of user reviews .
we use top four categories from their taxonomy and merge other minor categories into an others category.
table i illustrates the definitions and sample review sentences for each category.
to classify review sentences into the above mentioned categories we follow a supervised machine learning approach.
we first collect historical review sentences extract their text features and manually label them according to the definitions in table i. then we train a classifier using these text features and labels.
finally we execute the classifier on new review instances to predict their categories.
we adopt a well known classifier max entropy which has great performance on text classification .
in the following we present the text features that we designed for classification.
text feature extraction we extracted two dimensions of text features lexicon features and structural features.
lexicons are important to characterize review categories since different review categories may have significantly different lexicons.
for example amazing and great appear 2the full list of typos is at xguaa srminer appendix.
htmltable i definition of five review categories category definition examples praise expressing emotions without specific reasonsexcellent!
i love it!
amazing!
aspect evaluation expressing opinions for specific aspectsthe ui is convenient.
i like the prediction text.
bug report reporting bugs glitches or problems it always force closes when i click the .com button.
feature request suggestions or new feature requestsit would be better if i could give opinion on it.
it s a pity it doesn t support chinese.
i wish there was a deny button.
others other categories that are defined in i ve been playing it for three years frequently in praising reviews while bug and fix are representative words for bug reports .
we choose character ngram andtrunk word as two lexicon features since they reflect lexicons of different categories.
character n gram character n gram an important lexical representation is a commonly used feature in text classification .
it has also been found to be effective in many applications such as malicious code detection and duplicate bug report detection in software engineering.
character n gram features for a sentence are all n consecutive letters in the tokens of that sentence.
for example the grams for the sentence the ui is ok are the heu eui uii iis iso andsok.
we use grams for classification.
trunk words we also propose trunk word as a lexicon feature.
we define trunk word as the word at the root of a semantic dependence graph which is introduced later in this section.
for example the trunk word of the sentence the graphics are amazing is are .
sentence structures can also reflect text features as different review categories may have different syntax and semantics.
for example for aspect evaluations users tend to use descriptive syntax such as the graphic noun is amazing adjective while for feature request users often use imperative sentences such as please add more themes and it could be better to have more themes noun .
we leverage three structural features pos tags parsing tree and semantic dependence graph.
pos tag part of speech pos is a widely used grammatical feature for texts.
it indicates the property of each word in a sentence.
for example pos tags for sentence the user interface is beautiful are dt nn nn vbz jj in sequence .
here the pos tag for the word isis vbz which means isis a verb of 3rd person present singular.
we generate pos tags using the stanford corenlp tools and concatenate all pos tags together as a text feature.
parsing tree a parsing tree is a typical representation of the grammatical structure of a sentence .
it shows how a sentence is comprised.
each node represents a grammar unit and its children are subunits that it is comprised of.
figure illustrates a parsing tree for a sample review sentence the user interface is not very elegant which is generated by the stanford parser .
the label in each node denotes a pos tag.
this tree means that the sentence root is constituted by a noun phrase np and a sub sentence s where the noun phrase is constituted with a determiner dt and two nouns nn .
in order to represent a parsing tree as a flat text feature we dtnp s theroot nn nnvp adjp adjp vbz rb rb jjuser interface elegant very not isfig.
parsing tree for the sentence the user interface is not very elegant.
the dt interfa ce nn is vbz detnsubj copelegant jj user nn nnnot rb neg fig.
semantic dependence graph for the sentence the user interface is not elegant.
traverse tree nodes in the breadth first order and select the first five nodes.
we concatenate the pos tags of these five nodes as the text feature.
for example the feature for the parsing tree in figure is root np s dt nn nn .
semantic dependence graph sdg semantic dependence graph sdg exhibits the semantic dependence among words in one sentence.
it is a directed graph .
nodes in the graph represent words and the corresponding pos tags.
edges represent semantic relations between words e.g.
noun subjection and adjective modifier .
each sdg has a root node which has no incoming edges.
figure illustrates an sdg of a sample review sentence the user interface is not elegant which is generated by the stanford parser .
the root node is the word elegant which is an adjective noted as jj .
it has three children a noun subjection nsubj interface a copula cop isand a negation modifier neg not.
the child interface also has two children a determiner det theand a noun compound modifier nn user.
to convert an sdg into a flat text feature we traverse itsnodes in breadth first order then concatenate edges and pos tags in the traversal.
we ignore leaves that are not linked to the root.
for example the feature for the sdg in figure is vbz nsubj nn cop vbz neg rb .
c. step aspect opinion extraction our next goal is to summarize users opinions toward corresponding aspects.
to do that we need to identify words that express aspects and words that express opinions toward these aspects.
in this step sur miner extracts aspect opinion pairs i.e.
aspect and opinion words from each review sentence classified in the aspect evaluation category.
for example the resulting aspect opinion pairs for the review sentence the prediction is accurate but the auto correct is annoying are hprediction accuracy iandhauto correct annoying i. in general the state of the art techniques extract aspects by frequent item mining or by topic model which views user reviews as bags of words .
such an assumption may be problematic for software reviews that exhibit multiple purposes and sentiments.
as an empirical study indicates software reviews have quite monotonous patterns for different purposes .
therefore it is possible to determine aspect opinion pairs from the sentence patterns directly.
based on this assumption we design a pattern based parsing method which makes use of the syntax and semantics of review sentences and parses aspects and corresponding opinions from them directly.
to do that we first apply an nlp parser to annotate a semantic dependence graph sdg for a review sentence.
then we build a patternbased parser to extract aspect opinion pairs from the sdg.
pattern based parsing our pattern based parser is implemented as a sequence of cascading finite state machines .
the parser accepts a sdg and identifies aspect opinion pairs based on predefined semantic templates.
table ii lists some typical semantic templates we use.
the two letters at the beginning e.g.
jj and nn represent the pos tag of the root.
words in the following round brackets e.g.
have andlike represent root words.
the children of the root are listed in the square brackets as edge pos pairs.
for example the template in the first row means a root node with a pos tag of jj and two children a noun subjection nsubj with a pos tag of nn and a copula cop with a pos tag of vbz.
we generated the templates by manually identifying aspect part and opinion part from review sentences.
we randomly selected reviews sentences labeled as aspect evaluation except those we later used for evaluating the accuracy.
first we went through all these sentences and generated their sdgs.
then we associated each sdg with a template which denotes the places of the aspect part and the opinion part in the sdg.
we selected all those templates which were associated with more than sentences in order to avoid accidental associations.
we identified such templates to design the finite state machine3.
then given a new sdg instance the parser travels from the root to all other nodes checking the nodes edges and the corresponding children to determine the aspect and opinion 3the full list of templates is at xguaa srminer appendix.htmlwords according to the templates.
for example given the sdg in figure the parser checks the pos tag of the root.
since it is an adjective jj that matches the first and second templates in table ii it further checks whether it has three children a noun subjection nsubj with a pos tag of noun nn a copula cop with a pos tag of vbz and a negation modifier neg with a pos tag of rb.
the second template is matched.
then it checks whether the first child has a child of noun compound modifier nn with a pos tag of noun nn .
as the second template is an absolute match with the sample sdg the parser recognizes the nsubj nn node interface with its child user as aspect words and the neg rb node nottogether with the root node elegant as opinion words.
d. step aspect sentiment analysis in addition to opinions a quantitative summarization of users feeling towards each aspect may also be useful to grasp users preferences.
users ratings can provide such summarization objectively.
however an overall rating cannot satisfactorily characterize users preferences for different aspects.
for example consider the review the ui is nice but the sound sucks.
with a rating of out of .
the user obviously likes the aspect uibut dislikes the aspect sound .
therefore the actual ratings for both two aspects cannot be it could be for uiand for sound .
at the fourth step we apply sentiment analysis for each review sentence and associate the sentiments to the corresponding aspects with user ratings and a sentiment analysis tool.
we first apply a state of the art sentiment analysis tool deeply moving to analyze sentiment for each review sentence.
the deeply moving produces sentiments in a to scale where represents strongly positive means strongly negative and means neutral.
then to improve accuracy we adjust the sentiments by user rating to .
specifically if the rating for a whole review is strongly positive we add to the sentiments of .
if the rating is strongly negative we minus to the sentiments of .
for example the following review has two sentences the interface is beautiful.
i don t like the theme.
the sentiments for the two sentences are and respectively.
if the user rating for the review is we adjust the sentiment for the second sentence to .
if the user rating is we adjust the sentiment for the first sentence to .
e. step aspect clustering and summarization at this step we group aspect opinion pairs with the same aspects and summarize sentiments and typical opinions for each aspect group.
to group the aspects we first mine frequent items for all aspect words namely aspect words in the extracted aspectopinion pairs.
then we cluster aspect opinion pairs with common frequent items words .
for example given that auto correct is a frequent item in all aspect words if there are two aspect opinion pairs which contain this item they will be clustered into one group.
in particular if a pair has two or more frequent items that can be clustered into more than two different groups we cluster it into the group with the highest frequency of items or words.
for example a pairhbackground color nice ican be grouped with bothtable ii examples of dependency relation templates templates sample sentence aspect words opinion words jj the ui is beautiful!
nsubj nn jj jj cop vbz neg rb the user interface is not elegant.
nn nn nsubj nn neg rb jj nn nice ui!
nn amod jj vb have the frame has nice ui!
nsubj nn have nobj nn vb like i like the ui!
nobj nn like hbackground beautiful iandhcolor disgustingi.
however if we have already known that the aspect background has a higher frequency than that of color we will group the first pair with the second one instead of the third one.
if there is no frequent item in the two aspect opinion pairs we group them together when they have common words in their aspects.
for each group we select a group keyword as the word or the item which has the largest frequency in that group.
we also calculate a group sentiment as the average adjusted sentiment of aspect opinion pairs in that group.
f .
step visualization we designed two interactive diagrams namely aspect heat map andaspect trend map to illustrate the summaries.
the aspect heat map demonstrates popular aspects that users are concerned with.
it aims to help developers and managers grasp which parts aspects of the app are loved or disliked by users.
figure shows an example of the aspect heat map with each circle indicating an aspect.
the larger the circle is the more popular and liked the aspect is.
we define the size of the circle as size log comments sentiment .
the horizontal axis represents the number of comments and the vertical axis represents the adjusted rating.
therefore circles in the top right represent most popular and loved aspects and vice versa.
to get insight into an aspect group developers can click each aspect circle to view the specific comments with the top positive and top negative sentiments.
for each comment the aspect words are underlined and the opinion words are in bold.
the aspect trend map demonstrates the sentiment trends over time.
capturing user reactions is important for developers to select and prioritize features .
the aspect trend map aims to help developers assess whether their recent changes affected users satisfaction.
it also enables developers to estimate and predict users preferences so that they can improve parts of their product in the future.
figure shows an example of the diagram with each line indicating the sentiment trend for a popular aspect.
the horizontal axis represents date and the vertical axis represents user sentiments.
both the aspect heat map and aspect trend map are available on our project website at xguaa srminer .
iv.
empirical evaluation we evaluate our framework through three dimensions effectiveness comparison and usefulness.
to evaluate the effectiveness and advantages we apply common measures in the text mining literature and compare the results with state of theart methods.
we also conduct developer surveys to evaluate thetable iii overview of app subjects data set category time period swiftkey productivity .
.
.
.
camera360 photography .
.
.
.
templerun2 game .
.
.
.
wechat social network .
.
.
.
kakaotalk communication .
.
.
.
googleplaybooks books .
.
.
.
spotifymusic music .
.
.
.
yahooweather weather .
.
.
.
googlemap map .
.
.
.
googlecalendar productivity .
.
.
.
espn sports .
.
.
.
textplus social .
.
.
.
duolingo education .
.
.
.
chasemobile finance .
.
.
.
medscape medical .
.
.
.
yelp food .
.
.
.
imdb entertainment .
.
.
.
usefulness.
specifically our evaluation addresses the following research questions rq1 effectiveness how effectively can sur miner classify reviews extract aspects and opinions and analyze sentiments for app reviews?
rq2 comparison how does the sur miner compare to state of the art techniques for app review summarization?
rq3 usefulness how the summaries by sur miner useful for developers?
a. data collection we choose popular android apps such as swiftkey camera360 wechat and templerun2 from google play as our subjects.
these apps cover most popular categories such as games communication books and music.
we collected the reviews roughly in the period from aug to mar using an open source android market api .
for each review we collect its timestamp rating title and content.
table iii shows the description of the subjects.
b. effectiveness rq1 in this section we present our evaluation of sur miner s effectiveness in each single step namely review classification aspect opinion extraction and sentiment analysis.
review classification first we evaluate sur miner on the review classification task.
we sampled review sentences from each dataset and compared the predicted results with golden standard labels.
we manually labeled golden standard classes according to the rules in table i. to reduce the labeling bias two researchers separately applied the labeling rules to the review sentences.
consensus labelsfig.
demonstration of aspect heat map fig.
demonstration of aspect trend map were selected in the first iteration.
for the disagreements we discussed and clarified our labeling rules and relabeled again.
a second iteration resulted in agreement between the two researchers.
we use f1 score to measure the classification accuracy.
the f1 score is widely used in the text classification literature .
it is defined as follows f1 precision recall precision recall where the precision is the ratio of the number of instances correctly classified as a class tp to the number of instances classified as the class tp fp .
precision tp tp fp the recall is the ratio of the number of instances correctly classified as a class tp to the number of instances in the class tp fn .
recall tp tp fn we performed a five fold cross validation in the data sets times with each folder containing review sentences.
table iv shows the f1 scores for different categories4.
each column shows the f1 scores of a review category in all subjects.
the last column averages the results for each subject in all review categories and the last row averages f1scores for each review category in all subjects.
as indicated in the table the classification performance is reasonable with an 4full results including precisions and recalls are at xguaa srminer appendix.htmltable iv f1 scores of review classification in all subjects category evaluation praise request bug other overall swiftkey .
.
.
.
.
.
camera360 .
.
.
.
.
.
templerun2 .
.
.
.
.
.
wechat .
.
.
.
.
.
kakaotalk .
.
.
.
.
.
googleplaybooks .
.
.
.
.
.
spotifymusic .
.
.
.
.
.
yahooweather .
.
.
.
.
.
googlemap .
.
.
.
.
.
googlecalendar .
.
.
.
.
.
espn .
.
.
.
.
.
textplus .
.
.
.
.
.
duolingo .
.
.
.
.
.
chasemobile .
.
.
.
.
.
medscape .
.
.
.
.
.
yelp .
.
.
.
.
.
imdb .
.
.
.
.
.
average .
.
.
.
.
.
table v f1 scores for aspect opinion extraction data set aspect opinion sentiment positivesentiment negative swiftkey .
.
.
.
camera360 .
.
.
.
templerun2 .
.
.
.
wechat .
.
.
.
kakaotalk .
.
.
.
googleplaybooks .
.
.
.
spotifymusic .
.
.
.
yahooweather .
.
.
.
googlemap .
.
.
.
googlecalendar .
.
.
.
espn .
.
.
.
textplus .
.
.
.
duolingo .
.
.
.
chasemobile .
.
.
.
medscape .
.
.
.
yelp .
.
.
.
imdb .
.
.
.
average .
.
.
.
average f1 score of .
as well as for the aspect evaluation category with an average f1 score of .
.
that means the classification step can accurately provide different developers with different types of review sentences.
in particular it provides reliable review sentences for the aspect evaluation.
the f1 scores for specific categories such as bug are not good in some apps.
we manually checked those reviews and found that these apps received rare bug reports.
the extremely unbalanced data could be the main reason for these outliers.
aspect opinion extraction to evaluate sur miner s performance on aspect opinion extraction we follow the same procedures as in the review classification experiment to check if sur miner correctly extracts aspects and corresponding opinions from review sentences.
for each subject we sampled review sentences and selected those in the aspect evaluation category.
we use f1 score to measure the accuracy of aspect extraction and opinion extraction separately.
in particular the number of true positives tp in equation is the number of correctly extracted aspects or opinions the number of false positives fp means the number of mistakenly extracted aspects or opinions the number of false negatives fn is defined as the number of aspects or opinions that have not been extracted.
the results are shown in the first two columns in table v.as is indicated both aspect extraction and opinion extraction have reasonable accuracy with average f1 scores of .
and .
respectively4.
the results suggest that the aspect extraction step provides reliable aspects and opinions.
sentiment analysis to evaluate the sentiment analysis step we also follow the same procedures as in the classification and aspect extraction stages.
for each subject we sampled review sentences and selected those in the aspect evaluation category and compared the sentiment for each aspect opinion pair with golden standard sentiment labels.
to simplify the estimation we divided the sentiment scale into two polarities that is positive and negative and labeled them according the their polarities.
we labeled the golden standard sentiments manually as we did for review classification.
we use f1 score to measure the accuracy of each sentiment category.
in particular the number of true positives tp in equation is defined as the number of correctly classified sentiments the number of false positives fp means the number of misclassified sentiments the number of false negative fn means the number of sentiments that are not classified in that category.
the results are shown in the last two columns in table v. as is indicated both positive and negative sentiments have acceptable accuracy with average f1 scores of .
and .
respectively4.
the average f1 score for both is .
.
the reason that the negative sentiment has a relatively low performance in camera360 and duolingo could be that these two apps received much more positive reviews so that the sentiment categories become extremely unbalanced.
the results suggest that the sentiment analysis step produces reliable results.
sur miner provides reliable results on review classification aspect opinion extraction and sentiment analysis with average f1 scores of .
.
and .
respectively.
c. comparison rq2 our next evaluation aims to compare sur miner with state of the art techniques with respect to final summaries.
quantitative comparison we first compare the accuracy of sur miner for aspect extraction with those of related work reviewspotlight and guzman s method .
as discussed in section ii reviewspotlight is a review summarization tool for general products by identifying noun adjective pairs section ii c and guzmans tool is the most related work to ours that also extracts aspects from app user reviews section ii b .
we run aspect extraction by simulating real world usage scenarios.
for each subject we randomly select review sentences in all categories from the original dataset except those for training classifiers.
first we run review classification on these sentences.
then we apply aspect extraction on sentences that are classified as aspect evaluation .
we compare the extracted aspects with golden standard aspects that were manually labeled.
we use f1 score to evaluate the accuracy using the same definition in section iv b2.table vi comparison of aspect extraction accuracy with related works metric sur miner reviewspotlight guzman and maalej f1 score .
.
.
table vi shows the average f1 scores of three approaches in all subjects.
we reproduced reviewspotlight and applied it for extracting app aspects.
the result of guzmans approach is excerpted from their paper .
as we can see the f1 score for sur miner is .
significantly greater than those of the reviewspotlight .
and guzman s tool .
.
to investigate the reasons for these results we manually checked the results of reviewspotlight.
we found that without distinguishing review categories it tends to extract aspects for reviews in other categories such as aspect requests and bug reports .
for example consider the review i hate that you can t use offline dictionary which requires for a new aspect offline dictionary .
the reviewspotlight just outputs hdictionary offlineiwhich is meaningless while sur miner can filter such review from aspect evaluation since it talks about a nonexistent aspect.
another shortcoming of these related approaches is that they cannot identify complex phrases as they simply consider frequent items or noun adjective pairs as aspects.
for example for the review also love the way it auto ads reminders the reviewspotlight simply outputs hads autoiwhile sur miner outputshthe way it auto ads reminers love i. it is also interesting to see that even though both the classification and extraction stages have mistakes combining them does not result in worse accuracy.
the classification step has an f1 score of .
.
the aspect extraction step has an f1score of .
section iv b .
however when extracting aspects from the outputs of classification stage the final f1 score is .
even greater than that in the classification stage.
by manually checking the extracted aspects we found that though some reviews were misclassified during the classification stage the aspect extraction stage can still re correct them since a misclassified review may not be parsed by our semantic patterns.
for example consider a misclassified review no public transportation navigation!
which requires a new aspect but was misclassified as aspect evaluation in the classification stage.
nevertheless sur miner still cannot recognize any aspect since there is no semantic pattern to parse this review.
qualitative comparison topic models such as lda are widely used by most state of the art app review summarization tools .
to investigate the advantages of surminer over these topic based techniques we qualitatively compare the extracted aspects by sur miner with topics extracted by topic models.
table viii compares the top five aspects we extracted with the top five topics by ar miner a state of the art review summarization tool that applies emnb lda topic model in the swiftkey subject.
we collected data in the same period from google play as ar miner did.
we have two observations sur miner can distinguish different review purposes.
for example opinions extracted by sur miner are aspect evaluations except some noises while top words by lda arminer are miscellaneous.
for example if a manager wouldtable viii comparison of topics by lda and aspects by surminer a topics and typical words by ar miner lda .
the first row lists the top topics.
the following rows list the top words for each topic topics theme chinese jelly bean predict space keywordsmore languag bean word space theme chines jelli predict period wish need galaxi text email love wait note complet enter custom user keyboard auto insert b aspects and opinions extracted by sur miner.
the first row lists top five aspects with most comments.
the following three rows show opinions with top positive sentiments while the last two rows show opinions with top negative sentiments aspects predictions.... auto correct.... words.. theme... key opinionsamazing flawless love great best excellent good like love like amazing amazing like over top accurate stubborn a pain ugly breaker hate nightmare not need just obnoxious like to know users evaluations on the aspect prediction surminer can provide users opinions such as excellent accurate hate while ar miner lda cannot provide such information sur miner can distinguish users sentiments while arminer lda cannot.
for example managers and developers can find both positive and negative sentiments by sur miner but cannot tell whether user like or dislike the aspect prediction by lda.
overall sur miner produces much clearer summaries in distinguishing review purposes and sentiments than lda models.
sur miner produces much more accurate and clearer summaries than state of the art methods.
d. usefulness rq3 as the usefulness evaluation may be subjective we consulted developers to assess the usefulness of sur miner.
we applied sur miner to the latest user reviews of popular android apps such as swiftkey camera360 wechat and templerun2.
we presented the visualized summaries as demos in our website and asked the questions shown in table vii to developers.
the two questions are related to the two diagrams respectively.
we provided five options for each of them strongly agree agree neither disagree and strongly disagree .
the number of choices to each option was also listed for each question.
we sent invitation mails to developers of the selected apps posted our website to android developer communities in google and also invited developers from it companies such as samsung tencent and baidu for feedback.
developers showed great interest in our sur miner.
as indicated in table vii of all answers received of them agreed that our tool helps developers.
only two held conservative opinions .
and two .
disagreed.
figure shows the box plot statistics of developer feedback.table vii questions in the developer survey and results questionsstrongly disagreedisagree neither agreestrongly agreetotal q1.
do you think figure aspect heat map is useful to understand users preferences for aspects?
q2.
do you think figure .
aspect trend map helps developers to understand the users preferences trend over time?
5trend mapheat map g series1 strongly disagree strongly agree fig.
developer rating on usefulness we quantize the answers to ratings from to .
each box shows answers for a question.
as the results indicate answers to both questions have average ratings much greater than .
that means developers agree the usefulness of sur miner in general.
in addition we received the following encouraging comments from developers this is a great project.
the visualization data impresses me much!
i think if possible we would like to work with these researchers.
i really like the performance of your sentiment classifier.
the provided visualized information serves as quite clear way to get insight of products including advantages and disadvantages.
analyzing large scale user comments calls for great human efforts.
such project makes the understanding as well as iteration of products fast.
these comments indicate that developers appreciate our tool to help grasp users opinions toward different aspects.
developers feedback indicates our sur miner helps developers grasp users opinions and sentiments in practice.
v. threats to validity we have identified the following threats to validity subjects are all free android apps.
all projects investigated in this paper are free android apps.
hence they might not be representative of charged apps and apps in other markets e.g.
appstore .
commercial apps may have different review patterns.
in the future we will mitigate this threat by investigating user reviews of commercial apps and apps in other markets.
ground truth labels were judged by two people.
as the golden standard labels require large human efforts they were judged by only two people in our experiments.
they could be biased from real app developers.
for mitigating this threat we presented final results to developers and made sure that they were satisfied with the accuracy.
in the future we will further reduce this threat by inviting more developers for labeling.vi.
conclusion we proposed sur miner for effective and automatic user review summarization.
the summaries from sur miner provide a desirable answer to the important question what part of your apps are loved by users for developers.
our evaluation results show that sur miner provides reliable results with average f1 scores of .
.
and .
for review classification aspect opinion extraction and sentiment analysis respectively.
the final aspects from surminer are significantly more accurate and clearer than stateof the art techniques with an f1 score of .
greater than that of reviewspotlight .
and guzmans method .
.
feedback from app developers is also very encouraging and answers from developers agree with the usefulness of sur miner.
in the future we will summarize other review categories such as feature requests .
in addition we will propose techniques to summarize other software text data such as code comments and bug reports.