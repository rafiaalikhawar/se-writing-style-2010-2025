learning to prioritize test programs for compiler testing junjie chen1 yanwei bai1 dan hao1 yingfei xiong1 hongyu zhang3 bing xie1 1key laboratory of high confidence software technologies peking university moe 2institute of software eecs peking university beijing china chenjunjie byw haodan xiongyf xiebing pku.edu.cn 3the university of newcastle nsw australia hongyu.zhang newcastle.edu.au abstract compiler testing is a crucial way of guaranteeing the reliability of compilers and software systems in general .
many techniques have been proposed to facilitate automated compiler testing.
these techniques rely on a large number of test programs which are test inputs of compilers generated by some test generation tools e.g.
csmith .
however these compiler testing techniques have serious efficiency problems as they usually take a long period of time to find compiler bugs.
to accelerate compiler testing it is desirable to prioritize the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier.
in this paper we propose the idea of learning to test which learns the characteristics of bug revealing test programs from previous test programs that triggered bugs.
based on the idea of learning to test we propose let an approach to prioritizing test programs for compiler testing acceleration.
let consists of a learning process and a scheduling process.
in the learning process let identifies a set of features of test programs trains a capability model to predict the probability of a new test program for triggering compiler bugs and a time model to predict the execution time of a test program.
in the scheduling process let prioritizes new test programs according to their bug revealing probabilities in unit time which is calculated based on the two trained models.
our extensive experiments show that let significantly accelerates compiler testing.
in particular let reduces more than of the testing time in .
of the cases and reduces between and of the testing time in .
of the cases.
i. i ntroduction compiler is one of the most fundamental software tools and almost all software systems rely on it.
therefore it is vitally important to guarantee the reliability of compilers.
compiler testing is an effective and widely recognized way of ensuring the correctness of compilers .
over the years many techniques have been proposed to facilitate automated compiler testing.
these techniques rely on some test generation tools e.g.
csmith to generate a large number of test programs which are test inputs of compilers .
compiler bugs can be detected by running the generated test programs.
however compiler testing still suffers from the serious problem of efficiency.
for example yang et al.
spent three years on detecting c compiler bugs and le et al.
spent eleven months on detecting c compiler bugs.
that is with existing techniques compiler corresponding author.
sorted in the alphabet order of the last names.testing consumes an extremely long period of time to find only a small number of bugs.
therefore it is very necessary to accelerate compiler testing.
since only a subset of test programs are able to trigger compiler bugs intuitively compiler testing can be accelerated by running these test programs earlier.
in other words test prioritization may be adopted to accelerate compiler testing.
however existing test prioritization approaches can hardly be used to accelerate compiler testing due to the following reasons.
the dominant prioritization approaches rely on structural coverage information e.g.
statement coverage and branch coverage which is collected through regression testing .
however most test programs used in compiler testing are generated on the fly by random test generation tools like csmith thus the structural coverage information of these test programs is not available.
in other words these prioritization approaches based on structural coverage cannot be applied to accelerate compiler testing more discussion in section v a .
recently researchers proposed some input based prioritization approaches which rely on only test inputs i.e.
test programs in compiler testing and do not require structural coverage information.
however our experimental results more details in section iv a show that the existing input based approaches can hardly accelerate compiler testing because of their low efficiency and effectiveness.
in summary the existing test prioritization approaches cannot accelerate compiler testing.
to accelerate compiler testing in this paper we present an idea of learning to test which learns the characteristics of bugrevealing test programs to prioritize new test programs.
that is by learning from the existing test programs that trigger bugs we model the relationship between the characteristics of the test programs and the discovery of compiler bugs.
we then use the model to help us prioritize new test programs that are more likely to trigger bugs quickly.
based on this idea we develop let short for learning to test a learning totest approach to accelerating compiler testing.
given a set of new test programs before using a compiler testing technique to test compilers let prioritizes these test programs so that programs that have higher chance to trigger bugs in unit time are executed earlier.
in particular in this paper we target at c compilers because of the following reasons.
first the quality ieee acm 39th international conference on software engineering ieee acm 39th international conference on software engineering .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
of c compilers is very important since many safety critical software systems are written in c. second many different tools and techniques are available for c compiler testing allowing us to evaluate our approach in different settings.
more specifically in our work we study many existing compiler bugs and identify a set of features on test programs that are related to bug detection.
using the set of features we train two models from the existing test programs a capability model that predicts the probability of a new test program to detect a bug and a time model that predicts the execution time of a new test program.
given a set of randomly generated test programs let prioritizes the programs based on the descendant order of their bug revealing probabilities in unit time which is calculated by dividing the predicted bug revealing probability by the corresponding predicted execution time.
in this way our approach accelerates existing compiler testing techniques leading to more efficient detection of compiler bugs.
we evaluate let using two compiler testing techniques dol and emi two subjects gcc and llvm and two application scenarios cross compiler and cross version scenarios .
the evaluation results show that in terms of time spent on detecting each bug let substantially accelerates compiler testing in all settings let reduces more than of the testing time in .
of the cases and reduces between and of the testing time in .
of the cases.
we also compare let with two recent input based test prioritization approaches i.e.
tb g and arp .
the experimental results show that let is more effective and stable than tb g and arp for accelerating compiler testing.
to sum up the major contributions of this paper are as follows the idea of learning to test which learns from existing test programs to accelerate future test execution.
the development of let a learning to test approach to prioritizing test programs for accelerating c compiler testing.
an extensive experimental study confirming the effectiveness of our approach.
ii.
a pproach figure presents the overview of our approach which contains an offline learning process section ii a and an online scheduling process section ii b .
a. learning process the key insight of our approach is that programs with certain language features or combinations of language features are inherently difficult to compile or optimize and such programs are more likely to trigger bugs in compilers.
if we can correctly identify these features we should be able to predict the probability of a test program to trigger bugs and thus execute them earlier.
we illustrate this with an example.
when implementing c compilers compiling structs is usually complex and error prone as the compiler needs to correctly align structs based on the requirement of theunderline operating system and also needs to find an optimal alignment for efficient space usage.
as a matter of fact there are a large number of bug reports in gcc repository related to structs.
for example figure shows a bug report1for gcc bug id is where volatile in the struct is not properly treated when performing tree optimization the optimization that manipulates gimple trees.
from the bug report we can see several features of the code pieces related to the discovery of the bug for example the existence of both struct and violatile and the number of times a volatile variable is written.
the former relates to the existence of program elements existence features and the latter relates to how these elements are used usage features .
we identify a large set of existence and usage features and design methods to obtain them from generated programs.
we further use a machine learning algorithm to train a prediction model based on the identified features to predict the probability of a test program to trigger a bug.
moreover in order to get the bug revealing probability in unit time we also use a machine learning algorithm to train a regression model to predict the execution time of each test program.
we use the same identified features when training the regression model.
in this paper we call the former model as the capability model and the latter model as the time model.
in short our learning process has three components identifying features training a capability model and training a time model.
identifying features the identified features are divided into two types.
the first type of features existence features are concerned with whether certain types of elements exist in the target program.
intuitively some bugs occur only on certain specific programming elements thus the existence of these programming elements can serve as features to characterize test programs triggering bugs.
for example bugs in loop optimization occur when test programs have loop statements.
more concretely existence features are defined as four sets exist stmt expr va r op where stmt is the set of all statement types in c language expr is the set of all expression types in c language va r is the set of all variable types in c language and op is the set of all operation types in c language.
when there exist at least a program element belonging to the associated type exists the feature is set to one otherwise it is set to zero.
the second type of features usage features are concerned with how the elements in a program are used.
intuitively certain bugs may only be triggered when the program elements are used in a specific manner.
for example a bug concerning pointers may only be triggered when the pointer has pointed to muliple addresses i.e.
the size of its alias set must be larger than a threshold.
in this paper we utilize a characteristic of the random test generation tool csmith one of the most widely used random c program generator.
when generating a program csmith records a set of usage features from the program such as the size of alias set the depth of bug.cgi?id .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
extracting features extracting features feature selection training training existing test programs new test programs training set testing set capability model time model bug revealing probabilities execution time ranking execution order bug revealing probabilities in unit time learning process scheduling process in u ng processed training set fig.
overview of let typedef struct volatile int a volatile int b s int main void s x s y x y y x return x.a y.a description the bug occurs in the tree optimization part of gcc.
the code does not treat volatile struct members as volatile because sra creates new variables and then goes and makes them renamed.
fig.
an example of gcc bug report pointer dereference etc.
to save the feature collection time we directly use the usage features collected by csmith for our offline training.
more concretely we use the following features address features e.g.
the number of times the address of a struct or a variable is taken.
struct bitfield features e.g.
the times of a struct with bitfields on lhs rhs and the number of non zero zero const violate full bitfields.
pointer dereference features e.g.
the times of a pointer is dereferenced on lhs rhs and the depth of pointer dereference.
pointer comparison features e.g.
the number of times a pointer is compared with null with the address of another variable or with another pointer.
alias set features e.g.
the size of alias sets.
jump features e.g.
the times of forward jumps and backward jumps.
used variable features e.g.
the percentage of a freshmade variable i.e.
the variable defined and used in the same statement is used and the percentage of an existing variable is used.
currently we use only the preceding two types of features due to the tradeoff between acceleration effectiveness and cost.
intuitively the more features we use in training the better prioritization results we may get.
however besides the learning process in the scheduling process let also needs to extract the values of these features for new test programs.
since the scheduling process is conducted online it is necessary to control its cost.
therefore let uses only these easy to extract features.
training a capability model we collect a set of test programs generated by existing test program generation tools some of which trigger bugs whereas the others do not.
each test program is taken as a training instance whose label is true false triggering bugs or not .
for each test program we extract the values of the identified features through program analysis2.
based on the set of training instances including their features and labels let first conducts feature selection so as to filter useless features and then normalizes them in order to adjust values measured on different scales to a common scale and finally builds a capability model through machine learning.
finally the capability model outputs the probability of a test program triggering bugs.
feature selection.
let conducts feature selection by calculating the information gain ratio of each feature.
information gain ratio is a ratio of information gain to the intrinsic information which is usually a good measure for identifying the contribution of a feature and is able to reduce the bias towards multi valued features in existing feature selection metrics .
after calculating the information gain ratio of each feature let filters the useless features i.e.
the features whose information gain ratios are zero.
normalization.
since the features are either numeric type or boolean type i.e.
or let normalizes each value of these features into the interval using min max normalization .
supposed the set of new test programs to be scheduled is denoted as t t1 t2 ... t m and the set of features is denoted as f f1 f2 ... f s we use a variable xijto represent the value of the feature fjfor the test program tibefore normalization and use a variable x ij to represent the value of the feature fjfor the test program tiafter normalization i mand1 j s .
the normalization formula is as follows x ij xij min xkj k m max xkj k m min xkj k m building the capability model.
after feature selection and normalization we adopt a machine learning algorithm 2as our approach is implemented to accelerate compiler testing by using the test programs generated by csmith we directly extract the values of features including the two types of features from each test program during the test generation process of csmith.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
sequential minimal optimization abbreviated as smo algorithm to build the capability model.
the smo algorithm is a support vector machine algorithm which speeds up standard support vector machines by breaking a very large quadratic programming optimization problem into a series of smallest possible quadratic programming optimization problems .
training a time model we collect a set of test programs and record the execution time of each test program.
the execution time of a test program includes the time for compiling the program the time for running the program and obtaining its result and the time for any oracle checks necessarily to be performed.
we use the previous version of the compiler under test to obtain the execution time.
each test program is taken as a training instance whose label is its execution time.
similar with training the capability model we also extract the values of the identified features and then normalize the set of training instances.
based on the set of normalized training instances let builds a regression model i.e.
the time model using gaussian processes since their labels are in a continuous domain.
gaussian process uses lazy learning and a measure of the similarity between points i.e.
the kernel function to predict the value for an unseen point from the training set .
b. scheduling process based on the learned capability model and time model let schedules the execution order of new test programs through the scheduling process.
initially let extracts the values of the aforementioned features from each new test program and uses the two models to predict the bug revealing probability and execution time for each test program respectively.
then let calculates the bug revealing probability in unit time for each test program by dividing the bug revealing probability predicted by the learned capability model with the execution time predicted by the learned time model .
finally let prioritizes new test programs based on the descendent order of their bug revealing probabilities in unit time.
iii.
e xperimental study in the study we address the following research questions.
rq1 how effective is let in accelerating c compiler testing?
rq2 how does let perform when being applied to different compiler testing techniques i.e.
dol and emi ?
rq3 how does let perform in different application scenarios i.e.
cross compiler and cross version scenarios ?
rq4 can the major components of let i.e.
feature selection and time model contribute to the overall effectiveness?
more specifically rq1 investigates overall acceleration effectiveness of let by comparing it with two existing prioritization approaches rq2 investigates the effectiveness of let on accelerating different compiler testing techniques rq3 investigates two application scenarios which differ in the subjects used in the learning process and the schedulingtable i subject statistics subject loc usage gcc .
.
learning gcc .
.
scheduling llvm .
learning scheduling llvm .
scheduling open64 .
learning process more details referred to section iii b .
furthermore for let its key part is to train a capability model using the identified features from existing test programs.
to improve the effectiveness of its key part there are two important complementary parts feature selection and time model.
therefore rq4 studies whether feature selection and time model make contributions to the effectiveness of let respectively.
a. subjects and test programs in this experimental study we use three mainstream opensource c compilers namely gcc llvm and open64 for the x86 linux platform.
the statistics of these compilers are presented in table i where the last column presents whether the corresponding compiler is used in training the capability model marked aslearning or is used in testing the approach marked as scheduling .
note that we always use the previous version to train the time model.
the subjects we use are not the newest versions because it is easier for us to collect enough bugs on the old versions to perform statistical analysis.
we only use open64 in the learning process because it has no available bug reports making it impossible to measure the number of bugs detected.
more discussion can be found at iii e. the test programs we use in both learning and scheduling processes are c programs randomly generated by csmith which is commonly used in the literature of c compiler testing .
the programs generated by csmith are always valid and do not require external inputs and the output of any program is the checksum of the non pointer global variables of the program at the end of program execution.
to avoid imbalance data problem in building a capability model for each subject used in the learning process we randomly collect the same number of test programs triggering bugs and test programs not triggering bugs3.
b. application scenarios we consider two application scenarios of let in this study.
cross compiler scenario let learns a capability model from one compiler and applies the capability model to prioritize test programs for another compiler.
to evaluate let in this scenario we use open64 .
in the learning process and use gcc .
.
as well as llvm .
respectively in the scheduling process and use gcc .
.
in the learning process and use llvm .
in the scheduling process.
cross version scenario let learns a capability model from one version of a compiler and applies the capability model to prioritize test programs for its later versions.
to 3for open64 .
and gcc .
.
we use failed test programs and passed test programs.
for llvm .
we use failed test programs and passed test programs because csmith do not generate enough failed test programs.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
evaluate let in this scenario we use gcc .
.
in the learning process and gcc .
.
in the scheduling process and use llvm .
in the learning process and llvm .
in the scheduling process.
in particular in this scenario all the bugs used in the learning process have already been resolved and closed before the versions used in the scheduling process.
c. baseline approach random order approach ro which randomly selects an execution order of new test programs is taken as the baseline in our study.
ro demonstrates the effectiveness of compiler testing without any accelerating approaches.
d. independent v ariables we consider three independent variables in the study.
compiler testing techniques in this study we consider two compiler testing techniques to accelerate i.e.
different optimization level dol and equivalence modulo inputs emi .
dol is an effective compiler testing technique which determines whether a compiler contains bugs by comparing the results produced by the same test program with different optimization levels i.e.
o0 o1 os o2 and o3 .
given an execution order decided by a prioritization approach we compile and execute test programs under different optimization levels and determine whether the test program triggers a bug by comparing their results.
emi is first proposed by le et al.
which generates some equivalent variants for any given test program and determines whether a compiler contains bugs by comparing the results produced by the original test program and its variants4.g i v e n an execution order decided by a prioritization approach we generate eight variants for each original test program by randomly deleting its unexecuted statements as emi did and then compile and execute each pair of a test program and its variants under the same optimization level i.e.
o0 o1 os o2 and o3 .
finally we compare the corresponding results to determine whether a bug is detected by them.
compared prioritization approaches we implement two latest input based test prioritization approaches for comparison.
token vector based prioritization tb g which is the first test case prioritization approach for compilers.
tb g regards each test program as text and transforms each test program into a text vector by extracting corresponding tokens from text and then prioritizes test programs based on the distance between the text vector and the origin vector ... .
a s a n e xisting study reveals tb g is the most cost effective strategy among a set of studied techniques.
adaptive random prioritization arp which selects the test input that has the maximal distance to the set of already selected test inputs .
although arp is not proposed to accelerate compiler testing in our study we adopt it for compiler testing by treating a test program as a test input and 4in fact emi has three instantiations namely orion athena and hermes .
in our paper emi refers to orion.calculating the distance between test programs using their edit distance.
v ariants of let in our study we explore the impact of some parts i.e.
feature selection and time model of let on compiler testing acceleration thus we implement the following variants of let.
let a which removes the feature selection process described in section ii a2 from let.
that is let a uses all identified features to train the capability model and then trains a time model and finally prioritizes new test programs based on their bug revealing probabilities in unit time.
let b which removes the time model as described in section ii a3 from let.
that is let b conducts feature selection to filter useless features and then uses the processed training set to build a capability model and finally prioritizes new test programs based on only their bug revealing probabilities.
to implement let let a and let b we use weka .
.
which is a popular environment for data mining.
we use the smo algorithm implemented by weka to build the capability model choosing pu k kernel with omega .0and sigma .
.
we use the gaussian process implemented also by weka to build the time model choosing pu k kernel with omega .3andsigma .
.
the parameter values are decided by a preliminary study that we conduct on a small dataset.
other parameters in these two algorithms are set to the default values provided by weka.
e. dependent v ariables an important issue is how we measure the number of bugs detected by a test suite.
if two test programs both fail there may be two bugs or the two test programs may trigger the same bug.
to solve this problem we use the correcting commits technique used in previous work .
that is given a failed test program we determine in which future commit the bug is corrected i.e.
the test program passes since that commit.
if two bugs are corrected by the same commit we assume the two bugs are the same bug.
the dependent variable considered in our study is the time spent on detecting kbugs where k nandnis the total number of bugs detected when executing all test programs in our study.
this dependent variable is used to measure the effectiveness of let.
note that we do not use the average percentage of detected faults abbreviated as apfd because developers usually care more about the time spent in compiler testing rather than the number of test programs used in compiler testing.
for ease of presentation we use formula to calculate the corresponding speedup on detecting kbugs where tro k represents the time spent on detecting kbugs through ro i.e.
without any accelerating approaches and ta cc k represents the time spent on detecting kbugs using a prioritization approach i.e.
let let a let b tb g or arp .
speedup k tro k ta cc k tro k authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
since all the accelerating approaches e.g.
let tb g and arp studied in this paper consume extra time on scheduling test programs5 the time spent on detecting kbugs includes the scheduling time.
f .
experimental process for compilers under test we generate c programs using csmith which serve as the new test programs to be scheduled.
first we apply ro to the test programs and feed the scheduled test programs to the two compiler testing techniques respectively.
during the process we record the execution time of each test program and which test programs triggered which bugs then calculate the time spent on detecting each bug.
the results of ro demonstrate the compiler testing results without using any accelerating approaches.
to reduce the influence of random selection we apply ro times and calculate the average results.
next we apply let to each compiler under test in two application scenarios by using two compiler testing techniques.
during this process we also calculate the time spent on detecting each bug.
following the same procedure we apply let a and let b. finally we apply tb g and arp to the new test programs of compilers under test and feed the prioritized test programs to the two compiler testing techniques respectively calculating the time spent on detecting each bug.
the experimental study is conducted on a workstation with eight core intel xeon e5620 cpu with 24g memory and ubuntu .
operating system.
g. threats to v alidity the threats to internal validity mainly lie in the implementations of our approach the compared approaches and the compiler testing technique emi.
to avoid implementation errors at least two authors of this paper review the source code.
furthermore in implementing emi we use the same tools i.e.
libtooling library of clang6and gcov7 a sl ee t al.
did in their implementation.
the threats to external validity mainly lie in compilers and test programs.
to reduce the threat resulting from compilers we use all the c compilers that have been used in the literature on compiler testing .
in the future we will use more compilers and versions as subjects.
to reduce the threat resulting from test programs we use c test programs randomly generated by csmith as the prior work did .
however these test programs are not necessarily representative of c programs generated by other tools.
in the future we will further use more other test generation tools.
the threats to construct validity lie in how the results are measured.
in measuring acceleration effectiveness we 5in our approach the extra time refers to only the time spent on the scheduling process described in section ii b because the learning process is conducted offline.
correcting commits to automatically identify duplicated bugs .
as correcting commits relies on developers edition different bugs may be regarded as the same one.
however it may not be a big threat because developers do not tend to fix many of bugs in one commit to guarantee software quality .
h. v erifiability the replication package of the experiments is available at our project website8.
the package includes the tool and the data for reproducing the experiments.
it also includes the detailed experimental results.
the tool is open source allowing one to verify the details of the experiments.
the detailed results allow one to verify the result analysis without rerunning the experiments.
the tool also enables further studies on different subjects and different experimental settings.
iv .
e xperimental results and analysis a. rq1 acceleration effectiveness table ii presents the acceleration results of let and tb g in terms of the time spent on detecting bugs.
in this table column scenarios presents the application scenarios and the subject before an arrow is used in the learning process whereas the subject behind the arrow is used in the scheduling process column bug presents the number of detected bugs column ro presents the average time spent on detecting the corresponding number of bugs columns let and tb g present the difference between the time spent on detecting the corresponding number of bugs using let tb g and using ro.
if the difference is less than zero the corresponding approach accelerates compiler testing because the used accelerating approach let or tb g spends less time than ro on detecting the corresponding number of bugs.
overall effectiveness table ii shows that the values in column let are mostly smaller than zero.
moreover the absolute values that are smaller than zero are far larger than the absolute values that are larger than zero in most cases e.g.
in the scenario open64 .
gcc .
.
using dol the absolute values that are smaller than zero includes .
and .
but the absolute values that are larger than zero are only .
and .
.
furthermore as the testing proceeds the time spent on testing will become larger and the absolute saving of let will become larger even if the relative speedup stays the same.
overall let does accelerate compiler testing.
we also analyze the distribution of speedups achieved by let9.
the results are shown in figure .
in this figure the xaxis represents the scope of speedups and the y axis represents the frequency in which speedups occur corresponding scopes.
from the left bar for each scope in this figure we can see that in more than .
cases let accelerates compiler testing and the speedups of let are mostly in the range from to medium acceleration and secondly in the range more than high acceleration .
that is let does accelerate compiler testing to a great extent.
9for each kand each setting we calculate the speedup of let using formula .
we then analyze these speedups together.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii time spent on bug detection 104seconds scenarios bugdol emiscenarios bugdol emi ro let tb g ro let tb g ro let tb g ro let tb g open64 .
gcc .
.
.
.
.
.
.
.
gcc .
.
gcc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
open64 .
llvm .
.
.
.
.
.
.
gcc .
.
llvm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
llvm .
llvm .
.
.
.
.
.
.
.
.
.
.
.
.
g1 g2 g1 g1 g2 g3 g1 g2 g4 g1 g2 g5 g1 g2 g6 g7 g8 g9 g10 g1 g11 g9 g10 g1 g12 g9 g4 g10 g13 g11 g9 g4 g10 g12 g9 g3 g1 g13 g11 g9 g3 g1 g12 g1 g13 g11 g1 g12 g3 g1 g13 g11 g3 g1 g12 g4 g10 g13 g11 g4 g10 g12 g10 g1 g13 g14 g8 g10 g1 g15 g16 g17 g18 g19 g8 g17 g20 g8 g15 g18 g19 g19 g21 g22 g18 g23 g8 g24 g25 g13 g26 g27 g19 g28 g22 g19 g29 g16 g30 g31 g32 g33 g33 g34 g9 g35 fig.
speedup distribution of let comparison with tb gtable ii shows that the values in column let are smaller than those in column tb g in most cases thus let spends less time on detecting the same number of bugs than tb g in most cases.
besides the former are mostly smaller than zero whereas the latter are often larger than zero.
therefore let performs more stable than tb g in accelerating compiler testing.
similarly we also analyze the distribution of speedups calculated by formula achieved by tb g whose results are also shown in figure .
from the right bar for each scope in this figure in about .
cases tb g accelerates compiler testing and in .
cases it decelerates compiler testing.
moreover the speedups achieved by tb g are mostly in the range from to low acceleration and then the range from to .
in particular in only .
cases thespeedups achieved by tb g are in the range more than high acceleration .
therefore let performs much better and more stable than tb g in accelerating compiler testing.
to learn whether let outperforms tb g significantly we perform a paired sample wilcoxon signed rank test at the significance level .
whose results are shown in table iii.
since the number of bugs detected in llvm .
using either dol or emi is quite small during the corresponding testing periods in our study10 we cannot perform this statistical test on those settings.
in this table rows p value represent the p value of the respective scenarios which reflect the significance in statistics.
the p values with denote that let outperforms tb g significantly in the corresponding setting.
besides row mean represents the mean speedups between let and tb g which is calculated by adapting formula where tro k refers to the time spent on detecting k bugs through tb g and ta cc k refers to the time spent on detecting k bugs through let.
from this table let outperforms tb g in all settings and mean improvements range from .
to .
and let outperforms tb g significantly at five settings.
therefore let outperforms tb g significantly in majority cases.
in conclusion let does perform much better and more stable than tb g in accelerating compiler testing.
10dol detects only one bug and emi detects only three bugs.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii statistical analysis between let and tb g scenariosopen64 .
open64 .
gcc .
.
gcc .
.
gcc .
.
llvm .
gcc .
.
llvm .
dolmean .
.
.
.
p value .
.
.
.
emimean .
.
.
.
p value .
.
.
.
table iv comparison between different compiler testing techniques and application scenarios summarytechniques scenarios dol emi cross compiler cross version mean .
.
.
.
p value .
.
.
.
comparison with arp in our experiment arp does not accelerate compiler testing at all which is consistent with the existing study .
in particular with arp dol detects only one bug of gcc .
.
in 104seconds i.e.
about the total execution time of all test programs using dol and detects no bugs of llvm .
or llvm .
by dol in the same period.
arp performs even worse on emi because with arp no bug is detected in gcc .
.
llvm .
and llvm .
in100 104seconds i.e.
about the total execution time of all test programs using emi .
the reason arp performs worse on accelerating compiler testing is that it spends too much time on calculating the edit distance between test programs whenever selecting a new test program.
therefore although arp does not rely on structural coverage information and is reported to achieve acceptable effectiveness in general software it cannot be applied to accelerate c compiler testing.
b. rq2 impact of compiler testing techniques to answer rq2 we statistically analyze all the experimental results of let for each compiler testing technique ignoring the impact of subjects and application scenarios.
the analysis results are given by the first three columns of table iv where row mean presents the mean speedups of let and the row p value presents the p values of the paired sample wilcoxon signed rank test between let and ro.
from this table let accelerates compiler testing significantly regardless of using either dol or emi.
moreover the acceleration effectiveness of let using any of dol and emi is quite obvious namely their mean speedups are .
and .
respectively.
therefore let achieves great effectiveness for accelerating different compiler testing techniques.
c. rq3 impact of application scenarios to answer rq3 we also statistically analyze the results of let for each application scenario ignoring the impact of subjects and compiler testing techniques.
the analysis results are given by the latter two columns of table iv.
from this table in either the cross compiler scenario or the cross version scenario let accelerates compiler testing significantly.
moreover the acceleration effectiveness of let in any of the two application scenarios is quite obvious namely their mean speedups are .
and .
respectively.
this is an evidence to demonstrate the soundness of let.
that is let g1 g2 g1 g1 g2 g3 g1 g2 g4 g1 g2 g5 g1 g2 g6 g7 g8 g1 g9 g1 g10 g3 g1 g11 g9 g3 g1 g10 g4 g12 g11 g9 g4 g12 g10 g12 g1 g11 g13 g8 g12 g1 g14 g15 g11 g2 g8 g16 g17 g18 g8 g19 g2 g20 g2 g8 g16 g17 g18 g21 g22 g16 g17 g18 g16 g17 g18 g21 g22 g1 g2 g1 g1 g2 g3 g1 g2 g4 g1 g2 g5 g1 g2 g6 g7 g8 g1 g9 g1 g10 g3 g1 g11 g9 g3 g1 g10 g4 g12 g11 g9 g4 g12 g10 g12 g1 g11 g13 g8 g12 g1 g14 g15 g11 g2 g8 g16 g17 g18 g8 g19 g2 g20 g2 g8 g16 g17 g18 g21 g22 g16 g17 g18 g16 g17 g18 g21 g22 fig.
comparison between let and its variants with respect to the distribution of speedups accelerates compiler testing no matter which compiler or version is used to train the models.
d. rq4 contributions of major components of let figures presents the comparison between let and its variants i.e.
let a and let b .
in these figures the xaxis represents the scope of speedups and the y axis represents the frequency in which speedups occur corresponding scopes.
from figure a the number of deceleration cases by let a is larger than that by let.
that is let is more stably effective than let a. moreover the speedups of let a are mostly in the range from to and the range less than but the speedups of let are mostly in the range from to and the range more than .
that is let performs better than let a. therefore feature selection does improve the acceleration effectiveness of let by filtering useless features.
furthermore we identified the features that contribute more to bug detection through the analysis of information gain ratio.
these features include the number of times a non volatile is written the maximum expression depth the existence of struct type the number of struct whose depth is one etc.
some features are expected such as the existence of struct type which is consistent with the example presented in figure .
however some features are a bit surprising to us such as the number of struct whose depth is one.
intuitively test programs with larger depth of struct tend to be more complex thus they are more likely to detect compiler bugs.
but our evaluation finds that when the depth of struct is one it is more likely to detect compiler bugs.
we will investigate the relationship between features and bug triggering ability in our future work.
similarly from figure b the number of deceleration cases by let b is larger than that by let.
that is let is more stably effective than let b. furthermore although the speedups of both let and let b are mostly in the range from to and the range more than the total frequency in those scopes of let i.e.
.
is larger than that of let b i.e.
.
.
moreover the frequencies in the range from to and the range from to of let is also larger than those of let b respectively.
that is let performs better than let b. therefore training a time model also does improve the acceleration effectiveness of let by prioritizing test programs based on their bug revealing probabilities in unit time.
overall feature selection and time model actually improve the prioritization effectiveness and make contributions to the acceleration effectiveness of let.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
v. d iscussion a. coverage based prioritization doesn t work in this paper we did not investigate existing coveragebased prioritization approaches because they can hardly be applied to accelerate compiler testing.
typically coveragebased prioritization schedules the execution order of tests for the project under test by utilizing the coverage information of its previous version.
that is coverage based prioritization approaches are proposed based on a hypothesis that many tests designed for the previous version can be reused to detect bugs for the current version.
however this hypothesis may not hold in compiler testing.
to verify this hypothesis we conduct a preliminary study on gcc using dol.
in particular we randomly generate test programs by using csmith and run these programs on gcc .
.
recording the bugs detected by dol.
then we search the subsequent versions of gcc .
.
to find its closest version i.e.
gcc .
.
that fixes all the detected bugs.
to verify whether the test programs are still useful in detecting bugs for new versions we run these test programs on gcc .
.
which is a later version of gcc .
.
and find no bug at all.
on the contrary we generate another new test programs using csmith and find that new programs reveal bugs in gcc4.
.
.
comparing the bugs detected by old programs and new programs in gcc .
.
we can tell that in compiler testing new test programs may outperform old test programs.
similarly le et al.
also demonstrated new test programs perform better than the regression test suite using emi.
moreover there are mature program generation tools like csmith which can generate a large number of test programs efficiently.
therefore practical compiler testing usually uses new test programs rather than reuse old test programs.
besides existing compiler testing techniques also use new test programs to test compilers.
for new test programs it is difficult to acquire their coverage information from a previous compiler version.
moreover we can hardly collect such coverage information without running the new test programs.
therefore coverage based prioritization can hardly be applied to accelerate compiler testing and we do not compare with these approaches in this study.
b. training efficiency of let let needs to train two models a capability model and a time model.
to train a capability model let uses a fixed previous version of a compiler or another compiler so as to reduce the cost of retraining a capability model.
that is in practical usage when a capability model has been trained it can be used to test a series of versions or compilers.
from the results of our study when a capability model is trained from gcc .
.
let accelerates the testing of gcc .
.
and llvm .
using the capability model when a capability model is trained from open64 .
let also accelerates the testing of gcc .
.
and llvm .
using the capability model.
that is the trained capability model is indeed robust.in particular even if we need to retrain a capability model the cost is very small compared with the testing time we reduce.
in our experiments training a capability model takes less than two minutes while the trained model on average reduced about hours in test execution time.
to train a time model let uses the previous version of the version under test as the training version in order to make the prediction results more accurate.
we assume frequent retraining on time model because the cost of retraining is even smaller than that of the capability model to train a capability model we need at least a set of programs triggering bugs but to train a time model the training programs and their labels can be collected when testing the previous version.
furthermore the retraining cost of a time model is also less than two minutes.
on the other hand even if a time model is not available let b still significantly accelerates compiler testing.
c. potential applications we believe that the general concept of learning to test proposed in this paper has many potential application areas.
first besides the compiler testing techniques used in the evaluation let can be applied to accelerate other compiler testing techniques e.g.
randomized differential testing rdt by feeding the prioritized test programs to these techniques.
second besides c compilers let can be applied to compilers of other languages e.g.
java .
in fact as long as testers identify relevant features our learning to test approach can be applied to compilers of other languages directly.
finally our learning to test approach can be also applied to other types of complex software besides compilers e.g.
browsers operating systems and image processing software.
like c compilers the inputs of complex software tend to be complex too e.g.
images for image processing software whose complexity provides an opportunity for us to identify different characteristics that are related to detected bugs from them thus our learning to test approach can be applied to test other types of complex software.
vi.
r ela ted work a. compiler testing compiler testing is difficult because the test inputs of compilers are programs and the generated programs must strictly meet complex specifications e.g.
c99 specification for c programs .
moreover compiler testing suffers from the test oracle problem as it is hard to tell the expected outputs of a compiler given some test programs .
in the literature there are two main aspects on compiler testing namely generating test programs and addressing the test oracle problem.
for test program generation random test program generation is a main generation technique .
for example yang et al.
proposed and implemented a tool called csmith to randomly generate c programs without undefined behaviors for testing c compilers and regehr et al.
proposed test case reduction authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
for c compiler bugs.
zhao et al.
developed an integrated tool jtt which automatically generates programs to test uniphier an embedded c compiler.
lidbury et al.
developed clsmith based on csmith to generate programs to test opencl compilers and pflanzer et al.
proposed test case reduction for opencl.
to address the test oracle problem of complex software systems including compilers mckeeman et al.
coined the term of differential testing which is a form of random testing.
in particular differential testing needs two or more comparable compilers and determines whether some compilers have bugs by comparing the results produced by these compilers which has been widely used to detect various compiler defects .
le et al.
proposed to generate some equivalent variants for each original c program which determines whether a compiler has bugs by comparing the results produced by the original program and its variants.
this technique is called equivalence modulo inputs which has three instantiations orion athena and hermes .
tao et al.
proposed to test compilers by constructing metamorphic relations e.g.
the equivalent relation.
boussaa et al.
proposed notice a component based framework for non functional testing of compilers according to user requirements.
furthermore chen et al.
conducted an empirical study to compare mainstream compiler testing techniques i.e.
rdt dol and emi and sun et al.
conducted a study to analyze the characteristics of the bugs in gcc and llvm.
different from these approaches our work addresses another important problem in compiler testing i.e.
the test efficiency problem.
it is time consuming to detect compiler bugs which is a common problem of the existing compiler testing techniques.
as the efficiency problem is quite serious to compiler testing our work targets at accelerating compiler testing.
b. test prioritization in the literature there is a considerable amount of research on test prioritization which can be mainly classified into four groups.
the first group focuses on the criterion used in test prioritization including structural coverage criterion the probability of exposing faults dataflow coverage and system model coverage .
the second group focuses on the algorithms used in test prioritization including greedy algorithms i.e.
total and additional strategies and many meta heuristics algorithms .
the third group focuses on the evaluation of existing test prioritization techniques including measurement on the effectiveness of test prioritization techniques and the influence of some factors in test prioritization.
the fourth group focuses on constraints that affect test prioritization e.g.
time constraints and the work in this group investigates the influence of the constraints and prioritization techniques specific to some constraints .
in this group time aware prioritization approaches also utilize execution time of testcases.
they use the execution time collected from a previous version in regression testing while our work predicts the execution time of new test programs for the current version by training a time model.
similar to existing work on test prioritization our work targets at prioritizing test cases that are more likely to reveal bugs so as to accelerate compiler testing.
however as discussed in section v a most test prioritization approaches are based on coverage information and cannot be applied to accelerate compiler testing.
recently jiang et al.
proposed an adaptive random prioritization technique which prioritizes the execution order of test cases based on only test inputs rather than coverage information.
this approach is motivated by adaptive random testing which is a test generation technique to spread the test inputs evenly in the input domain.
chen et al.
proposed a text vector based prioritization approach which transforms each test case into a text vector by extracting corresponding tokens.
the two approaches may be adopted to accelerate compiler testing but in our study their acceleration effectiveness is not satisfactory and in many cases they even decelerate compiler testing.
in this paper we propose a learning to test approach to accelerating compiler testing which achieves good and stable acceleration performance.
c. software defect prediction there is a large amount of research on software defect prediction which predicts defect prone programs and code modules by training classifiers based on the features extracted from historical defective code.
different from defect prediction our work targets at predicting the bug revealing probabilities of test programs not the defect proneness of test programs themselves.
vii.
c onclusion in this paper we propose the idea of learning to test which utilizes the characteristics of existing test cases that trigger bugs.
based on this idea we develop let a learning to test approach to accelerating c compiler testing.
this approach has two processes learning and scheduling.
in the learning process let identifies a set of features of bug revealing test programs and trains a capability model and a time model.
in the scheduling process let ranks new test programs based on the two models.
we evaluate our approach using two compiler testing techniques i.e.
dol and emi two subjects i.e.
gcc and llvm and two application scenarios i.e.
crosscompiler scenario and cross version scenario .
our experimental results demonstrate that let accelerates c compiler testing significantly in all settings.
viii.
a cknowledgment this work is supported by the national key r d program of china under grant no.2016yfb1000801 and the national natural science foundation of china under grant no.
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.