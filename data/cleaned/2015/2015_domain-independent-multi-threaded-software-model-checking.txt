domain independent multi threaded software model checking dirk beyer lmu munich germanykarlheinz friedberger lmu munich germany abstract recent developmentof softwareaims atmassively parallelexecution because of the trend to increase the number of processing units per cpu socket.
but many approaches for program analysis are not designed to benefit from a multi threaded execution andlacksupport toutilizemulti corecomputers.rewriting existingalgorithmsisdifficultanderror prone andthedesignofnew parallel algorithms also has limitations.
an orthogonal problem is thegranularity computingeachsuccessorstateinparallelseems too fine grained so the open question is to find the right structurallevelforparallelexecution.weproposeanelegantsolutionto theseproblems blocksummariesshouldbecomputedinparallel.
many successful approaches to software verification are based on summaries of control flow blocks large blocks or function bodies.
block abstractionmemoizationisasuccessfuldomain independent approach for summary based program analysis.
we redesigned the verification approach of block abstraction memoization starting fromitsoriginalrecursivedefinition suchthatitcanruninaparallelmannerforutilizingtheavailablecomputationresourceswithout losing its advantages of being independent from a certain abstract domain.
we present an implementation of our new approach for multi core shared memory machines.
the experimental evaluation showsthatoursummary basedapproachhasnosignificantoverhead compared to the existingsequential approach and that it has a significant speedup when using multi threading.
ccs concepts software and its engineering formal software verification theory of computation parallel algorithms keywords program analysis software verification parallel algorithm multithreading block abstraction memoization acm reference format dirk beyer and karlheinz friedberger.
.
domain independent multithreadedsoftwaremodelchecking.in proceedingsofthe201833rdacm ieee international conference on automated software engineering ase september3 montpellier france.
acm newyork ny usa 11pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september montpellier france association for computing machinery.
acm isbn ... .
introduction program verification hasbeen appliedsuccessfully to find errors inapplicationsortoprovetheircorrectness.recenthardwaredevelopmentaimstowardsparallelexecutionofprogramseitheron multi coremachinesorsharedacrossseveralmachinesinacomputing cluster.
for large scale program verification we do not only need efficient algorithms but also make use of available hardware resourcesuptotheirlimits.therearesomeapproachestoleverage such systems but most recent algorithms for program verification and model checking are not designed to work in parallel mannerandutilizeonlyasmallpartofavailableresources.thereare several reasons for this either the verification algorithms have dependencies between intermediate results such that only a sequential execution is useful or the amount of parallelism is bound by a small number e.g.
only two analyses are executed in paral lel and communicate information effectively using only a small numberofcpucores.themainquestioniswhetherandhowwe can re design existing verification techniques such that they can be executed on parallel computer architectures.
we contributetheidea touse summariesasthe objectsto compute in parallel instead of inventing new parallel state space iterationalgorithms.block abstractionmemoization bam isaparticularly nice method to summarize blocks of program statements because it is independent from a particular analysis it wrapsan existing analysis without much interference and stores block summaries in a cache.
we use this concept to develop a domainindependent analysis that distributes a verification problem across multipleprocessingunitswithoutchangestotheanalysistechnique.
our analysis is based on a standard state space exploration using a control flowautomatonthatrepresentstheprogram.theapproach is orthogonal to other data flow based analyses and thus it can be combinedwithanalysesbasedondifferentabstractdomainslike bdds explicit values intervals or predicates.
thevalueofourapproachisitslevelof separationofconcerns it separatestheconcernofmakingananalysismulti threadedfrom the concern of designing and implementing an abstract domain and its operators.webase our approach onbam and use most of itsdatastructures suchthatmostpartsofthe wrapped analysis and its implementation remain unchanged.
we redesigned the algorithm such that we can efficiently execute it across severalprocessing units.
the parallelism of the analysis is only bound bythe structureof theprogram tobe analyzedandthe amountof workfoundduringtheanalysis.ourworkincludesatransformation of the existing algorithm of bam from a sequential recursively definedalgorithmintoaparallelapproach.additionally webenefit fromtheexistinginfrastructureofbam i.e.
wealsouseacache forblockabstractionsandapplytheoperators reduceandexpand to increase the cache hit rate.
the analysis is sound implemented authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france dirk beyer and karlheinz friedberger in the open source verification framework cpachecker and can be combinedwithexistingcomponentsoftheframework including cegar or witness export .
contributions.
we make the following contributions we introduce a new technique for parallelization of verification algorithms that is independent from particular abstract domainsbecauseitisbasedonaflexibleandconfigurableblock summarization.
we implemented the technique in the open source verification framework cpachecker .ourimplementationandallexperimental data are available to other researchers and practitioners for replication via our artifact and supplementary website.
we evaluated our new technique on a large set of benchmarks andshow thattheparallelversionofbam ifusingonlyone cpu core behaves similar to the sequential version i.e.
there isnosignificantoverheadforparallelization and thatthe parallel version of bam significantly improves the responsetime of the verification process for programs that are large enough to benefit from multi threading.
related work.
the idea to use parallel algorithms in software verification is not new.
there exist several approaches reaching from plain parallel execution of different algorithms until the first analysis succeeds via one way communication between some analyses one analysis provides additional information for another one to fully parallel analyses dividing the state space into par titions that are explored separately .portfolio approaches.
a simple but effective approach is to run a portfolio analysis i.e.
a fixed number of predefined analyses in parallel to leverage the available cpu cores on a single machine suchthattheverifierterminateswiththefirstsucceeding analysis e.g.
.
this strategy is applied either to separately explore the state space e.g.
with different domains or in a way thatoneanalysisprovidesinformationforanotherone forexample to enrich it with additional invariants .
such approaches for parallel software verification are not scalable due to its fixednumber of different analyses and they suffer from the problem that each single analysis only uses a small fraction of the available resources.
if all but one analysis fail to determine a verificationresult because of unsupported features in the task or imprecisionoftheanalysis theremainingworkissometimeslimitedto a single analysis and thus a single core.
multi threadingapproaches.
spin anddivine arebased on pure explicit model checking and use a central hash table to check for existing already analyzed states.
ltsmin either performs explicit state space search in a parallel manner or uses a bdd based approach using the bdd library sylvan that internallyparallelizesitsoperations.otherapproachesdivideagiven problemintosmallercomponentsthatareverifiedseparately before joining the results to get a proof for a whole program .
anexampleimplementationforsuchatechniqueisthetool softverthat uses bdds and predicates.
structurally definedconditionalanalysis isanapproachthat splits a program according to conditions as in conditional modelchecking that is given a program pand a condition t w o instances can be created one conditional analysis of p and and one conditional analysis of pand .
the two analysis instances are completely independent and can be executedin parallel.
the approach can scale up to an arbitrary number of splits.
the elegance of this approach is that it does not depend on a specific implementation but can be built on top of existing off the shelf tool components.multi machine approaches.
state space exploration can be distributed across several machines by partitioning the possible statespace.
tools like spin cseq swarm or the spark analysis tools divide the verification problem after a short pre analysis of the program and split the potential state space and the verification condition according to given time and memory limitations available processing units or other criteria.
this approach is potentiallyproblematicduetotheunknownnatureoftheprogram to be analyzed e.g.
it might not match the pre defined schedul ing.
for degenerated state spaces the parallel analysis might be imbalancedbetweendifferentthreads processes.othertoolslike ltsmin o rdivine circumvent such imbalances by a dynamic scheduling approach.
the approach of structurally defined conditional analysis can also be extended to benefit from multi machine environments.
our contribution is a more general parallel technique for program analysis and can be applied to an arbitrary domain and even combinations of several domains.
thus explicit value analysis bdd based analysis as well as predicate analysis can benefit from our approach.
the parallelism of the approach presented in this paperisbasedontheinternalstructureoftheprogram i.e.
anautomatic partitioning of the control flow and tries to use all available processingunits onlydependingonthedynamicbehaviorofthe programanalysis i.e.
theunfoldingoftheabstractstatespace.
background thefollowingsectionprovidesanoverviewofbasicconceptsand definitions that our approach is based on.
we describe the program representation configurable program analysis the details of block abstractionmemoization andhowweadvancedittowards anefficientparallelalgorithmforprogramanalysis formorede tail see the original articles .
.
program representation werestrictthepresentationtoasimpleimperativeprogramming language where all operations are either assignment or assume operations.aprogramisrepresentedbya control flowautomaton cfa a l l0 g which is a directed graph consisting of a set lof program locations modeling the program counter a set g l ops lofcontrol flowedges modelingthecomputationsteps fromonelocationtothenext assignmentorassumeoperations and an initial program location l0 entry point of the program .
.
cpa and cpa algorithm aconfigurable program analysis cpa is specified by an abstractdomainforaprogramanalysisandoperatorstomodelthe behavior of the program analysis a cpa d d leadsto merge stop consists of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
domain independent multi threaded software model checking ase september montpellier france an abstract domain d c e that consists of a set cof concrete states a lattice e e subsetsqequal over a set eof abstractdomainelements i.e.
abstractstates andapartialorder subsetsqequal and a concretizationfunction that mapseach abstract domain element to the represented set of concrete states.
atransferrelation leadsto e ethatyieldssuccessorsofanabstract state.
amergeoperator merge e e ethatdetermineshowto merge two abstract states when control flow meets .
a termination check stop e 2e bthat specifies whether an abstract state is covered by a set of abstract states.
algorithm 1cpaalgperforms a state space exploration.
it computesanoverapproximationofthereachablestatesbyconstructing abstract states for the program based on a given cpa and an initialabstractstate.thealgorithmisafixed pointiterationandmaintains a setwaitlistof abstract states that still have to be explored and asetreachedofalreadyexploredabstractstates.ineachiteration thealgorithmtakesanabstractstatefrom waitlist line2 andcomputes itssuccessors line3 .
thealgorithm checkswhether a new statecanbemergedwithanexistingstate andupdatesthework setsaccordingly lines5 .theoperator stopensuresthatthenew abstract state is only added to the work sets if the abstract state is notalreadycoveredbyanyoftheexistingstatesin reached lines9 .thealgorithmterminatesifeithertheset waitlistisemptyor thereisanotherreasontoabortearly e.g.
apropertyviolation.
we use a simplified version of algorithm cpaalg in order to shorten the presentation.
the precision and precision adjust ment which determine the granularity of the analysis within a cegarloop areneglectedinthisdescription butfullyavailable and supported in our implementation.
differentaspectsofaprogramareanalyzedbydifferentcpas and compositions of cpas allow more advanced analyses.
cpas algorithm cpaalg d reached waitlist taken from input ac p ad d leadsto merge stop whereedenotes the set of elements of the lattice of d a setreached eof abstract states a setwaitlist reachedof frontier abstract states a function abort e bthat defines whether the algorithm should abort early output the updated sets reachedandwaitlist whilewaitlist nequal do pop e f r o mwaitlist foreache primewithe leadstoe primedo foralle prime prime reacheddo enew merge e prime e prime prime ifenew nequale prime primethen reached reached enew e prime prime waitlist waitlist enew e prime prime if stop e prime reached then reached reached e prime waitlist waitlist e prime ifabort e prime then return reached waitlist return reached waitlist have been defined formany abstract domains such as bdd based analysis explicitorsymbolic valueanalysis predicate analysis orcombinationthereof .alsothetracking of the program counter and of the call stack for procedures are defined as cpas.
we will not go into detail for all their definitions and descriptions here because our approach works on an abstract level and is independent from a specific domain.
for our evalua tion later we use a value analysis that tracks variables and their values explicitly e.g.
an abstract state is a partial function that maps program variables to values.
.
bam block abstraction memoization bam is a modular approach forreachabilityanalysisofabstractstategraphs suchasabstract modelsofprograms .therefore ittreatsalargeprogramasaset ofblocks and analyzestheblocksseparately.
theresultof ablock analysis the block abstraction of a nested block is embedded in thesurroundingblock sanalysis.blockabstractionsarealsostored inacacheforlaterreuseinordertoavoidrepeatedcomputationof the same block abstraction to speed up the analysis.
bam defines the two operators reduceandexpandthat aim at a higher cache hit rate.
for simplicity we will neglect both operators inthe further description.
they are orthogonal to the approach ofparallel analysis that we present here.
thecomponentsofbamaredefinedindetailinthefollowing .
.
blocks.
the basic components of bam are blocks which are formally defined as parts of a program a block b l prime g prime o fac f a a l l0 g consists of a set l prime lof connected program locations and a set g prime l1 op l2 g l1 l2 l prime of control flow edges.
two different blocks b1 l prime g prime and b2 l prime g prime are either disjoint l prime l prime or one block is completely nested in the other block l prime l prime .
each block b l prime g prime hasentryandexit locations which are defined as in b braceleftbigl l prime l prime op l g l prime nelementl prime l prime op l g bracerightbigand out b braceleftbigl l prime l op l prime g l prime nelementl prime l op l prime g bracerightbig r e spectively.in general theblock sizecanbe freelychosenin bam.
inmostcases functionsandloopsareusedasblocksize because theyrepresentthelogicalstructureofaprogramandleadtonatural block abstractions.
figure1showsaschematicexampleofacfaandhowitcouldbe dividedintoblocks.itdoesnotshowanyoperations weomitdetailsfor ease of presentation.
the largest block denoted as ba consists of all locations and represents the whole cfa of the program.
the other blocks denoted as bbtobf are smaller and consists of fewer locations.
block bfis nested in block be which in turn is nestedinblock ba.location3istheentrylocationofblock bb i.e.
in bb and location is its exit location i.e.
out bb .
.
.
bam cpa.
the basis of cpachecker is the idea of configurableprogramanalysis.thus bamisformalizedasacpa bam dbam leadstobam mergebam stopbam .bamworksonanabstract domain independent level and uses an abstract domain dependent wrappedanalysis likethebdd based explicitvalue interval or predicateanalysis totrackvariablesandvalues.thiswrappedanalysis is also given as cpa w dw leadstow mergew stopw based on which we now formalize bam authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france dirk beyer and karlheinz friedberger 16ba bb bc bd be bf figure schematic control flow automaton with blocks the domain dbamwraps the domain dw.
the transfer relation leadstobamfor a block bhas a transfer s leadstobams primefor two abstract states sands primeif s prime s prime prime sbsub leadstobams prime prime ifl in bsub apply bam to bsub s prime prime s leadstows prime prime ifl nelementout b delegate to w wherelis the program location of s. dependingonthecurrentlyanalyzedprogramlocation l the transfer relation chooses between two possible steps for an entrylocationofablock bsub theoperationbsub leadstobamrepresents the block abstraction for the block bsuband the block entry abstract state s. the block abstraction is computed by a call cpaalg dbam s s .forexitlocationsofblocks thereisno succeedingabstractstate intheanalysisofthecurrentblock b .
for other program locations the wrapped transfer relation leadstowis applied.
themergeoperator mergebam mergewandthetermination checkstopbam stopwcorrespond to the wrapped analysis.
the performance of bam can easily be increased by a cache cache blocks e 2e 2e whichmapsablockandanentry abstract state of the block to the set of reached abstract states and thesetoffrontierstates.thecacheisoptionalfortheapplication ofbam butthememoizationofblockabstractionsimprovesthe performance.additionally theoperators reduceandexpandcanbe appliedforahighercachehitrate.weignorethemforsimplicity.
.
towards parallel bam a simple state space exploration that enumerates all reachable abstract states and only checks whether they were already part of the set reachedcan be done with the operators mergesepandstopsep defined as mergesep e e prime eand stopsep e r e prime r e subsetsqequale prime or even with a simpler formstopsep e r e prime r e e prime .
well known techniques for explicit statemodelchecking usesuchanapproachtoanalyze the state space.
this approach can be parallelized easily by synchronizing the access to the existingabstract states in the sets reachedandwaitlistandapplyingtheoperators leadsto mergesep and stopsepconcurrently.
with lock free implementations of the set data structures for reachedandwaitlistthere is only minimal synchronization necessary for an efficient analysis.
however whenusing more general and possibly more expensive operator in stances the complete sets reachedandwaitlist and also larger parts of the cpa algorithm would need to be locked to ensuresingle thread access which prevents an efficient parallel appli cation of the algorithm.
tocircumventthisproblem ournewapproachdoesnotintroduce parallelismwithin the cpaalgalgorithm butapplies several independent cpaalginstances in parallel.
each cpaalginvocation is executed in a separate thread on its own part of the state space i.e.
with its own sets reachedandwaitlistof abstract states such that there is only minimal communication between the algorithminstances.thenecessaryinfrastructureforsuchanapproach is based on bam.
the previously given basic definition of bam leaves room for several implementation details such that both the sequential and the parallel implementation match the given specification.
the computation and application of block abstractionscan be done in sequential or parallel manner.
parallel bam our contribution is a scalable parallelization of the sequential algorithm of bam.
block abstractions are independent from each otherandalsofromthesurroundingcontext.thus theycanbecomputedinparallel assoonastheinitialabstractstateofablockabstraction is known.
the sequential version of bam which was defined by wonischandwehrheim recursivelycallsanothercpaalgorithmforeachnewlyenteredblock waitsforitsterminationand directly uses the result as a block abstraction of the entered block.
in contrast to that our parallel version schedules the computation of a nested block abstraction in another thread and continues with the analysis of further abstract states from the set waitlist.
each block abstraction is computed by a separate instance of thecpaalgalgorithm inownthread withowninstancesofthe setsreachedandwaitlist andathread safeinstanceofthetransfer relation leadstoand the operators mergeandstop.
the operators are stateless and thus can be used in parallel from several threads.
thereisnoneedtolockthedatastructuresofa cpaalginstance.
in parallel algorithms a critical point is the number of synchronizations.blockabstractionsare largeenough toavoidexpensive synchronization for single steps during the computation.
synchronizationisonlyneededwhenenteringorleavingablock i.e.
whenstartingorterminatingablock sanalysisinstance.additionally the communication only happens between dependent block abstrac tions such that no global locking is required in the algorithm.
.
jobs as components with dependencies our technique is based on the parallel execution of componentsnamedjobs.
a job job d reached waitlist b consists of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
domain independent multi threaded software model checking ase september montpellier france ac p ad d leadsto merge stop that determines the analysis in our case we always set d bam asetreachedandasetwaitlistofabstractstatestobeanalyzed and ablockb l prime g prime representingthepartitionoftheprogram s cfa to be analyzed.
a job is executed by applying alg.
1cpaalgwith the given cpadon the sets reachedandwaitlist.
note that there can be severaljobsforthesameblock b buteachset reachedandeachset waitlistareassignedtoexactlyonejob.therearenoshareddata basedonabstractstatesfordifferentjobs.thisallowsustoexecute jobsinparallel becausethejobexecutionsareindependentfrom each other.
if a block has nested blocks the corresponding block abstractiondependsontheblockabstractionsofthosenestedblocks.
inthesequentialimplementationofbam thedependenciesofblock abstractions on nested block abstractionsareimplicitly solved by callingalgorithm cpaalgrecursively i.e.
theanalysisofanouter blockwaitsuntilanestedblockabstractioniscomputedcompletely andthencontinues.intheparallelapproachweexplicitlymaintain such dependencies between analyses of block abstractions.
a relationdeps jobs e jobstracksatwhichabstractstateablock abstractionneedstobecomputedandapplied.thisrelationneedsto be globally visible shared across all threads and modifications are appliedatomically.asdependenciesareonlymodifiedwhenajobisstartedorterminated theoverheadforsynchronizationisnegligible.ourimplementationdoescurrentlynotsupportrecursivetasksand thustherearenocyclicdependenciesbetweenblockabstractions.
.
scheduling and job execution the parallel execution of analyses needs a scheduling algorithm that distributes the parallel running analyses onto the availableprocessing units.
in our case we chose a simple task queue fromthe java concurrency api where we insert our jobs and let the frameworkdothescheduling.wecansetthenumberofrunning threads to the available hardware by using the default java thread pool.
for simplicity of alg.
the actual scheduling is hidden in thecallschedule that asynchronously executesthegivenjobwith the given data.2this solution has only small overhead for run timeandfordevelopers andisperformantenoughfortheanalysis even when applied to a larger scale of computing resources.
we have nearly linear speedup when using multiple cores see sect.
thus we assume that the build in scheduling is efficient enoughfor our currently available hardware.
the basic idea of a parallel implementation is given in algs.
and3.
the function abortof alg.1cpaalgterminates the analysis as soon as a nested block abstraction needs to be computed.
in this case we determine the necessary data to compute the block abstraction in our scheduling algorithm and schedule a new analysistocomputethenested blockabstractionasynchronously.the abstractstatebeforeenteringtheblockisremovedfromthecurrent setwaitlistand stored as a part of the dependency relation deps.
afterthecomputationofthenested blockabstractionisfinished thedependencyisremovedfrom depsandthestateisre addedinto 2the pseudo code omits some scheduling related code as this would be too much detail for this description and can be looked up in our reference implementation.algorithm parallelbam d reached waitlist initial step for parallel bam input ac p ad d leadsto merge stop whereedenotes the set of elements of the lattice of d a setreached eof abstract states a setwaitlist reachedof frontier abstract states a global relation deps jobs e jobsto track computations of block abstractions output a set of reachable abstract states a subset of frontier abstract states deps mainjob d reached waitlist mainblock jobexecutor mainjob deps return mainjob .reached mainjob .waitlist algorithm jobexecutor job deps statestoadd job execution for parallel bam input ajob d reached waitlist b a global relation deps jobs e jobsto track computations of block abstractions a setstatestoadd eof abstract states to be added before starting the analysis job.waitlist job.waitlist statestoadd deps deps braceleftbig job e deps e statestoadd bracerightbig job.reached job.waitlist cpaalg d job.reached job.waitlist missin bas e reached hasmissin ba e ifmissin bas nequal then nested ba needed fore missin bas do job.waitlist job.waitlist e childjob c e e getblock e deps deps job e childjob schedule childjob deps schedule job deps else finished job.waitlist job deps shouldabort e job.reached abort e iffinished shouldabort then registerba job .reached shouldabort parents job deps for parentjob updatestate parentsdo schedule parentjob deps updatestate deps deps parents thesetwaitlist.thefunction schedule executesthegivenjobasynchronouslywithalgorithmalg.
.theasynchronousexecutionofa jobcanbedelayedduetolimitedresourcesorbecausethesamejob isscheduledtwice i.e.
withdifferentarguments.weuseathread pool for scheduled jobs based on a job queue with a fifo ordering strategy.
the function scheduleandwait does the same but awaits theterminationofthejob.themethod registerba isexecutedwheneverablockanalysisterminates.itextractstheblockabstraction from the analyzed set reachedand updates the cache of bam.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france dirk beyer and karlheinz friedberger timeblock babbbcbdbebf a1a2a3a4a5a6a7 a8b1c1d1e1f1 e2 figure2 schematictimelineofapossibleexecutionofjobs with parallel bam for the example in fig.
.
example application of parallel bam the cfa in fig.
1consists of two characteristic parts the upper parthasheavybranchingandseveralcontrol flowpaths thepart belowlocation17consistsofasimplechainoflocations.parallel bamimplicitlyrecognizesthisstructureandtheschedulingwill apply a parallel analysis for the upper part.
figure 2shows a possible time line for the execution of the new algorithm for the cfa given in fig.
.
the heavy branching part of the program results inindependentblocks bb bc andbd whichcanbeanalyzedin parallel.
each box in fig.
2represents a job consisting of a cpa w a setreached as e twaitlist and a block b ba ... bf .
for each block more concretely for each set reached there can be several jobs that are applied in sequential order.
fortheexample letusassumeadepth firstsearchasiteration order and an expensive computation in the blocks bb bc and bd.
in general the iteration order for the program analysis can be configured by the user and the effort to analyze blocks de pends of course on the given task.
initially alg 2createsjob a1 alg.
line2 fortheanalysisof the block ba.
figure2shows the execution of job a1 with alg.
as a box along the time axis.
internally alg.
1cpaalganalyzes the first abstract states of the given task alg.
line until the entrylocationofblock bbisreached.algorithm cpaalgterminates for the job a1 and two further independent jobs a2 andb1a r e scheduled alg.
line and and executed in parallel.
the jobb1 analyses the block bband is not interrupted by another block entry location.
the job a2 is scheduled because there is a branching at location in the cfa such that the set waitlistof the terminated cpaalgin joba1 was not empty.
for the example we assume that the job a7 analyzes the program location with cfa location .
for the part below location however inter block dependencies prevent a parallel execution of jobs and we need to explicitly wait for nested block abstractionsto be computed.
in fig.
2this is visible for jobs e1 f1 e2 and a8 which do not have any parallel execution.
overall our parallel version of bam uses a dynamic scheduling such that suchimbalances are prevented in most cases.
.
soundness of the parallel approach we take a short look at the soundness of the parallel algorithm basedonitssequentialinstance.themaindifferencebetweenthe sequentialandtheparallelversionofbamisthecomputationorderofblockabstractions.insteadofcomputingoneblockabstraction after another they are computed in parallel whenever possible.
as thecomputationsofblockabstractionsthemselvesareindependent and do not share any relevant data the theoretical basis for soundnessdoesnotchange.thus theparallelapproachisassoundasthe sequential algorithm that was proven to be sound in i.e.
only the iteration strategy for the state space differs and the soundness relies on the underlying analysis wof bam.
in other words if thereexistsanabstractpathintheanalyzedsourcefilethatreaches a property violation then the same path is also explored by theparallelalgorithm consistingofthesameblockabstractionsandabstract states as computed by a sequential analysis.
.
requirements for parallel execution our parallel approach has some additional requirements on the usedcomponents eachusedcpahastoallowmulti threadedaccesstoitsmaincomponents theoperatorsmustbethread safeand usableinparallel.thiscaneitherbeimplemented a bystateless operators which is the intended behavior of operators anyway or b by separate instances of the operators for each accessing thread including independent data structures .
a an ideal frameworkwouldonlyhavestatelessoperators justastheirtheoretically definedmathematicalpendant andthus theywould easilybeusable in multi threaded context without locking or synchronization.
b while the operators are stateless in theory a large software system such as the framework cpachecker where the developers integrateseveraldifferenttheoreticalapproaches requiresanimplementationthatpartiallydeviatesfromtheconceptofstateless operators.
we noticed that the transfer relation leadstoand also the operators mergeandstopfor several cpas were already designed and implemented in a stateless manner such that they can eas ily be used for our parallel bam implementation.
depending onthecpa mostofthecode andalsomostofthetheoreticalbackground isplacedinthetransferrelation andthustheconceptual difficulty was to rewrite those parts that are critical and might needtobesynchronized.toavoidheavysynchronization wehave convertedsome non critical partslikestatisticsandtimemeasurement into a thread safe implementation or provide independent instances of operators for special cases.
we have not only added the new algorithms alg.
2and3 for parallel bam into the framework but also modified some other componentssuchthattheycanbecombinedandusedwiththenew algorithm.thefollowinglistcontainsafewcornercasesofcpas that were touched or are usable with our approach locationcpa the program location for the current analysis is trackedwiththelocationcpa.astheprogramlocationofeachstatementisconstantafterparsingtheprogramandwrittenintothe cfa location the state of the operators is the immutable cfa itself.
thus no changes had to be made.
callstackcpa thecallstackforthecurrentanalysisisdetermined by the callstackcpa.
as the corresponding operators arestateless i.e.
onlydependingontheabstractcall stackstate given as parameter no changes were required.
valuecpa thevaluecpaperformsanexplicit valueanalysis and tracks numerical values for variables.
the analysis itself does not need to be changed for synchronization.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
domain independent multi threaded software model checking ase september montpellier france evaluation this section compares our new parallel approach with the existing sequentialimplementationandshowsthatthenewapproach can reducetheresponsetimeconsiderablywhenexecutedonseveral cores.first wecomparetheoldsequentialimplementationwiththe newimplementation executedwithonlyonethread inorderto show that no regression appears and that both analyses behave as similar as possible.
second we explore the speedup of the analysis dependingonthenumberofthreads asfarasourhardwareallows .
.
evaluation goals itisclearfromtheorythatnotallverificationtaskswillbenefitfrom ourparallelizedverificationapproach a therearemanyprograms wheremostpathshavesequentialdependenciesbetweenblocksand therefore thereisnotmuchroomforperformanceimprovements from parallelization and b there are many small programs for whichparallelizationdoesnotmakeadifference.weclaimthatour approachiseffectiveinbothr egards itparallelizesand speedsup verificationprocess responsetime ifthestructureoftheprogram containssufficientbranchingandthesizeoftheprogramislarge enoughanddoesnotnegativelyinfluencetheperformanceforthose verificationtasksthataresmallorhavesequentialdependencies.
claim .
thebam basedapproachtoparallelizationdoesnot negatively impact the performance of verification tasks overall.evaluation plan we take a large benchmark set of verification tasksandverifythemwithandwithoutparallelization restricted to one processing unit.
if the run time is not worse for the par allel version then the claim is valid.
claim .
the bam based approach to parallelization reduces the response time of verification tasks by leveraging several processing units.
evaluation plan we take a large set of verification tasks that can potentially benefit from parallelization and comparetheresponsetimeoftheverificationwithdifferentnumbers of processing units.
if this experiment is positive the question raises where the benefitcomesfrom isitthebam basedapproachtoparallelization or arethereothertechnicalcomponentsoftheverifierthatcontribute to the speed up?
what are the configurable parts of the verifier that can benefit from parallelization?
can they be controlled in an experiment switched on and off separately ?
claim3.
theparallelizationoftheprogramanalysisusingbam contributes considerably to the speedup.
evaluation plan after identifying variables to control we run experiments to investigate the influence of the identified components.
.
benchmark environment and limitations benchmark sets.
for our evaluation we use a large subset of the sv benchmarks repository containing over 5400verification tasks3 sortedintodifferentcategoriesaccordingtheirspecification internal structure or behavior.
for the comparison of the existing sequential implementation with the new parallel approach limited toonecpucore weuseallverificationtaskswithareachability property in order to evaluate on a diverse set that the approach has no negative effect claim .
to demonstrate the positive effect of parallelization of the new approach we chose those verification from the category reachsafety eca that consists of rather large problems with a highly branching control flow claim .
setup.werantheexperimentsonaclusterof168identicalmachineswithahardwarespecificationthatroughlymatchesavailable resourcesonmachinesofsoftwaredevelopers.thisway replication of our experiments does not require specific hardware.
foreachsingle verificationrunwe limitthecpu timeto15min andthememoryto15gb andweuseanintelxeone3 1230v5cpu with3.40ghzwith8processingunits 4physicalcoreswithhyperthreading .the limitof cputimeenables usto evencompare the effectiveness of parallelization response time vs. cpu time for thoseverificationtasksforwhichtheverifierrunsintoatimeout.
weevaluatedourimplementationin cpachecker4 revision r28809 from the official project repository5.
because we use intel processors with hyper threading where two neighboring virtual processing units share some hardware components and influence each other we pair the virtual process ingunitsanduseastepwidthof2forourexperimentswithvarying numberofprocessingunits i.e.
weuse2 8processingunits and omit the odd numbers of processing units.
the benchmarking framework benchexec takescareofcorrectlyassigningthetwo processing units of the same physical core together to the verificationprocesses.wereportalltimesinsecondsandusetheterm cpu time for the accumulated usage of processing units of a cpu andtheterms responsetime orwalltime forthetimethatelapses between the start and the termination of the verification run.
analysis configuration.
we configure bam to use function and loop bodies as blocks.
bam can be combined with several analy ses for our evaluation we choose a combination where the per formance influence from additional components is small bam with an explicit value analysis va without cegar.
this way we configure a simple state space exploration based on an explicit tracking of variables and their values.
both the sequentialand the parallel configurations apply a depth first search as ex ploration strategy i.e.
the set waitlistof the cpa algorithm is a fifo queue for each configuration.
unfortunately wecannotcomparetoothermulti threadingverifiers for reachability properties of sequential c programs because thereexistsnoequivalentapproachtothebestofourknowledge cf.relatedworkintheintroduction thereareportfolioverifiers .
.
claim i sequential vs. parallel algorithm configuration.
in our first experiment we compare the existing sequential algorithm with the new parallel algorithm.
therefore werunallexperimentsononlyoneprocessingunit.thefifoordering of the job queue see sect.
.
in the parallel algorithm guarantees that block abstractions are computed in the same order as their blocks are reached i.e.
it behaves as similar as possibleto the sequential algorithm.
results.figures3aand3bshow the response time of the configurationsforthebenchmarksetcontainingallverificationtasks withareachabilityproperty.aquantileplotcontainsgraphsthat indicate the quantile of solved problem instances x axis each authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france dirk beyer and karlheinz friedberger n th fastest resultresponse time s va bam va parallelbam thread a verification tasks with correctness proof n th fastest resultresponse time s va bam va parallelbam thread b verification tasks with property violation figure3 quantileplotsforresultsofbamwithvalueanalysis sequentialcomparedtoparallelversionwithonethread withinacertainresponsetime y axis .6itdoesnotshowadirect comparison for individual verification tasks but allows to compare the overall behavior of an analysis configuration.
we divided the benchmarks into two groups the plot in fig.
3acontains results for all verification tasks for which a correctness proof wascomputed the plot in fig.
3bcontains results for all verification tasks for which a property violation was found.
the overall im pression is that the single threaded parallel technique does not haveanynoticeableoverheadabovethesequentialapproach i.e.
the scheduler and the job executor from alg.
3are efficient.
the new approach behaves almost identical when computing proofs andforfindingpropertyviolations itisevenfasterandcansolve more problems which we discuss in the following.
discussion.
the difference in fig.
3bbetween the verification approaches results from the exploration order of the state space.
afteranested blockabstractionhasbeencomputed thereisasmall differenceinthesortingofabstractstatesinthesets waitlistofboth approaches.
the existing sequential analysis has and keeps theabstract states in the set waitlist.
the single threaded parallel approach removes abstract states when finding a missing block abstraction cf.
alg.
line and re adds those abstract states into each setwaitlist cf.
alg.
line after computing the necessary block abstraction.
there are small differences in the exploration orderanddependingonthetask sstructure differentpathsmightbe analyzed first.
in those cases the parallel approach does not apply a pure depth first exploration order but partially prefers paths 6a detailed description of quantile plots can be found in the literature .thatdonottraversedeeplynestedblocks whichseemsbeneficial when it comes to finding property violations.
for this reasoning weconcludethatforevaluatingclaimii itwouldnotbevalidto consider the verification tasks with property violations because the variable exploration order is not controlled.
we conclude that claim holds because we did not observe any negative impact of our new approach.
.
claim ii scalability of parallel bam configuration.
we show the effectiveness of the parallelization of our new approach by increasing the number of threads 8threads andobservetheimprovementoftheresponsetime.
theupperlimitofthenumberofthreadsisdeterminedbythehardware that we use.
we set the number of processing units assigned to the verification process to be equal to the number of threads.
we chose a subset of tasks from the category reachsafety eca suchthattheyneedareasonableamountoftime atleast 3swith onlyonethread anddonotcontainapropertyviolation.withatoo small analysis time the default overhead of the cpachecker framework itself like jvm startup time or parsing time hides the effect of the parallel approach and blurs the picture.
additionally finding a path to a property violation with a parallel verification approach easilyleadstonon deterministicresultsifthereareseveralproperty violationsinaverificationtaskorapropertycanbereachedviadifferentprogrampaths7.thus weselectfromthebenchmarksetonly thoseverificationtaskswithoutpropertyviolation inordertomake sure to compare the response time that is necessary to analyze the wholestatespace.theusedbenchmarksetconsistsofthreegroups 47simpletasks 36mediumtasks and71difficulttasks.thedifficulty is roughly given by the size of the state space to be explored.
results.figure4ashows the response time of the configurations for the benchmark set.
each function graph in the quantile plot referstoadifferentnumberofthreadsusedintheanalysis.asmaller response time of the analysis corresponds to a smaller state space and relates to a simpler task.
the different groups of verification tasks simple medium anddifficult areclearlyrecognizablebythe levelofresponsetime i.e.
theplotcontainslargersteps.overall additional threads improve the performance of the analysis.
figure4bshowsthespeedupofourparallelapproachoverthe single threaded application in the evaluation using box plots.
each entryintheplotshowsthemedianasthehorizontallinewithinthebox togetherwithitstwosurroundingquartilesbetweentheupper andlowerlineofthebox aswellastheminimumandmaximum as whiskers.
the speedup becomes larger the more threads we use.
theevaluationwith2threadsoutperformsthesingle threadedexecutionbyabout20 median .theparallelapproachwith8threads is about three times as fast as with threads.
discussion.
the results look impressive only by parallelizing independent bam explorations in a way that is not tailored in any specific way towards the framework or to a particular abstract domain weobserveasignificantimprovementoftheresponsetime.
obviously somepartsoftheverificationprocesscannotbeexecuted inparallel.thisdeniesa perfect parallelizationandisknownas amdahl slaw .thesequentialpartsincludethestartupprocessof 7the supplementary artifact and website include additional data about the evaluation of our approach on a benchmark set of tasks containing a property violation.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
domain independent multi threaded software model checking ase september montpellier france source fileresponse time s threads a quantile plot for verification tasks without property violation threadsspeedup b box plot comparing thread to n threads without property violation figure comparison of response time for different numbers of threads based on restricted benchmark set cpachecker aswellastheinitialoverheadoftheanalysistocompute blocksforbamandanalyzepartsofthemostouterblockuntila nested block isreached which inturn can be analyzed ina parallel manner.
some parts of the implementation cause an additionalsynchronization overhead like multi threaded statistics for the concurrent access to shared resources like the cache of bam.
the rathermodestimprovementfrom1threadto2threadsismostlikely due to hyperthreading of the processor where the two processing unitsofonephysicalcoreshareimportanthardwareresources.
we conclude that claim holds because the experiments show thatforthoseprogramsthathavepotentialforspeedupbyparallelization we actually observe a significant speedup.
.
claim iii control influencing variables thepreviousexperimentsshowthatseveralprocessingunitsare effectively used by the verification tool but it is unclear where the benefit comes from.
therefore we need to investigate which parts oftheverifierareparallelizedandmakesurethatournewapproach contributed to the benefit.
since our implementation is based on java we have also enabled the jvm to use multi threaded garbage collection gc because if we create abstract states in a parallelmanner we should also deallocate them in parallel.
the default strategyforgcinopenjdk1.
.0isacombinationof psmarksweep andps scavenge.
the mark sweep collector applies a full marksweepgarbagecollectionalgorithmforold generationobjects.the parallel scavenge collector cleans up young generation objects.
8in our experiments we assigned successive processing units to the verification runs the experiment with threads could be improved by using two processing units of different physical cores.
analysis threads major with gc threads minor speedup a boxplotcomparingtheresponsetimeof1threadto8threads evaluated on as many processing units as analysis threads analysis threads major with gc threads minor speedup b box plot comparing the response time of thread to threads evaluated on processing units figure comparison of different numbers of analysis threads and different numbers of gc threads configuration.
weusethe154tasksfromthepreviousexperiment andre evaluatethem.wedivideourevaluationintotwocases first the number of available processing units is equal to the numberof analysis threads.
second the number of available processingunits is set to which is the upper limit the available hardware.
forbothcases weevaluatedallcombinationsofanalysisthreads using1 8threads major largenumbersinfigure andgc threads using threads minor small numbers in figure .
results.wepresentthespeedupstatisticsforcomparingtheresponse time of a single threaded analysis with a single threaded gconasingleprocessingunittoanexecutionwithagivennumber of analysis threads with a given number of threads for gc on agivennumberofprocessingunits.infig.
5athenumberofavailable processing units is equal to the number of analysis threads.in fig.5ball processing units of the machine are available to the verifier.
in both figures we configure the number of analysis threadsandgcthreads.ineachplot thehorizontalaxiscontains major groups representing the number of analysis threads of each minor entries number of threads for gc .
for example the five first most left entries in each figure show the speedup of the approachifusingonethreadfortheanalysisandavaryingnumber ofthreadsforgc.unsurprisingly theoverallresultisthatusingmultiple threads for both the analysis and additionally the gc is beneficial.nearlyalltasksaresolvedfasterifmultipleprocessing units are assigned to the verification process.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france dirk beyer and karlheinz friedberger discussion.
in fig.5a the first entry of each group shows the speedupoftheanalysiswhenusingonlyonethreadforgc.thisisolates the the benefit of multi threading caused by our new analysis approach.similarly fig.
5bshows withineachofthe5groups that keeping the number of analysis threads constant and incrementing thenumberofthreadsforgcalso speedsupthe verificationprocess.thereforeboth analysisandgc benefitfrommulti threading.
figure5bshows that if the analysis is bound to one thread the benefitfrommulti threadingis ratherlimited whilethespeedupis improvedifweusemorethreadsfortheanalysis.themostinterest ingindicatorsarethemedianvalue middlelineinsidethebox and the minimal speedup values lower whisker .
the overall variance fortheresponsetimeandspeedupisquitelargeifthereareseveral processing units available.
this might indicate a non deterministic scheduling of workload across free resources in contrast to the narrowboxesinfig.
5ainthelefttwogroups wherethenumber of processing units is bound to one and two respectively .
we conclude that claim holds because we were able to isolate and control the only other cause for a significant speedup and the experiments confirmed that our new approach is reponsible forthe improved performance of the analysis while the parallel gcalgorithms of the jvm take care of parallelized deallocation.
.
threats to validity external validity our benchmark suite consists of a large set of c source files.
we use the largest publicly available benchmark suite inordertooptimizethediversityinsizeandtypeofprograms.thisisparticularlyimportantforevaluatingclaim1.forclaims2and3 werestricted thebenchmarkset toverificationtasks thathavepotentialtobenefitfromparallelization.ourevaluationisrestricted tothelanguagec andwhileitseemsclearthattheconceptsand results can be transferred to other imperative languages such a claim is notbacked up by our experiments.
thechosen time limit of15minandmemorylimitof15gbforverifyingagiventaskis inspired by the research community on software verification cf.
one of the reports on the international competition on software verification .ofcourse theevaluationofourapproachdepends on the tool in which it is implemented.
there is currently no other tool implementing the same approach and a comparison with a completely different approach for parallel analysis might be mis leading.
9with the assumption that the default configuration is optimized for most use cases we did not change the configuration of the jvm except the increment of maximal heap memory and the adjustment of the garbage collection strategy such that the effect of the number of threads can be measured.
the available hardware mightalsoinfluencetheresults.forparallelexecution theinternal structure of the cpu is a critical element i.e.
low level cachingand the hierarchy of processing units have a large effect on therun time of tasks.
we used a modern intel xeon e3 v5 thatis available on the market for a reasonable price in order to obtain results that have a higher externally validity than experiments on special high performance clusters.
9thesupplementaryartifact andwebsiteincludeanadditionalcomparisonwith some non bam analysis approaches in order to show that using the bam technology does not negatively effect an analysis performance known result .internalvalidity besidesgarbagecollectionofthejvm thereare otherfactors thatinfluence thespeedupof theparallel approach.
some components of cpachecker e.g.
counters and measurements for statistics are not yet fully optimized for parallel execution.
additionally it depends on the task s structure how many blocks can be analyzed in parallel.
controlling this variable number of parallelization blocks is not possible or very difficult thus we prefer to increasetheinternalvaliditybythelargenumberofexperiments ondifferenttasks.anothercontrolvariableistheblocksize.larger blocksarebeneficialforaconcurrentanalysis duetothesmaller synchronizationfootprint.forclaims2 and3 thebenchmarkset was already chosen such that it contains only programs where the block size is very large.
thus we did not further analyze differ ent block sizes.
we also need to consider that the explicit value analysis computes a large number of abstract states while other abstractdomainsmightlead tomorecompactrepresentationsof the state space and the fewer abstract states areexploredthe less might be parallelized.
our time measurement includes the mem ory allocation for the jvm parsing time and internal statistics which adds processing workload that cannot be parallelized currently.wemitigatethiseffectbyusingonlythoseverificationtasks that need more than 3swhen using one thread i.e.
we consider verificationtasksforwhichtheanalysisitselfconsumesaportion of the run time that is not negligible.
conclusion wepresentedanewapproachformulti threadedsoftwareverification that is based on program block summaries.
our emphasis isonprovidingasolutionthatfollowstheprincipleofseparation of concerns the problem of making the analysis benefit from multiple processing units is treated completely orthogonal from theproblem of designing and implementing an abstract domain and theoperatorsforaprogramanalysis.weformallydefinethenew algorithm in the framework provide a working implementation anddemonstrateitsapplicabilityonalargesetofbenchmarks.the experiments show that our approach a does not add noticeable overhead for verification tasks that do not benefit from parallelization b canconsiderablyspeeduptheverificationprocessinmany cases given the verification task has a certain minimal size and some independent branches to explore and c contributes largely to the performance improvements i.e .
the speedup is not only due to multi threading features that the jvm provides.
the presented algorithm is implemented as a shared memory approach which allows efficient interaction of all components.
as thenumberofcpucorespermachineandalsotheamountofmem oryperhostislimited weplantoextendouralgorithmtoleverage severalprocessesthatmightbedistributedoverseveralmachinesin acluster.anadditionalbenefitwouldbeasimplerusageofabstract domains that rely on libraries that are not thread safe because there is no problem with interleaved usage of libraries in separate processes.additionallyweplantooffloadthecacheofbamtoa disk based storage in order to lower the memory usage for veryresource intensive tasks.
the combination of both a distributed multi process verification algorithm and a disk based cache seems tobeverypromisingfortheverificationofverylargeprograms.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
domain independent multi threaded software model checking ase september montpellier france