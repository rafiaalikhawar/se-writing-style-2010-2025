query based configuration of text retrieval solutions for software engineering tasks laura moreno1 gabriele bavota2 sonia haiduc3 massimiliano di penta4 rocco oliveto5 barbara russo2 andrian marcus1 1the university of texas at dallas richardson tx usa 2free university of bozen bolzano bolzano italy 3florida state university tallahassee fl usa 4university of sannio benevento italy 5university of molise pesche is italy abstract text retrieval tr approaches have been used to leverage the textual information contained in software artifacts to address a multitude of software engineering se tasks.
however tr approaches need to be configured properly in order to lead to good results.
current approaches for automatic tr configuration in se configure a single tr approach and then use it for all possible queries.
in this paper we show that such a configuration strategy leads to suboptimal results and propose quest the first approach bringing tr configuration selection to the query level.
quest recommends the best tr configuration for a given query based on a supervised learning approach that determines the tr configuration that performs the best for each query according to its properties.
we evaluated quest in the context of feature and bug localization using a data set with more than queries.
we found that quest is able to recommend one of the top three tr configurations for a query with a accuracy on average.
we compared the results obtained with the configurations recommended by quest for every query with those obtained using a single tr configuration for all queries in a system and in the entire data set.
we found that using quest we obtain better results than with any of the considered tr configurations.
categories and subject descriptors d. .
distribution maintenance and enhancement restructuring reverse engineering and reengineering general terms documentation measurement keywords text retrieval in software engineering configuration feature and bug localization permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse august september bergamo italy copyright acm .
isbn ... .
.
.
introduction software systems contain a large amount of information in text format captured in a multitude of software artifacts such as requirements documents design documentation source code bug reports developer communication user manuals test documents etc.
this information when properly utilized can help developers understand various aspects of a software system and can help them in their daily tasks.
text retrieval tr approaches have been proposed to leverage the large amount of text information available in software.
these techniques have been successfully applied to support more than di erent software engineering tasks such as feature and bug localization in source code traceability link recovery impact analysis bug triaging and so on.
the research on using tr in software engineering se has generated several hundred papers in the last decade with a focus on investigating how to support specific tasks and how to improve the performance of specific tr approaches.
most of the published work is empirical in nature and many tools and research prototypes have been produced.
tr techniques need to be configured before using them which requires calibrating several parameters.
di erent parameter values lead to di erent tr configurations and the work on determining the best tr configurations in se reached the conclusion that the performance of a tr configuration varies widely from system to system even when used for the same task.
more than that a tr configuration that works best for one software corpus may not be the best for another.
di erent automated approaches have been proposed to calibrate some tr techniques for specific se tasks and for individual software corpora either based on specific heuristics or by relying on search based optimization techniques .
while this previous work has acknowledged the need to adapt the configuration of the tr techniques to the characteristics of each individual software corpus and se task it has overlooked one aspect the query .
other previous work showed that while some queries lead to good results others fail to retrieve any useful information when using a specific tr configuration.
this is mainly because di erent queries have di erent lexical and semantic properties which may require di erent approaches for finding relevant documents.
in consequence researchers also focused on automatically or manually changing the queries to fit a given tr configuration and software corpus.
based on all existing work we argue that there is no silver bullet tr configuration that can work equally well for permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august september bergamo italy c acm.
... .
567all queries formulated in the context of a se task and a given software corpus.
in order to verify such assumption we conducted an exploratory study and reported the performance of tr configurations used in previous se work using a large data set of software systems and queries for the task of feature and bug localization.
this study see section confirmed our conjecture that no single tr configuration works best for all queries in a system and that the di erence in performance between tr configurations varies greatly across queries in the same system.
in other words these findings indicate that the query is an important factor in determining the best tr results for a given software corpus.
in this paper we tackle the tr configuration problem at a new granularity level i.e.
the query level.
we propose a new approach named quest que ry based configuration for software re trieval which considers both the software corpus and the query to automatically determine the best tr configuration to use for each individual query in the context of a given se task.
quest relies on supervised learning and uses a training set of queries and their relevant results for building a classification model.
for each query quest computes a set of measures that capture different properties of the query that have been shown to be useful in other tr tasks such as query quality prediction and query reformulation .
the learning process relies on automatically running a set of queries using di erent tr configurations on labeled training data and determining which configuration performs the best for each query according to its properties.
when a new query is issued the model is then used to automatically determine the best tr configuration to use for the query based on its properties.
we evaluated quest in an empirical study on feature and bug localization data.
part of the evaluation focused on determining how accurate is quest in determining the best configuration for a given query.
we found that quest is able to recommend one of the top three tr configurations for a query with a accuracy on average.
the evaluation also focused on determining whether the configuration selected byquest for each query leads to better results for feature and bug localization than running any of the individual tr configurations on all the queries.
we found that for of the queries on average the configuration selected byquest improves or preserves the quality of the retrieval.
novel contribution.
while previous work attempted to find the best tr configuration for a given software corpus and or for a specific task or to reformulate a query to fit a given tr configuration quest is the first technique that determines the best tr configuration for an individual query and a given software corpus.
replication package.
a replication package is available online1.
this package includes i data sets used in our studies ii complete results of the exploratory study iii the definition of the query property measures currently implemented by quest and iv complete results and analysis of the empirical study.
.
related work hundreds of approaches have used tr techniques to address software engineering tasks.
the vast majority of such fse2015 quest approaches have determined the tr configurations to use based on previous work in natural language processing or by relying on ad hoc decisions .
the most relevant work to this paper deals however with comparing di erent tr configurations for various software engineering tasks or automatically determining the best tr configuration for a particular task and data set.
we discuss each of these two research directions below.
no prior work has addressed tr configuration at query level in software engineering.
several studies have compared tr configurations in order to determine the most suited one for a specific software engineering task.
abadi et al.
compared five tr engines and several parameter values for each engine for the task of traceability link recovery and found lucene and jensenshannon as the best engines to use.
this same task was addressed by gethers et al.
which combined orthogonal tr approaches for achieving better results.
falessi et al.
compared a large number of tr configurations and combinations of tr engines for discovering equivalent requirements.
shi et al.
used bm25 and bm25f with several fixed parameter configurations and compared them with other tr approaches for feature location showing that the bm25 approaches performed the best.
rao et al.
addressed bug localization and compared five tr engines using di erent parameter configurations as well as combinations of two tr engines.
they found that the unigram model and its respective composite approach performed the best.
thomas et al.
also considered bug localization for their large scale comparison of tr configurations over configurations were tested and combinations of tr engines were also considered.
the study showed that the best individual tr engine is the vector space model vsm and that combinations of tr are always beneficial no matter which tr models are combined.
wang et al.
followed this direction and used a genetic algorithm in order to determine a near optimal composition model for several vsm variants for bug localization.
lukins et al.
and nguyen et al.
addressed latent dirichlet allocation lda configuration for bug localization comparing it with other approaches.
most of the studies mentioned so far focus on finding one tr configuration or a combination of tr configurations that achieve the best performance across all data sets as a general prescription to be used by researchers for similar tasks.
however one of the most outstanding and frequent lessons of these studies is that di erent tr configurations may be needed for di erent data sets.
as an illustration wong et al.
who used vsm for bug localization found that the optimal parameter settings are di erent from project to project.
biggers et al.
focusing on the configuration of lda for the task of feature location showed that di erent parameter values are needed for systems having di erent sizes.
binkley et al.
emphasized this point and studied the impact of lda parameter configuration on results.
the study underlines the fact that there is no universal best setting for lda parameters and that the optimal settings depend on the se task the input corpus and the information needs of the developer.
as a result some approaches were proposed for determining the best tr configuration for a particular data set.
kuhn et al.
proposed a formula for determining the number of latent semantic indexing lsi dimensions for a system based on its corpus size.
on a similar note grant et al.
devised an approach to determine the number of topics to use in lda for se applications based on the 568table data set used in the studies.
system version kloc of methods of issues of queries apache nutch .
apache nutch .
bookkeeper .
.
commons math3 .
derby .
.
.
mahout .
openjpa .
.
pig .
.
pig .
.
solr .
.
tika .
zookeeper .
.
total number of code fragments in a system.
recent approaches made use of genetic algorithms in order to determine the best parameter values to use with a given data set for lda see panichella et al.
as well as vsm and lsi see lohar et al.
.
a parallel set of approaches called learning to rank ltr has been studied for retrieving relevant software artifacts for di erent se tasks.
binkley and lawrie studied ltr in the context of traceability link recovery and feature location and ye et al.
used it for bug localization.
ltr approaches which were first introduced in the last decade in the field of information retrieval make use of machine learning in order to learn the rank of each document in response to a given query or set of queries .
these approaches use a set of features for each query document pair which can indicate some properties of the query and document including the similarity of the query to the document based on some tr approach.
however when tr approaches are used to obtain such features their parameters are set to the same values for all queries.
in ltr approaches the final list of results for a query is not given by a tr approach therefore ltr does not deal with tr configuration.
moreover ltr builds a model that is trained for ranking documents and then the same model is used to rank the documents for all queries.
in our approach we use di erent tr configurations depending on the query.
ltr approaches are also very computationally intensive as they require learning the rank of each individual document with respect to each query making these models very complex and impractical for large corpora .
our approach requires learning only the tr configuration to use for a query which can be applied to all documents as opposed to learning the rank of every document in the corpus.
.
exploratory study is there a single tr configuration that works best for all queries given a software corpus?
we performed an exploratory study to answer this question in the context of feature and bug localization.
.
data set we performed the study on a set of versions of ten software systems which are maintained by the apache software foundation2 belong to various domains and have various sizes see table .
in feature and bug localization studies using tr approaches it is common to use the issue tracking and the versioning system of a software in order to extract the evaluation data.
the queries for a system are extracted from change requests i.e.
bug reports andfeature requests found in the issue tracker of that system.
each change request has a title and a description both containing textual information which describes the problem in the case of bug reports or the feature to be implemented in the case of feature requests .
queries in tr studies are usually extracted from these titles and descriptions .
in our study we extracted three representative queries for each change request one from its title one from its description and one merging the title and the description.
therefore for the issues considered in the study we obtained a total of queries.
after extracting the queries we applied common processing techniques to the text in the queries such as splitting i.e.
separating composed words such as identifiers into individual terms filtering i.e.
removing common english words and programming keywords and stemming i.e.
mapping words to their lexical root .
we used the porter stemmer and a stop word list that is available in the replication package.
the gold set a.k.a.
ground truth for the evaluation is formed by the set of changed methods in response to each bug report or feature request.
so we focused on bug reports and feature requests that have been successfully resolved i.e.
those marked as resolution fixed and status resolved closed .
we extracted the modified methods from the commits that implemented the change requests found in the versioning system of each software.
in particular we linked each resolved issue to the commit pushing it in the versioning system by using the approach proposed by fischer et al.
which relies on matching the issue id in the commit note e.g.
fix commit id fixing id .
all the systems used in our study adopt this convention for keeping traceability between commits and issues.
for each software system we built a text corpus from its source code by considering each method as a separate document and extracting the identifiers and comments of each the method.
this corpus was then processed using the same sequence of techniques applied to the queries splitting filtering and stemming.
the corpus for each system was indexed using each tr configuration and used to retrieve the relevant methods in a system based on the queries.
.
methodology we applied di erent tr configurations see table to the queries in the data set and analyzed their results.
the tr configurations were selected by considering the ones rec569table tr configurations used in the studies.
engine configuration id lucene default e1c1 variable according to the system size e2c1 n iterations .
.
e2c2 n iterations .
.
e2c3lda n iterations .
e2c4 k1 .
b .5e c k1 .
b .
e3c2 bm25 k1 .
b .5e c .
e4c1 .5e c lmjm .
e4c3 lmd default e5c1 model bose einstein after effect bernoulli norm h e6c1 model poisson based tf idf after effect laplace norm h e c model poisson based tf idf after effect laplace norm h e6c3 model poisson based tf idf after effect bernoulli norm h e c 4dfr model poisson based tf idf after effect bernoulli norm h e6c5 distribution l o g l o g i s t i c df norm n o e c distribution l o g l o g i s t i c ttf norm n o e7c2 distribution smoothed power law df norm n o e c 3ibm distribution smoothed power law ttf norm n o e7c4 tr configurationse1c1e2c4e2c3e5c1e6c1e6c2e3c1e4c3e6c4e2c2e7c1e4c1e6c5e3c2e2c1e4c2e7c3e6c3e3c3e7c2e7c40 of queries with best effectivenessfigure percentage of queries for which the tr configurations obtain the the best e ectiveness.
ommended in previous work focusing on tr tuning in se .
in addition we also considered some new tr configurations which were not used before in the context of se applications.
the tr configurations correspond to seven di erent tr engines lucene lda okapi bm25 language model with jelinek mercer smoothing lmjm language model with dirichlet smoothing lmd divergence from randomness dfr and information based model ibm .
we evaluated the performance of the tr configurations by measuring the e ectiveness of the retrieval for each query in the data set.
e ectiveness is a measure widely used for evaluating tr approaches for feature and bug localization .
it represents the position of the first relevant document in the list of results therefore being a proxy of the number of documents a developer would have to look at before finding the first relevant answer to the query i.e.
the developer s e ort for finding the answer .
.
results by analyzing the e ectiveness results on our data set we observed that none of the considered tr configurations consistently performs the best across all the queries.
figure reports the percentage of queries for which each tr configura tion achieved the best e ectiveness in the entire data set.
as it can be observed the highest number of queries for which a tr configuration performed the best is no higher than i.e.
out of queries .
this result is achieved by the default configuration of lucene e1c1 .
we also observed that the highest percentage of queries for which any of the tr configurations performed the best in a particular system varies between and see table .
this means that if only one tr configuration were used for all queries of the systems between and of the queries would be suboptimally answered and this is in the best case scenario when the best performing tr configuration is chosen.
in addition the di erence in the results between choosing di erent tr configurations for the same query can be tangible impacting the success or failure of the retrieval task.
for example in openjpa .
.
if we were to choose its best tr configuration e2c4 for all of its queries we would come across cases like query for which e2c4 retrieves the first relevant document on position thus resulting in an unsuccessful search as developers rarely look past the first to results.
choosing the configuration e5c1 on the other hand would have returned the relevant document on the first position in the list of results for the same query.
we also observed that the same tr configuration performs 570table percentage of queries for which the tr configurations obtain the the best e ectiveness per system.
system e1c1 e2c1 e2c2 e2c3 e2c4 e3c1 e3c2 e3c3 e4c1 e4c2 e4c3 e5c1 e6c1 e6c2 e6c3 e6c4 e6c5 e7c1 e7c2 e7c3 e7c4 apache nutch .
apache nutch .
bookkeeper .
.
commons math3 .
derby .
.
.
mahout .
openjpa .
.
pig .
.
pig .
.
solr .
.
tika .
zookeeper .
.
average !
!
!
!
figure e ectiveness of the tr configurations in the whole data set sorted by median values the lower the better .
quite di erently across systems.
for example e2c4 is the configuration which leads to the best results over the largest set of queries in derby .
.
.
i.e.
e2c4 has the best results among the configurations for .
of its queries whereas the same configuration never obtains the best results among the tr configurations for any of the queries inapache nutch .
.
these examples are not outliers.
figure shows the distribution of e ectiveness highest rank for the relevant method so the lower the better for all queries for each configuration.
data reveals that the e ectiveness of the retrieval widely varies across configurations as it can be seen by comparing the di erent boxes and from query to query as it can be seen from the large boxes i.e.
interquartile di erences and from the presence of several outliers exhibiting a rank having value of orders of magnitudes greater than the median .
for example e2c4 and e2c3 corresponding to lda are the 2nd and 3rd best configurations based on the number of queries for which they obtain the best e ectiveness see figure .
if someone is determined to use lda for their task she might be tempted to choose one of these two configurations.
however based on median e ectiveness see figure these two configurations are ranked as 18th median e ectiveness and 20th median e ectiveness among the configurations respectively.
in other words their performance over all the systems and queries is nearly the worst.
the complete results of this exploratory study can be found in our replication package.
these results underlinetwo facts i choosing one tr configuration for all queries in a system or across several systems leads to suboptimal results and ii in some cases the di erence between the optimal results that could be achieved using the best tr configuration for each individual query and those obtained using only one tr configuration for all queries is significant.
as shown above choosing the right tr configuration can make the di erence between a successful and an unsuccessful search .
since developers get easily discouraged using a particular tool if the results retrieved are not good even for a few instances choosing the tr configuration that leads to the best results for each query could have a significant impact on the adoption of tr approaches by practitioners.
in the next section we describe our approach which premiers query level tr configuration on software corpora.
.
an approach for query based tr configuration quest as any technique based on supervised learning quest follows two major steps.
the first one is a training step where a classifier is built from a training data set.
the second step is recommending the tr configuration that performs the best for a given query based on the trained classifier.
the idea behind quest is that the performance of tr configurations varies according to di erent properties of the queries.
since both the training and recommendation steps make use of such properties we provide first a brief overview of them.
.
query properties and measures previous work on query quality prediction and query reformulation in software engineering adopted a series of information retrieval measures that capture linguistic and statistical properties of queries and a software corpus.
these measures were used to predict whether a query will lead to relevant or irrelevant tr results given a tr configuration and to automatically find the reformulation for a query that leads to the best results given a tr configuration .
in the context of this paper we use these measures as indicators of the performance of particular tr configurations since they capture properties of the query software corpus and the results retrieved by a tr approach.
existing query and corpus measures are categorized into pre retrieval and post retrieval depending on the moment when they are computed and the type of information they capture.
pre retrieval properties and measures.
pre retrieval measures are computed before the query is run and measure linguistic and statistical properties of the query and its relationship with the software corpus.
pre retrieval measures assess properties such as coherency specificity similarity and term relatedness .
coherency indicates how focused a query is on a particular topic .
the coherency of a query is usually measured as the level of inter similarity between the documents in the corpus containing the query terms.
the more similar the documents are the more coherent the query is.
specificity refers to the ability of the query to represent the current information need and discriminate it from others .
the main idea behind this property is that a query composed of terms commonly used in the corpus is considered having low specificity as it is hard to di erentiate the relevant documents from non relevant ones based on its terms.
for example when searching source code the query initialize members could have low specificity if a comment containing this text would be found in most class constructors in a system.
specificity measures are usually based on the query terms distribution over the collection of documents but the way this information is captured di ers from measure to measure.
thesimilarity between the query and the entire document collection is another property that reflects an aspect of the query and corpus .
the argument behind this type of measure is that it is easier to retrieve relevant documents for a query that is similar to the corpus since high similarity potentially indicates the existence of many relevant documents in the corpus to retrieve from.
finally term relatedness measures make use of term cooccurrence statistics in order to assess the performance of a query .
the terms in a query are assumed to be related to the same topic and are thus expected to occur together frequently in the corpus.
post retrieval properties and measures.
while preretrieval measures capture some general characteristics of the query and corpus they do not take into consideration the list of results returned by a tr engine.
post retrieval measures rely on the analysis of the search results that is the list of documents in the corpus ranked highest in response to the query.
robustness based measures evaluate how robust the results are to perturbations in the query and the documents in the result list .
some of these measures assessthe robustness of the result list to small modifications of the query.
when small changes in the query cause large changes in the search results the confidence in the capacity of the query to capture the essential information diminishes.
document perturbation measures on the other hand rely on injecting the top documents in the result list with noise and re ranking them measuring the di erence in their ranks before and after the perturbation.
for a robust query small perturbations of the documents in the result list should not result in significant changes in their ranking.
score distribution based methods analyze the similarity between the query and the results which are used to rank the results of the retrieval .
for example the highest retrieval score i.e.
similarity and the mean of top scores indicate query performance since in general low scores of the top ranked documents indicate some di culty in retrieval.
clarity based methods directly measure the focus clarity of the search results with respect to the corpus.
due to the considerable execution time of the clarity based measures we did not include them in the current implementation of our approach.
.
training phase the starting point for building quest s classifier is a training set built from a collection of queries their properties and their respective relevant documents.
quest is a general approach which can be applied in the context of any se task making use of tr techniques.
therefore what each query in the collection represents and how it is collected can vary according to the se task at hand.
in the case of feature and bug localization on which we focus in this paper the queries and relevant documents represent respectively text extracted from change requests and the methods modified in response to the requests as in the case of our exploratory study in section .
the next are the steps followed by quest during the training phase .for each query in the data collection quest computes the values of a predefined list of nquery property measures.
the current version of our approach uses pre retrieval and seven post retrieval query measures shown to be useful in other query related tasks in se .
the measures currently implemented in quest are listed in table and fully described in our replication package.
as post retrieval measures require the analysis of retrieval results for each query we use the results provided by lucene to compute such measures.
we chose lucene as the exploratory study indicated that its standard configuration is one of the better ones among the ones we studied i.e.
e1c1 .
using other tr engine or configuration for computing the post retrieval measures does not change our approach.
future work will investigate the use of other tr configurations for the computation of the postretrieval measures.
.for each query quest then applies one by one a set of tr configurations to rank the documents relevant to the query and identifies the one performing the best for each query.
in feature and bug localization the best configuration is the one that retrieves any of the relevant artifacts to a query on the highest position in the list of results i.e.
lowest e ectiveness values .
572table query property measures implemented by quest .
property measure maximum of the inverse document frequency standard deviation of the inverse document frequency average inverse collection term frequency maximum inverse collection term frequency standard deviation of the inverse collection term frequency average entropy median entropy maximum entropy standard deviation of the entropy.
query scopespecificity pre simplified clarity score average of the variances maximum of variances sum of variancescoherency pre coherence score average of the collection query similarity maximum of the collection query similarity similarity pre sum of the collection query similarity average pointwise mutual informationterm relatedness pre maximum pointwise mutual information subquery overlap robustness score first rank change clustering tendencyrobustness post spatial autocorrelation weighted information gainscore distribution post normalized query commitment .then each data point in the training set represents a query described by a vector of nattributes corresponding to the values of the query properties and one attribute corresponding to the best tr configuration for the query.
this latter represents the class label of a data point.
.finally a supervised learning algorithm is run on the training set.
currently quest uses classification trees which produce human understandable rules and implicitly perform feature selection.
this is important forquest as it reduces the sensitivity to the choice of query property measures.
in other words given as input all nmeasures of a query the classification tree will automatically determine those properties that are relevant for the classification with little overhead.
classification trees are generally suitable to solve problems where the goal is to determine the values of a categorical variable based on one or more continuous and or categorical variables.
in our approach the categorical dependent variable is represented by the best tr configuration to use for a particular query while the independent variables are the query property measures.
the classifier uses the training data to automatically select the independent variables and their interactions that are most important in determining the dependent variable to be explained.
future work will investigate other learning algorithms.
two approaches can be followed to train the classifier within project i.e.
training a classifier for each software system independently and cross project i.e.
training a global classifier for all the considered systems .
cross project training can be useful when training data for a specific software system may not be obtained.
within project training has been shown to outperform cross project training in previous work and our experimental evaluation from section confirms this trend also for quest .
a classification tree is the outcome of quest s training stage.
in this tree each root to leave path represents a series of conditions or rules on the query properties that need to besatisfied in order to reach a class label given by the path s leaf.
.
recommendation phase for a new query quest computes the same nquery properties considered in the training phase.
based on these values quest uses the classification tree obtained in the training phase to automatically determine the tr configuration that is expected to perform the best for the query.
finally the selected tr configuration is run to retrieve the documents relevant to the query.
.
quest evaluation study we conducted an empirical study to assess quest s prediction accuracy and to compare it with the traditional way of applying tr approaches in software engineering i.e.
one tr configuration for an entire data set in the context of feature and bug localization.
the study aims at answering the following two research questions rq .how accurately does quest predict the best tr configuration for a give query?
rq .does quest improve feature and bug localization compared to using a single tr configuration for a software system?
.
study design we built an instance of quest to perform the empirical study.
the methodology followed in this study is in many ways similar to the exploratory study presented in section so we will not go into details whenever the same data or settings were used and previously explained.
data set.
we used the same data set as in our exploratory study consisting of queries extracted from resolved issues i.e.
feature requests and bug reports of versions of ten software systems described in table .
as a reminder each query in the data set was extracted from either the title the description or the concatenated ti573tle and description of an issue.
for each query the set of methods that were modified in response to its corresponding change request are known and represent the gold set used to determine the performance of the tr configurations and quest .
the text corpus of a system contains its methods as documents and each document contains the identifiers and comments extracted from a method.
both the queries and the corpus of each system were processed following the same procedure as in the exploratory study.
performance measurement .
we measure the performance of a tr configuration and of quest as its e ectiveness i.e.
the position of the first relevant document in the ranked list of retrieved results.
as mentioned in section this is an inverse measure so a lower value is better i.e.
an e ectiveness value of is the best possible result which means that one of the relevant methods is ranked the highest in the list of retrieved documents .
tr configurations.
it is impractical to build classification trees that select among categories so we limited the number of tr configurations used in this study to eight.
we selected the ones with the highest percentage of queries for which they obtain the best e ectiveness from our exploratory study e1c1 e2c3 e2c4 e3c1 e4c3 e5c1 e6c1 and e6c2.
the rationale behind this choice is to ensure that there are enough samples in each category.
quest will consider each of these as a category and will recommend applying one of them for a given query based on its properties.
training strategy.
we used both within and crossproject strategies to train quest .
in the within project case one classification model was individually trained on each system and a stratified fold cross validation was performed following the next steps .randomly divide the set of queries of a system into four equal subsets if possible containing approximately the same percentage of samples of each target class as the complete set .set aside one of the subsets as a test set and build the classification model with the queries in the remaining subsets i.e.
the training set .use the classification model built on the training set to identify the best tr configuration for the queries in the test set .repeat this process setting aside each subset in turn.
note that each query is used only once in the test set.
in the case of the cross project training the queries of of the systems in the data set are used for training and the queries from the remaining project are used for evaluation.
this is repeated such that the queries in each project are tested.
in order to answer rq we evaluate the fitness of the training strategies by computing their top krecommendation accuracy this is for each query if the tr configuration recommended by quest is in the top ktr configurations i.e.
the best kconfigurations according to the performance measurement we consider it as a success otherwise we consider it as a failure.
baselines.
in order to answer rq we considered each of the eight tr configurations selected for the evaluation as baselines and ran them for the entire data set.
then we compared the results of applying each individual tr configuration to each system with the results obtained by running the tr configuration recommended by quest for each indi table quest s top krecommendation accuracy when using within and cross project training.
training strategy system top top top apache nutch .
apache nutch .
bookkeeper .
.
commons math3 .
derby .
.
.
mahout .
openjpa .
.
pig .
.
pig .
.
solr .
.
tika .
zookeeper .
.
within project average cross project average vidual query.
the comparison was based on the e ectiveness of the approaches and was made by considering the number of queries for which the e ectiveness measure obtained by quest improves preserves or deteriorates each of the baselines e ectiveness.
the sets of results were also analyzed through statistical analysis using the mann whitney test .
we chose this test as we cannot assume normality of data and the test does not make normality assumptions.
the results are interpreted as statistically significant at .
.
however since we performed multiple tests we adjusted our p values using the holm s correction procedure .
this procedure sorts the p values resulting from ntests in ascending order multiplying the smallest by n the next by n and so on.
.
results we present and discuss the results for each research question separately.
.
.
rq classification accuracy table reports the top krecommendation accuracy achieved by quest when using within and cross project strategies to train the classifier.
the data reveals that the classification model based on cross project training is significantly less accurate than the models built for each individual project.
as expected the accuracy of both the training within and cross project strategies increases as the top set increases its size k. the accuracy of the withinproject trained classification models varies from system to system see table .
the best top recommendation accuracy is achieved for the commons math3 .
project in which the best possible tr configuration was selected of the times.
this percentage increases to when considering the top tr configurations as success factor.
the worst top accuracy corresponds to pig .
.
where the classification model predicted the best tr configuration in only of the cases.
recommending any of the top configurations increases the accuracy to for this system.
on average the within project training ensures a success in the selection of the top kconfiguration in of cases when k when k and when k see table .
.
.
rq comparison with baselines in order to determine if query based configuration brings improvement over the traditional way of applying tr based approaches for feature and bug localization we compared quest with the eight considered baselines according to the procedure described in section .
.
given the lower accuracy 574table percentage of queries for which quest improved in parenthesis median of improved positions preserved and deteriorated in parenthesis median of deteriorated positions the retrieval performance with respect to each baseline.
baseline improved preserved deteriorated med.
improv.
med.
deter.
e1c1 .
.
.
.
e2c3 .
.
.
e2c4 .
.
.
e3c1 .
.
.
e4c3 .
.
.
e5c1 .
.
.
.
e6c1 .
.
.
e6c2 .
.
.
.
average .
.
.
.
.
results of the cross project training described in the previous subsection we focus here only on the recommendations using the within project training model.
table shows how quest s recommended configuration compares with the baselines aggregated for all queries i.e.
.
the rows of the table correspond to each of the eight baselines and the columns show the percentages of queries from the data set for which quest leads to an improved e ectiveness measure i.e.
the tr configuration recommended byquest led to a better e ectiveness measure than the tr baseline on that row preserved the same e ectiveness results i.e.
the tr configuration recommended by quest returned the same e ectiveness value as the tr baseline on the row or deteriorated the e ectiveness measure i.e.
the tr recommended by quest returned a higher e ectiveness value than the tr baseline respectively.
the values in parenthesis indicate the median change in the e ectiveness.
it must be pointed out that a lower e ectiveness value indicates a better result i.e.
a lower rank in the list of retrieved results for the first relevant document to the query.
preserved means that the e ectiveness measure was the same between the baseline and the tr configuration recommended by quest however in some cases this e ectiveness actually represents the best value among all tr configurations considered.
therefore in some preserved cases quest s recommendation led to the best results even if the baseline obtained the same value.
when a tr based feature or bug localization technique improves in some cases compared to a baseline and it deteriorates in other cases it is important to preserve the best results i.e.
where improvements cannot be achieved .
we consider these cases also a successful recommendation table lists these cases for the comparison of quest with each baseline.
the results indicate that quest improves the e ectiveness for .
of the queries maintains the e ectiveness for .
of queries and deteriorates the e ectiveness in .
of the cases when averages across all eight baselines are considered.
therefore using the tr configuration recommended by quest for each query results in an improved or preserved e ectiveness for of the queries.
this means that there are twice as many queries with improved results than those for which the results deteriorate compared to the average baseline results.
we note that among the cases where the e ectiveness is preserved quest s tr configuration led to the best e ectiveness results in more than half .
of the cases see table .table queries for which the results were preserved by quest but represent optimal results.
baseline preserved preserved preserved but best but best e1c1 .
e2c3 .
e2c4 .
e3c1 .
e4c3 .
e5c1 .
e6c1 .
e6c2 .
all .
when compared to individual tr baselines quest always leads to more cases of improvement than deterioration of the e ectiveness.
the percentage of improved queries ranges from .
when compared to e1c1 with only .
of deterioration to .
when compared to e2c3 with only .
of deterioration .
we note however that for e1c1 .
of the preserved results represent the best results among all tr configurations and therefore are successful recommendations as the e ectiveness cannot be improved.
also in e6c1 which has the second lowest percentage of improved queries of the preserved e ectiveness results are optimal.
the highest improvement in e ectiveness was obtained over e2c3.
.
of the results were improved and .
of the preserved results are optimal.
one must also note that the median size of improvement in the e ectiveness is somewhat lower than the size of the deterioration vs. see table .
however considering the larger number of improvements than deteriorations we consider the results promising.
we performed also the mann whitney statistical test between the results obtained by the recommendation of quest and those obtained by each of the baselines in order to see if the di erences between them are statistically significant.
after holm s correction the results revealed three baselines for which there is no statistically significant di erence between the results of quest and that of the baseline.
these baselines are e2c3 adjusted p value of .95e e2c4 adjusted p value of .44e and e6c1 adjusted pvalue of .89e .
while the results of the mann whitney test did not show statistically significant di erence between the performance of quest and the rest of the tr baselines the results do show that overall quest improves the e ectiveness measure.
as previously said there is no single tr configuration that performs the best across all systems see table in section .
therefore we performed a more in depth analysis of the comparison between the results of the baseline performing the best in each system and quest .
table presents the per system results of this comparison in terms of the percentage of queries for which the results were improved preserved or deteriorated.
we observe that quest leads to more improved queries than deteriorated ones for all but two systems solr .
.
andzookeeper .
.
where the percentage of improved queries lags behind the percentage of deteriorated ones.
incidentally these are two of the systems where the predictor had the lowest accuracy see table .
however from the cases where the e ectiveness was preserved in solr .
.
and .
in zookeeper .
.
represented optimal values that were preserved by quest s recommendation when compared to the respective best tr 575table percentage of queries per system for which quest improved in parenthesis median of improved positions preserved and deteriorated in parenthesis median of deteriorated positions the retrieval performance with respect to the best tr baseline.
system best improved preserved deteriorated baseline med.
improv.
med.
deter.
apache nutch .
e1c1 .
.
.
apache nutch .
e2c3 .
.
.
bookkeeper .
.
e1c1 .
.
.
.
commons math3 .
e1c1 .
.
.
.
derby .
.
.
e2c4 .
.
.
mahout .
e1c1 .
.
.
openjpa .
.
e2c4 .
.
.
pig .
.
e1c1 .
.
.
.
pig .
.
e5c1 .
.
.
solr .
.
e6c1 .
.
.
.
tika .
e1c1 .
.
.
zookeeper .
.
e1c1 .
.
.
average .
.
.
.
.
baselines i.e.
e6c1 and e1c1 .
over the entire data set the average number of queries with worse results than the best individual baseline i.e.
.
is lower than the average of improved queries across all systems i.e.
.
showing the potential of using tr configurations at query level over system level.
we answer rq 2as follows.
quest s recommended tr configurations lead to better results for .
of the queries it preserves the e ectiveness for .
of the queries and it deteriorates the results for of the queries.
the results are statistically significant for three of the eight baselines.
while the results indicate that there is room for improvement they are promising and support the use tr configurations at query level.
.
threats to validity regarding the construct validity both the exploratory study section and the empirical study section rely on a query performance measure i.e.
the e ectiveness that is widely used in tr based feature and bug localization studies and provides a proxy measure of the developer s e ort in a feature location task.
several co factors can influence our results internal validity .
we automatically extracted the set of queries used in both studies from the online issues trackers of the object systems.
such queries are approximations of actual user queries and we believe that they resemble real usage scenarios.
nonetheless increasing the size of the data set could a ect the classification model and its accuracy.
another influencing co factor is the set of query property measures implemented by quest .
we selected measures that capture di erent aspects of the queries and have shown to be useful in other se tasks e.g.
query quality prediction and query reformulation .
the eight tr configurations selected for the empirical study were the ones with most best ranked instances which ensures better distribution of classes in the training data.
using a di erent set of tr configurations can a ect the accuracy of the predictors.
we believe however that quest will improve or preserve the performance of traditional tr based approaches in most of the cases.
the last co factor is the computation of the post retrieval query properties based on lucene.
the results can vary when using another tr engine for this task.
threats to conclusion validity concern the relationship between treatment and outcome.
where appropriate we usednon parametric statistical tests mann whitney to show statistical significance for the obtained results.
concerning the generalization of the obtained results external validity we selected queries from versions of ten java software systems from di erent domains and with different size.
a larger set of queries from di erent systems and written in other programming languages would clearly strengthen the results from this perspective.
moreover we used eight tr configurations when training the classification models.
the results might vary when using other tr configurations.
finally we only evaluated the proposed approach in the context of feature and bug localization.
thus we cannot and do not generalize the results to other se tasks.
.
conclusions and future work we proposed a novel approach quest which is the first to perform query specific tr configuration selection in the context of a software engineering task.
quest uses machine learning techniques in order to learn the best tr configuration to be used for each query.
the learning is based on a set of linguistic and statistical properties of the query itself the software corpus and the list of results returned for it.
in an exploratory study we found that choosing any individual tr configuration to use for all the queries formulated for a system or for an entire data set leads to suboptimal results.
then in our empirical evaluation we showed that quest is able to correctly assign one of the top performing tr configurations for a query with accuracy on average and overall it leads to improved results compared to any individual tr configuration applied to the entire data set.
while we consider the results positive and promising there is room for improvement.
for a few systems and when compared with a few tr configurations quest lead to slightly more queries for which the results deteriorated than the number for which it improved.
our future work will focus on determining what properties of the software or queries led to this suboptimal performance and will focus on overcoming these cases.
other directions of future work include i considering more tr configurations in order to determine a more variate set of post retrieval query measures ii considering di erent classifiers iii including more tr configurations to choose from in quest and iv experimenting with the size of the training set to see the impact it has on the accuracy of the prediction.
last but not least we will evaluate quest in the context of other software engineering tasks where tr is used.
we believe the work presented in this paper opens the door to a new research direction in tr based software engineering which will take the query into the consideration when applying tr techniques to se tasks.
since developers often get discouraged from using tools if the results they retrieve are not correct even for a few instances we believe that devising and perfecting an approach like quest could have a positive impact on the adoption of tr approaches by practitioners.
.