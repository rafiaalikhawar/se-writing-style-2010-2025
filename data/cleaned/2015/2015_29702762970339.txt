too much automation?
the bellwether effect and its implications for transfer learning rahul krishna tim menzies and wei fu computer science north carolina state university usa i.m.ralk tim.menzies gmail.com wfu ncsu.edu abstract transfer learning is the process of translating quality predictors learned in one data set to another.
transfer learning has been the subject of much recent research.
in practice that research means changing models all the time as transfer learners continually exchange new models to the current project.
this paper offers a very simple bellwether transfer learner.
givenndata sets we find which one produces the best predictions on all the others.
this bellwether data set is then used for all subsequent predictions or until such time as its predictions start failing at which point it is wise to seek another bellwether .
bellwethers are interesting since they are very simple to find just wrap a for loop around standard data miners .
also they simplify the task of making general policies in se since as long as one bellwether remains useful stable conclusions for ndata sets can be achieved just by reasoning over that bellwether.
from this we conclude this bellwether method is a useful and very simple transfer learning method bellwethers are abaseline method against which future transfer learners should be compared sometimes when building increasingly complex automatic methods researchers should pause and compare their supposedly more sophisticated method against simpler alternatives.
ccs concepts software and its engineering !software creation and management keywords defect prediction data mining transfer learning .
introduction when building software quality predictors it might be best to look are more than just the local data.
researchers in transfer learning report that data from other projects can yield better predictors than just using local data .
this is especially true when the local data is very scarce.
for example consider a new project based on a technology that previously has not been used at thissite.
if that technology that has been extensively explored elsewhere then it makes good sense to borrow other people s data in order to import other people s quality predictors to the new project.
there are many transfer learning methods such as the the dimensionality transform approaches of nam jing et al.
and the similarity based approaches of kocaguneli peters and turhan et al.
.
in both approaches when new code modules are created these approaches comment on code quality using examples taken from similar projects.
rahman et al.
warn that if quality predictors are always being updated based on the specifics of new data then those new predictors may suffer from over fitting.
such over fitted models are brittle in the sense that they can undergo constant changes when new data arrives.
that is when learning from all available data then what we learn may be always changing whenever the available data is changed.
such updates are very common and occur when when considering newly constructed code modules or when we are learning using data from other newly available projects for details on this see .
and the discussion on the burak filter .
conclusion instability is unsettling for software project managers struggling to find general policies.
such instability prevents project managers offering clear guidelines on many issues including a when some module be inspected b when modules should be refactored c where to focus expensive testing procedures d what return on investment might we expect due to decreased defects after purchasing some expensive tool etc.
how to support those managers who seek stability in their conclusions while also allowing new projects to take full benefit from data arriving from all the other projects constantly being completed by other programmers?
perhaps if we cannot generalize from all data a more achievable goal is to stabilize the pace of conclusion change.
while it may be a fool s errand and wait for eternal and global se conclusions one possible approach is for organizations to declare some prior project as the bellwether 1that offers predictions that generalize across nprojects.
this paper defines and distinguish the bellwether effect from the bellwether method the bellwether effect states that when a community of programmers work on a set of projects then within that community there exists one exemplary project called the bellwether which can define quality predictors for the other projects.
the bellwether method searches for that exemplar project and applies it to all future data generated by that community.
1according to the oxford english dictionary the bellwether is the leading sheep of a flock with a bell on its neck.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
the rest of this paper explores bellwethers.
after some background notes as well as an explanation of the bellwether method we ask and answer five research questions rq1 how rare are bellwethers ?
we explore four communities of data containing and projects each.
in a result consistent with bellwethers notbeing rare we find that all these communities have a bellwether i.e.
a single data set from which a superior quality predictor can be generated from the rest of that community.
rq2 how does the bellwether data set fare against local models?
the alternate to transfer learning is to just use the local data to build a quality predictor.
to answer this research question we compare the predictions from the bellwether to predictions from just the local data.
in our experiments the bellwether predictions proved to be better than those generated from the local data.
rq3 is bellwether better than other transfer learning methods?
to answer this question we compare the data predictor generated from the bellwether to the predictions generated from other transfer learning methods.
our bellwether s predictions were observed to be superior to those other transfer learners.
rq4 can we predict which data set will be bellwether?
to answer this question we tried reasoning about the data in candidate bellwethers to see if they shared some property that indicates they will be a useful bellwether.
the results of this investigation were not positive since we could find no such property.
hence our recommended method for finding the bellwether is to try it out against other data sets.
rq5 how much data is required to find the bellwether?
since rq4 failed to find a statistical property that selects for bellwethers then the only way we have to find bellwethers is to compare the performance of pairs of data sets from different projects.
a natural question that arises from this experimental approach is rq5.
our experiments show that program managers need not wait very long to find their bellwethers a few dozen examples of defective code modules can suffice for creating and testing candidate bellwethers.
from the above we conclude that the original motivation for transfer learning in se might have been misguided.
initial experiments with transfer learning in se built defect predictors from the union of data taken from multiple projects.
that approach lead to some very poor results so researchers turned to relevancy filters to find what small subset of the data was relevant to the current problem .
these relevancy filters generated adequate predictions but introduced the instability problem that motivates this paper.
our bellwether results suggest that relevancy filtering would never have been necessary in the first place if researchers had instead hunted for bellwethers.
.
background .
defect prediction our example quality predictors are static code attributes defect prediction.
hall et al.
offers an extensive review on the defect prediction literature .
for an extensive experimental comparison of different learning algorithms for defect prediction see lessmann et al.
.
for brief introduction notes on defect predition see the rest of this section.human programmers are clever but flawed.
coding adds functionality but also defects so software will crash perhaps at the most awkward or dangerous time or deliver wrong functionality.
since programming introduces defects into programs it is important to test them before they are used.
testing is expensive.
according to lowry et al.
software assessment budgets are finite while assessment effectiveness increases exponentially with assessment effort .
exponential costs quickly exhaust finite resources so standard practice is to apply the best available methods only on code sections that seem most critical.
any method that focuses on parts of the code can miss defects in other areas so some sampling policy should be used to explore the rest of the system.
this sampling policy will always be incomplete but it is the only option when resources prevent a complete assessment of everything.
one such lightweight sampling policy is defect predictors learned from static code attributes.
given software described in the attributes of figure data miners can learn where the probability of software defects is highest.
the rest of this section argues that such defect predictors are easy to use widely used and useful to use.
easy to use static code attributes can be automatically collected even for very large systems .
other methods like manual code reviews are far slower and far more labor intensive.
for example depending on the review methods to loc minute can be inspected and this effort repeats for all members of the review team which can be as large as four or six people .
widely used researchers and industrial practitioners use static attributes to guide software quality predictions.
defect prediction models have been reported at google .
verification and validation v v textbooks advise using static code complexity attributes to decide which modules are worth manual inspections.
useful defect predictors often find the location of or more of the defects in code .
defect predictors have some level of generality predictors learned at nasa have also been found useful elsewhere e.g.
in turkey .
the success of this method in predictors in finding bugs is markedly higher than other currently used industrial methods such as manual code reviews.
for example a panel at ieee metrics concluded that manual software reviews can find of defects.
in another work raffo documents the typical defect detection capability of industrial review methods around for full fagan inspections to for less structured inspections.
not only do static code defect predictors perform well compared to manual methods they also are competitive with certain automatic methods.
a recent study at icse rahman et al.
compared a static code analysis tools findbugs jlint and pmd and b static code defect predictors which they called statistical defect prediction built using logistic regression.
they found no significant differences in the cost effectiveness of these approaches.
given this equivalence it is significant to note that static code defect prediction can be quickly adapted to new languages by building lightweight parsers that find information like figure .
the same is not true for static code analyzers these need extensive modification before they can be used on new languages.
.
defect prediction and transfer learning when there is insufficient data to apply data miners to learn defect predictors transfer learning can be used to transfer lessons learned from other sourcesprojects to the target projectt.
initial experiments with transfer learning offered very pessimistic results.
zimmermann et al.
tried to port models between two web browsers internet explorer and firefox and found that crossproject prediction was still not consistent a model built on firefox 123wmc weighted methods per class dit depth of inheritance tree noc number of children cbo coupling between objectsincreased when the methods of one class access services of another.
rfc response for a classnumber of methods invoked in response to a message to the object.
lcom lack of cohesion in methodsnumber of pairs of methods that do not share a reference to an instance variable.
ca afferent couplingshow many other classes use the specific class.
ce efferent couplingshow many other classes is used by the specific class.
npm number of public methods locm3 another lack of cohesion measureifm a are the number of methods attributes in a class number and a is the number of methods accessing an attribute then lcom apa j aj m m .
loc lines of code dam data access ratio of private protected attributes to total attributes moa aggregation count of the number of data declarations class fields whose types are user defined classes mfa functional abstractionnumber of methods inherited by a class plus number of methods accessible by member methods of the class cam cohesion amongst classessummation of number of different types of method parameters in every method divided by a multiplication of number of different method parameter types in whole class and number of methods.
ic inheritance couplingnumber of parent classes to which a given class is coupled includes counts of methods and variables inherited cbm coupling between methodstotal number of new redefined methods to which all the inherited methods are coupled amc average method complexitye.g.
number of ja v a byte codes max cc maximum mccabemaximum mccabe s cyclomatic complexity seen in class avg cc average mccabeaverage mccabe s cyclomatic complexity seen in class defect defect boolean where defects found in post release bug tracking systems.
figure sample static code attributes.
was useful for explorer but not vice versa even though both of them are similar applications.
turhan s initial experimental results were also very negative given data from projects training on s 9source projects and testing on t 1target projects resulted in alarming high false positive rates or more .subsequent research realized data had to be carefully sub sampled and possibly transformed before quality predictors from one source to target.
that work can be divided two ways homogeneous vsheterogeneous similarity vsdimensionality transform .
homogenous heterogenous transfer learning operates on source and target data that contain the same different attribute names respectively .
this paper focuses on homogenous transfer learning for the following reason.
as discussed in the introduction we are concerned with an it manager trying to propose general policies across their it organization.
organizations are defined by what they do which is to say that within one organization there is at some overlap in task tools personnel and development platforms.
hence data can contain overlapping attributes.
as evidence for this the data sets explored in this paper fall into communities and each community has many overlapping attributes specifically our four communities have overlapping attributes see figure .
as to other kinds of transfer learning similarity approaches transfer some subset of the rows or columns of data from source to target.
for example the burak filter builds its training sets by finding thek nearest code modules in sfor everyt2t.
aside note that the burak filter suffers from the instability problems described in the introduction whenever the source or target is updated data miners will learn a new model since different code modules will satisfy the k nearest neighbor criteria.
other researchers doubted that a fixed value of kwas appropriate for all data.
that work recursively bi clustered the source data then pruned the cluster sub trees with greatest variance are pruned where the variance of a sub tree is the variance of the conclusions in its leaves .
this method combined row selection with row pruning of nearby rows with large variance .
other similarity methods combine domain knowledge with automatic processing e.g.
data is partitioned using engineering judgment before automatic tools cluster the data.
to address variations of software metrics between different projects the original metric values were discretized by rank transformation according to similar degree of context factors.
similarity approaches uses data in its raw form.
dimensionality transform methods manipulate the raw source data until it matches the target.
in the case of defect prediction a dimension might be one of the static code attributes of figure .
for example nam et al.
originally proposed an optimization based method that used dimensionality rotational and expansion contraction to align the source dimensions to the target .
subsequently that team found they could dispense with the optimizer by combining feature selection on the source target following by a kolmogorov smirnov test to find associated subsets of columns.
other researchers take a similar approach prefer instead a canonical correlation analysis cca to find the relationships between variables in the source and target data .
our reading of the literature is that dimensionality transform is used mostly in heterogeneous and not homogeneous transfer learning.
hence our experiments use similarity based methods.
.
bellwethers a new approach the previous section sampled some of the work on transfer learning in software engineering.
this rest of this paper asks the question is the complexity of .
really necessary?
to answer this question we propose a process that assumes some software manager has a watching brief over nprojects which we 124will call the community c .
as part of those duties they can access issue reports and static code attributes of the community.
using that data this manager will apply three operators generate apply monitor .
generate using historical data check if the community has bellwether.
see if data miners can predict for the number of issues given the static code attributes.
for all pairs of data from projects pi pj2c predict for issues in pjusing a quality predictor learned from data taken from pi report a bellwether if one pigenerates the most accurate predictions in a majority of pj2c.
.
apply using the bellwether generate quality predictors on new project data.
that is having learned the bellwether on past data we now apply it to future projects.
.
monitor go back to step if the performance statistics seen during apply start decreasing .
note the simplicity of this approach just wrap a for loop around some data miners.
note also that these steps use none of the machine described in .
.
.
research questions rq1 how rare are bellwethers ?
if bellwethers occur infrequently we cannot rely on them.
hence this question explores how common are bellwethers.
to this end we applied the generate method described above to the four communities shown in figure .
this data was selected according to the following rules the data has been used in prior transfer learning paper e.g.
the communities are quite diverse e.g.
the nasa projects are proprietary while the others are open source projects.
in addition the projects also vary in their granularity of data description file class or function level .
rq2 how does the bellwether fare against local models?
one premise of transfer learning is that using data from other projects is as useful or better then using data from the local project.
this research questions tests that this premise holds for bellwethers.
to answer this question we implemented apply as follows.
one of our communities apache comes in multiple versions e.g.
in apache the xalan system has versions .
.
.
.
.
each versions are historical releases where version iwas written before version jwherej i .
rq2 was explored in this community as follows the last version of each project was set aside as a hold out.
generate was then applied across the older versions within the community to find the bellwether.
a defect predictor was then learned from the older data seen in the bellwether.
the predictor was then applied to the latest data.
we compare the above to local learning i.e.
for each project the last version of that project was set aside as a hold out the older versions of that project were then used to train a defect predictor.
the predictor was then applied to the latest data.
note that the local learner only ever uses data from earlier in the same project while the bellwether uses data from any member of the community.
rq3 is bellwether better than other transfer learning methods?
our reading of the literature is that dimensionality reduction transfer learning is the preferred choice for heterogeneous transfer learning while for the homogeneous transfer learning studied here similarity based approaches are the norm.
hence we compare bellwether against two similarity based transfer learners the first classic burak filter from as well as a more recent mixed approach that uses a small sample of the target along with the available source data .
rq4 can we predict which data set will be bellwether?
this question tries to reason about bellwether dataset to identify characteristics if any that make it unique.
to do this we compare the distributions of the code quality metrics that make up the bellwether data with that of the other datasets.
we employ a multiple comparison test .
rq5 how much data is required to find the bellwether?
a core process in all the above is the generate step.
if this requires too much data to find bellwethers then that would indicate developers should eschew bellwethers in favor of standard transfer learning.
hence it is important to ask how much data is required before a community can find adequate bellwethers.
.
methodology .
benchmark datasets this study uses data sets grouped into communities taken from previous transfer learning studies.. the projects measure defects at defect levels of granularity ranging from function level to file level figure summarizes all the communities of datasets used in our experiments.
for the reasons discussed in .
we explore homogeneous transfer learning using the attributes shared by a community.
that is this study explores intra community transfer learning and not crosscommunity heterogeneous transfer learning.
the first dataset aeeem was used by .
this dataset was gathered by d amborse et al.
it contains metrics objectoriented metrics previous defect metrics entropy metrics measuring code change and churn of source code metrics.
the relink community data was obtained from work by wu et al.
who used the understand tool2 to measure metrics that calculate code complexity in order to improve the quality of defect prediction.
this data is particularly interesting because the defect 125community dataset of instances metrics nature total bugs aeeemeq .
classjdt .
lc .
ml .
pde .
relinkapache .
filesafe .
zxing .
apacheant .
classivy .
camel .
poi .
jedit .
log4j .
lucene .
velocity .
xalan .
xerces .
nasacm .
functionjm .
kc .
mc .
mw .
figure defect datasets from communities.
the metrics columns shows the number of metrics that are shared by all members of that community.
information in it has been manually verified and corrected.
it has been widely used in defect prediction .
in addition to this we explored two other communities of datasets from the promise repository3.
the first set of group contains defect measures from several apache projects.
it was gathered by jureczko et al.
.
this dataset contains records the number of known defects for each class using a post release bug tracking system.
the classes are described in terms of oo metrics including ck metrics and mccabes complexity metrics.
each dataset in the apache community has several versions.
there are a total of different datasets.
for more information on this dataset see .
further we used proprietary datasets from nasa containing similar metrics .
for the sake of consistency we cleaned up the dataset so that they all share the same metrics.
these datasets measure mccabe and halstead s cyclomatic complexity metrics in addition to other complexity metrics such as parameter count and percentage comments.
.
learning methods there are many ways to predict defects.
a comprehensive study on the same was conducted by lessmann et al.
.
they endorsed the use of random forests for defect prediction over several other methods.
random forests is an ensemble learning method that builds several decision trees on randomly chosen subsets of data.
the final reported prediction is the mode of predictions by the trees.
is known that the fraction of defects in the training samples affects the performance of defect predictors.
figure shows that in most datasets the percentage of defective samples varies between to except in a few projects like log4j where it is .
handling this class imbalance has been shown to improve the quality of defect prediction.
pelayo and dick report that the defect prediction is improved by smote .
smote works by under sampling majority class examples and over sampling minority class examples to balance the training data prior to applying prediction models.
after an extensive experimentation in this study we randomly sub sampled non defective defective examples until the training data had only non defective defective examples respectively .
important methodological note sub sampling was only applied to training data so the test data remains unchanged .
.
evaluation strategy in our context we consider modules with defects as positive instances and those without as negative instances.
prediction models are not ideal they therefore need to be evaluated in terms of statistical performance measures.
on classification we construct a confusion matrix with this we can obtain several performance measures such as accuracy percentage of correctly classified classes both positive and negative recall or pd percentage of the target classes defective instances predicted.
the higher the pd the fewer the false negative results.
false alarm or pf percentage of non defective instances wrongly identified as defective.
unlike pf lower the pd better the quality precision probability of predicted defects being actually defective.
either a smaller number of correctly predicted faulty modules or a larger number of erroneously predicted defect free modules would result in a low precision.
there are several trade offs between the metrics described above.
there is a trade off between recall rate and false alarm rate.
there is also a trade off between precision and recall.
these measures alone do not paint a complete picture of the quality of the predictor.
therefore it is very common to apply performance metrics that incorporate a combination of these metrics.
one such approach is to build a receiver operating characteristic roc curve.
roc curve is a plot of recall versus false alarm pairing for various predictor cut off values ranging from to .
the best possible predictor is the one with an roc curve that rises as steeply as possible and plateaus at pd .
ideally for each curve we can measure the area under curve auc to identify the best training dataset.
unfortunately building an roc is not straight forward in our case.
we have used random forest for predicting defects owing to it s superior performance over several other predictors .
random forest lacks a threshold parameter it is capable of producing just one point on the roc curve.
it is therefore not possible to compute auc.
in a previous work ma and cukic have shown that distance from perfect classification ed can be substituted for auc in cases where a roc curve cannot be generated.
ed measures the distance between obtained pd pf pair and the ideal point on the roc space weighted by cost function .
it is given by ed p pf pd2 note that for ed the smaller the distance the better the predictor.
setting to e.g.
.
places equal weights on pd and pf.
from a software engineering perspective it is more important to reduce 126misclassification of defective module that to reduce false classification of fault free modules.
with this in mind we have set as0 thereby placing more weight on pd.
in this work we report only on ed measures.
however for the reader of this work who wishes to use different performance measures we have made available a replication package4in order to facilitate the computation of other statistical measures.
.
statistics to overcome the inherent randomness introduced by random forests and smote we use repeated runs each time with a different random number seed we use since that is more than the samples needed to satisfy the central limit theorem .
the repeated runs provide us with a sufficiently large sample size to statistically compare all the datasets.
each run collects the values of ed equation .
note we refrain from performing a cross validation because the process tends to mix the samples from training data the bellwether and the test data the other projects which defeats the purpose of this study.
to rank these numbers collected as above we use the scottknott test recommended by mittas and angelis .
scott knott is a top down clustering approach used to rank different treatments.
if that clustering finds an interesting division of the data then some statistical test is applied to the two divisions to check if they are statistically significant different.
if so scott knott recurses into both halves.
to apply scott knott we sorted a list of l values of equation values found in ls different methods.
then we split l into sub lists m n in order to maximize the expected value of differences in the observed performances before and after divisions.
e.g.
for lists l m n of sizels ms ns wherel m n e ms lsabs m l ns lsabs n l we then apply a statistical hypothesis test hto check ifm n are significantly different in our case the conjunction of a12 and bootstrapping .
if so scott knott recurses on the splits.
in other words we divide the data if both bootstrap sampling and effect size test agree that a division is statistically significant with a confidence of and not a small effect a12 .
for a justification of the use of non parametric bootstrapping see efron tibshirani .
for a justification of the use of effect size tests see shepperd and macdonell kampenes and kocaguenli et al.
.
these researchers warn that even if a hypothesis test declares two populations to be significantly different then that result is misleading if the effect size is very small.
hence to assess the performance differences we first must rule out small effects using a12 a test recently endorsed by arcuri and briand .
.
results .
rq1 how rare are bellwethers ?
figures show the results of generate within our four communities.
it is immediately noticeable that for each community there is one data set that provides consistently better predictions when compared to other datasets.
for example apache s bellwether is lucene nasa s bellwether is mc aeeem s bellwether is lc and relink s bellwether is safe.
thhat is all the communities studied here have a bellwether.
hence answer our results suggest bellwethers are not rare.
.
rq2 how does the bellwether fare against local models?
figure compares ed scores of defect predictors built on local models against those built with a bellwether.
for this question we used data from the apache community since it has the versions required to test older data against newer data.
as see in the figure the prediction scores with the bellwether is very encouraging in case of the apache datasets.
in all cases except for jedit defect prediction models constructed with the lucene bellwether performs as well as local data.
in some cases xerces lucene performs much better than local data.
therefore the answer to the second research question is research answer for projects evaluated with the same quality metrics training a defect prediction model with the bellwether is just as good as so with local data.
rq3 is bellwether better than other transfer learning methods?
figure shows results comparing three homogenous transfer learning methods bellwether the classic burak filter denoted turhan09 and turhan s subsequent update to that method denoted turhan11 .
we note that in usual case bellwethers perfrom much better than those other methods.
hence research answer the bellwether method out performs standard homogenous transfer learning methods.
.
rq4 can we predict which data set will be bellwether?
to study existing trends in the bellwether data we tried to identify the existence of statistical similarities between the distributions of the bellwether samples and the samples from our test cases.
to do this we performed a multiple comparison test using kruskalwallis h test.
the kruskal wallis h test tests the null hypothesis that the population median of two or more groups are equal.
it is a non parametric version of the anov a test.
in each group we compared the distribution of the metric values of bellwether dataset and all other datasets.
if the null hypothesis as formulated above is rejected that means there doesn t exist a statistically significant similarity between medians of the bellwether and the test data for that metric.
if the bellwether bore any resemblance to the test data we would expect to see several metrics with statistically significant similarity.
for instance in the apache projects in figure we noticed that there doesn t exist any noticeable similarities.
further as highlighed in figure the same effect was noticed in all the other projects as well.
a mere reflection on the distribution or feature importance is not sufficient to determine of a specific dataset is a bellwether or not.
therefore our response to the third research question is as follows 127ant camel ivy poi velocity xalan xerces jedit log4j lucene med iqr med iqr med iqr med iqr med iqr med iqr med iqr med iqr med iqr med iqr ant .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
camel .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ivy .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
poi .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
velocity .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
xalan .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
xerces .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
jedit .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
log4j .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cm jm kc mw mc med iqr med iqr med iqr med iqr med iqr cm .
.
.
.
jm .
.
.
.
kc .
.
.
.
mw .
.
.
.
0eq jdt ml pde lc med iqr med iqr med iqr med iqr med iqr eq .
.
.
.
.
.
.
.
jdt .
.
.
.
.
.
.
ml .
.
.
.
.
.
.
.
pde .
.
.
.
.
.
.
.01apache zxing safe med iqr med iqr med iqr apache .
.
.
zxing .
.
figure identifying the bellwether this figure compares the prediction performance of the bellwether highlighted in bold against other datasets.
med refers to the median value seen in repeats.
iqr refers to the 75th 25th percentile seen in those repeats and the iqr is very small .
all performance figures here are the ed from equation so lower values are better .
cells highlighted in gray produce the best performance with the highest scott knott ranks.
bellwether local med iqr med iqr apacheant .
.
.
.
camel .
.
.
.
poi .
.
.
.
xalan .
.
.
xerces .
.
.
.
ivy .
.
.
.
velocity .
.
.
log4j .
.
.
.
jedit .
.
.
.
figure bellwether vs. local data.
performance scores are ed so lower values are better .
we note that in all datasets except for jedit the performance of bellwether dataset is just as well as local data.
research answer although there seemingly always exists a bellwether dataset identifying this dataset by reflecting on distribution of the data is not trivial.
rq5 how much data is required to find the bellwether?
as yet we do not have a theoretical analysis offering a lower bound for the the number of examples required for finding the bellwethers.
what we do have is the following empirical observation all the above results were achieved using the sub sampling methods of .
i.e.
randomly selected non defective modules and randomly selected defective modules.
that is research answer bellwethers can be found after projects have discovered a few dozen examples of defective modules.bellwether turhan09 turhan11 med iqr med iqr med iqr apacheant .
.
.
.
.
.
camel .
.
.
.
.
.
ivy .
.
.
.
.
.
poi .
.
.
.
.
velocity .
.
.
.
.
xerces .
.
.
.
.
jedit .
.
.
.
.
.
log4j .
.
.
.
.
.
xalan .
.
.
.
.
aeeemeq .
.
.
.
.
jdt .
.
.
.
.
.
ml .
.
.
.
.
.
pde .
.
.
.
.
relinkapache .
.
.
.
.
zxing .
.
.
.
.
.
nasacm .
.
.
.
jm .
.
.
.
kc .
.
.
mw .
.
.
.
figure bellwether vs. homogeneous transfer learning methods turhan09 and turhan11 .
all results are ed solower values are better .
cells highlighted in gray indicate datasets with superior prediction capability.
.
threats to v alidity .
sampling bias sampling bias threatens any classification experiment what matters in one case may or may not hold in another case.
for example 128wmc dit noc cbo rfc lcom ca ce npm lcom3 loc dam moa mfa cam ic cbm amc max cc avg cc ant x x x camel x x ivy x x x x jedit x x xxx log4j x x x poi x xx x xx x x velocity x xx x x xalan xx x x x xerces xx x x figure results of kruskal wallis h test comparing the bellwether dataset lucene with the other datasets from the apache.
each dataset contains static code metrics for a description of each of these metrics please refer to .
the rows contain the test data and the columns contain the metrics.
a x symbol represents a significant statistical similarity with a confidence interval and a represents a no similarity.
group dataset metricsbellwether significant aeeemeq lc jdt ml pde relinkapache 0safe zxing apacheant luceneivy camel poi jedit log4j velocity xalan xerces nasacm mc jm kc mw figure results of kruskal wallis h test comparing the bellwether datasets with the test datasets.
even though we use open source data sets in this study figure which come from several sources apache and nasa were obtained from the promise repository and relink and aeeem were obtained from they were all supplied by individuals.
that said this paper shares this sampling bias problem with every other data mining paper.
as researchers all we can do is document our selection procedure for data as done in and suggest that other researchers try a broader range of data in future work.
.
learner bias for building the defect predictors in this study we elected to use random forests.
we chose this learner because past studies shows that for defect prediction the results were superior to other more complicated algorithms and can act as a baseline for other algorithms.apart from learner choice one limitation to our current study is that we have focused here on homogenous transfer learning where the attributes in source and target have the same name .
the implications for heterogeneous transfer learning where the attributes in source an target have different names are no clear.
we have some initial results suggesting that an bellwether like effect occurs when learning across the communities of figure but those results are very preliminary.
hence for the moment we would conclude for the homogenous case we recommend using bellwethers rather than similarity based transfer learning.
for the heterogenous case we recommend using dimensionality tranforms.
.
evaluation bias this paper uses one measure of prediction quality ed see equation .
other quality measures often used in software engineering to quantify the effectiveness of prediction discussed in .
.
a comprehensive analysis using these measures is left for future work.
.
order bias with random forest and smote there is invariably some degree of randomness that is introduced by both the algorithms.
random forest as the name suggests randomly samples the data and constructs trees which it then uses in an ensemble fashion to make predictions.
to mitigate these biases we run the experiments times the reruns are greater than in keeping with the central limit theorem .
note that the reported variations over those runs were very small see the low iqr values in tables and .
hence we conclude that while order bias is theoretically a problem it is not a major problem in the particular case of this study.
.
conclusions and discussion when historical data is limited or not available e.g.
perhaps due the project being in its infancy developers might seek data from other projects.
our results show that regardless of the granularity of data see the file class file values of figure there exists a bellwether data set that can be used to train relatively more accurate defect prediction models.
this bellwether does not require elaborate data mining methods to discover just a for loop around the data sets and can be found very early in a project s life cycle after uncovering a few dozen defective code modules .
129as discussed in the introduction the results of this paper cast some doubts on the results that originally motivated much of the transfer learning results in defect prediction since the original turhan paper on the burak filter .
our bellwether results suggest the relevancy filtering of the burak filter would never have been necessary in the first place if researchers had instead discovered bellwethers.
finally when discussing this work there are three frequently asked questions .do bellwethers guarantee permanent conclusion stability?
no and we should not expect them to.
the aim of bellwethers is to slow but do not necessarily stop the pace of new ideas in software engineering e.g.
as in the paper new quality prediction models .
sometimes new ideas are essential.
software engineering is a very dynamic field with a high churn in techniques platforms developers and tasks.
in such a dynamic environment it is important to change with the times.
that said changing more than necessary is not desirable hence this paper.
.how to detect when bellwethers need updating?
the conclusion stability offered by bellwethers only lasts as long as the bellwether remains useful.
hence bellwether performance must always be monitoried and if that performance starts to dip then seek a new bellwether.
.what happens if a set of data has no useful bellwether?
in that case there are numerous standard transfer learning methods that could be used to import lessons learned from other data .
that said the result here is that all the communities of data explored by this paper had useful bellwethers.
hence we would recommend trying bellwethers before moving on to more complex methods.
further to this last point in his text on empirical software engineering cohen recommends benchmarking supposedly more sophisticated methods against simpler alternatives.
going forward from this paper we would recommend that the transfer learning community uses bellwethers as a baseline method against which they can test more complex methods.
.