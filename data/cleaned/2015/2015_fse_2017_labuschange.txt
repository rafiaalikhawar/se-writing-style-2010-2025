measuring the cost of regression testing in practice a study of java projects using continuous integration adriaan labuschagne university of waterloo waterloo on canada alabusch uwaterloo.calaura inozemtseva university of waterloo waterloo on canada lminozem uwaterloo.careid holmes university of british columbia vancouver bc canada rtholmes cs.ubc.ca abstract software defects cost time and money to diagnose and fix.
consequently developers use a variety of techniques to avoid introducing defects into their systems.
these techniques have their own costs the benefit of using a technique must outweigh the cost of using it.
in this paper we investigate the costs and benefits of automated regression testing in practice.
specifically we studied projects that use travis ci a cloud based continuous integration tool in order to examine real test failures that were encountered by the developers of those projects.
we determined how developers resolved the failures they encountered and used this to classify the failures as being caused by a flaky test by a bug in the system under test or by a broken or obsolete test.
we consider test failures caused by bugs represent a benefit of the suite while failures caused by broken or obsolete tests represent test suite maintenance costs.
we found that of test suite executions fail and that of these failures are flaky.
of the non flaky failures only were caused by a bug in the system under test the remaining were due to incorrect or obsolete tests.
in addition we found that in the failed builds only .
of the test case executions failed and of failed builds contained more than one failed test.
our findings contribute to a wider understanding of the unforeseen costs that can impact the overall cost effectiveness of regression testing in practice.
they can also inform research into test case selection techniques as we have provided an approximate empirical bound on the practical value that could be extracted from such techniques.
this appears to be large as over of the test case executions could have been eliminated with a perfect oracle.
ccs concepts software and its engineering software testing and debugging software verification and validation software libraries and repositories keywords regression testing test economics flaky tests permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse september paderborn germany copyright held by the owner author s .
publication rights licensed to association for computing machinery.
acm isbn ... .
reference format adriaan labuschagne laura inozemtseva and reid holmes.
.
measuring the cost of regression testing in practice a study of java projects using continuous integration.
in proceedings of 11th joint meeting of the european software engineering conference and the acm sigsoft symposium on the foundations of software engineering paderborn germany september esec fse 10pages.
introduction software defects cost developers time and money.
among other things bugs can discourage new customers from adopting a product and can drive away existing customers.
many techniques exist for avoiding the introduction of bugs and for quickly identifying bugs once they have been introduced.
unfortunately these techniques have their own costs so developers must carefully assess each technique s cost effectiveness before deciding whether and to what extent to adopt it.
automated regression testing is one technique for fault detection that has seen wide adoption .
this technique involves regularly executing a suite of tests to determine if recent changes to the software have negatively impacted existing functionality.
regression testing can be costly developers must write maintain and regularly execute their test suite as the code evolves.
kasurinen et al.
conducted a survey of industrial developers that found that the development expenses and maintenance costs associated with automated testing were a major impediment to adoption .
one of their participants stated developing that kind of test automation system is almost as huge an effort as building the actual project.
they also saw some organizations decreasing their regression testing investments due to test suite maintenance costs.
the costs of automated regression testing are only worth incurring if the use of this technique provides adequate compensatory benefits.
in particular the suite must detect enough faults that the developers feel their investment in the suite is justified.
previous studies have considered the cost effectiveness of automated regression testing as we will discuss further in section .
however these studies share a common limitation they were conducted by mining software repositories.
repository mining allows one to see how the test suite changes over time which gives some indication of the development and maintenance cost of the suite.
however it does not allow one to measure the benefits in terms of detected faults.
to better understand the cost benefit tradeoff we examined the build history of open source systems that use the travisci1 821esec fse september paderborn germany a. labuschagne l. inozemtseva and r. holmes continuous integration service.
when a test suite failed we attempted to determine whether the failure was caused by a nondeterministic test by a fault in the system under test or by an obsolete or incorrect test.
we consider that a detected fault represents a benefit of the suite while an obsolete or incorrect test represents a suite maintenance cost.
determining the number of suite failures that belong to each group therefore provides some insight into the costs and benefits of the test suite.
section 3describes our procedure in detail.
specifically we asked the following research questions rq1 what proportion of test suite executions are non deterministic or flaky ?
rq2 once these flaky test suite executions are accounted for what proportion of test suite failures represent a maintenance cost and what proportion represent a benefit?
rq3 why do tests usually require maintenance and can maintenance costs be reduced?
rq4 what proportion of test case executions detect a fault?
our answers can be summarized as follows a118 of test suite executions failed the remaining passed.
of the failures were flaky.
a2defects in the test suite itself were the cause of of nonflaky failures.
defects in the system under test were the cause of of non flaky failures.
a3there were a number of reasons for test suite maintenance.
some of the maintenance could have been avoided through the use of better testing processes.
a4an individual test case that was run as part of a build that failed had only a .
chance of failing.
this establishes a useful bound for test suite reduction a perfect reduction technique could reduce the number of test executions by over and still detect the same number of faults.
of failed builds contained more than one failed test.
our findings are described in depth in section .
following the presentation of results section 5presents threats to validity and describes our replication dataset.
finally section 6concludes.
related work previous work has considered the cost benefit tradeoff of automated regression testing.
we first discuss studies that used repository mining to measure the cost of maintaining a regression test suite.
next we discuss studies that captured test outputs but were not able to use this information to measure the costs and benefits of regression testing.
finally we briefly discuss studies that attempted to decrease the costs of regression testing by reducing the amount of time required to execute the suites.
these particular studies of test selection and prioritization relate to our fourth research question.
.
measuring test suite maintenance seven previous studies have considered test suite maintenance costs by exploring how test suites evolve over time.
zaidman et al.
examined how developers evolve their test suites along with the test code and whether testing effort varied by the project schedule.
they found both close synchronous evolution as well as separate stepwise evolution but failed to find any increase in testing effort before a major release.
pinto et al.
studied test evolution and found that of the changes made to test suites were modifications were test additions and were test deletions.
marsavina et al.
studied the co evolution of production and test code.
they found that test code changes made up between and of all code changes for the projects they studied.
beller and zaidman found that tests and production code have some tendency to change together but that tests were not changed every time the system under test was changed and vice versa.
kasurinen et al.
conducted a survey of industrial developers that among other findings identified development expenses and maintenance costs as the main obstacles to adopting automated testing.
in fact one company that experimented with automated testing eventually removed the test suite due to the cost of maintaining it.
this indicates that maintenance is perceived as very expensive and in at least one case the cost was high enough that developers could not justify using automated regression testing.
grechanik et al.
estimated that the costs associated with maintaining and evolving test scripts are million to million per year.
they also showed that even simple changes could result in to of test scripts needing maintenance a process that could take hours or days and caused interruption to continuous integration systems.
while developers in the study saw the benefits of the automated test suite the maintenance burden often caused them to throw away their tests and start from scratch often with faulty logic due to time pressures.
herzig et al.
developed a test selection tool called theo that selects tests to be executed if the probable cost of executing the test is lower than the probable cost of skipping the test.
to calculate these costs theo uses the past defect detection rate for each test case i.e.
the true positive rate and the past false alarm rate of a test case i.e.
the false positive rate .
these probabilities can then be used along with data such as machine costs number of engineers and inspection costs in order to estimate the cost of skipping or executing a test.
an evaluation of the system using historical data from microsoft projects allowed to of tests to be skipped while only letting .
to of defects escape.
in section 4we show that in our dataset a perfect test selection technique would execute less than .
of tests.
all seven of these studies share the same limitation as they were performed retroactively by mining software repositories the execution results of the suites are unknown.
this means we cannot measure the benefit provided by the test suites in terms of the number of faults detected.
we also cannot know the impact of flaky tests on these results since these can only be validated by repeatedly executing the same test.
without this information we do not have a complete picture of the cost benefit tradeoff.
.
capturing test outputs three previous studies have managed to capture test outputs.
anderson et al.
provided insight into microsoft dynamix ax r2 a large industrial project.
this project comprised over .
million lines of product source code and over .
million lines of regression 822measuring the cost of regression testing in practice a study of java projects using continuous integration esec fse september paderborn germany test code.
the authors reported that running the regression suite resulted in a test case failure rate during each execution of the test suite.
memon and soffa found that of all tests failed between successive releases of a single industrial product suggesting that test failures are relatively common.
beller et al.
report that of ide based junit test runs fail.
vasilescu et al.
explored the relationship between travis ci build results success or failure and the way the build was started i.e.
direct commit from a developer with write access to the repository or a pull request .
the authors found that builds started by pull requests are more likely to fail than those started by direct commits.
they also found that although of the projects they studied are configured to use travis ci only actually do we observed the same behaviour as we will describe in section .
unfortunately while these studies captured test execution results it is not clear what proportion of the observed failures occurred because of faults and what proportion occurred because the tests required maintenance.
additionally developers in these studies may have been using test driven development methodologies which would further influence the test failure rates.
it is therefore hard to draw conclusions about the amount of developer time devoted to test maintenance and thus the cost benefit ratio of regression testing from these studies.
.
reducing suite execution time a number of studies have attempted to reduce the cost of regression testing by reducing the cost of executing the suite.
the basic assumption is that re running all the tests every time is too expensive and some way of reducing this cost is required.
as an example rosenblum and weyuker proposed the use of test coverage information to predict the cost effectiveness of regression testing strategies.
elbaum et al.
used test suite2selection at the pre submit stage and test suite prioritization at the post submit stage to increase the cost effectiveness of testing.
during the pre submit phase i.e.
before code was pushed to the central repository suites that failed during a pre determined failure window were selected to be executed the basic intuition being that recent failures are likely to predict future failures.
new tests and tests that had been skipped more than a set number of times were also selected during this phase.
during the post submit testing phase the authors attempted to avoid skipping test suites and thus prioritized rather than selected test suites using the same criteria used during the pre submit phase.
this strategy ensured that all test suites were executed during the post submit phase while running the suites that were most likely to fail first thereby shortening the feedback loop.
anderson et al.
developed two test prioritization techniques that used test result history.
the first technique most common failures is based on the intuition that tests that failed the most in the past are the most likely to fail in the future.
the second technique failures by association attempts to use association rule mining to improve on the first technique.
the authors found that the techniques had similar performance when predicting future failures 2note that test suites not test cases are being selected and or ordered in this study.and that both worked better when a small window of recent executions was used rather than the entire historical dataset.
most test suite reduction techniques use coverage to detect redundant tests.
this leads to a loss in fault detection ability because fully overlapping coverage does not necessarily mean the tests will always fail under the same circumstances .
to address this issue koochakzadeh and garousi developed a test reduction tool that brings a human tester into the loop.
their tool identifies potentially redundant tests using coverage analysis then lets testers inspect these tests to identify the true positives that can be removed from the suite.
using this technique they were able to remove eleven of the tests in their subject system without affecting the suite s mutation score.
in contrast the coverage based approach identified tests as redundant and reduced the mutation score by .
rothermel et al.
studied how the granularity of test suites influenced the cost effectiveness of regression testing.
they concluded that typical regression testing techniques usually do not lose fault detection capability when operating on coarse grained test suites but they do tend to save test execution time.
while the information produced by these studies sheds light on the costs of regression testing they are concerned only with the cost of executing the test suite while we focus on the cost of maintaining the suite.
in addition like other studies that we have discussed the ones above do not measure the benefits of regression testing in terms of faults detected.
however these studies are relevant to our final research question which asks what proportion of test case executions are truly necessary.
method we began our study by identifying a large set of mature java based projects that use the travis infrastructure to execute their tests in the cloud.
while cloud based systems do not prevent developers from running and fixing their tests locally they encourage deferring test execution to external services.
this means that the execution history of a test suite which is lost when the suite is run locally is captured by the cloud based system.
this history enables us to observe the test failures that developers actually encountered as they worked on their systems.
we selected subject programs by querying the github archive3 for java projects that received more than push events between and this time frame was chosen based on travis support for java which began in february .
the query returned projects of these projects had travis accounts.
we were able to successfully clone of these projects from github.
as the focus of our analysis was on regression testing we eliminated early stage projects that were less than three years old.
this resulted in projects.
of these projects only actively used travis to execute their test suites the other projects had signed up for the service but did not use it.
unfortunately several of these remaining projects did not configure travis correctly or did not examine the travis output resulting in long stretches of broken builds.
other projects almost never experienced a failure possibly because the developers were testing their code locally before pushing to travis.
if this is the case the test suite execution results have been lost meaning we cannot assess the costs and benefits of the 823esec fse september paderborn germany a. labuschagne l. inozemtseva and r. holmes .
.
.
.
.
.
.
source lines of code test code product code figure growth of production source code and test source code over the study period.
at the start of the study period test code accounted for only .
of all code but by the end of the study period this had increased to .
.
mloc .
writing and maintaining this amount of test code represents a significant investment that must be carefully considered given finite developer resources.
suite.
to account for these two extremes we removed the of the projects that had the most errors and failures and the that had the fewest errors and failures resulting in a final set of projects .
table size and percentage increase in terms of sloc of the production and test code over the two year study period for the subject systems.
code test lines of code increase .
.
having identified these projects we examined the total size and churn in the test suites and production code of these systems to determine how large the testing effort was.
the aggregate results for the size of the production code and test code are shown in table .
this table shows that although the proportion of product code is greater than test code the test code grew faster than production code the proportion of test code grew from .
to .
see figure during the study period.
in total the test code for our systems totalled over .
million sloc4.
while sloc is an imperfect measure of development effort the results still show that these development teams spent considerable time and effort creating and evolving their test suites.
our next step was to gather information about the development history of these projects.
when using the travis infrastructure each time a developer pushes a change or opens a pull request travis downloads the change builds the project and executes its test suite this process is called a build .
travis stores the state of every build which can have one of the following five values 4calculated using non comment source lines ncsl using cloc aldanial cloc .
pass the build was successfully compiled and all tests in the test suite passed.
error the build failed before test execution began i.e.
there was a compilation or configuration error.
fail the build was successfully compiled but one or more test assertions failed or encountered an unexpected runtime exception.
cancel a developer manually terminated the build while it was running.
started the build was started but has not yet finished.
table 2summarizes the number and proportion of builds with each state in our dataset5.
note that the build frequently does not pass of builds have a state other than pass .
if these projects made only one build per day every day of the year their products would spend more than weeks in non passing states.
this is problematic because resolving build failures takes time away from other active development tasks and thus represents a cost that must be factored into the effort required to build real software systems.
table aggregated build results for open source java projects that use travis and the state of build executions from those projects.
state builds builds pass fail error cancel started total 5started and cancelled builds are byproducts of the way travis stages builds and are hereafter elided from our discussion.
824measuring the cost of regression testing in practice a study of java projects using continuous integration esec fse september paderborn germany in this study we were interested in whether resolving a build failure required fixing a fault in the system under test or maintaining the test suite itself.
in other words we were interested in cases where the build transitions from fail to pass .
however we wanted to ensure the build failure was not the result of a flaky test as in this case the changes made by the developer are not the reason the build returned to the pass state.
to determine how non flaky build failures were resolved we performed the following three steps determined which commits were associated with each build for transitions from the fail to the pass state determined whether test code system under test code or both were changed in the commit s that fixed the build and reran the failing builds to determine which ones were nondeterministic and should be excluded from the analysis.
we discuss these three steps in turn.
.
identifying commits associated with a build in addition to storing build state travis stores the branch and sha of the head commit associated with each build.
travis builds a project and executes its tests every time a developer pushes a change or opens a github pull request.
this means that a travis build consists of a set of one or more commits for example a pull request can be composed of three commits that comprise a single build.
we examined the git repository for each build to determine which commits were part of the travis build.
unfortunately not all commits associated with each build could be recovered from the project repository.
there are two reasons for this the build was associated with a pull request.
travis creates a new build every time a pull request is created or updated.
the commits associated with these builds however are never present in the repository even if the pull request is merged as the builds are run against the merge commit between the pull request and the up stream branch which does not exist in the master repository.
history rewrites.
if a developer rewrites history the build s that were triggered by the rewritten commits will no longer be traceable to a commit in the repository.
ultimately of the builds executed by travis had associated commits in the project version control repositories.
the remainder of our analyses are on these builds.
.
categorizing failure resolving changes figure 2shows how the builds we analyzed transitioned between their various states.
the data show that the build continues to pass only of the time and systems stay in the pass state for an average of .
builds before transitioning to a fail or error state.
error states are fixed more quickly persisting for an average of .
builds while fail states persist for an average of .
builds.
note that as it does not make sense to compare arbitrary commits to each other in a distributed version control system we had to carefully consider the flow of changes between branches.
for instance if a project has been split into three concurrent branches a b and c it only makes sense to compare adjacent changes in ato each other and adjacent changes in b to each other.
also if only one commit is ever made to c there is no other change to compare it to.
consequently the number of transitions between builds differs from the number of builds throughout the paper.
we next discuss transitions grouped by their start state.
errorpass fail58 figure summary of the transitions between build states in our dataset.
each transition is caused by one or more commits.
pass !pass persisted for an average of .
builds error!error persisted for .
builds while fail !fail persisted for .
builds.
pass!
.the largest proportion of changes in this category were changes where the test suite passed before and after a change.
this occurred as a result of changes to non code resources e.g.
documentation or changes to code resources production and or test that did not introduce failures or errors.
builds transitioned into error states when changes were made to either the build infrastructure itself for instance by changing a configuration file or when the system could no longer be compiled which was often caused by missing dependencies.
while we expected newly introduced failures to be caused by failure inducing changes to code resources we also saw a large proportion of commits that seemed to be failing for no reason at all further examination of these changes showed that these failure transitions were being caused by flaky tests see section .
.
fail!
.the largest proportion of changes to failing builds did not resolve the failure once a build failed this state persisted for an average of .
builds.
most systems persisting in failing states seemed to continue to fail because developers were making multiple unrelated changes to the system.
while in some cases it seemed that they were aware that these changes were larger and that the build would continue to fail in other cases they seemed to be disparate changes that happened by chance while the build was already failing.
while failing builds sometimes transitioned into error states primarily through adding new unresolved dependencies the failures were usually resolved to a passing state by fixing the test failures.
825esec fse september paderborn germany a. labuschagne l. inozemtseva and r. holmes error!
.builds stayed in error states for the shortest period of time .
builds on average.
while error states often took a few tries to resolve mainly by committing changes to configuration files build scripts and code dependencies once these were resolved the build was able to transition back to a passing state.
transitions from the error state to fail state usually corresponded to added dependencies that allowed the code to be compiled so the test suite could run and fail .
key build transitions.
we noted above that once a build fails it remains in the failing state for an average of .
builds.
when a build fails for a prolonged period of time it could be because the developers are not heeding the failures for instance if they are landing a diverse set of changes.
we therefore chose to focus our investigation on failures that lasted for exactly one build i.e.
pass !fail!pass .
our analysis does not consider all fail!
pass transitions due to the difficulty in reasoning about the diversity of changes made to a system that remained in fail orerror states for prolonged periods of time.
this also reduced the number of builds that needed to be re executed to identify flaky tests see section .
making the study more tractable.
we identified a total of such build tuples.
each pass!fail!pass instance comprised three builds by definition and each build contained an average of .
commits .
commits per tuple .
figure 3shows a triple matching this pattern that consists of three builds and eight commits.
.
eliminating flaky builds having identified these build tuples we used the travis api to rerun the failed build for each of the pass!fail!pass tuples three times.
we created three github repositories for each project each on a separate account and connected each of these accounts to travis ci.
we then created a branch at the commit associated with each of the failed builds.
the branches were then pushed to each of the github repositories triggering three identical builds for each branch on travis ci.
to ensure that a build is triggered when a branch is pushed we removed the branch whitelist and blacklist sections from the travis configuration file.
we also removed the deploy and notification hooks from the configuration file to avoid disturbing the original developers.
unfortunately in many cases all three re executions resulted in error builds.
we found that this usually happened when dependencies were no longer available or when builds required features no longer supported by travis ci these builds amounted to of the re executions we tried and were discarded.
this decreased our transitions to a final dataset of build tuples that could be re executed and reliably analyzed.
results in this section we answer the research questions defined in section1.
.
rq1 what proportion of tests are flaky?
if all three re executions of a build failed see section .
we considered the build to be a non flaky deterministic failure.
we also considered three passes to be non flaky despite a pass being different from the original failure.
only re executions that included atleast two different results were classified as flaky e.g.
fail fail pass orpass pass fail .
of the failures were not consistent according to this categorization and were removed from subsequent analyses.
note that executing builds three times establishes only a lower bound on flakiness executing the builds more times may have led other builds to be classified as non deterministic.
however as we were using the travis infrastructure running the builds many more times would have exceeded rate limits and placing this burden on their infrastructure could have raised ethical concerns.
answer to research question .
.
of the failing builds in the dataset of failed non deterministically.
.
rq2 how often are test failures beneficial?
having identified deterministic build failures we addressed our second research question which of these failures represent a cost of regression testing and which represent a benefit?
for example in figure a developer made three consecutive code only commits to their passing system.
in so they introduced a fault that caused a test failure i.e.
a transition to the fail state.
to determine the cause of the failure we diff the last commit of thepass build against the last commit of the fail build.
in this case only source code files were changed meaning that the cause of the test failure was a source code change.
the developer then performed three commits that were executed together to return the build to the pass state since we cannot determine which commit fixed the fault we label this as a code test fix.
table 3describes all of the possible transitions between three builds that pass!fail!pass .
the builds have been grouped by the kind of resource changes that resolved the test failure.
from this table we can see that .
of non flaky fail builds are resolved with source code changes alone .
are resolved by fixing tests alone and .
are fixed by a combination of source and test changes.
we consider the three categories of failing builds that were resolved by code fixes alone .
as a positive indication that the test failure identified a source code defect.
we make this determination because the developer resolved the failure by only changing the source code that is the failing test was correct and the fix was to modify the code to make the test pass in future builds.
in contrast the three categories of failing builds resolved by test fixes alone .
represent a cost for the developer the fault was resolved by maintaining the suite itself.
these changes acknowledge that the test itself was faulty or obsolete typical remediation of these failures requires either modifying the test or removing it altogether.
we do not claim that fixing these failing tests could not provide a benefit in the future by detecting faults however the maintenance cost for these tests must be considered when modelling the overall cost of the testing strategy.
to gain insight into the .
of builds that were resolved with both source and test code changes we examined the proportion of each change that was made to the source code and the test code to fix the build.
for these builds we observed that on average of lines changed were in test files while the remainder were in source code files.
as we considered fixing failing builds by 826measuring the cost of regression testing in practice a study of java projects using continuous integration esec fse september paderborn germany table categorization of how non flaky builds transition between pass !fail !pass.
the left hand side of !
denotes the resources that changed to cause the test failure.
the right hand side of !denotes the resources that were changed to resolve the test failure.
fix type resources changed code fixescode!code .
test!code .
code test !code .
!code .
test fixescode!test .
test!test .
code test !test .
!test .
mixed fixescode!code test .
test!code test .
code test !code test .
!code test .
cctcccccctcode changes onlycode test changespassfailfailing difffixing diffpass figure as developers work on their systems their commits often change the state of the build.
each box represents a commit builds are often not run on every commit but instead on blocks of commits.
a c label on a commit means the code under test was changed a t label means the test code was changed.
the figure shows a failure caused by a code change that was resolved by changing both code and test files.
changing the source code beneficial we classify .
of all changes of the mixed builds as fixing defects and the remainder .
of all changes as test maintenance.
while splitting test and code changes in this way is not optimal it consistently captures the proportion of changes made to both kinds of files.
figure 4summarizes the proportion of failing builds that are resolved by fixing faults and by maintaining tests.
answer to research question .
in the systems under study .
of deterministic test suite failures represented a cost of regression testing the test suite had to be maintained to return it to a pass state.
.
of deterministic failures detected real faults and therefore represented a benefit of regression testing.
.
rq3 what leads to test maintenance?
given the answer to the previous research question it is natural to wonder what kinds of test maintenance were done.
to answer this question we identified tests that required maintenance disproportionately often.
our first step was to identify pass fail behaviour executable failing builds deterministic failures .
code defect .
test defect .
flaky failure .
figure the categorization of test failures in practice for our executable failing builds.
ultimately one quarter of non flaky failures did not find defects in the code under test and represent failures that required developer effort to investigate and resolve without improving the overall quality of the code under test.
of individual test cases.
to do this we parsed the test logs of our pass!fail!pass tuples.
unfortunately parsing test logs is a difficult process prone to project specific noise and one time errors due to this we were only able to parse the logs for of the projects.
given the list of individual test cases that passed and failed in each build we assigned each individual test case a score the number of times it failed and caused a code only fix minus the number of times it failed and caused a test only fix.
a positive number indicates that the test found bugs in the code under test on more occasions than it required maintenance.
a negative number is the opposite the test required maintenance more often than it found bugs.
figure 5shows the average scores obtained for the tests of projects.
the projects apache pdfbox andapache jackrabbit oak are elided for clarity the first as it has a score of .
and the second because its tests experienced code and test fixes.
ten projects where the sum of test only and code only fixes is smaller than five were removed to reduce clutter.
the size of the bubbles indicates the portion of test failures that were resolved by a mixture of code and test changes.
this can be seen as the portion of test fixes where there is some doubt as to whether the fix is a return on investment or a maintenance cost.
on the x axis we plot the sum of all test failures that were resolved by code or test changes alone on a logarithmic scale this indicates the number of data points we have for each project.
from the figure we can see that on average the tests of nine projects require maintenance more often than they find bugs.
it is therefore possible that these test suites add very little value or even represent a loss for the projects.
we also notice that the difference between code only fixes and test only fixes for most projects is small ranging from .
graylog2 server to .
cloudify .
this is due in part to the fact that of tests that fail only fail 827esec fse september paderborn germany a. labuschagne l. inozemtseva and r. holmes once and fail two times or fewer.
there are however tests that seem to provide a large return on investment tests have a score of or higher.
some tests are costly to maintain tests have a score of and tests a score of .
table manually inspected tests as well as an example of a broken build and the change that fixed the test.
the break and fix entries are hyperlinks to the original travis ci test suite execution output.
test provisioningtests.cancreateuser... break cloudfoundry .. fix test packageapitest.delete break basexdb .. fix test schedulerservicetest.savetask... break openmrs .. fix test ftindexquerytest.testfttest break basexdb .. fix test fnclienttest.clientclose break basexdb .. fix test javafunctest.staticmethod break basexdb .. fix test commandtest break basexdb .. fix openmrs core graylog2 serverjooqrultorcloudify spring bootokhttpquasar killbillopenmicroscopy sonarqubethreddsbasexjackson databindessentials uaasupernode bnd cucumber jvmnodebox dynjsontrackowlapicamunda bpm platformorientdbpicketlink .
.
.
.
.
101001000total code only and test only fixesrelative benefit figure average difference between the number of code fixes and the number of test fixes.
the radius of the bubbles represents the relative size of code test fixes compared to all fixes.
the sum of all code fixes and test fixes for a project is plotted on the x axis on a logarithmic scale.to understand the causes of maintenance we qualitatively studied the failures associated with seven of the least valuable tests.
these tests are listed in table .
one of the tests experienced a testonly fix twice the remaining six experienced only one test only fix and none of the tests experienced mixed or code only fixes.
two of the tests failed due to unintended interaction between tests.
the first test test depended on an after method that did not function correctly.
the test deleted the user with email address jo foo.com from the database but failed because the users s email address was in lower case in the database.
this test used to work since mysql by default is not case sensitive but stopped working when the test was run on postgres which is case sensitive by default.
the second test test was fixed by re initializing the context between tests.
the breaking change removed the reinitialization code and the fix reverted the change.
the developers for one of the tests test were aware that their test was non deterministic.
after the second failure of this test the developers disabled it.
one test test was too difficult to analyze and we could not determine whether it was non deterministic.
another test test as well as the four others that failed with it are an example of new tests being added to the suite that failed on the first execution.
they were fixed by changing constants.
of the last two tests the first is an example of a functional change resulting in test suite maintenance test and the second is an accident where changes were applied before they were ready test .
from these tests we can see that test failures that lead to maintenance occur for a diverse set of reasons.
these include tests that fail because of interaction between tests non deterministic tests new tests failing immediately after creation tests failing after a change in functionality and an accident where changes were merged before they were ready.
this reveals that test code maintenance is not necessarily tied to product code evolution.
ensuring that tests do not depend on assumptions e.g.
test and do not depend on other tests e.g.
test may reduce the frequency of test maintenance.
the prevalence of flaky tests and the developer discussions around them stand out as major costs in automated testing and hinder empirical studies of test results as it is hard to automatically distinguish between deterministic and non deterministic failures.
answer to research question .
tests need to be maintained for a variety of reasons.
in some cases such as when the tests depend on invalid assumptions or other tests the maintenance costs could be avoided via the use of better development processes.
.
rq4 how often do individual tests expose faults?
the log parsing technique described in the previous section allowed us to count the number of tests that passed and failed on each build to establish the percentage of tests that failed in failing builds.
figure 6shows the proportion of test failures from the projects and pass!fail!pass test suite executions for which we could parse the results.
from this set the average test failure rate was .
.
this is notthe global test case failure rate but the test case failure rate within the builds that had at least one test failure.
828measuring the cost of regression testing in practice a study of java projects using continuous integration esec fse september paderborn germany figure proportion of test cases that fail for the projects we were able to parse individual test results for across the failing build of tuples.
three data points .
.
and .
have been elided for clarity.
the average project failure rate was .
.
this number is helpful for understanding the potential effectiveness of test selection approaches because it establishes the absolute minimum fraction of tests a selection approach could execute.
that is if a test selection approach only executed tests from builds that would fail and then only executed the failing tests the approach would have to execute an average of .
of the test suite.
in addition we found that of failed builds contain more than one failed test.
these failures usually have the same root cause and therefore all the failed tests were not necessary to find the fault.
test selection strategies typically use coverage overlap to determine whether a test is redundant or not but this alone can be inaccurate .
by only considering a test case redundant if all past failures occurred with other tests the false positive rate could be reduced.
on the other hand if a test has failed alone it would indicate that it is not redundant even if does have complete coverage overlap with another test.
answer to research question .
in the failed builds under study only .
of test case executions failed.
of failed builds contained more than one failed test.
discussion in this section we discuss threats to the validity of our empirical study.
we then describe the replication dataset.
.
threats to validity the dataset developed for this study has several limitations most notably to external validity.
while it presents a diverse set of projects they were all java based projects that used the travis continuous integration platform.
in the process of trying to find representative projects we also filtered out projects that we believed to be unusual but whose test inducing behaviour may have been interesting.
that said the set of projects was diverse and contained test suites that were being actively maintained and executed.
in addition although we believe removing projects with few test failures removed projects where tests are run locally it is still possible that some test suite executions were not captured in the travis history.our goal was to measure how often a test suite failure indicated that test suite maintenance was necessary.
though it seems reasonable to assume that if only the test code was changed to fix a test failure the developers were maintaining the test it is possible that we misclassified some of the test failures.
however examining each change manually would have been infeasible and feel our approach is an adequate approximation.
we believe our approach to be a reasonable lower bound on test maintenance.
additionally in the code fixes category some of the code test changes could have involved test edits that were related to test maintenance made by the developer before the tests failed for example because they knew their code changes would have caused failures once the tests were executed .
in terms of internal validity these analyses may be overly conservative as we could have excluded some test changes that could have been classified as maintenance.
our results for the flaky analysis were also limited while we found that .
of the tuples we examined were flaky we examined only builds that failed when the developer originally executed them.
it is possible for builds to pass by chance and thus some of the passing builds we did not re execute could have been flaky as well.
a threat to construct validity is our use of a proxy metric for cost.
the cost of test suite maintenance can be measured in time dollars lines of code changed or various other units but we have limited ourselves to counting the number of times the suite was maintained.
this suggests that all changes to the suite are equally taxing which is naturally not the case.
however the scope of our study and the variety of projects studied makes it impossible to objectively assign an exact cost to every test failure and maintenance activity we observed.
likewise the cost of a bug can be represented by hours taken to fix the bug money lost due to the bug number of lines changed and so on deriving the true dollar value for these costs was also impractical.
.
replication package the data underlying this paper represent an oracle consisting of code changes and test failures that arose in practice.
this dataset is valuable because it augments past studies on industrial code for which the full data could not be released e.g.
.
it also provides crucial information into the dynamic outcomes of test executions that cannot be recovered from mining studies alone e.g.
.
the data includes a database image of the build results of travis builds run by projects some projects not studied in this paper are included .
build results include the build state timestamps and for builds whose logs we could parse the test identifiers of failed tests.
these data represent the largest open collection of practical test failures and will prove valuable for many future studies that want to evaluate their approaches on real data by for instance measuring how many actual failures would be missed by a test selection approach or how effective a test selection approach is relative to a known best case .
the full replication package for this study can be found and contributed to online.
829esec fse september paderborn germany a. labuschagne l. inozemtseva and r. holmes conclusion regression testing is widely used and widely studied.
despite this it is not always clear that the benefits of having fewer faults in the program are outweighed by the cost of writing maintaining and executing regression tests.
previous studies that attempted to quantify these tradeoffs were not able to measure the benefits of fault detection due to their use of repository mining.
to address this limitation we studied java based projects that use travis ci.
we found that of test suite executions fail and that of these failures are flaky.
of the non flaky failures only were caused by a bug in the system under test the remaining were due to incorrect or obsolete tests.
in addition we found that in the failed builds only .
of the test case executions failed and of failed builds contained more than one failed test.
our findings contribute to a wider understanding of the unforeseen costs that can impact the overall cost effectiveness of regression testing in practice.
they can also inform research into test case selection techniques as we have provided an approximate empirical bound on the practical value that could be extracted from such techniques.