test driven code review an empirical study davide spadini fabio palomba tobias baum stefan hanenberg magiel bruntink alberto bacchelli delft university of technology the netherlands software improvement group the netherlands university of zurich switzerland leibniz universitat hannover germany paluno university of duisburg essen germany abstract test driven code review tdr is a code review practice in which a reviewer inspects a patch by examining the changed test code before the changed production code.
although this practice has been mentioned positively by practitioners in informal literature and interviews there is no systematic knowledge of its effects prevalence problems and advantages.
in this paper we aim at empirically understanding whether this practice has an effect on code review effectiveness and how developers perceive tdr.
we conduct i a controlled experiment with developers that perform more than reviews and ii semi structured interviews and a survey with respondents to gather information on how tdr is perceived.
key results from the experiment show that developers adopting tdr find the same proportion of defects in production code but more in test code at the expenses of fewer maintainability issues in production code.
furthermore we found that most developers prefer to review production code as they deem it more critical and tests should follow from it.
moreover general poor test code quality and no tool support hinder the adoption of tdr .
public preprint https data and materials https i. i ntroduction peer code review is a well established and widely adopted practice aimed at maintaining and promoting software quality .
in a code review developers other than the code change author manually inspect a code change to find as many issues as possible and provide feedbacks that need to be addressed before accepting the code in production .
the academic research community is conducting empirical studies to better understand the code review process as well as to obtain empirical evidence on aspects and practices that are related to more efficient and effective reviews .
a code review practice that has only been touched upon in academic literature but has been described in gray literature almost ten years ago is that of test driven code review tdr henceforth .
by following tdr a reviewer inspects a patch by examining the changed test code before the changed production code.
to motivate tdr p. zembrod senior software engineer in test at google explained in the google blog when i look at new code or a code change i ask what is this about?
what is it supposed to do?
questions that tests often have a good answer for.
they expose interfaces and state use cases .
among the comments also s. freeman one of the ideators of mocks and tdd commented how he covered similar ground .
recently in a popular online forum for programmers another article supported tdr collecting morethan likes by looking at the requirements and checking them against the test cases the developer can have a pretty good understanding of what the implementation should be like what functionality it covers and if the developer omitted any use cases.
interviewed developers reported preferring to review test code first to better understanding the code change before looking for defects in production .
despite these compelling arguments in favor of tdr w e have no systematic knowledge on this practice its effectiveness in finding defects during code review its prominence in practice and what are its potential problems advantages.
this knowledge can provide insights for both practitioners and researchers.
developers and project stakeholders can use empirical evidence about tdr effects problems and advantages to make informed decisions about when to adopt it.
researchers can focus their attention on the novel aspects of tdr and challenges reviewers face to inform future research.
in this paper our goal is to obtain a deeper understanding oftdr.
we do this by conducting an empirical study set up in two phases an experiment followed by an investigation of developers practices and perceptions.
in the first phase we study the effects of tdr in terms of the proportion of defects and maintainability issues found in a review.
to this aim we devise and analyze the results of an online experiment in which developers with at least two years of professional development experience complete reviews using tdr or two alternative strategies i.e.
production first or only production .
two external developers rated the quality of the review comments.
in the second phase we investigate problems advantages and frequency of adoption of tdr valuable aspects that could not be studied in the experiment.
to this aim we conduct nine interviews with experiment participants and deploy an online survey with respondents.
key findings of our study include with tdr the proportion of functional defects bugs henceforth found in production code and maintainability issues issues henceforth found in test code does not change.
however tdr leads to the discovery of more bugs in test code at the expenses of fewer issues found in production code.
the external raters judged the quality of the review comments as comparable across all review strategies.
furthermore most developers seem to be reluctant to devote much attention to tests as they deem production code more important moreover applying tdr is problematic due to widespread poor test quality reducing tdr s applicability and no tool support not easing tdr .
ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ii.
r elated work to some extent tdr can be considered as an evolution of classical reading techniques as it shares the general idea to guide code inspectors with software artifacts i.e.
test cases and help them with the code review task.
scenario based inspections.
among reading techniques porter v otta defined the scenario based approach based on scenarios that provide inspectors with more specific instructions than a typical checklist and focus on a wider variety of defects.
they discovered that such technique is significantly more useful for requirements inspectors.
later on porter et al.
and miller et al.
replicated the original study confirming the results.
other studies by fusaro et al.
and sandahl et al.
reported contradictory results however without providing explanations on the circumstances leading scenario based code inspection to fail.
a significant advance in this field was then provided by basili et al.
who re visited the original scenario based as a technique that needs to be specialized for the specific issues to be analyzed.
they also defined a new scenariobased technique called perspective based reading the basic idea is that different aspects of the source code should be inspected by inspectors having different skills .
all in all the papers mentioned above provided evidence of the usefulness of reading techniques their similarities with tdr give an interesting rationale on why tdr could bring benefits.
ordering of code changes.
research on the ordering of code changes is also related to tdr.
in particular baum et al.
argued that an optimal ordering of code changes would help reviewers by reducing the cognitive load and improving the alignment with their cognitive processes even though they made no explicit reference to ordering tests.
this may give theoretical value to the tdr practice.
code ordering and its relation to understanding yet without explicit reference to tests or reviews has also been the subject of studies .
reviewing test code.
many articles on classical inspection e.g.
underline the importance of reviewing tests however they do not leave any specific recommendation.
the benefits of reviewing tests are also highlighted in two case studies .
already in fagan s seminal paper the inspection of tests is discussed in this case noting fewer benefits compared to the inspection of production code.
winkler et al.
experimented with writing tests during inspection and found neither large gains nor losses in efficiency and effectiveness.
elberzhager et al.
proposed to use results from code reviews to focus testing efforts.
to our knowledge in academic literature tdr has been explicitly referred to only by spadini et al.
.
in a more general investigation on how test files are reviewed the authors reported that some practitioners indeed prefer to review test code first as to get a better understanding of a code change before looking for defects in production code.
our work builds upon the research on reviewing test code by investigating how reviewing test code can not be beneficial for the whole reviewing process.iii.
m ethodology in this section we describe the research questions and the methodology we follow to conduct our study.
a. research questions this study has two parts corresponding to two research questions.
in the first part we design and run an experiment to investigate the effects of tdr on code review effectiveness.
we measure the effectiveness as the ability to find bugs and maintainability issues during a code review i.e.
the main reported goal of code review .
hence our first research question rq .does the order of presenting test code to the reviewer influence the code review s effectiveness?
more formally the goal of the experiment is to test the following null hypotheses h0pcmi for production code there is no difference in the proportion of found maintainability issues between the different review practices tfand pf .
h0pcbugs for production code there is no difference in the proportion of found bugs between the different review practices tfand pf .
h0tcmi for test code there is no difference in the proportion of found maintainability issues between the different review practices tfand pf .
h0tcbugs for test code there is no difference in the proportion of the number of bugs found between the different review practices tfand pf .
h0review time there is no difference in time required for a review between the review practices tfand pf .
h0review quality there is no difference in the review qualities between tf pf and op.
subsequently we investigate the prominence of tdr and developers perception toward this practice also focusing on problems and advantages.
to do so we conduct semistructured interviews and deploy an online survey.
hence our second research question rq .how do developers perceive the practice of testdriven code review?
b. method rq1 design overview figure depicts the flow of our experiment.
we follow a partially counter balanced repeated measures design augmented with some additional phases.
we use a browser based tool to conduct the experiment and answer rq .
the tool allows to i visualize and perform code reviews and ii collect data from demographic like questions and the interactions that participants have with the tool.
the welcome page provides information on the experiment to perform and requires informed consent.
after the welcome page an interface is shown to collect demographics as well as information about some confounding authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
valid participants participants completed both reviewswe l c o me participant!
review participants completed only the 1st review2nd review with randomized change ordering review 1st review with randomized change ordering access to the controlled software environment for the experimentdemographics confounders figure .
experiment steps flow and participation factors such as i the main role of the participant in software development ii java programming experience iii current practice in programming and reviewing and iv the hours already worked in the day of the experiment to approximate the current mental freshness.
these questions are asked with the aim of measuring real relevant and recent experience of participants as recommended by previous work .
once filled in this information the participant receives more details on the reviews to be performed.
each participant is then asked to perform two reviews the first is mandatory the second is optional randomly selected from the following three treatments1that correspond to the tdr practice and its two opposite strategies tf test first the participant must review the changes in both test code and production code and is shown the changed test code first.
pf production first the participant must review both production and test and is shown the production code first.
op only production the participant is shown and must review only the changes to the production code.
for the treatments tfand pf the tool shows a toggle shown code button that allows the participant to see and review the other part of the change e.g.
the production code 1we propose two of the three treatments to keep the experiment as short as possible thus stimulating a higher response rate as also recommended by flanigan et al.
.
this choice does not influence our observations as the random selection balances the treatments see table iv .
figure .
example of the review view in the browser based experiment ui showing the code change.
in this case the treatment is pf thus to see the test code the participant must click on the toggle shown code button.
change if the treatment is tf .
we do not limit the number of times the toggle shown code button can be clicked thus allowing the participant to go back and forth.
the participant can annotate review remarks directly from the gui of our tool.
before submitting the experiment we ask the participants if they would like to be further contacted for a semi structured interview if so they fill in a text field with their email address.
further comments impressions on the study can be reported using a text block.
c. method rq1 browser based experiment platform as previously mentioned we adapt and use a browserbased experiment platform to run the experiment.
this has two main advantages on the one hand participants can conveniently perform code reviews on the other hand the tool assists us when gathering data from the demographic questions conducting the different treatments and collecting information for the analysis of co factors.
to reduce the risk of data loss and corruption almost no data processing is done on the server instead participants data is recorded as log records and analyzed offline.
the tool implements a gui similar to other browser based code review tools e.g.
githubpull requests it presents a code change in the form of two pane diffs.
an example of the implemented gui is reported in figure review remarks can be added and changed by clicking on the margin beneath the code.
the tool logs many user interactions such as mouse clicks and pressed keys which we use to ensure that the participants are actively performing the tasks.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
d. method rq1 objects the objects of the study are represented by the code changes or patch for brevity to review which need to be properly selected and eventually modified to have a sufficient number of defects to be discovered by participants.
patches.
to avoid giving some developers an advantage we use a code base that is not known to all the participants of the experiment.
this increases the difficulty of performing the reviews.
to keep the task manageable we ensure that i the functional domain and requirements are well known to the participants and ii there is little reliance on special technologies or libraries.
to satisfy these goals we select an open source project jpacman framework the project consists of production classes 2k loc as well as test classes loc it received pull requests in his history and has a total of contributors.
to select suitable patches we screen the commits of the project and manually select changes that are self contained involve both test and production code are neither too complicated nor too trivial and have a minimum quality e.g.
not containing dubious changes.
the selected patches are those of commits 6d7c14d and 698ac7d .
in the first one a new feature is added along with the tests that cover it.
in the second one a refactoring in the production code is applied and a new test is added.
seeding of bugs and maintainability issues.
code review is employed by many software development teams to reach different goals but mainly detecting bugs functional defects and improving code quality e.g.
finding maintainability issues spreading knowledge .
since the online experiment is done by single developers we measure code review effectiveness by considering only the first two points detecting bugs functional defects and maintainability issues.
to this aim we seed in the code bugs and maintainability issues.
examples of injected bugs are a wrong copy paste and wrong boundary checking.
for maintainability issues we mainly mean smells that do not fail the tests e.g.
a function that does more than what it was supposed to do wrong documentation or variable naming.
concerning the nature of faults one bug was real and identified some commits later.
the other six were manually injected based on standard errors such as handling corner cases and null pointer exceptions.
in the end the two patches contain a total of bugs and issues patch and bugs and issues patch .
the total number of bugs per file and is higher than in the realworld code indeed in our experiment we opted for a higher density of errors to ensure an attainable number of participants and reduce confounding factors in the experiment.
the reason is that if the number of errors in the code is too low the statistical power decreases and many more participants are needed to measure an effect.
moreover if subjects must read too much correct code the effects such as reading speed become stronger confounding factors.table i variables used in the statistical model metric description dependent v ariables prodbugspropproportion of functional defects found in the production code prodmaintissuespropproportion of maintainability issues found in the production code testbugspropproportion of functional defects found in the testcode testmaintissuespropproportion of maintainability issues found in the testcode independent v ariable treatment type of the treatment tf pf or op control v ariables review details totalduration time spent in reviewing the code isfirstreviewboolean representing whether the review is the first or the second patch p a t c h1o r2 profile role role of the participant reviewpractice how often they perform code review programpractice how often they program profdevexpyears of experience as professional developer javaexp years of experience i nj a v a workedhourshours the participant worked before performing the experiment see table iii for the scale e. method rq1 v ariables and analysis we investigate whether the proportion of defects found in a review is influenced by the review being done under a tf pf o r optreatment controlling for other characteristics.
the first author of this paper manually analyzed all the remarks added by the participants.
our tool explicitly asked and continuously highlighted to the participants that the primary goal is to find both bugs and maintainability issues therefore each participant s remark is classified as identifying either a bug or an issue or as being outside of the study s scope.
a remark is counted only if in the right position and correctly pinpointing the problem.
by employing values at defect level we could compute the dependent variables at review level namely proportions given by the ratio between the number of defects found and the total number of defects in the code dependent vars in table i .
the dependent variables are then given by the average of njbinary variables yi assuming a value 1if the defect is found and if not where njis the total number of defects present in the changej so that the proportion jresults from njindependent events of defect finding and yjare binary variables that can be modeled through a logistic regression.
j nj summationdisplay 1yj nj 2to validate this coding process a second author independently re coded the remarks and compared his classification with the original one.
in case of disagreements the two authors opened a discussion and reached a consensus.
we compute the cohen s kappa coefficient to measure the inter rater agreement between the two authors before discussion we find it to reach .
considerably higher than the recommended threshold of .
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the main independent variable of our experiment is the review strategy or treatment .
we consider the other variables as control variables which include the time spent on the review the review and programming practice the participant s role the reviewed patch i.e.
p1 or p2 and whether the review is the first or the second being performed.
in fact previous research suggests the presence of a trade off between speed and quality in code reviews following this line we expect longer reviews to find more defects to check this we do not fix any time for review allowing participants to perform the task as long as needed.
moreover it is reasonable to assume that participants who perform reviews more frequently to also find a higher share of defects.
we run logistic regressions of proportions where logit j represents the explained proportion of found defects in review j 0represents the log odds of being a defect found for a review adopting pf or op and of mean totalduration isfirstreview etc.
while parameters 1 treatment j 2 totalduration j 3 isfirstreview j 4 patch j etc.
represent the differentials in the log odds of being a defect found for a change reviewed with tf for a review with characteristics totalduration j mean isfirstreview j mean patch j mean etc.. logit j 0 1 treatment j 2 totalduration j 3 isfirstreview j 4 patch j ... other vars and omitted f .
method rq2 data collection and analysis while through the experiment we are able to collect data on the effectiveness of tdr we cannot collect the perception of the developers on the prevalence of tdr as well as the motivations for applying it or not.
hence to answer rq we proceed with two parallel analyses we i perform semistructured interviews with participants of the experiment who are available to further discuss on tdr and ii run an online survey with the aim of receiving opinions from the broader audience of developers external to the experiment.
semi structured interviews.
we design an interview whose goal is to collect developers points of view on tdr.
they are conducted by the first author of this paper and are semistructured a form of interview often used in exploratory investigations to understand phenomena and seek new insights on the problem of interest .
each interview starts with general questions about code reviews with the aim of understanding why the interviewee performs code reviews whether they consider it an important practice and how they perform them.
then we ask participants what are the main steps they take when reviewing starting from reading the commit message to the final decision of merging rejecting a patch focusing especially on the order of reviewing files.
up to this point the interviewees are not aware of the main goal of the experiment they participated in and our study we do not reveal them to mitigate biases in their responses.
after these general questions we revealthe goal of the experiment and we ask their personal opinions regarding tdr.
the interview protocol is available .
during each interview the researcher summarizes the answers and before finalizing the meeting these summaries are presented to the interviewee to validate our interpretation of their opinions.
we conduct all interviews via s kype .
with the participants consent the interviews are recorded and transcribed.
later we analyze the interviews applying a grounded theory approach we use descriptive coding as first cycle coding and pattern coding as second cycle.
we first summarize in a short phrase the essential topic of each passage from the interviews then we identify explanatory codes to create emergent themes that we discussed among the authors.
overall we conduct nine minute interviews table ii summarizes the demographics of the participants.
table ii interviewees experience in years and working context id developer reviewer working context applying tdr p1 oss almost never p2 company a almost always p3 company a almost always p4 company b almost never p5 company c always p6 company d sometimes p7 company e always p8 company f oss sometimes p9 company g oss never online survey.
we create an anonymous minute online survey with two sections.
in the first one we ask demographic information of the participants including gender programming reviewing experience policies regarding code reviews in their team e.g.
if all changes are subject of review or just a part of them and whether they actually review test files the respondents who answer no are disqualified .
in the second section we ask respondents i how often they start reviewing from tests vs. production files and ii to fill out a text box explaining the reasons why they start from test production files.
the questionnaire is created using a professional tool and is spread out through practitioners blogs e.g.
reddit and through direct contacts in the professional network of the study authors as well as the authors social media accounts on twitter and facebook.
furthermore we neither revealed the aim of the experiment nor provided incentives to participate.
we collected valid answers which complement the semi structured interviews.
among the respondents have one year or less of development experience have to years have to years and more than years.
iv .
r esults rq ontheeffects oft d r a total of people accessed our experiment environment following the provided link.
from their reviews if any we exclude all the instances in which the code change is skipped or skimmed by demanding either at least one entered remark or more than minutes spent on the review.
we also remove an outlier review that lasted more than standard deviations from the mean review time without entering any comments.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii participants characteristics descriptive statistics n. dev student resear cher architect analyst other curr ent role61 experience years withnone java prog.
profess.
dev.
curr ent frequency ofnever yearly monthly weekly daily programming reviewing table iv distribution of participants reviews across treatments tf pf op total patch1 patch2 total after applying the exclusion criteria a total of participants stay for the subsequent analyses.
table iii presents what the participants reported in terms of role experience and practice.
only of the participants reported to have no experience in professional software development most program daily and review code at least weekly .
table iv shows how the participants reviews are distributed across the considered treatments and patches.
despite some participants completed only one review and the aforementioned exclusions the automated assignment algorithm allowed us to obtain a rather balanced number of reviews per treatment and by patch.
a. experiment results table v shows the average values achieved by the reviews for the dependent variables e.g.
prodbugsprop and the average review time by treatment.
the most evident differences between the treatments are in d1 the proportion of maintainability issues found in production code pfand op have an average of .
and .
respectively while tfof .
and d2 the proportion of bugs found in test code tfhas an average of .
while pfof .
.
by applying a wilcoxon signed rank test we tested and rejected h0 pcmi p .
and rejected h0 tcbugs p .
.3on the contrary based on the same test we accept h0 pcbugs and h0 tcmi the minimum pis higher than .
.4applying again a wilcoxon signed rank test rejects h0review time p .
05in all cases .
overall the comparison of the averages highlights that within the same time developers who started reviewing from tests tf spot a similar number of bugs in production while discovering more defects in the tests but fewer maintainability issues in the production code.
thus there seems to be a compromise between reviewing test or production code first when considering defects and maintainability issues.
with regression modeling we investigate whether these differences are confirmed when taking into account the char3in the first case cliff s delta is medium in the latter case it is small 4it is arguable whether there is a need to apply a bonferroni correction because we tested the variable tc more than once.
but even applying a correction necessarily leads to a p still .05table v average proportion of bugs and issues found by treatment and review time .colored columns indicate a statistically significant difference between the treatments p .
with the color intensity indicating the direction .
proportion of found production test bugs maintissues bugs maintissuestime tf .
.
.
.
7m11s pf .
.
.
.
6m27s op .
.
5m29s table vi regressions for prodmaint issues prop and testbugsprop testbugsprop prodmaintissuesprop estimate s.e.
sig.
estimate s.e.
sig.
intercept .
.
.
.
totalduration .
.
.
.
.
isfirstre view true .
.
.
.
.
patch p2 .
.
.
.
treatment pf .
.
treatment tf .
.
.
.
reviewpractice .
.
.
.
.
programpractice .
.
.
.
profde vexp .
.
.
.
javaexp .
.
.
.
workedhours .
.
.
.
... significance codes p .
p .
p .
.
p .
role is not significant and omitted for space reason acteristics of participants and reviews variables in table i .
in section vi we describe the steps we take to verify that the selected regression model is appropriate for the available data.
we build the four models corresponding to the four dependent variables independently.
confirming the results shown in table v the treatment is statistically significant exclusively for the models with prodmaintissuesprop and testbugsprop as dependent variables table vi reports the results.5we observe that also considering the other factors tfis confirmed a statistically significant variable in both prodmaintissuesprop and testbugsprop with negative and positive directions respectively.
to calculate the odds of being a maintainability issue found in a review with tfcompared to the baseline op we exponentiate the differential logit thus exp .
.
which means fewer chances to find the issue in case oftfthan op or pf .
instead the odds of being a test bug found in a review with tfis .
thus almost more chances to find the test bug than with pf.
also we see that the specific patch under review has a very strong significance for both models thus confirming that differences in the code are an essential factor in the final outcome.
the review time plays a significant role for testbugsprop and to a lesser degree for prodmaintissuesprop in the expected direction.
unexpectedly the professional experience plays a negative role for prodmaintissuesprop we hypothesize that professionals focus more on functional defects than maintainability issues.
5for space reasons we omit the other two models in which no variable is significant but these models are available in our online appendix .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
finding .
in reviews in which tests are presented first participants found significantly more bugs in test code yet fewer maintainability issues in production code.
the production bugs and test maintainability issues found is stable across the treatments.
b. assessing code review quality after having found differences in bugs issues found with tf we check whether the different treatments tf pf op influence the quality of the review i.e.
we intend to test h0review quality .
to this aim two external validators manually classify each review rating each comment.
these validators have more than years of industrial experience in code review and have collaborated in many open source projects.
we request them to go through each of the code reviews done by the developers involved in the experiment and rate each comment aimed at fixing a defect or maintainability issue in the production or test code using a likert scale ranging between very poor comment and very useful comment .
they also give a score to each comment that is outside the scope i.e.
a comment that does not fix any of our manually injected defects plus a final overall score for the review.
each validator performs the task independently and then their assessments are sent back to the authors of this paper.
the validators are unaware of the treatment used in each review.
a set of reviews is classified by both authors so that we can measure their inter rater agreement using cohen s kappa .
to mitigate the personal variability of the raters we cluster their ratings into three categories below average review comment average review comment and good review comment .
then we check the raters agreement.
the result shows that in all ratings there is at least a substantial agreement between the validators production issues .
test issues .
production bugs test bugs .
.
to test h0 review quality we apply an anov a on the independent variables score e.g.
productivity issues and the dependent variable review practice for each independent variable in separation .
we find that for no score the independent variable is a significant factor productivity issues p .
test issuesp .
productivity bugs p .
test bugs p .
.
hence we accept h0 review quality and conclude that raters do not see a difference in the quality of the reviews across treatments.
finding .
there is no statistically significant difference between the quality of the reviews made under the three considered treatments according to two external raters blinded to the underlying treatments.
v. r esults rq ontheperception oft d r we interview some of the participants of the experiment on what they perceive as advantages and disadvantages of tdr.
we also survey developers to enrich the data collection with people that do not participate in the experiment and canprovide a complementary view on tdr.
in this section we report the answers obtained during our interviews and surveys.
we summarize them in topics covering both advantages and disadvantages of this practice.
we refer to the interviewees by their id shown in table ii.
a. adoption of tdr in practice the majority of the respondents applies tdr only occasionally reported that they always start from test code almost always occasionally sometimes almost never never.
b. perceived problems with tdr when analyzing data coming from surveys and interviews we discover a set of blocking points for the adoption of tdr.
they can be grouped around four main themes i.e.
perceived importance of tests knowledge gained by starting reviewing from tests test code quality and code review tool ordering that we further discuss in the following.
themes are discussed based on the frequency of appearance in the survey.
tests are perceived as less important.
from the comments left by the participants of the survey it seems clear that test code is considered much less important than production code and that as stated by one participant they want to see the real change first .
this strong opinion is confirmed by other participants 6for example a participant explains that s he starts looking at production files in order to understand what is being changed about the behavior of the system.
views tests as checks that the system is behaving correctly so it does not seem possible to evaluate the quality of the tests until i have a clear understanding of the behavior .
while the semistructured interviews confirmed this general perception around tests they also add a more practical point to the discussion p1 7state that they need to prioritize tasks because of time and often higher priority is given to the production code.
a closely related factor contributing to this aspect is the tiredness associated with reviewing code for a long time.
as reported by one of the survey participants the longer you are reviewing the more sloppy you get .
i would rather have a carefully reviewed production file with sloppy test than vice versa .
in other words when performing multiple code review at once developers often prefer to pay more attention to the production code than test files.
participants also report that is the production code to drive tests rather than the opposite.
a clear example of this concept is enclosed in the following participant s quote to me tests are about checking that a piece of software is behaving correctly so it doesn t make sense to me to try to understand if the tests are testing the right conditions if i do not understand what the code is supposed to do first.
finally another aspect influencing the perception that developers have of tests is the lack of testing experience.
one of the participants affirms that not everyone in my team has lots of experience 6it should be noted that in the survey we gave the possibility to leave open comments having or more participants agreeing on exactly the same theme indicates a noteworthy trend.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
with testing so usually just looking at the production code will tell me if there will be problems with the tests.
thus having poor experience in testing practice might bias the perception of the advantages given by tests in the context of code review.
tests give less knowledge on the production behavior.
both interviewees and survey respondents report that the main advantage of starting with production code is that they can immediately see the feature and how it is changed and only later they will check if it is properly tested.
for example a survey respondent says i want to understand what the production code does form my own opinion of what should be tested and then look at the tests afterward.
from the interviews p 9also adds that it is hard to capture all the possible behaviors with tests while looking to production code first helps him her figuring out the failure modes before seeing what the tests the developer proposed are.
nevertheless an interesting trend emerges from our results.
despite most developers claim that tests cannot give enough knowledge to review a change six of them declare that the decision of start reviewing from test code basically depends on the degree of knowledge they have of the production code under test.
as explained by a survey participant i fia m familiar with the topic and the code i will start with the production files.
otherwise i will choose the test files .i n other words tests only seem to be useful in time of need i.e.
when a developer does not have any other instrument to figure out the context of the proposed code change.
tests have low code quality.
participants mention poor test code quality as a reason to not apply tdr in practice.
the use of tests in a code review has the prerequisite that such tests can properly exercise the behavior of the production code.
one of the participants when explaining why s he prefers starting from production code reports that sometimes the tests are bad and it is easier to understand how the code behave by looking at the feature code .
this is also confirmed by the semi structured interviews the main obstacle when reviewing tests first is the assumption that the test code is well written .
both p 1and p 9said that most of the times they prefer to start from production because they assume developers do not write good tests.
p 1says usually i start from production and maybe the reason is that many of the projects where i worked do not have that many tests .
even if the tests are present sometimes reviewers find them difficult to understand according to p .
.
.
the test needs to be written in a clear way to actually understand what s going on.
the solution to this problem according to our interviewees is to impose test rules e.g.
all the changes to the production code should be accompanied by tests .
p5says that if are not good i will ask to modify them.
if they are not even present i will ask to add them and only after i will review the patch .
code review tool ordering.
the final disadvantage is related to a practical problem current code review tools present code changes following an alphabetic order meaning that most of the times developers review following such order.
this ishighlighted by survey participants and confirmed by the interviewees.
for example p 3says if there is a front end we do not have integration tests for all the features so sometimes i do manual testing.
in this case i would stick with the order of gitlab.
i think gitlab present tests later than production so i generally start from that .
interestingly this point came up also from of the survey respondents who do not apply tdr that indicated they start from production code because they simply follow the order given by the code review tool.
according to interviewees p 3and p they follow the order of github because in this way they are sure to have reviewed all the files in the patch while going back and forth from file to file may result in skipping some files.
finding .
perceived problems with tdr developers report to consider tests as less important than production not being able to extract enough knowledge from tests not being able to start a review from tests of poor quality and being comfortably used to read the patch as presented by their code review tool.
c. perceived advantages of tdr we identify two main themes representing the major perceived advantages of adopting tdr i.e.
the concise high level overview tests give on the functionalities of production code and the ability of naturally improving test code quality.
we discuss those aspects based on the frequency in the survey.
tdr provides a black box view of production code.
of the respondents explain that the main advantage they envision from the application of tdr is the ability of tests to provide a concise high level overview of the functionalities implemented in the production code.
in particular one participant reports if i read the test first without looking at the production implementation i can be sure that the test describes the interface clearly enough that it can serve as documentation.
moreover developers appreciate that few lines of test code allow them to contextualize the change.
most interviewees agreed that starting reviewing tests allows them to understand better the code change context .
p3says i think starting from tests helps you in understanding the context first the design the what are we building here?
before actually looking at how they implemented it .
the common feeling between the interviewees is that tests better explain what the code is supposed to do while the production code shows how the developer implemented it .
p says when you are reviewing complex algorithms it is nice to immediately see what type of outputs it produces in response to specific types of input.
tdr improves test code quality.
three of the survey participants explicitly report that tests must be of good quality and a practice like tdr would enable a continuous test code quality improvement as one put it tests are often the best documentation for how the production code is expected to function.
getting tests right first also contributes to good tdd authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
practices and the architectural values that come with that .
in other words the developers report that in situations where reviewers inspect tests first the improvement of test code quality have to necessarily happen otherwise reviewers could not properly use tests as documentation to spot problems in production code.
as a natural consequence tdr would also enforce tests to be updated thus producing overall benefits to the system reliability.
furthermore one participant reports tdr to be efficient because it captures and should capture all the bugs e v e n if we obtain this kind of feedback by a small number of developers it is still interesting to remark how some of them perceive the potential benefits of tdr which we empirically found rq as being more effective in terms of test bugs discovered while spotting the same proportion of production bugs.
the semi structured interviews confirm all the aspects discussed so far.
most of the interviewees agreed that tdr somehow helps reviewers to be more focused on testing.
according to p i think it would encourage the development of good tests and i think better tests mean more bugs captured.
so yes in the end you may capture more bugs .p6and p also say that when reviewing the production code the reviewer already knows what the code is tested for so it could be easier to catch not covered paths.
for example p 3refers to happy vs.bad paths starting from the tests you think a little bit on the cases that apply so for example if they only test the happy path and not the bad path .
finding .
perceived advantages of tdr developers report that tdr allows them to have a concise high level overview of the code under test and helps them in being more testing oriented hence improving the overall test code quality.
vi.
t hreats to v alidity construct validity.
threats to construct validity concern our research instruments.
most constructs we use are defined in previous publications and we reuse existing instruments as much as possible the tool employed for the online experiment is based on a similar tool used in an earlier work .
to avoid problems with the experimental materials we employed a multi stage process after tests among the authors we performed three pre tests with external participants and only afterward we started the release phase.
one of the central measures in our study is the number of defects and maintainability issues found.
the first author seeded the defects and later checked by the other authors.
nevertheless we cannot rule out implicit bias in seeding the defects as well as in selecting the code changes.
we asked the participants to review test and production code separately using a toggle shown code button to switch between them.
however we cannot ensure that all the participants used this button correctly or used it at all .
to mitigate this we analyzed the results mining only the showncode e.g.
the second part of the review would not exist obtaining very similar results hence we can conclude that this threat is not affecting the final results.
a major threat is that the artificial experiment created by us could differ from a real world scenario.
we mitigated this issue in multiple ways we used real changes we reminded the participant to review the files as they would typically do in their daily life and we used an interface very similar to the common code review tools guis.
furthermore to validate the reviews done by the participants we involved two external validators.
the only information they had at their disposal when rating the reviews was the patch and the comments of the participants i.e.
they did not know what treatment it was the duration of the review and all the other information we collected.
thus the rate given by the validators was based on their personal judge and past experience in code review.
to mitigate this issue we involved two validators that have strong experience in java and mcr one had worked for many years in a large russian based se company and the other validator holds a ph.d. in se and has worked in many oss.
internal validity credibility.
threats to internal validity concern factors we did not consider that could affect the variables and the relations being investigated.
in an online setting a possible threat is the missing control over participants which is amplified by their full anonymity.
to mitigate this threat we included questions to characterize our sample e.g.
experience role screen size .
to identify and exclude duplicate participation we logged hashes of participant s local and remote ip addresses and set cookies in the browser.
furthermore to exclude participants who did not take the experiment seriously we excluded experiments without any comments in the review and manually classified the comments to delete the inappropriate ones.
we do not know the nature of the population that did our experiment hence it might suffer from self selection bias.
indeed it could be possible that the sample contains better and more motivated reviewers than the population of all software developers.
however we do not believe this poses a significant risk to the validity of our main findings since we would expect stronger effects with a more representative sample.
furthermore as depicted in table iv the participants experience is quite various with a lower than years and with more than .
external validity transferability.
threats to external validity concern the generalization of results.
a sample of professional software developers is quite large in comparison to many experiments in software engineering .
however it is still small compared to the overall population of software developers that employ mcr.
we reduce this issue by interviewing and collecting the opinions of other developers who did not participate in the experiment.
statistical conclusion validity.
a failure to reach statistically significant results is problematic because it can have multiple causes e.g.
a non existent or too small effect or a too small authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
sample size.
even though we reached a quite large sample of participants our sample is not large enough to detect smaller effects for rq .
a major threat to our rq 1results is to employ the wrong statistical model.
to ensure that the selected logistic regression model is appropriate for the available data we first compute the variance inflation factors vif as a standard test for multicollinearity finding all the values to be below .
values should be below thus indicating little or no multicollinearity among the independent variables run a multilevel regression model to check whether there is a significant variance among reviewers but we found little to none thus indicating that a single level regression model is appropriate ascertain the linearity assumed by logistic regression of our independent continuous variable the review time and log odds using the box tidwell test and build the models by adding the independent variables stepby step and found that the coefficients remained stable thus further indicating little to no interference among the variables.
finally in our statistical model we control for the type of the patch namely patch or .
however we do not control for product or process metrics of the code i.e.
size complexity churn etc.
we control only for the patch as it encloses all the characteristics that previous literature already demonstrated as related to review effectiveness .
vii.
d iscussion and implications our findings provide two key observations to be further discussed and that lead to implications for practitioners educators tool vendors and research community.
ordering of files within the code review.
interviewees and survey respondents indicated that they often review the files as presented by their code review tool.
while this process has the advantage that at the end a reviewer is sure to have reviewed all files it may be problematic.
for instance our experiment rq showed that simply presenting test first allows a reviewer to capture more test bugs which have been shown to be extremely harmful to the overall reliability of software systems .
at the same time tdr still allows catching the same amount of bugs in production code thus being nearly equivalent to the case of reviewing production files first.
however the drawback consists of finding fewer issues in production.
a study could be designed and conducted to verify whether and to what extent using static analyzers could help to mitigate this drawback as they can spot several maintainability issues automatically .
furthermore this finding can inform both next generation review tools makers which could base the order of files within the code review on the context or allow the reviewers to make this choice.
we also found that developers decide on whether to start reviewing from test or production based on different factors such as familiarity with the code or type of modification applied.
this suggests that to improve productivity and code review performance tool vendors might enable the option to let developers decide on the code review ordering.
at the same time the research community is called to the definitionof novel techniques that can exploit a set of metrics e.g.
change type or past modifications of the developer on the code under review to automatically recommend the order that would allow the reviewer to be more effective this would lead to new adaptive mechanisms that take into account developerrelated factors to improve the reviewability of code .
test code quality.
a critical enemy of tdr seems to be the poor quality of test code .
many interviewees and survey respondents indicated this as the main reasons tonotapply tdr.
if tests are poorly written or incomplete it becomes almost impossible or even dangerous to start reviewing from test code as it is harder to spot errors in production code.
however this would create a vicious cycle in which tests are reviewed less and less carefully thus gradually losing quality.
furthermore we believe that the programming language can potentially influence tdr in fact the availability of testing frameworks e.g.
junit affects this practice and the readability of tests which may also depend on the framework and language can also do it.
a possible solution consists of enforcing the introduction of code of conducts that explicitly indicate rules on how to review tests .
the development of a good team culture in which test code is considered as important as production code should be a must for educators.
indeed as previous work already pointed out good reviewing effectiveness is found mostly within teams that value the time spent on code review hence practitioners should set aside sufficient time for reviewing all the files present in the patch including test code.
viii.
c onclusion in this paper we empirically investigated a code review practice mentioned among practitioners blogs and only touched upon in academia test driven code review.
we performed a study set up in two phases an experiment with developers and two external raters followed by a qualitative investigation with nine semi structured interviews and a survey with respondents.
among our results we found that with tdr the quality of the review comments does not change and neither does the time spent and the proportion of found production bugs and test issues.
tdr leads to the discovery of more bugs in test code at the expenses of fewer maintainability issues found in production.
we report that developers see the application of tdr as problematic because of the perceived low importance of reviewing tests poor test quality and no tool support.
however test first review is also deemed to offer a concise high level overview of a patch that is considered helpful for developers not familiar with the changed production code.
ix.
a cknowledgment this project has received funding from the european union s h2020 programme under the marie sklodowska curie grant agreement no .
a. bacchelli and f. palomba gratefully acknowledge the support of the swiss national science foundation through the snf project no.
pp00p2 .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.