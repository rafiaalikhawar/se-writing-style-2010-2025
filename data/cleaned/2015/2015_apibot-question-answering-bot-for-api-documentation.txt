apibot question answering bot for api documentation yuan tian ferdian thung abhishek sharma and david lo school of information systems singapore management university singapore fyuan.tian.
ferdiant.
abhisheksh.
davidlog smu.edu.sg abstract as the carrier of application programming interfaces apis knowledge api documentation plays a crucial role in how developers learn and use an api.
it is also a valuable information resource for answering api related questions especially when developers cannot find reliable answers to their questions online offline.
however finding answers to api related questions from api documentation might not be easy because one may have to manually go through multiple pages before reaching the relevant page and then read and understand the information inside the relevant page to figure out the answers.
to deal with this challenge we develop apibot a bot that can answer api questions given api documentation as an input.
apibot is built on top of siriusqa the qa system from sirius a state of the art intelligent personal assistant.
to make siriusqa work well under software engineering scenario we make several modifications over siriusqa by injecting domain specific knowledge.
we evaluate apibot on api questions answers of which are known to be present in java documentation.
our experiment shows that apibot can achieve a hit score of .
.
index terms api documentation question answering bot i. i ntroduction when developing applications using application programming interfaces apis the official api documentation is one of the excellent sources for finding answers to api related questions.
unfortunately to find a desired piece of information developers may need to sift through numerous pages in documentation which is a tedious and time consuming activity.
the process is even worse for apis having a sharp learning curve .
as a consequence developers often do not read api documentation .
this may cause developers to use apis incorrectly resulting in bugs and even security vulnerabilities .
finding answers in api documentation can be made much simpler through a question answering bot.
the bot can simulate an expert answering developer queries directly and reducing the need of developers to browse multiple documents to find answers.
such a bot can be particularly helpful for new or closed apis which have few available experts.
a bot can automatically learn from documentation of these new or closed apis and help developers with their queries.
there are recent interests in creating bots for software engineering purposes.
murgia et al.
created joey a question answering bot for stackoverflow that is able to answer questions that are asked before .
storey and zagalsky visioned the first authors have contributed equally to the work.the use of bots to automate tasks in software development .
in this paper we build a bot that is able to answer apirelated questions by analyzing api documentation.
different from joey our approach is able to answer questions regardless whether it has been asked before or not.
recently hauswald et al.
developed an open end to end personal assistant that they named sirius .
sirius makes use of existing state of the art technologies such as pocketsphinx kaldi and rwth s rasr for speech recognition openephyra for question answering and surf for image detection.
siriusqa i.e.
the qa system inside sirius works by relying on a set of question and answer patterns.
question patterns are used to extract important phrases from an input question while answer patterns are used to rank candidate answers extracted from a text corpus.
siriusqa has been shown to work well with general knowledge questions.
however it is unknown whether siriusqa would still work for domain specific questions like api related questions due to its use of general question and answer patterns.
these patterns do not capture api domain knowledge.
faced with siriusqa s inherent limitations for the api domain we are motivated to build apibot a specific question answering bot for api documentation.
we address limitations of siriusqa to make it work well for api documentation.
first we naturalize api documentation to make hidden structural information appear in the form of natural language sentences.
second we create api specific question patterns to make siriusqa understand important parts of api questions and categorize them.
third we modify siriusqa to consider apispecific answer patterns and terms.
finally we learn answer patterns by building a probabilistic model for each answer category.
these answer patterns require a smaller training corpus than siriusqa which needs to learn answer patterns in form of regular expressions from a large set of questionanswer pairs.
to evaluate the performance of apibot we collect a benchmark by first inviting a group of people to ask questions about jdk api documentation.
we then invite another group of people to provide the ground truth answers for the collected questions.
our experimental result shows that apibot can achieve a hit score of .
on the evaluation dataset.
the rest of this paper is structured as follows.
section ii describes background knowledge of siriusqa and its limitations when applied on api documentation.
section iii introduces the design of apibot.
section iv presents our evaluation .
c ieeease urbana champaign il usa technical research new ideas153 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
methodology and results.
section v describes related studies.
section vi gathers the conclusions derived from this work and presents future work.
ii.
s irius qa and itslimitations a. overview of siriusqa sirius is the leading state of the art open source intelligent personal assistant ipa system.
it consists of three major components automatic speech recognition asr questionanswering qa and image matching imm .
in this work we enhance the question answering component of sirius henceforth referred to as siriusqa.
siriusqa is based on the open source openephyra system .
the system incorporates a number of nlp algorithms to effectively answer natural language questions.
siriusqa consists of three major components question interpretation document search and answer selection.
we describe them briefly below question interpretation.
inquestion interpretation component siriusqa identifies the target i.e.
the entity that a question asked about contexts i.e.
the qualifying attributes of the target and category of a question based on a pre defined set of manually constructed question patterns.
a question pattern is a regular expression with placeholders to identify target and context phrases from questions.
these patterns are grouped into different categories.
for example consider the following question what medal has joseph schooling won in olympics?
.
given the following question pattern what medal has htargetiwon in hcontexti?
we can find that the target is joseph schooling and the context is olympics .
document search.
indocument search component relevant documents matching a query are returned.
the query is generated from the target context and keywords extracted by thequestion interpretation component.
answer selection.
inanswer selection component candidate answers are identified from returned documents by matching series of consecutive words in the documents with answer patterns.
the answer patterns are again in the form of regular expressions with placeholders to identify specific text that is to be returned as an answer.
a confidence score is assigned to each answer pattern and this score is automatically inferred based on a training set of question answer pairs.
b. limitations of siriusqa siriusqa has been demonstrated to work well in answering general questions by analyzing a corpus of textual articles.
however it is unlikely to work well in answering apispecific questions by analyzing api documentations due to the following limitations.
limitation siriusqa only handles unstructured textual documents.
api documentation on the other hand is a structured document.
for example in an api documentation of a library written in an object oriented language e.g.
a javadoc page it would have pages explaining about classes.
throughout the page there would also be links to other pages in theapi documentation representing inheritance or other kinds of relationships.
unfortunately such structural information is fully ignored in siriusqa.
limitation siriusqa uses a set of manually crafted question patterns which are designed for general knowledge questions rather than api specific questions.
an example of general knowledge question that siriusqa can handle is what is the name of the actor who star in titanic?
.
this question is far from questions developers might ask about an api.
in addition each question pattern is assigned to a category in siriusqa such as author capital etc.
however none of the categories are related to the type of knowledge that are commonly contained in api documentation.
limitation answer pattern learning in siriusqa requires a large amount of manually created training data.
however manually creating a large amount of api specific questionanswer pairs is difficult and time consuming.
stackoverflow records many questions including those that are related to apis and their corresponding answers.
however converting stackoverflow data into something that siriusqa can handle requires expensive manual cleaning step.
thus a strategy is needed to learn answer patterns from small amount of data.
limitation siriusqa does not have any knowledge about software terms e.g.
it does not know that arraylist is a class in java .
it thus cannot differentiate software specific words from other words and cannot use domain specific heuristics to return better answers.
iii.
p lugging insoftware knowledge to sirius qa a. overview we have built our system apibot on top of siriusqa by modifying and enhancing some of its components.
the overall framework of apibot is shown in figure with boxes shaded in grey representing our new or enhanced components.
it consists of two major parts domain adaptation and enhanced siriusqa.
a brief overview of each part is given below.
domain adaptation.
the domain adaptation component enables apibot to understand software engineering questions and produce appropriate answers through a one time training process.
at the end of the process this component produces outputs i.e.
enhanced api documentation question patterns andanswer patterns.
the enhanced api documentation consists of natural language sentences which describe all the structured and unstructured information present in the original api documentation.
an enhanced api documentation enables us to take into consideration the structure of an api documentation and hence addresses limitation .
question patterns which consist of regular expressions with placeholders to identify target and context words from questions are another output.
these expressions are grouped into different categories capturing different kinds of knowledge stored in an api documentation .
our question patterns which are grouped into these domain specific categories address limitation .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the domain adaptation component also outputs a probabilistic model for each category of questions.
each model is able to assign a probability to a candidate answer sentence in the enhanced api documentation based on its likelihood to be an answer to a target question of a given category.
these models constitute the third output i.e.
answer patterns.
our answer patterns are different from those of siriusqa rather than being regular expressions they are probabilistic models.
both siriusqa and apibot automatically learn answer patterns from a set of question answer pairs.
we choose to learn a probabilistic model over regular expressions since the latter tends to overfit especially when we only have a small set of training data.
our probabilistic answer patterns thus addresses limitation .
enhanced siriusqa.
all of the three outputs produced are then plugged in to an enhanced siriusqa.
we enhance siriusqa by modifying its answer selection component to be able to use the probabilistic answer patterns generated by the domain adaptation component and address limitation .
the enhanced siriusqa receives a question and produces an answer by executing the following process the question interpretation component interprets the question by matching it against each of the question patterns generated by the domain adaptation component.
it produces a query for the document search component and a question category.
it follows the same process described in section ii a. the document search component takes in a query and outputs a ranked list of documents that are then fed to the answer selection component.
it follows the same process described in section ii a. the answer selection component first breaks returned documents into candidate answer sentences.
it then ranks candidate answer sentences using the answer pattern of the identified question category and a customized keyword matching strategy that addresses limitation .
answer selectionenhanced api documentation original question interpretation document searchquery answerquestion new enhancedlegendanswer patternsquestion patterns domain adaptation enhanced siriusqa fig.
.
apibot framework b. domain adaptation the domain adaptation is completed through two steps.
each step is described in detail below.
step document naturalization in an api documentation much information is stored in the document structure.
for example the structure indicating api elements e.g.
classes methods constructors fields etc.
is described by different description block.
unfortunately as discussed in section ii siriusqa cannot deal with structured documents.
simply flattening structured documents into regular text files would not work since essential pieces of information stored in the document structure would be lost.
to tackle this challenge we convert the structured information in api documentation to natural language sentences which can be understood by siriusqa.
we design a number of rule based heuristics that make use of the regularities in the structure of api documentation to create natural language sentences that explicitly describe the implicit information stored in the api documentation structure.
in this work we focus on javadoc api documentation which is one of the most popular api documentation formats.
similar rule based heuristics can be designed for other api documentation formats which we leave as future work.
the documentation naturalization step takes in a javadoc api documentation and converts it to a collection of natural language documents which we refer to as enhanced api documentation.
this step iterates over all documentation pages for classes and interfaces and then transforms implicit information stored in the javadoc structure into plain sentences.
for instance information from the inheritance block in a javadoc html document will be transformed into a sentence like arraylist extends abstractlist.
.
step question answer pattern generation in this step we generate question and answer patterns from a set of training question answer pairs.
figure shows a overview of this step.
the question patterns are generated through an annotation process while the answer patterns are generated through an automated inference process.
both patterns are grouped according to a predefined category.
original process new enhanced processlegendcategoryannotation training qa pairsquestion patterns inferenceanswer patterns fig.
.
question answer pattern generation question pattern annotation we consider a similar methodology as siriusqa to create question patterns from a set of training question answer pairs see section ii a .
the major difference is that we consider a new set of domain specific categories.
siriusqa has categories that are not suitable for api documentation.
we use the categories related to api domain by following the work of maalej and robillard .
excluding non information they defined types of domain authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i taxonomy of question categories category description example question code examplequestions requesting a code example for a specific task or functionality.do you have a sample code to append a string to a stringbuilder?
conceptsquestions related to explanation of a particular api entity or domain concepts.what is stringbuilder?
control flowquestions related to actions which need to follow a particular execution order.what happens when you call notifyobservers method in observable class?
directivesquestions asking about allowed or disallowed contracts provided by api elements.can i control the number of threads to execute completablefutures?
environmentquestions about api version licences compatibility issues etc.what are the new things provided by arraylist class in this version of api?
functionality and behaviorquestion asking about a feature or functionality provided by the api.what will happen if there are multiple duplicate objects exist when using remove object o of arraylist?
patternsquestions describing a specific operation and query about how the api can be used to complete the operation.how can i add element into arraylist?
purpose and rationalequestions related to a design and or purpose rationale of an api element.why is hashmap not thread safe?
quality attribute and intenal aspectsquestions related to non fucntional aspects of an api.what are the two parameters of an instance of hashmap that affect its performance?
referencequestions asking for a reference or extra information related to an api element.what classed or interfaces i can read about which are most similar to arraylist?
structure and relationshipsquestions pertaining to the hierarcy or organization of api.
what are the implemented interfaces of hashmap?
knowledge inside api documentation.
we create a category for each one of the knowledge types.
table i shows the definition and a sample question for each category.
given a new question apibot makes use of annotated question patterns to decide the category target and contexts of a new question.
answer pattern inference this step takes as input a training set of question answer pairs from a given category.
for every question its target phrase has been identified and this target must appear in the corresponding answer.
we abstract an answer into a generalized answer by following these steps we replace the target phrase with htargeti we identify api elements e.g.
class names package names etc.
from each answer sentence and replace them with their types e.g.
hclassi hinterfacei hpackagei we replace every be verb e.g.
is are etc.
with hbei and every verb with hverbi1 we use the symbol to represent the starting point of a sentence all other words are kept unchanged.
next we reduce the size of each generalized answers by considering nwords before and after the htargeti.
for instance in the experiment nis set to by default then every generalized answer is reduced to a sequence of words including words before and words after the htargeti.
we refer to this sequence of words as a reduced answer.
for each question category we then create an answer pattern based on reduced answers of the category.
an answer pattern is a probability model consisting of a collection of word probability distributions.
each distribution corresponds to one of thenpositions before and after htargeti.
it is created by computing the relative frequency of words appearing on the corresponding position in the reduced answers.
given a question which has been interpreted by the question interpretation component question category and htargeti are identified.
answer pattern of the corresponding question 1we identify verbs by using a part of speech pos taggercategory is then used to assign a probability score to a candidate answer sentence.
each candidate answer sentence is first abstracted to a generalized answer and then reduced to a sequence of 2n words with thehtargetiin the center.
the candidate answer sentence s score is calculated by multiplying probabilities of words appearing before and after thehtargeti.
these probabilities are inferred from the learned answer pattern of the corresponding category.
c. answer selection the output of document search component is a ranked list of documents that contain query keywords.
we consider only top documents for finding the correct answer to a given question.
from these documents we extract candidate answer sentences and rank them.
the answer ranking process contains three steps document splitting and sentence filtering sentence scoring and sentence ranking.
we describe the details of these steps below.
step document splitting and sentence filtering we split the retrieved documents into sentences.
we consider that a new sentence begins when a dot sign is followed by whitespace s and the following word starts with a capitalized first character.
next we remove sentences that are unlikely to be an answer to the question.
in particular we check if names of classes or interfaces described in the api documentation appear in the question.
if the names of such classes or interfaces appear then we remove sentences that do not appear in the javadoc api pages for those classes or interfaces except those that contain the names.
step sentence scoring after retrieving documents splitting them into sentences and filtering out of scope sentences we now have candidate answer sentences.
to rank these sentences we use two scoring strategies pattern specific anddomain specific.
if the question category is known we compute two scores for each candidate answer sentence using the two strategies.
these scores are combined together into a authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
final score.
however if the category is not known we only run the domain specific strategy.
both scoring strategies and the score combination strategy are described below.
pattern specific scoring.
we use the answer pattern corresponding to the category of an input question to assign a score to a candidate sentence.
the answer pattern is a probabilistic model described in section iii b2.
domain specific scoring.
while an answer pattern captures the general pattern of nearby words surrounding a question s target it ignores words that are located some distance away from the target.
these words may include important domainspecific words.
in this work we consider all class and interface names in a target javadoc api documentation as domainspecific words.
all other words are considered general words.
next we match the domain specific and general words in the question with the words in the answer candidate.
a matching with a domain specific word is given a higher weight and boosts the score higher compared to that of a general word.
based on the matching we compute the following domainspecific score dmscore dmscore w1 genword w2 domainword words wheregenword anddomainword are the number of matched general and domain words respectively.
words is the total number of words.
w1andw2are the weights for general and domain words respectively.
we set w1andw2 to be and respectively.
consequently we boost the score by giving a matching of a domain specific word twice the importance of that of a general word.
step score normalization and ranking if a question category can be inferred for each answer candidate we have two scores and these scores need to be unified.
to unify the scores we first normalize them.
next we combine the normalized scores as follows finalscore pmnormscore dbnormscore in the above equation pmnormscore anddbnormscore are the normalized pattern specific and domain specific scores respectively.
by default we consider both scores to be equally important so we set both and to be .
.
iv.
p reliminary experiments and results a. dataset we select javadoc of java development kit as our api documentation corpus as it is one of the largest and most popular apis.
as most java api documentation follows the same structure we believe results achieved on current dataset should be generalizable enough for other java apis as well.
for training data we create our own question patterns for categories in table i. we create the questions by figuring out varying ways one can ask a question about a particular category.
we also make sure that our questions have answers inside the api documentation.for evaluation data we ask phd students and java developers all with more than years of programming experience in java to ask questions about each category with a requirement that the answer to each question should be found in an api documentation page.
we collect a total of questions for concept category and for each of the remaining categories.
after the collection of questions we assemble a team of phd students.
we give the collected questions to this team who are required to go through all the questions and then find a sentence in the api documentation both enhanced and original which could be considered as the correct answer for each question assigned.
both the team members had to discuss and come to an agreement before coming up with sentence s that constitute as an answer to each question.
for of the questions the team cannot properly understand them or find sentences in the documentation that answer them.
so in the end we had a total of question answer pairs and we use this data as the ground truth to evaluate the performance of apibot.
our dataset is of similar size as used in evaluating other specialized qa systems e.g.
.
although it is not very large it covers all the categories identified by maalej and robillard which we believe to be sufficient for preliminary experiments.
b. experiment setting evaluation metric in the experiment we query each question and record the returned answers from apibot.
by default apibot returns five possible answers per question.
we evaluate the answers by matching them against the ground truth.
to measure the effectiveness of apibot in answering questions correctly we use hit n metric which measures the percentage of questions for which we are able to find the correct answer in the top n e.g.
returned answers.
baselines we consider baselines to compare with apibot.
baseline it is the original siriusqa system .
as the original siriusqa system returns only one top ranked answer for each question we modify it to return the top ranked answers so that it can be compared against apibot using the hit n evaluation criteria discussed above for n .
for this baseline system we use the unnormalized text extracted from original api html documents as the knowledge source.
baseline this baseline consists of the original siriusqa system using the enhanced api documentation as the knowledge source.
we use this baseline to measure the performance benefits provided by the question answer pattern generation step and enhanced answer selection component.
baseline this baseline is our apibot system which uses the original api documentation as the knowledge source rather than the enhanced api documentation .
this baseline is used to measure the performance benefits provided by the document naturalization step discussed in section iii.
c. research questions rq1 how effective is our approach in answering the questions related to api documentation?
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii shows the hit score of apibot system as compared to baseline .
the hit score of apibot is .
as compared to .
for the baseline system.
thus the result shows that apibot greatly improves the performance of baseline siriusqa showing the value of injecting domain knowledge to it.
without domain knowledge sirius cannot make use of question patterns and unable to find information that is encoded in documentation structure.
table ii performance of apib ot and sirius qa approach hit apibot .
baseline original siriusqa .
rq2 how important are document naturalization and answer pattern inference steps for accurately answering questions related to api documentation?
the results of of our experiment w.r.t.
rq2 are shown in table iii.
it shows that baseline which performs document naturalization step before using siriusqa does not result in performance gain.
this is because siriusqa cannot identity targets andcontexts for api related questions.
on the other hand removing document naturalization step from apibot results in a big performance drop i.e.
from .
to .
.
thus the two steps need to be performed together to build an effective question answering system for api documentation.
table iii benefit of main steps of apib ot approach hit baseline original siriusqa .
baseline siriusqa enhanced api doc.
.
baseline enhancedsiriusqa original api doc.
.
apibot enhancedsiriusqa enhanced api doc.
.
v. r elated work murgia et al.
developed an experimental bot to answer questions on stackoverflow .
it was trained to handle simple and reoccurring questions related to giterror messages and provide answers based on previous solutions to similar problems.
the main focus was to understand how the bot is perceived when presented as a person versus as a bot.
storey and zagalsky suggested that we could use bots to automate many tasks in software development .
there also has been work on retrieving relevant information from api documentation .
our work enriches the above mentioned studies by proposing a support bot to help in answering questions related to api documentation.
in information retrieval and natural language processing domain many work has been done in building general purpose qa systems e.g.
.
in this paper we propose a domain specific qa system by incorporating domain knowledge to customize a general purpose qa system.
vi.
c onclusion and future work in this paper to help developers find answers to api related questions we developed the first question answering bot for api documentation namely apibot.
apibot is built on top of siriusqa.
we made various efforts in adapting siriusqafor api documentation.
our preliminary experiment with api related questions demonstrates that apibot can achieve a promising hit score of .
and highlights the value of our adaptation strategies.
as a future work we plan to pursue the following directions.
first we would like to build a larger benchmark data from various api documentations.
second we would like to investigate possibility to reuse answer patterns learned from one api documentation for another.
third we want to investigate the possibility of using other underlying generalpurpose qa systems beyond siriusqa by following similar adaptation strategy presented in this paper.
fourth we also plan to develop additional strategies to better handle structural information inherent in api documentation.
the strategy that we employ in the document naturalization step may not be the optimal one.