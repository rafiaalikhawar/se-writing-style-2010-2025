an empirical investigation into the nature of test smells michele tufano1 fabio palomba2 gabriele bavota3 massimiliano di penta4 rocco oliveto5 andrea de lucia2 denys poshyvanyk1 1the college of william and mary usa 2university of salerno italy 3universit della svizzera italiana usi switzerland 4university of sannio italy 5university of molise italy abstract test smells have been defined as poorly designed tests and as reported by recent empirical studies their presence may negatively a ect comprehension and maintenance of test suites.
despite this there are no available automated tools to support identification and repair of test smells.
in this paper we firstly investigate developers perception of test smells in a study with participants.
the results show that developers generally do not recognize potentially harmful test smells highlighting that automated tools for identifying such smells are much needed.
however to build e ective tools deeper insights into the test smells phenomenon are required.
to this aim we conducted a large scale empirical investigation aimed at analyzing i when test smells occur in source code ii what their survivability is and iii whether their presence is associated with the presence of design problems in production code code smells .
the results indicate that test smells are usually introduced when the corresponding test code is committed in the repository for the first time and they tend to remain in a system for a long time.
moreover we found various unexpected relationships between test and code smells.
finally we show how the results of this study can be used to build e ective automated tools for test smell detection and refactoring.
ccs concepts software and its engineering !software evolution keywords test smells mining software repositories software evolution .
introduction testing represents a significant part of the whole software development e ort .
when evolving a software system developers evolve test suites as well by repairing them when needed and by updating them to sync with the new versionof the system.
to ease developers burden in writing organizing and executing test suites nowadays appropriate frameworks e.g.
junit conceived for unit testing but also used beyond unit testing are widely adopted.
concerning other code artifacts in the following referred to as production code researchers have provided definitions of symptoms of poor design choices known as code smells for which refactoring activities are desirable.
subsequently researchers developed automated tools to detect them e.g.
and empirically studied the developers awareness of such smells as well as the relationship between smells and negative phenomena such as higher fault and change proneness .
at the same time researchers have also confuted some common wisdom about software aging showing that often the presence of code smells is not necessarily due to repeated changes performed on source code artifacts during their evolution rather in most cases smells are introduced when such artifacts are created .
in conclusion both researchers and practitioners are carefully looking at the code smell identification and nowadays smell detection is often included as part of continuous integration and delivery processes .
a quite related phenomenon to code smells can also occur in test suites which can be a ected by test smells .t e s t smells defined by van deursen et al.
are caused by poor design choices similarly to code smells when developing test cases the way test cases are documented or organized into test suites the way test cases interact with each other with the production code and with external resources are all indicators of possible test smells.
for instance mystery guest occurs when a test case is using an external resource such as a file or a database thus making the test not self contained and assertion roulette when a test case contains multiple assertions without properly documenting all of them .
empirical studies have shown that test smells can hinder the understandability and maintainability of test suites and refactoring operations aimed at removing them have been proposed .
nevertheless it is still not clear how developers perceive test smells and whether they are aware of them at all.
also it is not known whether test smells are introduced as such when test suites are created or whether test suites become smelly during software evolution and whether developers perform any refactoring operations to remove test smells.
such information is of paramount importance for designing smell detection rules and building automated detection tools to be incorporated in the development process and especially in the continuous integrapermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
tion processes where automated tools could identify test smells and because of that make the build fail and notify developers about the presence of the test smells.
highlighting test smells in scenarios where it is known that developers do not want and need to maintain them e.g.
because there is no better solution would make automated smell detection tools usable avoiding recommendation overload and even build failures.
paper contribution.
this paper reports a thorough empirical investigation into the perceived importance of test smells and of their lifespan across software projects change histories.
first we conducted a survey with developers assessing whether developers could recognize instances of test smells in software projects.
such a survey obtained a clear negative result indicating that unlike what previously found for code smells there is basically no awareness about test smells highlighting the need for semi automatic support to aid in detecting these design issues.
thus we conducted a mining study over the change history of software projects to gather the deeper knowledge needed to design e ective test smells detectors.
in the context of this study we investigate i when test smells are introduced ii how long test smells survive and whether developers try to remove them and iii whether test smells are related to the presence of smells in production code and therefore there can be synergies in their detection.
the achieved results indicate that i test smells mostly appear as the result of bad design choices made during the creation of the test classes and not as the result of design quality degradation over time ii test smells stay in the system for a long time with a probability of that a test smell would not be fixed after days from its introduction and iii complex classes e.g.
blob classes in the production code are often tested by smelly test classes thus highlighting a relationship existing between code and test smells.
paper structure.
section describes the survey we performed to investigate developers perception of test smells.
section details the mining study definition and planning while the results are reported in section .
threats to the studies validity are discussed in section .
after a discussion of the related work section section concludes the paper and outlines directions for future work.
.
test smell perception in this section we report the design and the results of a survey we conducted with the aim of understanding whether developers perceive test smells as design problems.
specifically we aim at answering the following research question rq are test smells perceived by developers as actual design problems?
our study focuses on five types of test smells from the catalogue by van deursen et al.
.assertion roulette ar as defined by van deursen et al.
this smell comes from having a number of assertions in a test method that have no explanation .
thus if an assertion fails the identification of the assert that failed can be di cult.
besides removing the unneeded assertions to remove this smell and make the test more clear an operation of add assertion explanation can be applied .
.eager test et at e s ti sa e c t e db y eager test when it checks more than one method of the class to be tested making the comprehension of the actual test target di cult.
the problem can be solved by applying extract method refactoring splitting the test method in order to specialize its responsibilities .
.general fixture gf a test class is a ected by this smell when the setup method is too generic and the test methods only access part of it .
in other words the test fixture is not only responsible for setting the common environment for all the test methods.
a suitable refactoring to remove the smell is the extract method which reorganizes the responsibilities of the setup method .
.mystery guest mg this smell arises when a test uses external resources e.g.
a file containing test data and thus it is not self contained .
tests containing such a smell are di cult to comprehend and maintain due to the lack of information to understand them.
to remove a mystery guest asetup external resource operation is needed .
.sensitive equality se when an assertion contains an equality check through the use of the tostring method the test is a ected by a sensitive equality smell.
in this case the failure of a test case can depend on the details of the string used in the comparison e.g.
commas quotes spaces etc.
.
a simple solution for removing this smell is the application of an introduce equality method refactoring in which the use of the tostring is replaced by a real equality check.
while several other smells exist in the literature we decided to limit our analysis to a subset of such smells in order to focus the questions for survey s participants on a few smell types allowing to collect more opinions for the same smell.
however we take into account a fairly diverse catalogue of test smells which are related to di erent characteristics of test code.
as reported in a previous work our selection includes test smells having the greatest di usion in both industrial and open source projects.
to answer rq we invited the original developers of five projects from the apache and eclipse ecosystems namely apache james mime4j apache jspwiki apache poi eclipse mylyn and eclipse platform ui .
these projects represent a subset of those considered in our larger mining study described in section and they were selected because they contain all types of smells from the considered catalogue.
for this study smells were manually identified by one of the authors and double checked by another author.
we chose to involve original developers rather than external developers i.e.
developers with no experience with the subject systems since we wanted to collect the opinions of developers that actually developed the systems under analysis and therefore have a good knowledge about the rationale behind the design choices applied during the development of such systems.
in total we invited developers receiving responses from of them three from apache james mime4j one from apache jspwiki six from apache poi one from eclipse mylyn and eight from eclipse platform ui .
note that even though the number of respondents appears to be low .
response rate our results are close to the suggested minimum response rate for the survey studies which is defined around .
.
survey questionnaire design the general idea behind the study design was to show to each developer one test smell instance of each type.
this is done to avoid having a long questionnaire that might have discouraged developers to take part in our study.
for each test smell instance the study participants had to look at the source code and answer the following questions .in your opinion does this class have any design problem?
please rate your opinion from strongly disagree to strongly agree.
.ifyou agreed or strongly agreed to the question number please explain what are in your opinion the design problems of this class.
.ifyou agreed or strongly agreed to the question number please explain why the design problem has been introduced.
.ifyou agreed or strongly agreed to the question number do you think that this class needs to be refactored?
please rate your opinion from strongly disagree to strongly agree.
.ifyou agreed or strongly agreed to the question number how would you refactor this class?
the survey was designed to be completed within approximately minutes.
to automatically collect the answers the survey was hosted using a web application esurveypro1.
developers were given days to respond to the survey.
note that the web application allowed developers to complete the questionnaire in multiple rounds e.g.
to answer the first two questions in one session and finish the rest later.
at the end of the response period we collected developers answers of the complete questionnaires in a spreadsheet in order to perform data analysis.
note that the developers of the five systems were invited to evaluate only the test smells detected in the system they contribute to.
.
analysis method to answer rq 0we computed .the distribution of values assigned by developers when evaluating whether the analyzed test classes had a design problem question of the survey .
.the percentage of times the smell has been identifiedby the participants.
by identified we mean cases where participants besides perceiving the presence of a smell were also able to identify the exact smell affecting the analyzed test code by describing it when answering question of the survey.
note that we consider a smell as identified only if the design problems described by the participant are clearly traceable onto the definition of the test smell a ecting the code component.
.the distribution of values assigned by developers when evaluating whether the test classes analyzed should be refactored question of the survey .
.the percentage of times the refactoring of the test smell has been identified by the participants question of the survey .
in this case by identified we mean cases where participants correctly identified how answers for questions and .
questionanswer the design problems a ecting the test class should be removed.
moreover we collected the answers to question in order to understand the reasons why test smells are introduced.
.
analysis of the results table reports the distribution of values assigned by developers when answering the questions and respectively.
we can see that often developers do not perceive test smells as actual problems.
indeed only in cases out of the total test smells analyzed by the developers a design flaw has been identified i.e.
answers to question with value .
these answers come from only five developers out of .
in these cases however participants were often not able to correctly diagnose the test smell a ecting the analyzed test code only in of the cases developers correctly identified a test smell .
moreover when analyzing the answers to question of the survey we found that almost always of the cases participants did not feel that the refactoring activity would be beneficial to improve the design of the considered test classes.
for this reason developers were not able to provide good suggestions for possible refactoring operations question .
the most important feedback we obtained from this study is related to the answers provided when answering question .
as an example analyzing an instance of eager test a developer from apache poi claimed that probably the code was written in a hurry and was never reviewed .
also a developer from eclipse platform ui claimed that the code analyzed was introduced in and hasn t got much attention .
this feedback highlights that even when perceiving design flaws developers are not able to correctly identify and explain the reasons behind test smell introduction.
this constitutes the need for automated tool support in order to alert developers about the presence of test smells in the test code that they produce.
.
test smell lifecycle design thegoal of the study is to analyze the change history of software projects with the purpose of investigating when test smells are introduced by developers what is their survivability and whether they are associated with code smells in production code.
the context of the study consists of open source projects belonging to two ecosystems apache and eclipse for which we investigated the presence and evolution of test smells.
.
research questions and context the study aims at answering the following rqs rq when are test smells introduced?
this research question aims at assessing whether test smells are introduced as a consequence of maintenance activities performed on test classes or whether they are introduced as soon as the corresponding test class is committed to the repository for the first time.
results of 6table ecosystems under analysis.
ecosystem proj.
classes kloc commitsmean story min max length story length apache eclipse overall rq 1will help understand the kind of automatic detection tools developers need to identify test smells and the way such tools should be integrated in the development process.
indeed if test smells are introduced as the result of continuous maintenance and evolution activities then detectors can be executed periodically or as a part of a continuous build process as it happens with approaches proposed in the literature to catch code smells e.g.
.
instead if test smells are introduced when the test code is written in the first place then just in time refactoring tools could be built such tools should continuously monitor the test code written in the ide alerting the developer when it is deviating from good design practices thus avoiding the introduction of the test smells in the first place.
rq what is the longevity of test smells?
this research question aims at analyzing the lifetime of test smells with the goal of understanding to what extent they remain in a software project from their introduction until their possible removal.
long living test smells are likely to indicate design issues di cult to catch for software developers thus indicating the need for semi automated detection tools.
rq are test smells associated with particular code smells a ecting the production code?
in this research question we intend to assess whether test smells are usually associated with design problems occurring in production code.
while test and code smells have a di erent nature uncovering relationships between the two can highlight possible synergies in their detection and refactoring recommendation.
thecontext of the study consists of software projects belonging to two di erent ecosystems i.e.
those managed by the apache software foundation and by the eclipse foundation.
table reports for each ecosystem i the number of projects analyzed ii size ranges in terms of the number of classes and kloc iii the overall number of commits analyzed and iv the average minimum and maximum length of the projects history in years analyzed in each ecosystem.
all the analyzed projects are hosted in gitrepositories.
the apache ecosystem consists of java systems randomly selected among those available2 while the eclipse ecosystem consists of projects randomly mined from the list of github repositories managed by the eclipse foundation3.
the choice of the ecosystems to analyze is not random but guided by the will to consider projects having i di erent scope ii di erent sizes and iii di erent architectures e.g.
we have apache libraries as well as plugin based architectures in eclipse projects.
from the original dataset we discarded projects having no test case in their entire change history.
this was the case for apache test cases found in the analyzed projects.
ecosystem min.
1st qu.
median mean 3rd qu.
max.
apache eclipse projects and two eclipse projects thus leading to total projects analyzed.
table reports the distribution of number of test cases identified in the analyzed projects.
the complete list of projects and the number of test cases identified is available in our anonymized online appendix .
the study focuses on the five types of test smells also considered in the survey assertion roulette eager test general fixture mystery guest and sensitive equality .a l s o since in the context of rq 3we assess the possible relationship between test and code smells in our study we consider the following types of code smells .blob class a large class with di erent responsibilities that monopolizes most of the system s processing .class data should be private a class that exposes its attributes thus violating the information hiding principle .complex class a class having a high cyclomatic complexity .functional decomposition a class where inheritance and polymorphism are poorly used declaring many private fields and implementing few methods .spaghetti code a class without structure that declares long methods without parameters .
this analysis is limited to a subset of the smells that exist in the literature due to the computational constraints.
however we preserve a mix of smells related to complex large code components e.g.
blob class complex class as well as smells related to the lack of adoption of good object oriented coding practices e.g.
class data should be private functional decomposition .
.
data extraction to answer our research questions we firstly cloned the git repositories of the subject software systems.
then we mined the evolution history of each repository using a tool that we developed named historyminer it checks out each commit of a repository in chronological order and identifies the code files added and modified in each specific commit.
note that only java files are considered as code files while the remaining files are discarded e.g.
building files images .
.
.
artifact lifetime log for each commit historyminer classifies code files in two sets test classes andproduction classes .
specifically a file is classified as a test class if the corresponding java class extends the class junit.framework.testcase .
those code files which are not classified as test classes are considered as production classes .
finally for each test class identified our tool analyzes the structural dependencies of the test code to identify the list of associated production classes i.e.
the classes exercised by the test class .
all production classes having structural dependencies e.g.
method invocations with a test class ctare considered as exercised by ct. the output of historyminer is an artifact lifetime log for each code file in a project repository.
the tool generates two 7table rules used to detect test smells .
name abbr.
description assertion roulette ar junit classes containing at least one method having more than one assertion statement and having at least one assertion statement without explanation.
general fixture gf junit classes having at least one method not using the entire test fixture defined in thesetup method eager test eg junit classes having at least one method that uses more than one method of the tested class.
mystery guest mg junit classes that use an external resource e.g.
a file or database .
sensitive equality se junit classes having at least one assert statement invoking a tostring method.
di erent lifetime logs for test cases andproduction classes .
for each test ftcin the repository the tool outputs a logtc which contains a row for each commit which modified ftc.
thei th row contains the following fields ftc.name the fully qualified name of ftc ci.id the commit hash of the i th commit which modified ftc ci.time the timestamp when the i th commit has been performed in the repository tstc the list of the test smells a ecting ftcatci.time if any pc tc the list of fully qualified names of production classes having structural dependencies with ftc.w e assume these classes to be the ones exercised by ftc.
similarly for each production class file fpcin the project repository the tool outputs a logpc which contains a row for each commit which modified fpc.
the i th row contains the following fields fpc.name the fully qualified name of ftc ci.id the commit hash of the i th commit which modified fpc ci.time the timestamp when the i th commit has been performed in the repository cs pc the list of code smells a ecting fpcatci.time if any .
with the collected information we are able to identify for a given test file ftcthe list of code smells a ecting the production classes it tests pc tc at any moment in time this is needed in order to answer rq .
in particular given a commit cimodifying ftc we retrieve the list of code smells a ecting each class c2pc tcat time ci.time by retrieving among all commits a ecting i.e.
adding modifying c the one cj having the greatest timestamp lower than ci.time i.e.
the commit a ecting c being the closest in terms of time to ciamong those preceding ci .
consider the smells a ecting cat time cj.time as the ones a ecting it also at time ci.time when the commit modifying ftcwas performed.
.
.
identification of smells in the context of our analysis we had to identify test and production code smells at each commit.
given the high number of commits analyzed i.e.
a manual detection is practically infeasible.
for this reason during theanalysis of each commit historyminer used detection rules which identify both test and code smells.
concerning the test smell detection historyminer relied on the approach proposed by bavota et al.
to detect the five analyzed test smells.
such an approach applies a heuristic metricbased technique that overestimates the presence of test design flaws with the goal of identifying all the test smell instances i.e.
it targets recall .
table reports the set of rules used by the tool in order to detect instances of test smells.
for example it marks junit classes as a ected by general fixture those having at least one test method not using the entire test fixture defined in the setup method.
this approach has been shown to have a precision higher than for all detected smells .
however it is not able to recommend refactoring operations to remove the identified smells thus only providing a very limited support to software developers interested in removing test smells from their system.
as for the code smells detection we run on each commit an implementation of the decor smell detector based on the original rules defined by moha et al.
.
decor identifies smells using detection rules rooted in internal quality metrics4.
the choice of using decor is driven by the fact that i it is a state of the art smell detector having a high accuracy in detecting smells and ii it applies simple detection rules that make it very e cient.
.
data analysis in this section we describe the data analysis performed to answer each of the three formulated research questions.
.
.
rq when are test smells introduced?
we analyzed each lifetime log for a test file ftcto identify for each test smell tska ecting it the following two commits ccreation the first commit creating ftc i.e.
adding it in the versioning system csmell k the first commit where tskis detected in ftc i.e.
the commit in which tskhas been introduced .
then we analyze the distance between ccreation and csmell kin terms of number of commits considering only the commits which involved ftc.
we show the distribution of such distances for each type of test smell as well as for all test smell instances aggregated.
.
.
rq what is the longevity of test smells?
to address rq we need to determine when a smell has been introduced and when a smell disappears from the system.
to this aim given a test smell tska ecting a test fileftc we analyze logtcand formally define two types of commits smell introducing commit the first chronological commitciwhere tskhas been detected.
smell removing commit the first chronological commitcjfollowing the smell introducing commit ciin which the test smell tskno longer a ects ftc.
given a test smell tsk the time interval between the smell introducing commit and the smell removing commit 4an example of detection rule exploited to identify blob classes can be found at 8is defined as smelly interval and determines the longevity oftsk.
given a smelly interval for a test smell a ecting the fileftcand bounded by the last smell introducing commit ci and the smell removing commit cj we compute as proxies for the smell longevity commits the total number of commits between ci andcj tcchanges the number of commits between ciand cjthat modified the test case ftc a subset of the previously defined set commits days the number of days between the introduction of the smell ci.time and its fix cj.time .
note that since we are analyzing a finite change history for a given repository it could happen that for a specific file and a smell a ecting it we are able to detect the smellintroducing commit but not the smell removing commit due to the fact that the file is still a ected by the test smell in the last commit.
in other words we can discriminate two di erent types of smelly intervals in our dataset closed smelly intervals intervals delimited by a smellintroducing commit as well as by a smell removing commit censored smelly intervals intervals delimited by a smell introducing commit and by the end of the change history i.e.
the date of the last commit we analyzed .
the collected data was used to answer the following subquestions.
rq .1how long does it take to fix a test smell?
we analyze only the closed smelly intervals which correspond to the fixed test smell instances.
for each type of test smell we show the distribution of commits tcchanges and days related to the closed smelly intervals.
rq .2what is the percentage of test smell instances fixed?
we report for each type of test smell the percentage of fixed instances i.e.
closed smelly intervals .
in this we pay particular attention to the censored intervals related to test smells introduced towards the end of the observable change history of each project repository.
this is because for those instances developers might not have had enough time to fix them.
indeed we compute the percentage of fixed and not fixed instances by progressively removing instances introduced xdays before the date of the last commit of the project repository we mined.
to define x we use the results of the previous sub research question related to the number of days usually needed to fix a test smell.
therefore we compute the percentages of fixed and not fixed test smell instances using i all the smelly intervals ii excluding censored intervals using x 1stquartile of the days to fix a test smell iii x median value and iv x 3rdquartile.
it is worth noting that while in general any test smell instance defining a censored interval could potentially be fixed in the future this is progressively less likely to happen as the time goes by.
rq .3what is the survivability of test smells?
we answer this subquestion by relying on survival analysis a statistical method that aims at analyzing and modeling the time duration until one or more events happen.
such time duration is modeled as a random variable and typically it has been used to represent the time to the failure of a physical component mechanical or electrical or the time to the death of a biological unit patient animal cell etc.
.
the survival function s t pr t t indicates the probability that a subject in our case the code smell survives longer than some specified time t. the survival function never increases as tincreases also it is assumed s at the beginning of the observation period and for time t!
s !
.
the goal of the survival analysis is to estimate such a survival function from data and assess the relationship of explanatory variables covariates to survival time.
time duration data can be of two types .complete data the value of each sample unit is observed or known.
for example the time to the failure of a mechanical component has been observed and reported.
in our case the code smell disappearance has been observed.
.censored data the event of interest in the analysis has not been observed yet so it is considered as unknown .
for example a patient cured with a particular treatment has been alive till the end of the observation window.
in our case the smell remains in the system until the end of the observed project history.
for this sample the time to death observed is a censored value because the event death has not occurred during the observation.
both complete and censored data can be used if properly marked to generate a survival model.
the model can be visualized as a survival curve that shows the survival probability as a function of the time.
in the context of our analysis the population is represented by the test smell instances while the event of interest is its fix.
therefore the time to death is represented by the observed time from the introduction of the test smell instance till its fix if observed in the available change history .
we refer to such a time period as the lifetime of a test smell instance.
complete data is represented by those instances for which the event fix has been observed while censored data refers to those instances which have not been fixed in the observable window.
we generate survival models using both the of days and the of commits in the smelly intervals as time variables.
we analyzed the survivability of test smells by ecosystem.
that is for each ecosystem we generated a survival model for each type of test smell by using rand the survival package5.
in particular we used the surv type to generate a survival object and the survfit function to compute an estimate of a survival curve which uses kaplan meier estimator for censored data.
it is important to highlight that while the survival analysis is designed to deal with censored data we performed a cleaning of our dataset aimed at reducing possible biases caused by censored intervals before running the analysis.
in particular test smell instances introduced too close to the end of the observed change history can potentially influence our results since in these cases the period of time needed for their removal is too small for being analyzed.
thus as done for the previous sub question we excluded from our survival analysis all censored intervals for which the last smellintroducing commit was too close to the last commit we analyzed in the system s change history i.e.
for which the developers might not have had enough time to fix them .
in the paper we report the survival analysis considering all the closed and censored intervals no instances removed .
9table rq number of commits between the creation of the test case and the introduction of a test smell.
ecosystem smell min.
1st qu.
median mean 3rd qu.
max smelly 1st ar .
et .
apache gf .
mg .
se .
ar .
et .
eclipse gf .
mg .
se .
the results of the survival analysis with di erent thresholds to clean the dataset are available on our online appendix .
.
.
rq are test smells associated with particular code smells affecting the production code?
for each test file ftc we collect the set of test smells tstc a ecting it at the time of its creation as well as the set of code smells cs pctc a ecting the production classes exercised byftc pc tc at the same time.
the goal is to identify patterns of co occurrence among items belonging to tstc and items of cs pctc.
to do so we generate a database of transactions which contains a transaction for each test fileftc.
each transaction contains the test and code smells intstcandcs pctcas items.
note that if tstcis empty we insert the item cleantc to represent a test file created without any test smell.
similarly if cs pctcis empty we insert the item cleanpc .
we analyze such database of transactions using association rule mining to identify patterns of co occurrence of test and code smells.
in particular we use the statistical software rand the package arules .
.
test smell lifecycle results this section reports the results of our empirical study.
.
rq introduction of test smells table reports for each ecosystem the statistics of the distribution of the number of commits needed by each test smell type to manifest itself.
also the last column of table reports the percentage of test smell instances that have been introduced when the a ected test class has been committed for the first time.
the results shown in table clearly highlight one finding test smells mostly appear as the result of bad design choices made during the creation of the test classes and not as the result of design quality degradation during maintenance and evolution activities .
indeed for all the considered test smell types the 3rdquartile of the distributions equals zero i.e.
the commit which introduces the smell is the same which introduces the test class .
also for all test smells and in both ecosystems at least of the smell instances are introduced when the artifact is created and committed for the first time in the repository see last column in table .
this result is in line with previous findings on code smells in production code and it contradicts the common wisdom for which test smell instances are the result of test code evolution .
for this reason future automatic identification tools should take into account the fact that the lifespan of test smells starts in most of the cases with the creation of a test case.
ides and au table percentage of fixed instances in the observed change history using di erent thresholds to remove censored intervals.
smell history apache eclipse overall all .
.
.
excluding 1st q. .
.
.
excluding median .
.
.18ar excluding 3rd q. .
.
.
etall .
.
.
excluding 1st q. .
.
.
excluding median .
.
.
excluding 3rd q. .
.
.
all .
.
.
excluding 1st q. .
.
.
excluding median .
.
.31gf excluding 3rd q. .
.
.
mgall .
.
.
excluding 1st q. .
.
.
excluding median .
.
.
excluding 3rd q. .
.
.
all .
.
.
excluding 1st q. .
.
.
excluding median .
.
.31se excluding 3rd q. .
.
.
allall .
.
.
excluding 1st q. .
.
.
excluding median .
.
.
excluding 3rd q. .
.
.
tomatic refactoring tools should pay particular attention to when test classes are firstly created and committed to the repository since their quality can be compromised by the presence of a design flaw.
promptly suggesting an alternative design or refactorings for a newly created test class could significantly increase the chances of having a clean test class.
.
rq longevity of test smells when evaluating the survivability of test smells we take into account several aspects related to this main research question as detailed in the following subsections.
.
.
how long does it take to fix a test smell?
fig.
shows the distribution of the number of days between the introduction of a test smell and its removal.
remember that this data is only available for test smell instances for which we observed both their introduction and their removal over the analyzed change history.
the boxplots are organized by the ecosystem with the rightmost one showing the overall results obtained by combining both apache and eclipse projects.
overall we can observe that test smell instances are removed on average after days from their introduction6.
we do not find any significant difference among the di erent test smell types.
we also considered the distribution of the number of modifications a developer performs before fixing a test smell shown in fig.
.
as we can see no more than five modifications involving the a ected test class are performed before the test smell disappears.
this finding shows on one hand that test classes are not often modified during the history of a software project on average five changes in days and on the other hand that a limited number of modifications is generally required to remove a test smell.
6on median commits and on average commits are performed in this time period.
we do not report these boxplots due to space constraints.
complete data are available in our online appendix .
ar et gf mg se1 500number of days log scale a apache ar et gf mg se1 500number of days log scale b eclipse ar et gf mg se1 500number of days log scale c overall figure distribution of number of days a test smell remained in the system before being removed.
ar et gf mg se1 20number of tcchanges log scale a apache ar et gf mg se1 20number of tcchanges log scale b eclipse ar et gf mg se1 20number of tcchanges log scale c overall figure distribution of number of modifications a smell remained in the system before being removed.
.
.
what is the percentage of fixed smell instances?
table shows the percentage of test smell instances fixed in apache eclipse and in the complete dataset.
in a conservative fashion we show the percentage of fixed instances considering all the change history and progressively removing censored instances i.e.
not fixed yet introduced too close to the end of the mined change history.
it is interesting to notice that the percentage of fixed instances is definitely small.
indeed only .
and .
of all test smell instances respectively in apache and eclipse are fixed .
overall .
during this analysis we noticed that in the eclipse ecosystem specific test smells appear to have higher fixing percentage .
and .
for sensitive equality and mystery guest respectively with respect to the apache ecosystem.
even when conservatively discarding instances too close to the end of the observable change history the percentage of fixed instances remains very small .
overall .
this result highlights the poor attention devoted by developers to the removal of test smells that however have been shown to hinder code comprehensibility and maintainability .
automated tools promptly recommending developers on how to refactor test smells could help in drastically increasing the percentage of fixed instances.
.
.
what is the survivability of test smells?
fig.
shows the survival curves in terms of number of days for the di erent test smell types grouped by ecosystems.
overall test smells have a very high survivability.
indeed after days the probability that a test smell survives i.e.
has not been fixed yet is .
after days the survival probability is still around .
fig.
reports instead the survivability of test smells when considering the number of commits.
again the survivability is very high with of probability that a test class is still a ected by a test smell after commits from its introduction.
having test smells lasting that long in the systems i further stresses the need for automatic detection tools and ii poses questions on the high maintenance costs the a ected test classes could have.
we plan to empirically investigate the latter point in our future work.
.
rq test and code smells table reports the results achieved when applying association rules mining to identify patterns of co occurrence between test and code smells.
we observed several interesting associations relating design flaws occurring in test and production code.
a clear case is the first rule shown in table associating clean test cases to clean production class.
this indicates that test classes do not a ected by test smells i.e.
clean test classes usually tests production classes do .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0number of dayssurvival probabilityaretgfmgse a apache .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0number of dayssurvival probabilityaretgfmgse b eclipse .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0number of dayssurvival probabilityaretgfmgse c overall figure survival probability of test smells with respect to the number of days .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0number of commitssurvival probabilityaretgfmgse a apache .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0number of commitssurvival probabilityaretgfmgse b eclipse .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0number of commitssurvival probabilityaretgfmgse c overall figure survival probability of test smells with respect to the number of commits .
table rq relationships between test and code smells.
id lhs rhs support confidence lift 1cleantc cleanpc .
.
.
ar sc bc .
.
.
3cdsbp et .
.
.
sc et .
.
.
5bc et .
.
.
cdsbp sc et .
.
.
7ar cdsbp et .
.
.
cdsbp gf bc .
.
.
9cdsbp mg ar .
.
.
ar cdsbp et .
.
.
11ar sc cc .
.
.
gf se ar .
.
.
13ar cc et gf sc bc .
.
.
gf se et .
.
.
15mg se ar .
.
.
bc se ar .
.
.
17bc se et .
.
.
not a ected by code smells i.e.
clean production classes .
eager test et in test classes is often associated with code smells in production code see rules and .
specifically in our dataset we found several cases in which three code smells namely class data should be private cdsbp spaghetti code sc and blob class bc cooccur with the eager test smell.
this result is quite reasonable if we consider the definitions of these smells.
indeed a cdsbp instance appears when a production class violates the information hiding principle by exposing its attributes and often co occur with code smells related to long or complex code such as sc and bc .
on the other hand aneager test appears when a test method checks more than a single method of the class to be tested .
thus it is reasonable to think that when testing large and complex classes developers tend to create more complex test methods exercising multiple methods of the tested class.
similar observations can explain the results achieved for the assertion roulette ar test smell rules and .
indeed this smell appears when a test case has several assertions with no explanation .
when testing complex code such as the one represented by a blob class more assertions which test the behavior of the production class might be needed.
the results of this analysis highlight some interesting findings that should be taken into account for building e ective tools for detecting code and test smells.
indeed on one side the presence of an eager test could indicate complex and long production code to test.
on the other side complex code to test is likely to trigger design issues in the related test code.
.
threats to validity threats to construct validity concern the relationship between theory and observation and are mainly related to the 12measurements we performed.
in particular such threats are related to the way we detect test smells and for rq code smells.
as explained in section .
we rely on the implementation of previously proposed approaches to identify test smells and code smells both exhibiting a reasonable precision in smell detection.
clearly we cannot ignore the fact that some smells have not been detected by the tools and while for the survey study we have manually validated the considered smells we are aware that for the second study the reported results could be a ected by tools imprecision.
another threat is related to the observation of the smells lifetime.
we consider a period of time since the first observable commit which however might not correspond to when a file has been created and this threat can a ect the results ofrq until the last observed snapshot the latter can introduce a threat in rq 2because smells could be removed in the future however survival analysis properly deals with censored data .
threats to internal validity concern factors internal to our study that we cannot control and that could have influenced the results.
above all the results observed in rq 2about smell survival might not only be due to the lack of awareness observed in the survey study but also to the lack of necessity to perform risky improvements to working test suites.
threats to external validity concern the generalization of our findings.
on one hand the results of the survey are clearly confined to the specificity of our respondents.
although the results are quite consistent it is possible that other developers might exhibit di erent levels of awareness about test smells.
while large the mining study surely needs to be extended to other projects beyond the open source ones that we considered which belong to two ecosystems .
also it is desirable to extend the study to other test smells beyond the five considered however the considered test smells are the most di used ones .
.
related work as well as production code test code should be designed following good programming practices .
during the last decade the research community spent a lot of e ort to define the methods and tools for detecting design flaws in production code as well as empirical studies aimed at assessing their impact on maintainability .
however design problems a ecting test code have been only partially explored.
the importance to have well designed test code has been originally highlighted by beck while van deursen et al.
defined a catalogue of test smells i.e.
a set of a poor design solutions to write tests together with refactoring operations aimed at removing them.
this catalogue takes into account di erent types of bad design choices made by developers during the implementation of test fixtures e.g.
setup method too generic where test methods only access a part of it or of single test cases e.g.
test methods checking several objects of the class to be tested .
besides the test smells defined by van deursen et al.
meszaros defined other smells a ecting test code .
starting from these catalogues greiler et al.
showed that test smells related to fixture set up frequently occur in industrial projects and therefore presented a static analysis tool namely testhound to identify fixture related test smells.
van rompaey et al.
proposed a heuristic struc tural metric based approach to identify general fixture and eager test instances.
however the results of an empirical study demonstrated that structural metrics have lower accuracy while detecting these test smells.
bavota et al.
conducted an empirical investigation in order to study i the di usion of test smells in software projects and ii their e ects on software maintenance.
the results of the study demonstrated that of junit classes in their dataset are a ected by at least one test smell and that the presence of design flaws has a strong negative impact on the maintainability of the a ected classes.
.
conclusion this paper presented i a survey with developers aimed at investigating their perception of test smells as design issues and ii a large scale empirical study conducted over the commit history of open source projects and aimed at understanding when test smells are introduced what their longevity is and whether they have relationships with code smells a ecting the tested production code classes.
the achieved results provide several valuable findings for the research community lesson .test smells are not perceived by developers as actual design problems.
our survey with original developers of five systems showed that developers are not able to identify the presence of test smells in their code.
however recent studies empirically highlighted the negative e ect of test smells on code comprehensibility and maintainability .
this highlights the importance of investing e ort in the development of tools to identify and refactor test smells.
lesson .in most cases test artifacts are a ected by bad smells since their creation .
this result contradicts the common wisdom that test smells are generally due to a negative e ect of software evolution and it is inline with what observed for code smells .
also this finding highlights that the introduction of most smells can simply be avoided by performing quality checks at commit time or even while the code is written in the ide by recommending the developer how to stay away from bad design practices i.e.
justin time refactoring .
tools supporting these quality checks could avoid or at least limit the introduction of test smells.
lesson .test smells have a very high survivability .
this result further stresses the fact that developers are not catching such problems in the design of their test code.
this might be due to i the limited time dedicated to refactoring activities ii the unavailability of test smells refactoring tools or as shown in our survey iii the fact that developers are not perceiving test smells as bad design practices.
the reason behind this result must be further investigated in the future work.
lesson .there exist relationships between smells in test code and the ones in the tested production code .
given the di erent nature of test and code smells we found this result to be quite surprising.
still knowing the existence of these relationships could definitively help in better managing both types of smells by using the presence of test smells as an alarm bell for the possible presence of code smells in the tested classes and vice versa .
these lessons learned represent the main input for our future research agenda on the topic mainly focused on designing and developing a new generation of code qualitycheckers such as those described in lesson .
.