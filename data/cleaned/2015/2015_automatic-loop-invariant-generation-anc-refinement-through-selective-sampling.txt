automatic loop invariant generation and refinement through selective sampling jiaying li1 jun sun1 li li1 quang loc le2and shang wei lin3 1singapore university of technology and design singapore 2school of computing teesside university united kingdom 3school of computer science and engineering nanyang technological university singapore abstract automatic loop invariant generation is important in program analysis and verification.
in this paper we propose to generate loop invariants automatically through learning and verification.
given a hoare triple of a program containing a loop we start with randomly testing the program collect program states at run time and categorize them based on whether they satisfy the invariant to be discovered.
next classification techniques are employed to generate a candidate loop invariant automatically.
afterwards we refine the candidate through selective sampling so as to overcome the lack of sufficient test cases.
only after a candidate invariant cannot be improved further through selective sampling we verify whether it can be used to prove the hoare triple.
if it cannot the generated counterexamples are added as new tests and we repeat the above process.
furthermore we show that by introducing a path sensitive learning i.e.
partitioning the program states according to program locations they visit and classifying each partition separately we are able to learn disjunctive loop invariants.
in order to evaluate our idea a prototype tool has been developed and the experiment results show that our approach complements existing approaches.
index terms loop invariant program verification classification active learning selective sampling i. i ntroduction automatic loop invariant generation is fundamental for program analysis.
a loop invariant can be useful for software verification compiler optimization program understanding etc.
in the following we first define the loop invariant generation problem review existing approaches and then briefly introduce our proposal.
without loss of generality we assume that we are given a hoare triple in the following form.
fpreg ?assumption ?
while cond fbodyg ?loop body?
fpostg ?assertion?
assume that v fx1 x2 xngis a finite set of program variables which are relevant to the loop body.
pre cond and post are predicates constituted by variables in v. lets fx17!v1 xn7!vngbe a valuation of v. let be a predicate constituted by variables in v. is viewed as the set of valuations of vsuch that evaluates to true given the valuation.
we thus write s2 to denote that is evaluated to true givens.
otherwise we write s62 .body is an imperative program that updates the valuation of v. for simplicity we assume that it is a deterministic function1on valuations of 1our approach works as long as the non determinism in body orcond is irrelevant to whether the postcondition is satisfied or not.variablesv and write body s to denote the valuation of vafter executing body given the variable valuation s. for convenience bodyi s wherei 0is defined as follows body0 s sandbodyi s body bodyi s .
the goal is to either prove or disprove the hoare triple.
to prove it we would like to find a loop invariant inv which satisfies the following three conditions.
pre inv 8s s2inv cond body s 2inv inv cond post to disprove it we would like to find a valuation ssuch that s2pre and executing the loop until it terminates results in a valuations0such thats062post .
for simplicity we assume that the loop always terminates and refer the readers to for research on proving loop termination.
loop invariant generation is a long standing problem.
many approaches have been proposed to solve this problem .
these approaches all rely on some form of constraint solving and often suffer from scalability issues.
recently a number of guess and check approaches have been proposed.
these approaches start with generating a set of valuations of v a.k.a.
the samples and categorize them into different groups e.g.
one containing those satisfying the loop invariant and another containing those not.
learning techniques are then applied to generalize the valuations in a certain form to guess candidate loop invariants.
the candidates are then check ed using program verification techniques like symbolic execution to see whether they satisfy the three conditions.
if any of the conditions is violated counterexamples in the form of variable valuations can be obtained.
for instance given a candidate loop invariant if condition is violated a valuations2 pre is generated which proves that is not an invariant.
with this sample s we can learn a new candidate invariant.
this guess and check process is repeated until the hoare triple is either proved or disproved.
existing guess and check approaches vary in how samples are generated and candidate invariants are guessed.
we refer the readers to section v for a detailed discussion.
a common problem with these guess and check approaches is that their effectiveness is often limited by the samples generated in their first phases.
in order to guess the right invariant often a large .
c ieeease urbana champaign il usa technical research782 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
number of samples are necessary.
if classification techniques are employed often those samples right by the boundary between variable valuations which satisfy the actual invariant and those which do not must be sampled so that classification techniques would identify the right invariant.
obtaining those samples through random sampling is however often hard.
as a result many iterations of guess and check are required.
another problem is that the kinds of loop invariants obtained through existing guess and check approaches are often limited e.g.
conjunctive linear inequalities or equalities .
despite the approaches presented in learning disjunctive loop invariants remains a challenge.
our contribution in this work we propose a technique to improve the existing guess and check approaches by making the following contributions.
firstly we propose an active learning technique known as selective sampling to overcome the limitation of random sampling.
that is selective sampling allows us to automatically generate samples which are important in improving the quality of the candidate invariants so that we can improve the candidates prior to checking them using heavy program verification techniques.
as a result we can reduce the number of guess and check iterations.
secondly we propose to generate disjunctive invariants through path sensitive learning.
that is we partition the samples according to the control locations they visit classify each partition separately and construct a disjunction of the learned results for each partition as the loop invariant.
thirdly our approach is designed to be extensible so that we can learn different kinds of invariants.
for instance we generate candidate invariants in the form of polynomial inequalities or their conjunctions when different classification algorithms are adapted.
lastly we implement our framework as a tool called z ilu available at and compare it with state of the art tools like interproc cpachecker invgen and blast .
most test subjects are gathered from previous collections as well as the software verification repository .
the results show that for those programs that are compatible with zilu s input restrictions z iluis able to prove the maximum number of programs.
furthermore it is shown that selective sampling is able to reduce the need for checking sometimes completely.
organization the remainders of the paper are organized as follows.
section ii presents an overview of our approach using simple illustrative examples.
section iii shows how candidate loop invariants are generated through classification and refined through selective sampling.
section iv evaluates our approach using a set of benchmark programs.
section v reviews related work and section vi concludes.
ii.
t heoverall approach through this paper loop invariant generation using a guessand check approach is an iterative process of data collection guessing i.e.
classification in this work and checking i.e.
verification of the invariant candidate .
in the following wepresent how our approach works step by step and illustrate each step with simple examples.
example .
four hoare triple examples are shown in fig where an assume statement captures the precondition and an assert statement captures the postcondition.
the set vfor each program contains two integer variables xandy.
for simplicity we write a b whereaandbare integer constants to denote the evaluation fx7!a y7!bg.
furthermore we interpret integers in the programs as mathematical integers i.e.
they do not overflow .
one example invariant which can be used to prove the hoare triple is shown for each program.
for instance the hoare triple shown in fig.
a can be proven using a loop invariant x y whereas conjunctive or disjunctive invariants are necessary to prove the rest of the hoare triples.
we remark that there might be different loopinvariants which could be used to prove the hoare triples.
in the following we show how we generate loop invariants for proving these hoare triples.
our overall approach is shown in algorithm .
we start with randomly generating a set of valuations of v denoted as sp at line a.k.a.
random sampling .
random sampling provides us an initial set of samples to learn the very first candidate for the loop invariant.
in this work we have two ways to generate random samples.
one is that we generate random values for each variable in vbased on its domain assuming a uniform probabilistic distribution over all values in its domain.
the other is that we apply an smt solver to generate valuations that satisfy pre as well as those that fail pre.
these two ways are complementary.
on one hand without using a solver we may not be able to generate valuations which satisfy pre ifpre is very restrictive or fail pre if the negation of pre is very restrictive .
on the other hand using a solver often generates biased valuations.
next for any valuation sinsp we execute the program starting with initial variable valuation sand record the valuation ofvafter each iteration of the loop.
we write s s0 to denote that there exists i 0such thats0 bodyi s andbodyk s 2cond for allk2 i .
that is if we start with valuation s we obtain s0after some number of iterations.
at line of algorithm we add all such valuations s0intosp.
next we categorize spinto four disjoint sets ce positive negative andnp.
intuitively ce contains counterexamples which disprove the hoare triple positive contains those valuations of vwhich we know must satisfy any loop invariant which proves the hoare triple negative contains those valuations of vwhich we know must not satisfy any loop invariant which proves the hoare triple and np contains the rest.
formally ce sp fs2spj9s0 s0 s02pre s0 s s0 s062cond s062postg a valuation since sp starts from a valuation s0which satisfiespre and becomes a valuation s0which fails post authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
1assume x y 2while x y f if x x x else x x if y y y else y y 7g 8assert y x y a invariant x y 161assume x 0 y 2while x y f if x f x x 5gelsef y y 7g 8g 9assert x 0 y b invariant x 0 y 01assume x y 2while f x x y y y 5g 6assert x y c invariant y x y1assume x 2while x f x x y y 5g 6assert y d invariant x 0 y fig.
example programs when the loop terminates.
if ce sp is non empty the hoare triple is disproved.
positive sp fs2spj9s0 s0 s02pre s0 s s0 s062cond s02postg positive sp contains a valuation sif there exists a valuation s0inspwhich satisfies pre and becomes safter zero or more iterations.
furthermore ssubsequently becomes s0 which satisfiespost when the loop terminates.
let inv be any loopinvariant that proves the hoare triple.
because s02pre s02inv sinceinv satisfies condition .
since inv satisfies condition and body s0 2inv ifbody s0 2cond .
by a simple induction we prove s2inv.
negative sp fs2spj9s0 s0 s062pre s0 s s0 s062cond s062postg negative sp is a valuation swhich starts from a valuation s0 violatingpre and becomes a valuation s0which violates post when the loop terminates.
we show that s62inv for allinv satisfying condition and .
assume that s2inv by condition s0must satisfy inv through a simple induction.
by condition s0must satisfy post which contradicts the definition of negative sp .
np sp sp ce sp positive sp negative sp np sp contains the rest of the samples.
we remark that a valuationsinnp sp may or may not satisfy an invariant inv which satisfies condition and .
example .
take the program shown in fig.
a as an example.
assume that the following three valuations are randomly generated and at line .
three sequences of valuations are generated after executing the program with these three valuations h i h iandh i respectively.
note that the loop is skipped entirely for the latter two cases.
after categorization set ce sp is empty positive sp isf g negative sp isf g andnp sp isf g. after obtaining the samples and labeling them as discussed above method activelearn sp at line in algorithm 1algorithm algorithmverify 1letspbe a set of randomly generated valuations of v 2while not time out do add all valuations s0such thats s0for some s2spintosp callactivelearn sp to generate a candidate invariant return proved if the program is verified with otherwise add the counterexample into sp is invoked to generate a candidate invariant .
we leave the details on how candidate invariants are generated in section iii which is our main contribution in this work.
once a candidate is identified we move on to check whether satisfies condition and at line .
in particular we check whether any of the following constraints is satisfiable or not using an smt solver .
pre sp cond body cond post wheresp cond body is the strongest postcondition obtained by symbolically executing program body starting from precondition cond .
if all the three constraints are unsatisfiable we successfully prove the hoare triple with the loop invariant .
if any of the constraints is satisfiable a model in the form of a variable valuation is generated which is then added to spas a new sample.
afterwards we restart from line i.e.
we execute the program with the counterexample valuations collect and add the variable valuations after each iteration of the loop to the four categories accordingly move on to active learning and so on.
example .
for the example shown in fig.
a a candidate invariant which is automatically learned is x y .
it is easy to check that this candidate satisfies all the three conditions and thus the hoare triple shown in fig.
a is proved.
for fig.
c a candidate invariant returned by method activelearn sp is as follows.
16x 9y 6x 29y y 2x 5y authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm algorithmactivelearn sp 1while true do if ce sp is not empty exit and report disproved let be a set of candidates generated by classify sp if is the same as last iteration return addselectivesampling intosp add all valuations s0such thats s0for some s2spintosp a counterexample is generated when we check the satisfiability of which is then used to generate a new candidate.
after multiple iterations of guess and check the following invariant is generated.
y x y x different from the invariant in fig.
d this candiate still succeeds in proving the given hoare triple.
thus the loopinvariant is found.
iii.
o urapproach classification active learning and selective sampling in this section we present details on how candidate invariants are generated.
algorithm shows how activelearn sp is implemented in general i.e.
it iteratively generates a candidate through classification at line and improves it through selective sampling at line until a fixed point is reached.
note that once a counterexample is identified at line it exits and reports that the hoare triple is disproved.
the method call classify sp in algorithm generates a candidate invariant based on classification techniques.
intuitively since we know that valuations in positive sp must satisfyinv and valuations in negative sp must not satisfy inv a predicate separating the two sets a.k.a.
a classifier may be a candidate invariant.
in the following we fix two disjoint sets of samples pandnand discuss how to automatically generate classifiers separating pandn.
for now pcan be understood as positive sp andncan be understood as negative sp .
we discuss alternatives in section iii d. to automatically generate classifiers separating pandn we apply existing classification techniques.
there are many classification algorithms e.g.
.
in our approach the classification algorithms must generate perfect classifiers.
formally a perfect classifier forpandnis a predicate such thats2 for alls2pands62 for alls2n.
furthermore the classifier must be human interpretable or can be handled by existing program verification techniques.
in the following we first briefly discuss how to generate conjunctive invariants using the approach proposed in and then propose a path sensitive approach to generate disjunctive invariants.
afterwards we show how to improve candidate invariants systematically through selective sampling.a.
conjunctive invariants in the following we show how to generate loop invariants in the form kwhere each iis a polynomial inequality up to certain degree constituted by variables in v. our approach is based on support vector machines svm .
svm is a supervised machine learning algorithm for classification and regression analysis .
in general the binary classification functionality of svm works as follows.
given p andn svm can generate a perfect classifier to separate them if there is any.
we refer the readers to for details on how the classifier is computed.
in this work we always choose the optimal margin classifier if possible.
intuitively the optimal margin classifier could be seen as the strongest witness why pandnare different.
svm by default learns classifiers in the form of a linear inequality i.e.
a half space in the form ofc1x1 c2x2 kwherexiare variables in vandci are constant coefficients.
we can easily extend svm to learn polynomial classifiers.
givenpandnas well as a maximum degree dof the polynomial classifier we can systematically map all the samples inp similarlyn to a set of samples p0 similarlyn0 in a high dimensional space by expanding each sample with terms which have a degree up to d. for instance assume that the maximum degree is the sample valuation fx7!
y7!1g inpis mapped tofx7!
y7!
x27!
xy7!
y27!1g.
svm is then applied to learn a perfect linear classifier for p0andn0.
mathematically a linear classifier in the high dimensional space is the same as a polynomial classifier in the original space .
note that the size of each sample in p0 orn0grows rapidly with the increase of the degree and thus the above method is limited to polynomial classifiers with a relatively low degree.
a polynomial classifier can represent some classifiers in the form of disjunctive or conjunctive linear inequalities.
for instance the classifier x d0 x d1 x d2 where d0 d d 2are constants can be represented equivalently as the following polynomial inequality.
x3 d0d1 d0d2 d1d2 x2 d0 d1 d2 x d0d1d2 however this representation transformation is not always possible i.e.
some conjunctive or disjunctive linear inequalities cannot be expressed as a polynomial classifier.
one typical example is x y .
to generate conjunctive classifiers we adopt the algorithm svm i proposed in .
the idea is to pick one sample s fromneach time and identify a classifier iin the form of a polynomial inequality to separate pandfsg remove all samples from nwhich can be correctly classified by i and then repeat the process until nbecomes empty.
the conjunction of all the classifiers iis then a perfect classifier separatingpandn.
we refer the readers to for details of the algorithm.
we remark that if we switch pandn the negation of the learned classifier using this algorithm is in the form of a disjunction of polynomial inequalities.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
b. disjunctive invariants it is often challenging to automatically generate disjunctive invariants whereas certain hoare triples can only be proved with disjunctive invariants.
two examples are shown in fig.
b and fig.
d .
in the following we show one way to learn disjunctive invariants i.e.
invariants in the general form of 1 2 m where each i i i i nis a conjunctive polynomial inequality.
our observation is that disjunctive invariants are often required to prove certain hoare tripe because the program contains branching commands i.e.
if and while .
for instance proving the hoare triple shown in fig.
b requires a disjunctive loop invariant which is largely due to the branch at line .
based on this observation we propose to learn disjunctive invariants through path sensitive classification.
without loss of generality we assume that the loop bodybody can be modeled as a transition system c init end t l .cis a finite set of control locations.
init2cis a unique entry point i.e.
the start of the program .
end2cis a unique exit point i.e.
the end of the program which is assumed to be always reachable .
t c!cis a transition function2which captures the control flow.
lastly lis a labeling function which labels each transition with a pair g f wheregis a guard condition and fis a function updating variable valuation.
note that gis used to model branching conditions whereas fis used to model program statements like assignments.
for instance the loop body in the first program in fig.
b can be modeled as a transition system with four control locations representing line and and the transition from the control location representing line to the one representing line is labeled with a guard conditionx 0and a function which does not change any variable valuation.
given a valuation sofvsatisfying the loop condition cond we can obtain a unique path through the program path s hc1 c2 ckiwhereci2cfor allisuch thatc1 init ck end and every guard condition along the path is satisfied.
for instance given the loop body in fig.
b and valuation fx7!
y7!
3g the unique path is h3 6i.
ifsviolates the loop condition cond we setpath s to be an empty sequence.
intuitively path s is the set of sequence of control locations visited bysin one iteration of the loop.
our path sensitive classification starts with partitioning p into a set of disjoint partitions such that for each partition pi path s path s0 for all valuation sands0inpi.
for eachpi we can construct a unique path condition pci i.e.
a formula over the symbolic variables in vand the accumulated constraints which the symbolic variables must satisfy in order for an execution to follow the corresponding path.
for instance given the program shown in fig.
b if pis set to be positive sp we have three partitions.
the first one contains 2it is a function as we assume body is deterministic.all valuations swithpath s beingh3 4iwhose path condition isx y x the second one contains all valuationsswithpath s beingh3 6iwhose path condition isx y x 0and the last one contains all valuations swithpath s beinghiwhose path condition is x y .
next we apply the approach presented in section iii a to learn a conjunctive classifier for each partition pi i.e.
we learn a classifier ifor separating pifromni.
then the disjunctionw i i pci is a perfect classifier separating pfromn.
since iis a conjunctive predicate we learn candidate invariants in the form of disjunction of conjunction of polynomial inequalities.
example .
though the program shown in fig.
d contains noifcommand variable valuations in positive sp can be partitioned into two partitions according to our definition one containing those visit line and the other containing those skipping the loop.
in the following we show how to learn a disjunctive loop invariant based on these two partitions.
note that a valuation sis innegative sp only ifs2 y x .
if we have every valuation of vfor these two partitions a classifier we could learn for the former partition is x i.e.
a valuation must satisfy the invariant if it enters the loop and the classifier we learn for the latter partition is y .
as a result conjuncted with the path condition we learn the candidate invariant x x y x which can be simplified as x 0 y 0and proves the hoare triple.
we remark that in the above discussion we assume that we can obtain every variable valuation which is often infeasible in practice as there are too many of them.
in the following subsection we aim to solve this problem.
c. active learning and selective sampling one fundamental problem with applying machine learning techniques to learn loop invariants is that we often have only a limited set of samples.
that is with the limited samples inpositive sp andnegative sp it is unlikely that we can obtain an accurate classifier.
for instance as shown in example positive sp isf gand negative sp isf g. a linear classifier identified using svm for this example is 3x 10y .
although this classifier perfectly separates the two sets it is not useful in proving the hoare triple and is clearly the result of having limited samples.
one obvious way to overcome this problem is to generate more samples.
however often a large number of samples are necessary in order to learn the correct classifier.
one particular reason is that we often need the samples right on the classification boundary in order to learn the correct classifier which are often difficult to obtain through random sampling.
in existing guess and check approaches the problem is overcome by checking whether the candidate invariant proves the hoare triple through program verification.
that is new samples are obtained from counterexamples generated by the program verification engine which are then used to refine the classifier.
the issue is that authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
often many iterations of guess and check are required before the invariant would converge to the correct one.
researchers in the machine learning community have studied extensively on how to overcome the problem of limited samples.
one of the remedies is active learning .
active learning is proposed in contrast to passive learning.
a passive learner learns from a given set of samples that it has no control over whereas an active learner actively selects what samples to learn from.
it has been shown that an active learner can sometimes achieve good performance using far fewer samples than would otherwise be required by a passive learner .
active learning can be applied for classification or regression.
in this work we apply it for improving the candidate invariants generated by the above discussed classification algorithms.
a number of different active learning strategies on how to select the samples have been proposed.
for instance version space partitioning tries to select samples on which there is maximal disagreement between classifiers in the current version space e.g.
the space of all classifiers which are consistent with the given samples uncertainty sampling maintains an explicit model of uncertainty and selects the sample that it is least confident about.
the effectiveness of these strategies can be measured in terms of the labeling cost i.e.
the number of labeled samples needed in order to learn a classifier which has a classification error bounded by some threshold .
for some classification algorithms it has been shown that active learning reduces the labeling cost from to the optimal o dlg1 wheredis the dimension of the samples .
that is if passive learning requires a million samples active learning may require just lg to achieve the same accuracy.
in this work we adopt the active learning strategy for svm proposed in called selective sampling to improve the invariant candidates.
this strategy has been shown to be effective in achieving a high accuracy with fewer examples in different applications .
in particular at line of algorithm after obtaining a classifier based on existing samples in sp we apply method selectivesampling to selectively generate new samples.
it works by generating multiple samples on the current classification boundary .
afterwards the samples are added into spat line and and we repeat from line until the classifier converges.
the implementation of selectivesampling depends on the type of classifiers.
for classifiers in the form of linear inequalities identifying samples on the classification boundary is straightforward i.e.
by solving an equation.
in the above example given the current classifier 3x 10y we apply selective sampling and generate new valuations and by solving the equation 3x 10y .
for classifiers in the form of polynomial inequalities the problem is more complicated since existing solvers for multi variable polynomial equations have limited scalability.
we thus use a simple approach to identify solutions of a polynomial equation which we illustrate through an example in the following.
assume that we learn the classifier 4x2 2y .
the following steps are applied for selective sampling.
choose a variable in the classifier e.g.
x. generates random value for all other variables.
for example we let ybe12.
substitute the variables in the classifiers with the generated values and solve the univariable equation e.g.
4x2 .
if there is no solution go back to and retry.
in our example x .
roundoff the values of all the variables according to their types in the program.
in our example we obtain the valuation .
in the case that a conjunctive or disjunctive classifier is learned we apply above selective sampling approach to every clause in the classifier to obtain new samples.
with the help of active learning and selective sampling we can often reduce the number of learn and check iterations.
as the empirical studies shown in section iv one iteration of guess and check is sufficient in some cases to prove the hoare triple.
advantages of selective sampling in the following we briefly discuss why selective sampling is helpful from a high level point of view.
in this work we collect samples in three different ways.
firstly random sampling provides us an initial set of samples.
the cost of generating a random sample is often low.
however we often need a huge number of random samples in order to learn accurately.
secondly selective sampling has a slightly higher cost as it requires us to solve some equation system.
however it has been shown that selective sampling is often beneficial compared to random sampling .
the last way of sampling is sampling through verification.
when a candidate invariant fails any of the three conditions and in the candidate verification stage the verifier provides counter examples which are added as new samples.
sampling through verification provides useful new samples by paying a high cost.
furthermore for complex programs sampling through verification may not be feasible due to the limited capability of existing program verification techniques.
thus in this work our approach is to start with random sampling use selective sampling to improve the classifier as much as possible and apply sampling through verification only as the last resort.
fig.
visualizes how different sampling methods work in a d plane.
we start with the figure in the top left corner where the dots are the samples obtained through random sampling.
the green area above the line represents the space covered by the actual invariant.
based on these samples a classifier shown as the red line is learnt to separate the random samples as shown in the top right figure.
selective sampling allows us to identify those samples along the classification boundary as shown in the bottom left figure.
in comparison sampling through verification would provide us a sample between the two lines as shown in the bottom left figure.
the classifier will be improved by either selective sampling or sampling through verification as shown in the bottom right figure.
the benefit of always applying selective sampling before applying sampling through verification is that sampling through verification is often costly or even worse not available due to the limitation authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
randomsamplingactualinvariant actualinvariantlearnedinvariant selectivesamplingactualinvariantlearnedinvariantsampling throughverification actualinvariantnewinvariant fig.
sampling approaches of existing program verification techniques.
thus we would like to avoid it as much as possible.
d. making use of undetermined samples so far we have focused on learning and refining classifiers between positive sp andnegative sp as candidate invariants.
the question is then how do we handle those valuations innp sp ?
if we simply ignore them there may be a gap between positive sp andnegative sp and as a result the learnt classifier may not converge to the invariant we want even with the help of active learning.
this is illustrated in fig.
where the set of valuations in positive sp marked with negative sp marked with and np sp marked with ?
for the example in fig.
a are visualized in a d plane.
many samples between the line x yandx y may be contained in np sp .
as a result without considering the samples in np sp a classifier located in the np sp region e.g.
x y orx y may be learned to perfectly classify positive sp andnegative sp .
worse identifying more samples may not be helpful in improving the classifier if the new samples are in np sp .
to solve the problem in addition to learn a classifier separating positive sp andnegative sp we learn candidate invariants making use of np sp .
in principle we should enumerate all the possible categorization of the samples in np sp and run classification algorithm on each of them.
however at most time it is very time consuming and instead we only try two extreme case in our implementation which is far from perfect and will be refined in the future.
in our current setting we learn classifiers separating positive sp from negative sp np sp i.e.
assuming valuations in np sp fail the actual invariant and classifiers separating negative sp from positive sp np sp i.e.
assuming valuations in np satisfy the actual invariant .
for the example in fig.
a if we focus on classifiers in the form of linear inequalities the classifier separating positive sp from the 16x y x y ?
?
??
?
?
?
?
?xyfig.
samples visualization rest converges to null no such classifier whereas the classifier separating negative sp from the rest converges tox y which can be used to prove the hoare triple.
note that this is orthogonal to which classification algorithm is used and whether selective sampling is applied.
iv.
i mplementation and evaluation we have implemented our approach for loop invariant generation in a tool called z ilu available at .
for candidateinvariant verification we modify the klee project to symbolically execute c programs prior to invoking z3 for checking satisfiability of condition and .
we remark that as a concolic testing engine klee may concretely execute the programs and return under approximated abstraction.
this may affect the soundness of our system.
to overcome this problem we detect those path conditions produced from concrete executions and return a sound abstraction i.e.
true .
our evaluation subjects include a set of c programs gathered from multiple resources such as previous publications e.g.
and the software verification competitions sv comp .
we remark that the loops in these benchmark programs often contain authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
non deterministic choices which are often used to model i o environment e.g.
an external function call .
as nondeterminism is beyond the scope of this work in general we manually examine each program to check whether our assumption is satisfied or not i.e.
whether the non determinism is relevant in satisfying the post condition or not.
only those programs which do not satisfy our assumption are excluded from our experiments.
for those which do satisfy our assumption we replace those non determinism with random free boolean variables.
in total out of the benchmark programs we gathered programs are excluded as they do not satisfy their specification programs are excluded as they do not have non trivial precondition or postcondition or the loop body contains unsupported constructs like break or goto statement are excluded as they contain unsupported operations such as array operation are excluded due to multiple loops and are excluded due to non determinism.
we also exclude programs which are trivial to prove and copies of the same program.
furthermore we construct programs benchmarks in the table due to lack of programs requiring polynomial or disjunctive invariants in these benchmarks.
all programs are available at .
the parameters in our experiments are set as follows.
for random sampling we generate random values for every input variable of a program from their default ranges.
during selective sampling we generate values for every input variable along the classification boundary.
the ratio between random samples and selective samples is thus which we consider to be reasonable as selective sampling is slightly more costly.
when we invoke libsvm for classification the parameter c which controls the trade off between avoiding misclassifying training examples and enlarging decision boundary and the inner iteration for svm learning are set to their maximum value so that it generates only perfect classifiers.
during candidate verification integer type variables in programs are encoded as integers in z3 not as bit vectors .
since we have different ways of setting the samples for classification e.g.
by setting the two sets of samples pandndifferently as discussed in section iii d and different classification algorithms linear vs. polynomial or conjunctive vs. disjunctive we simultaneously try all combinations and terminate as soon as either the hoare triple is proved or disproved.
for polynomial inequalities the maximum degree is bounded by .
in order to give priority to simpler invariants we look for a polynomial classifier with degree donly if we cannot find any polynomial classifier with lower degree.
all of the experiments are conducted using x64 ubuntu .
.
kernel .
.
59generic with .
ghz intel core i7 and 32g ddr3.
each experiment is executed five times since there is randomness in our approach and we report the median as the result.
in order to evaluate our approach in learning loop invariants we investigate the following research questions rq1 the first research question which we would like to answer is does selective sampling help to reduce the number of guessand check iterations?
we compare z ilu with and without selective sampling.
the results are summarized in table i. thefirst column shows the number of the program the second column shows the type of loop invariant needed to prove the hoare triple and the next six columns show details on verifying the hoare triple with and without selective sampling.
we compare the total number of samples generated and the number of guess and check iterations.
to reduce randomness the same set of initial random samples are used in both settings.
each experiment has a time limit of minutes.
the winner of each measurement is highlighted boldly.
the results show that z ilusuccessfully verifies all programs with help of selective sampling and fails to verify programs without selective sampling.
for these programs z ilutimeouts due to too many guess and check iterations.
this clearly evidences the usefulness of selective sampling.
furthermore in cases selective sampling helps to reduce the number of guess and check iterations.
though it rarely happens due to the randomness in our approach it may happen that the right invariant is learned by luck with fewer samples.
this happens in cases i.e.
where z iluwithout selective sampling has fewer by or iterations.
we would like to highlight that for programs ziluis able to learn the correct invariant within one guess and check iteration with selective sampling.
it is never the case without selective sampling.
furthermore it happens when the invariant is a linear inequality.
we remark that being able to learn the correct invariant without program verification is useful for handling complex programs.
that is even if we are unable to automatically verify the generated invariant due to the limitation of existing program verification techniques z ilu s result is still useful in these cases as the generated invariant can be used to manually verify the program.
in addition we observe that z ilu often takes more samples and guess and check iterations to learn conjunctive or disjunctive invariants.
on average z ilutakes .
.
and .
guess and check iterations to learn linear polynomial conjunctive and disjunctive loop invariants.
for conjunctive invariants more iterations are needed because the algorithm adopted from for learning conjunctive classifiers often requires more samples before convergence.
for disjunctive invariants this is because we need sufficient samples in each partition in order to learn the right invariant.
rq2 the second research question which we would like to answer is does selective sampling incur significant overhead?
this is a valid question as selective sampling requires solving equation systems.
in table i we show the total execution time of both z iluwith and without selective sampling.
it can be observed that the overhead of selective sampling is reasonable.
all programs are verified with minutes.
a close look reveals that most of the time is spent on classification and selective sampling.
for programs the overall time is reduced with selective sampling due to a reduced number of guess and check iterations.
z ilu often takes more time to learn conjunctive polynomial or disjunctive invariants.
this is because in such a case svm classification is invoked many times in one guess and check authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i e xperimental results ziluwith selective sampling ziluwithout selective sampling benchmark type sample iter time s sample iter time s interproc cpachecker blast invgen benchmark01 conjunctive .
.
.
benchmark02 linear .
.
.
benchmark03 linear .
.
.
benchmark04 conjunctive .
.
.
benchmark05 conjunctive .
.
.
benchmark06 conjunctive .
.
.
benchmark07 linear .
to to to benchmark08 linear .
.
to benchmark09 conjunctive .
.
.
benchmark10 conjunctive .
to to to .
benchmark11 linear .
.
.
benchmark12 linear .
.
.
benchmark13 conjunctive .
.
.
benchmark14 linear .
.
.
benchmark15 conjunctive .
to to to .
benchmark16 conjunctive .
.
.
benchmark17 conjunctive .
.
.
benchmark18 conjunctive .
.
.
benchmark19 conjunctive .
.
.
benchmark20 conjunctive .
.
benchmark21 disjunctive .
to to to .
benchmark22 conjunctive .
.
.
benchmark23 conjunctive .
to to to .
to benchmark24 conjunctive .
.
.
benchmark25 linear .
.
.
benchmark26 linear .
.
.
benchmark27 linear .
.
.
benchmark28 linear .
.
benchmark29 linear .
.
.
benchmark30 conjunctive .
.
.
benchmark31 disjunctive .
.
.
benchmark32 linear .
.
.
benchmark33 linear .
.
.
benchmark34 conjunctive .
.
.
benchmark35 linear .
.
.
benchmark36 conjunctive .
.
.
benchmark37 conjunctive .
.
.
benchmark38 conjunctive .
.
.
benchmark39 conjunctive .
.
.
benchmark30 polynomial .
.
benchmark41 conjunctive .
.
.
benchmark42 conjunctive .
to to to .
benchmark43 conjunctive .
to to to .
benchmark44 disjunctive .
.
.
benchmark45 disjunctive .
.
.
benchmark46 disjunctive .
.
.
benchmark47 linear .
to to to .
benchmark48 linear .
.
.
benchmark49 linear .
.
.
benchmark50 linear .
.
.
benchmark51 polynomial .
.
benchmark52 polynomial .
to to to benchmark53 polynomial .
iteration.
comparing the number of samples generated for each program it can be observed that z ilu with selective sampling often generates more samples.
this is expected as we generate multiple samples for each call of selectivesampling whereas only one sample is generated during the verification phase.
this is because sampling through verification has in general a high cost and we aim to avoid it as much as possible.
rq3 the third research question is does ziluoutperform existing state of the art program verification tools on verifying these programs?
ideally we would like to compare with thosetools reported in .
unfortunately those tools are not maintained.
we instead compare z iluwith four state of the art tools on loop invariant generation and program verification.
in particular interproc is a program verifier which generates invariants based on abstract interpretation.
in the experiments it is set to use its most expressive abstract domain i.e.
the reduced product of polyhedra and linear congruences abstraction.
cpachecker is a state ofthe art program verifier.
the cpachecker which we use in this work is the version used for sv comp .
note that cpachecker supports a variety of verification methods and it authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
is configured in the exact same way as in sv comp .
blast is a software model checker based on counterexampleguided abstraction refinement .
lastly invgen is a tool which aims to generate linear arithmetic invariants using a combination of static and dynamic analysis techniques.
the results are shown in the last columns of table i where 3means that the hoare triple is verified and 7means either it outputs no conclusive results or false positives.
we remark that because the tools use approaches which are different from each other the comparison should be taken with a grain of salt.
interproc and invgen are very efficient in handling the programs i.e.
within second for each program and thus we skip the verification time.
blast is similarly efficient except that it timeouts in two cases.
we show the timed taken by cpachecker in case it successfully verifies the program.
we have the following observation based on the experiment results.
first for all programs z ilu is able to find a loop invariant which proves the hoare triple.
in comparison interproc failed in cases cpachecker failed in cases blast failed in cases and invgen failed in cases.
we note that this comparison might be not fair as we have not compared other tools on the excluded programs.
more precisely for the programs that are compatible with input restrictions of z ilu ziluworks best compared with other tools.
secondly existing tools often complement each other.
for instance blast successfully proves all programs which require disjunctive loop invariant whereas it failed in several cases where a polynomial loop invariant is required.
in contrast programs which require disjunctive loop invariants are often challenging for other tools except z ilu .
moreover due to the dynamic nature z iluoften requires more time.
nonetheless we consider that z iluis relatively efficient.
for all programs zilufinishes the proof within seconds.
v. r elated work the closest related work are those guess and check approaches on invariant generation.
in the authors proposed to generate samples through constraint solving and learn loopinvariants based on svm classification.
in comparison z ilu learns more expressive invariants in the form of polynomial inequalities or their disjunctions and conjunctions.
more importantly we apply active learning with selective sampling so as to overcome the limitation of too few samples or too many guess and check iterations.
in the authors proposed to apply pac learning techniques for invariant generation.
it has been demonstrated that their approach may learn invariants in the form of arbitrary boolean combinations of a given set of propositions under certain assumptions .
in the authors developed a guess and check algorithm to generate invariants in the form of the algebraic equation.
it learns invariants of polynomial form by operating the null space operation on matrix.
in the authors proposed a framework for generating invariants based on randomized search.
in particular their approach has two phases.
in the search phase it uses 3we thank the help from a researcher in the sv comp evaluation team.randomized search to discover candidate invariants and uses a checker to either prove or refute the candidate in the validate phase.
in pranav garg et.
al proposed to synthesize invariants by learning from implications along with positive and negative samples.
they further extend their approach by modifying existing decision tree classification algorithm with heuristics adopted from .
in this way they could cope with implication better and as a result handle invariants of combination of conjunctions and disjunctions in theory.
one limitation of their work is that the terms in the decision tree e.g.
the propositions must be pre defined.
compared with the above mentioned work z iluimproves loop invariant generation through active learning with selective sampling so as to avoid applying the invariant checker as much as possible.
to the best of our knowledge z iluis the first to combine selective sampling with invariant inference.
in particular in the guessing phase we additionally adopt a learnand refine iteration which improves the invariant candidates through classification and selective sampling.
in comparison other guess and check approaches solely rely on the checkers to improve invariant candidates.
furthermore we show z ilucan be extended easily to learn disjunctive loop invariants through data partitioning and classification.
lastly our approach can be extended to learn other forms of classifiers classification algorithms.
for instance in principle any arbitrary mathematical classifiers can be learnt using methods like svm with kernel methods but interpreting the output models is challenging.
nonetheless we focus on invariants in the form of polynomial inequalities or conjunctions disjunctions of polynomial inequalities in our evaluation.
the experiment results show that our approach effectively learns loop invariant for proving a set of benchmark programs and complements the existing approaches.
besides the guess and check approaches some alternative approaches have been proposed for invariant generation.
examples include those based on abstraction interpretation those based on counterexample guided abstraction refinement or interpolation and those based on constraint solving and logical inference .
these approaches depend on constraint solving and thus suffer from scalability.
for instance the work in is restricted to generate invariants in abstract domains for which constraint solving is manageable.
vi.
c onclusion in this work we propose a systematic approach to learn loopinvariants based a combination of selective sampling and guessand check.
as for future work we would explore methods for learning more expressive loop invariants as well as methods for discovering new features for our classification.