on the techniques we create the tools we build and their misalignments a study of klee eric f .
rizzi grammatech inc. ithaca ny usa erizzi grammatech.comsebastian elbaum matthew b. dwyer university of nebraska lincoln usa elbaum dwyer cse.unl.edu abstract our community constantly pushes the state of the art by introducing new techniques.
these techniques often build on top of and are compared against existing systems that realize previously published techniques.
the underlying assumption is that existing systems correctly represent the techniques they implement.
this paper examines that assumption through a study of klee a popular and well cited tool in our community.
we briefly describe six improvements we made to klee none of which can be considered new techniques that provide order of magnitude performance gains.
given these improvements we then investigate how the results and conclusions of a sample of papers that cite klee are affected.
our findings indicate that the strong emphasis on introducing new techniques may lead to wasted effort missed opportunities for progress an accretion of artifact complexity and questionable research conclusions in our study of the papers that depend on klee can be questioned .
we conclude by revisiting initiatives that may help to realign the incentives to better support the foundations on which we build.
ccs concepts general and reference !empirical studies software and its engineering!software libraries and repositories software verification and validation keywords research incentives research tools and infrastructure replication .
introduction the software engineering research community like many in computer science relies on publication for both the dissemination of ideas that advance the state of the art and as a primary means of career advancement .
it has been observed that this dual nature of publication has skewed the review process so that papers that make claims of novelty are much more likely to be published .
in turn this has shaped the incentive structure for researchers in ways that link discovery to career advancement with other forms of scholarly contribution relegated to a secondary role at best .
in particular the important work of improving and maintaining the systems on which our field is so dependent has permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may austin tx usa c acm.
isbn .
.
.
.
lost as a priority.
we contend that the software engineering research community is worse for this.
the focus on discovery leads much of the research published in software engineering to make claims of the form technique a is the new state of the art .
to support such claims it is very common to manifest a technique in the implementation of a software system.
every year there are papers in major conferences and journals reporting evaluations using for example test generators program analyzers refactoring tools program comprehension systems fault localizers recommendation systems and user interfaces.
as a community we rely on the fidelity and quality of these implementations to support conclusions we draw about the techniques that they realize but it is notoriously difficult to distinguish discovery from mistaken or sub optimal implementation .
demonstrating the value of technique a may involve direct comparison with or building on top of technique b. in either case the implementations of a and b play a crucial role in the validity of the conclusions that can be drawn about a. inadequacies in those implementations can lead to different kinds of problems.
a faulty implementation of b may lead to invalid conclusions about the value of a. researchers may waste effort in creating a new technique a because of a perceived inadequacy in b but that inadequacy may simply be a fault in the implementation of b. faults or limitations in the implementation of b may discourage advances in building further work along those lines.
finally an accretion of artifact complexity may result from layering implementations that work around weaknesses in other implementations.
this complexity can present a barrier to further work in the field.
in this paper we explore these challenges through a mixed methods study involving the symbolic execution symexe engine klee .
the study and this paper grew out from our own experiences and missteps as we attempted to implement a new and highly effective technique or so we thought into klee.
we present in section six fixes and optimizations that we made to the klee code base.
these improvements cannot be considered new techniques but instead are best thought of as enhancements that might naturally arise in the evolution of a high quality software system.
the fact that these opportunities for improvement exist is by no means an indictment of klee.
in fact as we discuss in section klee is by all measures a well developed and maintained research tool among the best in our research community.
nevertheless the fixes and optimizations on such a recognized and popular tool yielded performance gains ranging from 2x to 17x on different configurations of klee including 13x on the configuration used in the original klee paper .
the technical knowledge required to implement each of the improvements was available at the time klee was originally released.
this meant that many papers were published that either compared ieee acm 38th ieee international conference on software engineering against or were built upon a version of klee that did not represent the true capabilities of existing techniques.
given the magnitude of the performance gains we observed and how they affected our judgments about own techniques we wondered how claims about published techniques would be strengthened or weakened if they had been compared to klee with our improvements.
to assess this we sampled and analyzed 100papers from international conferences and journals that cited klee.
of these papers 47referenced klee superficially.
of the remaining 53papers 27referenced klee but ended up using a symexe implementation other than klee to evaluate their technique.
the remaining 26papers used klee in some substantive way in their evaluations.
in an ideal world we would have downloaded the experimental bundles for all 26papers applied our improvements to the klee dependent parts of the bundle and replicated the experiment to measure the differences between the original and improved versions.
unfortunately only two of the papers provided all of the necessary details to repeat their experiments.
therefore we examined each paper and based on our knowledge of the parts of klee that were affected by our improvements identified 11papers whose conclusions we felt could be impacted by our improvements to klee.
we then attempted to replicate these 11papers as faithfully as possible searching the web for repositories and emailing the authors with questions when necessary.
following this effort when it was possible to re run the experiments we did.
in other cases we were forced either due to non responses or refusals to approximate the likely effects of our improvements.
in three cases the author s modifications to klee made their version different enough from any other version of klee that we were able to evaluate that we had to withhold all judgements.
of the papers that remained we conclude that research findings in 7of them may be affected by our improvements to klee.
we believe that the results of this study provide insight into the challenges faced by our community given its sharp focus on discovery.
we are spending significant effort building new techniques on top of and comparing against existing implementations that may either lag behind or are incorrect implementations of the known best techniques.
this has the potential to lead to false claims and redundant or misguided work.
these problems are exacerbated by the difficulties in replicating or even approximating the reported studies on these techniques where small differences can have dramatic effects.
our findings also align with and support observations made in other fields that point to and attempt to address the under emphasis of the implementation and maintenance of software in research .
recent efforts emphasizing artifact sharing and evaluation to promote replication principles for example can help to better understand the implementations that inform future work.
and although more can be done to support replication distinct complementary efforts are needed to incentivize the work of maintaining and improving the important systems we build on .
in section we describe other ways for the se community to foster such activities.
for example we describe how changes in the citation practices can help recognize researchers working on tools how competitions could help to promote the introduction of known successful techniques into tools and have known baselines and how specialized conference tracks where tool improvements are judged based on their potential impact to the community could provide a venue for researchers focusing their efforts on maintaining the sophisticated infrastructure that we all build on.
it is our hope that such steps could help to reduce the number of false research starts and mis steps and enable more consistent progress by the community as a whole.
.
background on klee released in klee is designed to explore and automatically generate tests for c programs.
what made klee unique from the symexe tools that preceded it was the scale of programs it could test.
klee broke new ground testing 89of unix s coreutils programs exposing new bugs and achieving higher coverage in a few hours than test suites developed by experts over decades .
inspired in part by klee s success our research community has developed a wide variety of symexe tools including ones for binary code java c and c .
.
how klee is maintained in spite of the number of emerging symexe tools klee remains among the most popular and our community keeps building on it.
it is being used to test a range of systems including gpu kernels networks and file system checkers .
research groups have used klee for document synthesis stress testing and aiding debugging .
in addition it is often used as a baseline to show the efficacy of a new technique the implication being that if klee cannot do it then it is unlikely any other tool could.
since it was first released klee has had 24different contributors to the main trunk.
this effort translates into 989total commits over the course of the project including 177total commits in the past year .
when compared with other symexe projects that share their code klee s research and development community is among the largest.
for example crest jcute caut and spf have equivalent or smaller development communities .
besides the code klee has a website with a series of tutorials on how to get the tool running a klee dev email list and a trove of information that not only allows new users to replicate the experiments presented in the original klee paper but also provides instructions on how to set up klee for development.
these are evidence that klee s community is responsive diverse active and among the most mature in our research community.
.
how klee works klee and symexe tools in general represents inputs and values computed from them symbolically during a program execution.
this means that whenever a branch is encountered the tool forks ostensibly exploring both sides of the branch.
in order to focus towards generating meaningful inputs symexe tools use a satisfiability sat solver to determine if a particular path is feasible.
a path is judged feasible if a logical formula encoding its branch conditions is satisfiable which implies that there exists some input that would cause the traversal of the path.
programs may have an enormous number of feasible paths so in practice symexe tools rely on heuristic search methods to target certain paths sooner.
invoking the solver is usually the most expensive part of symexe tools.
to mitigate these costs klee uses a solver chain made of three filters to reduce the size and number of constraint sets that reach the solver.
the first filter looks for constraint independence factoring the incoming constraint sets into smaller independent subsets .
these subsets simplify computations in the latter parts of the solver chain.
the second filter caches previously determined solutions and checks whether the answer to an incoming constraint set has already been computed.
the third filter uses a ubtree to support the checking of subsumption relationships between constraint sets.
should the incoming constraint set be a subset of a stored sat constraint set then the incoming constraint set must also be sat.
if a stored unsat constraint is a subset of the incoming constraint set then the incoming constraint set must also be unsat.
133table our six improvements to klee.
improvement shows improvement s name.
pull shows improvement s pull request number .
size shows how many lines of code were changed in improvement.
status shows status of pull request as of .
improvement pull size status solution re computation optimization accepted quick cache bug pending array factory bug accepted default enabled no prefer cex bug accepted unnecessary ubtree superset bug accepted inequality concretization optimization pending .
improving klee in this section we survey six improvements that we made to klee and evaluate their effect on its various configurations.
the key insight is that none of the improvements we made can be considered a new technique.
instead four address performance bugs in klee and two add simple optimizations that were introduced almost ten years ago.
.
fixes and optimizations table provides a brief description of these changes.
.
.
solution re computation optimization when klee traverses program paths the solver chain s first filter minimizes the size of every collected constraint set by factoring it .
when it attempts to generate concrete tests to follow these paths however the original implementation of klee did not factor incoming constraint sets.
therefore instead of using cached solutions a constraint set would be passed to the solver.
to fix this we changed klee so that the constraint sets associated with test case generation would be fully factored.
once each subsets solution is retrieved they are stitched together to form a single complete solution.
this approach was introduced in .
.
.
quick cache bug this bug occurred when the second filter was invoked to check whether a particular branch could be traversed along both edges.
if the constraint set associated with the first edge of the branch had not been seen before then the entire constraint set was forwarded to the third filter.
this meant that the constraint set associated with the other edge of the branch was never checked in the second filter even if its solution was already cached.
in practice this meant that the second filter allowed cached constraint sets to reach the more expensive third filter.
to fix this we implemented a hashmap that is checked for a cached solution before invoking the third filter.
.
.
array factory bug whenever klee encounters an array a data structure that tracks the reads and writes to that array is created.
in the original implementation when two separate paths encountered the same array two different instances of this data structure were created.
this meant that it was possible for equivalent constraint sets to not be recognized as such meaning that cached solutions would not be retrieved.
to fix this we created a factory method that examined every incoming request for an array.
if the request was for an already created array we simply returned a reference to that array.
.
.
default enabled no prefer cex bug no prefer cex is a command line option that forces klee to try to make the generated test cases more readable.
in the original implementation of klee buried among 160other command line options it was turned on by default.
we consider this a bug because 1commit 3bd3789 was our baseline.
all experiments data and builds in this paper are available at behavior it triggers is rarely desirable and very costly.
indeed none of the papers that we examined mentioned that they wanted human readable output for their test cases.
while it seems possible that there is a place for this option such as introducing new users to klee making it the default behavior reduces the performance of klee for the average user.
.
.
unnecessary ubtree superset bug this bug was the result of what turns out to be a counter productive optimization added to klee s third filter .
in the original implementation the ubtree checked for both supersets and subsets of an incoming constraint set.
the subset query was very efficient taking on average less than of the overall computation.2the superset query however accounted for of the computation time.
the reason for this inefficiency is that as constraints are placed into the ubtree the response time for superset queries increased at a much faster rate than the subset query.
these escalating costs eventually outweighed the costs of simply bypassing the superset check.
to fix this problem we disabled the superset check by default.
.
.
inequality concretization optimization concretization is an important optimization for symexe tools.
if a symbolic variable can be proven to have a single value then it no longer needs to be symbolic.
this logic is used to simplify many of the constraint sets that need to be solved.
klee s original implementation did not have a mechanism to handle inequalities which can constrain a variable to the point where it can only be a single value.
for example the constraints x x can be used to show that x .
to implement this optimization we created a series of maps that hold the maximal and minimal values that a variable can possibly take.
should the maximal and minimal values ever become equal then the variable would be concretized.
this optimization has been used since at least .
.
assessing fixes and optimizations in this section we assess the following question what is the impact of our fixes and optimizations on klee s performance?
to answer that question we evaluate two different instances of klee.
the first instance is our treatment that is klee incorporating all our improvements denoted as all .
the second one is our control that is klee without any of the changes denoted as none .3we ran each of these instances on the same version of the core utils programs utilized in the original klee paper .
for each artifact we used the same number and size of symbolic arguments as suggested on the klee website.
since klee can be configured to operate in many different ways and the performance may vary depending on which configuration is used we structured our analysis across two popular configuration dimensions targeted optimizations and testing goals.
more specifically in terms of testing goals we differentiated between generatingregression test suites one for every path klee traversed and generating coverage test suites where only tests that increase coverage are generated.
in terms of targeted optimizations the first one we cover generic mimics a default configuration with no optimizing command line options at all.
the second one core utils uses the command line parameters suggested on the klee website .
combining these two dimensions leads to the four explored configurations generic regression generic coverage core utils 2based on data generated from all core utils programs using the none treatment of klee using the generic regression configuration to be discussed in section .
.
3for details on the performance gains discriminated by individual improvement we refer the reader to .
134figure results for executing klee with allandnone on the generic regression generic coverage core utils regression and core utils coverage configurations.
the box plots show how much faster the allinstance is relative the none instance for each configuration a indicates that allandnone have the same performance .
regression and core utils coverage we also refer to them as g.r g.c c.r and c.c later in the paper .
finally we designed a procedure to ensure a common baseline for the comparisons across configurations and treatments.
since klee does not provide a timeout for test generation we resorted to a proxy for it.
our procedure consisted of counting the number of forks the allinstance of klee could reach in one hour for each artifact and each configuration .
using this initial target we then ran each instance of klee five times averaging the execution times across all runs.
if a particular execution took longer than 24hours we terminated it.
this process resulted in almost observations instances x configs.
x artifacts x runs .
figure summarizes the results with the four klee configurations on the x axis and the relative performance improvements ratio of none over all on the y axis log scale .
the box plots show the minimal first quartile median third quartile and maximal relative performance improvement across the set of all core util programs that could be run for a particular configuration.
the median improvements were 4x for core utils regression 13x for core utils coverage 1x for generic regression and 2x for generic coverage.4the performance variation across configurations was expected given their distinct behaviors.
for example the regression configurations usually produce many more tests cases than the coverage configurations therefore the solution re computation optimization and the default enabled no prefercex bug fix lead to much greater improvement for the generic regression andcore utils regression configurations.
similarly the coverage configurations produce many more instances that can be concretized through the inequality concretization optimization than with generic configurations leading to greater improvements for the core utils based configurations.
independent of these variations notice that these fixes and optimizations while not new techniques result in more than an order of magnitude performance improvement on average across configurations.
this made up wonder how research that justifies a new technique via the performance of an implementation would perform in comparison.
4under core utils regression andcore utils coverage we could not test three of the programs date touch and fmt due to preexisting assertion violations that were repeatedly encountered by both the alland the none instances.
we omitted sortfrom the boxplots since we consider it an outlier it was over 1000x faster for thegeneric regression and the generic coverage configurations and10x slower for the core utils regression configuration.
.
assessing work using klee the research questions we attempt to answer are rq1 how is klee cited and used by the research community?
rq2 how do our improvements to klee affect the results and conclusions of papers that depend on klee?
.
study setup and procedures paper selection process.
we used google scholar an aggregator from many publications and professional venues to build the pool of papers that cited the original klee paper .
our search parameters allowed for all papers that were available on or before the date of our search .
we began our search by locating the original klee paper in google scholar.
we then clicked on the cited by button to access the papers that cited the original klee paper .
we then scraped the contents of all the resulting pages to create a local repository of papers.
next we iteratively and randomly selected a paper from the local repository and evaluated it based on our inclusion criteria until we had 100papers for analysis.
there were three criterion for inclusion.
first the article had to cite the original klee paper.
while our search process would seem to imply this there were a few papers returned by google scholar that did not cite the original klee paper and were therefore removed from the study.
second the papers had to be full peer reviewed articles published by international computer science conferences or journals.
finally we excluded papers that were duplicates of papers that had already been included in the analysis.
the first author of this document was the original judge of all of the papers with the other two authors verifying samples of the results.
data collection and processing.
for each paper in the study we collected bibliographic data.
we also classified each paper based on how klee was used.
if it relied on klee we further analyzed the degree to which its study depended on klee s execution time and whether the results may be affected in light of our improvements.
if it used symexe tools other than klee then we examined whether the authors expressed a belief that their techniques could be implemented in klee.
the first author of this document extracted and classified the information.
particularly difficult decisions were discussed among all authors until a consensus was reached.
to support such analysis we developed a series of rubrics.
the usage rubric helped us classify how a particular paper used klee into one of three categories.
the superficial citations category contains papers that cite the original klee paper but do not use it or any other form of symexe in their experiments or implementation.
the symexe dependent category contains papers that cite the original klee paper but use some other symexe tool in their evaluation or implementation.
finally the klee dependent category contains papers that both cite the original klee paper and use klee in some part of their study.
thetime dependence rubric helped us classify papers into three categories representing an escalating dependence on execution time.
thetime reported category t.r.
contains papers that only report the time required to execute a technique.
the timeout used category t.o.
contains papers that use a timeout in their evaluation.
finally the time compared category t.c.
contains papers that use time to directly compare two or more techniques.
theeffects of improvements rubric helped us identify how studies might be affected by our improvements to klee.
it also con5instructions for accessing the papers that were evaluated against the inclusion exclusion criteria along with a full justification for each of the decisions we made in our analysis are available at http bit.ly 1pwvviz.
135figure replication and approximation protocol.
sists of three categories.
the unaffected category contains papers whose results we believed would not be affected by our improvements to klee.
the positively affected category contains papers whose experimental results would likely improve.
the negatively affected category contains papers whose experimental results would likely worsen due to the improvements we made to klee.
approximation and replication.
the effects of improvements rubric provides a first layer of analysis to determine the potential impact of our klee improvements on a paper s results but it does not investigate the magnitude of the improvements or how the improvements would affect the conclusions in a paper.
to provide such insights we attempted to replicate the studies for papers that we deemed candidates for having their conclusions affected.
we devised the protocol in figure to guide us through this process.
the process started by carefully analyzing each paper searching for setup and implementation details of each study including klee version and parameters relevant libraries artifacts and supporting tools.
if the paper provided enough information then we attempted to replicate it.
otherwise we performed a web search attempting to find a repository associated with the paper.
the search targeted the authors web sites the technique names and common hosting sites like github and sourceforge.
if a repository was found we attempted to use information to replicate the study.
otherwise we sent an email to the authors requesting further information.
in total we reached out to 22authors of 10papers and received a response from the authors of eight papers.
if viable information was provided by the authors we attempted to replicate the study and iterated with the authors as needed.
we continued this iterative process for a period of three weeks.
in the end replication was not feasible for nine of 11papers we discuss the implications of this later in the paper .
since so many of them could not make available critical information for replication we were forced to approximate their setup.
we carefully examined each paper and conservatively estimated the klee configuration they were most likely to have used.
we adopted the strategy where we assumed that a paper used klee in the most effective way possible.
if it was clear that klee was being used to create a large body of tests we assumed its setup was most similar to the generic regression configuration discussed in section .
.
if klee was used to achieve coverage we assumed the generic coverage configuration was used.
adopting this strategy also meant that we were being conservative in terms of the effects of our improvements since we never mapped a paper to either of the core utils configurations the ones that our improvements affected the most.
finally if the way klee was employed did not map to these common configurations we did not attempt to force an approximation .
our final mapping of papers to configurations is given in parentheses in column five of table .
6instructions for accessing the complete communications with the authors are available at figure breaking down the 100analyzed papers.
attempt denotes papers we attempted to replicate.
rep. denotes papers that provided enough information to replicate their studies.
approx.
denotes papers that we approximated due to lack of information.
n adenotes papers that we were unable to either replicate or approximate.
.
results the sankey diagram in figure provides an overview of the results for the 100analyzed papers.
the first breakdown from left to right shows how the community cites and uses klee are symexe dep.
are klee dependent and are superficial citations.
the next breakdown shows for the klee dependent papers that 11may have their conclusions affected by our improvements so we attempted to replicate them attempt .
among those the next breakdown shows we could replicate 2and approximate .
in the end of the klee dependent papers could have their conclusions significantly affected.
the next sections examine in detail these results.
.
rq1 how is klee cited and used?
to understand how our research community uses and cites klee we employed the klee usage rubric as a coarse filter and then analyzed each category in an attempt to characterize them.
superficial citations.
papers in this category cover a wide range of research areas including configuration space testing software upgrades and performance testing .
in some cases klee is used to provide context and motivation.
for example one paper offers a formalization of translation rules for c language that allows a ... test generator to validate properties like klee .
in other cases klee is presented as an option that would not quite work .
an example of this occurs in a paper that addresses compile time configurability and states that the most powerful symbolic execution techniques such as klee ... would currently not scale to the size of the problem being studied .
in such cases even when klee does not appear to be well suited for a particular problem it is both popular and powerful enough to at least merit discussion.
symexe dependent.
papers in this category are mostly focused on testing and debugging.
some papers propose new ways to make the results of symexe testing more meaningful for developers .
other papers are meant to support symexe through changes to the underlying theories techniques or tools .
finally papers in this category often create their own hybrid techniques to either target new languages or to mitigate the path explosion problem in symexe .
these papers are interesting because they were written by symexe researchers who are aware of klee but chose to use a different 136table analysis of papers in klee dependent category.
id paper identifier.
time classification with time dependence rubric.
eff.
classification with effects of improvements rubric with p n andumeaning positively affected negatively affected and unaffected.
rep. attempt of replication.
nomeans we did not try to replicate it approx.
means we attempted to replicate but could only approximate n ameans we could not even approximate and yesmeans we were able to replicate.
con.
significant impact on paper s conclusions.
n a shows that we were unable to reach a conclusion based on the available information.
paper title id time eff.
rep. con.
kleenet kn t.r.
p no no modeling firmware as service functions and its application to test generation mf t.r.
p no no a soft way for openflow switch interoperability testing ofs t.r.
p no no symbolic software model validation smv t.r.
p no no automatic detection of floating point exceptions fpe t.o.
p no no control flow obfuscation using neural network to fight concolic testing cfo t.o.
n no no craxweb crx t.o.
p no no detecting problematic message sequences and frequencies in distributed systems pms t.o.
p approx.
g.r.
yes docovery doc t.o.
p no no minthint mh t.o.
p no no static analysis driven cache performance testing cpt t.o.
p no no automated software testing of memory performance in embedded gpus tmg t.c.
n approx.
g.c.
yes automatic concolic test generation with virtual prototypes for post silicon validation vpv t.c.
n approx.
g.r.
yes body armor for binaries preventing buffer overflows without recompilation ba t.c.
u no no bugredux br t.c.
n n a n a chaining test cases for reactive system testing rst t.c.
n approx.
g.r.
yes directed symbolic execution dse t.c.
n no no gklee gk t.c.
p no no hampi hmp t.c.
n yes no industrial application of concolic testing approach ia t.c.
n yes yes postconditioned symbolic execution pse t.c.
n approx.
g.r.
yes reproducing field failures for programs with complex grammar based input rff t.c.
n n a n a selecting peers for execution comparison spc t.c.
n n a n a scalable testing of file system checkers fsc t.c.
p approx.
g.r.
yes software dataplane verification sdv t.c.
n no no a synergistic analysis method for explaining failed regression tests sa t.c.
p no no tool.
as we examined these papers it became clear that a majority of them believe that their proposed techniques could be mapped onto klee and achieve similar results to those presented in the paper.
in of these papers this belief is explicit.
for example one paper states our algorithms are applicable in the context of other languages for which symbolic execution tools exists e.g.
klee ... for c .
in of the papers the possibility of extending the proposed technique to klee is implied.
this usually occurs when a paper identifies a problem with symexe as a whole and then fixes the problem on a particular tool.
an example of this occurs in a paper that asserts ther concolic and symbolic testing tools could integrate our algorithm to solve complex path conditions without having to sacrifice any of their own capabilities leading to higher overall coverage .
the reader is left with the impression that all symexe tools including klee would benefit.
given the questions we raise about papers that actually depend on klee directly next section it seems that claims about the generality of a technique across tools merit at least further evaluation.
we found that it is rare for a technique to be implemented in more than one tool.
indeed only three of the 27papers in the symexe dependent category went so far as to incorporate and test their technique on multiple tools.
following publication there seems to be even less chance that a technique will be implemented across the set of symexe tools we could not find any of the techniques proposed by the papers within this category in klee s code base.
we speculate this disconnect leaves the community open to both duplication and confounding techniques.table time dependence rubric xeffects of change rubric.
the numbers represents the papers whose studies results would be affected by our improvements to klee across each time dimension.
the numbers in parenthesis correspond to instances where the conclusions of the work are likely to be affected.
negatively unaffected positively total affected unaffected affected time reported timeout used time compared total klee dependent.
when examining the papers in this category the one thing that was immediately obvious was the versatility of klee.
several papers extend klee to test gpu programs networks and file systems checkers .
there are also papers that used klee to help a developer fix a buggy line and recreate field failures in the lab .
two other papers argue that klee has become such a powerful tool that it may be necessary to rethink altogether how we address certain problems .
for example one of these papers argues for the adoption of a particular programming paradigm since it preserves performance while also enabl verification through its amenability to klee .
finally klee is also used as a baseline to assess the efficacy of new techniques the implication being that if klee cannot do it then the proposed technique must be powerful.
we examine this category in more detail next.
.
rq2 how do our klee improvements affect klee dependent papers?
to get a better understanding of the different types of evaluations being conducted among the 26klee dependent papers we classified them using the time dependence rubric.
we first classified four papers as time reported seven as timeout used and 15as time compared.
in addition we examined how the papers in each category would be affected by our improvements to klee using theeffects of improvements rubric and for the ones that we could replicate and approximate we re assess their conclusions.
the results of combining the time and effects rubrics can be seen in table .
note that in the subsequent analysis we use the ids from the second column of table to refer to the papers.
when examining table we see that out of papers seem to be affected by our improvements to klee the single unaffected paper smv uses a tool that builds off klee but its technique and study focus on buffer overflow detection .
from this initial breakdown it seems that papers that compare performance are much more likely out of to have their results negatively affected given the klee improvements we implemented.
time reported.
there are four papers in the time reported category.
these papers use symexe to solve new problems.
for example the paper mf examines how long it takes for klee to explore a particular type of driver design.
overall papers in this category use time to show that applying klee to a particular problem domain is possible.
effect.
since a faster klee only strengthens what is possible the results of papers in this category are positively affected by our improvements but the conclusions are not altered.
timeout used.
the timeout used category has seven papers where authors determine through the timeout at what point the cost benefit of a particular technique is no longer acceptable.
for example fpe builds a system on top of klee for finding flaws in floating point programs.
in their evaluation the authors set a timeout and because of this several executions are not completed.
137effect.
six papers in this timeout category have results that could be positively affected and one that could be negatively affected by our changes to klee.
in terms of the effects on the conclusions however for six of the seven papers the likely importance of our klee improvements is small.
in the case of cpt the authors used exceptionally short timeouts meaning the effects of the improvements would be limited.
in other cases the results were already so persuasive e.g.
in fpe discovered inputs that generated floating point exceptions ... across the 467functions that improving them further would seem to be of little note.
in others such as cfo andmh the results did not rely on klee in such a way that our improvements would alter their results enough to significantly strengthen or weaken their conclusions.
the one paper that we believe could have its conclusions significantly affected by our klee improvements was pms .
for this reason we mark this paper for further analysis in the next section.
time compared.
thetime compared category has 15papers that somehow utilize klee s performance to quantify a problem or a technical solution.
effect.
we deemed that 11papers would have their results negatively affected one not affected and three positively affected.
among the papers with affected results we deemed 10as likely to have their conclusions affected.
we mark those for further analysis in the next section.
the reasons for papers with affected results not being marked for further analysis varied.
some such as sdv addressed an underlying theoretical problem of symexe like path explosion.
in other cases such as ba anddse17 klee s performance does not seem to affect the outcome of the research questions being investigated.
for example in dse the authors investigate different search strategies to reach particular lines in test artifacts.
they include klee s search strategy in their study implementing it in their own symexe tool.
while they test how well klee does on these test artifacts they seem only to include it as an implicit endorsement that their implementation of the search was correct.
finally in sa the runtimes are short enough that modifications in the results are unlikely to translate to significant improvement for the technique.
overall studies that used time to make value judgements seem much more susceptible to performance improvements affecting their results than those showing that an approach is possible.
the challenge is that as a community we cannot just rely on studies that focus just on what is possible.
studies that show what is better allow us to judge the relative merits of a technique in turn enabling experiments that make more things possible.
unfortunately better performance is contingent on a whole host of factors.
from the clock rate of the cpu all the way up to the configuration details of a tool with many variables interacting in highly complicated ways.
how best to achieve progress in such fluid and complicated environments is an exceptionally difficult question.
replication and approximation of the papers whose results could be affected by our klee improvements our analysis identified one from the timeout category and 10from the time compared category that required a deeper examination because we deemed that their conclusions could be significantly affected.
this deeper examination consisted not just in analyzing the papers in more detail but also attempting to replicate studies.
in spite of our efforts we were able to replicate the studies in only two of these papers.
we were able to replicate hmp which contained a reference to an online repository with all of the necessary code and data.
the other paper we were eventually able to replicate with the authors assistance was ia20 although as we shall see even in this instance the result of the replication did not quite match those in the paper.
we did not refigure reproduction of figure from paper pse comparing the time required to run pse klee combined with the technique proposed in the paper against the time to run base klee klee .
ceive a response for two papers while for the remainder we were informed that pending patents work with industrial bodies or unrecoverable code and data prevented the authors from being able to help us replicate their experiments.
overall out of the 11papers that we examined more deeply 2had their conclusions strengthened by our findings 5had their conclusion negatively affected and hence weakened by our findings 1paper was not affected and we were not able to approximate 3papers.
we now discuss them in detail with the caveats mentioned in terms of our ability to replicate the studies and their contexts.
the findings were shared with all the authors of the affected papers and for three the discussion was adjusted based on that feedback.
pms approximated as g.r.
positively affected.
even though one of the co authors is a co author of this paper the experimental data and modified artifacts required to support this experiment were not preserved after project completion.
hence we approximated the study using the generic regression configuration which closely matches what was originally used.
using this approximation we would expect a 10x speedup with our klee improvements.
considering that in the paper s study klee ran for 24hours the improvement would likely have strengthened the paper s conclusions allowing it to be run more frequently discovering more faulty sequences or scaling up the analysis to multiple nodes.
fsc approximated as g.r.
positively affected.
this paper uses test suites generated by klee to find faults in file system checkers.
in their evaluation the authors compare their work against a test suite that accompanied one of the checkers.
we again had to approximate as the authors replied that it was not available to the public.
however the configuration as described is similar togeneric regression that resulted in a 10x speed up with our improvements.
in their discussion the authors observe that applying to the file system checker led to a lower code coverage than that obtained with the...test suite .
we believe however that given the magnitude of our improvements to klee such statements may no longer be true either the coverage would dramatically increase or the time required to run the technique would be dramatically reduced.
pse approximated as g.r.
negatively affected.
this paper uses state matching to identify and eliminate common path suffixes that are shared by multiple test runs .
an implementation of the proposed technique was not available one author replied that they could not provide the source code since it is currently under contract with our project .
therefore we had to approximate using the generic regression configuration as the proposed technique produces thousands of tests.
the authors report a reduction in the number of generated test cases of 5x and state that the proposed technique can have a significant speedup over state of138figure reproduction of table from the paper rst .
the table shows klee s performance klee relative to other techniques at creating test case chains.
the art methods in klee .
their technique however introduces non trivial overhead.
they provide a graph reproduced in figure that compares the time to run klee versus klee with their technique added.
as can be seen in the figure there is a tipping point around seconds on the x axis where klee s cost becomes high enough to warrant the proposed technique.
upon applying our improvements resulting in a 10x speedup for this likely configuration the tipping point would likely be delayed.
this in turn would call into question their conclusions about the trade offs between effective redundancy removal and the computational cost of detecting and eliminating such redundancy .
vpv approximated as g.r.
negatively affected.
this paper proposes a test reduction technique by first identifying important states under test from concrete executions ... and then symbolically executing them.
an implementation of the proposed technique was not available as urrently all the code is under that patent and not open source .
the paper claims that the proposed technique significantly reduces the time required to generate a test suite while only reducing test coverage by a small amount.
for example in one of their experiments their technique takes minutes which includes 5minutes for state selection and minutes for test generation.
when examining a larger set of states ... selected using the random strategy.
it takes 1day after which only two new test cases are generated .
our improvements call into question the cost benefit analysis presented in the paper.
by reducing the costs of n ive klee by 10x the loss in coverage that is associated with the technique that the authors propose may no longer be acceptable.
rst approximated with g.r.
negatively affected.
this paper attempts to discov a test case chain a single test case that covers a set of multiple test goals and minimises overall test execution time .
while the tool itself was made available by the authors upon request the experiments involving klee along with the test harnesses required were not provided within the time specified in the protocol we did receive a response after the time limit established by the protocol .
the authors compare four techniques tools chaincover fshell random and klee as seen in figure in terms of the number of test cases in the chain tcs the length of the test case chain len and the time it took to find the chain.
when the half hour timeout is reached the maximum coverage achieved so far is reported.
they state that klee found test case chains on a few of the benchmarks in very short time did not achieve full coverage within an hour on half of the benchmarks .
while they were many complexities to how klee was used7 the paper declares exhaustive exploration i.e.
klee is not suitable for our problem .
we find it likely that a 10x faster klee would have 7among the added complexity was that of test harness built on top of klee for which the cost is unclear.a greater chance of finishing several of these benchmarks and of finding the chains it did find faster.
if this were the case it might call into question the conclusion reached in their paper that chaincover was the best choice.
ia replicated negatively affected.
the authors of this paper create a new symexe tool to generate more tests and achieve greater coverage in a set amount of time.
in their study the authors compare their tool s performance to klee s when using different search algorithms.
in the end the authors observe that their tool is 10to28times faster than klee in terms of test case generation speed .
while the paper seems to provide all of the necessary details to replicate the study after attempting to do so we were unable to achieve results comparable to those reported in the paper.
the authors were helpful in confirming that we were using the correct settings but even then our results turned out to be quite different from those reported in the paper.
in the process we realized that the paper s study as described neglected to include a command line option max sym array size that would be recommended as it dramatically speeds up klee s performance.
indeed when comparing the only two searches klee and crest bv have in common the improved klee outperforms the reported data of crest bv .
these increases in test case generation performance occur while simultaneously slightly improving coverage.
these facts combined with our difficulties reproducing the results reported mean that the assertion that he speed of test case generation by crest bv was 28times faster than that of klee is now questionable.
tmg approximated as g.c.
negatively affected.
this paper presents a technique to the inefficiency of...software developed for embedded gpus .
to expose these problems the authors create a tool that combines several existing tools including gklee with several new techniques such as a static analyzer and a thread selector .
they then compare the performance of their tool under several different configurations.
unfortunately the proposed tool was not available since the related project currently involves industrial bodies .
the complexity of their setup made it difficult to rely on approximations so we conservatively used the generic coverage configuration to illustrate the potential impact of our improvements to their tool chain.
while the 2x improvement of the generic coverage configuration approximation would likely increase the overall effectiveness of the tool that is tested in the paper it also calls into question many of the specific proposed optimizations.
specifically the paper proposes a new static analyzer that first produc a summary of all gpu threads via static analysis .
these summaries are used to guide the test generation to produce memory performance stressing test case .
with the improvements applied to gklee this initial computation may prove unnecessary since the cost of static analysis may end up being greater than simply allowing gklee to explore the states in a n ive fashion.
hmp replicated not affected.
this paper presents an efficient and easy to use string solver called hampi.
in the kleebased portion of the study the authors test programs with structured input formats using hampi to generate constraints that specify legal inputs to these programs .
by utilizing this initial filter the authors argue that many of the redundant paths that klee would explore are removed allowing it to explore deeper into the program.
to see the effects that our improvements would have on their proposed technique we ran two versions of klee one with our improvements and one without and compared the coverage achieved by each instantiation.
on each of the three artifacts investigated in the paper our improvements did not result in any increase in coverage.
this shows that our improvements are not enough to overcome the challenges that come with structured in139put formats and thus that the paper s conclusions hold.
spc not able to approximate unknown.
this paper compared the ability of different techniques to generate a non failing trace that is similar to a faulty trace allowing a developer compare and contrast the control flow paths taken .
one of the tools examined was klee.
in the discussion of their implementation it was clear that the authors had altered klee s program exploration.
we did not receive a response from the authors to our queries for details.
due to the nature of the changes to klee we did not feel comfortable approximating the likely effects of our improvements and refrained from making judgements as to the effects of our improvements.
br not able to approximate unknown.
this paper presents a tool called bugredux that attempts to synthesize ... executions that mimic the observed field failures .
it uses traces from a native execution to guide klee to bugs.
we found a bugredux repository after the online search but had difficulty running the experiments performed in the paper as provided by the authors despite our email exchanges.
as with the previous case we refrained from making a judgement.
rff not able to approximate unknown.
this paper used the same bugredux tool described above.
in this paper however the authors were examining programs that had structured input formats similar to the ones examined by hmp .
unfortunately due to some confusion caused by the fact that this paper both used the bugredux tool and was authored by many of the same authors as the br paper we did not contact the authors of this paper soon enough to have it qualify for our protocol.
therefore we excluded this paper from further analysis.
.
threats to v alidity we have provided evidence that klee is a well regarded and popular tool in the software engineering community.
many of us have built and continue to build on it which is why we expect that our findings may resonate with many.
yet a major threat to the external validity of this work is whether the findings extend beyond klee.
our personal experience with many other tools and frameworks indicates that it does.
our research group has faced similar challenges when building on powerful and complex tools within the program analysis domain like jpf spf daikon and soot .
a new set of unknowns and a different set of incentives is further introduced when commercial tools are considered but the key underlying issue of whether a tool embodies the state of the art remains.
from an internal validity perspective the studies we conducted have several limitations including how we implemented and measured our klee improvements how we sampled and analyzed the papers the possibility of different build processes and setups yielding different artifacts other contextual elements we may have missed that were relevant to the authors or their tools and how our attempts at replication mostly ended in different levels of approximation.
we have mitigated these concerns through transparency all data and experiments setups are shared by reaching out to the authors to improve our attempts at replication and also to get feedback on the reported findings and by making what we consider to be the most conservative estimates that favor the papers we studied.
from a construct perspective our focus has been almost exclusively on performance while keeping other measures constant or assuming they were negligible e.g.
executing code other than klee had no cost .
similarly we recognize that gains in performance alone even when measured in orders of magnitude may not translate into practical gains e.g.
more coverage more faults found .
these limitations together with the available data point forfurther opportunities to better understand the relationship between the techniques we develop and the tools we create.
.
moving forward we now examine our findings from the perspective of the larger themes of replication and incentive structure.
.
replication in this work we have seen some of the extremes of replication.
on one end the assessment of the improvements we made to klee were enabled by the klee developers who provided code artifacts and configuration documentation used in the original klee paper .
on the other end as discussed in section .
attempting to closely follo the baseline experiment for many of the papers in the klee dependent category was difficult.
only one of the 11papers provided sufficient information so that the original study could be closely followed without interacting with the authors.
most are missing critical pieces of information that forced us to resort to approximation.
we admit to the possibility of wrongly approximating even when it conservatively.
but the key take away is that the current situation makes us unable as a community to distinguish implementation limitations versus technique limitations further undermining our conclusions.
replication difficulties are not new.
they fit into a long running conversation about the importance of replication in our field .
much of the work on this topic focuses on how the lack of replication makes understanding contradictory studies difficult.
for example in the paper reproducible research what why and how the authors detail their efforts to do a meta analysis of codereading techniques.
after compiling the results from 18different studies the authors were unable to determine which technique was the best.
this revelation leads them to strongly advocate the adoption of by software engineering and computer science researchers and data analysts.
more recently proebsting et al.compile a set of 601papers from acm conferences and journals and attempt to build the code associated with each study.
after an extensive web search and hundreds of emails they determined that they could locate and build the code associated with of these papers.
while proebsting et al.
do not attempt to verify the results in the papers their work illustrates the extent of the problem of replicability within our field.
our paper shows that even when we can build these tools there is a large gap between techniques and their implementation.
still in spite of the slow adoption of replication some corrective steps are being taken.
continuously evolve repository of artifacts.
there has been an emergence of repositories of artifacts to support experimentation across tools e.g.
dacapo sir .
such repositories help to make studies across tools more consistent by providing a common set of evaluation artifacts.
they have accelerated research progress by lowering the cost of performing experimentation hence making it more common.
still these repositories need to keep evolving in order to avoid techniques over fitting the body of artifacts available and to better represent the universe of artifacts.
the funding model to support this evolution however is not yet established.
entice artifact submission and checking.
conferences like esec fse oopsla and issta have put in place an artifact submission and evaluation process.
authors of accepted papers are encouraged to submit artifacts along with their papers.
such artifacts are separately reviewed to assess how helpful they are to reproduce the results in the paper and to be extended and are given a badge of approval if they are found to be of enough value.
the percentage of accepted papers receiving the badge of approval seems to 140be slowly increasing about a third of the accepted papers at issta received the badge .
still artifact and paper evaluation are completely decoupled and papers can receive a badge without sharing the artifacts with the community.
it may be time to revisit these conservative practices.
similarly it would be beneficial for funding organizations to make it a requirement for researchers to share their infrastructure and results with the community.
.
aligning incentives making studies replicable is necessary but not sufficient for research tools to keep maturing much less for them to keep up with the pace of discovery.
we conjecture that the over emphasis on creating new techniques at the expense of working on the robustness of existing infrastructure is causing at least three problems.
first unnecessary complexity ends up being added to the tools we build.
for example in the context of our study we observed that four separate papers addressed klee s test generation difficulties through new techniques when debugging klee or incorporating existing techniques might have led to similar or better outcomes.
this leads to wasted community effort and has the potential to generate layers of techniques that correct the mis implementations of the techniques below .
second there is little incentive to add a newly proposed technique to more than one tool.
it is a common practice to assert that a technique once shown to be effective on one tool will be effective on all tools section .
.
we ourselves are guilty of this .
despite these assertions and as we have seen with two of our improvements to klee a powerful technique can remain un implemented for years.
this makes it difficult to know which problems need to be solved and could lead to a situation where the same problem is solved repeatedly by different research groups.
finally we are underestimating what it means for a novel technical contribution to be successful .
in our study we discovered for example how techniques compared against a weaker version of klee reached at best questionable conclusions.
in some cases the conclusions derived lead to the rejection of a whole family of techniques overlooking much of the progress that has been made in the field since .
our study s findings highlighted these misalignments but we are cognizant that they are not just happening in software engineering but across the computer science field .
yet addressing them in software engineering is particularly pressing because our relationship with software is more complicated than most other disciplines we often evaluate the artifacts we create.
so we now examine several proposed solutions to mitigate these problems believing them to be worth further discussion in our community.
cite papers and tools.
citations are used as a means of authentication and authority and as a provision of credit and acknowledgment .
unfortunately activities that facilitate science ... are not currently rewarded or recognized having detrimental effects on funding future library development and even scientific careers .
to combat this several proposals have been made to increase the visibility of work that maintains and improves software.
for example one proposal involves a system of transitive credit where the authors of a tool reward their developers with a percentage of the credit .
these proposals aim to create an evolving list of contributors so new work is rewarded.
applying proposals like this to tools like klee would require developers to define how to quantify the credit different projects and developers should receive.
to be effective however these suggestions must be understood and their implementation valued by relevant institutions through for example assessing them as part of hiring and promotion .increase institutional support.
another suggestion is to create a staff of dedicated developers to act as stewards of the tool .
while there are instances of this funding model they are clearly not dominant and the focus on maintenance and improvement is lacking.
this would require the identification of efforts worthy of further investment but also the recognition of and support for talented tool builders in the academic world.
in the context of klee it seems likely that the bugs we found could have been discovered by a team of dedicated engineers.
integrating some of the proposed sophisticated optimizations however would likely require the participation of the researchers that created them since the information in papers is often not enough to successfully replicate a technique .
bringing together the people who understand the tool with those who understand the technique would likely increase the chances of success.
promote competition.
a potential solution is to rely on competitions.
this model is being used by the verification community facilitat the proliferation of different...approaches algorithms and implementations and the success of this model is propagating to other venues .
still it seems that competitions have limited traction in the community.
we conjecture that in part this could be corrected by defining problems with the right scope manageable yet impactful to entice more community participation.
create a tool maintenance track.
another potential solution is adding a maintenance specific track to conferences.
such a track would help to incentivize improvements to important tools and systems through the traditional and accepted reward mechanism of publication thereby making it easier to fit into hiring and tenure practices .
papers submitted to this track would be judged based on new criteria that account for the potential impact to the community based on the number of users or citations of the target system and the degree to which the tool is improved.
.
conclusion our community constantly pushes the state of the art by introducing new techniques.
to do this we often build on top of and compare against existing implementations that realize previously published work.
yet we often ignore the fact that the veracity of these new techniques depends on whether the underlying systems really embody the techniques they implement.
this omission can lead to false claims to multiple techniques that redundantly target the same problem to new techniques that simply work around bugs in prior work to new techniques that are undervalued because underlying faults mask their true potential and to an accretion of complexity as techniques unnecessarily layer upon one another.
from our study we know that this happens in the research community that builds on klee.
the fact it occurs with a tool as highly cited and well regarded as klee along with our experience with other analysis tools leads us to hypothesize that it occurs with many other tools as well.
further studies are needed to support this hypothesis.
clearly our community is focused on advancing the state of the art but robust advances must be built on robust foundations.
the unknowns of se research lay both ahead and behind and only by addressing both we will make consistent progress.