see discussions st ats and author pr ofiles f or this public ation at .researchgate.ne t public ation ensemble methods for app review classification an approach for software evolution n conf erence paper no vember .
ase.
.
citations 167reads author s including bernd brue gge technic al univ ersity of munich publica tions citations see profile all c ontent f ollo wing this p age was uplo aded b y emitz a guzman on januar y .
the user has r equest ed enhanc ement of the do wnlo aded file.ensemble methods for app review classification an approach for software evolution emitza guzman muhammad el halaby bernd bruegge technische universit at m unchen garching germany emitza.guzman mytum.de m.halaby tum.de bruegge in.tum.de abstract app marketplaces are distribution platforms for mobile applications that serve as a communication channel between users and developers.
these platforms allow users to write reviews about downloaded apps.
recent studies found that such reviews include information that is useful for software evolution.
however the manual analysis of a large amount of user reviews is a tedious and time consuming task.
in this work we propose a taxonomy for classifying app reviews into categories relevant for software evolution.
additionally we describe an experiment that investigates the performance of individual machine learning algorithms and its ensembles for automatically classifying the app reviews.
we evaluated the performance of the machine learning techniques on reviews that were systematically labeled using content analysis methods.
overall the ensembles had a better performance than the individual classifiers with an average precision of .
and .
recall.
i. i ntroduction mobile distribution platforms or app stores have grown exponentially in the past years.
apple s appstore has around .
million apps and million registered developers whereas its competitor google play shares similar numbers1.
app stores allow users to download the apps of their interest and provide feedback about the apps they download in the form of ratings and user reviews.
previous research has shown that user feedback is important in the development of software applications.
due to the iterative process that developers take when developing apps expedient and useful feedback is decisive in the evolution of the app.
therefore it is advantageous that developers and other stakeholders involved in software evolution access high quality feedback in a short amount of time and with low effort.
nevertheless the processing of feedback from app stores presents three main challenges thehigh amount of user reviews for popular apps.
for example a recent study found that ios users submit on average reviews per day per app and that facebook received around reviews per day in the relatively low proportion of informative user reviews .
previous research found that about one third of the reviews contain information that can be useful for the evolution of the app i.e.
bug reports feature requests and user scenarios and theunstructured nature of app reviews.
in app stores feedback is given in free form and no predetermined fields are used.
to help developers process large amounts of feedback and distinguish feedback that is relevant for software evolution we propose a taxonomy for classifying user reviews into evolution categories and describe an experiment to research the performance of machine learning approaches for automatically classifying the reviews into the categories defined in our taxonomy.
in our experiment we examine the performance of individual machine learning approaches that have previously had a good performance when classifying text.
moreover we use ensemble methods to combine their predictions.
ensemble methods emphasize the strengths and dilute the weaknesses of the individual classifiers and are therefore good candidates for improving the prediction performance of individual classifiers.
the main contributions of this work are a fine grained taxonomy consisting of seven user review categories that are relevant to software evolution a truth set of reviews created systematically through content analysis methods empirical evidence about the effectiveness of supervised machine learning algorithms and the combination of their results via ensemble methods.
ii.
u serreview taxonomy for software evolution the definition of our taxonomy is based on the categories found in a previous study that manually analyzed the content of app store user reviews.
for the development of our taxonomy two of the authors manually annotated the relevance to software evolution of each previously defined category.
overall of the original categories were considered relevant for software evolution.
categories were deemed as important for software evolution when they gave information about aspects of the app that needed to be improved or implemented.
additionally categories that highlighted the app features or functionality that satisfy users were also contemplated as relevant to software evolution because we considered that this information notifies developers about aspects of the app that are important for users and about features that are being actively used2.
we considered general praise and complaint as categories relevant to software evolution because they give information about the overall user acceptance and this knowledge might affect software evolution.
we renamed some of the original categories into terms we considered more descriptive and modified some of the previous definitions for better clarity during the annotation of our truth set see section iii .
the taxonomy we arrived at consists of the following categories 2previous studies have found that developers are highly interested in features or software functionality that users use and like .
bug report reviews that report a problem such as faulty behavior of the application or of a specific feature.
feature strength reviews that identify an aspect about an existing feature that users are satisfied with.
feature shortcoming reviews that identify an aspect about an existing feature that users are unsatisfied with.
user request reviews that ask for a missing feature functionality or content as well as reviews that ask for the improvement of an existing feature.
praise reviews where users express general appreciation with the application.
it focuses on general judgment unlike feature strength which emphasizes on the positive feedback about a specific feature.
complaint reviews where users express general dissatisfaction with the application.
in contrast with feature shortcoming which focuses on the negative feedback about a specific existing feature general complaint concentrates on general judgment.
usage scenario reviews where users describe workarounds use cases and scenarios involving the app.
iii.
r esearch method the purpose of our research is to assess the performance of supervised machine learning techniques when classifying app user reviews into the categories proposed in our taxonomy.
our research questions is rq what is the performance of machine learning algorithms when automatically classifying user reviews into categories relevant for software evolution?
to answer our research question we performed an experiment in which we compared different machine learning classifiers and its combinations ensembles against a truth set created manually by annotators.
in the following sections we describe the steps for the creation of the truth set and the experiment setup.
a. truth set creation we created our truth set by sampling some of the reviews from the dataset collected in previous work .
the dataset contains reviews of the angrybirds dropbox and evernote apps available in apple s app store and reviews from the tripadvisor picsart pinterest and whatsapp apps from android s google play store.
we chose the dataset due to the popularity of the contained apps the diversity of app categories and the high volume of reviews.
for the creation of the truth set we used the content analysis methods described by neuendorf and maalej and robillard .
during the truth set creation human annotators systematically assessed the contents of a sample of user reviews according to an annotation guide.
for each review two annotators independently assessed if the review described any of the categories defined in our taxonomy as well as an additional category named noise used to refer to reviews that are written in languages other than english that only contain non character symbols and that in general do not make any sense to the annotators.
the truth set creation process consisted of four steps design of an annotation guide sampling of user reviews annotation of user review sample and disagreement handling between annotators.
in the following we describe each of the steps.
annotation guide design we created an annotation guide to systemize the truth set creation task and to minimize the disagreement between annotators.
the guide contained instructions about the annotation task as well as clear definitions and examples of the categories defined in our taxonomy.
furthermore it also contained a brief description of each of the apps from which the reviews were collected.
user reviews sampling we selected reviews from each app based on a stratified random sampling scheme that took into consideration the rating distribution of each app.
the main advantage of this sampling scheme is the comprehensive representation of the dataset.
user review annotation five annotators independently labeled reviews reviews per app from the total sample of reviews.
all annotators were graduate students with software development experience and good english knowledge.
the annotation was done through a specialized web tool.
for each review the title comment and rating were displayed and the annotators labeled the corresponding categories of the review.
annotators could label more than one category for each review.
all reviews in the sample were annotated twice and a disagreement analysis explained next was performed to increase the confidence in the truth set.
before starting the annotation task all annotators were requested to read the annotation guide and conduct a pilot annotation with reviews that were not part of the sample that was later used for training the classifiers.
afterwards their answers were compared with an answer key and common misunderstandings and errors were clarified.
one of the most common errors was the labeling of the reviews as belonging to only one category when they could be classified into more.
each annotator was asked to record the time required to realize the complete annotation task.
the average recorded time was hours for labeling reviews.
this result corroborates the large amount of effort required to manually analyze user feedback reported in previous studies .
disagreement handling the level of disagreement between annotators indicates the difficulty of manually categorizing a review.
the disagreement can be due to ambiguity in the annotation guide or in the review content.
first we analyzed the frequency of disagreement between annotators and implemented strategies for its automatic reconciliation.
then to gain more understanding about the causes of the disagreement we manually inspected samples of the disagreements per category.
in total reviews .
out of reviews had a disagreement.
we took a sample of reviews for of the apps dropbox tripadvisor evernote where reviews belonged to each category.
afterwards we manually inspected each review in this sample noted its responsible annotator and classified its problems as false positive a category is mistakenly annotated when there is no clear evidence of its presence.
false negative a category is mistakenly not annotated when there is clear evidence of its presence.
ambiguous a review is either vague or has a category that is not covered by the guide or its category could be interpreted as both a false negative or false positive.
furthermore we ignored erroneously labeled reviews that is reviews which categories are clearly chosen by mistake.
for example if a review contains many positive words i.e.
great fabulous or awesome and is assigned to the complaint category then we assume that the category was mistakenly chosen.
from this analysis we identified a problematic annotator and found that annotators tended to not label reviews that mention a feature shortcoming as a complaint even if there is sometimes a general complaint present.
afterwards we executed the following disagreement reconciliation steps in the mentioned order if disagreement includes a problematic annotator select the category chosen by the other annotator.
if disagreement includes a category that tends to not be labeled false negative or includes a category that tends to be erroneously labeled as belonging to the category false positive then correct accordingly.
a category is considered to have a tendency when at least of the analyzed reviews present the problem.
if disagreement includes two problematic annotators then select the category chosen by the annotator with less errors.
if neither of the above cases apply we consider the disagreement to be ambiguous and remove the category from the current review.
table i shows an overview of the truth set after performing the disagreement steps.
in the truth set the praise category is the most common category confirming the results of previous research .
to encourage replication the truth set is openly available3.
b. experiment setup in our experiment we compared the performance of four different classification algorithms and their ensembles.
more concretely we compared the performance of naive bayes support vector machines svms logistic regression and neural networks as well as the performance of combinations of the predictions of these classifiers.
the choice of the classifiers for our experiment was motivated by the effectiveness of the algorithms when categorizing text .
the goal of the classification step is to automatically organize the reviews into the categories defined in our taxonomy.
we perform the classification on a review granularity.
this choice is not random but based on a small study of app review content.
we randomly sampled app reviews containing sentences.
then two of the authors attempted to make a manual classification of the reviews and the single sentences contained in the reviews into the categories defined in our taxonomy.
however they concluded that many of the single sentences lacked the necessary context to make an accurate n1 images publications emitza n guzman nortega truthset.tsvclassification and we therefore decided for a review granularity in our experiment.
a review can be associated to different categories e.g.
a review can describe a bug report and contain a general praise the app is crashing after the newest update.
please fix it i love and need this app!
.
in machine learning the classification of documents into one or more categories is referred to as multi label classification and can be solved via the binary relevance method where a classifier for each category is trained.
we use this method in our experiment.
to train each classifier we apply the following steps on the title and comment of each single review perform stemming and stop word removal on all review text convert review text into a vector space model using tf idf as a weighting scheme add additional features into the vector space model for each review i.e.
review rating number of words in the review number of characters in the review number of lower case characters number of upper case characters number of exclamation marks number of symbols number of spaces average word length ratio of positive sentiment words ratio of negative sentiment words4 reduce the dimensions of our data by applying the chi squared metric or alternatively support vector machines svm train our classifiers on a set of manually labeled reviews and predict the categories of user reviews using the trained classifiers.
we used the scikit learn5 toolbox for all activities described in our classification step.
for training our classifiers and reporting on their performance we divided the annotated reviews into two different sets a training and validation set of the reviews and a test set of the reviews .
we trained the different classifiers and fine tuned the parameters of each classifier by applying a fold cross validation on the training and validation set.
furthermore we used the test set to evaluate the final performance of the classifiers and avoid overfitting due to the parameter tuning performed during the cross validation6.
furthermore we used ensemble methods to combine the prediction of the different classification algorithms.
ensemble methods provide techniques to merge a set of classifiers and then predict a new result using the vote of the individual predictions .
the motivation for using ensembles is to emphasize the strengths of different classification algorithms while diluting their weaknesses.
we apply the majority voting scheme to combine the output of the four chosen classification algorithms.
let rbe a review hj r the prediction of the ensemble on review rfor the categoryjandhi j r the prediction of a specific classification algorithmion category jfor reviewr.
further let nbe the number of individual classifiers conforming the ensemble.
we define the majority voting scheme of our ensemble as follows hj r 1ifpn i 1hi j r n 0otherwise 4positive and negative sentiment words were obtained from the predefined lists of the lexical sentiment analysis tool sentistrength 6we used the grid search functionality from scikit for the parameter tuning of each classifier http scikit learn.orgstablemodulesgrid search.htmltable i overview of the truth set app bug report feature strength feature shortcoming user request praise complaint usage scenario noise angrybirds dropbox evernote tripadvisor picsart pininterest whatsapp total where denotes that the review belongs to the jcategory and that it does not.
in our experiment we evaluated three ensembles a b and c .
in ensemble a the classifiers were grouped to vote for the final prediction.
in ensemble b we excluded the naive bayes classifier since it had the worst performance among all individual classifiers.
in ensemble c naive bayes and svm the first and second worst performing algorithms were excluded.
we use three metrics traditionally used in supervised machine learning for evaluating the accuracy of the classifiers precision recall and f measure defined as the harmonic mean of the precision and recall measures .
iv.
e valuation results table ii shows the results of the naive bayes svm logistic regression and neural network classifiers as well as for the three ensembles.
overall the logistic regression and neural network classifiers showed a better precision than the naive bayes and svm models.
furthermore the neural network model had the highest f measure average among all individual classifers whereas the svm and neural network models had the highest recall values.
the ensembles performed similar to the individual classifiers when predicting most categories.
although a high precision could be achieved by solely using logistic regression or a neural network the ensembles achieved higher recall.
however all ensembles failed to detect the complaint category.
nevertheless they managed to identify more of the usage scenario andfeature strength reviews as reflected in the higher recall values when comparing with the individual classifiers.
to get more insight into the performance of each classifier we performed a mcnemar test on the performance of each classifier for predicting each category and comparing supervised classification learning algorithms as done in previous work .
we focused on the f measure while testing our results since it considers both the precision and recall performance.
overall neural networks performed statistically significant better than the other individual approaches.
among the ensembles ensemble c performed the best.
with the exception of the praise and feature shortcoming categories where neural networks performed statistically better ensemble c was statistically better or equal to the other machine learning algorithms or ensembles.
furthermore ensemble c n1 images publications emitza n guzman nortega mcnemartest.pdftended to outperform the individual algorithms with statistical significance with the exception of neural networks were there was almost no statistical significant difference among their performance in the different classification categories.
v. d iscussion a. result interpretation while neural networks had the best performance among the individual classifiers during the training and testing phases it used the most storage space whereas naive bayes required the least.
moreover the neural network was the slowest algorithm.
the number of parameters to be tuned is an indicator for the ease of use of an algorithm.
in this respect the neural network was by far the most complex and most sensitive to tuning.
on the other hand naive bayes was the easiest to use because of its few parameters.
accordingly the results of the naive bayes algorithm could be easily interpreted because of the simple nature of its algorithm that relies on probabilities unlike the neural network logistic regression and svm.
as table ii shows all the classifiers had a similar performance when predicting the praise andbug report categories.
the reason for the precise prediction is the high frequency of certain words observed in the feature selection results.
for example the words crash and fix were highly correlated to thebug report category while the words good awesome and nice were highly correlated to the praise category.
as a result these words provided the best discrimination between the categories.
moreover according to the truth set the reviews that were labeled as praise andbug report were very frequent see table i hence the models were trained using a high number of reviews belonging to these categories.
unfortunately there were few discriminative words for the other categories.
consequently categories such complaint and usage scenario were poorly detected.
however we believe that more reasons might have contributed to the poor performance of the complaint category.
one problematic source might be the annotation guide where definitions could have been misunderstood or apparently similar categories i.e.
complaint and feature shortcoming could have been confused.
to diminish this threat at least one clarification session with each of the annotators and one of the authors was held.
however misunderstandings could still be possible.
moreover we observed some overlapping words between categories in the 2results.
for example within the reviews associated to the complaint andfeature shortcoming categories words such as horrible and terrible overlapped.
in addition reviews labeled as feature shortcoming were far more frequent than those labeled ascomplaint and therefore more prevalent in the training settable ii the individual classifier and ensemble results on the test set.
p stands for precision r for recall and f for f measure.
naive bayes svm logistic regression neural network ensemble a ensemble b ensemble c p r f p r f p r f p r f p r f p r f p r f bug report .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
complaint .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
user request .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
feature shortcoming .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
feature strength .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
noise .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
praise .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
usage scenario .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
see table i .
the same problems applied to praise andfeature strength categories with the difference that in this case the praise category was more prevalent in the truth set than the feature strength category.
we noticed that some of the features were redundant such as good and gud two words with the same meaning but different spelling.
the stemming step is not able to fix such issue therefore a spell checker method could provide an improvement to our preprocessing step by removing duplicate words.
we also observed that some of the reviews associated to the praise category were actually sarcasm.
the poor prediction performance regarding certain categories might be also a result of the limitations of the binary problem transformation performed in the preprocessing of the classification step.
the ground assumption of the category independence is a key disadvantage of this approach.
to overcome this issue capturing the loss information through the introduction of category dependency might be an improvement to our work.
b. threats to validity a construct validity threat in our study was the creation of the truth set.
annotators might have misconceptions about the categories included in our taxonomy and could therefore erroneously label the truth set.
we tried to reduce this threat by providing an annotation guide with detailed definitions and by holding a trial run of the annotation task with each annotator and discussing the occurred errors with one of the authors.
misconceptions between annotators were also handled by labeling each review twice and handling disagreements between labels by detecting problematic annotators and giving a higher vote to annotators who had a low error rate when labeling the relevant category.
reporting classification results obtained on the validation set when fine tuning parameter models during the fold cross validation can result in overfitting and therefore results can be overly optimistic.
we handled this threat by fine tuning the parameters on the validation set during the fold cross validation and then reporting the results of the classifier on the previously unseen test set.
the classification step of our approach was evaluated on app reviews from different apps from two different app stores from a wide range of categories and with reviews written in different styles and with varying vocabularies.
however the results cannot be generalized for all types of apps.vi.
r elated work we focus our related work on two main research directions mining app store reviews and the classification of software artifacts that contain natural language.
harman et al.
introduced app store mining and analyzed technical and business aspects of apps by extracting app features from the official app descriptions.
chandy and gu classified spam in the appstore through a latent model capable of classifying app reviews into the normal and malicious categories.
iacob and harrison extracted feature requests from app store reviews by means of linguistic rules and used latent dirichlet allocation lda to group the feature requests.
galvis carre no and winbladh applied lda to summarize user reviews whereas guzman and maalej extracted app features and their sentiments using a collocation algorithm and lexical sentiment analysis.
fu et al.
applied a linear regression model combining the text from user reviews and its ratings to identify words with a negative connotations.
they inputed these words into an lda model to find the main reason why users are unsatisfied with the app.
li et al.
analyze user reviews to measure user satisfaction by matching words or phrases in the user comments with a predefined dictionary.
in contrast we are interested not only in the satisfaction of users but also on their requests failure reports and the description of the scenarios where they are using the app.
chen et al.
used naive bayes for finding informative review sentences.
additionally they rank the groups of reviews according to a scheme which analyzes volume time patterns and ratings.
our approach could be extended to include a review ranking and could also benefit from the filtering of uninformative reviews.
automatic classification of different software artifacts has received widespread attention from the community.
the work that is perhaps most similar to ours is that of panichella et al.
who classified user reviews into a taxonomy created after the analysis of developer emails.
in their approach they use linguistic rules and machine learning for classifying the reviews.
in comparison our approach does not make use of predefined rules and the taxonomy presented in this paper can map to finer grained evolution tasks and was created from content present in app store reviews.
furthermore we analyze the performance of the combination of individual classifiers or ensembles.
bacchelli et al.
presented an approach to classify useful information from development emails using naive bayes and a natural language parser.
antoniol et al.
built classifiers to categorize posts in bug tracking systems as either bugs or change requests.
furthermore pingclasai etal.
proposed an alternative for classifying bug reports based on topic modeling whereas zhou et al.
applied machine learning techniques for classifying bug reports based on structured and unstructured text.
machine learning approaches have also been applied to classify software blogs and the content in api reference documentation .
vii.
c onclusions and future work we presented a taxonomy for ordering app reviews into categories that are relevant for software evolution.
furthermore we performed an experiment to compare the performance of different machine learning approaches into the categories defined in our taxonomy.
our results for the classification of the reviews are encouraging.
we achieved an average precision of .
recall of .
and f measure of .
for the neural network the best performing classifier.
furthermore we show that when combining the predictions of logistic regression and neural networks our precision remains similar to the best performing individual classifiers however the recall values improve.
our approach could be used by software developers and others involved in software evolution for analyzing user reviews and prioritizing their tasks.
we believe that our approach can be extended to automatically rank the categorized reviews by taking ratings and sentiments contained in the review comments into consideration.
in this way reviews with a more negative sentiment or rating could be given a higher priority.
additionally mechanisms to summarize and visualize the content of the classified reviews could further reduce the processing effort.
acknowledgment this work was partially funded by the mexican council of science and technology conacyt .