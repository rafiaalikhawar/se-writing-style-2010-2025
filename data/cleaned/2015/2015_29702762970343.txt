sofia an automated security oracle for black box testing of sql injection vulnerabilities mariano ceccato fondazione bruno kessler trento italy ceccato fbk.eucu d. nguyen dennis appelt lionel c. briand snt centre university of luxembourg luxembourg duy.nguyen dennis.appelt lionel.briand uni.lu abstract security testing is a pivotal activity in engineering secure software.
it consists of two phases generating attack inputs to test the system and assessing whether test executions expose any vulnerabilities.
the latter phase is known as the security oracle problem.
in this work we present sofia a security oracle for sqlinjection vulnerabilities.
sofia is programming language and source code independent and can be used with various attack generation tools.
moreover because it does not rely on known attacks for learning sofia is meant to also detect types of sqli attacks that might be unknown at learning time.
the oracle challenge is recast as a one class classification problem where we learn to characterise legitimate sql statements to accurately distinguish them from sqli attack statements.
we have carried out an experimental validation on six applications among which two are large and widely used.
sofia was used to detect real sqli vulnerabilities with inputs generated by three attack generation tools.
the obtained results show that sofia is computationally fast and achieves a recall rate of i.e.
missing no attacks with a low false positive rate .
.
ccs concepts security and privacy !web application security penetration testing software and its engineering !software testing and debugging keywords security testing security oracle sql injection .
introduction sql injection sqli vulnerabilities are amongst the top security threats to web based software systems .
such vulnerabilities stem from defects in data validation procedures such that when an attacker provides input values that contain fragments of sql code they eventually get injected into sql queries that are executed on databases.
with such a vulnerability attackers can run arbitrary malicious code on databases to acquire or compromisesensitive data such as medical records or financial transactions.
the impact of sqli exploitations can range from enabling fraud to compromising an organisation s reputation or even shutting down its activities.
though the root cause of sqli has been well studied in reality mostly due to time constraints and undisciplined development practices many systems remain vulnerable to sqli .
when engineering secure software systems and services software testing is one of the main practices to detect faults as well as security vulnerabilities.
security testing also called penetration testing is a branch of software testing devoted to stress programs with respect to their security features with the aim of identifying vulnerabilities.
security testing involves two major challenges generating input values referred to as test payloads intended to exercise vulnerabilities and evaluating whether such payloads manage to expose an actual vulnerability.
the security oracle addresses the latter.
security testing is highly expensive given the complexity of modern systems typically providing a wide range of services and the sophistication of attacks and exploitations.
to reduce effort and cost the research community has focused on automating security testing.
regarding sqli the test input generation problem has been extensively investigated and automated approaches are available .
automating the test oracle problem for sqli vulnerabilities however remains an open problem.
this is a significant obstacle to test automation as manual oracles severely limit the number of test execution results a test team can process .
in this work we present sofia a security oracle for sqli attacks.
our goal is to satisfy three important requirements.
first it must be independent from known attack instances so that new types of attacks can be detected in the future.
this is an important leap forward since existing solutions based on attack patterns can only detect publicly known and documented attacks.
second the oracles should not rely on knowledge about test input data or their generation algorithm in order to be usable with any given test generation tool.
third our proposed oracle should not require the source code of the sut since we target black box testing.
this is often a mandatory requirement for external security testing carried out by third party penetration testers or for systems whose source code is not available.
most of the existing sqli oracles either require known attacks in the learning phase or access to source code .
the few approaches that still meet all the three requirements are fundamentally different than our solution in ways that affect recall and false positive rates.
whereas they detect user inputs in sql statements and compare them with user inputs observed at learning time we prune data from sql statements and compare their parse trees.
by comparing structure instead of data our permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
goal is to enable sofia to yield high recall and low false positive rates.
the main motivation is that modelling all possible safe data is highly difficult if feasible at all and false positives are caused by incomplete models.
further we observed that a change in query structure is the most direct manifestation of an sqli attack.
sofia is built using one class machine classification.
sql statements issued by a sut to its database are logged and parsed to create sql parse trees which are fed to a clustering algorithm.
the tree edit distance is used to measure the distance amongst parse trees.
our approach consists of two phases training and testing .
during training legitimate sql statements which are obtained from regular executions are grouped into clusters of similar statements.
we refer to this set of clusters as a safe model .
such a model represents legitimate database accesses in the absence of attacks.
in the testing phase when test inputs trigger new sql statements our oracle assesses whether the statements can be assigned to the clusters of the safe model.
in the positive case we can assert that the statements are safe and no vulnerability is reported.
otherwise such statements are classified as anomalous and hence vulnerability alerts are reported.
we have carried out an experimental evaluation in terms of false positive rate recall rate and computational cost on six real applications and with three different attack generation tools.
the obtained results show that the proposed oracle achieves a very low false positive rate .
and misses no attack recall with a low computational overhead.
the proposed oracle is meant to support security testing by classifying sql statements triggered by test cases as legitimate statements or as sqli attacks.
however since it relies on a black box strategy and is trained only on legitimate executions it could be also deployed as a database firewall in production to filter sql statements and block sqli attacks before they are actually executed.
investigating such potential application is out of the scope of this paper though and we present and assess the proposed oracle only in the context of security testing.
the next section provides background and discusses related work.
in section we discuss the requirements and our strategy for the security oracle.
in section we present in detail our approach.
section reports our experiments to assess the accuracy and speed of the oracle.
finally section concludes our work.
.
background sql injection vulnerabilities .
in systems that use databases the sql statements that query the back end database are usually treated by the native application code as strings.
these strings are formed by concatenating different string fragments based on user choices or the application s control flow.
for example an sql statement can be formed as follows sql select from hotellist where country sql sql .
country sql sql .
result mysql query sql or die mysql error the variable country is an input provided by the user which is concatenated with the rest of the sql statement and then stored in the string variable sql.
the string is then passed to the function mysql query that sends the sql statement to the database server to be executed.
sqli is an attack technique in which attackers inject malicious sql code fragments into input parameters that lack proper validation or sanitisation.
an attacker might construct input values in a way that changes the behaviour of the resulting sql statement and performs arbitrary actions on the database e.g.
exposure of sensi tive data insertion or alteration of data without authorisation loss of data or even taking control of the database server .
in the previous example if the input country received the attack payload or the resulting sql statement is select from hotellist where country or1 the clause or is a tautology i.e.
the condition will always be true and is thus able to bypass the original condition in the where clause making the sql query return all rows in the table.
hence the above piece of code is vulnerable to sqli attacks.
in security testing for sqli one important task is to generate such attack payloads so that they can circumvent projection layers e.g.
web application firewalls or input filtering inject into vulnerable sql code and trigger executable sql statements .
another equally important task is the oracle problem that is how to assess whether a test payload detects an sqli vulnerability.
the traditional way in a black box testing context is to analyse the responses of the sut to which legitimate or attack inputs are sent or to analyse the triggered sql statements for common attack patterns.
we elaborate the limitations of such techniques further in the next section.
tree edit distance .
ordered labelled trees refer to a tree structure in which nodes are labelled and edges capture predecessorsuccessor relationships amongst nodes.
the left to right order amongst siblings is also significant to the semantics of the trees.
parse trees that structure sentences or programs according to some context free grammars are ordered labelled trees.
transforming one ordered labelled tree or just tree for brevity into another involves three basic types of edit operations changing a node label delete a node and insert a node .
as an example taking the trees t1andt2in figure transforming t1intot2can be performed with a sequence of four edit operations change e to k delete c delete d add h or alternatively change e to k change d to h delete c .
fedcabfkhbat1t2 figure an example of two ordered labelled trees.
formally each edit operation oiis assigned a cost usually one unit .
the cost of a sequence of edit operations sj ho1 o2 o ni is the sum of the cost of all operations oi.
since there are usually alternative sequences to transform a tree into another the tree edit distance between two trees is the minimum cost amongst possible sequences.
when the cost of all edit operations is equal to one then the edit distance between two trees is the number of operations of the shortest sequence that transforms one tree into the other.
.
requirements and general strategy .
security oracle requirements most of the classifiers used as security oracles are learned on a training set that contains both positive and negative examples attacks and legitimate executions .
however these approaches 168are expected to suffer from two main limitations namely i the availability of attacks and ii the representativeness of attacks especially when the sample is small as in most practical contexts.
documented attacks are usually unavailable for many systems.
some ethical attackers make their techniques and payloads available on the internet but software developers are usually unaware of them.
even if they are the number of known attacks is limited.
we also need to account for unknown new attacks that may appear after the system s deployment.
as a result a security oracle would be much more beneficial for testing and also in other contexts like monitoring in production if it does not require the availability of documented attacks.
in addition even when available attacks used to train security oracle classifiers are often expected to be representative of possible attacks that could target a system.
unfortunately this is normally not the case as new attacks are being introduced at a very fast pace.
therefore a classifier should not rely on documented attacks at the risk of being ineffective with new ones.
thus the first requirement for a security oracle is the following requirement req the security oracle should be independent from known instances of successful attacks.
often in security testing oracles depend on knowledge about what attack generation algorithm and what test input data have been used to determine the expected output if there is a vulnerability.
for example some oracles classify a test as a successful attack when the execution output contains the same attack payloads as the inputs .
this strategy suffers from the observability problem as the output can be masked by a generic error message or worse the impact of the successful attack cannot be easily observed by the tester e.g.
like in second order sqli attacks.
furthermore this strategy because it is specific to test generation algorithms or input data limits the portability of the oracle.
as a result it may not work with most attack generation tools without additional adaptation overhead.
it is desirable to define an oracle that is independent from attack generation strategies so that it can be used with many attack tools and a variety of attack generation approaches.
the second requirement is thus requirement req the security oracle has no knowledge on what input data are used to test the system.
often systems are written using frameworks and third part libraries.
since commercial libraries are rarely distributed with source code a white box approach in the generation of a security oracle is of limited applicability in real industrial settings.
in many contexts such as for third party penetration testers access to source code is not an option.
thus the third requirement is requirement req the security oracle should not rely on the source code of the sut.
.
our strategy our strategy to create a security oracle for sqli vulnerabilities that satisfies the above requirements relies on a black box security safe model that is a model of safe execution inputs.
safe model to be independent from known attacks and attack tools we decided to exclude attacks from the training set used to learn the security oracle.
we propose a security oracle that only takes into consideration legitimate executions and builds a model of safe sql statements.
tests will be classified as legitimate if they generate sql statements satisfying this safe model and potential attacks otherwise.black box the sql statements sent by the sut to the backend database are the only features considered by the oracle either in the training phase and in the testing phase.
as a result such an oracle can be deployed to test systems developed in many languages and has no dependency or limitation regarding specific development frameworks.
the proposed safe model depends on the specific legitimate executions that are being considered.
however using classical blackbox testing techniques it is much easier to generate a large number of representative legitimate inputs than it is to generate attacks.
the next section explains in detail the process to construct our security oracle.
.
sofia the security oracle the procedure to build and apply the security oracle is summarized in figure .
it consists of two phases training andtesting including five steps parsing pruning computing distance clustering and classification .
these two phases share the first three steps.
clustering is exclusively part of training while classificationis exclusively part of testing.
the process starts with a set of sql statements obtained from safe executions of a sut either by executing functional tests or by monitoring regular system executions.
the sql statements are parsed and the parse trees represent the objects to be classified.
the fact that our oracle uses only legitimate statements allows us to avoid the task of manual labelling training data as legitimate statements or attacks as often required by other supervised techniques.
information from the parse trees which is specific to concrete sql statements and irrelevant for detecting attacks is removed by pruning the parse trees.
this helps not only in reducing the number of unique trees to be clustered and better scale but also improves the overall attack detection performance.
clustering relies on the edit distance amongst pruned parse trees.
clustering is used to group together similar sql statements.
statements with low distance are assigned to the same cluster while statements with larger distance are assigned to different clusters.
the final safe model consists of the optimal partition of sql statements computed by clustering.
note that the training process that creates safe models from sql execution logs takes place only once.
safe models are then ready to support security testing in detecting sqli vulnerabilities.
new statements triggered by executing security tests will be classified using the safe model by assessing their distance to the centres of the clusters.
if a new statement is close enough to a cluster centre it satisfies the model and is classified as benign statement.
conversely in the case a new sql statement does not fit into any existing cluster it is considered anomalous and classified as a potential attack.
further details of this process are provided in the sections that follow with the help of a running example.
regarding the defined requirements for the security oracle requirement req independence from known attacks is satisfied since our approach relies only on logs of benign executions.
moreover the fact that only database logs are considered by the oracle ensures that requirement req 3is also achieved no access to the application source code is required.
our classification procedure complies with requirement req 2since it exclusively relies on the sql statements sent to the database and not the test case input values.
.
training data training data are used to construct the security model.
however differently from other approaches that require both attack and 169testingtrainingsqlexecutionlogsparsetrees clusteringprunedparse treesdistancematrixsafe modelclassificationcomputingdistancenewsqlstatements safe attackparsingpruning process flowi olegend figure the overview sofia the training process for learning safe models from sql execution logs the classification process for classifying new sql statements.
legitimate sql statements our approach only relies on the latter to learn a classifier.
in fact our goal is to construct a model of safeexecutions and classify as anomalous everything that does not conform to this model.
training data are collected by executing the functional test suite of a sut or by monitoring its usage during production or acceptance testing.
we collect execution logs containing sql statements executed on the database.
to this end different technologies can be used depending on the underlying dbms.
for mysql we use themysql proxy tool1that monitors all traffic to and from mysql databases.
this solution can be ported to other dbmss as well.
for example for java applications that use a jdbc driver to connect to oracle database we can customise the driver to log sql statements.
let us use a running example where the execution log includes the sql statements shown in figure .
while the three statements query the user andpassword columns from the table users they differ from one another in the where conditions.
stmt1 select user password from users where id stmt2 select user password from users where id stmt3 select user password from users where id and role figure three samples of sql log of the running example.
.
parsing an sqli attack modifies the semantic of sql statements usually by replacing a value with a piece of sql code for example by adding a tautology to the where clause or by injecting an additional select orunion statement.
these injections if successful result in sql queries that are valid sql statements according to the sql but yet have different parse trees.
we resort to the parse trees of sql statements to detect sqli attacks.
in our work we rely on the general sql parser2 gsp for short .
gsp is a java library that supports various dbmss including oracle sql server db2 mysql teradata and access.
output parse trees are stored as xml documents for other analysis.
figure shows the parse trees for the sql statements of our running example.
as we can see in the figure the parse trees contain information that is irrelevant and therefore detrimental to the process of learning a classifer through clustering e.g.
specific user ids.
indeed some elements in the trees are very specific to the captured sql executions and are irrelevant for the detection of attacks.
the pruning process described next aims at removing such irrelevant information from the parse trees.
.
pruning pruning could be done according to different strategies depending on what piece of information should be removed.
to decide about the most effective pruning in our context we should consider how attacks are typically carried out.
sqli attacks aim at altering sql statements by replacing data with a new piece of sql code i.e.
a string or numeric literal is replaced by code.
thus two legitimate sql statements collected during the execution of the same feature but with different input data should only differ in terms of data values.
instead a legitimate statement and an attack statement should differ not only with respect to data but also the sql command structure in the maliciously injected part.
based on the above consideration we decided to prune data values in parse trees we replace all the constant numeric and string values in the tree with the same placeholder e.g.
with the empty string or with the constant zero .
as a result statements that just differ in values are characterised by a single pruned tree.
figure shows the pruned parse trees of our running example.
nodes with data values are replaced with a place holder the ?character .
since stmt1 andstmt2 only differ in data value of the idattribute their pruned versions are equivalent.
for learning purposes redundant versions of the same pruned trees will be discarded.
out of the three statements of the training set for the running example only two distinct pruned trees will be considered to construct the safe model.
our pruning procedure is straightforward.
we analyse the xml parse trees and replace the content of the leaf nodes of type tconstant which contain concrete data values with a placeholder.
.
computing distance parse trees of sql statements are ordered and labelled trees.
a metric of tree edit distance for this class of trees has been proposed by zhang et al.
as discussed in section .
zhang et al.
have also proposed a fast algorithm to calculate tree edit distances in a polynomial time complexity.
our work makes use of a tool called approxlib3 that implements this specific algorithm.
let us consider the sql statement of the attack in figure a for which the corresponding pruned parse tree is shown in figure c .
we note that this tree is quite similar to the parse tree of the legitimate statement in figure b .
in fact we note that these trees are more similar distance is than the two legitimate statements stmt1 andstmt3 distance is .
this example highlights the fact that parse tree distance alone is not enough to detect attacks.
we need to infer a classification amongst legitimate statements in this case using clustering to group similar trees and compare a candidate attack with its closest cluster.
the underlying rationale augsten src select userpassword where id a stmt1 select userpassword where id b stmt2 select userpassword where and id role c stmt3 figure parse trees of the sql statements.
select userpassword where id ?
a stmt1 and stmt2 select userpassword where and id ?
role ?
b stmt3 figure pruned parse trees of the example sql statements.
is that though a legitimate sql statement does not obviously need to be similar to all legitimate statements resulting from test executions there should be a cluster with similar statements.
once parse tree distances are computed amongst all the pairs of pruned parse trees an algorithm can be applied to generate an optimal set of clusters as described next.
.
clustering we address our clustering problem using the k me algorithm .
this algorithm is a variant of k means clustering based on the search for krepresentative samples namely the me amongst the observations of the dataset.
after finding a set of k me kclusters are constructed by assigning each observation to the nearest me we need to find krepresentative samples that minimize the sum of the dissimilarities of the observations to their closest representative object.
furthermore we consider a measure of cluster diameter .
the diameter of a cluster is the maximal distance between the observations in the cluster and its me to build a safe model training data are clustered into kclusters characterised by their me and diameters.
the adoption of k me clustering is more appropriate in our context than the standard k means approach.
k means involves the notion of mean point which in our case would mean an average tree for all the observations in the same cluster.
since such a hypothetical tree is insensible in our context we adopt kme instead.
it picks a representative element for each cluster i.e.
the me instead of computing a fictitious average tree.
it is very important to identify the appropriate number of clustersk.
clusters should be small enough to distinguish attacks from legitimate statements based on parse tree distances but clusters should also be large enough to capture representative groups of similar legitimate statements.
more specifically a too small value ofkwould elaborate a partition that contains few large clusters.
a large cluster would contain very different parse trees with large distances from each other and its me would not be representative of all the members of the clusters.
also with large clusters the distance between an attack and the cluster me may be compa rable to the cluster diameter.
thus actual attacks would be wrongly classified as legitimate statements false negatives .
on the other hand a large number of clusters kmay result in many clusters that contain too few elements to be representative and enable reliable comparisons with new parse trees.
false alarms false positives may result from new legitimate statements whose tree is at a distance from the closest me that is higher than the cluster s diameter.
to decide the most appropriate number of clusters i.e.
the value ofk we adopt a standard approach the akaike information criterion aic .
it entails balancing the trade off between the goodness of fit of the model and the size of the model.
the underlying rationale is to increase the complexity of the model i.e.
kincreases as far as the gain in precision is high and stop when the increase in precision is not significant.
to achieve this objective we adopt a penalty factor for each new cluster.
to determine the number of clusters in this way we select the best kthat minimizes the fitness function fcomposed of two terms i distortion a measure of the extent to which sql statements deviate from the prototype of their clusters e.g.
rss for k me and ii the model complexity that is proportional to the number of clusters f k rss k m k where rss is the residue sum of square i.e.
the error that we commit by approximating each observation in a cluster by the corresponding me and mis the dimensionality of the vector space.
in our case m because the only feature used in clustering is the tree edit distance.
the resulting optimal set of clusters each associated with a me and diameter represents the safe model used as an oracle to classify newly executed sql statements.
table shows the final clustering configuration for the running example.
table clustering results for the running example.
cluster me elements stmt1 stmt1 stmt2 stmt3 stmt3 .
classification intercepted sql statements undergo the testing classifying process depicted in figure they are parsed pruned and eventually classified as safe or malicious.
the classification procedure is described with respect to the test sample a malicious statement shown in figure and consists of the following steps .
parsing and pruning when the test sql statement figure a is intercepted by our database proxy it is parsed figure b malicious injected code highlighted in red and pruned figure c pruning highlighted in green as described previously.
.
computing distance the tree edit distance is used to identify the closest cluster in the safe model.
to compute this result the pruned parse tree tof the test figure c is compared to the pruned parse tree of each me in our example the distances from the me are shown in figure d as and respectively.
the me with the smallest distance from the test stmt3 in our example determines the nearest cluster cluster .
determining the nearest cluster is fast since the me parse trees are precomputed and only ktree edit distances need to be evaluated.
in the example only two comparisons are required.
.
distance versus diameter classification we check whether the test fits the nearest cluster by comparing its diameter and the distance between the test parse tree and me alternative measures could be used such as the distance corresponding to the 95th percentile of cluster elements instead of its diameter to deal with potential outliers.
however this is out of scope for this work.
in our example the diameter4of cluster is equal to .
we compare the distance between the test tand the nearest me distance is with the diameter equals to .
when the distance to the me is smaller than or equal to the diameter the test is deemed to fit this cluster and it is classified as a safe execution.
however in our example the test falls outside of the cluster border distance diameter and is classified as a potential attack.
.
experimental ev aluation this section presents the experimental evaluation designed and conducted to assess the proposed security oracle.
.
research questions and variable selection we investigate the following main research questions rq how accurate is sofia in classifying legitimate sql statements and sql injection attacks?
rq how fast is sofia in classifying sql statements as legitimate or attacks?
rq how does sofia compare to main stream alternative tools in terms of accuracy and speed?
the first research question concerns the accuracy of the oracle in classifying sql statements.
missed attacks may lead to unaddressed security defects and false alarms lead to significant wasted effort which should remain within reasonable bounds.
the second research question is about the amount of time taken by the classifier to make a decision on a newly observed sql statement.
fast run time classification of attacks during testing is important to support efficient test automation.
the third research questions is meant to compare sofia with available and comparable alternative solutions which can be considered a baseline on which to improve both in terms of classification accuracy and speed.
4because of the small size of the running example each cluster contains just one pruned parse tree so diameter is equal to .select user password from users where id orrole role a statement select userpassword where or id role role b parse tree select userpassword where or id ?
role role c pruned parse tree cluster me diameter test distance stmt1 stmt3 d cluster diameters and distance figure classification of a malicious statement.
accuracy in our context is characterised by attack detection rate we want to detect as many attacks as possible and false positive rate.
we need to minimise false positives as they trigger unnecessary manual analysis which is expensive.
the more false positives the more analysis effort is being wasted and the scalability of the approach is being compromised.
accuracy is quantified by the standard recall andfpr false positive rate metrics from information retrieval true positives tp the number of actual attacks that are correctly classified by the oracle as attacks false positives fp the number of legitimate statements that are incorrectly classified by the oracle as attacks true negatives tn the number of legitimate statements that are correctly classified by the oracle as safe false negatives fn the number of actual attacks that are incorrectly classified by the oracle as safe statements recall the ratio between the correctly detected attacks and all the actual attackstp tp fn fpr the ratio between false positives and all the actual legitimate statementsfp fp tn classification time is measured as follows c time amount of time spent by the classifier to make a decision on a test outcome.
time is measured by instrumenting the oracle.
system time is probed before starting and after concluding the complete classification procedure for each sql statement.
it includes the amount 172of time spent for parsing the statement pruning its tree and the amount of time for computing its distances to the me of the safe model oracle.
the amount of time required to train the oracle is less interesting because training is done just once in a while and has therefore limited practical implications.
it will not be further discussed here.
.
subject applications the subject applications considered in this study are web applications and web services that use an sql relational database.
moreover since the security oracle will be assessed on real vulnerabilities we selected applications by inspecting their bug tracking systems and the common vulnerabilities and exposures repository5that keeps track of publicly known vulnerabilities and exposures.
the chosen subject applications contain real sql injection vulnerabilities.
the applications considered in the study are hotelrs written in php hotelrs is a service oriented based system providing web services for room reservation.
it was developed and used by coffey et al.
.
sugarcrm written in php sugarcrm is a popular customer relationship management system6.
taskfreak written in php taskfreak is a web project management application7.
theorganizer a web application that supports management and organisation of the activities in a personal agenda8.
the server is written in java using servlets j2ee and spring jdbc .
wordpress written in php wordpress is a popular blogging and news publishing platform9.wordpress has many utility plugins that are vulnerable to sqli.
we have two variants of wordpress one with the newstatpress plugin10that provides access statistics to wordpress the other with landing pages11 a plugin for customising templates and attracting more visits to blogging sites.
we use the test suites of the subject applications for generating training data.
wordpress comes with a large test suite of more than phpunit test cases.
for taskfreak andtheorganizer we reuse the test suites generated by available techniques and its accompanying tool12.
for the remaining two hotelrs andsugarcrm we manually defined test suites that exercise all operations of their web services with various domain inputs.
our reliance on real vulnerabilities in applications makes our results more representative of the current situation though it is impossible to predict what these results will be with future types of vulnerabilities.
however as described above because our approach does not learn from specific attacks and relies on learning to characterise safe statements we hope that the safe model will be able to handle future types of vulnerabilities as well.
.
attack generation to evaluate the classifier both legitimate executions and attacks are required.
however our oracle is independent of the input data generation strategy adopted to generate the attacks.
thus we will the accuracy of the oracle with diverse attack generation tools burpsuite a commercial security testing tool suite13.
it has a vulnerability scanner that targets many types of vulnerabilities including those in the owasp top .
for detecting sqli burpsuite version .
.
has a fixed list of built in sqli test payloads such as a or replace drop table when scanning for sqli vulnerabilities burpsuite uses these payloads as request parameters and submits them to a target system.
it then analyses the obtained responses from the system to detect sql code or error messages in order to report sqli issues.
sqlmap a popular open source tool for penetration testers to detect and exploit sqli vulnerabilities14.
it supports various database management systems and implements many heuristics to generate test payloads for different types of sqli including boolean based blind time based blind error based union query based stacked queries and out of band .
xavier a framework for the automated testing of web services for sqli vulnerabilities .
powered by a grammar developed specifically for sqli attacks and machine learning xavier can generate diverse test payloads that can bypass web application firewalls and detect sqli vulnerabilities.
.
alternative oracles apart from assessing sofia on diverse applications it is interesting to investigate how it fares when compared to existing tools with similar goals.
we found only two alternative tools antisqli andgreensql15.
antisqli is a tool provided by the vendor of the sql parser we use in our work which takes log files containing sql statements as inputs and reports whether their content is classified as attacks or legitimate statements.
greensql is a popular database security solution for controlling database accesses blocking sqli attacks among other features.
it intercepts communications between applications and databases learns patterns of regular sql statements and then blocks malicious ones from getting to databases under protection.
.
experimental procedure to collect sql statements we install and configure the subject applications each on a separate virtual machine having mysql and mysql proxy ready.
mysql proxy helps intercept and log all the sql statements that an application sends to its database.
we then execute the test suite that comes with each application to collect legitimate executions i.e.
safe statements.
after that we run the attack tools to generate attacks.
the logs of attack tools are manually analysed and statements are labelled as attack or safe.
such analysis is required as safe statements can result from attacks since the system might perform routine updates or run additional queries before executing the attack statements.
note that the labelling task is only relevant for our assessment purposes it is not needed in the real usage of the oracle.
we adopt a fold validation strategy to check our oracle on different partitions of training and testing data.
the training data of each subject application consisting exclusively of safe statements 173is divided randomly into sets of approximately the same size to form partitions nine sets of legitimate statements represent the training set .
they are used to train the safe model of the oracle.
in our case this is done only on legitimate statements and there is no need to split attacks across training and testing sets the remaining set of legitimate statements is merged with the attacks to form the testing set .
the oracle is used to classify each entry in the testing set using the safe model.
this process is iterated times once per each of the possible partitions.
the classification elaborated by the oracle for the testing set is compared with the actual labelling to evaluate the oracle accuracy.
the fold validation including training and testing for all subject applications was executed using a hpc high performance computing system at the university of luxembourg where the cpu speed on nodes is .26ghz and a 4gb ram was available to each process.
.
experimental results table reports the number of sql statements per application that have been considered in our study.
the first two columns contain the name of the application and the tool used to generate the attacks respectively.
the subsequent columns report the number of legitimate statements and the number of successful attacks.
the last two columns indicate the number of distinct pruned trees for legitimate statements and attacks.
we cannot explore all combinations subject applications and tools because of certain application characteristics web based and web services and the intended usage of the tools xavier targets web services while the others target standard web based applications.
in total we obtain nine datasets for the experimental evaluation.
table summary of the datasets used in our experiment nine datasets obtained from six applications and three attack tools.
application t. tool legit.
attack pruned pruned legit.
attack hotelrs xavier sugarcrm xavier taskfreak burpsuite taskfreak sqlmap theorganizer burpsuite theorganizer sqlmap wordpressnewstatpressburpsuite wordpressnewstatpresssqlmap wordpresslandingpagesqlmap we can observe based on the data shown in table that the datasets used in our experiments are diverse in terms of the number of safe statements ranging from 1k to 170k.
likewise the number of attack statements generated by different tools varies from three to attacks.
also we can see that the number of parse trees after pruning is significantly reduced.
for example for subjectwordpress landingpage and tool sqlmap there were originally more than 171k safe statements and attacks.
however after pruning there are only safe and attack cases left respectively.
overall the percentage of reduction for all subject applications after tree pruning ranges from to more than .
as a result pruning helps reducing the time required by sofia especially for training.
note that burpsuite cannot generate any attackonwordpress landingpage and therefore this pair is not investigated.
table provides experimental results.
for each application the performance of sofia is measured using recall false positive rate and c time as previously described.
the values in the table represent the average over the partitions of training testing data and executions.
table results of our approach data averaged from fold cross validation.
t. ms is classification c time measured in millisecond.
app.
tool tp fp tn fn recall fpr t. ms hotelrs xavier1 .
.
.
.
.
sugarcrm xavier196 .
.
.
.
.
taskfreak burpsuite3 .
.
.
.
.
taskfreak sqlmap4 .
.
.
.
.
theorganizer burpsuite28 .
.
.
.
.
theorganizer sqlmap27 .
.
.
.
.
wordpressnewstat burpsuite4 .
.
.
.
.
wordpressnewstat sqlmap170 .
.
.
.
.
wordpresslandingpage sqlmap314 .
.
.
.
.
regarding rq results in table show that sofia yields a perfect recall of for all the subject applications and achieves a very low false positive rate across all applications .
at the highest .
when we consider the absolute number of false positives fp they are mostly below .
on average.
the highest fp is only .
therefore suggesting that manual analysis is feasible even in the worst case.
thus we can provide a clear answer to rq sofia yields a very high accuracy when classifying both legitimate sql statements and attacks.
moreover considering rq the time required to classify a new statement is small with an average across case studies of 92ms and a median of 29ms.
for most of the case studies classification takes around 30ms per statement with the exception of sugar xavier that takes more than 400ms.
the reason for this difference is that on average the parse trees of sql statements used by sugar are larger and thus lead to longer tree edit distance calculations.
thus we can answer rq sofia is fast in classifying sql statements taking on average 92ms per classification.
furthermore to answer rq we compared sofia to antisqli andgreensql .
following the same procedure as for sofia we ran these two industrial tools against all nine datasets and measured their accuracy and time.
greensql was given the same training data that had been used to train sofia.
antisqli inspects sql statements based on its own sqli filters and therefore no training was needed.
the same testing data were then checked by greensql andantisqli .
174for each dataset tp and fn were measured by counting the number of attack sql statements correctly classified as attacks or incorrectly classified as safe respectively.
likewise we measured the average number of safe statements classified as attacks fp and safe tn .
further we measured the average execution time the tools required to parse and check a statement.
table results of antisqli andgreensql .t.
ms is the average time in millisecond the tools need to process one statement.
app tool tp fp tn fn recall fpr t. ms antisqli hotelrs xavier1 .
.
.
.
sugarcrm xavier50 .
.
.
.
taskfreak burpsuite1 .
.
.
.
taskfreak sqlmap4 .
.
.
.
theorganizer burpsuite28 .
.
.
.
theorganizer sqlmap27 .
.
.
.
wordpressnewstatpress burpsuite0 .
.
.
wordpressnewstatpress sqlmap154 .
.
.
.
.
wordpresslandingpage sqlmap155 .
.
.
.
greensql hotelrs xavier1 .
.
.
.
.
sugarcrm xavier133 .
.
.
.
.
taskfreak burpsuite3 .
.
.
.
taskfreak sqlmap4 .
.
.
.
theorganizer burpsuite28 .
.
.
.
theorganizer sqlmap27 .
.
.
.
wordpressnewstatpress burpsuite4 .
.
.
.
wordpressnewstatpress sqlmap170 .
.
.
.
wordpresslandingpage sqlmap314 .
.
.
.
table shows the recall fpr and execution time of antisqli andgreensql .
we can observe that fpr and recall of antisqli are significantly worse compared to those of sofia.
antisqli missed many attacks on all the case studies fn and as a result recall of some cases is very low.
it wrongly classified safe statements as attacks high fp rate on many cases leading to a poor fpr of .
or .
.
in particular no attack was correctly identified on one case tp .
greensql is as good as sofia in all but one subject sugarcrm with respect to recall.
however greensql reported many false positives that resulted in an order of magnitude higher fpr up to .
as compared to that of sofia .
.
regarding the time taken to process an sql file antisqli takes on average 300ms which is higher than the average time requiredby sofia 92ms .
greensql takes only about 5ms and is therefore faster than sofia at the cost of a much higher number of false positives.
it is worth noticing that greensql is a leading industrial tool while sofia is a currently research prototype.
besides because of technical reasons greensql could not run on the hpc but on a server that happened to have a higher cpu frequency than the computer used for sofia and antisqli .
this setting clearly favoured greensql in detecting attacks faster and prevents us from drawing objective conclusions regarding its comparison with sofia regarding its run time speed.
to compare accuracy and classification time we used the nonparametric wilcoxon test.
the use of non parametric tests requires no distributional assumption.
such a test checks whether differences in performance recorded for sofia and antisqli are statistically significant16.
results show that sofia performs significantly better than antisqli with respect to recall fpr and time pvalues are respectively .
.
and .
.
similarly sofia fares significantly better than greensql in terms of fpr p value .
.
thus we can provide a clear answer to rq sofia is significantly more accurate than antisqli and greensql and significantly faster than antisqli in classifying legitimate sql statements and sqli attacks.
.
threats to validity to help increase the external validity of our results which is the main challenge in our study we relied on various applications from different domains and written using different programming languages and three different attack generation tools.
further to ensure our accuracy results were realistic we resorted to standard 10fold validation involving multiple training testing data sets.
however we have to recognise the inherent limitations of such studies as we cannot predict accuracy on future vulnerabilities.
the fact that our learning approach does not rely on the specific vulnerabilities in our application systems helps alleviate this problem but does not eliminate it entirely.
.
related work we review sqli detection techniques that are based on analysing sql statements at run time.
table summarizes which of our security oracle requirements see section .
are met by related work.
our requirements are req 1training is independent from known attacks req 2classification is neither based on the knowledge of test input data nor on the input data generation algorithm andreq 3analysis does not require access to source code.
table security oracle requirements met by related work.
papers req 1req 2req halfond et al.
bisht et al.
buehrer et al.
kemalis et al.
x x pinzon et al.
x x liu et al.
valeur et al.
x x x white box approaches require the source code to be available for instrumentation or analysis so they do not meet requirement req .
halfond et al.
proposed amnesia a method based on program analysis to build models non deterministic 16we assume a confidence level so a p value .
indicates a statistically significant result.
175finite automata for each and every legitimate query of an application.
the application is instrumented and each sql query sent to the database is validated by finding an accepting path in the automaton.
if not possible the query is considered to be an attack.
bisht et al.
proposed candid an approach that compares a developer s intended query structure with the actual query structure found during program execution.
while both amnesia and candid show promising evaluation results and they meet requirement req 1andreq but they require access to the source code and its instrumentation which limits their applicability and violates requirement req .
buehrer et al.
have proposed sqlguard which compares parse trees of each sql statement before and after the inclusion of user inputs at runtime .
if the trees corresponding to a statement with and without user inputs are different after removing constants then the statement is considered to result from sqli attacks.
in comparison to our approach which does not require any change to the source code the application of sqlguard requires sql queries to be rewritten using a java library provided by sqlguard s authors thus req 3is not fully met.
several approaches based on anomaly detection have been proposed in the literature .
many of them look similar to ours because they contain the same two high level steps training and detection.
we provide below a detailed comparison with our work which aims at addressing three main limitations of practical importance false positives the difficulty to obtain a somewhat complete set of of actual and varied attacks for learning purposes and the need to handle new attack variants.
kemalis et al.
proposed sql ids a specification based approach to detect malicious sql statements .
even if this approach does analyse source code directly the user has to provide the specification of all benign sql statements for the application under protection.
sql ids monitors the application during runtime and each query that does not comply with the specification is treated as malicious.
req 3is not fully met by this approach because it requires precise knowledge of the source code to be manually provided to the tool.
in our proposed approach the user does not have to provide any specification for benign statements because the safe model is automatically inferred from the learning set.
the remaining approaches are black box i.e.
they do not require source code so they meet requirement req .
pinzon et al.
proposed an anomaly detection approach combining neural networks and support vector machine to classify sql queries into benign or malicious statements .
in contrast to our approach they employ supervised machine learning techniques that require a sufficient number of known attack statements.
as it is very much driven by what known attacks were fed to the learner such an approach does not met requirement req 1and it might have difficulties recognising new attacks.
to the best of our knowledge only two approaches fully meet all of our three requirements.
however we overcome their limitations by achieving i low sensitivity to learning set incompleteness and ii a very low false positive rate.
liu et al.
proposed sqlprob a tool that uses string alignment to detect the part of an sql query that corresponds to user input by detecting the difference between a new query requirements req 2andreq and all the queries observed at learning time requirement req .
the tool reports an anomaly when the part of the parse tree that corresponds to detected user input contains nonconstant leaf nodes e.g.
arithmetic or logic operators .
as one might expect the reliability of this approach is very sensitive to the completeness of learning that is whether all types of queries are accounted for.
completeness is required to identify correctly theuser input in the sql query.
identifying user inputs is a difficult and error prone step that could lead to false positives when training is partial.
unfortunately false positives were not reported by the authors.
after investigation it turned out that the tool was not available and therefore a comparative study was not possible.
nevertheless our approach does not entail extracting user inputs and is therefore by design more robust.
we do not need all types and variants of safe queries to be available at the training phase and thanks to the distance measure that we adopt as demonstrated by our empirical study we obtain accurate results even when we have no guarantee of completeness during training.
valeur et al.
used machine learning to learn relevant characteristics from user inputs of benign sql queries.
in the training phase several statistical models characterising relevant features such as character distribution and string length are learned from attack free sql queries requirement req in order to capture patterns and ranges of expected values.
in the detection phase a query is intercepted requirements req 2andreq and parsed and its input values are compared to detect anomalies against models resulting from training queries with identical parse tree structure.
this approach requires an exact match between the parse tree to classify and those in the learning set while we tolerate a degree of difference using tree distance.
moreover valeur et al.
learn a statistical distributions of values from legitimate sql statements while we remove these values and consider only pruned parse trees.
these two fundamental differences allow us to dramatically reduce false positives based on results reported by valeur et al.
in our approach legitimate statements are correctly classified as safe if they belong i.e.
show acceptable distance to one of the clusters of our model.
our approach is therefore more resilient as demonstrated by our empirical results.
.
conclusion having in mind realistic industrial settings we elicited three requirements for an ideal security oracle for security testing of sqli vulnerabilities.
we presented a novel approach that satisfies all of these requirements and we implemented it into a tool that we call sofia.
sofia learns a safe model characterising legitimate sql statements based on information logged during normal system executions.
to do so sql statements are parsed and then parse trees are pruned to remove information that is irrelevant to whether an sql statement is safe or not.
based on their tree edit distance similar parse trees are grouped using clustering and the resulting set of clusters is used as a safe models.
sofia classifies new sql statements by comparing and contrasting them with these clusters.
executions whose sql statements do not sufficiently fit into any of the clusters of the safe model are classified as attacks.
we assessed the accuracy of sofia as a security oracle with three different attack generation tools on six php and java systems.
no attack was missed and the rate of false positives was very low thus making sofia a reliable and cost effective approach.
further the classification of sql statements was on average below 100ms thus making it possible to execute a large number of test cases within time constraints.
last sofia significantly outperformed two widely used alternative tools in terms of classification accuracy and execution speed for one of the tools.
.