understanding flaky tests the developer s perspective moritz eck university of zurich zurich switzerland moritz.eck uzh.chfabio palomba university of zurich zurich switzerland palomba ifi.uzh.ch marco castelluccio mozilla software foundation london united kingdom mcastellucio mozilla.orgalberto bacchelli university of zurich zurich switzerland bacchelli ifi.uzh.ch abstract flaky tests are software tests that exhibit a seemingly random outcome pass or fail despite exercising unchanged code.
in this work we examine the perceptions of software developers about the nature relevance and challenges of flaky tests.
we asked professional developers to classify flaky tests they previously fixed in terms of the nature and the origin of the flakiness as well as of the fixing effort.
we also examined developers fixing strategies.
subsequently we conducted an online survey with developers with a median industrial programming experience of five years.
our research shows that the flakiness is due to several different causes four of which have never been reported before despite being the most costly to fix flakiness is perceived as significant by the vast majority of developers regardless of their team s size and project s domain and it can have effects on resource allocation scheduling and the perceived reliability of the test suite and the challenges developers report to face regard mostly the reproduction of the flaky behavior and the identification of the cause for the flakiness.
data and materials ccs concepts software and its engineering software testing and debugging .
keywords flaky tests empirical studies mixed method research.
acm reference format moritz eck fabio palomba marco castelluccio and alberto bacchelli.
.
understanding flaky tests the developer s perspective.
in proceedings of the 27th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august tallinn estonia.
acm new york ny usa pages.
esec fse august tallinn estonia copyright held by the owner author s .
publication rights licensed to acm.
this is the author s version of the work.
it is posted here for your personal use.
not for redistribution.
the definitive version of record was published in proceedings of the 27th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august tallinn estonia introduction software tests are flaky when they exhibit a seemingly random outcome despite exercising code that has not been changed.
even though previous work has proposed techniques to automatically fix some types of flaky tests our scientific knowledge about flaky tests is still very limited.
luo et al.
presented the earliest and most significant work advancing our empirical knowledge on flaky tests.
the researchers inspected commits that likely fixed flaky tests from apache projects.
they thus derived a taxonomy of the most common root causes their nature henceforth of flaky tests as well as identified strategies to manifest and fix certain types of flakiness .
the work we present in this paper continues on this line of research about understanding flaky tests.
here we present the developer s perspective our goal is to investigate developers perception on the causes and effort in fixing flaky tests the significance of the problem as well as what they deem to be the most important challenges.
an improved comprehension of these aspects is key to the definition of solutions and research lines to better help practitioners in diagnosing and fixing test flakiness.
to achieve our goal we ask professional mozilla developers to classify real world flaky tests they had previously fixed in terms of the nature of the flakiness the origin of the flakiness test or production code and the fixing efforts.
we complement this analysis with information about the fixing strategy which we collect from the source code repository.
focusing on a single ecosystem gives us the opportunity to detect the less frequent cases that would not likely appear by panning wide.
subsequently we conduct an online survey that was answered by developers professionals and academics .
since these developers are from different projects and backgrounds their answers give us the opportunity to learn from a variety of cases.
the categorization by the mozilla developers uncovered four previously unreported causes of flakiness which are also deemed as those requiring the most effort to fix.
most surveyed developers consider flaky tests a moderate to serious problem regardless of their team s size and project s domain and deal with flaky tests at least weekly.
in terms of problems flaky tests are reported to have serious consequences on the scheduling allocation and reliability of the testing process.
finally reproducing the flaky behavior and classifying its cause are perceived as the major challenges.
we publicly release the full dataset concerning our contributions as well as the data and materials for the futher analyses .arxiv .01466v1 jul 2019esec fse august tallinn estonia moritz eck fabio palomba marco castelluccio and alberto bacchelli goals and method s overview the goal of this study is to capture s the developer s perspective on flaky tests with the purpose of understanding the nature of test flakiness investigating why and where flaky tests arise as well as the fixing effort and strategies gathering the relevance of and problems caused by flaky tests in practice and collecting the challenges faced by developers when handling flaky tests.
.
research questions we start our research by asking professional developers to classify in terms of nature origin and fixing effort flaky tests they previously fixed.
differently from luo et al.
we do not classify the underlying causes of flakiness ourselves rather the original developers do and add information on the origin and fixing effort.
moreover we complement the developers analysis with information on the fixing strategies.
our goal is to triangulate the taxonomy proposed by luo et al.
from a different data source and with a different methodology.
hence our first research question rq .how do professional developers categorize flaky tests in terms of nature origin and fixing effort?
how do they fix them?
subsequently we turn to a broader audience of respondents.
we conduct a survey targeting software developers on their experience with flaky tests.
we investigate the frequency of flaky tests in practice and how problematic flaky tests are from the developer s perspective.
we ask rq .how prominent is test flakiness and how problematic is it as perceived by developers?
finally we investigate the challenges that developers perceive as most critical when they have to deal with flaky tests.
we ask rq .what are the main challenges that developers face when dealing with flaky tests?
.
overview of the research method our study features a mixed methods approach where quantitative and qualitative research are run in parallel with the goal of converging toward an empirical understanding of test code flakiness.
we design and conduct a study to obtain data from two main sources a novel annotated dataset ds of flaky tests with instances annotated by professional developers and the responses to an online survey os with valid responses from developers .
.
developers analysis of flaky tests in the following we describe the methodological steps conducted to address our first research question.
subject flaky tests.
we first need to collect a dataset of flaky tests which we then present to developers for their analysis.
to this aim we mine the bugzilla issue tracker of mozilla i.e.
a large free software organization counting more than active contributors spread around the world and developing a variety of software ranging from layout engines to operating systems .mozilla has a database of flaky tests verified and fixed by developers.
according to such a database the organization has to new flaky tests being detected every week this makes it particularly suitable for our study because developers are very frequently in charge of diagnosing and fixing flaky tests being therefore able to provide authoritative information on the nature of test flakiness its origin and the fixing effort.
we extract the data available after a flaky test has been reported.
thus we gather the log files produced by the continuous integration system and the source code associated with the fixed flaky test if available .
in a first step we identify all fixed flaky tests themozilla s bug tracking system tagged resolved andfixed between may and april .
we consider this time window since the company only stores the log files for twelve months after their creation.
from the set of retrieved flaky tests we remove all instances for which no patch has been applied or other type of attachment is available e.g.
some flaky tests stop occurring after a while and are closed as fixed as they were never fixed or were false alarms.
this step filters out tests resulting in a set of flaky tests which we use in the next steps.
recruiting mozilla developers.
the remaining actual flaky tests are taken into further consideration and sorted according to the developer who is listed as the assignee or patch creator.
from this list to ensure we select people with a concrete knowledge on flaky tests we exclude programmers who have only fixed less than five flaky tests.
we end up with a list of developers for a total of fixed flaky tests.
we contact these programmers through email and ask them to analyze the flaky tests as detailed in the following subsection.
developers analysis.
we create a spreadsheet for each developer that contains two columns the first reports the link to the bug report related to each flaky test fixed by a certain developer the second is used by the programmer to classify the nature of each flaky test in the list.
the participating developers are allowed to use the taxonomy defined by luo et al.
as a starting point for the classification process as it eases their task and standardizes their answers this is commonly done in both confirmatory and exploratory surveys .
however they are also allowed to create additional categories if none of the existing ones fit at the same time they can indicate more than one type to specify the reasons behind the flakiness of a test it may be flaky because of both a race condition and a network problem .
we also ask the developers to specify the development effort they spent to fix the flaky tests based on a likert scale between very low to very high as well as the origin test or production code of the flakiness.
once each developers completes the analysis they send back their spreadsheets with the annotated classification.
we received a total of responses from professional mozilla developers.
these developers have .
years median of experience with mozilla bug reports median assigned to and over the last year flaky tests fixed median .
each developer provided us with .
median answers min max .
of those responses we excluded i cases turned into permanent failures i.e.
they were labeled as intermittent by a mozilla sheriff but the fixing developer found them to be permanent failures ii in the other cases the type of flakiness was unknown even to the developers meaningunderstanding flaky tests the developer s perspective esec fse august tallinn estonia that the fixing developers worked around the flakiness rewriting the whole test and code under test but they could not ascertain the cause of the flakiness.
by removing these cases we intend to rely on precise information only.
nevertheless the excluded cases are available to our online appendix .
analysis of the developers responses.
we use the classified flaky tests to define a taxonomy on the nature of flaky tests.
in cases the assigned categories do not correspond to those available in the taxonomy by luo et al.
and therefore they are new and need to be defined.
to this aim we conduct an iterative content analysis to assign a common name to these new categories.
the process involves three software engineering researchers all authors of this paper one graduate student one research associate and one faculty member and consists of the two iterative sessions iteration the first author of this paper goes over the classifications made by developers.
if the categories assigned belong to the taxonomy by luo et al.
he leaves them as they are if not he assigns a temporary label to them.
as an output this step provides a draft categorization of the types of flakiness.
iteration the three researchers open a discussion on the draft taxonomy with the aim of reaching a consensus on the assigned categories.
afterwards the first author re categorizes flaky tests according to the decisions taken during the discussion.
finally the second author of the paper double checks the classifications to reduce the risk of errors he found no wrong classifications thus reaching a total agreement .
the resulting extended taxonomy is then used to address rq .
besides reporting the list of such categories we also characterize them with additional information on i their frequency computed on the basis of the distribution of the categories within the dataset and ii the fixing strategies.
as for the latter point to characterize the fixing strategies adopted by developers when dealing with flaky tests two authors of this paper conduct a new iterative content analysis that similarly to the previous one consists of two iterations.
in the first iteration the first author analyzes the patches associated to the resolution of the considered flaky tests to label them with a name and a short description of the fixing action performed by developers.
in the second iteration the two authors open a discussion on the initial labels assigned to reach a consensus.
then the first author applies the changes according to the discussion while the second re checks the final classifications to reduce threats in the interpretation of the results.
.
survey research method to answer rq 2andrq we design and deploy an online survey and analyze the collected answers.
overall survey design.
following the guidelines provided by flanigan et al.
we limit common issues possibly arising in survey studies and affecting the response rate we keep the survey short respecting the anonymity of participants and preventing our influence in the answers.
we create an anonymous online survey using a professional tool i.e.
surveygizmo .
the survey first describes to the respondents the concept of flaky tests following the definition accepted by the research community intermittent tests that sometimes pass sometimes fail even though there are no changes in the code they test .
then the survey asks for demographic information about the participants including programming testing experience as well as company and team size domain.
subsequently the survey contains other two main sections to collect data about the relevance of flakiness rq and its challenges rq .
rq collecting information about relevance.
we gather data on i how many times flaky tests occur ii how problematic they are and iii what are the top to problems caused by test flakiness.
participants can answer the first two questions by using a point likert scale indicating the extent to which the problem exists and how much it is serious while they are free to write down the problems if any in a text box.
rq collecting information about challenges.
we first conduct a multivocal literature review mlr .
this allows us to let emerge eight pieces of information that may be linked to challenges in the process of fixing flaky tests.
afterwards we ask the survey respondents to rate how important is each of those information pieces from not at all important to extremely important on a point likert scale and the difficulty in obtaining it from very easy to very difficult on a point likert scale .
we also let the respondents indicate any information needs and or challenges they face that are not included in the list respondents indicated a total of another eleven needs challenges .
for our mlr we analyze as described in the following protocol both previously published papers on the topic i.e.
white literature and online sources e.g.
blog posts websites and documents written by practitioners related to flaky tests i.e.
gray literature .
identifying relevant white literature we perform a keyword search ongoogle scholar ieee explore and acm digital library to create an initial selection of articles.
as keywords we use flaky tests flaky test resolution practices intermittent failures .
this step results in an initial set of papers.
we also perform forward and backward snowballing inspecting sources referred or that refer to those belonging to the initial set of primary studies.
at the same time we consider the proceedings of all the relevant top tier conference venues in software engineering such as icse issta icst icsme esec fse and journals such as ieee tse acm tosem and springer s emse .
this step results in the inclusion of additional four papers.
once having this set of resources we filter out the papers that do not report information on what are the practices used by developers to diagnose and or fixing flaky tests this is achieved by reading study setting methodology and conclusions made in all the considered papers.
the filtering process is jointly discussed among all the authors of the paper and in the end we agree on a list of three papers.
identifying relevant gray literature we conduct the gray literature survey following the guidelines by garousi et al.
.
we query google by using flaky tests flaky test resolution practices intermittent failures as keywords.
we perform our search on the results appearing in all the relevant google pages until saturation is reached i.e.
until the search results contain