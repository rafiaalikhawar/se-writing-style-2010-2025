actgan automatic configuration tuning for software systems with generative adversarial networks liang bao xin liu fangzheng wang and baoyin fang school of computer science and technology xidian university xi an shaanxi china department of computer science university of california davis davis california usa 1baoliang mail.xidian.edu.cn 2xinliu ucdavis.edu abstract complex software systems often provide a large number of parameters so that users can configure them for their specific application scenarios.
however configuration tuning requires a deep understanding of the software system far beyond the abilities of typical system users.
to address this issue many existing approaches focus on exploring and learning good performance estimation models.
the accuracy of such models often suffers when the number of available samples is small a thorny challenge under a given tuning time constraint.
by contrast we hypothesize that good configurations often share certain hidden structures.
therefore instead of trying to improve the performance estimation of a given configuration we focus on capturing the hidden structures of good configurations and utilizing such learned structure to generate potentially better configurations.
we propose actgan to achieve this goal.
we have implemented and evaluated actgan using workloads with eight different software systems.
experimental results show that actgan outperforms default configurations by .
on average and six state of the art configuration tuning algorithms by .
.
.
furthermore the actgan generated configurations are often better than those used in training and show certain features consisting with domain knowledge both of which supports our hypothesis.
index t erms software system automatic configuration tuning generative adversarial networks i. i ntroduction many software systems such as middleware databases and web servers provide a large number of parameters for users to configure in order to achieve desirable functional behaviors and non functional properties e.g.
performance and cost .
for example both spark and hbase have parameters that a user can configure among which are considered critical .
performance e.g.
execution time throughput requests per second is one of the most important non functional properties .
previous studies have shown that configurations have a significant impact on performance because software systems are complex with many configurable options that control nearly all aspects of their runtime behaviors.
with this complexity tuning a large number of configuration parameters requires a deep understanding of the target system and has far surpassed the abilities of typical system users .
as a result users often have to accept the default settings.
alternatively organizations may choose to hire human experts to configure their software systems.unfortunately manual configuration is labor intensive timeconsuming and often suboptimal.
therefore there is a dire need for automatic configuration tuning on software systems.
one intuitive approach of automatic configuration tuning is to measure the performance of all configurations of a system to identify the configuration with the best performance.
unfortunately this is usually infeasible because of the combinatorial nature of configuration parameters and the highdimensional configuration space.
specifically there are three main challenges in configuration tuning of complex software systems heterogeneity .
software systems that require configuration tuning are highly heterogeneous including message system like kafka data analytic systems like spark and hive database systems like redis mysql cassandra and hbase or web servers like tomcat .
performance metrics also differ including throughput execution time transactions per minute etc.
furthermore application workload can vary significantly.
for example figure illustrates the heterogeneity of performance surface under only two parameters.
we can see that redis and hive have bumpy performance surfaces while kafka has a relatively smooth performance surface.
complexity .
given different performance metrics and different workloads a deployed software system can have highly complex performance surfaces for a given set of configuration parameters as shown in figure .
it often takes an experienced system developer months of efforts to reason about the underlying interactions between parameters let alone novice users.
costing evaluation .
it is often costly to evaluate the performance of an individual configuration in a large software system.
because no performance simulator exists for general systems performance evaluation often requires real experiments on production systems.
such an evaluation is not only time consuming e.g.
executing a complex benchmark takes minutes or hours but also expensive e.g.
running a real system for one minute may cost hundreds of dollars .
hence obtaining performance samples i.e.
the performance results with different configurations is costly.
existing automatic configuration tuning methods include model based simulation based search based and learning4652019 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
.
.
19104requests per second 351redis hz301 threshold lower251 .
.
250105execution time ms .1hive mapjoin.bucket.cache.size6.
.
join.cache.size104 .
.
.
.
19throughput mb s 36kafka batch.size mb num.network.threads1160 figure .
performance surfaces of three different software systems based as discussed in section ii.
among them learning based tuning has received much recent attention.
the key idea of learning based tuning is to construct a performance prediction model using training samples of different configurations and then explores better configurations using some searching algorithms.
although previous studies on learning based tuning show promising results one critical drawback is that it requires a lot of samples to train an accurate prediction model.
for example previous study has shown that a typical learning based approach needs about samples to obtain a good prediction model with configuration parameters.
apparently the assumption of the abundance of samples used in many learning based methods is invalid considering the constrained time for tuning.
given the limited tuning time in practice only a small number of samples can be obtained.
it typically results in a less accurate prediction model that jeopardizes the exploration for better configurations.
to overcome these challenges we propose actgan automatic configuration tuning using generative adversarial network that aims to automatically generate good configurations for different software systems under a given time constraint.
we approach the configuration tuning for software system ctss problem in an unconventional manner our hypothesis is that good configurations share certain hidden structures .
therefore instead of trying to improve the performance estimation of a given configuration we focus on capturing the hidden structures of good configurations and using such hidden structures to generate potentially better configurations .
we choose gan generative adversarial network for this purpose because gan s inherent game like structure forces the neural networks to seek important features hidden in good configuration samples it circumvents the need to obtain a highly accurate prediction model that is typically required a significant number of samples and thus is more practical under the limited tuning time and gan allows us to generate better configurations leveraging the learned hidden structures.
in summary our work makes the following contributions we introduce and formulate the ctss problem and model it as a combinatorial optimization problem.
we propose actgan a novel approach to address ctss from an unconventional direction.
actgan can recommend promising configurations by directly learningand utilizing the hidden structures of existing good configurations without the need of learning a performance prediction model.
we evaluate the performance of actgan through extensive experiments using workloads under eight different software systems.
we show that actgan outperforms default configurations by .
on average and six state of the art tuning algorithms by .
.
.
ii.
r elated work automatic configuration tuning for software systems has received much attention from both industry and academia.
we classify the previous studies on this problem into four categories model based measurement based search based and learning based configuration tuning approaches where the latter two are most recent and relevant to our work.
model based configuration tuning .
the key to modelbased configuration tuning is to construct an analytical performance model for a software system on the early stage of system development see and for reviews.
model based approaches rely on domain specific knowledge mathematical theories or software architecture abstraction.
they can be labor intensive imprecise and difficult to evolve.
measurement based configuration tuning .
measurementbased configuration tuning aims to derive performance models for software systems by statistical inferencing based on benchmarked measurement data .
they collect program profiles to identify performance bottlenecks which often fail to capture the overall program performance .
also most of these approaches lack generality .
search based configuration tuning .
search based approaches regard configuration problem as a black box optimization problem and use search algorithms to solve it such as in .
the search strategies include recursive random search rrs heuristics evolutionary algorithms hill climbing algorithms and recursive bound search .
search based configuration is simpler and more general compared to other approaches because it takes the target software system as a black box function and does not need detailed information about the internals.
however they fail to exploit the knowledge of already known good configurations and need to search for the parameter space more times to obtain good configurations .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
system under tune sut workload w time constraint tc performance p configuration c given figure .
overview of ctss problem learning based configuration tuning .
the most relevant to our work is learning based configuration tuning.
these approaches try to construct performance prediction models first by observing a collection of running results under different configurations and then apply some search algorithms to find the optimal configuration based on these models.
the methods and models often used include regression svm svr cart fourier learning fl random forest artificial intelligence ai planning spectral learning classification and regression training caret bayesian optimization reinforcement learning comparison based model and transfer learning .
learning based approaches require considerable numbers of samples to construct a good performance prediction model for a software system .
the high demand of samples is challenging here because only a limited set of samples can be acquired under a given time constraint.
iii.
p roblem statement in this paper we study configuration tuning for software systems ctss .
as illustrated in figure for a given system and a specific workload the goal of ctss is to search for an optimal configuration to achieve the best performance by exploring the high dimensional parameter space within a limited time period.
we call this process of selecting configuration settings as configuration tuning or just tuning .
more specifically a ctss problem can be defined with the following components.
system under tune s .
many software systems such as data analytic framework databases and web servers provide a large number of parameters for users to configure.
the subject system that we want to tune is called the system under tune sut and is represented as s. configuration c .
we represent a configuration of an sut as a setc c1 c2 cn ofnparameters and the value of each parameter cimust be within cb the configuration bound predefined by the sut.
each configuration ciof an sut is represented as an n tuple assigning a valid value to each parameter in c. workload w .
a workload wrepresents a specific task running on an sut.
example workloads include an application on spark a set of transactions on mysql or a group of requests on tomcat.
in the ctss problem we assume the workload wis given.
furthermore we can measure theperformance of the sut under a given configuration cby simply executing the won sut under c and record its performance.
performance p .
performance is an essential nonfunctional property of an sut that can directly affect user perception and running cost.
it is the optimization goal of our ctss problem.
we treat the performance p as a blackbox function and use the p s w c to denote the performance value given an sut s a workload w and a configuration c. different suts have different performance goals for configuration tuning for a data access workload on mysql the performance goal is to increase the throughput while for a data analytical job on spark it would be to reduce the execution time .
without loss of generality we assume that we want to maximize the performance value over all configurations.
time constraint tc .
in a practice the time for configuration tuning is often restricted.
we define this restricted tuning time as time constraint denoted as tc.
any solution to the ctss problem must complete within tc.
in summary the ctss problem can be defined as follows max c cbp s w c s.t.tuning time tc where states that given an sut sand a workload w ons the goal of ctss is to find a configuration camong all valid configurations of sthat maximize the performance.
the constraint is that any solution to the problem must terminate after a tcamount of tuning time.
this definition shows that the goal of ctss is to search for an optimal configuration of a set of parameters to maximize the performance.
according to a previous study ctss is essentially an instance of classic combinatorial optimization co problems which is known to be np complete.
the np completeness proof by restriction is established in .
therefore a naive exhaustive search solutions would not be practical due to the high dimensionality of parameter space and the combinatorial nature of brutal force search.
iv .
actgan for ctss p roblem in this section we introduce actgan a generative adversarial network gan model to solve the ctss problem.
its key idea is to capture the hidden structures of good configurations by learning how already known good configurations are distributed in a high dimensional configuration space.
in the following we first analyze existing tuning approaches and their limitations.
we then present actgan a novel approach which solves the ctss problem in an unconventional manner.
finally we discuss the details of the actgan algorithm.
a. motivation for a given sut finding a good configuration is challenging because of the high dimensionality of the configuration space the limited amount of tuning time and the lack of a priori authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
knowledge about its performance surface.
to address these challenges the following approaches have been considered.
recently learning based approaches are the most popular ones for addressing the ctss problem.
the key step is to construct a performance prediction model for an sut using collected performance samples under different configurations.
however given the time constraint for tuning one can obtain only a limited number of samples because it often takes minutes or hours to evaluate the performance of an individual configuration on an sut.
having insufficient samples leads to an inaccurate prediction model and directly affects the tuning performance .
bayesian optimization bo based approaches are also popular ways to optimize objective functions that are costly to evaluate .
they build a surrogate for the objective quantify the uncertainty in that surrogate using gaussian process regression and then use an acquisition function defined from this surrogate to decide where to sample .
essentially bo uses all the information available from previous evaluations of the objectives instead of relying simply on local gradient and hessian approximations.
while bo methods often have good convergence guarantees their short term performance under a limited number of samples is not always desirable in practice because they may spend too much time exploring uncertain spaces.
search based approaches such as random search on the other hand take the sut as a black box function and do not need the detailed information about its internals.
however because they fail to exploit the knowledge of alreadyknown good configurations they have to explore the highdimensional configuration space more times in order to obtain the optimized configuration .
other heuristic algorithms such as genetic algorithm ga particle swarm optimization pso etc.
are able to handle high dimensional combinatorial optimization problems .
unfortunately they often need at least thousands of evaluations to find good configurations with dimensions according to our experiments which is infeasible in our time constrained ctss scenario.
most of the above approaches focus on improving the performance estimation modeling of a configuration under a limited number of samples.
by contrast our hypothesis is that good configurations often share certain hidden structures .
therefore instead of trying to improve the performance estimation of a given configuration we focus on capturing the hidden structures of good configurations .
in the literature gans have been shown to be a powerful tool for finding hidden structures which is why we adopt them here to address the ctss problem.
figure illustrates the automatic configuration tuning process of actgan.
actgan simultaneously trains two models a generative model gthat captures the hidden structures of the already known good configurations from the training examples and a discriminative model dthat estimates the probability that a sample comes from the training data rather than from g. more specifically we use the random sampling method to generate a set of configurations test them on sutconfiguration bound cb random samplingb training examplesexecution generated samplesgeneratorlatent random variablesdiscriminatorgood badlosstraining phase recommendation phase generatorlatent random variablesnr generated configurations sutthe best configurationconfiguration bound cb executionb training examplesns samples configuration selection figure .
overview of actgan approach the sut and select a subset of configurations i.e.
training examples with the best performance.
using this training set we train a gan with a discriminator and a generator network.
the gan structure forces the networks to learn the hidden structures of good configurations.
to utilize the hidden structures we use the trained generator to generate a set of potentially good configurations.
finally we test these configurations on the sut and select the best one.
in summary we propose an innovative way of utilizing the gan structure for ctss that uncovers and utilizes the hidden structures of good configurations .
b. actgan algorithm using the tuning process shown in figure we now discuss the actgan algorithm in algorithm .
estimating the sample size .
given a tuning time constraint tc an sut sconsisting of cparameters and a specific workload wrunning on s lett c be the execution time under the configuration c. the execution time of two different configurations may differ significantly.
for example the running time of a pagerank workload on spark using the default configuration is 440s this time falls to 242s under an optimized configuration.
suppose the tuning time constraint is tc and the number of recommended configurations from g isnr we need to run swith these nrconfigurations to find the best one see line then we can use the running time under default configuration to make a conservative estimate of the sample size ns line .
random sampling .g i v e ntc s c w and a set bc representing the configuration bound for every parameter actgan first generates a set of configurations tfrom cwithin bc.
this collection of configurations serves as a training set for our gan model.
we use the random sampling rs strategy as it can effectively search for a larger configuration space especially when configuration parameters are not equally important line .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm actgan s w c tc cb input s the target sut w workload c configuration tc time constraint cb configuration bounds.
variables t the set of initial samples t ns r the set of potentially good configurations recommended by g r nr.
t c0 the time of running susing the default configuration c0 ns tc t c0 nr t rs ns s w c cb b a set ofbconfigurations with the best performance from t fornumber of training iterations do foranyksteps do sample minibatch of mnoise samples z z z m from noise prior pg z sample minibatch of mconfigurations x x x m fromb update the discriminator by ascending its stochastic gradient d1 mm summationdisplay i end for sample minibatch of mnoise samples z z z m from noise prior pg z update the generator by descending its stochastic gradient g1 mm summationdisplay i end for r nrconfigurations recommended by g run workload wonswith each configuration in r select the configuration c with the best performance in r uniontextb returnc gan architecture design .
after obtaining t a subset bof tconsisting of the best bconfigurations are selected to serve as the training set for the discriminator dof the actgan line .
specifically we model both the generator gand the discriminator dusing three layer feedforward neural networks nns with one hidden layer.
simple neural networks work here because the performance surfaces of suts are often continuous although not necessarily smooth so nns can capture the hidden structures of the performance surfaces.
we note that the original convolution neural network cnn structured gan works well for image processing because such convolution operators generally capture intensity gradient and edge characteristics of images.
however in our case cnns fail to capture the hidden structures among the parameters for a given sut.
the detailed architecture of actgan is discussed in the section v b. model training .
at any step of the training process the generator gtakes as input a gaussian noisy source z pg z and produces a set of synthetic configurations z z z m that aims to follow a unknown target data distribution for good configurations line .
meanwhile the discriminator dtakes a subset of alreadyknown good configurations x x x m as input and learns to maximize the probabilities that zis fake andxis real.
the min max adversarial loss for training the generator networkgand discriminator network dis formulated as min gmax dex p x ez pg z where we train dto maximize the probability of identifying the good configurations to both training examples and samples from g and simultaneously train gto minimize log d g z .
the goal of this process is to learn the hidden distribution of already known good configurations p x viag and generate realistic fake good configurations line .
when we train ggivend we minimize the following loss ofg line ez pg z min g in our actgan algorithm we adopt adam an optimized mini batch stochastic gradient descent sgd method to update the parameters in the networks in an efficient and stable way.
configuration recommendation .
after finishing the training we use the generator gto construct a set of nrrecommended configurations rand run them on the sut.
finally we select the best configuration in r uniontextb wherebis the best configurations generated using random sampling method.
line .
v .
actgan i mplementation a. data preparation categorical data processing .
categorical variables are parameters that contain label values rather than numeric values and the number of possible values is often limited to a fixed set.
categorical variables are common in software configurations.
for example there are two categorical variables i.e.
spark.io.compression.codec snappy lz4 lzf and spark.serializer javaserializer kryoserializer of spark framework in our experiments.
because our neural networks cannot operate on categorical data directly they require all input variables and output variables to be numeric.
there are two encoding methods i.e integer encoding and one hot encoding that can convert categorical data to numerical data.
in ctss problem categorical variables often have no ordinal relationship using integer encoding and allowing the model to assume a natural ordering between categories may result in poor performance or unexpected results.
in this case we use one hot encoding to add binary variable for each unique categorical value.
in the spark.io.compression.codec variable example there are three categories and therefore three binary variables are needed.
a value is placed in the binary variable for the used compression options and values for the other options as shown in table i. data normalization .
software configuration parameters are often in highly different ranges.
take the web server tomcat for example the of minimal processors parameter ranges from in our experiments while the connection timeout authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i one hot encoding for spark.io.compression.codec snappy lz4 lzf input layer nodeshidden layer nodes sigmoidoutput layer nodes input data ig m random matrixoutput data og m matrixgenerator m ogotkgki kgkikgki kgkh kgkoigit kgkom kgko figure .
the generator architecture of actgan ranges from and higher.
further analysis shows that the parameter connection timeout intrinsically influences the result more due to its larger value.
but this doesn t necessarily mean it is more important as a predictor.
to address problem we use standard score to normalize the dataset.
more specifically let xbe the original value of a parameter zbe the normalized value we have z x where is the mean value of the parameter and is the standard deviation of the parameter values.
b. gan architecture like any typical gan architecture actgan consists of two main components a generator gand a discriminator d. the goal of the generator is to generate synthetic configurations that are plausible in the input good configurations.
at the same time the discriminator learns to distinguish the suboptimal configurations from the good ones that come from the training set.
both ganddare trained end to end using backpropagation .
at each step of the training process actgan uses gto generate a set of synthetic configurations takes one configuration at a time to feed as d s input.
doutputs a single score that represents the probability of the configuration being real.
an overview of actgan s detailed architecture of the generator and the discriminator can be seen in figure and respectively.
generator .
the generator gdefines an implicit probabilistic model for generating good configurations c c1 c2 cn g. we model gas a three layer feedforward neural network.
more specifically gconsists of an input layer nodeshidden layer nodes reluoutput layer node sigmoid input data id m configurations m matrix output data od m vectordiscriminator od iditm kd ki kd kh kd ko m kg ko kg ko figure .
the discriminator architecture of actgan input layer of non computational units one for each input one hidden layer of computational units and an output layer of computational units.
the key procedure of gis to pass the input signal forward and the error signal backward through the network.
in the forward pass a latent m ki gmatrixig represents m noise samples drawn from a multivariate gaussian distribution is presented to the input layer and the input signal is filtered forward through the subsequent hidden layer with kh gnodes of the network to generate a set of outputs.
the network outputs i.e.
a set of msynthetic configurations a m ko g matrixog are fed into the discriminator dand the possibility value for each configuration can lead to good performance is generated an error signal.
in the backward pass the synaptic weights are updated using the learning constant according to the effect they have in generating the incorrect outputs.
figure demonstrates the passing of noise prior igthrough a neural network and the structure of our three layer network.
the presentation of noise samples to the network is repeated in each minibatch until the network is stabilized and no more adjustments are required for the synaptic weights or the maximum number of epochs has been reached.
as described in table iii the number of input neurons for gis set to ki g the number of hidden neurons for gis set to kh g and the number of output neurons for g is set ton ko g n i.e.
the number of parameters for a given sut.
discriminator .
the discriminator dis also a three layer feedforward neural network with a hidden layer consisting of kh dnodes.
at every time step a m ko gmatrixid denoting mconfigurations generated by g i.e.ki d ko g n is fed as input.
after processing these configurations the discriminator outputs a vector of mscalar values that represents the probability of each synthetic configuration being good.
as shown in table iii the number of initial training samples fed intodis set to b the number of input neurons authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fordis set ton ki d n i.e.
the number of parameters for a given sut the number of hidden neurons for dis set to kh d and the number of output neurons for dis set t o1 ko d .
c. model training to stabilize our model training procedure we modify the negative log likelihood objective by clipping the values between and .
this loss performs more stably during training and generates higher quality results.
we use the adam optimizer with a momentum value of .
.
all networks were trained from scratch with a learning rate of .
and the total number of epochs are set to for all workloads.
finally we apply instance normalization with batch size of m .
we tried the conventional value of batch size at the first place but observed the model oscillation of g anddduring the training process and finally found out that the is an appropriate value for the ctss problem after many attempts.
vi.
e xperiments we have implemented the actgan and other algorithms and conducted extensive experiments under diverse workloads.
the source code and the data can be found in the online anonymous repository in this section we first describe our experiment setup and then present the experimental results to prove the efficiency and effectiveness of the proposed approach.
a. experimental settings running environment .
we conduct our experiments on a local cluster of physical servers.
each server is equipped with two core intel intel xeone5 2650v2 .6ghz processors 256gib ram .5tb disk and running centos .
and java .
.
.
all of servers are connected via a high speed .5gbps lan.
to avoid interference and comply with the actual deployment we run the suts the workload generators and the actgan algorithm on different physical servers at each experiment.
suts benchmarks parameters and performance metrics.
we choose eight widely used software systems to evaluate actgan namely kafka spark hive redis mysql cassandra hbase and tomcat kafka is a distributed message system dms for publishing and subscribing to streams of records spark is a cluster computing engine for big data analytics hive is a data warehouse software that manages large datasets residing in distributed storage using sql redis is an opensource in memory data structure store mysql is an opensource relational database management system rdbms cassandra is an open source column oriented nosql database management system hbase is a distributed and scalable big data store and tomcat is an open source implementation of the java web technologies.
we use six customized workloads for kafka hibench for spark ycsb for hive cassandra and hbase redis bench1for redis tpcc2for mysql and jmeter for tomcat.
for each sut we use domain expertise to identify parameters that are considered critical to the performance as in .
note that even with only these parameters the search space is still enormous and exhaustive search is infeasible.
the reason we choose a small subset of parameters that have great impact on performance instead of all configurable parameters is because reducing the number of tuning parameters can reduce the search space exponentially and most existing approaches also adopt this manual feature selection strategy.
table ii summarizes the suts along with the benchmarks the numbers of tuned parameters and performance metrics respectively.
baseline algorithms and hyperparameters .
to evaluate the performance of actgan we compare it with six stateof the art algorithms namely random search bestconfig rfhoc smac hyperopt and autoconfig .
we provide a brief description for each algorithm as follows and report its hyperparameters including actgan in table iii.
random search random is a search based tuning approach that explores the configuration space uniformly at random .
bestconfig3is a search based tuning approach that uses divide and diverge sampling and recursive bound and search algorithm to find the best configuration.
rfhoc is a learning based tuning approach that constructs a prediction model using random forests and a genetic algorithm to automatically explore the configuration space.
smac4is a learning based tuning method using random forests and an aggressive racing strategy.
hyperopt5is a learning based tuning approach based on bayesian optimization.
it is widely used for hyperparameter optimization.
autoconfig6is a learning based tuning approach using a comparison based prediction model and a weighted latin hypercube sampling method.
for each run in our experiments every algorithm is executed under the same time constraint and stops once the constraint is met.
to ensure consistency we run each workload five times and calculate the average of these five runs.
b. experiment results performance results .
given a fixed time constraint about running times using default configuration for each sut and workload we run seven different tuning algorithms plus default configuration independently.
table iv lists the experiment results.
as expected the default configuration does not perform well.
actgan achieves an average of .
improvement over the default configurations.
furthermore actgan outperforms all other six algorithms authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii sut s benchmarks parameters and performance metrics suts categories benchmarks of tuned parameters of all parametersperformance tuning goal kafka dms customized throughput tp mb s max spark data analytics hibench execution time et ms min hive data analytics ycsb execution time et ms min redis in memory db redis bench requests per second rps max mysql rdbms tpcc transactions per minute tpm max cassandra nosql db ycsb operations per second ops max hbase nosql db ycsb execution time et ms min tomcat web server jmeter requests per second rps max table iii hyperparameters for each algorithm algorithm hyperparameters random of configurations bestconfiginitialsamplesetsize rrsmaxrounds comt2iteration comt2multiiteration timeoutinseconds rfhocpopulation size epoch crossover parameters .
mutation probability .
smacrunobj quality runcount limit deterministic true hyperoptsearch algorithms hyperopt.rand.suggest hyperopt.anneal.suggest max evals autoconfig of iterations .
.
h b actgan1ns nr epoch learning rate .
b m ki g kh g ko g n ki d n kh d ko d .
.
improvement over random .
.
improvement over bestconfig .
.
improvement over rfhoc .
.
improvement over smac .
.
improvement over hyperopt and .
.
over autoconfig.
finally we plot the overall performance improvement percentage of bestconfig rfhoc smac hyperopt autoconfig and actgan on kafka spark and other six software systems in figure and respectively using the random algorithm as the baseline.
in each figure x axis lists the workloads and y axis represents the performance improvement over the random algorithm.
we observe that compared with the random algorithm our approach achieves .
.
improvement among all workloads.
actgan achieves an average of .
improvement over random .
improvement over bestconfig .
improvement over rfhoc .
improvement over smac .
improvement over hyperopt and .
improvement over autoconfig.
we can conclude from figure and that actgan achieves stable and significant improvements compared with the other six algorithms.
another interesting observation is that the random search achieves surprisingly good results in our experiments.
this is consistent with the findings of bergstra and bengio in .
in the above experiments the number of the random sam l100p1r1 l100p1r l100p3r1 l1000p1r1 l100p1r l1000p1r1 kafka workloads 100102030imp random bestconfig rfhoc smac hyperopt autoconfig actgan figure .
performance comparison on six kafka workloads with different algorithms bayes kmeans pagerank sort wordcount spark workloads 201001020imp random bestconfig rfhoc smac hyperopt autoconfig actgan figure .
performance comparison on five spark workloads with different algorithms authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iv performance results from different algorithms under fixed time constraints sut perf.
workload tc h default random bestconfig rfhoc smac hyperopt autoconfig actgan kafka tp mb s l100p1r1 .
.
.
.
.
.
.
.
l100p1r .
.
.
.
.
.
.
.
.
l100p3r1 .
.
.
.
.
.
.
.
l1000p1r1 .
.
.
.
.
.
.
.
l1000p1r .
.
.
.
.
.
.
.
.
l10000p1r1 .
.
.
.
.
.
.
.
spark et ms bayes kmeans .
pagerank .
sort wordcount .
hive et ms ycsb .
redis rps redis bench .
.
.
mysql tpm tpcc .
cassandra ops ycsb .
.
.
.
.
.
.
.
.
hbase et ms ycsb .
tomcat rps jmeter .
.
.
.
.
.
.
.
hive redis mysql cassandra hbase tomcat workloads of six software systems 100102030405060imp random bestconfig rfhoc smac hyperopt autoconfig actgan figure .
performance comparison on six suts with different algorithms plesnsand the number of recommended configurations nr are set to and respectively.
we believe that the optimal values of these two hyperparameters are ctss specific and can be trained.
in our experiment we have evaluated different values of nsandnrand observed that the results are not sensitive to these hyperparameters.
for the ease of reporting we choose to report the same set of values.
the quality of configurations .
to evaluate the quality of configurations recommended by actgan we plot the normalized performance the higher the better i.e.
means the worst performance and means the best histogram over the initial training samples blue bars and the configurations generated by actgan red bars for eight different software systems in figure .
we can see from figure that the generated configurations can achieve the best performance in all cases.
the generated configurations in general outperform the training samples a clear demonstration of the effectiveness of the actgan approach.
another interesting observation is that these actgan generated configurations incline to choose a fixed value on certain categorical parameters for a specific workload.
for example the spark parameter spark.serializer javaserializer kryoserializer is all set to javaserializer for bayes sort and wordcount workloads and for kmeans and pagerank workloads it is all set to kryoserializer .
such choices are in accord with domain knowledge and field experience obtained from long term software system operation and maintenance.
c. threats to v alidity internal validity to increase the internal validity we performed controlled experiments by executing each test case five times and calculate the average of these five runs.
such method can avoid misleading effects of specifically selected test cases and ensures the stability of the result.
besides we set the same time constraint value suggested by users for all algorithms including actgan in each test case and each algorithm is forced to stop when the time constraint is met.
such results are considered to be fair and reliable.
in addition we set the values of hyperparameters for each compared algorithm as suggested by their authors see table iii .
finally we have tried multiple values of hyperparameters for actgan in our experiments and observed that the good values of these hyperparameters that can lead to better results are almost the same from test case to test case.
external validity we increase the external validity by choosing eight different suts including one message system kafka two big data processing systems hive and spark four database systems redis mysql cassandra and hbase and one web application server tomcat .
furthermore we choose different workloads on these suts and among them six testing scenarios with various applicationlevel parameters are for kafka and five hibench workloads are for spark.
finally we are aware that because our actgan is a general black box approach and is independent to the suts the results of our evaluations are transferable to other software systems.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
kafka .
.
.
.
.
.
.
.
.
.
normalized performance020406080 of configurationstraining set generated spark .
.
.
.
.
.
.
.
.
.
normalized performance0204060 of configurationshive .
.
.
.
.
.
.
.
.
.
normalized performance051015 of configurationsredis .
.
.
.
.
.
.
.
.
.
normalized performance0102030 of configurations mysql .
.
.
.
.
.
.
.
.
.
normalized performance010203040 of configurationscassandra .
.
.
.
.
.
.
.
.
.
normalized performance05101520 of configurationshbase .
.
.
.
.
.
.
.
.
.
normalized performance0102030 of configurationstomcat .
.
.
.
.
.
.
.
.
.
normalized performance0102030 of configurations figure .
normalized performance distributions between the initial training set and the actgan generated configurations for eight different software systems d. discussion our work builds upon the hypothesis that good configurations share hidden structures.
although we could not prove this hypothesis mathematically our results clearly support this hypothesis.
first our extensive experimental evaluations show that the actgan generates configurations are better than the ones used for training which suggests that actgan successfully uncovers hidden structures of good configurations.
furthermore the above observation of actgan choosing a fixed value on certain categorical parameters for a specific workload also supports our hypothesis because it is consistent with domain knowledge.
the more interesting question is why actgan could improve upon training samples.
an analogy is as follows suppose we feed real pictures of good looking people into a gan and train it to generate artificial good looking faces.
psychological studies suggest that a face being symmetry is important to good looking a hidden structure that gan needs to learn.
after learning this structure gan can generate perfectly symmetric faces.
given no real human faces are perfectly symmetric the generated pictures are better in terms of the hidden structure of symmetry.
in the ctss scenario our intuition is that actgan can identify such hidden structures in good configurations and then generate configurations that inherit such hidden structures while avoid some of the imperfectness in existing training samples which are generated and selected from random sampling .
therefore in the end actgan improves upon the training samples.
we note that given the limited tuning time noalgorithm can explore the entire the high dimensional parameter space.
random sampling is a simple yet robust mechanism to find good configurations without prior knowledge and thus chosen here.
other effective learning schemes can be easily adopted to replace random sampling to generate the initial set of good training samples.
therefore our actgan can complementand augment existing learning algorithms.
we also note that because of the high dimensional configuration space and limited tuning time it is possible that obtained training samples do not contain all hidden structures of an optimal configuration and thus actgan could not learn all them.
this is an inherent limitation that all algorithms are likely to suffer in a high dimensional non convex general optimization problem.
vii.
c onclusion and future work in this paper we propose actgan a novel approach to address ctss.
actgan can recommend promising configurations by directly learning and utilizing the hidden structures of existing good configurations.
the performance superiority of actgan has been illustrated by extensive real system evaluations in comparison to six state of the art algorithms.
actgan is a first attempt to leverage gan structure to solve ctss and future work is multifold.
first it is well known that gan although a powerful tool to identify hidden structures has its limitations such as sensitivity to mode dropping and mode collapsing.
it is ongoing research to identify and address these limitations.
our future work is to study how these limitations affect actgan and its performance on ctss.
furthermore most existing work on addressing the limitations of gan focuses on image processing.
how can we adapt these studies to ctss e.g.
how do we quantify and identify mode dropping in ctss?
ctss has unique features such that existing solutions may or may not work effective and thus require further investigation.
the holy grail is to leverage actgan to identify and interpret the hidden structures of good configurations.
our observation of fixed values on certain categorical parameters seems to be promising.
however this is a challenging task that requires much domain knowledge and significant future work.
it is also our future work to refine our actgan approach to adaptive adjust the hyperparameters nrandns based on authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
time constraint sut and workload.
we will also investigate the performance dynamics of more software systems and improve our gan structure to obtain better configurations.