leopard identifying vulnerable code for vulnerabilityassessmenthroughprogrammetrics xiaoning du bihuan chen y uekang li jianmin guo yaqin zhou yang liu y u jiang school of computer science and engineering nanyang technological university singapore school of computer science and shanghai key laboratory of data science fudan university china kliss bnrist school of software tsinghua university china college of information science zhejiang sci tech university china co first authors abstract identifying potentially vulnerable locations in a code base is critical as a pre step for effective vulnerability assessment i.e.
it can greatly help security experts put their time and effort to where it is needed most.
metric based and pattern based methods have been presented for identifying vulnerable code.
the former relies on machine learning and cannot work well due to the severe imbalance between non vulnerable and vulnerable code or lack of features to characterize vulnerabilities.
the latter needs the prior knowledge of known vulnerabilities and can only identify similar but not new types of vulnerabilities.
in this paper we propose and implement a generic lightweight and extensible framework l eopard to identify potentially vulnerable functions through program metrics.
l eopard requires no prior knowledge about known vulnerabilities.
it has two steps by combining two sets of systematically derived metrics.
first it uses complexity metrics to group the functions in a target application into a set of bins.
then it uses vulnerability metrics to rank the functions in each bin and identifies the top ones as potentially vulnerable.
our experimental results on real world projects have demonstrated that l eopard can cover .
of vulnerable functions by identifying of functions as vulnerable and outperform machine learning based and static analysis based techniques.
we further propose three applications of l eopard for manual code review and fuzzing through which we discovered new bugs in real applications like php radare2 andffmpeg and eight of them are new vulnerabilities.
index terms program metric vulnerability fuzzing i. i ntroduction vulnerabilities are one of the key threats to software security .
security experts usually leverage guided fuzzing e.g.
symbolic execution e.g.
or manual auditing to hunt vulnerabilities.
as only a few vulnerabilities are scattered across a large code base vulnerability hunting is a very challenging task that requires intensive knowledge and is comparable to finding a needle in a haystack .
therefore a large amount of time and effort is wasted in analyzing the non vulnerable code.
in that sense identifying potentially vulnerable code in a code base can guide vulnerability hunting and assessment in a promising direction.
there are two types of existing techniques to automatically identify vulnerabilities metric based and pattern based techniques.
metric based techniques inspired by bug prediction leverage supervised or unsupervised machine learning to predict vulnerable code mostly at the granularity level of a source file.
following security experts beliefthat complexity is the enemy of software security they use complexity metrics as features or combine them with code churn metrics token frequency metrics dependency metrics developer activity metrics and execution complexity metrics .
on the other hand pattern based techniques leverage patterns of known vulnerabilities to identify potentially vulnerable code through static analysis.
the patterns are formulated based on the syntax or semantics abstraction of a certain type of vulnerabilities e.g.
missing security checks on security critical objects security properties code structures and vulnerability specifications .
while vulnerability identification has been attracting great attention some problems still remain.
on one hand metricbased techniques are mostly designed for one single application or a few applications of the same type .
thus they might not work on a variety of diverse applications as machine learning may over fit to noise features.
moreover while an empirical connection between vulnerabilities and bugs exist the connection is considerably weak due to the differences between vulnerabilities and bugs .
as a result the research on bug prediction cannot directly translate to vulnerability identification.
unfortunately the existing metric based techniques use the similar metrics as those in bug prediction and thus fail to investigate the characteristics of vulnerabilities.
on the other hand metric based and pattern based techniques mostly require a great deal of prior knowledge about vulnerabilities.
in particular a large number of known vulnerabilities are needed for effective supervised machine learning in some metric based techniques.
the number of vulnerabilities is much smaller than the number of bugs and the imbalance between non vulnerable and vulnerable code is severe which hinders the applicability of supervised machine learning to vulnerable code identification.
similarly a prerequisite of those patternbased techniques is the existence of known vulnerabilities as the guideline to formulate patterns.
they can only identify similar but not new vulnerabilities.
further patterns are often application specific and thus those techniques are usually used as in project but not cross project vulnerable code identification.
in this paper we propose a vulnerability identification frame t ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
work named l eopard1 to identify potentially vulnerable functions in c c applications.
l eopard is designed to be generic to work for different types of applications lightweight to support the analysis of large scale applications and extensible with domain specific data to improve the accuracy.
we design leopard as a pre step for vulnerability assessment but not to directly pinpoint vulnerabilities.
we propose three different applications of l eopard to guide security experts during the manual auditing or automatic fuzzing by narrowing down the space of potentially vulnerable functions.
leopard does not require any prior knowledge about known vulnerabilities.
it works in two steps by combining two sets of systematically derived program metrics i.e.
complexity metrics and vulnerability metrics.
complexity metrics capture the complexity of a function in two complementary dimensions the cyclomatic complexity of the function and the loop structures in the function.
vulnerability metrics reflect the vulnerable characteristics of functions in three dimensions the dependency of the function pointer usage in the function and the dependency among control structures within the function.
leopard first uses complexity metrics to group the functions in a target application into a set of bins.
then l eopard leverages vulnerability metrics to rank the functions in each bin and identify the top functions in each bin as potentially vulnerable.
we propose such a binning and ranking approach as there often exists a proportional relation between complexity and vulnerability metrics which is evidenced in our experimental study.
as a result each bin has a different level of complexity and our framework can identify vulnerabilities at all levels of complexity without missing low complexity ones.
we implemented the proposed framework to obtain complexity and vulnerability metrics for c c programs.
we evaluated the effectiveness and scalability of our framework with different types of real world projects.
l eopard can cover .
of vulnerable functions by identifying of functions as potentially vulnerable outperforming both typical machine learning based and static analysis based techniques.
applying leopard onphp mjs xed ffmpeg andradare2 and with further manual auditing or automatic fuzzing we discovered new bugs among which eight are new vulnerabilities.
in summary our work makes the following contributions.
we propose a generic lightweight and extensible framework to identify potentially vulnerable functions which incorporates two sets of program metrics.
we propose three different applications of l eopard to guide security experts during the manual auditing or automatic fuzzing to hunt for vulnerabilities.
we implemented our framework and conducted large scale experiments on real world projects to demonstrate the effectiveness and scalability of our framework.
we demonstrated three application scenarios of our framework and found new bugs.
1leopard is known for its opportunistic hunting behavior broad diet and strength which reflect the identification capabilities we are pursuing.function binningfunction rankingpotentially vulnerable functionscomplexity metricsvulnerability metrics applicationvulnerability assessmentdomain knowledge fig.
an overview of the proposed framework ii.
m ethodology in this section we present the overview of l eopard and elaborate each step of the proposed approach.
a. overview fig.
1presents the work flow of l eopard which is designed to be generic lightweight and extensible.
the input is the source code of a c c application.
l eopard works in two steps function binning and function ranking and returns a list of potentially vulnerable functions for vulnerability assessment.
in the first step ii b we use complexity metrics to group all functions in the target application into a set of bins.
the complexity metrics capture the complexity of a function in two dimensions the function itself i.e.
cyclomatic complexity and the loop structures in the function e.g.
the number of nested loops .
each bin has a different level of complexity which is designed to identify vulnerabilities at all levels of complexity i.e.
avoid missing vulnerable functions with low complexity .
in the second step ii c we use vulnerability metrics to rank the functions in each bin in order to identify the top functions in each bin as potentially vulnerable.
the vulnerability metrics capture the vulnerable characteristics of a function in three dimensions the dependency of the function e.g.
the number of parameters the pointer usage in a function e.g.
the number of pointer arithmetic and the dependency of control structures in the function e.g.
the number of nested control structures .
by incorporating such metrics we can have a high potential of characterizing and identifying vulnerable functions.
leopard is designed to support and facilitate confirmative vulnerability assessments e.g.
to guide security experts during automatic fuzzing or manual auditing by providing potentially vulnerable function list and the corresponding metrics information.
with such knowledge security experts can prioritize the assessment order choose the appropriate analysis technique and analyze the root cause.
further based on application specific domain knowledge e.g.
vulnerability history and heavily fuzzed function lists security experts can further rank or filter the potentially vulnerable functions to focus on those more interesting functions.
using program metrics in a simple binning and ranking way makes l eopard satisfy our design principle of being generic and lightweight.
it is applicable to any large scale applications of any type and does not require prior knowledge about known vulnerabilities.
the two sets of metrics are comprehensive but also are extensible with new metrics as we gather more usage feedback from security experts see discussion in v .
thus leopard also satisfies our design principle of being extensible such that it can be further enhanced.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i complexity metrics of a function dimension id metric description cd1 function c1 cyclomatic complexity cd2 loop structuresc2 of loops c3 of nested loops c4 maximum nesting level of loops b. function binning different vulnerabilities often have different levels of complexity.
to identify vulnerabilities at all levels of complexity in the first step we categorize all functions in the target application into a set of bins based on complexity metrics.
as a result each bin represents a different level of complexity.
afterwards the second step ii c plays the prediction role via ranking.
such a binning and ranking approach is designed to avoid missing low complexity vulnerable functions.
complexity metrics.
by complexity we refer to the approximate number of paths in a function and derive the complexity metrics of a function from its structural complexity.
a function often has loop and control structures which are the main sources of structural complexity.
cyclomatic complexity is a widely used metric to measure the complexity but without reflection of the loop structures.
based on such understanding we introduce the complexity of a function with respect to these two complementary dimensions as shown in table i. function metric c1 captures the standard cyclomatic complexity of a function i.e.
the number of linearly independent paths through a function.
a higher value of c1 means that the function is likely more difficult to analyze or test.
loop structure metrics c2 c4 reflect the complexity resulting from loops which can drastically increase the number of paths in the function.
metrics include the number of loops the number of nested loops and the maximum nesting level of loops.
loops are challenging in program analysis and hinder vulnerability analysis.
basically the higher these metrics the more and possibly longer paths need to be considered and the more difficult to analyze the function.
binning strategy.
given the values of these complexity metrics for functions in the target application we compute a complexity score for each function by adding up all the complexity metric values and then group the functions with the same score into the same bin.
here we do not use a range based binning strategy i.e.
grouping the functions whose scores fall into the same range into the same bin as it is hard to determine the suitable granularity of the range.
such a simple strategy not only makes our framework lightweight but also works well as evidenced by our experimental study in iv c .
c. function ranking different from the structural complexity metrics in the second step we derive a new set of vulnerability metrics according to the characteristics of general causes of vulnerabilities and then rank the functions and identify the top ones in each bin as potentially vulnerable based on the vulnerability metrics.
existing metric based techniques rarely employ any vulnerability oriented metrics and make no differentiation between complexity metrics and vulnerability metrics.
here table ii vulnerability metrics of a function dimension id metric description vd1 dependencyv1 of parameter variables v2 of variables as parameters for callee function vd2 pointersv3 of pointer arithmetic v4 of variables involved in pointer arithmetic v5 max pointer arithmetic a variable is involved in vd3 control structuresv6 of nested control structures v7 maximum nesting level of control structures v8 maximum of control dependent control structures v9 maximum of data dependent control structures v10 of if structures without else v11 of variables involved in control predicates we propose and incorporate vulnerability metrics to have a high potential of characterizing and identifying vulnerable functions.
vulnerability metrics.
most critical types of vulnerabilities in c c programs are directly or indirectly caused by memory management errors and or missing checks on some sensitive variables e.g.
pointers .
resulting vulnerabilities include but are not limited to memory errors access control errors e.g.
missing checks on user permission and information leakage.
actually the root causes of many denial of service and code execution vulnerabilities can also be traced back to these causes.
the above mentioned types account for more than of all vulnerabilities .
hence it is possible to define a set of vulnerability metrics that are compatible with major vulnerability types.
here we would not favor any specific types of vulnerabilities e.g.
to include metrics like division operation which is closely related to divide by zero while the exploration of type specific metrics is worth of investigation in the future.
with either high or low complexity scores vulnerable functions we focus on are mainly with complicated and compact computations which are independent from the number of paths in the function.
based on these observations we introduce the vulnerability metrics of a function w.r.t.
three dimensions as summarized in table ii.
dependency metrics v1 v2 characterize the dependency relationship of a function with other functions i.e.
the number of parameter variables of the function and the number of variables prepared by the function as parameters of function calls.
the more dependent with other functions the more difficult to track the interaction.
pointer metrics v3 v5 capture the manipulation of pointers i.e.
the number of pointer arithmetic the number of variables used in pointer arithmetic and the maximum number of pointer arithmetic a variable is involved in.
member access operations e.g.
ptr m deference operations e.g.
ptr incrementing pointers e.g.
ptr and decrementing pointers e.g.
prt are all pointer arithmetics.
the number of pointer arithmetic can be obtained from the abstract syntax tree ast of the function via simple counting.
these operations are closely related to sensitive memory manipulations which can increase the risk of memory management errors.
alongside we count how many unique variables are used in the pointer arithmetic operations.
the more variables get involved the more challenging for programmers to make correct decisions.
for these variables we also examine how many pointer arithmetic operations they are involved in and record the maximum value.
frequent operations on the same pointer authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
void fibonacci int res int n if n return res res if n if n res return for int i i n i res res r e s fig.
a function to calculate fibonacci series make it harder to track its value and guarantee the correctness.
in a word the higher these metrics the higher chance to cause complicated memory management problems and thus higher chance to dereference null or out of bound pointers.
control structure metrics v6 v11 capture the vulnerability due to highly coupled and dependent control structures such asifand while i.e.
the number of nested control structures pairs the maximum nesting level of control structures the maximum number of control structures that are control or data dependent the number of ifstructures without explicit else statement and the number of variables that are involved in the data dependent control structures.
we explain the above metrics with an example calculating fibonacci series.
there are two pairs of nested control structures ifat line respectively with ifat line 8and for at line .
obviously the maximum nesting level is two with the outer structure asifat line .
the maximum of control dependent control structures is including ifat line 7and line and for at line .
the maximum of data dependent control structures is four since conditions in all four control structures make checks on variable n. all three ifstatements are without else .
there are two variables i.e.
nandiinvolved in the predicates of control structures.
actually the more variables used in the predicates the more likely to makes error on sanity checks.
the higher these metrics the harder for programmers to follow and the more difficult to reach the deeper part of the function during vulnerability hunting.
stand alone ifstructures are suspicious for missing checks on the implicit else branches.
there usually exists a proportional relation between complexity and vulnerability metrics because the more complex the independent path and loop structures of a function the higher chance the variables pointers and coupled control structures are involved.
the complexity metrics are used to approximate the number of paths in the function which are neutral to the vulnerable characteristics.
importantly for the set of control structure metrics used as vulnerability indicators they describe a different aspect of properties than complexity metrics.
first whether control structures are nested or dependent or whether if are followed by else are independent to cyclomatic complexity metrics.
second intensively coupled control structures are good evidence of vulnerability.
instead of directly ranking functions with complexity and or vulnerability metrics we propose a binning and ranking approach to avoid missing less complicated but vulnerable functions as will be evidenced in iv b .
ranking strategy.
based on the values of these metrics for thefunctions we compute a vulnerability score for each function by adding up all the metric values rank the functions in each bin according to the scores and cumulatively identify the top functions with highest scores in each bin as potential vulnerable functions.
during the selection we identify the top kfunctions from each bin where kis initially and increase by in each selection iteration.
notice that we may take more than kfunctions as we treat functions with the same score equally.
this selection stops when an appropriate portion i.e.
p of functions has been selected.
here pcan be set by users.
similar to the binning strategy we adopt a simple ranking strategy to make our framework both lightweight and effective.
iii.
a pplica tions of leopard leopard is not designed to directly pinpoint vulnerabilities but to assist confirmative vulnerability assessment.
l eopard outputs a list of potential vulnerable functions with complexity metrics and vulnerability metrics scores which can provide useful insight for further vulnerability hunting.
in this section we demonstrate three different ways to apply the results generated by l eopard for finding vulnerabilities.
with l eopard we found new bugs in five widely used real world programs.
the detailed experimental results will be introduced in iv f .
manual auditing.
in general with the help of l eopard manual auditing e.g.
code review can be greatly improved w.r.t.
effectiveness and efficiency.
instead of auditing all the functions security experts can focus on only those potentially vulnerable functions that are identified by l eopard .
furthermore the vulnerability metrics produced by l eop ard may help security experts to quickly identify the root cause of vulnerabilities with their domain knowledge especially for complicated large functions.
for example if a vulnerable function has a large number of instances of if without else security experts could pay attention to the logic of the missing else to see if there are potential missing checks and if a vulnerable function has a large number of pointers security experts could focus on the memory allocation and deallocation operations to see if there are potential dangling pointers.
although these metrics cannot directly pinpoint the root cause it can provide explicit hints on the possible root cause.
target identification for directed fuzzing.
fuzzing has been shown as an effective testing technique to find vulnerabilities.
specifically greybox fuzzers e.g.
afl and its variants have gained the popularity and been proven to be practical for finding vulnerabilities in real world applications.
current greybox fuzzers aim to cover as many program states as possible within a given time budget.
however higher coverage does not necessarily imply finding more vulnerabilities because fuzzers are blindly exploring all possible program states without focusing the efforts on the more vulnerable functions.
recently directed greybox fuzzers e.g.
aflgo and hawkeye are proposed to guide the fuzzing execution towards a predefined vulnerable function a.k.a.
target site to either reproduce the vulnerability or check whether a patched function is still vulnerable .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
since l eopard produces a list potential vulnerable functions a straightforward application with directed greybox fuzzers is to set potential vulnerable functions as target sites.
in this way we can quickly confirm whether a potentially vulnerable function is really vulnerable or a false positive by directing the fuzzer to concentrate on the function.
note that although the fuzzer can reach a vulnerable function the vulnerability hidden in the function may not always be triggered.
but still directed fuzzing has been shown as an effective technique to reproduce vulnerabilities .
to demonstrate the idea we utilize a directed fuzzing tool hawkeye which is built upon an extensible fuzzing framework fot and reported to outperform alfgo .
however due to the large number of the potential vulnerable functions generated by l eopard it is ineffective to set all potential vulnerable functions as target sites as it may confuse the fuzzer where to guide.
to this end we choose to separate the target application into smaller modules based on its architecture design or simply namespace and then let the hawkeye to fuzz with the targets grouped by modules separately.
seed prioritization for fuzzing.
greybox fuzzers often keep interesting test inputs i.e.
seeds for further fuzzing.
these seeds need to be continuously evaluated to decide which of them should be prioritized.
by default most fuzzers e.g.
afl prefer seeds with smaller file size and shorter execution time or more edge basic block transition coverage which are not vulnerability aware decisions.
since l eopard assigns each function a vulnerability score and a complexity score we can use these scores to help to evaluate which seed should be prioritized such that the fuzzer can find more vulnerabilities in the given time budget.
for this purpose we extended fot by enabling it to accept external function level scores for seed prioritization.
the detailed seed evaluation process is explained as follows.
first we calculate a priority score for each function based on the binning andranking strategy.
for a function fwithin top k its priority score is calculated using the following formula priority score f summationtextk i 1ni n whereniis the number of functions with rank iandnis the total number of all functions.
for example if the top functions contribute a portion of to the total number of all functions then these functions are assigned with a score of .
then the function score mapping is provided to fot .
after executing a test input i.e.
seed the fuzzer can get an execution trace consist of functions.
then the fuzzer will accumulate the priority scores of the functions on the execution trace to form the priority score of that trace.
as a result each seed is associated with a trace priority score representing its vulnerableness .
when the fuzzer chooses the next seed to fuzz it will select the one with highest trace priority score.
iv .
e v alua tion leopard is implemented in 11k lines of python code.
specifically we used joern to extract the values oftable iii details of the target applications project sloc func.vul.
func.cveexcl.
cve bind .
.
504k binutils .
336k ffmpeg .
.
986k freetype .
.
126k libav .
583k libtiff .
.
118k libxslt .
.
47k linux .
.
103k openssl .
.1t 360k sqlite .
.
172k wireshark .
.
551k total 886k complexity and vulnerability metrics given the source code of an application.
more details of the implementation and evaluation are available at our website .
a. evaluation setup target applications.
we used real world open source projects that represent a diverse set of applications.
bind is the most widely used domain name system dns software.
binutils is a collection of binary tools.
ffmpeg is the leading multimedia framework.
freetype is a library to render fonts.
libav is a library for handling multimedia data which was originally forked from ffmpeg .libtiff is a library for reading and writing tagged image file format tiff files.
libxslt is the xslt c library for the gnome project.
linux is a monolithic unix like computer operating system kernel.
openssl is a robust and full featured toolkit for the transport layer security tls and secure sockets layer ssl protocols.
sqlite is a relational database management system.
wireshark is a network traffic analyzer for unix and unix like operating systems.
the details of each target application are reported in table iii.
the first column gives the project version the second column reports the source lines of code and the third column lists the total number of functions in each project.
the last three columns report the number of vulnerable functions cves common vulnerabilities and exposures and cves excluded from our research collected as ground truth see below .
here we chose the recent versions of the projects that had large number of cves.
the number of functions ranges from forlibxslt to for linux which is diverse enough to show the generality of our framework.
in total 886k lines of code and functions are studied which makes our study large scale and its results reliable.
ground truth.
to obtain the ground truth for evaluating the effectiveness of l eopard we first manually identified the list of vulnerabilities that were disclosed before july in the projects from two vulnerability database websites cve details and national vulnerability database i.e.
we collected all the vulnerabilities reported for the given version of the project from its release date to july .
cves in external libraries used in a project are not claimed to the project.
the full list of cves in most projects are recorded by the above two websites.
however the patches of the cves are not well maintained and difficult to collect.
we obtained available patches of these cves in the projects from an industrial authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a binutils .
b ffmpeg .
.
c freetype .
.
d libav .
e libtiff .
.
f libxslt .
.
g linux .
.
h openssl .
.1t i sqlite .
.
j wireshark .
.
fig.
vulnerability score vs. complexity score for non vulnerable functions in blue and vulnerable functions in red collaborator who offers vulnerability scanning services for c c programs.
functions that are patched to fix the vulnerability are identified as vulnerable.
the results are reported in the fourth and fifth columns of table iii.
as an example we display the cve list available patches and corresponding patched functions of libav at our website .
some cves failed to be included in our research as shown in the last column of table iiibecause i there is no public detail about the fix that can directly identify the affected vulnerable functions as either the cve affects some closed source projects or other reasons e.g.
cve and cve forsqlite .
.
ii the fix does not involve direct code change on functions e.g.
cve for wireshark .
.
and cve for openssl .
.1t .
research questions.
we designed the experiments to answer the following research questions q1.
is the binning step before the ranking step reasonable?
iv b q2.
is our binning and ranking approach effective and can it outperform baseline approaches machine learning based techniques and some off the shelf static scanners?
iv c q3.
what is the sensitivity of the metrics to the effectiveness of our framework?
iv d q4.
what is the performance overhead i.e.
scalability of our framework?
iv e q5.
what are the potential application scenarios of l eop ard ?
iv f b. rationality of binning before ranking q1 to answer this question we first computed the complexity score and vulnerability score as in ii b and ii c for each function in all the projects as shown in table iii .
then we plotted in fig.
3the relationship between complexity score i.e.
x axis and vulnerability score i.e.
y axis using logarithmic scale where vulnerable and non vulnerable functions were respectively highlighted in red and blue.
the result of bind is omitted for space limitations but is available on our website .
we can see from fig.
3that all projects share the similar patterns vulnerable functions are scattered across nonvulnerable functions w.r.t.
complexity score and vulnerabilityscore and there exists an approximately proportional relation between complexity score and vulnerability score for vulnerable functions.
therefore if we directly ranked the functions based on complexity metrics and or vulnerability metrics we would always favor those functions with high complexity score and high vulnerability score and miss those with low complexity but vulnerable e.g.
vulnerable functions located in the first bins in fig.
3a 3gand 3j .
instead by first binning the functions according to complexity score and then ranking the functions in each bin according to vulnerability score our framework can effectively identify the potentially vulnerable functions at all levels of complexity see details in iv c .
for all projects the number of bins ranges from to with an average of .
each bin has functions on average and of bins contain vulnerable functions.
details of the function distribution among bins can be found at our website .
as can be seen from fig.
bins with smaller complexity scores have more functions and bins with larger complexity scores have more vulnerable functions.
sparsity of bins with larger complexity scores benefits the selection of most vulnerable functions while our ranking in bins with smaller complexity scores gives more chance to identify less complex but vulnerable functions.
moreover fig.
3also visually indicates the severe imbalance between non vulnerable and vulnerable functions see the third and fourth columns of table iii which indicates traditional machine learning will over fit and be less effective more details will be discussed in iv c .
our binning and ranking approach is reasonable for predicting vulnerable functions at all levels of complexity.
c. effectiveness of binning and ranking q2 we ran l eopard on all the projects and analyzed its effectiveness when selecting different portion of functions i.e.
the parameter pin the ranking step see ii c .
here we used the percentage of functions i.e.
iden.
func.
that are identified by l eopard as potentially vulnerable and the percentage of vulnerable functions i.e.
cov.
vul.
func.
that are covered by those identified potentially vulnerable functions as the two indicators of the effectiveness of our framework.
these two indicators are used throughout the evaluation section.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a binutils .
b ffmpeg .
.
c freetype .
.
d libav .
e libtiff .
.
f libxslt .
.
g linux .
.
h openssl .
.1t i sqlite .
.
j wireshark .
.
fig.
percentage of functions iden.
func.
that are identified as potentially vulnerable and percentage of vulnerable functions cov.
vul.
func.
that are covered by those identified potentially vulnerable functions table iv identified and covered vulnerable functions projectiden.
func.
bind .
.
.
.
.
.
.
.
binutils .
.
.
.
.
.
.
ffmpeg .
.
.
.
.
.
.
.
freetype .
.
.
.
.
.
.
.
libav .
.
.
.
.
.
.
libtiff .
.
.
.
.
.
.
.
libxslt .
.
.
.
.
.
.
.
linux .
.
.
.
.
.
.
.
openssl .
.1t .
.
.
.
.
.
sqlite .
.
.
.
.
.
.
.
wireshark .
.
.
.
.
.
.
.
average .
.
.
.
.
.
the results are shown in fig.
where the x axis denotes iden.
func.
and the y axis denotes cov.
vul.
func.. the legends are only shown in fig.
4aand omitted in others for clarity and the result of bind is omitted but available on the website .
in general as iden.
func.
increases the indicator cov.
vul.
func.
also increases.
for a small value e.g.
of iden.
func.
our binning and ranking approach can achieve a high value for cov.
vul.
func.
e.g.
.
furthermore we also report how many vulnerable functions are covered when we identify certain percentage of functions as vulnerable in table iv.
when identifying and of functions as vulnerable we can cover and of vulnerable functions.
this means by identifying a small part of functions as vulnerable we cover a large portion of vulnerable functions which can narrow down the assessment space for security experts.
comparison to baseline approaches.
a recent study o n existing cross project defect prediction models and two state of the art unsupervised defect prediction models has indicated that simply ranking functions based on source lines of code sloc in an increasing i.e.
manualup or decreasing i.e.
manualdown order can achieve comparable or even superior prediction performance compared to mostdefect prediction models.
we put the results of manualup which is much worse than l eopard at our website and only show results of manualdown in this section.
in fig.
the comparison of cov.
vul.
func.
between l eop ard and manualdown is shown for each project.
l eopardtable v comparison of l eopard to existing approaches approachiden.
func.
leopard .
.
.
.
.
.
manualdown .
.
.
.
.
.
random forest .
.
.
.
.
.
gradient boosting .
.
.
.
.
.
shows better results for all projects except for binutils and freetype where both approaches have similar performance.
on average compared to manualdown .
.
and .
improvement are achieved when identifying and of functions as vulnerable as shown in table v and we identify .
.
and .
less codes measured in sloc than manualdown.
on average .
of manualdown s true positives are covered by l eopard .
this demonstrates the effectiveness of l eopard .
comparison to machine learning based techniques.
we also conducted experiments to compare our framework with four machine learning based techniques namely random forest rf gradient boosting gb naive bayes nb and support vector classification svc .
the four techniques used all complexity metrics and vulnerability metrics as the features and conducted a cross project prediction by first training amodel with the data from ten of the projects and then using the model to predict the probability of being vulnerable for the functions in the other one project.
by rotating the project to predict we obtained the prediction results for all 11projects.
a larger predicted probability indicates that a function is more likely vulnerable.
we rank the functions according to the probabilities and identify a list of high probability functions as vulnerable.
a fair comparison to l eopard can be drawn when the same number of functions is identified.
the results are shown in fig.
4and table v. as shown in fig.
an obvious shortcoming of rf and gb is the unstable performance among different projects.
it indicates that machine learning approaches highly depend on the large knowledge base of various vulnerable functions which are however hard to obtain.
specifically rf only shows similar or slightly better performance than l eopard in fig.
3a and 3b while gb only shows similar performance in fig.
3a authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table vi number of alarms and recall of static scanners projects cppcheck alarm recall alarm recall bind .
.
.
.
binutils .
.
.
ffmpeg .
.
.
.
freetype .
.
.
.
libav .
.
.
libtiff .
.
.
.
libxslt .
.
.
.
linux .
.
.
openssl .
.1t .
.
sqlite .
.
.
.
wireshark .
.
.
.
3band 3i.l eopard outperforms rf and gb in fig.
3c 3d 3e 3f 3g 3hand 3j.
both rf and gb performs even worse than the manualdown baseline in fig.
3c 3hand 3j.
as numerically shown in table v when identifying of functions rf and gb separately cover .
and .
less of ground truth than l eopard .
again l eopard does not rely on any prior knowledge about a large set of vulnerabilities but machine learning based techniques do.
nb and svc presented extremely lower recalls among the four typical machine learning algorithms.
hence we omitted the results and put them at our website .
note that projects may not be an adequate dataset for training and testing especially given the severe imbalance between vulnerable and non vulnerable functions the validity of conclusions drawn can be threatened.
however such a prerequisite for prior knowledge of vulnerable functions motivate our design of l eopard .
comparison to static scanners.
we also applied two popular static software scanner tools to investigate their vulnerability prediction capability on our dataset including an open source tool cppcheck and a commercial tool.
to avoid legal disputes we hide the name of the commercial one and refer it as s .
cppcheck and s are among the most popular static code analysis tools used to detect bugs and vulnerabilities in software.
both tools report the suspicious vulnerable statements.
whenever an alarm locates within the vulnerable functions in our ground truth we claim a true positive for that tool.
the number of total alarms reported by these two tools and the recall can be found in table vi.
cppcheck was able to analyze all projects and identified a few vulnerable functions inbinutils freetype andwireshark .
s failed to analyze linux and for the other projects only a few vulnerable functions are detected in libtiff .
static scanners often rely on very concrete vulnerability patterns.
subtle pattern mismatch will cause false positives and negatives.
thus.
their recalls are nearly which indicate that they are not promising for general vulnerability identification.
f alse negative analysis.
by examining the vulnerable functions that l eopard fails to cover when functions are identified we summarize three main reasons for false negatives they are involved in some logical vulnerabilities which are hard to be revealed by function metrics they are implicated via some fixes indirectly related to the cve e.g.
if a fix changes the function signature callers of this function should not be counted as vulnerable or security critical information is in their surrounding context and unseen from the functionfig.
sensitivity analysis results of metrics itself e.g.
calculation of complicated pointer offsets sometime is done via a separate function where no pointer metrics can be inferred thus resulting in a lower vulnerability score.
for the first case such vulnerabilities are generally hard to identify via static analysis and should not be a concern of our approach.
case two is also irrelevant to the validity of our approach.
a mitigation for the third case is to include taint information to our vulnerability metrics as will be discussed in v. f alse positive analysis.
balancing the generality accuracy and scalability is always a very challenging task for static analysis.
since l eopard is designed to reveal general vulnerabilities it is impossible to avoid false positives.
however l eopard aims to assist vulnerability assessment rather than a stand alone static analysis tool.
false analysis is therefore not a critical criteria for evaluating its capability.
furthermore some vulnerabilities are previously patched in history secretly patched or currently unexposed and it is impossible to confirm whether they are indeed false positives.
this is also reflected in the experiments in iv f where new vulnerabilities have been found in the reported potential vulnerable functions.
our binning and ranking approach is effective i.e.
identifying of functions as vulnerable to cover .
of vulnerable functions on average.
such a small portion of functions can be very useful for security experts as will be shown in our application of l eopard in iv f .
besides l eopard outperforms machine learning based techniques and static analysis based approaches.
d. sensitivity of the metrics q3 to evaluate the sensitivity of the complexity and vulnerability metrics to our framework we removed one of the dimensions of the complexity and vulnerability metrics from leopard and then ran l eopard on all the projects.
we show the sensitivity results of complexity metrics and vulnerability metrics in fig.
.
the x axis and y axis represent iden.
func.
and the delta of recall i.e.
cov.
vul.
func.
compared to leopard with all metrics.
after removing one dimension of metrics the recall delta of each project when identifying certain percentage of functions are labeled by blue cross marks where positive delta means improvement in performance and negative ones means degradation.
the red dots are average recall delta among all projects.
we can see from fig.
5that basically there are much more degradation than improvement when removing any dimension of metrics.
moreover the average recall deltas across projects are negative for iden.
func.
at and authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
in all five experiments i.e.
less vulnerable functions are covered when the same percentage of functions is identified as vulnerable.
some improvement of average recall delta at and actually results from some relatively large improvements of only a few projects.
specifically most significant degradation occurs when the cyclomatic complexity metrics i.e.
cd1 is removed and most significant average degradation occurs when the loop structure metrics are removed which indicates they make substantial contribution to our framework.
it also proves the necessity of our binning strategy.
with the above observation we can conclude that all dimensions of our complexity and vulnerability metrics contribute to the effectiveness of l eopard but complexity metrics contribute the most and it is difficult or even impossible to derive an optimal model for the metric combination that can work well for all ranges of iden.
func.
for all projects.
hence we design a generic but not optimal model that treats each metric equally.
complexity metrics significantly contribute to l eopard and it is difficult to derive an optimal metric model that works for all projects which motivates our generic model without sacrificing much effectiveness.
e. scalability of our framework q4 to evaluate the scalability of our framework we collected the time of extracting complexity and vulnerability metrics and the time of identifying potentially vulnerable functions by l eopard .
the detailed results are reported at our website .
the time used to build the code property graph and query the graph to obtain metric values depends on the number of functions in each project.
for small scale projects it respectively takes and minutes to build and query the graph and it takes hours for large scale projects i.e.
wireshark andlinux .
it takes less than seconds to identify functions even for linux .
these results demonstrate that our framework scales well for large size projects like linux .
for machine learning based techniques gb on average takes minutes to train the model and make the prediction for each project and rf takes minutes.
considering they also depend on the metrics calculation l eopard is more efficient.
s basically takes several minutes to finish the static analysis but requires the project to be well compiled and built and fail to handle linux .
the lightweight static scanner cppcheck shows comparable performance as l eopard .
our framework scales well and can be applied to largescale applications like linux .
f .
application of leopard q5 manual auditing.
code review is a popular approach for vulnerability hunting.
in this section we demonstrate the role that l eopard plays in helping security experts to hunt vulnerabilities with a case study of ffmpeg .
.
.
in order not to overwhelm the security expert we showed the top candidates with l eopard which is a list of functions with detailed complexity and vulnerability metric scores astable vii zero day vulnerabilities in php module cve id typereproducible?
bit bit php mbstring cve stack out of bound read php mbstring cve heap out of bound write php mbstring cve heap out of bound write php zend cve stack out of bound read php mbstring cve heap out of bound write php mbstring cve invalid dereference dos well as the specific variables involved in the metrics e.g.
the variables involved in control predicates.
the security expert is experienced with code review and is familiar with the basic implementation and code structures of ffmpeg .h e firstly grouped the functions into different modules and chose libavformat as the target which is the module responsible for the streaming protocols and conversion and has been prone to vulnerabilities in history.
among all functions of them are in libavformat .
he spent one day to find a divide by zero bug in one of the functions with cve assigned.
intuitively he thinks the maximum of data dependent control structures metrics with the variables involved more interesting as he can be guided to trace backward and or forward the data flow of these sensitive variables.
detailed discussion about the aforementioned case can be found at our website .
directed fuzzing.
as discussed in iii l eopard can supply targets for directed fuzzing.
experimentally we ran l eopard onphp .
.
a popular general purpose scripting language that is especially suited to web development and identified around functions as potentially vulnerable.
notice that php is used by more than of all the websites and .
.
is the current stable version.
thus php is well tested by its users developers and security researchers and it is difficult to find vulnerabilities.
we selected top functions reported by leopard as the target sites for hawkeye for bug hunting.
we divided php into several modules based on its architecture and focused on the functions in the modules e.g.
mbstring and zend that are related to file system and network data as they are often reachable through entry points.
we excluded the functions in those well fuzzed modules e.g.
sqlite phar and gd .
this manual filtering process is different from manual auditing as the security expert does not pinpoint the vulnerability directly.
after hour fuzzing we discovered six vulnerabilities in php .
.
with details shown in table vii.
seed prioritization.
in iii we also discussed the application of applying the results of l eopard to the seed evaluation process during fuzzing.
we used l eopard to generate function level scores for three real world open source projects and utilized the scores to provide guidance to fot .
the three projects are mjs a javascript engine for embedded systems xed the disassembler used in intel pin and radare2 a popular open source reverse engineering framework .
for the experiment purpose we ran fot with and without the guidance from l eopard for hours and collected the detected crashes.
table viii shows the detailed performance differences of fot with and without l eopard .
from the results l eopard can help fot to detect more crashes in hours on average.
finally seven new bugs are found in mjs seven new bugs are found in xed and a new vulnerability cve authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table viii crashes detected in hours by fot with and without the results from l eopard project mjs xed radare2 average w o l eopard with l eopard is exposed in radare2 .
these results showed that l eopard can substantially enhance the vulnerability finding for a limited time budget which is the original purpose of designing l eopard .
v. m etrics extension the set of complexity and vulnerability metrics can be refined and extended to highlight interesting functions via capturing different perspectives.
to this end we have identified the following information to be vital to further improve our findings.
taint information.
leveraging taint information will help an analyst to identify the functions that process the external i.e.
taint input.
in general functions that process or propagate the taint information can be considered quite interesting for further assessment.
hence incorporating the taint information into vulnerability metrics will further enhance the l eopard s ranking step by assigning more weight or importance to the functions that process or propagate the taint information.
vulnerability history.
in general when a vulnerability is reported the functions related to the vulnerability will go through an intensive security assessment during the patching process.
hence such information can be used to refine the ranking by either giving more importance to recently patched functions due to the verified reachability with considerable risks of incomplete patch or introducing new issues or giving low priority to the functions that are patched long before the release of the current version assuming that the functions have gone through a thorough security assessment and it is not worth the effort to re assess it.
domain knowledge.
domain knowledge can play a vital role in prioritizing the interesting functions for further assessment.
information such as the modules that are currently fuzzed by others or the knowledge about the modules that are shared by two or more projects can be used to refine l eopard s ranking.
vi.
r ela ted work here we discuss the most closely related work that aim at assisting security experts during vulnerability assessment.
pattern based approaches.
pattern based approaches use patterns of known vulnerabilities to identify potentially vulnerable code.
initially code scanners e.g.
flawfinder pscan ra ts and its4 were proposed to match vulnerability patterns.
these scanners are efficient and practical but fail to identify complex vulnerabilities as the patterns are often coarsegrained and straightforward.
differently our approach does not require any patterns or prior knowledge of vulnerabilities.
since then security researchers have started to leverage more advanced static analysis techniques for pattern based vulnerability identification e.g.
.
these approaches require the existence of known vulnerabilities or security knowledge as the guideline to formulatepatterns.
as a result they can only identify similar but not new vulnerable code.
differently we do not require any pattern inputs or prior knowledge of vulnerabilities and can find new types of vulnerabilities.
besides several attempts have been made to automatically infer vulnerability patterns e.g.
.
while promising these approaches only support specific types of vulnerabilities e.g.
missing checking vulnerabilities for and taint style vulnerabilities for .
however our approach can find new types of vulnerabilities.
metric based approaches.
inspired by bug prediction a number of advances have been made in applying machine learning to predict vulnerable code mostly at the granularity level of a source file.
in particular researchers started by leveraging complexity metrics to predict vulnerable files.
then they attempted to combine complexity metrics with more metrics such as code churn metrics and token frequency metrics .
then advances have been made to use unsupervised machine learning to predict bugs using the similar set of complexity metrics.
these approaches use the similar metrics as those in bug prediction but do not capture the difference between vulnerable code and buggy code which hinders the effectiveness.
moreover the imbalance between vulnerable and non vulnerable code is severe which hinders the applicability of machine learning to vulnerable code identification.
instead our approach specifically derives a set of vulnerability metrics to help identify vulnerable functions.
vulnerability specific static analysis.
researchers have attempted to detect specific types of vulnerabilities via static analysis e.g.
buffer overflows format string vulnerabilities sql injections cross site scripting and client side validation vulnerabilities .
while they are effective at detecting specific types of vulnerabilities they often fail to be applicable to a wider range of vulnerability types.
moreover they often require heavyweight program analysis techniques.
differently our approach is designed to be generic and lightweight.
vii.
c onclusions we have proposed and implemented a generic lightweight and extensible framework named l eopard to identify potential vulnerable code at the function level through two sets of systematically derived program metrics.
experimental results on real world projects have demonstrated the effectiveness scalability and applications of l eopard .
viii.
a cknowledgment this research was supported in part by the national research foundation prime ministers office singapore under its national cybersecurity r d program award no.
nrf2014ncr ncr001 award no.
nrf2016ncrncr002 and administered by the national cybersecurity r d directorate.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.