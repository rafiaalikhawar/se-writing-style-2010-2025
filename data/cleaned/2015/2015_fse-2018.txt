which generated test failures are fault revealing?
prioritizing failures based on inferred precondition violations using paf mijung kim the hong kong university of science and technology hong kong china mjkimab cse.ust.hkshing chi cheung the hong kong university of science and technology hong kong china scc cse.ust.hksunghun kim the hong kong university of science and technology hong kong china hunkim cse.ust.hk abstract automated unit testing tools such as randoop have been developed to produce failing tests as means of finding faults.
however these tools often produce false alarms so are not widely used in practice.
the main reason for a false alarm is that the generated failing test violates an implicit precondition of the method under test such as a field should not be null at the entry of the method.
this condition is not explicitly programmed or documented but implicitly assumed by developers.
to address this limitation we propose a technique called pafto cluster generated test failures due to the same cause and reorder them based on their likelihood of violating an implicit precondition of the method under test.
from various test executions pafobserves their dataflows to the variables whose values are used when the program fails.
based on the dataflow similarity and where these values are originated pafclusters failures and determines their likelihood of being fault revealing.
we integrated pafinto randoop.
our empirical results on open source projects show that pafeffectively clusters fault revealing tests arising from the same fault and successfully prioritizes the fault revealing ones.
ccs concepts software and its engineering software verification and validation software testing and debugging keywords automated test generation test failure prioritization fault revealing test cases fault inducing data flow analysis acm reference format mijung kim shing chi cheung and sunghun kim.
.
which generated test failures are fault revealing?
prioritizing failures based on inferred precondition violations using paf.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
this work is supported by the hong kong rgc grf grant msra collaborative research grant and nvidia academic program.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa association for computing machinery.
acm isbn .
.
.
.
a b figure call graphs to illustrate a local and b non local dataflows regarding the crash variable cv .
introduction to reduce the manual effort on writing unit tests researchers have developed automated test generation tools based on various underlying techniques such as random testing search based testing and dynamic symbolic execution .
these automated testing tools provide developers with a set of failing tests that can possibly detect uncovered faults.
however the failures from those tests are often false alarms that do not reveal real faults .
this greatly hinders the usefulness of unit test generation tools .
the main reason for false alarm failures is that generated failing tests violate the implicit preconditions that developers would be aware of while coding .
rather than indicating the existence of a real fault a generated test for the method under test mut may fail just because it violates a method s precondition on the variables whose values are used when it fails.
we call these variables crash variables denoted by cv.
judging the legitimacy of a generated failing test is challenging as these preconditions made implicitly by developers are rarely documented.
we propose an automated technique called pafto rank a generated failing test s legitimacy in the situation of undocumented preconditions implicitly assumed by developers.
it is inspired by the following observation.
the red solid arrow in figure a shows a failing test exercising a dataflow that assigns a null value to the crash variable cv causing nullpointerexceptions.
as the null value is originated after the entry of mut this failure inducing dataflow is considered local to the mut computation.
it indicates that the dataflow is wholly induced by the mut s implementation programmed by its developers.
as a result the chances of its violating an mut s precondition is 679esec fse november lake buena vista fl usa mijung kim shing chi cheung and sunghun kim small and the failing test is likely fault revealing.
in another failureinducing dataflow given by the red solid arrow in figure b the value assigned to cvthat causes nullpointerexceptions is originated before entering mut.
the dataflow is considered non local to mut computation.
it indicates that a portion of the dataflow is not induced by the mut s implementation but by the generated test s logic.
there is a chance that this dataflow portion causes violation of some mut s preconditions that are implicitly assumed by its implementation.
after the locality of failure inducing dataflows is computed our idea is to cluster failures due to a common cause by dataflow similarity and estimate their likelihood of violating preconditions by further examining the dataflows of other related tests.
the likelihood increases when more dataflows concerning the crash variable are found in other passing tests.
suppose the test exercising the non local failure inducing dataflow above figure b crashes due to dereferencing the null value of cvat a statement sincallee .
however there are two other passing tests dotted green arrows with non local dataflows resulting in the use of the same variable cvats.
since these dataflows are non local they have exercised other def use relations before entering mut causing cvto hold anon null value at statement s. as such the existence of these passing dataflows might suggest an implicit mut precondition that prevents cvfrom holding a null value at s. when more such passing dataflows are found the null pointer exception of the failing test is more likely to occur because of an implicit precondition violation rather than a real fault.
there are existing test generation techniques that mitigate the implicit precondition issue while generating tests using simple heuristics such as exception types and method modifiers dynamic invariants and temporal properties among method invocations .
however the scope of what they consider implicit preconditions is limited to only certain types of exceptions or pairs of method invocations .
also the performance of many of these techniques heavily relies on the quality of manuallywritten tests.
therefore generated failures by these tools may still suffer from precondition violations and produce false alarms.
an effective alternative is to analyze generated failures and prioritize their likeliness of precondition violations.
existing test prioritization techniques aim to execute a test suite for regression testing or mutation testing .
they do not consider prioritization of failures found in generated tests.
in this paper we present a technique called paf prioritization of automatically generated failures that clusters failing tests arising from the same cause by means of similar dataflows for crash variables.
it also classifies likely fault revealing tests and prioritizes failing tests based on their likelihood of violating the implicit preconditions of the crash variables.
paffirst analyzes the failing test executions of mut and determines if they may violate a precondition implicitly assumed by the developers on crash variables.
for the failing tests that are not subject to such violation pafclassifies them as likely fault revealing and reports them at the top of the list.
pafthen sorts the tests in the classified list in a reverse order to their likelihood of violation.
the likelihood is estimated by observing other dataflows reaching the same crash variable in the generated passing tests.we implement pafand evaluate it on versions of five popular open source projects.
the results show that pafeffectively clusters fault revealing failing tests that share a common cause.
the results also show that pafaccurately classifies fault revealing tests and places fault revealing tests at higher priority.
pafoutperforms the precondition violation filtering component of existing test generation techniques such as jcrasher and techniques using dynamic invariants .
among the generated failures by randoop for the subjects pafreported fault revealing alarms.
the failures clustered by these alarms have a precision of .
.
among these alarms four are new faults detected by paf.
they were confirmed and fixed by the developers.
pafmakes the following contributions a technique that analyzes the dataflows of crash variables and thereby identifies if a failure can be induced by the synthesized logic of generated tests.
a technique that groups failures i.e.
failing tests based on the similarity of their failure inducing dataflows and prioritizes them using the associated likelihood of violating implicit preconditions of crash variables.
a prototype implementation that integrates pafinto randoop and an experiment of it on versions of five popular open source projects.
the experiment results show that paf achieves high fault detection rate.
in the rest of this paper we first illustrate our technique with a motivating example.
next we explain the methodology of our technique and present the experiment.
we then discuss the related work conclusion and future work.
motivating example let us consider a motivating example in figure .
it shows an automatically generated test suite of two failing and two passing tests for class projectentry .
the source code is real world code from our subjects although they are slightly modified for the illustration purpose.
the test cases are generated by a state of the art test generation tool randoop .
the failing tests throw nullpointerexceptions that are caused by dereferencing a null value of variables e and project at the crash statements and respectively.
as such they are considered the crash variables of the failing tests.
to investigate whether a given mut e.g.
indexof forftest1 and handleinput forftest2 prescribes a precondition on the crash variables paffinds where the null value used by each crash variable is originated.
to do that paflocates the statement that creates the null value to be received by the crash variable.
this statement is referred to the crash origin of the failure inducing dataflow.
details of identifying the crash origin will be explained in section .
.
based on the location of an crash origin pafdetermines whether the concerned dataflow is local or non local.
since the crash occurs during an mut s computation the locality of a dataflow follows the crash point i.e.
crash statement of its crash origin.
consider ftest1 in the motivating example.
the null value received by variable e causing nullpointerexceptions is originated from entries.get key at line during the computation of indexof .
so line is the crash origin of the failure inducing dataflow exercised by ftest1 .paf considers the failure inducing dataflow which starts at line and ends at line local to the mut indexof s call graph.
in the case forftest2 the null value received by variable project is originated 680which generated test failures are fault revealing?
esec fse november lake buena vista fl usa four generated test cases for projectenry class failing tests public void ftest1 project p new project projectentry pc new projectentry p pc.
put object .0d object .
d int i pc.
indexof object hi!
mut public void ftest2 projectentry pc new projectentry byte byte array new byte pc.
handleinput byte array mut passing tests public void ptest1 projectcomponent pc new projectcomponent pc.
setproject new project hi pc.
handleinput new byte public void ptest2 project p new project projectcomponent pc new projectcomponent p pc.
handleinput new byte source code for projectenry class private hashmap entries private project project public projectentry public projectentry project project this .
project project entries new hashmap public void put object key object value entry e new entry key value entries .
put key e public int indexof object key entry e entry entries .
get key int pos while e. prev !
sentinel npe pos e e. prev return pos public void setproject project project this .
project project public project getproject return project public int handleinput byte buffer int offset return getproject .
input buffer offset npe figure example of source code and a set of automatically generated tests.
from the default field initializer at line before handleinput is invoked.
so the failure inducing dataflow exercised by ftest2 is considered non local to the mut handleinput s call graph.
note that the def use relation of assigning the null value to project at line and the use of the project s value at the crash statement arises from the logic synthesized by the generated failing test.
there are chances that the synthesized logic is inapplicable to the assumed usages of handleinput i.e.
violating its implicit preconditions.
after determining the locality of a failure inducing dataflow paf observes whether there are other related dataflows to the concerned crash variable exercised by passing tests.
for ftest1 there are no other def use relation reaching the crash statement.
for ftest2 there is one def use relation exercised by passing tests reaching the crash statement and it is defined at line .
pafthen partitions the generated failing tests into two groups local and non local.
local failing tests are ranked before non local failing tests because a local failure inducing dataflow arises wholly from the logic of the mut and its callees.
in most cases the mut developers should be aware of the underlying implicit preconditions.
as such local failure inducing dataflows have less chances of precondition violation as compared with non local failure inducing dataflows.
pafthen reorders the failing tests in each group based on the proportion of the def use relations reaching the crash statement exercised in passing tests.
this is the more such passing tests exist the likelihood of a precondition violation increases.
table presents the summary of the collected data.
in this example ftest1 is more likely to be fault revealing than ftest2 because the failure inducing dataflow is local.
although the partitioning based on the locality is sufficient to reorder the failing tests in this example if we further investigate the likelihood it would be forftest1 and for ftest2 .
thus the failing tests are ordered ftest1 ftest2 .
in the real world ftest1 reveals a real fault thattable motivating example for the failing tests in figure .
locality of def use passing failure reaching tests crash inducing crash exercising test origin dataflows statement def use ftest1 line local none ftest2 line non local ptest1 ptest2 was reported in the bug repository and fixed2.
on the other hand ftest2 has been confirmed as a false alarm by the developer3.
our approach in this section we present the details of paf.
figure a shows the four processing phases of paf.
it takes a target program a set of generated failing tests and a set of generated passing tests as inputs and outputs a list of reordered failing tests.
in phase paffirst identifies the crash variable for each failing test.
it then finds the crash origin a statement where the value assigned to the crash variable is originated in the failing test.
this statement becomes the starting point of the failure inducing dataflow.
in phase pafgroups the failing tests into different sets called test flow sets based on the similarity of their failure inducing dataflows and then further partitions these sets i.e.
test flow sets into two categories local ornon local based on the location of their crash origins.
in phase pafdetermines the likelihood of potential precondition violations by examining the related dataflows exercised by other passing tests.
finally in phase pafreorders the classified groups of test flow sets based on their locality and 681esec fse november lake buena vista fl usa mijung kim shing chi cheung and sunghun kim a overview b output diagram figure overview of our approach a and an output example of the reordered list returned by paf b .
likelihood obtained in phases and .
figure b shows an example of a reordered list returned by paf.
.
phase find crash variable and its origin in phase pafinvestigates how an incorrect value propagates to a crash variable in each failing test.
identify crash variable pafidentifies the crash variable cv that is used at the crash statement for each failing test.
the information about the crash statement e.g.
file name method name and line number is provided by the top frame of a stack trace.
however identifying cvfrom the top stack frame is not always straightforward because a statement can involve multiple variables.
to cope with this challenge pafuses dynamic analysis by instrumenting the program and monitoring all variables used at the top stack frame.
pafconsiders different types of exceptions to choose candidate variables to monitor because a crash variable plays different roles in different exceptions.
suppose that the statement of the top frame is var array .foo .
when a test throws an arrayoutofboundexception candidates to monitor are those variables used as an array index which in this case is i jonly.
pafanalysis is done on the intermediate representation of the program e.g.
jimple for soot .
therefore when more than one variable is involved in an array index like i j array would be represented like r i j array .
thus in our analysis the crash variable candidate would be r. when a test throws a nullpointerexception however the candidates are array and array .pafthen runs the failing tests on the instrumented program to identify the exact crash variable which is the one accessed right before the failure.
note that pafalso supports multiple crash variables in a failing test.
for example if an illegalargumentexception occurs under a branch condition involving multiple arguments pafidentifies all the argument variables used in the branch condition as the crash variables.
pafperforms the analysis for the rest of phase and phase for each crash variable individually.
derive crash origin and failure inducing dataflow paf derives a crash origin of the crash variable s value v used at the crash statement in the failing test under consideration.
the crash origin is a statement where the value vis assigned in the failing execution.
this value is subsequently received by the crash variable resulting in a program failure at the crash statement.
given a failing execution and a crash variable pafidentifies the crash origin by transitively tracing backward from the the crashstatement the interprocedural def use associations dua given the execution trace of a failing test.
pafperforms the backward tracing only along copy statements that either copy a variable s value to another variable e.g.
a b or assign a method s return value to a variable e.g.
a getb .
this is because the value assigned at the crash origin should reach the crash variable without being modified.
as such the backward tracing stops when it hits a non copy statement.
for example consider a non copy statement sin an intermediate representation r a b where ris assigned with and causes a dividebyzeroexception.
in this case pafstops the backward tracing at sand returns it as the crash origin rather than continuing tracing further for aand b. the set of def use associations collected during the backward tracing form a chain that describes a failure inducing dataflow starting with the value v s creation at the crash origin and ending with the use ofvthrough the crash variable at the crash statement.
the dataflow traverses a def clear path of the crash variable from its assignment of value vto the use of its value at the crash statement.
revisit ftest2 in figure for illustration.
the backward tracing is done starting from the crash statement until it hits line where project is initialized with a default value null.
the failureinducing dataflow is formed with a chain of interprocedural duas project returnvar where returnvar refers to a return variable that copies the return value of getproject .paf returns as the crash origin which is the definition statement of the first dua in the failure inducing dataflow.
.
phase group and partition failing tests in phase pafgroups failing tests in two steps.
in the first step failing tests sharing the same crash origin crash variable and crash statement are clustered into the same set which we call a test flowset.
the clustering is motivated by an observation that generated failing tests exercising close failure inducing dataflows are mostly redundant to each other sharing the same failure cause.
it helps alleviate developers debugging effort from the need to inspect all failing tests in the same flow set.
in the second step pafpartitions the failure inducing dataflow of a flow set into two categories based on its crash origin s location local or non local .
note that all tests in the same test flow set share the same failure inducing dataflow.
pafconsiders a dataflow local if its crash origin is executed after the entry of mut otherwise non local .pafmakes a special treatment of field initializers since they do not belong to any methods.
as field initializers are executed 4a def use association represents a data flow relationship in a triple d u v such that a variable vwhich has been defined in statement dreaches and is used in statement uwithout being redefined along a control flow path that connects dtou.
682which generated test failures are fault revealing?
esec fse november lake buena vista fl usa spontaneously when a constructor is called pafbundles them with the constructor and considers their execution after the entry of the constructor.
note that the dataflow similarity and origin locality are orthogonal features.
therefore the order of clustering and partitioning does not matter.
.
phase calculate violation likelihood after partitioning test flow sets into local and non local groups pafmeasures the likelihood of a potential precondition violation within each group.
intuitively the likelihood of a failing test to violate preconditions increases when its crash statement can be reached by more passing tests.
that is a failing test is more likely a false alarm when its crash statement is reached times by different passing dataflows than when it is reached times by the same passing dataflow.
therefore counting just the number of passing tests reaching the crash statement alone might cause a bias towards or against specific dataflows.
to address this challenge pafcalculates the likelihood according to the dua coverage by passing tests reaching the crash statement.
passing tests that cover different duas are considered distinct.
specifically it works as follows.
for each failing test flow set paf extracts a set of duas with respect to the crash variable cvat the crash statement u. each extracted dua d u cv satisfies three criteria drefers to a statement where cvis defined urefers to the crash statement and there exists a def clear path from d touin the program s control flow graph.
we denote the set of extracted duas as .
using the extracted duas for each failing test flow set pafmeasures its likelihood based on the number of duas in covered by at least one passing test as follows likelihood duas in covered by passing tests duas in paforders failing test flow sets according to the likelihoods within their locality group.
suppose ftest1 and ftest2 in figure are two failing tests belonging to two different test flow sets fs1 andfs2 respectively.
fs1assumes the forftest1 which is e .
since there is no passing test that covers this dua fs1 s likelihood is .
note that all failing tests in the same test flow set assume the same likelihood because the set of extracted duas i.e.
would be identical for all tests in the same test flow set.
fs2assumes the forftest2 which is returnvar .
this dua is covered by two other passing tests ptest1 and ptest2 .
the likelihood of fs2is therefore .
.
phase reorder failing tests in phase pafreorders the set of failing test flow sets in each group i.e.
local and non local according to the computed likelihood.
a failing flow set with a lower likelihood is ranked before that with a higher one.
test flow sets ended up with a tie in likelihood values are reordered among themselves in a random order.
after the reordering is done the final output of a reordered list would look like figure b .
recall that pafcan handle multiple crash variables involved in a single failing test as discussed in phase .
pafperforms the analysis for each crash variable individually.
that is pafcomputes the likelihoods for each flow sets corresponding to the multipletable generated failures and their types from randoop total assertion subject fails npe errors other ant .
.
.
collections .
.
.
ivy .
.
.
math .
.
rhino .
.
.
average .
.
.
.
crash variables and uses the largest likelihood for reordering the test flow set.
the largest likelihood is used because a failing test would be a false alarm if any of its failure inducing dataflows to one of the crash variables violates implicit preconditions.
experiment setup to assess our approach we implemented pafand evaluated it on versions of five real world java programs.
our evaluation investigates three research questions rq1 how accurately does our approach cluster fault revealing tests with the same failure cause?
this study aims to evaluate whether test flow sets sharing the same crash origin crash statement and crash variable can accurately cluster fault revealing tests with the same failure cause.
rq2 how likely is a failing test fault revealing if it exercises a local failure inducing dataflow?
this study aims to evaluate the usefulness of partitioning failing tests based on the locality of the dataflows.
rq3 can our approach improve the rate of the fault detection of generated failing tests?
this study aims to demonstrate the effectiveness of our prioritization technique by considering whether our approach can detect faults faster than other approaches.
.
implementation we integrated pafinto randoop v3.
.
so that pafoutputs the ordered list of test flow sets.. we selected randoop because of its popularity and adoption by the industry and academia for automated test generation.
we implemented the paftool on top of the soot framework .
pafanalyzes and instruments the bytecode of a program by utilizing the jimple intermediate representation of soot.
to compute defuse associations duas and monitor def use coverage we used a fine grained data dependence analysis tool dua forensics that provides standard interprocedural data flow algorithms for object oriented languages on top of soot.
our implementation supports four types of runtime exceptions that are the most common types generated from randoop .
those types include nullpointerexceptions npes illegalargumentexceptions illegalstateexceptions and arrayoutofboundexceptions.
paftreats an application specific exception thrown under a branch condition as an illegalstatementexception.
as shown 683esec fse november lake buena vista fl usa mijung kim shing chi cheung and sunghun kim table subjects used in the empirical studies.
class class w under failing subject label loc test tests ant1.
.
ant1 86k .
.
ant2 102k collections2.
coll1 7k .
coll2 11k ivy2.
.
ivy1 50k .
.
ivy2 51k math .
math1 49k rhino1.
.r2 rhn1 43k .
.r3 rhn2 55k .
.r5 rhn3 57k in table pafcan support over of failures from randoop with these four error types.
note that pafdoes not handle assertion failures triggered from test cases.
since assertions written in tests check violations of postconditions rather than preconditions after mut calls are already returned it is difficult to automatically infer precondition violations in such situations.
.
subjects and experiment design table shows the five open source java projects that we used for our experiment subjects.
to select our subjects we identified open source projects used for evaluation in earlier related work using randoop such as palus ocat and palulu .
we theexcluded those projects that were not actively maintained for at least past months in github randoop did not produce any failing tests for or paffound no local test flow sets.
to conduct our experiments for each subject we ran pafon the deployed jar that bundles the entire set of classes counted in the fifth column of table and acquired the sets of failing tests for the classes counted in the last column of table .
to avoid possible biases caused we used the default command line settings for randoop.
particularly we used default settings for null inputs i.e.
no direct null passing as an argument .
that is nullpointerexceptions caused with null arguments are filtered out.
thus column in table lists npes occurred only with non null argument inputs.
we validated whether a generated failing test is fault revealing using the following criteria.
a failing test of a program pis considered fault revealing if the test passes in a subsequent version of p indicating that the relevant fault in phas been fixed.
for those failing tests exercising local failure inducing dataflows when we could not find the relevant patch making the test pass we reported it in the subject s bug repository.
as a result four bug reports filed by us have been confirmed and fixed two for ant one for math and one for ivy .
if a failing test still fails in the latest code we consider the test non fault revealing.
the rationale behind this criterion is based on an earlier study by ray et al.
.
it reports that if a bug is introduced to code the bug will be fixed within few months .
all the subjects that we have selected have been actively maintained 5assertion errors shown in table include ones triggered from the tests.table our test dataset.
fr denotes fault revealing.
randoop paf distinct failing fr test fr test passing subject faults tests tests flow sets flow sets tests ant1 ant2 coll1 coll2 ivy1 ivy2 math1 rhn1 rhn2 rhn3 total and released for at least months.
as such we consider faults of an actively maintained program are mostly fixed within months.
this implies that if there has been no changes to make the test pass for last months the test is likely to be non fault revealing.
to supplement this criterion on non fault revealing tests we ensure the test fails in the latest version with no changes made to the mut s execution.
for those tests that failed in the latest version with some changes we manually inspected and checked whether changes have been made to the evaluation of crash variables.
if not it is considered non fault revealing.
if so we filed an issue report seeking developers confirmation.
we found two such cases and reported.
the developer confirmed all of them as non faultrevealing which conforms to our criterion.
experimental results in this section we present and discuss our experimental results on the research questions.
the tool and dataset are available at table presents the information of the generated failing tests returned by randoop andpaf.
column shows the distinct number of faults that can be revealed by randoop tests.
we consider each patch identifies a distinct fault.
column lists the number of failing tests generated by randoop .
among these tests column shows the number of confirmed fault revealing tests based on the criteria described in section .
.
column lists among randoop s tests in column the number of test flow sets sharing the same crash origin crash statement and crash variable.
column represents the number of fault revealing test flow sets.
although tests in the same test flow sets may not execute the same path all tests in a same set are either all fault revealing or all not fault revealing.
our validation of the fault revealing flow sets found that the patches committed in a subsequent version to fix the faults made all tests in the same test flow sets pass.
this further indicates that all tests in a same fault revealing flow set reveal the same fault.
finally the last column lists the number of passing tests generated by randoop .
we used these passing tests to determine the likelihood of precondition violations.
note that ivy2does not have any confirmed fault revealing tests.
for this reason ivy2is not involved in the study of rq1 and rq3.
684which generated test failures are fault revealing?
esec fse november lake buena vista fl usa table results for rq1.
clustering accuracy f measure of clusters subject paf reb mse paf reb mse opt ant1 .
.
ant2 .
.
.
coll1 .
.
coll2 .
.
ivy1 ivy2 math1 .
.
rhino1 .
.
rhino2 .
.
rhino3 .
.
additionally for ant1 the reason why the number of fault revealing tests is significantly larger than other subjects is that out of tests are in the same test flow set.
because the crash statement is located in a superclass of many subclasses it is often executed and failed whenever the subclasses are tested.
.
rq1 clustering performance the goal of this study is to evaluate how accurately test flow sets derived by pafcan cluster fault revealing tests not all failing tests with the same failure cause.
we obtained the ground truth by considering multiple failing tests induced by the same fault if they failed before a patch was applied and passed after the patch was applied.
ideally the number of failing test clusters is the same as the number of distinct faults which is given in column of table .
as an evaluation metric we used the f measure which is widely used for evaluating clustering techniques .
to calculate the f measure we denote cas the set of all clusters grouped bypaf i.e.
fault revealing test flow sets and oas the set of ground truth clusters for the optimal clusters.
more specifically we denote cias the ithcluster ojas the jthcluster and nas the total number of fault revealing tests in column of table .
we calculate the precision and recall as follows precision ci oj ci oj ci recall ci oj ci oj oj f measure computes the weighted average of maximal f measure for each clusters as follows f ci oj recall ci oj precision ci oj recall ci oj precision ci oj f measure c i ci n max j f ci oj we compared our results with two baselines rebucket and mseer .
the two baselines are state of the art representatives of clustering approaches for program traces.
rebucket groups failing stack traces based on their similarity.
mseer leverages rankpromixity to group failing tests based on the suspiciousness rankings of statements by analyzing execution traces of failing and passing tests.
to obtain the suspiciousness rankings we used a stateof the art spectrum based fault localization tool called gzoltar .
the data in table show that pafoutperforms the two baselines rebucket reb and mseer mse in most cases.
the last column opt refers to the ideal case of optimal clustering where all failing tests due to the same fault are clustered in the same set.
pafachieves f measure for most subjects and at least for all subjects.
the reason of preventing paffrom attaining f measure for ant2 and math1 is that a crash origin flows to two different crash statements and the fix was made at the crash origin.
forrebucket if the call stack is not deep and identical to that of other tests it can also achieve high accuracy like ant2 and math1 .
however it poorly performs for some subjects rhino .
this is because rebucket trains hyper parameters by itself rather than requiring the users to tune them.
for rhino the parameters are poorly trained and rebucekt yields very low f measure.
for mseer since failing tests arising from the same fault may traverse different execution paths the suspiciousness scores of the same statement may vary across the tests causing them to be partitioned into different clusters.
forivy1 the f measure is for all approaches because there is only one fault revealing test as shown in table .
for ivy2 the results are inapplicable because there is no fault revealing tests as discussed earlier.
the results suggest that test flow sets grouped by the same crash origins crash variables and crash statements can effectively cluster failing tests subject to the same fault or cause .
therefore pafenables developers to examine significantly less failing tests because one test in a test flow set can represent the whole group.
.
rq2 accuracy of locality based partitioning inpafphase it partitions the failure inducing dataflows induced by failing tests into local and non local.
the goal of this study is to evaluate the accuracy of using this partitioning result to identify if a failing test is fault revealing.
to do this we measured the precision and recall of the partitioning.
since our test flow sets share the same failure inducing dataflows we measure the accuracy in terms of test flow sets rather than individual failing tests.
we also compared the results with two baselines other false alarm exception filtering approaches using jcrasher s heuristics and daikon s dynamic invariants .jcrasher s heuristics take into account exception types and access modifiers e.g.
public or non public of exception throwing methods to classify whether a given failing test is likely to violate preconditions.
for another comparison we used daikon s dynamic invariants mined from generated passing tests from randoop as counted in table .
we extracted relevant invariants with respect to the crash variable at the entry of mut.
if such invariants exist we classified the corresponding failing tests to the crash variable as false alarms otherwise as fault revealing.
existing test generation tools such asdsd crasher which is an extended version of jcrasher and eclat also leverage daikon s invariants to filter illegal inputs that violate invariants at the mut s entry.
table presents the comparison results.
since jcrasher anddaikon classify individual tests without grouping the precision and recall ofpafare measured for both test flow sets and individual tests.
fr flow set and fr test stand for the number of fault revealing flowsets and the number of fault revealing failing tests for each subject in our ground truth.
alarm and true stand for the number of 685esec fse november lake buena vista fl usa mijung kim shing chi cheung and sunghun kim table precision pre.
and recall rec.
results for rq2.
subjecttest flow sets individual tests fr paf paf jcrasher daikon flow true fr true true true set alarm pre.
rec.
test alarm pre.
rec.
alarm pre.
rec.
alarm pre.
rec.
ant1 .
.
.
.
.
.
ant2 .
.
.
.
.
coll1 .
.
.
coll2 .
.
ivy1 .
.
.
ivy2 math1 .
.
rhino1 .
.
rhino2 .
.
rhino3 .
.
total .
.
.
.
.
.
.
alarms selected as fault revealing by each individual tool and the number of true alarms among these alarms respectively.
the table shows the number of test alarms made by pafis less than that of other approaches in most cases.
pafachieves significantly higher precision than jcrasher and daikon for all subjects.
paf also achieves higher or equal recall for all subjects except ant1 and ant2.
moreover for all subjects pafachieves at least precision or recall.
for those subjects that could not achieve precision ivy1 and three rhinos the reason for imprecision is from one common pattern shown in the following code snippet.
public void mut m1 .foo npe because m1 is null public object m1 int i return i ?
field null null is returned the crash origin is local to the mut s call graph because m1explicitly returns null.
therefore this npe occurred due to an incorrect behavior of mut rather than the incorrect test input.
we reported this issue but our reports are pending.
however we believe this issue is a bug because we found this fault pattern was properly handled to avoid an exception in other subjects antand coll .
nevertheless since there has been no changes the relevant failing tests cannot be confirmed as fault revealing based on our criterion.
pafachieved low recall because of two reasons for ant1 and ant2.
first four test flow sets of both ant1 and ant2 reveal faults on fields involving concurrency and logging.
preconditions on these fields should be better handled in the code to reliably support method calls at any time.
we confirmed that the fixes had been applied in a subsequent version.
second five test flow sets of ant2 fails when mut is equals method that overrides a java library method.
although the null input option is disabled as default equals directly passes null in the tests because randoop uses it for its contract checks e.g o.equals null false .
this is an exceptional case where mut overrides such java library methods that strictly require explicit preconditions in the code.
forivy2 since no fault revealing tests were found the precision is and the recall is not applicable.
however the number of selected alarms from pafis significantly lower than other tools even for individual tests.the recall of paf individual tests for ant1 produces outliers that make the average recall dramatically drop.
this is because one testflow set containing tests mentioned in table s description is not selected.
although pafoutperforms other tools in all aspects the recall ofpaf individual tests for ant2 yields average recall lower than daikon .
however pafgenerates significantly less number of highly precise test alarms for all subjects for ant2 vs .
additionally our likelihood measurement successfully addresses this weakness by placing fault revealing tests in higher ranks.
overall the results suggest the locality of failure inducing dataflows offers a nice criterion to select fault revealing failing tests in two aspects.
first four of the local failure inducing dataflows local flows were newly detected by paf.
all of them were confirmed and fixed by the developers.
although three more bug reports associated with other local flows are still pending none of our reports have been rejected.
second randoop generated failures failing tests for versions of popular open source subjects.
pafidentified of them able to exhibit local flows.
it clustered these failures into test flow set alarms based on their common causes so that users need only to inspect one failure per flow set.
test flow sets were found to be truly fault revealing.
these flow sets revealed fault revealing failures.
as a result pafenables developers to inspect only alarms to confirm fault revealing failures out of its identified failures yielding a precision of .
.
.
rq3 prioritization performance the goal of this study is to evaluate how fast pafdetects the faults using a paf s ranked list.
to do this we measure the fault detection rate of prioritized test suites in this study.
we use a widely used metric called apfd average percentage faults detected .
apfd measures the weighted average of the percentage of detected faults over the life of a test suite.
an apfd value ranges from to the higher value means the better i.e.
faster fault detection rate.
apfd can be measured in an equation as follows apfd t f1 t f2 ... t fm nm 2n 686which generated test failures are fault revealing?
esec fse november lake buena vista fl usa a b figure example of the apfd average percentage faults detected measure.
nandmindicate the numbers of failing tests and faults respectively.
t fimeans the rank of the first test case that reveals fault iin the reordered test set.
figure gives an illustrative example borrowed from the prior literature on test prioritization .
figure a shows the information of a test suite of five tests a through e which is able to detect faults.
suppose the test suite is prioritized in order of a b c d e. figure b presents the percentage of detected faults when the fraction of test suite is reached.
the area under the curve represents the weighted average of the percentage of detected faults over the life of a test suite.
thus apfd is in this example.
we compare this apfd score with that of existing techniques jcrasher and daikon .
since they do not further rank tests after partitioning as described in rq2 we placed the group of tests classified with no potential violation alarm in table at the top the group of tests classified with potential violation at the bottom.
we then ranked the tests in each group randomly.
as discussed in rq2 since jcrasher and daikon do not handle clustering we measured apfd scores of pafwith and without clustering i.e.
test flow sets and individual tests .
figure presents the results.
the data in the graph show that on average the leftmost in the graph the fault detection rate of our prioritization approach outperforms jcrasher and daikon .
paf individual tests outperforms other techniques in most cases.
this demonstrates the usefulness of ranking failing tests using the violation likelihood.
although paf test flow sets performs slightly worse that paf individual tests the use of test flow sets reduces the number of test cases to examine significantly by .
.
the results for individual tests are better because apfd scores are calculated with regards to the rank of the first test that reveals the same fault.
since there are multiple tests sometimes tens or even hundreds in the same test flow set the apfd value can be pushed up by the top ranked fault revealing test.
on the other hand pafmeasures the likelihood of a test flow set based on the test with the highest likelihood i.e.
the lowest ranked fault revealing test in the set.
pafconsistently achieves at least apfd score for all subjects except ant1 and ant2 while jcrasher greatly fluctuates across different subjects.
the main reason is that the precondition violation filtering of jcrasher s heuristics depends solely on exception types and method modifiers.
these two types of information may not be effective in inferring a violation of a precondition.
daikon performs better than jcrasher in most cases although it performs poorly for some subjects ivyand math .
most importantly .
.
.
.
average ant1 ant2 coll1 coll2 ivy1 math1 rhino1 rhino2 rhino3 jcrasher individual test daikon individual test paf individual test paf test flow sets figure results for rq3 showing apfd scores.
however paf individual test outperforms daikon in all subjects except ant2.
this implies that considering only invariant violations is insufficient for effective prioritization because the quality of dynamic invariants mined varies by profiles and coverage of passing executions.
paf s performance may also be affected by this issue.
however unlike daikon pafbases its analysis on failure inducing dataflows.
the analysis can better identify the likelihood of precondition violations and reduce the ambiguities that may arise from the inadequate invariants mined.
in summary this study shows that the use of generated passing tests is useful for ranking.
the proportion of exercised duas by passing tests rather than the raw number of passing tests can affect the ranking.
this percentage represents the chances that the crash variable is reached in various passing dataflows.
this information helps improve the ranking effectiveness when recall is not .
.
threats to validity we consider several threats to validity of our experiments.
a potential threat in our experimentation is that the set of fault revealing tests used for the experiments may not be accurate.
in general there is no ground truth that a failing test is not fault revealing.
to address this we adopted well defined and objective criteria to label fault revealing tests as discussed in section .
.
threats also arise when the results from the experiment are not generalizable to other environments such as in other testing tools.
to mitigate these threats we evaluated our experiment using tests generated from randoop which is a popular automatic test generation tools used both in academia and industry.
in future we plan to investigate paf s results on other test generation tools.
since pafanalysis considers java runtime exceptions described in section .
pafcan be integrated into other test generation tools in the same way as randoop .
to alleviate the generalizability threat we selected five popular open source projects with different sizes and application types e.g.
commandline tools and libraries for experiments.
however our results may not be generalizable to commercial projects gui projects and projects written in other languages.
related work our contributions relate to the work that clusters failing tests with the same failure cause improves the input quality of generated tests and assesses the fault detection ability of test suites.
687esec fse november lake buena vista fl usa mijung kim shing chi cheung and sunghun kim for the area of clustering failing tests existing techniques mainly leverage similarities among stack traces or execution traces .
for the execution trace based approach there are two types rank proximity and trace proximity.
the rank proximity uses distances between pairwise rankings returned from spectrum based fault localization .
the trace proximity clusters tests based on the coverage or profile similarity of their execution traces .
it was reported that rankproximity is more advanced and outperforms trace proximity .
our technique not only clusters failing executions with the same causes but also prioritizes them based on the likelihood of precondition violations.
to improve the input quality of generated tests some existing techniques address the implicit precondition issue.
as discussed in introduction jcrasher leverages exception types and access modifiers dsd crasher and eclat use dynamic invariants of daikon mined from sample executions.
in addition fraser et al.
uses temporal properties among method invocations.
other work addresses the false alarm issue using specification mining of api protocols and search based approach at the gui level .
our approach is orthogonal to these techniques because they focus on generating fault revealing tests while ours prioritizes generated tests for early fault detection.
jain et al.
proposed a technique that determines the feasibility of argument inputs generated by dynamic symbolic executions using daikon s invariants and ranks the generated inputs based on the confidence values of invariants.
however this approach targets argument inputs generated for a single method rather than a sequence of methods that our approach targets.
various techniques were proposed to enhance the feasibility of generated test inputs by leveraging dynamic information from sample executions and learning desirable object states.
other techniques were proposed to generate feasible test oracles based on the realistic specifications of expected program behaviors.
these techniques use program invariants sequences of program execution seeded defects and javadoc comments .
however these techniques were developed to improve code coverage or fault detection ability rather than to reduce the false alarm rate of generated failing tests like paf.
there has been some research that assesses the effectiveness of a test suite in detecting faults based on several test adequacy criteria.
some studies showed that the dataflow adequacy criteria can influence the test suite effectiveness in the fault detecting ability.
this finding supports the underlying analysis of paf which uses data dependences chains to identify fault revealing tests.
an earlier study showed that the size of a test suite can also influence the fault detection ability.
it found that after a certain point the fault detection rate barely increases even though the size of a test suite keeps increasing.
this finding also supports our results where paf assigns high ranks for most fault revealing test flow sets.
additionally to improve the fault detection ability of test suites attention was paid to the problem of faulty test code which produces false alarms and reduces the quality of test suites .
herzig and nagappan developed a false alarm detection approach by mining association rules using the false alarm history in the past.
a test analysis technique developed by waterloo et al.
categorizes test patterns to find faulty tests.our approach differs from these techniques in that it targets automatically generated tests and uses program analysis rather than machine learning.
it does not assume the availability of a false alarm history.
moreover these techniques target faulty or obsolete tests written by developers.
thus the root causes of these faults differ from ours which is related to implicit precondition violations.
a number of techniques were developed to improve test suite maintenance.
some of them aimed to assess the quality of humanwritten tests using dynamic tainting analysis and test dependency analysis .
zoomin is a technique developed to help improve oracles of automatically generated tests using dynamic invariants from human written tests.
the main goal of these technique is different from ours because they try to improve the quality of test suites and test oracles for test suite maintenance rather than for failure inspection.
conclusion and future work in this paper we presented a technique called pafthat clusters generated failing tests into test flow sets due to the same fault and prioritizes these tests in a reverse order to their likelihood of violating an implicit precondition.
we introduce the concepts of crash variables and crash origins.
pafperforms the analysis in three steps.
first pafderives a failure inducing dataflow concerning the crash variable for each failing test.
tests inducing similar failure inducing dataflows are considered to share a common failing cause and clustered into the same test flow set.
second pafchecks whether such dataflows are local or non local to the execution of the method under test mut based on the location of their crash origins.
local dataflows are given higher priority than non local ones because a local dataflow indicates the failure is wholly induced by the mut s implementation programmed by its developers.
as a result the chances of its violating an mut s precondition is small.
third pafexamines the dataflows exercised by other related passing tests to estimate the likelihood of potential precondition violations.
the likelihood increases when more dataflows concerning the crash variable are found in other passing tests.
the likelihood is measured by calculating the proportion of other dataflows exercised by passing tests.
paffinally reorders failing tests first by placing the group of tests exercising local dataflows prior to that exercising non local ones.
paffurther sorts tests in each group by the violation likelihood.
we conducted experiments based on five popular open source projects with tests generated by randoop .
the experimental results show that test flow sets can effectively cluster fault revealing tests arising from a common cause.
the results also show that locality analysis can accurately classify fault revealing tests and the likelihood calculation is effective in prioritizing the fault revealing tests.
comparing with rebucket and mseer pafcan more accurately cluster fault revealing tests.
it also outperforms existing techniques that filter potential precondition violations using jcrasher s heuristics and daikon s invariants .
for future work we plan to extend our empirical studies with other testing tools such as evosuite.
we also plan to adapt our approach to human written tests and investigate their fault detection capability.
688which generated test failures are fault revealing?
esec fse november lake buena vista fl usa