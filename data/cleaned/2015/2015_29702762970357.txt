predicting semantically linkable knowledge in developer online forums via convolutional neural network bowen xu1 deheng y e2 zhenchang xing2 xin xia1y guibin chen2 shanping li1 1college of computer science and technology zhejiang university china 2school of computer science and engineering nanyang technological university singapore max xbw zju.edu.cn ye0014ng e.ntu.edu.sg zcxing ntu.edu.sg xxia zju.edu.cn gbchen ntu.edu.sg shan zju.edu.cn abstract consider a question and its answers in stack over ow as a knowledge unit.
knowledge units often contain semantically relevant knowledge and thus linkable for di erent purposes such as duplicate questions directly linkable for problem solving indirectly linkable for related information.
recognising di erent classes of linkable knowledge would support more targeted information needs when users search or explore the knowledge base.
existing methods focus on binary relatedness i.e.
related or not and are not robust to recognize di erent classes of semantic relatedness when linkable knowledge units share few words in common i.e.
have lexical gap .
in this paper we formulate the problem of predicting semantically linkable knowledge units as a multiclass classi cation problem and solve the problem using deep learning techniques.
to overcome the lexical gap issue we adopt neural language model word embeddings and convolutional neural network cnn to capture wordand document level semantics of knowledge units.
instead of using human engineered classi er features which are hard to design for informal user generated content we exploit large amounts of di erent types of user created knowledge unit links to train the cnn to learn the most informative wordlevel and document level features for the multiclass classication task.
our evaluation shows that our deep learning based approach signi cantly and consistently outperforms traditional methods using traditional word representations and human engineered classi er features.
ccs concepts software and its engineering !software libraries and repositories information systems !social networking sites joint rst authors contributed equally.
corresponding author.keywords link prediction semantic relatedness multiclass classi cation deep learning mining software repositories .
introduction in stack over ow computer programming knowledge has been shared through millions of questions and answers.
we consider a stack over ow question with its entire set of answers as a knowledge unit regarding some programmingspeci c issues.
the knowledge contained in one unit is likely to be related to knowledge in other units.
when asking a question or providing an answer in stack over ow users reference existing questions and answers that contain relevant knowledge by url sharing which is strongly encouraged by stack over ow .
through url sharing a network of linkable knowledge units has been formed over time .
unlike linked pages on wikipedia that follows the underlying knowledge structure questions and answers are speci c to individual s programming issues and url sharing in q as is opportunistic because it is based on the community awareness of the presence of relevant questions and answers.
a recent study by ye et al.
shows that the structure of the knowledge network that url sharing activities create is scale free.
a scale free network follows a power law degree distribution which can be explained using preferential attachment theory i.e.
the rich get richer .
on one hand this means that a small proportion of the knowledge units is attracting a large proportion of users attention.
on the other hand this means that large amounts of knowledge units in the long tail of the power law distribution are mostly under linked.
to mitigate this issue stack over ow recommends related questions when users are viewing a question.
stack over ow s recommendation of related questions is essentially based on lexical similarity of word overlap between questions .
however linkable knowledge units often share few words in common i.e.
lexical gap due to two main reasons.
first users could formulate the same question in many di erent ways.
for example table shows two duplicate questions from stack over ow.
the question id has been recognized as a duplicate to another question id by the stack over ow users because they can be answered by the same answer.
however literally we can hardly see common words and phrases between these two questions.
second two questions could discuss di erent but permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
table an example of duplicate knowledge units from stack over ow question id marked as duplicate to question id title python read a single character from the user title get python program to end by pressing anykey and not enter body is there a way of reading one single character from the body how can i get my python program to end by pressing any user input?
for instance they press one key at the terminal key without pressing enter.
so if the user types c the program and it is returned sort of like getch .
i know there s a should automatically end without pressing enter.
my code so far function in windows for it but i d like something that is print hi everyone!
this is just a quick sample code i made cross platform.
print press anykey to end the program.
table an example of directly linked knowledge units from stack over ow question id question id title iterating over a stack reverse list is there an isempty method?
title best way to check if a list is empty body what s the best way to iterate over a stack in python?
i couldn t nd body for example if passed the following an isempty method and checking the length each time seems wrong somehow.
a how do i check to see if a is empty?
table an example of indirectly linked knowledge units from stack over ow both have an link to question id question id title how to check if a nested list has a value title cost of len function body i have a nested list and i want to check if an item has a value or not.
body what is the cost of len function for python not really sure how to describe it so basically how do i get this to work?
built ins?
list tuple string dictionary relevant knowledge.
for example table shows two directly linkable questions one asks iterating over a stack reverse list while the other asks best way to check if a list is empty .
again the two questions share few common words but they embody strongly relevant knowledge stack iteration and list empty checking .
furthermore questions may discuss relevant knowledge but not directly for solving the questions as the examples in table shows.
as such questions discuss indirectly linkable knowledge they are unlikely to share many common words.
traditional word representations e.g.
bm25 lda cannot reliably recognize many cases of potentially linkable knowledge units when lexical gaps exist between them.
as shown in the above examples knowledge units can be linkable for di erent purposes.
ye et al.
s empirical study on the knowledge network in stack over ow con rms this phenomenon.
being able to classifying di erent classes of linkable knowledge units would support more targeted information needs when users search or explore the linkable knowledge.
for example duplicate questions allow users to understand a problem from di erent aspects directlylinkable questions help explain concepts or sub steps of a complex problem while indirectly linkable questions provide extended knowledge.
however such multiclass semantic relatedness in software engineering knowledge is not considered in existing work which focuses on only binary classi cation i.e.
related or not.
furthermore existing work heavily relies on human engineered word and document level features to train the classi er which are hard to design for informal user generated content in stack over ow.
in this paper we propose a novel approach for predictingmulticlass semantically linkable knowledge units in stack over ow.
inspired by ye et al.
we consider four classes of semantic relatedness duplicate direct linkable indirectly linkable and isolated.
to overcome the lexical gap issue our approach recognizes and quanti es semantic relatedness between knowledge units using word embeddings and convolutional neural network cnn which are the state of the art deep learning techniques for capturing word level and document level semantics for natural language processing nlp tasks.
furthermore our approachdoes not rely on human engineered features to classify semantic relatedness.
instead we collect large amounts of different types of user created knowledge unit links duplicate direct linked indirect linked from stack over ow to train the cnn to automatically learn the most informative wordand document level features to classify semantic relatedness between knowledge units.
we conduct extensive experiments to compare our deeplearning based approach with the baseline methods using traditional word representations tf idf and the humanengineered features for determining semantic similarity.
our results show that our approach signi cantly outperforms the baseline methods and performs much more consistently for di erent classes of semantic relatedness both word embeddings and cnn help improve the performance of multiclass classi cation of linkable knowledge units.
cnn plays a more important role for predicting duplicate directly linkable and isolated knowledge unit due to its capability of capturing document level semantic features while word embeddings are good at predicting indirectly linkable knowledge units domain speci c training corpus help improve the performance of deep learning techniques for speci c tasks.
our contributions are as follows we formulate the research problem of predicting multiclass linkable knowledge units in stack over ow we propose a deep learning based approach to tackle the problem we evaluate the e ectiveness of our approach against the traditional methods for predicting relevant software engineering knowledge .
.
related work many studies have been carried out on stack over ow .
our work predicts semantically linkable knowledge in developers discussions in stack over ow.
it is related to two lines of research link prediction in complex networks and semantic relatedness in software data.
.
link prediction in complex networks 52link prediction focuses on detecting potentially linkable objects in an observed network that is complex and evolves dynamically.
link prediction in complex networks has attracted enormous attention from physics biochemical and computer science communities .
a signi cant proportion of the related research falls into link prediction in social networks many of which have been found to be complex networks e.g.
the user user network in online q a forums in twitter .
these social network research work aims to understand the network evolution patterns and identify potential social relations between users .
ye et al.
report that the knowledge network formed by developers url sharing activities in stack over ow is also a complex network i.e.
the network structure is scale free.
they show that the rich get richer e ect in uences the knowledge sharing activities and the growth of the knowledge network in stack over ow.
as a result a small portion of questions and answers attracts a large portion of developers attention while large amounts of relevant questions and answers in the long tail of the power law distribution are under linked.
this nding motivates our work to predict potentially linkable knowledge units which could enhance the knowledge sharing and search in stack over ow.
unlike link prediction in social network which predicts only whether the two persons are linkable or not i.e.
binary classi cation our work predicts di erent types of relatedness duplicate direct link or indirect link between potentially linkable knowledge units i.e.
multiclass classi cation .
this milticlass link prediction is again inspired by the study of ye et al.
which reveals that users link relevant questions and answers for di erent purposes.
they suggest that mixing di erent types of linkable knowledge units together could hinder the knowledge search as users often need di erent types of relevant information in di erent situations.
therefore we treat the linkable knowledge prediction as a multiclass classi cation problem.
.
semantic relatedness in software data measuring the relatedness or similarity between two pieces of textual contents has long been studied.
the underlying mathematic model of textual contents has evolved from vector space model e.g.
tf idf n gram language model topic modeling e.g.
lda to the recent development of neural language model e.g.
distributed word representations or word embeddings .
the trend is towards semantically richer similarity measures.
in mikolov et al.
propose two neural language models referred to as word2vec in the literature continuous bagof words and continuous skip gram and e cient negative sampling method for learning word embeddings from large amounts of text.
in this work we adopt continuous skipgram model for learning domain speci c word embeddings from stack over ow text.
in the software engineering community researchers utilize the textual similarity between two software artifacts to solve various software engineering tasks such as duplicate or similar bug report detection relevant software tweets detection duplicate online question detection .
for example sun et al.
use a support vector machine svm classi er in and an ir based similarity measure bm25f in respectively to detect duplicate bug reports.
zhang et al.
detect duplicate stack over ow questions by comparing their titles and questiondescriptions using topic model lda .
these existing work are formulated as a binary prediction problem e.g.
duplicate or non duplicate.
in the nlp community the development of cnn architectures for sentence level and document level text processing is under intensive research.
some recent work utilizes cnn to learn the semantic relations between two pieces of texts.
for example kim proposes a simple cnn trained on top of mikolov s word embeddings and then apply the cnn to sentence classi cation.
bogdanova et al.
apply cnn with word embeddings to duplicate question detection in online q a sites.
these cnn based methods outperform traditional nlp methods for sentence classi cation and duplication question detection.
comparing with these existing works on semantic relatedness between two pieces of text we leverage cnn with domain speci c word embeddings for the problem of multiclass classi cation of potentially linkable knowledge units in stack over ow beyond the binary prediction of duplicate content or not.
.
the approach this section formulates our research problem and describes the technical details of our approach.
.
problem formulation we consider a question together with its entire set of answers in stack over ow as a knowledge unit regarding some programming speci c issues.
if two knowledge units are semantically related we consider them as linkable .
we further predict the types of the relatedness between the two knowledge units.
inspired by ye et al.
s study on the purposes of url sharing we de ne four types of the relatedness between the two knowledge units duplicate two knowledge units discuss the same question in di erent ways and can be answered by the same answer.
direct link one knowledge unit can help solve the problem in the other knowledge unit for example by explaining certain concepts providing examples or covering a sub step for solving a complex problem.
indirect link one knowledge unit provides related information but it does not directly answer the question in the other knowledge unit.
isolated the two knowledge units are not semantically related.
we formulate our task as a multiclass classi cation problem.
specially given two knowledge units our task is to predict whether the two knowledge units have one of the above four types of relatedness.
.
overview of main steps in nlp tasks words are usually represented as vectors.
in this work we use word embeddings distributed representations of words in a vector space as word representations because word embeddings require only large amounts of unlabeled text to train and have been shown to be able to capture rich semantic and syntactic features of words.
we use continuous skip gram model the python implementation in gensim to learn domain speci c word embeddings from large amounts of software engineering text from stack over ow questions and answers.
the output is a dictionary of word embeddings for each unique word.
53data extraction preprocessing knowledge unit textsword2vecword embeddingslookupku vector pairs prediction phaselookupknowledge unit knowledge unit 2ku vector ku vector 2trained cnn model duplicate link direct link indirect link isolateduser linked knowledge unit ku pairs duplicate link direct link indirect link isolatedtrainingphaseinput layerhidden layer output layerconvolutional neural network cosine lossfigure overview of the main steps word embeddings encode word level semantics.
to predict semantic relatedness between knowledge units we need to capture semantics at sentence and knowledge unit level.
to this end we resort to convolutional neural network .
given two knowledge units they are rst converted into word vector representations by looking up the word embeddings dictionary which are then fed into a cnn to produce a feature vector for each knowledge unit.
the similarity of the two knowledge units are measured by the cosine similarity of their feature vectors.
based on the similarity score the relatedness of the two knowledge units are classi ed as one of the four classes de ned in section .
.
to train the cnn for the prediction task we collect a set of user linked knowledge unit pairs from stack over ow for each type of relatedness duplicate direct link indirect link and isolated.
this set of training data is fed into the cnn and the training is guided by the cosine loss of the relatedness of the knowledge unit pairs against the user linked relatedness.
during the training process the cnn automatically learns the most informative word and sentence features for predicting multiclass linkable knowledge units.
.
learning word representations distributed word representations assume that words appear in similar context tend to have similar meanings .
therefore individual words are no longer treated as unique symbols but mapped to a dense real valued low dimensional vector space.
each dimension represents a latent semantic or syntactic feature of the word.
semantically similar words are close in the embedding space.
figure shows some examples of domain speci c word embeddings we learn from stack over ow java text visualized in a two dimensional space using the t sne dimensionality reduction technique .
semantically close words such as jpanel jbutton jframe andjlabel which belong to gui component are close in the vector space.
word embeddings are unsupervised word representations which requires only large amounts of unlabeled text to learn.
in this work we collect body content of questions and answers from stack over ow as software engineering text.
we preprocess the body content by removing large code snippets in pre html tag and cleaning html tags.
short code elements in code html tag in natural language sentences are kept.
we use the software speci c tokenizer javaeejunitguibuildfigure example of word embedding developed in to tokenize the sentences.
this tokenizer preserves the integrity of code tokens.
for example it tokenizes dataframe apply as a single token instead of a sequence of tokens dataframe .
apply .
this supports more accurate domain speci c word embeddings learning because code tokens will not be mixed with normal words.
word embeddings are typically learned using neural language models.
in this work we use continuous skip gram model a popular word to vector word2vec model proposed by mikolov et al.
.
as shown in figure continuous skip gram model learns the word embedding of a center word i.e.
wi that is good at predicting the surrounding words in a context window of k words k in this example .
the objective function of skip gram model is to maximize the sum of log probabilities of the surrounding context words conditioned on the center word nx i 1x k j k j6 0logp wi jjwi wherewiandwi jdenote the center word and the context word in a context window of length k .ndenotes the length of the word sequence.
54word vector of kkkkxxinput layerhidden layer conv relu pool feature vector of ffffxx feature vector offfffyyoutput layer word vector ofkkkkyy1 gram gram gram gram gramrelatedness kux kuy ffffxx ffffyy ffffxx ffffyy figure architecture of the proposed cnn the logp wi jjwi is the conditional probability de ned using the softmax function p wi jjwi exp v0t wi jvwi p w2wexp v0twvwi wherevwandv0 ware respectively the input and output vectors of a word win the underlying neural model and w is the vocabulary of all words.
intuitively p wi jjwi estimates the normalized probability of a word wi jappearing in the context of a center word wiover all words in the vocabulary.
mikolov et al.
propose an e cient negative sampling method to compute this probability.
the output of the model is a dictionary of words each of which is associated with a vector representation.
given a knowledge unit its title description and answers content after proper preprocessing will be converted into a knowledge unit vector by looking up the dictionary of word embeddings and concatenating the word embeddings of words comprising the body content.
let wvi2rdbe theddimensional word vector corresponding to the i th word in a knowledge unit.
the knowledge unit of nwords is represented as knv wv1 wv2 wvn which is used as input to the cnn.
.
the cnn architecture figure presents the architecture of our cnn.
our cnn takes as input a knowledge unit vector knv wv1 wv2 wvn and captures the most informative word and sentence features in the knowledge unit for determining semantic relatedness.
treating the knowledge unit vector as an image we perform convolution on it via linear lters.
because each word is represented as a d dimensional vector we use lters with widths kequal to the dimensionality of the word vectors i.e.
k d .
letwvi i h refer to the concatenation vector of hadjacent words wi wi w i h .
a convolution operation involves a lterw2rhk a vector ofh kdimensions and a bias termb2rh which is applied tohwords to produce a new value oi2r oi wt wvi i h b wherei n h and is the dot product between the lter vector and the word vector.
this lter is applied repeatedly to each possible window of hwords in the sentences zero padding where necessary in the knowledge unit i.e.
wv1 h wv2 h wv n h n to produce an output sequence o2rn h i.e.
o .
we apply the nonlinear activation function relu to each oito produce a feature map c2rn h 1whereci relu oi max o i .
to capture features of phrases of di erent length we vary thewindow size hof the lter i.e.
the number of adja cent words considered jointly.
in our cnn we use lters of di erent window size which can capture features of di erent n grams respectively.
for each window size we use lters to learn complementary features from the same word windows.
the dimensionality of the feature map generated by each lter will vary as a function of the knowledge unit length and the lter s window size.
thus a pooling function should be applied to each feature map to induce a xed length vector.
in this work we use 1max pooling which extracts a scalar i.e.
a feature vector of length with the maximum value in the feature map for each lter.
together the outputs from each lter can be concatenated into a top level feature vector denoted as fvku.
this feature vector would capture the most informative and grams in the input knowledge unit.
given two knowledge units kuxandkuy the cnn outputs their respective feature vectors fvxandfvy.
for this pair of feature vectors fvxandfvy the cnn computes a similarity score between fvxandfvyin the last layer using the cosine similarity i.e.
relatedness ku x kuy fvx fvy kfvxkkfv yk in the training phase the computed similarity score will be used to compute the mean square error against the groundtruth similarity score for the given pair of knowledge units see section .
.
in the prediction phase the computed similar score will be used to determine the class of semantic relatedness of the given two knowledge units.
because the computed similarity score is a continuous value we need to bin the similarity score as four discrete classes of semantic relatedness between the two knowledge units as isolated as indirect link as direct link and as duplicate.
.
training the cnn training of the proposed cnn follows supervised learning paradigm.
according to our task objective the training data must consist of su cient examples of four types of related knowledge unit pairs i.e.
duplicate direct link indirect link and isolated so that the cnn can learn to capture informative word and sentence features for classifying knowledge unit relatedness.
fortunately url sharing activities by stack over ow users create all types of the needed relatedness.
user created links between knowledge units are stored in the post links table.
we parse the post links to generate the training dataset as follows.
first we randomly select pairs of user linked knowledge unit pairs.
if the linktypeid is i.e.
duplicate posts we mark the selected pair as an instance of duplicate knowledge units.
otherwise the selected pair is an instance of direct link.
second we randomly select pairs of knowledge units that are only transitively linked i.e.
the shortest path between the knowledge units is at least .
these pairs of knowledge units are instances of indirect link knowledge units.
finally we randomly select pairs of knowledge units that do not have links in the post links table.
these pairs are instances of isolated knowledge units.
we select equal number of instances for di erent types of relatedness.
let the tuple ku x kuy simvalue be a pair of knowledge units in the training dataset td andsimvalue is the ground truth similarity score between kuxandkuy.
in this work we set .
.
.
and .
as the ground55truth similarity score for the four classes isolated indirect link direct link and duplicate respectively.
the cnn with the current parameter set computes a similarity score between the knowledge units i.e.
relatedness ku x kuy .
the training objective is to minimize the mean squared error of the computed similarity score and the ground truth similarity score i.e.
cosine loss with respect to !
jtdjx td simvalue relatedness ku x kuy we add the l2 norm regularization loss to the mean squared error data loss.
the l2 norm constraint penalizes large weight parameters.
it helps avoid model over tting because no input dimension can have a very large in uence on the predicted probabilities all by itself.
the parameters to be estimated include the convolution lters wand the bias termsb see eq.
.
we solve the optimization problem using adam update algorithm .
once we learn the cnn parameters the cnn can be used to predict the semantic relatedness of an unseen pair of knowledge units.
.
experiment we conduct a set of experiments to evaluate the e ectiveness of our approach compare it against a well designed baseline and investigate the impact of domain speci c word embeddings.
as our approach relies on deep learning techniques we also report the training cost of our approach.
.
baseline building as our task is essentially to predict relatedness between two pieces of text we design two baselines each of which is a multiclass svm classi er with di erent textual features i.e.
traditional term frequency tf and inverse document frequency idf versus word embeddings.
.
.
feature extraction with tf and idf tf and idf are widely used in predicting textual similarity with cosine distance .
in this baseline we de ne in total features based on the tf and idf values of the words in a pair of knowledge units.
that is the pair of the knowledge units is represented as a dimensional feature vector to be used as input to the svm classi er.
given a knowledge unit kux we split its text into three sub documents denoted as ci x i which correspond to question title i question body i and question title plus question body plus body of all answers i .
let ku x ku y be a pair of knowledge units.
for tf based features we rst obtain a vocabulary vi j x y with all words in a combination of di erent sub documents of the two knowledge units i.e.
vi j x y fwjw2ci xscj y i j g. the tf value of each word in the vocabulary vi j x yis then computed.
these word tf values are used to convert the sub document ci xof the knowledge unit kux into a vector representation vi x. the dimensionality of the vi xis the size of the vocabulary and each dimension is the tf value of the word in the sub document ci x or otherwise.
we compute the cosine similarity of the two vectors vi xand vj yto measure the similarity of the sub document ci xandcj y of the knowledge unit kuxandkuy respectively.
as each knowledge unit has three sub documents i.e.
i j we obtain tf based features for a pair of knowledge units.for idf based features we rst combine the respective sub documents ci xof all knowledge units in a dataset as a corpusci.
the idf value of a word win the corpus ciis then computed denoted as idfi w .
we measure the idf based similarity between the sub documents ci xof the knowledge unit kuxand the sub document cj yof the knowledge unitkuyas simidf k ci x cj y x w2cixtcj yidfk w i.e.
the sum of the idf value of the common words of ci x andcj yin the corpus ck.
as i j k we obtain idf based features for a pair of knowledge units.
.
.
feature extraction with word embeddings the semantic information of a piece of text can be captured by taking the mean of the word embeddings of the words comprising the text .
following this treatment we compute the mean of the word embeddings of the words in a knowledge unit kux including question title plus question body plus body of all answers as the word embedding of the knowledge unit i.e.
wv ku x nx w2bxwv w wherebxis the bag of words of the kuxandnis the size ofbx.
a pair of knowledge units kuxandkuyis then represented as the mean of the two knowledge unit word embeddings i.e.
wv ku x ku y wv ku x wv ku y which will be used as input to the svm classi er.
in this work we use dimensional word vector.
.
.
multiclass svm classifier we develop a multiclass svm classi er to predict the relatedness of the two knowledge units.
based on the feature vector used tf idf based or word embedding based as described above we have two baselines baseline1 tfidf svm and baseline2 wordembed svm .
the two baseline svm classi ers are trained using the same dataset as that for training our approach.
as our task is a multiclass classi cation problem we use rbf kernel in the svm model.
we set the parameter of the svm to k k denotes the dimensionality of the feature vector i.e.
for the tf idf svm baseline and for the wordembed svm baseline.
we use grid search to optimize the svm parameters.
.
dataset our experimental data is from stack over ow data dump of march .
word embedding corpora.
from the posts table we randomly select knowledge units tagged with java as the word embedding corpora.
note that our corpora contains not only questions but also question answers.
training and test linkable knowledge units.
from thepostlinks table we randomly select in total pairs of knowledge units that are tagged with java i.e.
pairs 56for each type of relatedness de ned in section .
duplicate direct link indirect link and isolated .
these pairs of knowledge units are used as training data.
for test data we select in total pairs of knowledge units that are tagged with java i.e.
pairs for each type of relatedness.
user created links are considered as ground truth label for the semantic relatedness of the selected pairs of knowledge units.
the knowledge units in the test data does not overlap with those in the training data.
.
evaluation metrics we represent the multiclass classi cation results as a k kmatrixa wherekis the number of classes in our work k .
the rows represent the ground truth labels and the columns represent the predicted labels.
the value of the aijis the number of times that a pair of knowledge units with the ground truth label lithat is classi ed as the label lj.
therefore the value of aiiis the number of correct classi cation for the label li p j kaijis the number of the ground truth label liin a dataset p i kaijis the number of predicted label ljin a dataset andp ip jaij is the number of knowledge unit pairs in a dataset.
precision recall and f1 scores as the evaluation metrics are standard and widely used to evaluate the e ectiveness of a prediction technique .
thus we use them to compare our approach with the two baselines accuracy is de ned as the proportion of numbers of correct classi cation in a dataset i.e.
accuracy p iaiip ip jaij precision for a class jis the proportion of knowledgeunit pairs correctly classi ed as the class jamong all pairs classi ed as the class j. precision for all classes is the mean of the precision for each class.
precision j ajjp i kaij recall for a class iis the percentage of knowledge unit pairs correctly classi ed as the class icompared with the number of ground truth label liin the dataset.
recall for all classes is the mean of the recall for each class.
recall i aiip j kaij f1 score for a classiis a harmonic mean of precision and recall for that class.
f1 score for all classes is the mean of the f1 score for each class.
f1i precision i recall i precision i recall i f1 score evaluates if an increase in precision or recall outweighs a loss in recall or precision .
as there is often a trade o between precision and recall f1 score is usually used as the main evaluation metric in many software engineering papers .
in this paper we also choose f1 score as the main evaluation metric.
.
research questions and findings in our experiment we are interested in the following two research questions table accuracy comparison accuracy baseline1 tf idf svm .
baseline2 word embedding svm .
our approach word embedding cnn .
.
.
overall comparison rq1 how much improvement can our approach achieve over the baseline approaches?
motivation.
our approach uses deep learning techniques word embeddings and cnn derived features to quantify semantic relatedness between two pieces of software engineering text which is very di erent from the baseline approaches using traditional tf idf word representations and human engineered features.
answer to this research question will shed light on whether and to what extent deep learning techniques can improve the results of the multiclass classi cation task on software engineering text.
approach.
we apply our approach and the baseline approaches to the test data i.e.
pairs of linkable knowledge units.
we compare the accuracy precision recall and f1 score metrics of di erent approaches.
result.
table and table present the results.
we can see that the accuracy of our approach outperforms the baseline1 and baseline2 by .
and .
respectively.
overall see the last column of table our approach outperforms thebaseline1 and baseline2 in terms of f1 score by .
and .
respectively.
similar improvement on overall precision and recall can be observed.
overall our approach achieves the best performance in all the evaluated metrics by a substantial margin compared with the two baseline approaches.
.
.
comparison by different classes rq2 how e ective is our approach in predicting different classes of semantic relatedness compared with the baseline approaches?
motivation.
di erent from existing work on binary text classi cation our approach is for multiclass text classi cation.
di erent classes of semantic relatedness may exhibit di erent word and sentence features which may a ect the e ectiveness of di erent semantic similarity measures.
we would like to investigate the advantages and disadvantages of our approach and the baseline approaches for predicting di erent classes of semantic relatedness.
approach.
we compare the precision recall and f1 score for each class of relatedness i.e.
duplicate direct link indirect link and isolated by our approach and the two baseline approaches see table .
result.
the performance of our approach and the baseline approaches do vary for di erent classes of relatedness.
our approach performs the best for three of the four classes i.e.
duplicate direct link and isolated on all the evaluation metrics while the baseline2 performs the best for the indirect link class only on recall and f1 score.
in fact the baseline2 achieves the extremely high recall .
but low precision lower than our approach for the indirect link class.
the performance variance of our approach for di erent classes is small .
.
in terms of f1 score compared with the performance variance of the two baseline methods for di erent classes in terms of f1 score .
.
for the baseline1 and .
.
for the baseline2 .
57table precision recall and f1 score of our approach and the baseline approaches duplicatedirect linkindirect linkisolated overall precisionbaseline1 tf idf svm .
.
.
.
.
baseline2 word embedding svm .
.
.
.
.
our approach word embedding cnn .
.
.
.
.
recallbaseline1 tf idf svm .
.
.
.
.
baseline2 word embedding svm .
.
.
.
.
our approach word embedding cnn .
.
.
.
.
f1 scorebaseline1 tf idf svm .
.
.
.
.
baseline2 word embedding svm .
.
.
.
.
our approach word embedding cnn .
.
.
.
.
our approach performs better on more classes of relatedness than the baseline methods .
our approach also performs more consistently for di erent classes of relatedness.
the baseline2 is extremely good at recalling indirectly linkable knowledge units with a sacri ce of precision but its performance varies greatly for di erent classes of relatedness.
.
.
effects of word embeddings and cnn rq3 what is the impact of word embeddings and cnn on the performance improvement respectively?
motivation.
word embeddings and cnn are two deep learning techniques our approach relies on.
they help capture semantics at word level and sentence document level respectively.
answer to this research question helps us understand the importance of di erent levels of semantics for the multiclass text classi cation task.
approach.
to understand the importance of word embedding we compare the performance of the baseline1 i.e.
tfidf svm and the baseline2 i.e.
wordembedding svm .
to understand the importance of cnn we compare the performance of the baseline2 i.e.
wordembedding svm and our approach i.e.
wordembedding cnn .
result.
overall the baseline2 outperforms the baseline1 by a small margin see accuracy in table and the overall column in table .
this suggests that word embeddings can moderately improve the text classi cation performance compared with the traditional tf idf word representation.
overall our approach outperforms the baseline2 by a substantial margin.
this suggests that cnn plays a more important role than word embeddings for improving the multiclass classi cation performance.
the performance of the baseline2 is not always better than that of the baseline1 for di erence classes of relatedness.
for some classes the baseline2 has better precision but worse recall than the baseline1 and vice versa for other classes.
in terms of f1 score the two baselines are almost the same for duplicate and direct link classes while the baseline2 is signi cantly better than the baseline1 for indirect link class but signi cantly worse for isolated classes.
this suggests that word level semantics encoded in word embeddings may not be appropriate for determining all classes of relatedness.
for isolated knowledge units traditional tf idf representation which is sensitive to lexical gap performs better than word embeddings.
taking the mean of word embeddings as knowledge unit representations may blur the semantic distinction between the knowledge units which helps the baseline2 recall indirectly linkable knowledge units but degrades the performance of the baseline2 for isolated knowledge units.
in contrast our approach consistently outperforms the baseline2 on all the evaluation metrics for di erent classesof relatedness except the recall and f1 score for indirectly linkable knowledge units.
this suggests that word level semantics are especially useful for determining semantic similarity of indirectly linkable knowledge units.
as indirectly linkable knowledge units may not exhibit semantic similarity at sentence document level considering sentence documentlevel semantics by the cnn could rule out false negative links which degrades its recall.
both word embeddings and cnn help improve the performance of multiclass text classi cation but cnn has a bigger impact than word embeddings.
word level semantics are especially useful for predicting indirectly linkable knowledge units while sentence document level semantics plays more signi cant role for predicting duplicate directly linkable and isolated knowledge units.
.
.
impact of domain specific word embeddings rq4 how sensitive is our approach to word embeddings learned from di erent corpus?
motivation.
in this work we predict linkable knowledge in developers online forum.
therefore we learn word embeddings from stack over ow text which is representative of the ways people ask answer questions and the vocabulary people use.
we would like to investigate whether and to what extent word embeddings learned from a di erent corpora a ect the performance of our approach.
this will help us understand the importance of suitable corpus for a software speci c machine learning task.
approach.
we collect a corpus of wikipedia web pages from the wikipedia data dump2.
the number of sentences and the size of the vocabulary of the wikepedia corpus is comparable to that of the corpus of the knowledge units from stack over ow.
we learn word embeddings from the wikipedia corpus and use it to train the cnn subsequently.
we compare the performance of the two cnns one trained with stack over ow word embeddings and the other trained with wikipedia word embeddings.
table performance of our approach with word embeddings learned from di erent corpus accuracyoverall precisionoverall recalloverall f1 score general corpus from wikipedia0.
.
.
.
domain speci c corpus from stack over ow0.
.
.
.
result.
table presents the accuracy and the overall precision recall and f1 score of our approach with domainspeci c word embeddings i.e.
stack over ow versus general word embeddings i.e.
wikipedia .
general word embeddings degrade the performance of our approach com2 58pared with domain speci c word embeddings.
however the degrade is moderate in terms of accuracy precision recall and f1 score by and respectively.
furthermore although the performance of our approach degrades with general word embeddings it still outperforms the two baseline methods at least on all the evaluation metrics .
our approach demonstrates certain level of reliability even with word embeddings learned from a general corpus that is completely di erent from stack over ow discussions.
however suitable domain speci c word embeddings leads to better performance.
.
.
training cost rq5 what is the time cost for training the underlying deep learning models?
motivation.
the underlying word embeddings and cnn models need to be trained before they can be used for prediction task.
training of word embeddings and the cnn model is done only once o ine.
after model training using the model to predict the relatedness of a pair of knowledge units takes negligible time.
understanding the training time cost helps us understand the practicality of our approach.
approach.
we record the start time and the end time of the program execution to obtain the training time cost of the word embeddings and the cnn model.
the experimental environment is an intel r core tm i7 .
ghz pc with 16gb ram running windows7 os bit .
result.
for learning word embeddings model the knowledge units contain words as bags of words and the vocabulary size i.e.
the number of unique words is .
it takes about minutes to analyze the text of these knowledge units to learn the word embeddings.
for training the cnn model it takes about hours for the cnn model to achieve the loss convergence e .
training of the word embeddings model and the cnn model can be done e ciently o ine and only need to be done once.
our approach is practical for large dataset.
.
discussion this section presents the qualitative analysis of some examples to illustrate the capability of our approach and discusses threads to validity of our experiments.
.
qualitative analysis table presents one example for each class of linkable knowledge unit and the classi cation results by our approach and the two baselines.
the rst example is a pair of duplicate questions.
however the two questions do not have many words in common.
the two baseline methods classify them as indirectly linkable knowledge units.
in contrast our approach can capture semantic similarity between terms like standard input output and system.out.println .
this helps our approach classify the two questions as duplicate.
in the second example the two knowledge units contain directly relevant knowledge i.e.
the gui repaint problem is directly relevant to text not displaying correctly problem.
unfortunately the two knowledge units share few words in common which makes the baseline methods classify them as isolated.
our approach can capture the semantic 3note that we use software speci c tokenizer which can preserve the integrity of code tokens.relatedness between the two technical problems and thus correctly classify the two knowledge units as direct link.
in the third example the two indirectly linked questions share some common words.
based on these overlap words the baseline1 which essentially relies on lexical similarity of overlapping words classi es the two questions as duplicate.
the baseline2 which considers word level semantics by word embeddings makes a better judgment classifying them as isolated.
our approach makes the most accurate prediction by taking into account not only word level but also document level semantics.
similarly in the fourth example the baseline1 makes the least appropriate prediction based on overlapping words the baseline2 makes a better judgment based on word level semantics and our approach makes the most accurate prediction.
.
error analysis we also analyze the cases in which our approach makes the wrong prediction.
we nd that many cases that fail our approach involve knowledge units whose essential information is presented as a code snippet4or an image5.
in this work we remove code snippets and images during preprocessing.
in the future we could incorporate code snippets and images into our approach.
incorporating image semantics into our approach would be relatively easy because cnn is originally invented for image classi cation.
incorporating code semantics into our approach could be a challenging task.
a recent work by mou et al.
proposes to use cnn to encode the program asts for program analysis tasks.
however their approach is not directly applicable to code snippets in q a discussion which is usually incomplete and cannot be complied.
we plan to tackle this challenge as our future work.
.
threats to validity there are several threats that may potentially a ect the validity of our experiments.
threats to internal validity relate to errors in our experimental data and tool implementation.
we have double checked our experimental data and tool implementation.
we have also manually checked the selected knowledge units in our dataset to ensure that they are really tagged with java and have the right types of knowledge units links.
threats to external validity relate to the generalizability of our results.
in this study we use a medium size training and test dataset knowledge units for word embedding learning pairs of knowledge units for cnn training and pairs of knowledge units for testing .
this allows us to perform some manual analysis to understand the capability and limitations of our approach.
in the future we will reduce this threat by extending our approach to larger word embeddings corpus and more knowledge unit pairs for training and testing.
.
conclusion and future work in this paper we propose a novel deep learning based approach for predicting multiclass semantically linkable knowledge units in stack over ow.
our approach can predict four types of semantic relatedness duplicate link direct link indirect link and isolated .
at word level our approach adopts 4for example ow.com questions simpledateformat bug 5for example ow.com questions android sdk installation doesnt nd jdk 59table examples of di erent classes of semantically linkable knowledge units knowledge unit1 knowledge unit2 prediction duplicatequestion id question id our approach duplicate link baseline1 indirect link baseline2 indirect linktitle java junit capture the standard input output for use in a unit testtitle in junit testing is it possible to check the method system.out.println body i m writing integration tests using junit to automate the testing of a console based application.
... i only need to have the test execute and verify the the output is what is expected given the input.
... body is it possible to check through junit testing if the method system.out.println one two actually prints one two?
direct linkquestion id question id our approach direct link baseline1 isolated baseline2 isolatedtitle text in label notdisplaying correctly withsettext methodtitle java gui repaint problem?
body i m trying to set the text in a label dynamically by calling the settext method whenever a button is clicked.
... that are passed to the settext method aren t visible on the screen when the submit button is clicked until i click on the window and drag to resize it.
... on a pc the strings are are visible but obscured until i resize thewindow .
... on the mac the strings appear as intended however they re obscured until the window is resized.
what am i missing?body i have a jframe.
this jframe contains a jbutton .
i click the jbuttonand 10jtextfields are created.
the problem i cannot see them until i force a repaint by resizing the window .
only then do i see the jtextfields created.
code ... answer calling validate on the frame as mentioned here link to question id solved the problem.answer container.add api docs sayeth note if a component has been added to a container that has been displayed validate must be called on that container todisplay the new component .
if multiple components are being added you can improve e ciency by calling validate only once.
... indirect linkquestion id question id our approach indirect link baseline1 duplicate link baseline2 isolatedtitle best way to build a plugin system with javatitle plugin base pattern for web app body how would you implement aplugin system for your java application?
is it possible to have an easy to use for the developer system which achieves the following users put their plugins into a subdirectory of the app.
theplugin can provide a con guration screen if you use a framework is the license compatible with commercial developement?body i would like in a web application to be able to add some functionnality.
is there any pattern for a web application to be able to add plugin ?
every plugins could be a jar le and the user could select the plugin it want to use.
does somebody have information about this kind of action?
isolatedquestion id question id our approach isolated baseline1 direct link baseline2 indirect linktitle hibernate how to store java.net.url into a database through hibernatetitle does hibernate entitymanager include core?
body i have a eld url countryurl in a country class.
i want to store its data into a country table in a database through hibernate.which hibernate type i should use in the hibernate mapping le ...code... it is not excepting string and text type.body i m using hibernate s implementation of jpa.
... my question is does hibernate entitymanager depend on or use the hibernate core code?
... i want to know if entitymanager uses core and thus has this x. the state of the art distributed word representations i.e.
word embedding to encode word semantics in dense lowdimensional real valued vectors.
at document level we train a cnn to automatically learn the most informative word and sentence features for classifying the semantic relatedness between two knowledge units.
the training of the cnn is guided by su cient examples of di erent types of semantically linkable knowledge units.
due to the adoption of word level and document level semantics our approach is robust to the lexical gap i.e.
sharing few words in common between linkable knowledge units.
our experiments con rm the e ectiveness and consistency of our approach for predicting multiclass semantically linkable knowl edge units compared with the well designed baselines using tf idf word representations and human crafted word and sentence document features.
in the future we will enhance our approach by incorporating image and code snippet semantics into our framework.
we will also develop automated tool to help developers search and explore di erent types of linkable knowledge in stack over ow.