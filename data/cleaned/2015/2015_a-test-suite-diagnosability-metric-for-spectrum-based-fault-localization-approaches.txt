a test suite diagnosability metric for spectrum based fault localization approaches alexandre perez rui abreu arie van deursen university of porto haslab inesc tec portugal palo alto research center usa delft university of technology the netherlands alexandre.perez fe.up.pt rui computer.org arie.vandeursen tudelft.nl abstract current metrics for assessing the adequacy of a testsuite plainly focus on the number of components be it lines branches paths covered by the suite but do not explicitly check how the tests actually exercise these components and whether they provide enough information so that spectrum based fault localization techniques can perform accurate fault isolation.
we propose a metric called ddu aimed at complementing adequacy measurements by quantifying a test suite s diagnosability i.e.
the effectiveness of applying spectrum based fault localization to pinpoint faults in the code in the event of test failures.
our aim is to increase the value generated by creating thorough test suites so they are not only regarded as error detection mechanisms but also as effective diagnostic aids that help widely used faultlocalization techniques to accurately pinpoint the location of bugs in the system.
our experiments show that optimizing a test suite with respect to ddu yields a gain in spectrum based fault localization report accuracy when compared to the standard branch coverage metric.
keywords testing coverage diagnosability.
i. i ntroduction this paper proposes ddu a new metric for evaluating the diagnosability of a test suite when applying spectrum based fault localization approaches.
aimed at complementing adequacy measurements that focus on maximizing error detection of a suite ddu provides an assessment on its efficiency at pinpointing the root cause of failure given that an error is detected.
the proposed measurement increases the value of having a thorough test suite since an optimal suite with respect to ddu can not only act as an error detection tool but also as an aid to widely used fault localization approaches.
current test quality metrics quantitatively describe how close a test suite is to thoroughly exercising a system according to an adequacy criterion.
such criteria describe what characteristics of a program must be exercised.
examples of current metrics include branch and path coverage modified decision condition coverage and mutation coverage .
according to zhu et al.
such measurements can act as generators meaning that they provide an intuition on what components to exercise to improve the suite .
however this generator property does not provide any relevant actionable information on how to test those components.
these adequacy measurements abstract away the execution information of single test executions to favor an overall assessment of the suite and are therefore oblivious to anti patterns like the ice cream cone.1the anti pattern states that the vast majority of tests is written at the system level with very few tests written at the unit granularity level.
even though high coverage testsuites can detect errors in the system it is not guaranteed that inspecting tests will yield a straightforward explanation for the cause of the observed failures since fault isolation is not a primary concern.
our hypothesis is that a complementing metric that takes into account per test execution information can provide further insight about the overall quality of a testsuite.
this way if a regression happens we would have a test suite that is not only effective at detecting faults but also aids spectrum based techniques to pinpoint them among the code.
previous test suite diagnosability research has proposed measurements to assess diagnostic efficiency of spectrumbased fault localization techniques.
one measurement uses the density of a test coverage matrix also known as spectrum input to all spectrum based fault localization techniques which encodes what software components have been involved in each test.
gonz alez sanchez et al.
have shown that when spectrum density approaches the optimal values the effectiveness of spectrum based approaches is maximal .
another approach is one by baudry et al.
that proposed a test for diagnosis criterion that attempts to reduce the size of dynamic basic blocks to improve fault localization accuracy .
unfortunately the existing diagnosability metrics rely on impractical assumptions that are unlikely to happen in the real world.
the approach by baudry et al.
focuses on detection of single faults in the system.
the density approach assumes that all tests programmers write exercise a different path through the code and therefore produce different coverage patterns.
in practice it is common for tests to cover the same code.
if one does not account for test diversity it is possible to skew the test coverage matrix to have a supposedly optimal density by repeating similar test cases.
it also has the assumption that all tests cover on average the same number of code components.
in reality a test suite can encompass tests ranging from a targeted narrow unit test to a sweeping system test.
this paper details the optimal coverage matrix for achieving accurate spectrum based fault localization.
in this scenario 1ice cream cone software testing anti pattern mentioned in alister scott s blog accessed february .
ieee acm 39th international conference on software engineering ieee acm 39th international conference on software engineering .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the test suite contains a test case exercising every possible combination of components in the system so that not only single faults can be pinpointed but also allows for multiplefaults which require simultaneous activations of components for the fault to manifest can be isolated.
such a matrix is reached when its entropy is maximal.
this is the theoretically optimal scenario.
however this entropy maximization approach is intractable due to the sheer number of test cases required to exercise every combination of components in any real world system.
nevertheless the entropy optimal scenario helps elicit a set of properties coverage matrices need to exhibit for accurate spectrum based fault localization.
we leverage these properties in our proposed metric coined ddu.2this metric addresses the related work assumptions detailed above while still ensuring tractability by combining into a single measurement the three key properties spectra ought to have for practical and efficient diagnosability a density ensuring components are frequently involved in tests b test diversity g ensuring components are tested in diverse combinations and c uniqueness u favoring spectra with less ambiguity among components and providing a notion of component distinguishability.
the metric addresses the quality of information gained from the test suite should a program require fault localization activities and is intended as a complement to adequacy measurements such as branch coverage.
to measure the effectiveness of the proposed metric we perform two empirical evaluations of ddu by generating test suites for faulty software projects.
test generation facilitated by the e vosuite tool is guided to optimize test suites regarding a specific metric and oracles are generated from correct project versions.
the first empirical evaluation shows that generating tests that optimize ddu produces test suites that require less diagnostic effort to find the faults compared to density.
the second empirical evaluation generates test suites for a wide range of subjects in the d efects 4j collection.
it shows that optimizing a suite regarding ddu yields an increase of in diagnostic accuracy when compared to testsuites that only consider branch coverage as the optimization criterion.
this paper s contributions are a description of the theoretically optimal test suite for fault localization one that generates a coverage matrix with maximal entropy.
this optimal scenario is intractable due to the sheer number of test cases needed to be generated.
we elicit from the optimal scenario three key properties matrices ought to exhibit to preserve high diagnostic accuracy density diversity and uniqueness.
ddu a new metric based on the aforementioned properties to assess a test suite s diagnostic ability to pinpoint a fault in the system using spectrum based techniques.
the metric complements adequacy measurements such as branch coverage.
2ddu is an acronym for density diversity uniqueness.t1t2t3t4 method grounddistance circle circle circle if underwater circle circle circle return surfacedistance circle circle else circle return groundaltitude circle method groundaltitude circle circle circle if landed circle circle circle return circle circle else circle return sub gnd alt circle method sub a b circle retur na b circle pass fail status a per test coverage of a single faulted system.
t1t2t3t4 method descend increment circle circle circle if landed circle circle circle return status.stopped circle circle else circle descendmeters increment circle return status.descending circle method ascend increment circle circle if landed circle circle liftoff circle return status.liftoff circle else circle ascendfeet increment circle return status.ascending circle pass fail status b per test coverage of a multiple faulted system.
fig.
code snippets showing test and coverage information.
test passes and failures are represented by and .
circleindicates that the component in the respective row was exercised.
empirical evidence that ddu is more accurate at assessing diagnostic ability than the state of the art.
empirical evidence that optimizing a test suite with respect to ddu yields a gain in diagnostic efficiency when compared to similarly adequate suites.
ii.
m otiv ation we present two code snippets along with runtime information of several test cases as a motivational example demonstrating the need for a new metric that accurately describes the diagnostic ability of a test suite.
the first example depicted in figure 1a shows a snippet of code from a sensor array capable of measuring distance to the ground both when submerged and airborne.
the purpose ofgroundaltitude is to measure distance to the ground using the internal altitude sensor alt and the ground elevation sensor gnd .
this method has a bug it will produce negative values if alt is greater than gnd.
line should then read return sub alt gnd .
testt1does indeed detect the error in the system.
but the problem is that no other test also exercises the branches followed by t1to exonerate them from suspicion.
this results in the developer having to 3we use line of code as the component granularity throughout the motivation section.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
manually inspect all components that do not appear in passing tests.
six lines out of a total of will have to be inspected corresponding to nearly of the total code in the snippet.
in this small example it is feasible to inspect all components but component inspection slices can grow to fairly large numbers in a real world scenario.
so even though this test suite has branch coverage it does not provide many diagnostic clues.
the second example depicted in figure 1b contains a snippet of code for controlling the ascent and descent of a drone.
the descend method uses meters to quantify the amount of descent while the ascend method uses feet.
assuming there is no explicit check for altitude available testing these methods independently will not reveal the failure.
in fact only a test that covers both methods else branches may reveal it if for instance there is an unexpected liftoff after a descent.
even though we have reached branch coverage this test suite has not managed to expose the fault in the code.
also note that even satisfying a stronger coverage criterion like the modified condition decision coverage or even a stronger intra procedural analysis will not expose the fault.
to expose the fault in this example one would need to exercise combinations of decisions from different methods.
iii.
b ackground this section describes the background work on which the metric proposed on this paper is inspired.
namely we cover the concept of spectrum based reasoning sr which is amongst the best performing spectrum based fault localization approaches and detail previous attempts to define a diagnosability metric.
a. spectrum based reasoning sr sr reasons about observed system executions and their outcomes to derive diagnoses that can explain faulty behavior in software .
in sr the following is given a finite set c angbracketleftc1 c2 ... c m angbracketrightofmsystem components.
components can be any source code artifact of arbitrary granularity such as a class a method a statement or a branch a finite set t angbracketleftt1 t2 ... t n angbracketrightofnsystem transactions which can be seen as records of a system execution such as e.g.
test cases the outcome of system transactions is encoded in the error vector e angbracketlefte1 e2 ... e n angbracketright where ei if transaction tihas failed and ei otherwise an mactivity matrix a whereaijencodes the involvement of component cjin transaction ti.
the pair a e is commonly referred to as spectrum .
several types of spectra exist.
the most commonly used is called hit spectrum where the activity matrix is encoded in terms of binary hit and not hit flags i.e.
aij if cjis involved in tiandaij otherwise.
prior approaches using spectra were based on a so called similarity coefficient to find a correlation between a component cj s activity i.e.
angbracketleftaij i ..n angbracketright and the observed transactionoutcomes encoded in error vector e .
sr relies instead on a reasoning approach that leverages a bayesian reasoning framework to diagnose the system.
sr was also shown to outperform similarity based approaches .
the two main steps of sr are candidate generation and candidate ranking candidate generation the first step in sr is to generate a setd angbracketleftd1 d2 ... d k angbracketrightof diagnosis candidates.
each diagnosis candidate dkis a subset of c anddkis said to be valid if every failed transaction involved at least one component fromdk.
a candidate dkisminimal if no valid candidate d primeis contained in dk.
we are only interested in minimal candidates as they can subsume others of higher cardinality.
heuristic approaches to finding these minimal candidates which is an instance of the minimal hitting set problem thus np hard include s taccato s afari and m hs2 .
candidate ranking for each candidate dk their fault probability is calculated using the na ve bayes rule pr dk a e pr dk productdisplay i ..npr ai ei dk pr ai pr dk estimates the probability that a candidate without further evidence is responsible for the observed erroneous behavior.
typically candidates of higher cardinality have a lower prior probability of being faulty since conditional independence is assumed throughout the process .
the denominator pr ai is a normalizing term that is identical for all candidates.
lastly pr ai ei dk is used to bias the prior probability taking observations from the program spectrum into account.
one approach to computing this term is to use maximum likelihood estimation mle to maximize the probability that each component involved in a transaction ibehaves nominally.
further details on the inner workings of the candidate ranking step are detailed in .
b. measuring quality of diagnosis to measure the accuracy of fault localization approaches the cost of diagnosis cdmetric is often used .
it measures the number of candidates that need to be inspected until the real faulty candidate is reached given that the candidates are being inspected by descending order of probability.4a value of 0forcdindicates an ideal diagnostic report where the faulty candidate is at the top of the ranking and thus no spurious code inspections will occur.
another common metric is wasted effort or merely effort that normalizes cdover the total number of components in the system so that the metric ranges from optimal value no developer time wasted chasing wrong leads to worst value states that the whole system will be inspected until the fault is reached in all cases.
effort measurements assume perfect fault understanding meaning that when the real faulty candidate is inspected it is correctly identified as such.
this assumption may not always hold but there are approaches to mitigate it e.g.
.
4or likelihood score depending on the fault localization approach used.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
c. diagnosability assessment by measuring matrix density previous work has used matrix density as a measure for diagnosability summationtext i jaij n m the intuition is to find an optimal matrix density such that every transaction observed reduces the entropy of the diagnostic report set r angbracketleftpr dk a e dk d angbracketright .
it has been previously demonstrated that the information gain can be modeled as ig tg pr eg log2 pr eg pr eg log2 pr eg where pr eg is the probability of observing an error in transaction tg conversely pr eg is the probability of observing nominal behavior.
optimal information gain ig tg is achieved when pr eg pr eg .
.
with the assumption that transaction activity is normally distributed then it follows that a transaction s average component activation rate equals the overall matrix density.
thus it can be said that pr eg yielding .5as the ideal value for diagnosis using sr approaches .
density was also leveraged by campos et al.
to guide automated test generation .
this work shows that density guided testsuites managed to reduce diagnostic effort when compared to using branch coverage as the fitness function for the generation.
d. diagnosability assessment by measuring uniqueness baudry et al.
propose a diagnosability metric that tracks the number of dynamic basic blocks in a system .
dynamic basic blocks which other authors also call ambiguity groups correspond to sets of components that exhibit the same involvement pattern across the entire test suite.
for diagnosing a system the more ambiguity groups there are the less accurate the diagnostic report can be because one cannot distinguish among components in a given ambiguity group as they all show the same involvement pattern across every transaction.
this metric that we call uniqueness can be used to ensure that the test suite is able to break as many ambiguity groups as possible.
a matrix adecomposes the system into a partition g g1 g2 ... g lof subsets of all components with identical columns in a. then measuring the uniqueness uof a system can be done by u g m whenu mall components belong to the same ambiguity group.
when u all components can be uniquely identified.
iv .
d iagnosability metric this section presents the ddu metric.
first we detail a method for quantifying the exhaustiveness of a test suite using the notion of entropy motivated by the optimal diagnosability scenario.
although we use sr in our motivation the entropy approach can be applied to other spectrum based fault localization strategies as well because it focuses on isolating diagnostic candidates.
we show that entropy may not be suitable in practice due to the number of transactions needed to reach an ideal spectrum.
finally we propose the ddu metric as a relaxed alternative based on previous work that uses density as an indicator for diagnosability.
a. activity matrix entropy to maximize the effectiveness of sr approaches the ideal activity matrix is one that contains every combination of component activations since it follows that every possible fault candidate in the system is exercised.
a metric that accurately captures this exhaustiveness is entropy the measure of uncertainty in a random variable.
shannon entropy is given by h x summationdisplay ip xi log2 p xi in this context xis the set of unique transaction activities in the spectrum matrix.
p xi is the probability of selecting a transaction t t and it having the same activity pattern asxi.
whenh x is maximal it means that all possible transactions are present in the spectrum.
for a system with m components maximum entropy is mshannons i.e.
number of bits required to represent the test suite .
therefore we can normalize it to h x m. matrices with a normalized entropy of1.0would then be able to efficiently diagnose any fault single or multiple provided that the error detection oracles that classify transactions as faulty are sufficiently accurate.
the main downside of using entropy as a measure of diagnosability is that one would need 2m 1tests to achieve this ideal spectrum and thus a normalized entropy of .
.
in practice some transaction activities are impossible to be generated either due to the system s topology or due to the existence of ambiguity groups a set of components that always exhibit the same activity pattern.
b. ddu our ddu is detailed next.
its goal is to capture several structural properties of the activity matrix that make it ideal for diagnosing while avoiding the combinatorial explosion of the optimal entropy approach.
we start by considering activity matrix density as the basis for our approach and then propose the diversity and uniqueness enhancements so that the impractical assumptions of the base approach can be lifted.
density the metric captures the density of a system.
its ideal value for minimizing the diagnostic report entropy is .
as shown in the work of gonz alez sanchez et al.
.
it is also straightforward to show the optimality of the value of .5for the density measurement by induction.
suppose that we have an activity matrix a1 which is optimal for diagnosis.
suppose also that we want to add a new component c primeto our system.
to preserve optimality we would need to repeat the 5an example of an ambiguity group is the set of statements in a basic block.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
optimal sub matrix a1both when c primeis active and when it is inactive.
therefore c primeinvolvement rate will be .
.
since .5is our optimal target value we propose a normalized metric primewhere its upper bound .
is the actual target prime and the lower bound 0means that every cell in the matrix contains the same value.
however this optimal target is only valid assuming that all transactions in the activity matrix are distinct .
such assumption is not encoded in the metric itself see equation .
this means that a matrix with no diversity depicted in the example from figure 2a is able to reach the ideal value for the primemetric.
c1c2c3c4 t1 t2 t3 t4 a no test diversity.
prime .0g .0c1c2c3c4 t1 t2 t3 t4 b test diversity.
prime .0g .
fig.
impact of diversity on primeandg.
diversity the first enhancement we propose to the prime analysis is to encode a check for test diversity.
in a diagnostic sense the advantage of having considerable variety in the recorded transactions is related to the fact that each diagnostic candidate s posterior probabilities of being faulty are updated with each observed transaction.
if a given transaction is failing it means that the diagnostic candidates whose components are active in that transaction are further indicted as being faulty so their fault probability will increase.
conversely if the transaction is passing then it means that the candidates that are active in the transaction will be further exonerated from being faulty and their fault probability will decrease.
having such diversity means that more diagnostic candidates will have their fault probabilities updated so that they are consistent with the observations leading to a more accurate representation of the state of the system.
we use the gini simpson index to measure diversity g .
the gmetric computes the probability of two elements selected at random being of different kinds g summationtextn n n n wherenis the number of tests that share the same activity.
wheng every test has a different activity pattern.
when g all tests have equal activity.
figures 2a and 2b depict examples of repeated and diverse test cases respectively.
we can see that the primemetric by itself cannot distinguish between the two matrices as they have the same density.
if we also account for diversity the two matrices can be distinguished.
uniqueness the second extension we propose has to do with checking for ambiguity in component activity patterns.
if two or more components are ambiguous like components c1 andc2from the example in figure 3a then they form anc1c2c3c4 t1 t2 t3 t4 a component ambiguity.
prime .0g .
u .75c1c2c3c4 t1 t2 t3 t4 b no component ambiguity.
prime .0g .
u .
fig.
impact of component ambiguity on prime gandu.
ambiguity group see section iii d and it is impossible to distinguish between these components to provide a minimal diagnosis if tests t1andt3fail.
as finding potential diagnostic candidates can be reduced to a set cover minimal hitting set problem then two things may happen as a result of breaking an ambiguity group and having those components being tested independently.
one is that some diagnostic candidates containing components from that ambiguity group can become inconsistent with the observations and thus would be removed from the set of possible diagnostic candidates improving the tractability of the bayesian update step of the sr approach.
the other is that diagnostic candidates will be of lower cardinality thus improving our confidence in the accuracy of diagnosis.
this happens because as faults are considered to be independent then the probability of having multiple faults as the explanation for the system s behavior is generally several orders of magnitude lower when compared to low cardinality candidates.
we use a check for uniqueness u as described in equation to quantify ambiguity.
uniqueness is also used by baudry et al.
to measure diagnosability .
however we argue that uniqueness alone does not provide sufficient insight into the suite s diagnostic ability.
particularly it does not guarantee that component activations are combined in different ways to further exonerate or indict multiple fault candidates.
in that aspect information regarding the diversity of a suite provides further insight.
combining diagnostic predictors our last step is to provide a relaxed version of entropy which we call ddu by combining the three aforementioned metrics that assess the key properties i.e.
necessary and sufficient a coverage matrix ought to have to ensure proper diagnosability ddu prime g u and its ideal value is .
.
we reduce prime ganduinto a single value by means of multiplication.
the reason being that since in each term the value of .0corresponds to the worst case and1.0to the ideal case we are able to leverage properties of multiplication such as multiplicative identity and the zero property.
v. e mpirical ev aluation to evaluate the proposed metric with regard to its ability to diagnose faults we aim to address the following research 6thus having to be supported by many observations for our confidence on that diagnosis to increase.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
questions rq1 is the ddu metric more accurate than the state ofthe art in diagnosability assessment?
rq2 how close does the ddu metric come to the ideal yet intractable full entropy?
rq3 does optimizing a test suite with regard to ddu result in better diagnosability than optimizing adequacy metrics such as branch coverage in traditional scenarios?
rq1 asks if there is a benefit in utilizing the proposed approach as opposed to density and uniqueness which have been used in related work.
rq2 is concerned with assessing if ddu shares a statistical relationship with entropy the measurement whose maximal value describes an optimal yet intractable and impractical coverage matrix.
rq3 asks if using ddu as an indicator of the diagnostic ability of a test suite is more accurate than using standard adequacy measurements like branch coverage in a setting with real faults.
a. experimental setup our empirical evaluation compares ddu to several metrics in use today.
to effectively compare the diagnosability of testsuites of a given program that maximize a specific metric we leverage a test generation approach.
e vosuite7is a tool that employs search based software testing sst approaches to create new test cases.
it applies genetic algorithms gas to minimize a fitness function which describes the distance to an optimal solution.
the metrics to be compared are ddu our proposed measurement density and uniqueness to be able to answer rq1 entropy to answer rq2 and lastly branch coverage for rq3 .
these metrics were encoded as fitness functions in the e vosuite framework.
as the ga in evosuite tries to minimize the value of a function over a test suite ts the fitness functions for each metric mare as follows fm ts om m ts whereomis the optimal value of metric m e.g.
.0for the case of branch coverage and .5for density and m ts is the result of applying metric mto test suite ts.
to account for the randomness of e vosuite s ga we repeated each testsuite generation experiment times.
e vosuite s maximum search time budget was set to seconds which follows the setup of previous studies also using the tool .
evosuite by itself does not generate fault finding oracles otherwise a model of correct behavior would have to be provided.
instead it creates assertions based on static and dynamic analyses of the project s source code.
this means that if we run the generated test suite against the same source 7evosuite tool is available at version .
.
was used for experiments accessed february .code used for said generation all tests will pass provided the code is deterministic8 .
thus if the source code submitted for test generation contains faults no generated test oracle will expose them.
for the experiments comparing with the state of the art and the idealistic approach to answer rq1 and rq2 respectively we need a controlled environment so that oracle quality which in itself is an orthogonal factor does not affect results.
therefore the experiment described in section v b mutates the program spectrum of generated test suites to contain seeded faults and seeded failing tests.
in each experiment a set of components were considered as faulty and tests that exercise them were set as failing according to an oracle quality probability in our experiments the oracle quality is0.
meaning that whenever a faulty component is involved in a test there is a chance that the test will be set as failing.
the chosen value is a compromise between perfect error detection i.e.
oracle quality of1 and essentially random error detection oracle quality of0.
this fault injection approach is common practice among controlled theoretical evaluations of spectrum based diagnosis .
for assessing the applicability in real world scenarios and to answer rq3 we need real life bugs and fixes.
therefore in section v c we make use of d efects 4j9 a software fault catalog to generate test suites from fixed versions of a program and then gather program spectra by testing the corresponding faulty version.
spectrum gathering was performed at the branch granularity for both experiments so every component in our subjects coverage matrices corresponds to a method branch this way we can fairly compare our approach to branch coverage.
each program spectrum gathered in the previous step is then diagnosed using the automated diagnosis tool c rowbar .
this tool implements the spectrum based reasoning approach described in section iii a and generates a ranked list of diagnostic candidates for the observed failures.
for a given subject program to compare the diagnosability of a test suite generated by the ddu criterion with the one generated by a criterion c we use the following metric effort c effort c effort ddu where effort ddu is the effort to diagnose using the testsuite generated with the ddu criterion and effort cis the effort to diagnose with the test suite by maximizing some criterionc.
effort takes as input the ranked list of diagnostic candidates from c rowbar and estimates quality of diagnosis as described in section iii b. the effort c metric ranges from 1to1.
positive values of effort c mean that the bug is found faster in diagnoses that use the ddu generated 8evosuite also tries to replicate the state of the environment at each testrun so that even some non deterministic functionality such as random number generation can be tested.
9defects 4j tool is available at version .
.
was used for experiments accessed february .
10crowbar tool is available at crowbar maven plugin accessed february .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
kernel density estimation of seeded fault experiment.
entropy generation criterion shows similar diagnostic accuracy when compared ddu.
the remaining generation criteria exhibit worse diagnostic performance than ddu.
test suite.
negative values mean that the faulty component is ranked higher in the c generated test suite than the ddu one thus requiring less spurious diagnostic inspections.
effort c of value0means that the faulty component is ranked with the same priority in both test generations.
we make use of kernel density estimation plots to show the effort c values in figures and .
such plots estimate the probability density function of a variable i.e.
they describe the relative likelihood y axis for a random variable effort c in our case to take on a given value x axis .
in our experiments the higher the density value at a certain value in the x axis the more instances with effort c near that value were observed.
note that the observed data is shown as a rug plot with tick marks along the x axis reminiscent of the tassels on a rug .
b. diagnosing seeded faults our first experiment attempts to answer rq1 and rq2 by generating test suites and seeding faults in their spectra in a controlled way.
we same set of subjects as empirical evaluations from related work .
namely we use the opensource projects apache commons codec apache commonscompress apache commons math and jodatime.
for each subject we generate test suites that optimize ddu branchcoverage entropy density and uniqueness.
in total program spectra were generated and diagnosed.
experimental results are shown in figure .
when we consider the entropy generation we can say that the resulting test suites are very similar in terms of diagnosability compared to ddu since effort h is denser at the origin.
for the remaining generation criteria their respective effort probability masses are shifted to effort so their diagnostic reports perform worse at diagnosing the faults than when ddu is utilized.
in fact our inspection of experimental results reveals that when optimizing branch coverage of scenarios showed lower diagnostic accuracy when compared to ddu.
for both the density optimized and uniqueness optimized test generations which are the state of the art measurements for test suite diagnosability this figure rises to of scenarios.table i metric results for the seeded faults experiment.
median size correlation correlation p valuesubject h ddu u bc .
.
.
.
.
n.a.
.
.
.
.793apache commonscodecn.a.
.
.
.
.
.
.
.
.
.
.
.
n.a.
.
.
.
.968apache commonscompressn.a.
.
.
.
.
.
.
.
.
.
.
n.a.
.
.
.
.885apache commonsmathn.a.
.
.
.
.
.
.
.
.
.
n.a.
.
.
.
.654jodatime n.a.
.
.
.
.
we show in table i the dominant metric median values for each generation criterion along with the median number of tests generated.
by dominant metric we mean the metric which that particular test generation was trying to optimize.
along with the median value we also show where available the metric s pearson correlation with entropy denoted by rh and thep value of the correlation.
with confidence we can say that the correlation values shown are statistically significant.
ddu exhibits a high correlation with entropy havingrh .95for all subjects.
in all other generation criteria the correlation with entropy fluctuates considerably between subjects.
also note that for both and branchcoverage criteria their dominant mean values approach the theoretical optima at .5and1.
respectively while effort still shows that ddu test generation was able to produce suites with better diagnostic accuracy.
revisiting the first research question rq1 is the ddu metric more accurate than the state ofthe art in diagnosability assessment?
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a there is a clear benefit in optimizing a suite with regard to ddu compared to density if we consider the effort of finding faults in a system.
this is evidenced by the fact that of scenarios in our seeded fault experiment show improved diagnostic accuracy when using ddu when compared to the state of the art density and uniqueness measurements.
if we look at the second research question rq2 how close does the ddu metric come to the ideal yet intractable full entropy?
a table i shows a strong correlation between entropy and ddu with a pearson correlation value above .95for all subjects.
correlation of other metrics is much lower and varies greatly across subjects.
thus we can conclude that ddu closely captures the characteristics of entropy.
the reader might then pose the question if maximal entropy does indeed correspond to the optimal coverage matrix why should one avoid using it as the diagnosability metric?
while we agree that in automated test generation settings entropy can be plugged as the fitness function to optimize 11for manual test generation entropy will yield very small values for any complex system as one can see from table i. in fact for a system composed of only 30components the number of tests needed to reach entropy of .0surpasses the billion mark.
this makes it difficult for developers to leverage information out of their test suite s entropy value to gauge when can one confidently stop writing further tests.
c. diagnosing real faults we used the d efects 4j database for sourcing the experimental subjects.
d efects 4j is a database and framework that contains real software bugs from open source projects.
for each bug the framework provides faulty and fixed versions of the program a test suite exposing the bug and the fault location in the code.
the idea behind d efects 4j is to allow for reproducible research in software testing using realworld examples of bugs rather than using the more common hand seeded faults or mutants.
in our evaluation we generate test suites for each of d efects 4j s catalogued bugs using both branch coverage and ddu as e vosuite s fitness functions and then compare the two generated suites with regard to their diagnosability and adequacy.
the experiments methodology is as follows.
for every bug in d efects 4j s catalog we use e vosuite to generate test suites for the fixed version of the program.
the test suites are executed against the faulty program versions.
this means that any test failure is due to the bug which is the delta between the faulty and fixed program versions.
out of the catalogued bugs in d efects 4j not all were considered for analysis.
scenarios were discarded due to the following reasons evosuite returned an empty suite 11because tools like e vosuite can be configured with a time budget as another stopping criteria.table ii d efects 4j projects.
identifier project name scenarios considered chart jfreechart closureclosure compiler133 langapache commons lang65 mathapache commons math106 time jodat ime table iii metric medians and statistical tests.
branch co verage ddu generation generation branch coverage0.
.
ddu .
.
suite size291 effort .
.
w .
w .85shapir o wilkp value .
8p value .
wilcoxon z .
signed rank p value .
the generated suite did not compile or produced a runtime error no failing tests were present in either ddu or branchcoverage criteria for generating test suites.
in total scenarios were filtered out.
the remaining listed in table ii are fit for analysis and their results are used throughout this section.
experimental results are shown in figure .
results are shown per subject.
we can see that for every subject in the defects 4j catalog all their estimated probability density funtions are shifted towards effort bc meaning that the majority of instances have better diagnostic accuracy when test generation optimizes ddu.
in fact our experiments reveal that of scenarios in total yield a positive effort bc .
we performed statistical tests to assess whether the gathered metrics yielded statistically significant results.
table iii shows the relevant statistics.
the first four rows show the median values for branch coverage ddu generated suite size and diagnosis effort for both e vosuite test generations.
as to be expected the median branch coverage is higher in the branch coverage maximizing generation.
conversely the ddu criterion yields the higher ddu.
results in the effort row corroborate our observations from figure the test suites optimizing ddu take on average less effort to diagnose the fault.
in fact our results show that the effort reduction when considering ddu over branch coverage is on average.
however this fact alone does not guarantee that the results are authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
kernel density estimation of the effort bc metric for d efects 4j subjects.
of instances have a positive effort bc meaning that branchcoverage generations perform worse than ddu generations.
significant which prompted us to perform statistical tests.
the first test performed was the shapiro wilk test for normality of effort data for both generations.
the results which can be seen in the fourth row of table iii tell us that the distributions are not normal with confidence of .
given that the effort data is not normally distributed and that each observation is paired we use the non parametrical statistical hypothesis test wilcoxon signed rank.
our nullhypothesis is that the median difference between the two observations i.e.
effort is zero.
the fifth row in table iii shows the resulting zstatistic and p value.
with confidence we can refute the null hypothesis.
revisiting rq3 rq3 does optimizing a test suite with regard to ddu result in better diagnosability than optimizing adequacy metrics such as branch coverage in traditional scenarios?
a since the median effort in the ddu generation is lower the reduction amounting to on average we can say that optimizing for ddu produces better statistically significant diagnoses when compared to test suites that optimize for branch coverage.
vi.
d iscussion we discuss the practical implications of our findings from the previous section as well as outline the potential threats to their validity.
a. practical implications ddu was shown to be useful for evaluating the quality of a test suite.
but what are the practical implications of this finding?
we outline such assessments next.
we argue that the ddu analysis can suggest an ideal balance between unit tests and system tests i.e.
when ddu reaches its optimal value due to its density term.
we are then able to compare the balanced suites to ones created following testing practices currently established at software development companies.
for instance google suggests a split between unit system andend to end tests in a suite.12is this split indeed ideal in terms of diagnostic accuracy?
we believe a ddu analysis can provide guidance as to what the answer is.
we expect the ddu analysis to be used as the first step of a test design strategy that aims to increase diagnostic accuracy of a suite.
for that we envision that new test patterns that focus on optimizing diagnosability will need to be researched and incorporated in established test strategy corpora such as .
in coverage metrics it is straightforward to visualize the analysis of a system so that users know what code components were left untested highlighting where to focus when writing new test cases.
is there a way to visualize ddu analysis in a similar way?
we envision that visualization approaches for program comprehension such as e xtra vis and p angolin will constitute a solid starting point for a study on visual interactive and actionable ways to convey ddu information.
we show that ddu depicts the diagnosability of spectrum based fault localization approaches.
however our intuition is that ddu is general and applies to any diagnosis technique that uses a failing test suite as the basis for locating faults.
we plan to investigate this hypothesis as future work.
ddu provides an assessment of the diagnostic effectiveness of a given test suite.
it remains to be seen if that can also be said for assessing the fault finding effectiveness which is also a good avenue for future work.
in the meantime we consider our metric to be a complement to adequacy metrics and envision that testers will employ a hybrid approach that relies on branch coverage and ddu to assess adequacy and diagnosability respectively.
b. threats to v alidity the main threats to validity of this study are related to external validity.
when choosing the projects for our study our aim was to opt for projects that resemble a general largesized application being worked on by several people.
to reduce 12google testing blog just say no to more end to end tests.
accessed february .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
selection bias and facilitate the comparison of our results we decided to use the real world scenarios described in the d efects 4j database.
another threat to external validity relates to the choice of test suites generated by e vosuite .
additional research is needed to see how the metric behaves both with different test generation frameworks such as r andoop and with hand written test cases.
a potential threat to construct validity relates to the choice of effort as indicator for diagnosability.
however as argued in section iii b this choice reflects the effort that a programmer with minimal knowledge about the system would require to effectively pinpoint all the faults that explain the observed failures.
the main threat to internal validity lies in the complexity of several of the tools used in our experiments most notably the e vosuite test generator and our diagnosis tool.
vii.
r elated work related work in the assessment of the diagnosability of a test suite has focused on three key areas test suite minimization and generation strategies and assessing oracle quality.
the topic of test suite minimization is a prime candidate for our approach since it has been shown that there is a tradeoff between reducing tests and the suite s fault localization effectiveness .
in minimization settings one tries to reduce the number of tests and thus its overall running time while still ensuring that an adequacy criterion usually branch coverage is not greatly affected.
current minimization strategies can often improve the diversity score of a coverage matrix by removing tests with identical coverage patterns at the cost of overlooking density and uniqueness which we argue are of key importance to assess diagnosability.
the uniqueness property is also exploited by xuan et al.
with a test case purification approach that separates a test case into multiple smaller tests .
this approach overlooks the fact that density will decrease along with the ability to diagnose a multiplefault scenario.
current test suite minimization frameworks that take adequacy criteria into account could also benefit from our approach to preserve diagnostic accuracy if a multi objective optimization such as e.g.
to also account for ddu is employed.
this paves an interesting avenue for future work.
on the test suite generation front previous work has also started considering diagnosability as a generation criterion.
the work of campos et al.
which generated tests that would converge towards coverage matrix densities of .
has paved the way for creating improved measurements like ddu.
checks for diversity and uniqueness were not explicitly added and we show when we answer rq1 in section v that the density criterion produces results that are less diagnostically accurate.
another approach to suite generation is one by artzi et al.
that proposes an online approach that leverages concolic analysis to generate tests that are similar to existing failing tests in a system .
lastly we highlight some of the work targeting diagnosability by improving test oracle accuracy.
schuler et al.
proposechecked coverage as a way of assessing oracle quality .
checked coverage tries to gauge whether the computed results from a test are actually being checked by the oracle.
wang et al.
have proposed a way of addressing coincidental correctness when a fault is executed but no failure is detected by analyzing data and control flow patterns .
just et al.
investigated the use of mutants to estimate oracle quality and compared their performance against the use of real faults .
their results suggest that a suite s mutation score is a better predictor of fault detection than code coverage.
we consider this topic of assessing and improving oracle quality of critical importance towards test suite diagnosability but also orthogonal to ddu in that the two would complement each other.
viii.
c onclusion this paper proposes a new metric to assess a test suite s diagnostic ability to detect fault using spectrum based approaches given that there are failing tests.
the intuition is that a test suite where each test is diverse in that it exercises as many combinations of components as possible is more exhaustive than one that merely focuses on maximizing code coverage.
in fact a variable number of covered elements is crucial for good fault isolation when tests fail as it helps techniques like spectrum based reasoning indict and exonerate both singlecomponent faults and multiple component faults.
the metric coined ddu tries to emulate the properties of calculating per test coverage entropy to ensure accurate diagnosability.
ideal diagnostic ability can be proved to exist when a suite reaches maximum entropy however the number of tests required to achieve that is impractical as the number of components in the system increases.
ddu focuses on three particular properties of entropy a ensures that test cases are diverse b ensures that there are no ambiguous components c ensures that there is a proportional number of tests of distinct granularity while still ensuring tractability.
an experiment was performed to assess ddu as a metric for diagnosability.
it used the e vosuite tool to generate test suites for faulty programs from the d efects 4j catalog that would optimize different metrics.
we observed a statistically significant increase in diagnostic performance of about when locating faults by optimizing ddu compared to branchcoverage.
ddu paves the way for a more comprehensive use of testsuites making them not only a great tool for error detection and requirements elicitation but also an effective diagnostic aid when failures arise.
software artifacts we provide a fork of e vosuite .
.
implementing every generation criterion used in section v at a m av e n plugin for determining the ddu value for java projects is available at com aperez ddu maven plugin.