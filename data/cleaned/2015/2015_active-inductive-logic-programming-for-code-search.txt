active inductive logic programming for code search aishwarya sivaraman tianyi zhang guy v an den broeck miryung kim university of california los angeles dcssiva tianyi.zhang guyvdb miryung cs.ucla.edu abstract modern search techniques either cannot efficiently incorporate human feedback to refine search results or cannot express structural or semantic properties of desired code.
thekey insight of our interactive code search technique a lice is that user feedback can be actively incorporated to allowusers to easily express and refine search queries.
we design aquery language to model the structure and semantics of codeas logic facts.
given a code example with user annotations a lice automatically extracts a logic query from code features that are tagged as important.
users can refine the search queryby labeling one or more examples as desired positive orirrelevant negative .
a lice then infers a new logic query that separates positive examples from negative examples via activeinductive logic programming.
our comprehensive simulationexperiment shows that a lice removes a large number of false positives quickly by actively incorporating user feedback.its search algorithm is also robust to user labeling mistakes.our choice of leveraging both positive and negative examplesand using nested program structure as an inductive bias iseffective in refining search queries.
compared with an existinginteractive code search technique a lice does not require a user to manually construct a search pattern and yet achievescomparable precision and recall with much fewer searchiterations.
a case study with real developers shows that a lice is easy to use and helps express complex code patterns.
index t erms code search active learning inductive logic programming i. i ntroduction software developers and tools often search for code to perform bug fixes optimization refactoring etc.
for example when fixing a bug in one code location developersoften search for other similar locations to fix the samebug .
text based search techniques allow users toexpress search intent using keywords or regular expressions.however it is not easy to express program structures orsemantic properties using text based search thus hinderingits capability to accurately locate desired code locations.txl and wang et al.
provide a domain specificlanguage dsl for describing structural and semantic prop erties of code.
however learning a new dsl for code searchcan be cumbersome and time consuming.
several techniques infer an underlying code search pattern from a user provided example .
these techniquesadopt fixed heuristics to generalize a concrete example toa search pattern which may not capture various searchintent or allow a user to refine the inferred pattern.
lase and refazer use multiple examples instead of a singleexample to better infer the search intent of a user.
however this requirement poses a major usability limitation a usermust come up with multiple examples a priori.
critics allowsa user to construct an ast based search pattern from a singleexample through manual code selection customization andparameterization .
however users of critics report that the internal representation of a search pattern is not easy tocomprehend and that they could benefit from some hints toguide the customization process.
we propose an interactive code search technique a lice that infers a search pattern by efficiently incorporatinguser feedback via active learning.
a lice has three major components a novel user interface that allows a userto formulate an initial search pattern by tagging importantfeatures and iteratively refining it by labeling positive andnegative examples a query language that models struc tural and semantic properties of a program as logic factsand expresses a search pattern as a logic query and anactive learning algorithm that specializes a search patternbased on the positive and negative examples labeled by theuser.
in this interface a user can start by selecting a codeblock in a method as a seed and by tagging important codefeatures that must be included in the pattern.
a lice then automatically lifts a logic query from the tagged featuresand matches it against the logic facts extracted from theentire codebase.
our query language models properties in cluding loops method calls exception handling referencedtypes containment structures and sequential ordering in aprogram.
therefore our query language can easily expressa search pattern such as find all code examples that callthe readnextline method in a loop and handle an exception of type filenotfoundexception which cannot be accurately expressed by text based approaches.
our active learning algorithm utilizes inductive logic programming ilp to refine the search pattern.
ilpprovides an easy framework to express background knowl edge as rules and is well suited for learning from structureddata.
ilp does not assume a flat data representation and allows to capture structural properties of code not easilycaptured by other methods such as neural networks .additionally ilp has been proven successful in programsynthesis specification mining and modelchecking .
since users can only inspect a handful ofsearch results due to limited time and attention a lice allows the user to provide partial feedback by only labeling a few examples and gradually refine the search pattern inmultiple search iterations.
a lice uses an inductive bias ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
bob tags ifcondition range!
null andifcondition this.leadingptr as must have code elements.
to specialize the previous search pattern to separate the labeled positive and negative examples in each iteration.
this feedback loop iterates until the user is satisfied with the returned result.
we evaluate a lice using two benchmarks from prior work .
these benchmarks consist of groups of similar code fragments in large scale projects such as eclipse jdt.
a lice learns an intended search pattern and identifies similar locations with precision and recall in three iterations on average when the user initially tags only two features and labels three examples in each iteration.
a comprehensive simulation experiment shows that labeling both positive and negative examples are necessary to obtain the best search results compared to labeling only positives or only negatives.
it is because negative examples quickly reduce the search space while positive examples are used to capture the search intent.
we vary the number of labeled examples and find that labeling more examples in each iteration does not necessarily improve the final search accuracy.
this indicates that a user can label only a few in each iteration to reach good accuracy eventually.
a lice thus alleviates the burden of labeling many examples at once as its active learning can leverage partial feedback effectively.
the comparison with an existing technique critics shows that a lice achieves the same or better accuracy with fewer iterations.
this human in the loop approach in a lice opens a door for incorporating user feedback in a variety of software engineering techniques e.g.
bug detection code optimization refactoring etc.
for example a code optimization framework casper searches for java programs with loops that sequentially iterate over data and translates the loops to semantically equivalent map reduce programs .
however the search patterns in casper are hardcoded and cannot be easily expressed by a user.
a lice can aid in such scenarios by allowing a user to interactively construct a search pattern.
our paper makes the following contributions.1public int getextendedstartposition astnode node if this .leadingptr int range null for int i i this .leadingptr i if this .leadingnodes node range this .leadingindexes if range !
null return this .comments .getstartposition return node.getstartposition fig.
bob labels this example as positive.
this example is syntactically similar to figure but has a different loop condition i this.leadingptr .
we present a novel approach called a lice that integrates active learning and inductive logic programming to incorporate partial user feedback and refine code search patterns.
a lice is instantiated as an eclipse plug in and the tool is available online.
we conduct a comprehensive simulation experiment that investigates the effectiveness of a lice using different inductive biases different numbers of labeled examples and different numbers of annotated code features.
an additional robustness experiment shows that our search algorithm is resilient to labeling mistakes by flagging contradictory examples labeled by a user.
we conduct a case study with real developers demonstrating that participants can easily interact with a lice to search desired code by simply inspecting and labeling code examples.
on average each participant labels two examples in each search iteration and spends about seconds on each example.
ii.
m otiv a ting example this section describes the code search process using alice with a real world example drawn from eclipse java development toolkit jdt .
eclipse jdt provides basic tools and library apis to implement and extend eclipse plug ins.
as an eclipse jdt developer bob wants to update the getleadingcomments method figure to return an empty comment array when there are no leading comments instead of returning null which may cause a nullpointerexception in a caller method.
before modifying the code bob wants to check other similar locations.
bob could use a text based search tool to find other code fragments similar to the getleadingcomments method.
for example bob could use comment andarraycopy as keywords to find code locations that reference the comment type and call the arraycopy method.
such text based techniques are prone to return a large number of search results many of which merely contain the same keywords but have significantly different code structures or irrelevant functionality.
in the jdt codebase searching with the 1our tool and dataset are available at authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
1public void checkcomment ... 3if lastcomment this .modifierssourcestart this .scanner.commentstarts while lastcomment this .scanner.commentstops lastcomment if lastcomment this .javadocparser !
null if this .javadocparser.checkdeprecation this .scanner.commentstarts this .scanner.commentstops checkandsetmodifiers accdeprecated this .javadoc this .javadocparser.doccomment fig.
bob labels this example as negative.
though it has a similar ifcheck lastcomment its program structure is significantly different from figure .
arraycopy keyword returns method locations.
it is prohibitively time consuming to inspect all of them.
prior work has shown that developers only inspect a handful of search results and return to their own code due to limited time and attention .
to narrow down the search results it would be useful to describe the structural properties of getleadingcomments .
for example a code snippet must contain an ifcondition that checks for and contain a loop inside the ifblock.
instead of requiring a user to prescribe such a structural pattern manually a lice enables the user to tag important code features and label positive vs. negative examples instead.
iteration .
select a seed example and annotate important features.
bob first selects a code block of interest in the getleadingcomments method lines in figure .
he annotates anifcondition range!
null circlecopyrtin figure as a code element that must be included in the search pattern since he posits that similar locations would have a null check on the range object.
such must have code elements are colored in green.
a lice then automatically constructs a search query based on the selected code and annotations.
since other developers may rename variables in different contexts a lice generalizes the variable name range in the tagged if condition to match a null check on any variable.
among all java methods in the eclipse jdt codebase a lice returns methods with a null check which are too many to examine.
bob now has two choices.
he can annotate more features or label a few examples to reduce the search results.
bob tags another if condition this.leadingptr as another must have element circlecopyrt in figure .
the field name leadingptr is also generalized to match any variable name.
a lice refines the previous query and returns ten methods that both perform a null check and contain an ifcondition checking if a field is no less than .
the query is shown below and its syntax is detailed in section iii d. fig.
bob labels some examples as desired and some as irrelevant.
green circlecopyrtindicates newly returned examples red circlecopyrt indicates a previously marked negative examples and cyan circlecopyrt indicates a previously marked positive example.
query x methoddec x contains x if iflike if this.
contains x if iflike if .
!
null .
iterations to n. active learning by labeling positive and negative examples.
after inspecting two examples returned from the previous iteration bob labels one example figure as positive and another one figure as negative.
bob labels figure as irrelevant since this example has similarifconditions line but does not compute the range of a code comment or returns a range.
by incorporating this feedback a lice learns a new query that includes a unique structural characteristic the null check is contained by the if check for an index field greater or equal to that appears in both the initially selected example and the positive example but not in the negative example as shown below.
as a result a lice narrows down to six examples circlecopyrtin figure .
a user can further refine the search query and continue to iterate by labeling examples.
as a lice uses a top down search algorithm to specialize its search results in each iteration the resulting set is always a subset of the previous results.
query x methoddec x contains x if iflike if this.
contains if if iflike if .
!
null .
iii.
a pproach a. machine learning approach traditional code search techniques retrieve code examples by matching user specified patterns.
in contrast a lice s central thesis is that such patterns need not be fully specified by the user and can instead be induced from specific user interactions.
this is fundamentally a machine learning approach to code search the intended pattern is a hypothesis that is learned from data.
to achieve this goal a lice integrates several learning paradigms.
first it requires labels that categorize code examples as either matching the intended hypothesis or not.
classical supervised learning would require the user to provide prohibitively many labels.
a lice therefore employs active learning where the learner has the ability to present some unlabeled examples to the user during learning and ask to provide a label.
in a well designed active learning authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
setup the user only needs to provide a minimal number of labels for the learner to reliably find the correct hypothesis.
second most machine learning approaches represent data as feature vectors which cannot easily express the structure of source code.
inductive logic programming ilp is a form of relational learning that supports structured data encoded as logical facts.
in particular a lice uses the logical predicates listed in the first column of table i to represent each code example in a factbase.
the next section describes this process in detail.
given that the data is now in a suitable form ilp aims to learn a hypothesis by constructing a logical prolog query that returns exactly those code ids that the user is searching for.
queries in a lice are represented as definite clauses.
for example in the queries shown in section ii we are looking for values of the logical variable xfor which the body of the query is true.
the body is true if there exists a value for the remaining logical variables uppercase such that each atom in the body is also found in the factbase.
the process of learning a query is centered around two operations generalization and specialization .
generalization changes the query to return more results e.g.
removing atoms from the body or replacing constants with variables.
specialization is the reverse operation yielding a query with fewer results.
an ilp learner uses these operations to search the space of all hypotheses for the right one.
finally even with active learning a lice operates in a regime where little labeled data is available.
we know from learning theory that this is only possible when the learner has a strong inductive bias .
that is the learner already incorporates a lot of knowledge about the learning task before even seeing the data.
we address this in three ways.
first ilp affords the specification of declarative background knowledge which helps to create a more meaningful inductive bias as it applies to code search.
table ii shows a lice s background knowledge which enables a lice to use additional predicates to construct richer queries.
second we allow the user to annotate important code features in order to obtain a stronger inductive bias.
third a lice adopts a specialization procedure that is specifically designed to capture different program structures to strengthen the inductive bias.
we implement our own realtime ilp system based on the high performance y ap prolog engine .
b. logic fact extraction alice creates a factbase from a given code repository using the predicates described in table i. it parses program files to abstract syntax trees asts and traverses the asts to extract logic facts from each method.
predicates ifandloop capture the control flow within a method methodcall represents the relationship between a method and its caller exception captures the type of handled exceptions type captures the type of defined and used variables contains describes the relationship between each ast node and its parent recursively before captures the sequential ordering of ast nodes.
specifically before id id istable i predicates in logical representation fact predicates rule predicates if id cond iflike id regex loop id cond looplike id regex parent id id contains id id next id id before id id methodcall id call type id name exception id name methoddec id table ii background knowledge prolog rules iflike id regex if id cond regex match cond regex .
looplike id regex loop id cond regex match cond regex .
contains id1 id2 parent id1 id2 .
contains id1 id3 parent id1 id2 contains id2 id3 .
before id1 id2 next id1 id2 .
before id1 id3 next id1 id2 before id2 id3 .
true when node id1comes before node id2in preorder traversal while excluding any direct or transitive containment relationship.
for figure loop loop range null i this.leadingptr comes beforeif if range!
null creating before loop if2 .
ground facts are then loaded into a y ap prolog engine for querying.
c. generalization with feature annotation iteration when a user selects a code block of interest a lice generates a specific query which is a conjunction of atoms constructed using the predicates in table i. each atom in the query is grounded with constants for its location id and value representing the ast node content.
a lice replaces all id constants in the query with logical variables to generate an initial candidate hypothesis h0.f o r example one ground atom in the query is of the form if if range!
null and its variablized atom is of the form if if range!
null .
to find more examples alice generalizes the hypothesis by dropping atoms in h0 other than the user annotated ones producing h1.
regex conversion.
alice further abstracts variable names inh1to identify other locations that are similar but have different variable names.
a lice converts predicates if andloop toiflike andlooplike respectively.
as defined in table ii iflike and looplike are background rules that match a regular expression regex with ground conditions cond in the factbase.
for instance each variable name in loop id range null i this.leadingptr is converted from a string constant to a kleene closure expression generating looplike id .
null .
this.
.
the output of this phase is a generalized query h2and a set of code examples that satisfy this query.
this example set is displayed in the example view in figure .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
d. specialization via active learning iterations to n in each subsequent search iteration given the set of positive examples p and the set of negative examples n labeled by a user the previous hypothesis hi which is a conjunction of atoms is specialized tohiby adding another atom to exclude all negatives while maximally covering positives.
the specialization function is defined below.
specialize hi p n argmax hi summationdisplay p p such that hi hi 1and n n n negationslash hi suppose all positive examples call foo and all negative examples call bar instead of foo .
we add a new atom calls m foo to specialize hi which distinguishes the positives from the negatives.
as a design choice our active ilp algorithm is consistent i.e.
not covering any negative example but is not complete i.e.
maximally satisfying positive examples .
our learning algorithm is monotonic in that it keeps adding a new conjunctive atom in each search iteration.
this specialization procedure always returns a subset of the previous search results obtained by hi .
this feedback loop continues to the n th iteration until the user is satisfied with the search results.
given the large number of candidate atoms inductive bias is required to guide the specialization process of picking a discriminatory atom.
a lice implements three inductive biases which are described below.
the effectiveness of each bias is empirically evaluated in section iv.
feature vector.
this bias considers each code block to have a flat structure.
the feature vector bias does not consider the nested structure or sequential code order.
it specializes by adding a random atom that reflects the existence of loops if checks method calls types or exceptions in the code block.
it is used as the baseline bias in the evaluation since it does not utilize any structural information such as containment and ordering.
nested structure.
this bias utilizes the containment structure of the seed code example to add atoms.
in addition to adding an atom corresponding to the ast node the bias adds a contains predicate to connect the newly added atom to one that already appears in the query.
consider an ast with root node a whose children are bandc bhas children dande and chas child f. suppose that hincludes an atom referring to b. then based on the containment relationships a lice selects one ofdoreto specialize the query for the next iteration not f. if there are no available children or if this query fails to separate positives from negatives it falls back to the parent of bor its further ancestors to construct the next query.
we choose this nested structure bias as default since it empirically achieves the best performance detailed in section iv a .
sequential code order.
this bias uses sequential ordering of code in the seed example to determine which atom to add next.
consider an example ast with root node aand children bandc citself has childrendande.
atoms before b d before b c and before b e are generated according to the rules in table ii.
given a query that contains atoms referring to b a lice now chooses one of c d o re to connect tobusing the before predicate and adds this node to the query.
if there are no available atoms to add or if this query fails to separate positives from negatives it falls back to the original feature vector bias.
an alternative approach is to carry out logical generalization where we generate a query by generalizing positive examples and taking the conjunction with the negation of negative example generalization.
as a design choice we do not allow negations in our query for realtime performance since supporting negations would significantly increase the search space and execution time.
iv .
s imula tion experiment we systematically evaluate the accuracy and effectiveness of a lice by assessing different search strategies and by simulating various user behaviors.
dataset.
we use two complementary sets of similar code fragments as the ground truth for evaluation as shown in table iii.
the first dataset is drawn from the evaluation data set of lase .
this dataset consists of groups of syntactically similar code locations in eclipse jdt and swt where developers need to apply similar bug fixes.
we select groups with more than two similar locations resulting in groups in total.
each group contains an average of five similar code fragments and each fragment contains a median of lines of code ranging from to .
a lice extracts an average of 670k logic facts from each repository.
the second data set is from the evaluation dataset of casper an automated code optimization technique.
this dataset consists of groups of similar code fragments that follow the same data access patterns e.g.
a sequential loop over lists and can be systematically optimized by casper.
by evaluating alice with both datasets we demonstrate that a lice is capable of accurately and effectively searching code in two different usage scenarios bug fixing and code optimization.
because the second data set includes only the relevant files not the entire codebase we cannot reliably assess the rate of false positives.
hence we exclude this second dataset when assessing the impact of individual biases annotations and labeling effort in sections iv a iv b iv c and iv d. experiment environment.
all experiments are conducted on a single machine with an intel core i7 7500u cpu .7ghz cores threads x64 .
.
generic 16gb ram and ubuntu .
lts.
we use y ap prolog version .
.
a high performance prolog engine to evaluate search queries.
we write a simulation script to randomly select a code fragment in each group as the seed example.
in the first iteration the script randomly tags kimportant features that represent control structures method calls and types in the seed example.
in each subsequent iteration it simulates the user behavior by randomly labeling nexamples returned in the previous iteration.
if a code example appears in the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii simulation experiment dataset and results summary groupsrepo revision methods factbase size id loc snippets precision recall f1 iterations query length .
.
.
jdt .
.
.
11jdt .
.
.
.
.
.
swt .
.
.
.
.
.
.
.
.
swt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
arith .
.
.
big .
.
.
3phoenix .
.
.
.
.
.
3stats .
.
.
average .
.
.
.
total average .
.
.
.
ground truth it is labeled as positive.
otherwise it is labeled as negative.
the script terminates if no more examples can be labeled in a new search iteration.
to mitigate the impact of random choices we repeat the simulation ten times and report the average numbers for each group.
result summary.
table iii summarizes the precision recall and f1 score of a lice in the final search iteration.
when setting kand nto two and three respectively a lice empirically achieves the best result which we detail in sections iv c and iv d. on average a lice successfully identifies similar code locations with precision and recall in .
search iterations.
a lice achieves precision and recall in the first dataset while it achieves precision and recall in the second dataset.
the reason is that the second dataset contains code fragments that loop over a double array with no write to output operations which is a semantic constraint imposed by casper for loop optimization.
however a lice does not learn predicates that differentiate read and write operations on an array and therefore returns a large number of code fragments that write double arrays in a loop which leads to low precision.
in the first search iteration methods are returned by a lice on average median and maximum .
this set is large motivating our approach to leverage partial feedback i.e.
only three labeled examples at a time as opposed to labeling all returned results at once.
a lice effectively incorporates partial feedback to reduce the number of returned examples by in the second iteration.
since alice adopts a top down search process and always starts with a general query the initial search result tends to includeall code locations in the ground truth.
therefore the first search iteration often starts with recall and relatively low precision.
the precision then improves gradually as the query is specialized to eliminate false positives in the search results.
in table iii column query length represents the number of atoms in the final query.
on average the query length inferred by a lice contains atoms.
a. impact of inductive bias during query specialization an inductive bias is used to effectively navigate the space of all hypotheses.
we evaluate the effectiveness of each of the three implemented inductive biases discussed in section iii d feature vector nested structure and sequential order.
fig.
precision of a lice using different inductive biases figures and show the effectiveness of each bias in terms of precision and recall averaged across ten runs.
overall the nested structure bias converges fast by taking fewer iterations to reach the highest f1 score.
the sequential order bias performs better than the feature vector bias converging in six iterations as opposed to seven.
although authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
recall of a lice using different inductive biases both the sequential order bias and the nested structure bias perform well in terms of precision encountering the same sequential order between statements or methods is not common in the data set.
therefore the sequential order bias has a lower recall.
the nested structure bias adds atoms based on containment relationships and hence it filters out false positives early and converges faster.
1checkwidget 2if !parent.checkdata this true error swt.error widget disposed 3return font !
null ?
font parent.getfont fig.
a code snippet from group in table iii fig.
precision with different types of labeled examples fig.
recall with different types of labeled examples consider figure taken from group in table iii.
the first iteration returns examples when annotating checkdata anderror .
the feature vector bias takes six iterations the sequential order bias takes five and the nested structure bias takes four.
this is because the feature vector bias learns a query that describes calls to checkdata checkwidget and error regardless of their structure and order.
in contrast the sequential bias finds code that calls checkwidget before error andgetfont .
the nested structure bias finds code that calls checkdata within an if statement which respects the structure of the selectedtable iv v arying of tagged features in the first iteration feature features features features precision .
.
.
.
recall .
.
.
.
block.
we find that a flat representation of code as a feature vector is not powerful enough to effectively capture the search intent in a few iterations.
we need to incorporate the rich structural information already available in the code to achieve desired performance.
b. labeling positives and negatives to quantify the effect of using different types of labeled code examples we assess a lice in three different conditions during query specialization incorporating both positive and negative examples considering negative examples only and considering positive examples only.
figures and compare the precision and recall in different conditions.
when considering negative examples only a lice achieves a high precision while converging in five iterations whereas considering positives only takes more iterations to converge with lower precision.
this is because alice often starts with a general query in the first search iteration resulting in a large number of search results that contain many false positives.
hence it is essential to label several negatives to eliminate those false positives quickly.
however if we do not label any positives we are likely to remove some true positives as well.
this is reflected in the recall where giving negative examples only has the least recall.
therefore an optimal choice would be to use both positive and negative examples while iteratively specializing the query which justifies the design of a lice .
c. v arying the number of annotated features table iv summarizes the average precision and recall right after feature annotation when varying the number of annotations from one to five.
these features are randomly selected from the control structures method calls and types in the seed example and the results are averaged over ten runs.
the result suggests that annotating more code elements in the seed example can more precisely express the search intent and thus increase the precision.
however the recall is negatively impacted by increasing the number of annotations.
when the simulation experiment chooses more features it is likely to choose features that are not shared across expected code fragments in the ground truth.
thus the initial search query becomes too specific and therefore misses some expected code fragments.
let us consider the two similar but not identical code examples in section ii figures and .
if a user tags range null i this.leadingptr line in figure as important the recall decreases since the expected code example in figure does not contain this feature.
though a real user can annotate any number of code elements our experiment shows that tagging two code elements leads to the optimal precision and recall.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table v v arying of labeled examples in each iteration labels labels labels labels precision .
.
.
.
recall .
.
.
.
iterations total labels fig.
f1 score for different labels set at each iteration d. v arying the number of labeled examples the type and number of examples that a user labels in each iteration is an important aspect of active learning based tool design.
to quantify the benefit of iterative feedback over simultaneous labeling we vary the number of labeled examples nfrom two to five.
setting nto four or five converges the fastest with five iterations.
when nis three alice takes six iterations to converge.
when nis two it takes seven.
table v summarizes the average precision and recall for different number of labels in the final iteration.
overall increasing the number of labeled examples maintains precision but does not necessarily increase recall.
this is due to overfitting of labeled examples.
from table v and figure n gives a good trade off between f1 score and the required amount of user feedback and is more robust than n .
figure shows how user feedback helps and why it is better to spread labeling effort over multiple iterations rather than providing all labeled examples at once.
alice takes an average of seconds to parse a java repository of about 20k methods and extract logic facts.
this fact extraction is done only once in the beginning.
a lice takes .
seconds on average median seconds to infer a query and match the learned query to the entire codebase of about 670k facts.
this realtime performance is appreciated by study participants described in section v. v. a c ase study with users to investigate how a user perceives the paradigm of annotating features and labeling examples during code search we undertake a case study with three participants.
the goal of this study is to investigate how a lice may fit or augment current code search practices rather than assessing how much benefit a lice provides in comparison to alternative code search approaches.
therefore we use a case study research method to understand the in depth context of how a user may interact with a lice to search desired code.
in this study a user tagged features examined search results and labeled them as positive and negative examples.
allparticipants were graduate students with multiple years of java development experience.
participants reported that they usegrep or a regular expression search on a regular basis or have experience of using search features in ides.
in terms of the study setup we gave a tutorial about how to use a lice .
this included reading a written tutorial followed by a stepby step demonstration of tool features.
the participants were given a seed example of getselectiontext from eclipse swt which is part of group in table iii.
since the participants were not familiar with the codebase we provided two other similar snippets as a reference.
each session took about minutes to hour.
participants were encouraged to follow a think aloud protocol to verbalize their thought process.
do you think marking features in a snippet helps search?
i think marking features helps greatly when trying to find similar code snippets as a human my intuition is to identify key sections of code that i expect to be similar and ignore other sections that i expect to differ .
it is obvious that when we are searching for a piece of code and we are expecting to have certain features in it so marking features in a code snippet helps.
participants can easily identify important features in the seed example.
two of three participants annotated three features and one started with one feature.
participants found alice useful for refactoring and reorganizing fragmented yet similar code snippets.
they said that a lice would be more useful if integrated into code sharing websites such as github and bitbucket by enabling advanced code search in large scale repositories because novice programmers would often prefer to look up functions with similar implementations for optimization opportunities.
how do you think that labeling positive and negative examples fit in the search process?
as a developer i will mentally do a quick evaluation of search results to determine if something is definitely negative i.e.
i can skip it or potentially positive i.e.
it warrants more investigation .
it s normally not an explicit process but it makes sense to explicitly request feedback when interacting with tools.
i really liked the iterative refinement of examples and the almost realtime performance of the tool.
one aspect that may be improved is assigning a score or priority to the examples based on how many features are satisfied so that the user can prioritize which examples to inspect.
how do you think that an interactive and iterative approach such as alice compares to other search techniques that you used in the past?
alice seems useful for finding similar code snippets that span code blocks.
it provides a functionality separate from other search techniques i ve used such as ide based functionalities e.g.
call declaration method invocation locations and standard grep which is limited to single line searches.
i think alice is generally less applicable but also significantly more powerful.
it also helps promote better understanding for example ides can show where authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table vi time taken to label examples in iteration examples positives negatives time t aken s user user user a method is called from but alice can easily show the context in which that method invocation appears.
participants said a lice was more powerful than standard search tools like grep and built in search features in ides.
what do you like or not like about alice?
overall participants like the interactive feature as it allows for refinement and builds on developer understanding.
some participants find the color scheme in a lice confusing due to the conflict with existing syntax highlighting in eclipse.
we observe that participants were able to easily recognize the important features in the code example and tag them in the first search iteration.
though participants had little experience with the codebase they could still distinguish positive and negative examples without much effort.
table vi summarizes the number of examples returned in the first search iteration and the time taken for each user to refine the search.
in particular a user took an average of seconds to inspect each example and categorize it as positive or negative.
this indicates that the tool does not require much effort for a user to inspect and label examples.
vi.
c omparison we compare a lice with an existing interactive code search technique called critics.
we choose critics as a comparison baseline since critics also performs interactive search query refinement .
a lice differs from critics in two ways.
first in critics a user has to manually select code blocks such as an if statement or a while loop and parameterizes contents for example by replacing foo f new foo to t v new t .
such interaction is time consuming.
to reduce the user burden alice infers a syntactic pattern such as an if statement with a condition .
!
null inside a while loop from positive and negative methods.
second critics identifies similar code via tree matching while a lice abstracts source code to logic facts and identifies similar code via ilp .
we run a lice on the public data set obtained from critics s website.2table vii summarizes the results of critics vs. a lice .
in six out of seven cases a lice achieves the same or better precision and recall with fewer iterations to converge compared to critics.
in id a lice has low precision because the expected code contains a switch statement which is currently not extracted by a lice as a logic fact.
extending current logic predicates to support more syntactic constructs remain as future work.
vii.
d iscussion noisy oracle.
the simulation in section iv assumes that a user makes no mistakes when labeling examples.
however vii comparison against critics alice critics critics idprecision recall iterations precision recall iterations .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
table viii sensitivity of a lice to labeling errors.
error rates precision .
.
.
recall .
.
.
inconsistent cases it is possible that a real user may label a positive example as negative or even provide an inconsistent set of labels.
we investigate how resilient a lice is to labeling mistakes and how quickly a lice can inform the user of such inconsistencies.
we mutate our automated oracle with an error rate of and .
each of the groups from the first dataset is run five times trials with different annotations labels and errors.
in many cases a lice reports inconsistencies in user labeling and provides immediate feedback on the infeasibility of specializing queries to .
when a lice does not find any inconsistencies alice behaves robustly with respect to errors eventually reaching precision.
table viii summarizes the results.
threats to validity.
regarding internal validity the effectiveness of different inductive biases may depend on the extent and nature of code cloning in a codebase.
for example when there are many code clones with similar nested code structures while andifstatements the nested structure may perform better than other inductive biases.
the current simulation experiment is run on a lice by choosing one seed example from each group by randomly selecting annotations from the selected seed and by labeling a randomly chosen subset of returned results.
to mitigate the impact of random selection we repeat ten runs and report the average numbers.
in terms of external validity we assume that any user could easily annotate features and label examples.
however it is likely that a novice programmer might find it hard to identify important features.
to mitigate this threat to validity as future work we will investigate the impact of different expertise levels.
limitations and future work.
currently we generate facts based on structural and intra procedural control flow properties.
other types of analysis such as dataflow analysis or aliasing analysis could be used in identifying similar snippets.
in addition the query language itself can be extended to make it easier to capture the properties of desired code.
for example by introducing negations in the query language authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a user can specify atoms that should not be included.
there could be specializations that strictly require negations.
however in our experiments empirically we are always able to find a pattern without negations.
as mentioned in section iii d our learning process is monotonic and to learn a different query a user may need to start over.
to overcome this we may need backtracking and investigate new search algorithms that generalize and specialize the query in a different way.
viii.
r ela ted work code search and clone detection.
different code search techniques and tools have been proposed to retrieve code examples from a large corpus of data .
the most popular search approaches are based on text regular expressions constraints and natural language .
exemplar takes a natural language query as input and uses information retrieval and program analysis techniques to identify relevant code.
wang et al.
propose a dependencebased code search technique that matches a given pattern against system dependence graphs .
xsnippet allows a user to search based on object instantiation using type hierarchy information from a given example.
a lice differs from these search techniques in two ways.
first a lice allows a user to tag important features to construct an initial query.
second a lice uses active learning to iteratively refine a query by leveraging positive vs. negative labels.
alice is fundamentally different from clone detectors in two ways.
first while clone detectors use a given internal representation such as a token string and a given similarity threshold to search for similar code alice infers the commonality between positive examples encodes them as a search template and uses negative examples to decide what not to include in the template.
second alice presents the search template as a logic query to a user while clone detectors do not infer nor show a template to a user.
logic programming based techniques.
jquery is a code browsing tool based on a logic query language.
users can interactively search by either typing a query in the ui or selecting a predefined template query.
hajiyev et al.
present an efficient and scalable code querying tool that allows programmers to explore the relation between different parts of a codebase.
a number of techniques use logic programming as an abstraction for detecting code smells .
many program analysis techniques abstract programs as logic facts and use datalog rules including pointer and call graph analyses concurrency analyses datarace detection security analyses etc.
apposcopy is a semantics based android malware detector where a user provides a malware signature in datalog.
while a lice and this line of research both use logic programs as an underlying representation a lice does not expect a user to know how to write a logic query nor requires having a set of pre defined query templates.
instead alice allows the user to interactively and incrementally build a search query using active ilp .interactive synthesis.
some program synthesis techniques use input output examples to infer a program and interactively refine its output .
for instance codehint is a dynamic code synthesis tool that uses runtime traces a partial program sketch specification and a probabilistic model to generate candidate expressions.
interactive disambiguation interfaces aim to improve the accuracy of programming by example systems.
a lice is similar to these in leveraging interactivity but these do not target code search do not use ilp and do not assess the impact of iterative labeling and annotations.
machine learning.
active learning is often used when unlabeled data may be abundant or easy to come by but training labels are difficult time consuming or expensive to obtain .
an active learner may pose questions usually in the form of unlabeled data instances to be labeled by an oracle e.g.
a human annotator .
logan h is an ilp based active learning approach .
it learns clauses by either asking the oracle to label examples membership queries or to answer an equivalence query.
such oracles were first proposed by angluin in the query based learning formalism .
other approaches to inductive logic programming and relational learning are surveyed in de raedt .
alrajeh et al.
integrate model checking and inductive learning to infer requirement specifications .
other applications of ilp to software engineering include the work of cohen to learn logical specifications from concrete program behavior.
because ultimately our approach is not concerned with finding the right hypothesis and only with retrieving the right code examples it can also be thought of as a transductive learning problem .
ix.
c onclusion alice is the first approach that embodies the paradigm of active learning in the context of code search.
its algorithm is designed to leverage partial incremental feedback through tagging and labelling.
a lice demonstrates realtime performance in constructing a new search query.
study participants resonate with a lice sinteractive approach and find it easy to describe a desired code pattern without much effort.
extensive simulation shows that leveraging both positive and negative labels together can help achieve high precision and recall.
tagging features is also necessary for minimizing the size of initial search space.
our experimental results justify the design choice of a lice indicating that interactivity pays off labeling a few in a spread out fashion is more effective than labeling many at a time.
acknowledgment thanks to anonymous participants from the university of california los angeles for their participation in the user study and to anonymous reviewers for their valuable feedback.
this work is supported by nsf grants ccf ccf ccf ccf1837129 ccf iis onr grant n0001418 darpa grant n66001 and an intel capa grant.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.