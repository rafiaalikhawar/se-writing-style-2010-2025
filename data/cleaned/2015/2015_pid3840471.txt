cost efficient sampling for performance prediction of configurable systems atrisha sarkar jianmei guo university of waterloo canada email a9sarkar gjm gsd.uwaterloo.canorbert siegmund sven apel university of passau germany email norbert.siegmund apel uni passau.dekrzysztof czarnecki university of waterloo canada email kczarnec gsd.uwaterloo.ca abstract a key challenge of the development and maintenance of configurable systems is to predict the performance of individual system variants based on the features selected.
it is usually infeasible to measure the performance of all possible variants due to feature combinatorics.
previous approaches predictperformance based on small samples of measured variants butit is still open how to dynamically determine an ideal sample that balances prediction accuracy and measurement effort.
in this paper we adapt two widely used sampling strategies forperformance prediction to the domain of configurable systemsand evaluate them in terms of sampling cost which considers prediction accuracy and measurement effort simultaneously.
to generate an initial sample we introduce a new heuristic based onfeature frequencies and compare it to a traditional method basedon t way feature coverage.
we conduct experiments on six realworld systems and provide guidelines for stakeholders to predict performance by sampling.
i. i ntroduction selecting configuration options or features allows stakeholders to customize a configurable system in various ways giving rise to a multitude of system variants orconfigurations.
each feature can have an effect on the functional andnon functional properties e.g.
performance and cost of thesystem.analyzing performance is a critical step in the evaluationof software quality.
it helps developers in judging how farthe software matches the performance requirements.
however especially in the case of configurable systems this task is not trivial.
due to feature combinatorics the number of variantsof a configurable system often increases exponentially withthe number of features the system provides.
take sqlite one of the most widely used database engine for example only features give rise to over million variants .due to an often complex benchmarking process measuringeven a single system variant may be costly.
in the light ofthese problems recent approaches predict performance based on a small sample of measured variants.
siegmund et al.
proposed a measurement based prediction approach thatdetects performance relevant feature interactions using specificsampling heuristics that meet different feature coverage cri teria.
guo et al.
used a statistical learning technique toinfer performance prediction rules based on random samples.these approaches depend on certain sampling strategies i.e.
selecting a specific set of configurations to be measured thatprovide fixed termination criteria for the sampling process to achieve an acceptable prediction accuracy e.g.
.
however these sampling strategies cannot dynamically adjust the sampling process including termination in terms of thespecific characteristics of a given system so they may measuremore variants than necessary.
in this work we aim at a smart sampling strategy that dynamically determines a good sample for a given system.
a sample is good if it is small enough to decrease the measurement effort and large enough to increase the predictionaccuracy at the same time.
to quantify the goodness of asample we introduce a composite model of sampling cost which considers the measurement effort and prediction accur acy simultaneously.
we investigate two sampling strategieswidely used in data mining a classical technique calledprogressive sampling and a state of the art technique called projective sampling .
we conduct experiments on six real world configurable systems and compare the two samplingstrategies in terms of sampling cost.
furthermore we enhanceprojective sampling by incorporating two heuristics for initialsample generation a heuristic based on t way e.g.
way and3 way feature coverage as commonly used in combinatorialtesting and a novel heuristic based on feature frequencies.we empirically compare the performance of four projectivefunctions used in projective sampling on our six real worldconfigurable systems.
in summary we make the following contributions we adapt progressive and projective sampling strategies toperformance prediction of configurable systems and wecompare them in terms of the sampling cost balancing prediction accuracy and measurement effort.
we propose a heuristic based on feature frequencies to guide the initial sample generation of projective sampling.we compare it to a common heuristic based on t wayfeature coverage.
empirical results on six configurable systems demonstratethat projective sampling using the feature frequency heur istic is cost efficient.
that is it hits a sweet spot betweenprediction accuracy and measurement effort.
moreover we empirically identify the best projective function for projective sampling in our experimental setup.
the implementation and all experimental data are available at ii.
a b ird seyeview of performance prediction bysampling figure illustrates the general process of performance prediction by sampling.
it starts with an initial sample ofmeasured configurations which are used to build the pre diction model.
a good initial sample significantly reduces 30th ieee acm international conference on automated software engineering .
ieee updated 21the iterations of the entire prediction process.
state of the art approaches fix the size of the initial sample to the numberof features or potential feature interactions of a system .
however such a strategy might not be the optimal one as the number of features and their interactions can be high and at the same time an acceptable prediction accuracy mightbe achieved using a substantially smaller set of measuredconfigurations.
in our approach we use a combination ofrandom sampling and feature coverage heuristics to dynam ically build the initial sample.
in particular we propose afeature frequency heuristic for the initial sample generation and we compare it to a traditional technique based on t wayfeature coverage commonly used in combinatorial testing .
then we build prediction models using a statistical learning technique called classification and regression tree cart which has been demonstrated to be fast and accurate forperformance prediction of configurable systems .
in previous work prediction accuracy was the main evaluation metric used to estimate the utility of the predictionmodels .
in this paper we put forward the ideathat since there is a cost involved in measuring the sample ofconfigurations for building the prediction model a metric must consider both measurement effort and prediction accuracy to comprehensively evaluate the prediction model.
to this end wepropose sampling cost as the evaluation metric that quantifiesthe utility of a sampling strategy by taking not only predictionaccuracy into account but also measurement effort.
we willpresent the cost model in section iv.
most prediction models including the ones used in our study are built in an iterative manner.
the performance engineer measures a few configurations of a system i.e.
the sampling set which are divided into a training set and a testing set.
the training set is used to build a prediction model.
this model is then evaluated using an evaluation metricon the testing set.
if the value of the metric for this modelfalls within an acceptable range the process stops otherwisemore measurements are added to the sample for refining theprediction model.
this iterative process can be illustrated inthe form of a learning curve as shown in figure .
the learning curve of figure relates accuracy to the size of the training set.
the horizontal axis represents the size ofthe training set used to build the prediction model the verticalaxis shows the accuracy of the corresponding model calculatedusing the testing set.
an ideal learning curve has three distinctregions.
the first region has a steep incline indicating rapidincrease in accuracy when adding sample points.
the second optional region has a gradual increase in prediction accuracy.finally the third region saturates in a plateau where adding further sample points will not result in significant accuracy improvements anymore.
in traditional progressive sampling the smallest sample size for which the prediction model returnsacceptable values in terms of the evaluation metric is calledthe optimal sample size.
our goal is to design a smart sampling strategy that reaches the optimal sample size as fast as possible in terms of samplingcost.
to this end we define a stopping criterion based onthe sampling cost.
moreover we investigate two sampling strategies widely used in data mining progressive sampling and projective sampling which will be explained insection v. figure general process of performance prediction by sampling figure regions of the learning curve in dependence of the sample size steep incline gradual incline plateau n marks the optimal sample size iii.
d efinitions and running example we represent all features of a configurable software system as a setxof binary decision variables.
if a feature is selected in a configuration then the corresponding variable xis equal to and otherwise.
we denote the number of all features of a system as n that is x x1 x2 ... xn .
we represent each configuration of a system as an n tuple assigning value or to each variable in x. we denote all valid configurations of a configurable system as set x. the learning curve represents a mapping between the training set size and the corresponding accuracy.
the pair n n epsilon1n represents a point in the learning curve where n is the size of the training set and epsilon1ndenotes the prediction error of a model built with a training set of size n. assuming sn xas the training set of size n since we reuse samples from previous iterations the sample set snhas only one additional new configuration as compared to set sn .
for example one of our subject systems of section vii apache has a total of features and the total number of allvalid configurations is .
we follow the strategy used by guoet.
al.
to generate the valid configurations.
table i showsthe learning curve points for apache measured at an intervalof configurations.
at each step additional configurationsare measured and added to the training set.
the accuracy ofthe prediction model cart in our case is calculated at each step based on a testing set of a size equal to the training set randomly sampled from the set of configurations not measuredso far.
343table i learning curve points n for apache n sample set size epsilon1n relative error n epsilon1 nn epsilon1 n .
.
.
.
.
.
.
.
.
iv .
c ost model typically performance prediction models are evaluated on the basis of their prediction accuracy.
it is also common knowledge and apparent from the learning curve figure that usually a larger training set results in higher prediction accuracy.
however a large training set is often infeasible in terms of measurement effort.
thus any performance prediction model built for this purpose should be evaluated not only in terms of prediction accuracy but also in terms of measurement cost involved in building the training and testing sets.
weiss and tian introduced the concept of utility based sampling in which they combined the above two factors in the form of a composite cost model.
we have modified the original cost model of weiss and tian to include the cost incurred in measuring the testing set along with the training set totalcost cost measurement training cost measurement testing cost modelbuilding cost predictionerror we can simplify the above cost model by ignoring the cost incurred in building a performance prediction model as for cart which is used in our approach this cost is computationally insignificant compared to the other cost factors.
moreover we use a split between the training and testing sets in our sampling strategy i.e.
the size of training set is the same as the size of testing set .
therefore given a training and testing set of size neach we have the following cost function of n totalcost n 2n epsilon1n s r where2nis the number of sample configurations in the training and testing sets epsilon1nis the prediction error of this performance prediction model built with the nconfigurations s is the score set i.e.
the number of configurations whose performance value will be predicted by the model and ris a tuning parameter that controls the ratio of the cost incurred due to the prediction error to the cost of acquiring training samples.
for example r .
means that the cost to measure a configuration for the training sample is twice the cost arising from an incorrect prediction of the performance of a configuration.
the actual value of ris problem specific and shall be set by domain experts.
one way to do this is by basing the measurement effort and prediction cost in the same unit.
for instance companies often use man hours as the unit of choice to quantify investment efforts .
in such scenarios the value ofrcan be derived by calculating the ratio of investment required in man hours for the two factors.
an interesting characteristic of the cost function is that for a well behaved monotonically non decreasing learningcurve the cost function is convex see figure 3b .
this characteristic enables us to easily find the global minimum of the cost with respect to the sample size see section v .
v. p rogressive and projective sampling in this section we discuss two sampling strategies for building the training set independent of the prediction model.
we assume the learning curve of the prediction model to be well behaved that is monotonically non decreasing.
we argue that this is a reasonable assumption based on evidence from previous work and on our experience studying the data from our six subject systems.
there are local variations though where additional sample points sometimes result in a reduced prediction accuracy and we address them by using a moving average smoothing technique on the data points .
still for a wide range of sample sizes we see the learning curve to be monotonically non decreasing in our experiments.
a. progressive sampling progressive sampling is a popular sampling strategy that has been used for a variety of learning models .
the central idea is to use a sampling schedule n0 n1 n2 n3 ... nk where each niis an integer that specifies the size of the sample set that is used to build a performance prediction model at iteration i. based on how the size of the sample set in each iteration is calculated progressive sampling strategies can be divided into two kinds .
the first one is arithmetic progressive sampling where in each iteration we add a constant number of additional sample points to the training set according to the equation ni n0 i a. the second kind isgeometric progressive sampling where the sample set sizes are built in geometric progression according to the equation ni n0 ai.
the parameter ais a constant that defines how fast we increase the size of the sample set.
the primary difference between arithmetic and geometric progressive sampling is the number of sample points we add in each iteration and using geometric progressive sampling we can hit the plateau region in the learning curve in fewer iterations .
this is an advantage in cases where building the model is an expensive process.
for example as listed in table i if we set an acceptable prediction accuracy of the system to we can observe that the optimal sample size for that accuracy is .
if we start with an initial training set size of and add more configurations in each iteration reusing samples from preceding iterations the arithmetic sampling scheme needs iteraton iteraton iteraton iteraton iteraton that is 50measurements with the model being built times once in each iteration.
for geometric progressive sampling if we start with the same measurements and a minimum common ratio of the number of measurements needed is iteraton iteraton iteraton iteraton which is more than that of arithmetic progressive sampling although the model is built only times.
a gradient based b cost minimization figure two stopping criteria for progressive sampling for performance prediction of configurable systems the cost of acquiring training samples by measuring system configurations usually overrides the cost of building the performance prediction model cart in our case thus we consider only arithmetic progressive sampling in what follows.
stopping criteria for both arithmetic and geometric progressive sampling we need to decide when to stop sampling more configurations for measurement.
this is a critical step that needs to be performed in every iteration and to check whether the built prediction model has converged to an acceptable prediction accuracy.
next we discuss two common stopping criteria.
gradient based linear regression with local sampling lrls uses the gradient of the learning curve to detect convergence .
using this method we build additional models in the local neighborhood of niand determine their accuracy.
we use these additional sample points to fit a linear regression line and calculate the gradient as illustrated in figure 3a.
if the gradient is less than a certain threshold we stop sampling and designate the sample size used in that iteration to be the final sample size.
however this naive approach does not take the factor cost into account.
furthermore it has the drawback of possibly getting stuck in a local plateau of the learning curve.
cost minimization as the measurement cost is of primary importance in our case lrls based convergence detection is not well suited.
a more pragmatic approach is to use the cost function of equation to detect convergence of the learning curve .
using this method for each iteration we calculate the total cost of using the model with nisample measurements.
this problem now translates into an optimizafigure overview of projective sampling tion problem where the objective is to find nithat minimizes the cost function.
since the cost function is a convex function for well behaved learning curves cf.
figure 3b we can calculate a sample size that minimizes the total cost when we observe the first increase in total cost.
thus if the first increase in total cost is observed in iteration i for a sample size of ni then the optimal sample size guaranteeing minimum cost is ni .
as our evaluation of prediction models is based on cost we use the cost minimization stopping criterion for progressive sampling when comparing it to projective sampling which we describe next.
b. projective sampling one of the weaknesses of progressive sampling is that the prediction model can converge only after several iterations with a large sample size and there is no way to determine this unless we have actually built the model.
this defeats the entire purpose of the prediction procedure as there is a risk that even after spending resources in running benchmark tests for several configurations the cost and accuracy of the model at the point of convergence might not be acceptable for the user.
projective sampling addresses this problem by approximating the learning curve using a minimal set of initial sample points thus providing stakeholders with an estimate of the cost projection of the entire prediction process thus helping them to decide whether to adopt the prediction model for their system.
we define the cost incurred in generating this projected learning curve as the decision cost .
in figure we provide an overview of the steps involved in projective sampling.
projective sampling starts with an empty training set adding a constant number n of configurations to the training set in each iteration.
there is no minimum constraint to the value of the constant n apart from being greater than and we have seen from our experiments that even a value of can give good results.
in each iteration we build the model and calculate the accuracy this way generating a sample point for the learning curve.
the size of these initial sample points or the number of iterations the algorithm should run varies with the number of features of the system and we use a novel feature frequency heuristic for sample generation section vi to build this set.
this initial set of sample points represents a partial learning curve.
in the next step we search for a best fit function that can extrapolate the remaining learning curve.
learning curves for black box performance prediction methods such as cart exhibit usually good correlation with one of four different 345table ii projective functions of learning curves name equation optimal sample size logarithmic err n a b.log n n r s b weiss and tian err n a bn n n radicalbig r s b power law err n anbn r s a b b exponential err n abnn logb parenleftbig r s a lnb parenrightbig types of projective functions as shown in table ii.
parameters aandbare the co efficients of the projective functions and s andrare the score set and the cost ratio as defined in equation .
using the information from the initial sample points we follow the approach proposed by last in selecting the projective learning function that exhibits highest correlation with the sample points.
once we have determined the best fit function that can approximate the learning curve accurately we can calculate the coefficients of the projected function using the least squares method .
the optimal sample size can be defined as the size of the training sample that minimizes the cost function of equation ensuring the most optimal tradeoff between measurement cost and prediction accuracy.
in projective sampling we have knowledge about the projected learning curve which gives us an estimate of the prediction error as a function of the sample size.
if we substitute the value of err in the cost equation we can calculate the total cost of the prediction process as a function of the sample size n. figure 3b shows an example of cost versus sample size for apache.
we can see from the figure that the cost function is convex which holds for any monotonically non decreasing learning curve .
since all the candidate projective learning curves follow this property the first derivative d cost n dn of the cost function is a global minimum.
the solution of this equation gives us the sample size n that guarantees minimum cost.
table ii shows the values of n cf.
fig.
for the four different learning curve equations in terms of the cost equation variables of equation .
in the following sections we use n to represent the optimal sample size that minimizes the cost function of equation .
vi.
i nitial sample genera tion an important step in projective sampling is the generation of the initial sample points that are used to project the learning curve.
given a sample point iof a learning curve our objective is to sample a set 1 2 ... such that can be used to generate the projected learning curve accurately.
there are two key aspects that need to be considered when designing sampling strategies to build this set of sample points.
first one of the advantages of projective sampling is that it gives stakeholders an estimate of the optimal sample sizen with minimal investment in terms of measurement cost.
however to generate this projected curve they need to measure configurations and build the initial sample set.
as defined in the last section the cost of measuring these configurations is the decision cost.
thus the size of the set or the value of is critical.
to make the sampling strategy cost efficient the value of should be less than n otherwisetable iii coverage of selected features xi and deselected feature xi conf .
features x1x2x3 ..xi .. ..xn wewould end up measuring more than the optimal number of configurations.
second the accuracy of the projected learning curve matters.
it is important that the initial sample set should be able to produce a projected learning curve that approximates the real learning curve accurately.
since the size of this initial set needs to be kept small there is a high probability that a suboptimal strategy in generating these initial sample points will result in a projective function that is not an accurate reflection of the learning behavior of the model.
this can have a cascading effect throughout the entire sampling process and result in a value of sample size n that is not optimal in terms of cost.
to generate the initial sample set for projective sampling we propose a heuristic based on feature frequencies for a configurable system users can select or deselect features and there is typically a relation between features and the overall performance of the system in terms of throughput or execution time .
as a consequence the performance of the system may vary substantially depending on whether a certain feature is selected or not.
for example in a web application the system might slow down if logging is enabled because it takes extra time to perform i o operations involved in directing messages to a log file.
thus the first requirement of a good representative sample is that the sample configurations in our initial sample set should have each feature selected at least once.
also since feature deselection can have an influence on the performance values too it is important that the sample configurations in should have each feature deselected at least once.
these constraints on feature selection and deselection apply only to optional features mandatory features are active for all the configurations in the sample set.
in terms of our problem definition section iii the set s represents the set of distinct configurations in the initial sample set .
for a given feature i the frequency of this feature is defined by the following equation summationdisplay j 1xi j wherexi is a boolean variable representing a feature ibeing selected or deselected for a configuration j. table iii provides an example set s of sample configurations and their corresponding feature selections.
the initial sample set should exhibit a good correlation with the projected learning curve.
this is because the projected 346table iv feature frequency table t. row frequency of selected features.
row frequency of deselected features features x1x2x3 ..xi .. .. xn selected .. .. .. deselected .. .. .. learning curve generated using the sample set is indicative of the learning progress of the prediction model with respect to the sample size.
if the correlation is low the projected curves might not model the learning behavior of the system accurately.
prior work has shown that random sampling can be used for accurate performance prediction of configurable systems .
in our approach we use a combination of incremental random sampling and a feature frequency heuristic to build the initial sample.
to keep track of feature frequencies our algorithm uses a2 nfeature frequency table t table iv where the columns of the table represent noptional features of the system.
each cell in the first row contains the number of configurations in the training set for which the corresponding feature is selected the second row contains the number of configurations in the training set for which the feature is deselected.
table iv shows the feature frequencies for the configurations in table iii.
for example feature x2is selected in and deselected in configurations.
algorithm 1generate configurations for projective sampling while curr freq thresh freq and mean err err thresh or curr do c rand randomly generate a configuration c scurr scurr c update t update the feature freq table t epsilon1curr cart scurr epsilon1curr meanerr mean curr curr epsilon1 curr curr add the current learning curve sample point curr t o curr freq min t t curr curr end while algorithm defines the steps involved in the generation of .
the most important parameter in the algorithm is thresh freq.
this parameter sets a lower bound on the values of the feature frequency table which means that the sample configurations used to generate the sample points for the learning curve should have all the features selected and deselected for at least thresh freqtimes.
this threshold makes sample generation robust and diminishes the effect of any outliers in the sample configurations.
the second parameter is the error threshold err thresh which allows the algorithm to stop sampling if the models built so far have an acceptable accuracy.
for some systems the performance may be constant irrespective of the variability among configurations which means that the features do not have a significant influence on performance.
in these systems we can generate a prediction model with high accuracy using a very small set of configurations.
keeping a threshold value on the mean error avoids unnecessary measurements since the current performance prediction model already yields a sufficient accuracy.
the third parameter is thetable v overview of the six subject systems.
lang language loc lines of code x number of all valid configurations n number of all features system domain lang.
loc x n apache web server c llvm compiler c x264 encoder c berkeley db database c berkeley db database j ava sqlite database c number of sample points that we need to project for a nonlinear learning curve which is at least.
in the first step the algorithm randomly samples a valid configuration and adds it to the current sample set scurr lines and .
the feature frequency table tis then updated by calculating the number of features that are selected and deselected in scurr line .
in the next step line the cart prediction model is built using the current configuration setscurr and the prediction error epsilon1curr is calculated.
the mean prediction error which helps in evaluating the accuracy of the cart models built till this point is calculated in line .
using the set scurr and epsilon1curr we obtain a sample point curr for the learning curve.
the algorithm then adds the sample point curr generated in the current iteration to the set line .
v ariable curr freqholds the current minimum value of feature selection and deselection frequencies in t.i nt h e end the set forms the final set of sample points we need to project the learning curve.
the performance profile of a system may not depend solely on individual features but also interactions among features.
for example in a web application the performance may take a hit when caching is turned off and the application performs blocking reads .
effects of interacting features have been studied extensively for fault localization in the field of combinatorial testing where it has been seen that covering a way and way feature interactions can detect and of defects in a software .
effects of feature interactions on performance prediction have also been studied too .
whereas a strategy based on t way feature coverage might be an effective method to generate training samples for a prediction model our objective is fundamentally different.
our primary objective is to generate sample points that estimate the learning behavior of the prediction model.
in section viii we compare our method to a strategy based on way and way feature coverage and show that feature frequency based sample generation is more effective than one based on t way feature coverage.
vii.
subject systems to compare progressive and projective sampling we evaluate the two sampling strategies in terms of the sampling cost on six real world configurable software systems.
the six subject systems used in our evaluation include apache http server is the most popular web server on the internet.
in our experiments we consider features resulting in valid configurations.
berkeley db c is an embedded key value based database library that provides scalable high performance database management services to applications.
some of the applications using berkeley db are subversion bitcoin and sendmail.
in our experiments we consider features resulting in valid configurations.
berkeley db java is a re implementation of berkeley db in java with features and valid configurations.
llvm is a popular compiler and virtual machine framework used for a variety of languages.
we consider features and valid configurations.
sqlite is the most popular lightweight relational database management systems today.
it is used by several browsers and operating systems as an embedded database.
we consider features that give rise to more than million valid configurations.
x264 is a video encoding library that encodes video streams to h. mpeg a vc format.
it is used by several video converters and media players such as vlc.
x264 has features and valid configurations.
more information on these systems can be found as a part of spl conqueror project table v provides an overview of the subject systems.
viii.
e v alua tion by conducting experiments on six real world configurable systems we aim at answering the following research questions rq1 between progressive and projective sampling which is cost efficient?
section viii a rq2 which is the best projective function that fits the learning curve of a configurable system?
section viii b rq3 is a heuristic based on t way feature coverage or feature frequencies better for the initial sample generation of projective sampling?
section viii c a. rq1 progressive vs. projective since cost efficiency is the primary determinant for judging the effectiveness of a sampling strategy we compared the total cost of sampling and prediction according to equation for all six subject systems using progressive and projective sampling as shown in table vi.
for both progressive and projective sampling we calculated the cost and accuracy of building prediction models with the optimal sample set size n .
the value of n is determined through the respective sampling techniques.
in the case of projective sampling the accuracy shown is the real accuracy calculated after the prediction model is built with n samples and not the accuracy derived through the projected learning curve at n .
to calculate the optimal sample size we have set the size of the score set s t o be proportional to the total number of configurations of the system x .
specifically the size of sis set to x .
the tuning parameter r which controls the cost ratio between measurement effort and prediction accuracy is set to .
this means that we equally weigh the cost incurred in measuring samples and the cost due to prediction error.
we can see from table vi that for all the six subject systems projective sampling is more cost efficient than progressive sampling.table vi cost and accuracy of progressive and projective sampling cost accuracy progressive projective progressive projective apache berkeley db c berkeley db java llvm sqlite x264 although we prioritize cost efficiency over the absolute accuracy we also compared the two strategies based on accuracy to validate whether the accuracy of the model is acceptable.
the prediction accuracy was calculated based on a testing set that was of the same size as the training set.
also we ran the experiment times and the prediction accuracy reported was the average of the runs.
we see in table vi that projective sampling outperforms progressive sampling in terms of cost and also in terms of accuracy.
for berkeley db c progressive sampling gets stuck in a local optimum and produces a very low accuracy of .
progressive and projective sampling are comparable in terms of accuracy and cost for apache and berkeley db java where the learning curve behaves ideally with no jumps or temporal variations.
however for all other learning curves projective sampling is considerably more cost efficient than progressive sampling.
decision cost.
one of the key benefits of projective sampling is its ability to give a prognosis of the learning behavior of the prediction model.
in other words stakeholders can use the projected learning curve to obtain an estimate of the prediction accuracy.
they can check whether this value falls under an acceptable range and decide whether to use the prediction model.
the cost incurred in taking this decision is the same as the cost incurred in building the initial configuration set which is in our case the size of the set .
table vii shows this cost for each of the six systems.
we can see that is significantly smaller than the total cost incurred when the final prediction model is built with an optimal training set size.
b. rq2 comparison of learning curves figure shows the learning curves for each of the six subject systems generated by progressive and projective sampling.
the black squares denote the initial learning curve sample points generated using the feature frequency heuristic.
since the parameter err thresh controls the number of iterations of the sampling algorithm its value affects the size of .
we have set the value of err thresh to .
the only exception is sqlite where the value is set to .
the reason is that for sqlite we observed a high prediction accuracy from the cart model with a very small sample size and thus we set a lower value for effective comparison of featurefrequency sampling.
we have empirically set the value of the parameter thresh freqat for all subject systems.
for projective sampling we choose the projective function that has the highest correlation with the initial set and for which the optimal number of samples n generated by that curve is less than the total number of configurations of the system x .
the optimal sample size for a class of projective functions can be 348apache sample sizeaccuracy 100berkeley db c sample sizeaccuracy 0berkeley db java sample sizeaccuracy llvm sample sizeaccuracy 100sqllite sample sizeaccuracy 100sqlite sqlitex264 sample sizeaccuracy figure learning curves for progressive and projective sampling calculated from the equations in table ii.
the upper bound on the optimal sample size helps us to eliminate unrealistic sample sizes and thus to narrow down our set of candidate projective functions.
for comparison apart from the chosen projective function for each system figure also shows all the functions whose correlation value was greater than .
.
the triangular wedges show the optimal sample sizes n for each of the curves.
for both progressive and projective sampling these values are generated using the cost minimization techniques described in section v. we can see from figure that exponential functions are the most robust among all the projective functions when used to fit the learning curve.
they exhibit correlations greater than .
for all the six systems and they have been selected as projective functions for out of the systems apache berkeley c berkeley java sqlite .
for the remaining two systems a logarithmic function llvm and a power law function x264 have been selected.
the effectiveness of exponential functions is corroborated by the fact that they had overestimated the accuracy of the learning curve at the optimal sample size by only with a standard deviation of .
for the generation of a cost effective optimal sample size n progressive sampling is sensitive to local variations in the learning curve and getstable vii comparison of decision cost size of and total cost apacheberkeley db c berkeley db java llvm sqlite x264 decision cost total cost table viii size of t way vs. feature frequency apacheberk eley db c berkeley db java llvm sqlite x264 way way feature frequency stuck in a suboptimal region for example in berkeley c. in contrast projective sampling generates realistic values of the optimal sample set which are high in accuracy as well as cost efficient.
349table ix t way vs. feature frequencies r pearson s correlation coefficient.
r p value total cost way .
.
way .
apache feature frequency .
way .
.
way .
berkeley db c feature frequency .
way .
.
way .
berkeley db java feature frequency .
way way .
.
llvm feature frequency .
.
way .
way .
sqlite feature frequency .
way .
.
way .
x264 feature frequency .
c.rq3 t way vs. feature frequency in this section we compare the t way heuristic to select the initial sample set which is the de facto standard in combinatorial and product line testing to our proposal of a featurefrequency heuristic.
we mentioned earlier that the objective of the initial sample generation is to model the learning behavior of the system and achieve a high correlation between the initial dataset and the projective learning curve.
thus the first criterion we use to evaluate the t way and feature frequency heuristics is the degree of correlation between the initial dataset and the chosen projective function for each system.
the second criterion is based on the total cost of the prediction process.
ideally the correlation should be close to and the total cost calculated at optimal sample size n be minimal.
we use the tool jenny to generate way and way feature coverage configurations used as the initial sample points .1table ix shows the comparison between way way and feature frequency sampling in terms of correlation pearson correlation coefficient and pvalue and the total cost.
we can see in table viii that way feature coverage scores worse than both way and feature frequency in terms of both correlation and cost.
this is due to the number of sample configurations or the size of .
the size of is generally smaller when a way heuristic is used compared to a way or feature frequency heuristic.
feature frequency sampling induces comparable cost to way sampling for two systems apache and berkeley c .
however the correlation value for the feature frequency heuristic in both systems is higher.
for the rest of the systems we can see that the featurefrequency heuristic is better in terms of both correlation and cost.
these results along with the fact that the size of is smaller using the feature frequency heuristic as compared to the way heuristic for all the six systems shows us that 1jenn y feature frequency heuristic can generate more accurate learning curves with a lower numbers of measurements.
ix.
t hrea ts to validity to increase internal validity we performed automated random sampling this way avoiding misleading effects of specifically selected samples for building prediction models.
we randomly selected samples of specific sizes e.g.
from the entire population of each subject system.
we repeated each random sampling times for training and testing the prediction models and we reported only the mean of cumulative results for the evaluation metrics such as sampling cost prediction accuracy and correlation coefficients.
in addition we used a widely accepted tool for t way feature coverage generation to make sure that the generated sets are correct.
in our experiments the value of the tuning parameter r which controls the cost ratio between measurement effort and prediction accuracy is set to .
however the value of this parameter is domain specific and can be set by a domain expert.
nevertheless in cases where the precise value of this ratio is unknown giving equal weights to both the cost factors seems to be a fair assumption.
in addition we repeated our experiments with multiple values of the parameter rand observed our results to be robust for these small ranges.
however we leave a systematic sensitivity analysis to future work.
we have defined our cost model based on prior research and we argue that the measurement cost of both training and testing samples needs to be incorporated in the cost model.
still there might be alternative definitions of cost models which are different than ours and equally valid.
to increase external validity we used a public dataset consisting of six real world systems covering different domains with different sizes different configuration mechanisms and different programming languages.
all the subject systems 350used in our case study are deployed and used in real world scenarios.
however we are aware that the results of our experiments are not automatically transferable to all other configurable systems but we are confident that we controlled this threat sufficiently.
x. r ela ted work a. performance prediction recent approaches have used a combination of measurement and prediction techniques to evaluate the performance of software systems.
among the performance prediction models it is important to distinguish between two categories of models found in literature.
the first type of models which can be referred to as white box models are built early in the life cycle by studying the underlying design and architecture of the software system in question.
the idea is to identify performance bottlenecks early so that developers can take corrective actions.
queueing networks petri nets and stochastic process algebras are commonly used for this task .
the second type of models called black box models do not make any assumption on the design and architecture effectively treating the system as a black box.
in this paper we use black box predictive models.
guo et al used cart to predict the performance of configurable systems.
on the same dataset as ours they observed an average accuracy of .
we build on their prediction model and we study how to determine the minimum sample size rather than proposing a new learning technique.
yi et al.
proposed an algorithm based on fourier learning for the performance prediction of configurable systems.
their method provides theoretical guarantee of prediction accuracy and confidence level but it follows typical random sampling that is terminated only in terms of prediction accuracy.
westermann et al.
analyzed various statistical inference techniques to predict the performance of configurable systems.
they also analyzed three different configuration generation methods including random breakdown adaptive equidistant breakdown and adaptive random breakdown.
in their work they do not take the measurement cost into account.
we use a composite cost function to guarantee an optimum between accuracy and measurement cost.
our approach has further the advantage of giving stakeholders an early prognosis of the prediction model through a minimal decision cost.
siegmund et al.
used a measurement based technique to predict performance by detecting feature interactions.
in follow up work they consider also numeric configuration options by combining experimental designs with binary option sampling.
the number of samples needed to be measured using their approach is higher than other prediction models such as cart due to their focus on explaining the performance of a system i.e.
making the influences of all features and their interactions explicit which is a different goal.
b. sampling strategies provost et al.
introduced the idea of progressive sampling and proved that geometric progressive sampling is more efficient than arithmetic progressive sampling when model building cost is high.
however in the case of performance prediction of configurable systems the cost of buildingprediction models such as cart is comparatively low but the measurement cost is often high.
in this case arithmetic progressive sampling is more efficient.
weiss and tian combined measurement cost and accuracy into a single composite cost function and used it to evaluate the prediction process.
we use their cost functions in our approach.
they evaluated only progressive sampling approaches and did not develop a sampling strategy to minimize the cost.
last used the cost function proposed by weiss and tian and proposed projective sampling which guarantees a minimum cost and provides a numeric value for the optimal sample size.
we adapted their approach of projective sampling for performance prediction models.
however they did not provide guidelines on how to generate a good initial sample for projecting the learning curve.
we solve this problem using a heuristic based on feature frequencies and compare it to a typical heuristic based on t way feature coverage.
xi.
c onclusion we adapted two sampling strategies progressive sampling and projective sampling for the performance prediction of configurable systems.
to evaluate and compare the two sampling strategies we use the sampling cost which considers the prediction accuracy and the measurement effort simultaneously.
in addition we used two heuristics based on feature frequencies and t way feature coverage to generate the initial sample in projective sampling.
we conducted empirical studies on six real world configurable systems to determine an ideal sampling strategy for performance prediction of configurable systems.
our key findings are as follows.
first projective sampling is better than the progressive sampling in terms of both sampling cost and prediction accuracy but it suffers from a dependency on a proper initial sample and projective function.
to obtain a good initial sample for projective sampling our heuristic based on feature frequencies is more effective than the standard approaches based on way and way feature coverage.
among four common projective functions the exponential function is the best to fit the learning curves of our subject systems accurately and robustly.
furthermore we recommend arithmetic progressive sampling instead of geometric progressive sampling because measuring the performance of configurations is often more costly than learning a prediction model based on the training set.
our empirical findings are meant to help stakeholders in designing sampling strategies for performance prediction.
in the future we will perform a systematic sensitivity analysis of the cost ratio between measurement effort and prediction accuracy in our cost model.
acknowledgment this work has been funded by the canadian natural sciences and engineering research council ontario research fund and german research foundation ap ap and ap206 .
351references s. balsamo a. di marco p .
inverardi and m. simeoni modelbased performance prediction in software development a survey ieee transactions on software engineering vol.
no.
pp.
.
b. w. boehm software engineering economics .
prentice hall englewood cliffs nj vol.
.
m. b. cohen m. b. dwyer and j. shi coverage and adequacy in software product line testing in proceedings of the international symposium on software testing and analysis issta .
acm pp.
.
l. j. frey and d. fisher modeling decision tree performance with the power law in proceedings of the international workshop on artificial intelligence and statistics pp.
.
j. guo k. czarnecki s. apel n. siegmund and a. wasowski v ariability aware performance prediction a statistical learning approach in proceedings of the international conference on automated software engineering ase .
ieee pp.
.
w. h ardle smoothing techniques with implementation in s .
springer science business media .
g. h. john and p .
langley static versus dynamic sampling for data mining.
in proceedings of the conference on knowledge discovery and data mining kdd .
acm pp.
.
d. r. kuhn r. n. kacker and y .
lei practical combinatorial testing nist special publication vol.
no.
p. .
r. kuhn y .
lei and r. kacker practical combinatorial testing beyond pairwise it professional vol.
no.
pp.
.
m. last improving data mining utility with projective sampling inproceedings of the conference on knowledge discovery and data mining kdd .
acm pp.
.
a. lazarevic and z. obradovic data reduction using multiple models integration in principles of data mining and knowledge discovery .
springer pp.
.
r. leite and p .
brazdil improving progressive sampling via metalearning on learning curves in proceedings of the european conference on machine learning ecml .
springer pp.
.
h. liu and h. motoda on issues of instance selection data mining and knowledge discovery vol.
no.
pp.
.
d. c. montgomery g. c. runger and n. f. hubele engineering statistics .
john wiley sons .
f. provost d. jensen and t. oates efficient progressive sampling inproceedings of the conference on knowledge discovery and data mining kdd .
acm pp.
.
n. siegmund a. grebhahn s. apel and c. k astner performanceinfluence models for highly configurable systems in proceedings of the international symposium on f oundations of software engineering fse .
acm .
n. siegmund s. s. kolesnikov c. k astner s. apel d. batory m. rosenm uller and g. saake predicting performance via automated feature interaction detection in proceedings of the international conference on software engineering icse .
ieee pp.
.
g. weiss and y .
tian maximizing classifier utility when there are data acquisition and modeling costs data mining and knowledge discovery vol.
no.
pp.
.
d. westermann j. happe r. krebs and r. farahbod automated inference of goal oriented performance prediction functions in proceedings of the international conference on automated software engineering ase .
ieee pp.
.
c. yilmaz a. s. krishna a. memon a. porter d. c. schmidt a. gokhale and b. natarajan main effects screening a distributed continuous quality assurance process for monitoring performance degradation in evolving software systems in proceedings of the international conference on software engineering icse .
ieee pp.
.
y .
zhang j. guo e. blais and k. czarnecki performance prediction of configurable software systems by fourier learning in proceedings of the international conference on automated software engineering ase .
ieee .