actionnet vision based workflow action recognition from programming screencasts dehai zhao zhenchang xing research school of computer science australian national university australia dehai.zhao zhenchang.xing anu.edu.auchunyang chen xin xia faculty of information technology monash university australia chunyang.chen xin.xia monash.eduguoqiang li school of software shanghai jiao tong university shanghai china li.g sjtu.edu.cn abstract programming screencasts have two important applications in software engineering context study developer behaviors information needs and disseminate software engineering knowledge.
although programming screencasts are easy to produce they are not easy to analyze or index due to the image nature of the data.
existing techniques extract only content from screencasts but ignore workflow actions by which developers accomplish programming tasks.
this significantly limits the effective use of programming screencasts in downstream applications.
in this paper we are the first to present a novel technique for recognizing workflow actions in programming screencasts.
our technique exploits image differencing and convolutional neural network cnn to analyze the correspondence and change of consecutive frames based on which nine classes of frequent developer actions can be recognized from programming screencasts.
using programming screencasts from y outube we evaluate different configurations of our cnn model and the performance of our technique for developer action recognition across developers working environments and programming languages.
using screencasts of developers real work we demonstrate the usefulness of our technique in a practical application for actionaware extraction of key code frames in developers work.
keywords programming screencast action recognition deep learning i. i ntroduction screencasting is a technique to record computer screen output at a specific time interval e.g.
second .
in the context of software engineering programming screencasts provide a direct record of both a developer s workflow actions and the application content involved in programming tasks e.g.
typing code scrolling content switching windows .
they not only provide the data basis to study developer behaviors and information needs in software engineering research but they are also a common content carrier for disseminating software engineering knowledge .
in the application of studying developers programming screencasts provide direct observational data as opposed to survey and interview data that rely on self reporting .
according to a recent survey of data collection methods used in the papers that study developer behaviors of these papers rely on programming screencasts to study a wide range of software engineering activities such as corresponding authorfeature location debugging program comprehension tool design and distributed programming .
in the application of disseminating software engineering knowledge programming screencasts offer livecoding experience which is absent in text based tutorials.
millions of programming tutorials are published on y outube and are watched by millions of developers.
seeing a developer s coding in action for example how changes are made to source code step by step and how errors occur and are fixed can be more valuable than text based tutorials .
as all operating systems provide a simple api to record computer screen screencasting does not need applicationspecific support which makes it very easy to deploy compared with software instrumentation that requires sophisticated accessibility or ui automation apis .
however this easy to deploy convenience comes with a high barrier of video analytics.
as a screencast is a sequence of screencaptured images one cannot study the developer behaviors or harness the programming knowledge in the screencast until the workflow actions and the application content captured in the screencast can be effectively extracted .
it is very time consuming to manually identify the workflow actions and the application content in a screencast .
this limits the scalability of behavioral research on software developers.
existing automatic techniques focus on only the content extraction from screencasts using optical character recognition ocr techniques but ignore the workflow actions i.e.
actions that the developer takes to accomplish programming tasks.
ignoring the workflow actions makes programming screencasts less valuable in studying developer behaviors because we lose the dynamic aspects of the developer s coding practice.
for example we cannot see what actions lead to program errors and how the developers fix the errors or what is the bottleneck for code search.
ignoring the workflow actions in programming video tutorials also limits the ways that developers can search and navigate the video tutorials resulting in less effective learning experience .
in fact extracting content without considering workflow actions itself is problematic resulting in noisy extracted content that will negatively affect the effective use of programming screencasts in studying developer behaviors or learning programming knowledge.
for example as shown in the frames ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fxandfx in fig.
while the developer is typing the code a popup window appears to suggest apis.
this popup window makes the current screenshot very different from the previous screenshot and existing frame similarity based methods will select fx for content extraction.
first the popup window likely contains apis irrelevant to the developer s current code.
second it blocks the actual code in the main window.
third the content in the popup window mixed together with the main window content will degrade the ocr quality.
as another example the developer selects a piece of code see the frames fyandfy 1in fig.
.
this also results in enough screen changes which again triggers the content extraction by frame similarity based methods.
the selected code has different background and foreground color from other code which will degrade the ocr quality.
however the code does not actually change which means that the content extraction for fy 1is completely unnecessary.
to overcome the limitation of the content centric analysis of programming screencasts this paper presents a deep learning based computer vision technique to automatically recognize developer actions from programming screencasts.
in this work we focus on three categories of nine actions see table i frequently observed in programming work.
we do not limit the action occurrences in only ides as programming work may involve many other software tools e.g.
web browser interactive shell .
our approach first uses image differencing techniques to detect the change regions between the two consecutive frames resulting from developer actions.
the detected change regions are then fed into a convolutional neural network cnn model to extract abstract image features which will then be fed into a softmax classifier to predict the action that most likely causes the screen changes.
we collect programming screencasts for python and for java from the popular programming playlists on y outube.
these playlists are produced by different developers and use different development tools.
through intraplaylist inter playlist and inter programming language experiments we show that our approach can be effectively trained and deployed in very diverse working environment and programming language settings f1 score .
on par with the accuracy of popular human action recognition techniques .
we further collect hours of screencasts of two developers real work and ask the developers to identify keycode frames in the screencasts.
we demonstrate that actionaware extraction of key code frames identifies key code frames that correspond to the developers annotations much better than existing action agnostic methods.
we make the following contribution in this paper to the best of our knowledge this is the first work to recognize workflow actions from programming screencasts.
we extract finer grained workflow actions than previous work on the manual analysis of developer behaviors.
to extract workflow actions we propose a two stage deep learning based method.
it first detects screen changes by image differencing and then recognizes developer actions from screen changes with a cnn based model.table i theca tegory of actions to berecognized in this work general category id description control cursor mousec1 move cursor by keyboard c2 move mouse over text region c3 move mouse over non text region edit contentc4 enter text e.g.
char word paragraph c5 delete text e.g.
char word paragraph interact with appc6 trigger popups e.g.
menu tooltip c7 scroll text e.g.
code console output c8 select text e.g code console output c9 switch window within or across app c10 others e.g.
resize window click button through extensive experiments we not only confirm the effectiveness and generality of our method but also demonstrate its usefulness for action aware extraction of key code frames in programming screencasts which can enable more accurate code extraction or video search.
ii.
p roblem sta tement a programming screencast is a sequence of time stamped screenshots i.e.
computer screen outputs recorded at a specific time interval while the developer is working on a computer.
each screenshot is a screen image and is referred to as a frame in the screencast.
the interaction between the developer and the development tools during screencasting results in the visual changes on the computer screen over time for example typing a char results in the typed char appearing on the screen selecting a word results in the change of the foreground and background color of the selected word.
when people watch the screencast they can manually recognize a sequence of developer actions from the screen changes in consecutive frames.
the goal of this work is to develop a computer vision based technique to automate the recognition of developer actions in programming screencasts.
as illustrated in fig.
our technique actionnet takes as input a sequence of frames in a programming screencast and automatically produces as output a sequence of actions that the developer performs on the computer during screencasting.
developer actions can be at various levels of abstraction .
for example an edit file activity can comprise primitive actions such as enter text select text delete text scroll text .
a browse web activity can comprise primitive actions such as enter query scroll web page select web page content .
a debug code activity can comprise primitive actions such as scroll code switch file .
although highlevel activities differ greatly in goals and software involved they share primitive actions in the process of human computer interaction.
in this work we decide to recognize primitive actions in programming screencasts.
the primitive actions can be aggregated into high level activities by rule based or machine learning techniques .
we define nine classes of developer actions to be recognized in programming screencasts based on our own programming experiences the survey of hci literature and the empirical coding of frequent primitive actions in the randomly selected programming screencasts on y outube.
these nine classes of primitive actions are frequent actions in authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
an illustration of workflow action recognition in programming screencasts programming work and they fall into three general categories see table i .
these primitive actions can occur in ides interactive shell web browsers and text editors that are commonly used in software development.
except for mouse and cursor actions that can be instrumented using simple operatingsystem level apis instrumenting other classes of actions will require accessibility or ui automation apis we classify all other less frequent hci actions e.g.
resize window click button as an others class c10 .
note that we do not consider no action class because no action can be easily determined when the consecutive frames remain unchanged by image differencing see section iii a .
as a programming screencast often contains many no action periods considering no action class will superficially inflate the model performance but has no practical meaning.
recognizing developer actions on computer in a programming screencast differs significantly from recognizing human actions in a natural scene video.
human actions in a natural scene such as press key move mouse have a physical duration and they cause changes in at least several frames.
in contrast the computer screen changes resulting from the developer actions on computer are computer rendered and they happen instantly from one frame to next.
therefore we must be able to recognize developer actions with only the information in two consecutive frames.
we formulate our task as a multi class classification problem which predicts the probability of the above classes primitive actions including others given the two consecutive frames in a programming screencast.
iii.
a pproach fig.
presents the main steps of our approach.
given a screencast with nframes our approach analyzes the two consecutive frames fiandfi i n sequentially from the beginning to the end of the screencast.
first it uses image differencing technique to detect the largest change region diffi i in between the two frames fiandfi .
then it crops the screen region ri diffi i andri diffi i onfiand fi 1respectively with respect to diffi i. next the cropped screen regions are fed into a cnn based feature extractor that is trained to extract image features of the correspondence and change between the two cropped screen regions.
finally based on the extracted image features a softmax classifier is trained to predict the probability of the classes of primitive actions including others that most likely cause the screen changes from fitofi .a.
change region detection by image differencing a naive solution to our problem would be to predict developer actions directly from the two consecutive frames.
however this solution will not be effective because many developer actions such as typing a char or moving the mouse pointer result in very small screen changes compared with the size of the whole screen.
if we take the whole frames as input the important features in small screen changes would be too weak to recognize the corresponding developer actions.
this is especially the case when extracting image features using deep neural network .
therefore we decide to detect the change regions between the two frames and then use these change regions for action recognition.
we adopt the mature computer vision technique that is widely used to detect change regions between two frames .
specifically we use scikit image apis to detect change regions in the two frames fiandfi .a s illustrated in fig.
we first compute the structural similarity index between the same position pixels of the two frames.
structural similarity compares local patterns of pixel intensities that have been normalized for luminance and contrast.
based on the pixel structural similarities a black and white image can be obtained in which white means the same pixels and black means the different pixels between the two frames.
we find the bounding boxes of the black pixels which identify the change regions between the two frames.
we then crop the screen regions on fiandfi 1with respect to the identified change regions.
one technical challenge in using change regions for action recognition is that there can be more than one change regions between two frames.
for example the screencast may record system clock update in addition to the screen changes resulting from developer actions.
furthermore one developer action may result in several screen changes.
for example entering a word results in a direct screen change the word appears but may also result in indirect screen changes e.g.
a code assist icon appears on the editor ruler .
to identify change regions directly related to developer actions we use two simple filters observed during our manual labelling of developer actions in programming screencasts see section iv b .
first we observe that the minimum change regions related to developer actions resulting from cursor movement are always of by pixels.
therefore we discard any change regions smaller than by pixels.
second we observe that as a response to human actions screen authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
an overview of the main steps of our actionnet fi fi 1fi fi 1binary difference imagecompute image similaritycrop change regions 1 1 1 1 fig.
.
steps to detect changes regions in between fiandfi changes directly related to developer actions will be the largest changes.
therefore we keep only the largest change region between fiandfi .
we refer to this largest change region as diffi i and the screen regions on fiandfi 1with respect todiffi i asri diffi i andri diffi i respectively.
b. change region feature extraction by vision cnn to predict actions from screen changes we must be able to identify the correspondence and contrast features from screen changes.
in this work we consider classes of developer actions.
the screen changes resulting from these actions have intra class variations.
for example entering text can be either typing a char or pasting a paragraph of text.
the background and foreground color changes resulting from text selection may differ from one application to another.
meanwhile we have inter class similarities.
for example switching window scrolling text and triggering popups may look alike in term of large screen content changes.
finally we have working environment variations across developers.
some developers use ides some use text editors and others use the interactive console.
even for the same tool different developers may use different color themes font size etc.
all these variations make manual feature engineering for action recognition in programming screencasts infeasible.
inspired by the success of cnn for computer vision tasks we design cnn based feature extractors that automatically learn to extract effective image features from training data without the need for manual feature engineering.
input change regions to cnn for the cnn to extract effective features for action recognition we must provide it sufficient information about screen changes resulting from developer actions.
we adopt three strategies to produce the input screen change regions to cnn.
the first strategy change contrast uses ri diffi i onfi andri diffi i onfi 1with respect to the largest change region diffi i. this strategy contrasts the corresponding largest change region ri diffi i andri diffi i onfi andfi 1respectively to recognize developer actions.
the second strategy action continuity uses ri diffi i onfiwith respect to diffi i 1between fi 1and fiandri diffi i onfi 1with respect to diffi i between fi andfi .
this strategy leverages the fact that developer actions have continuity for example typing a sequence of chars scrolling text continually.
therefore it considers both the screen change ri diffi i 1onfiresulting from the previous action and the screen change ri diffi i onfi 1resulting from the current action.
however the second strategy does not consider ri diffi i the contrast of ri diffi i o nfi.
the third strategy change contrast action continuity is a combination of the first and the second strategies.
first based on the corner positions of ri diffi i 1onfiand ri diffi i onfi we determine the least screen region bri i onfiandfi respectively that can include both ri diffi i 1onfiandri diffi i onfi .
this screen region bri i onfiwill also include ri diffi i onfi the contrast of ri diffi i .
in addition it may include some screen regions that are the same between fiandfi which may provide additional context for action recognition.
fig.
illustrates these three strategies.
assume the developer starts with the code in f1 she types a .
which is recorded in f2 and this action further triggers a code completion popup window recorded in f3.
we box r1 diff2 onf1 r2 diff2 1onf2 r2 diff3 2onf2 r3 diff3 2on f3 br3 2onf2andf3.
to recognize the trigger popup action from f2tof3 the strategy uses r2 diff3 2onf2 andr3 diff3 2onf3as input change regions.
the strategy usesr2 diff2 1onf2andr3 diff3 2onf3as input.
note that although r2 diff2 1is determined by the change region diff2 between f1andf2 we do not use any screen information from f1.
the strategy uses br3 2onf2andf3as input.
note that the strategy may take two screen regions of very different size.
as cnn requires the input images to be of the same size we have to resize the input change regions to the same size.
however in the situation illustrated in the strategy2 resizing the two input change regions to the same size will distort the small size image.
in contrast the strategy does not suffer from this issue.
as the two input change regions are of the same size resizing will result in the same level of scaling of images.
cnn architectures our model is based on inception resnet v2 which combines the advantages of microsoft s resnet and inception architecture .
each input change region is an image i rwhdwhere wandh are the width and height of the image and d for rgb color image i.e.
the red green blue channel respectively .
given the two change regions we develop two architectures to extract image features.
as shown in fig.
early fusion architecture first concatenates the two change regions into a channel input volume which is fed into a single cnn to authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
strategy strategy strategy 3 1 23 2 23 2 2 23 f1 1 12 f2 2 12 1 23 f3 2 23 23 2 23 3 23 2 23 2 23 3 fig.
.
illustration of three strategies for input change regions extract image features.
in contrast late fusion architecture feeds each input region into a cnn separately and then concatenate the output feature vector of the two cnns.
it adopts a siamese network architecture in which the two cnns share the weights.
ri ri ri ri late fusion model early fusion modelcnnfeature vectorsoftmax fig.
.
early fusion versus late fusion architecture c. action recognition by multi class classification given the image feature vector sextracted by the cnn we train a softmax classifier to predict the developer action that most likely causes the screen change from fitofi .
specifically the softmax classifier predicts the probability distribution yover the class labels as defined in section ii i.e.
y softmax ws b where y r10is the vector of prediction probabilities over the class labels and w andbare the learnable parameters of the classifier.
the cnn model and the softmax classifier is training by minimizing the cross entropy loss of the predicted labels and the ground truth labels l y y summationtext i m summationtext j yijlog yij where m is the number of training samples yijis the ground truth label for the jth class for the ground truth class otherwise for theith training example and yijis the predicted probability of the jth class for the ith training example.
iv .
e v alua tion of model performance we collect programming screencasts from y outube to investigate the following three research questions rq1 how do the alternative designs of the vision cnn affect the model performance for action recognition?
rq2 how well do our action recognition model perform when training and testing across different developers working environments and programming languages?
rq3 what is the runtime performance of our model?a.
the dataset of programming screencasts on youtube in this study we consider two programming languages python and java.
to prepare data we use y outube data api to search java tutorial and python tutorial on y outube.
we retrieve the top returned playlists for python and java respectively.
from these candidate playlists we select playlists for each language in which the video authors are programming in the screencasts.
the authors of the selected playlists are all different.
we use y outube dl api to download all the screencasts at high definition resolution in these selected playlists.
finally we randomly select screencasts from each playlist and obtain a collection of screencasts for python and for java .
table ii shows the details of the programming screencasts we crawl.
the selected playlists cover beginner intermediate and advanced level of programming knowledge.
from video topic we can see that nine playlists cover the fundamental programming concepts and knowledge and one playlist p9 covers java gui.
the tools used in screencasts are diverse.
for python p1 p3 and p5 use the interactive shell p4 uses ide pycharm and p2 uses both interactive shell and pycharm.
for java all five playlists use ide four use eclipse and one p9 uses netbeans .
even for the same tool different developers may use different color themes font size etc.
the selected screencasts have a duration between four to minutes median minutes .
python and java screencasts have almost the same total duration.
the durations of the selected screencasts are appropriate for our study because they are long enough to contain adequate and diverse developer actions but they do not contain much repetitive work which may inflate model performance superficially.
the programming screencasts in a playlist as a whole can be regarded as about minutes of programming work by a developer.
b. manual labeling of developer actions in screencasts in this work we decode programming screencasts into frames at the rate of frames per second by opencv .
as our model takes two consecutive frames as input the model training and testing datasets are organized in the form of frame pairs.
as explained in our problem statement see section ii we discard frame pairs with no screen changes i.e.
no action authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii theda taset of programming screencasts crawled from youtube python java pl idpl nametoolsvideo idvideo topicdur s pl idpl nametoolsvideo idvideo topicdur s p1python programming tutorialsinteractive shellv1 bitwise operation p6java tutorial for beginnerseclipsev1 variables v2 variables v2 input v3 lists v3 switch case v4 dictionaries v4 while v5 arithmetic v5 string p2python .
programming tutorialsinteractive shell pycharmv1 numbers p7java beginner programming tutorialseclipsev1 variables v2 string v2 input v3 lists v3 if v4 if else v4 switch v5 for v5 classes p3python programming tutorialsinteractive shellv1 numbers p8java intermediate tutorialseclipsev1 array v2 variables v2 stack v3 strings v3 queue v4 dictionaries v4 hashset v5 for while v5 return p4python programming tutorialspycharmv1 while p9java gui tutorialsnetbeansv1 image v2 functions v2 event v3 dictionaries v3 numbers v4 bitwise operation v4 beeper v5 if else v5 grid layout p5python tutorial for beginnersinteractive shellv1 numbers p10java tutorial for beginners 2018eclipsev1 variables v2 variables v2 if else v3 models functions v3 while v4 string v4 arithmetic v5 lists v5 class by image differencing.
this removes about of frame pairs in the initial dataset.
we then manually label each frame pair with screen changes by one of the classes of actions defined in section ii.
take fig.
as example.
the frame pair fx fx is labeled as trigger popup c6 fy fy a s select text c8 and fz fz as scroll text c7 .
to ensure the quality of data labeling the two authors and another developer participate in the data labeling.
the annotators have at least years programming experience on python and java.
for efficient and consistent labeling we develop a python application by which the annotators can view frame pairs with screen changes in a screencast one by one and select a class label for each frame pair.
each annotator first labels the whole dataset independently.
the fleiss kappa of the three annotators labeling results is .
which indicates substantial agreement.
if two or three annotators assign the same label to a frame pair that label is the final label.
in the cases when the three annotators give three different labels for a frame pair the annotators discuss to decide the final label.
table iii summarizes the distribution of different classes of developer actions out of our manual labeling process.
this labeled dataset consists of frame pairs in total which requires significant human efforts about man months .
we can see that scroll text and switch window have fewer instances than other classes.
this is because programming screencasts on y outube usually do not involve very long code to scroll or many files to switch.
furthermore we observe that the action distributions for python and java are largely similar.
but java has relatively more trigger popups and switch window .
this is because all five java screencasts use ides while of python screencasts use interactive shell with nopopup support or fewer windows to switch .
c. evaluation metrics we evaluate and compare model performance for action recognition by four metrics accuracy precision recall and f1 score.
the correctness of the predicted action for a frame pair is determined against the human label of developer action for that frame pair i.e.
ground truth .
precision for an action class cis the proportion of frame pairs that are correctly predicted as camong all frame pairs predicted asc.
recall for an action class cthe proportion of frame pairs that are correctly predicted as camong all groundtruth frame pairs labelled as c. f1 score for an action class ccombines the precision and recall as p recision c recall c p recision c recall c .
as multiple action labels are predicted by our model we compute the weighted average of precision recall and f1 score for all action classes as a whole i.e.
summationtext10 c metric count c summationtext10 c 1count c which gives a view on the general prediction performance.
we also calculate accuracy to evaluate the overall performance i.e.
the number of frame pairs correctly predicted by the model over the total number of frame pairs.
d. impact of alternative model design rq1 motivation our approach relies on the cnn model to extract image features for action recognition.
the effectiveness of the cnn for feature extraction directly affect the recognition performance.
in this work we have three alternative strategies for preparing input change regions to the cnn change contrast strategy action continuity strategy and change contrast action continuity strategy which exploit different properties of developer actions and screen authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii sta tistics of developer actions by manual labeling action class python java all move cursor by keyboard c1 move mouse over text region c2 move mouse over non text region c3 enter text c4 delete text c5 trigger popups c6 scroll text c7 select text c8 switch window c9 total early fusion loss late fusion loss fig.
.
loss convergence for different model configurations changes.
in addition we extract image features by the two different cnn architectures early fusion versus late fusion.
we want to comparatively investigate the impact of these alternative model designs on the performance of action recognition.
method in this experiment we combine python and java data in table iii.
we use of frame pairs and their corresponding action labels for model training and the rest for testing.
we combine each strategy for input change regions and each cnn architecture.
as such we have six different models.
we train each model separately using the same training data and compare the model performance on the same testing data.
results fig.
shows the loss convergence during the model training process.
the horizontal axis is the number of training iterations and the vertical axis shows the loss value after each training iteration.
we can see that the same input strategy has similar loss convergence rate in early fusion and latefusion architecture but early fusion architecture converges a bit faster than late fusion architecture.
in both cnn architectures the change contrast action continuity inputstrategy converges faster than the other two input strategies.
table iv and table v show the performance results of the six model configurations.
we can see that using the input strategy in both early fusion and late fusion architecture achieves the much better performance in all evaluation metrics compared with the other two input strategies.
the average f1 score of the action classes for the input strategy is .
and .
in early fusion and late fusion architecture respectively.
the average f1 score for the other two input strategies is only about .
.
the input strategy and strategy2 have very similar performance.
they achieve acceptable performance only for move cursor and move mouse over text f1 score around .
which have the most number of training samples.
for the input strategy and strategy the f1 scores for most other action classes are around or below0.
.
in contrast for the input strategy the f1 scores for most action classes are around or above .
.
comparing the same input strategy in different cnn architectures we can see that the average f1 score of the classes is very close for the input strategy and strategy3.
the performance gap for input strategy is relatively larger.
in general late fusion architecture performs better than early fusion architecture especially for those action classes with small numbers of data instances.
however late fusion has to execute the cnn twice for the two input change regions which doubles the computing time see section iv f compared with early fusion that concatenates the two change regions and feeds them as a whole through the cnn once.
considering both change contrast and action continuity in the input change regions is beneficial for action recognition compared with considering change contrast or action continuity alone.
early fusion and late fusion have very close performance for action recognition but late fusion requires double computing time.
e. model performance across different settings rq2 motivation an effective action recognition model should be able to generalize over variations within one class and variations across developers working environments e.g.
tools color themes and programming languages.
this rq is set to evaluate the performance of our model across different developers working environments and programming languages.
approach considering the experiment results of rq1 our model in rq2 uses change contrast action continuity as input strategy and early fusion as cnn architecture.
first we conduct intra playlist experiments.
we randomly divide the human labeled frame pairs of each playlist into for training and for testing.
as the screencasts in a playlist are produced by the same developer in the same working environment the intra playlist experiments set the performance upper bound to compare and understand the interplaylist and inter programming language performance.
for inter playlist experiments we use the human labeled frame pairs of four playlists in a programming language as training data and the frame pairs of the left one playlist as testing data.
so we have inter playlist experiments.
considering the data characteristics of our crawled playlists see section iv a these inter playlist experiments can test our model performance across different developers and or working environments.
for inter language experiments we train the model using the human labeled frame pairs of the five playlists of one language and test the model using the data of the other language.
so we have two inter language experiments denoted as python java and java python .
in the interlanguage setting both developers and working environments are also different between model training and testing.
results next we report our intra playlist inter playlist and inter language experiment results intra playlist table vi shows our model s performance in the intra playlist experiments.
we show the accuracy and authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iv performance of three input stra tegies with early fusion architecture strategy strategy strategy action class precision recall f1 score precision recall f1 score precision recall f1 score move cursor by keyboard c1 .
.
.
.
.
.
.
.
.
move mouse over text region c2 .
.
.
.
.
.
.
.
.
move mouse over non text region c3 .
.
.
.
.
.
.
.
.
enter text c4 .
.
.
.
.
.
.
.
.
delete text c5 .
.
.
.
.
.
.
.
.
trigger popups c6 .
.
.
.
.
.
.
.
.
scroll text c7 .
.
.
.
.
.
.
.
.
select text c8 .
.
.
.
.
.
.
.
.
switch window c9 .
.
.
.
.
.
.
.
.
others c10 .
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
accuracy .
.
.
table v performance of three input stra tegies with lat e fusion architecture strategy strategy strategy action class precision recall f1 score precision recall f1 score precision recall f1 score move cursor by keyboard c1 .
.
.
.
.
.
.
.
.
move mouse over text region c2 .
.
.
.
.
.
.
.
.
move mouse over non text region c3 .
.
.
.
.
.
.
.
.
enter text c4 .
.
.
.
.
.
.
.
.
delete text c5 .
.
.
.
.
.
.
.
.
trigger popups c6 .
.
.
.
.
.
.
.
.
scroll text c7 .
.
.
.
.
.
.
.
.
select text c8 .
.
.
.
.
.
.
.
.
switch window c9 .
.
.
.
.
.
.
.
.
others c10 .
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
accuracy .
.
.
the average precision recall and f1 score for each playlist.
our model performs very well for intra playlist prediction with the mean f1 score .
.
.
we observe very small performance differences between the playlists.
inter playlist table vii shows our model s performance on each testing playlist in the inter playlist experiments.
we can see that our model still has very good performance for inter playlist prediction with the mean f1 score .
.
.
compared with the intra playlist performance the interplaylist performance is inevitably lower which is unsurprising considering the variations across developers and or working environments.
the performance is especially low for the testing playlist p1and p9.p1and p9have very different working environments from other playlists.
for example the mouse pointer over non text region in p1is a very unique blue circle which never appears in the training playlist p2 p3 p4 p5 .
this results in the very poor precision and recall for move mouse over nontext class.
p9is recorded with unusual dark ide theme which leads to very different screen features from the ides used in the training playlists p6 p7 p8 p10 .
this affects the prediction of most action classes.
inter language table viii shows the model performance in the two inter language experiments.
the inter language setting is even more challenging as it involves not only language variations but also developer and working environment variations.
in this challenging setting our model has reasonably good performance average f1 score .
for python java and .
for java python .
however the model does not have equally good performance on all action classes.
for example the model trained by java screencasts cannot recognize select text well in python screencasts but the model trained bypython data performs reasonably well for recognizing select text in java data.
by analyzing the data we find that select text in python video is more variable than that in java video.
select text in java video only has blue background but select text in python video has both blue and gray background.
furthermore neither model performs well for switch window and scroll text because these two action classes have much fewer train samples than other classes.
our model can be effectively trained and deployed in very diverse developer working environment and programming language settings.
exposing the model to diverse training data is crucial for good model performance.
table vi intra pla ylist results playlist id precision recall f1 score accuracy p1 .
.
.
.
p2 .
.
.
.
p3 .
.
.
.
p4 .
.
.
.
p5 .
.
.
.
p6 .
.
.
.
p7 .
.
.
.
p8 .
.
.
.
p9 .
.
.
.
p10 .
.
.
.
mean stddev .
.
.
.
.
.
.
.
f .
runtime performance rq3 we test our tool for action recognition on a pc with 64g ram i9 7900x cpu and titan xp gpu.
the cnn model is implemented in tensorflow .
using early fusion architecture our tool can process about frame pairs per authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table vii inter pla ylist results playlist id precision recall f1 score accuracy p1 .
.
.
.
p2 .
.
.
.
p3 .
.
.
.
p4 .
.
.
.
p5 .
.
.
.
p6 .
.
.
.
p7 .
.
.
.
p8 .
.
.
.
p9 .
.
.
.
p10 .
.
.
.
mean stddev .
.
.
.
.
.
.
.
table viii inter programming language results python java java python action precision recall f1 precision recall f1 c1 .
.
.
.
.
.
c2 .
.
.
.
.
.
c3 .
.
.
.
.
.
c4 .
.
.
.
.
.
c5 .
.
.
.
.
.
c6 .
.
.
.
.
.
c7 .
.
.
.
.
.
c8 .
.
.
.
.
.
c9 .
.
.
.
.
.
c10 .
.
.
.
.
.
average .
.
.
.
.
.
accuracy .
.
second.
using late fusion architecture our tool can process about frame pairs per second.
image differencing accounts for about of processing time.
our results show that our tool is fast enough for real time developer action recognition.
v. p ractical applica tion a ction awa r e key code frame extraction having evaluated the performance of our model for action recognition we want to demonstrate a practical application that our approach enables for programming screencast analysis.
a key challenge in programming screencast analysis is to determine the key frames from which important code should be extracted.
in this study we compare action aware extraction of key code frames based on recognized developer actions in a screencast with action agnostic extraction of key code frames commonly used in existing work.
table ix comparison of key code frame extraction methods method tp fp tn fn pre.
rec.
f1 fixed time interval .
.
.
image similarity .
.
.
actionnet based .
.
.
a. screencast dataset of real developer work we use screencast software to record about hours of real programming work of the two software developers.
one developer works on a java project in eclipse and the other developer works on a python project in jupyter an online python development environment .
the screencasts are decoded at the rate of frames per second.
we ask the two developers to identify the key code frames in their screencasts.
the twodevelopers define key code frames as the frames that contain code fragments before or after significant code changes or the frames that contain code fragments that may not be visible again.
they follow three behavioral patterns to identify keycode frames selects a block of code and deletes it which means that an old version of the code in the frame before code deletion should be extracted.
edits code and then switches to another window which means that code editing is temporarily finished and the latest version of the code in the frame before window switching should be extracted.
scrolls code and both the disappearing code in the frame before scrolling and the appearing code in the frame after scrolling should be extracted.
in total key code frames have been identified which are used as ground truth to compare different key code frame extraction methods.
b. methods for extracting key code frames we train our model change contrast action continuity with early fusion architecture using the dataset of python and java programming screencasts from y outube see section iv a .
then we use the trained model to recognize the actions in the hours of screencasts of real developer work.
based on the recognized actions we identify the key code frames in the screencasts of real developer work by searching certain sequential patterns of developer actions.
we compare our method with two action agonistic methods commonly used in existing work on programming screencast analysis .
the first method extracts the first frame at a fix time interval.
following the work we set the time interval at second.
the second method extracts the next frame that is different enough below a user specified similarity threshold from the previous frame.
following the work we set the similarity threshold at .
.
c. results table ix presents the comparison results.
we present true positive tp true negative tn false positive fp and false negative fn for the three methods.
the fixed timeinterval based method is completely unaware of the workflow and content of the screencasts.
it outputs too many frames tp fp among which only frames accidentally hit the ground truth key code frames.
so the fixed time interval based method has very low precision and recall.
it will waste so much computing resource to try to extract code from so many unnecessary frames.
frame similarity based method significantly decreases the number of extracted frames but it still extracts tp fp frames.
as the ground truth key code frames identified by the developers all involve substantial screen changes framesimilarity based method actually identifies all ground truth frames at the similarity threshold .
i.e.
recall in our experiment .
however this result may be sensitive to the scale of screen changes and the similarity threshold.
furthermore frame similarity based method still identifies many false positive frames fp for example frames with popups and text selection.
although these frames have substantial screen authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
changes they do not change the code and do not need to be repeatedly extracted if the key code frames have already been extracted.
furthermore screen content in such frames is noisy which will negatively affect the quality of ocr.
actionnet based method extracts the least amount of frames tp fp only about of the number of frames extracted by frame similarity based method.
but actionnetbased method has only a minor decrease in recall compared with the recall of frame similarity based method .
versus .
.
actionnet based method still extracts some unnecessary frames fp but its precision is about six times higher than that of frame similarity based method .
versus .
.
the main reason for the false positive frames by actionnet based method is the coarse grained definition of action classes.
for example both enter or delete a word and enter or delete several lines of code are now recognized as enter or delete text .
so some frames with minor code changes are extracted but the developers do not consider such changes significant enough so that they do not label such frames as key code frames.
such false positive frames could be filtered out by distinguishing finer grained actions.
vi.
r ela ted work developer behavioral research in behavioral research data collection methods include observation survey and interview.
observational data can be collected by using eyetracker and fmri software instrumentation or screencasting .
among these observational data collection methods screencasting is the easiest one to deploy.
however due to the image nature of screencasts the workflow actions and application content in screencasts must be extracted before any meaningful behavioral research can be conducted.
our work develops the first tool to automatically extract workflow actions from programming screencasts.
our tool can lower the barrier for action centric analysis of programming screencasts enabling large scale behavioral research in software engineering.
programming screencast analysis existing methods falls into two categories content extraction and video search and navigation .
content extraction in existing work is simply based on fix time interval or frame similarity.
some tools like waken use image differencing technique to identify ui elements e.g.
mouse pointer but not actions e.g.
move the mouse over text .
vtrevolution by bao et al.
shows that workflow actions in a programming screencast if available can significantly improve video search and navigation efficiency and enhance the learning experience.
however it uses software instrumentation to collect workflow actions during screencasting.
it envisions to support the interactive video watching experience for y outube programming screencasts that are not accompanied with workflow actions.
our tool is the first step towards making this vision closer to reality.
general human action recognition our work recognizes developer actions in the virtual world while general human action recognition recognizes human actions in the physicalworld.
early techniques for human action recognition include hidden markov model and discriminative svm models .
recent work has used deep learning techniques such as two stream cnns c3d 3d convolutional neural network .
considering the duration of physical human actions these techniques usually analyze multiple frames e.g.
frames in c3d .
however developer actions on computer cause instant screen changes.
as such developer actions must be recognized from the screen change between two frames.
the accuracy of our technique is on par with that of human action recognition .
human action recognition enables many applications such as action centric video search automatic surveillance smart homes .
these applications inspire downstream applications based on the recognition of developer actions in screencasts using our technique such as developer risk behavior surveillance.
existing tools e.g.
checkstyle findbugs are code centric.
none of them can prevent programming mistakes from behavioral perspectives.
deep learning for software data recently deep learning techniques have been successfully applied to many forms of software data such as source code and software text and user interface images .
different from these works our work is the first to apply deep learning to recognize workflow actions in programming screencasts.
a recent work uses cnn based techniques to predict programming languages used in screencasts which is a much easier task than our developer action recognition.
vii.
c onclusions and future work this paper fills in an important missing technique in the tool set for programming screencast analysis.
the core component of our technique is a cnn model.
this design is driven by the cnn s ability to automatically learn to extract image features from the screen changes resulting from developer actions thus removing the need for manual feature engineering which is a challenging task due to the diversity of developer actions working environments and programming languages.
our experiments show that our technique can generalize over variations within action classes and variations across developers working environments and programming languages.
this work develops an enabling technique for action aware analysis of programming screencasts e.g.
key code frame extraction .
in this future we will build a big database of developers workflow actions considering millions of programming screencasts on y outube.
such a database will enable much downstream research work which we will investigate such as large scale behavioral research in software engineering actionaware search and navigation of y outube programming screencasts or developer risk behavior surveillance for proactively avoiding programming mistakes.
viii.
a cknowledgment we gratefully acknowledge the support of nvidia corporation with the donation of the titan xp gpu used for this research.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.