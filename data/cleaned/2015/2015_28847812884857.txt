automated parameter optimization of classification t echniques for defect prediction models chakkrit tantithamthavorn1 shane mcintosh2 ahmed e. hassan3 kenichi matsumoto1 1nara institute of science and technology japan chakkrit t matumoto is.naist.jp2mcgill university canada shane.mcintosh mcgill.ca3queen s university canada ahmed cs.queensu.ca abstract defect prediction models are classifiers that are trained to identify defect prone software modules.
such classifiers have configurableparametersthatcontroltheircharacteristics e.g.
the number of trees in a random forest classifier .
recent studies show that these classifiers may underperform due to the use of suboptimal default parameter settings.
however it is impractical to assess all of the possible settings in the parameter spaces.
in this paper we investigate the performance of defect prediction models where caret an automated parameter optimization technique has been applied.
through a case study of datasets from systems that span both proprietary and open source domains we find that caret improves the auc performance of defect prediction models by as much as percentage points caret optimized classifiers are at least as stable as with of them being more stable than classifiers that are trained using the default settings and caret increases the likelihood of producing a top performing classifier by as much as .
hence we conclude that parameter settings can indeed have a large impact on the performance of defect prediction models suggesting that researchers should experiment with the parameters of the classification techniques.
since automated parameter optimization techniques like caret yield substantially benefits in terms of performance improvement and stability while incurring a manageable additional computational cost they should be included in future defect prediction studies.
ccs concepts general and reference experimentation software and its engineering software defect analysis searchbased software engineering keywords software defect prediction experimental design classification techniques parameter optimization permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may austin tx usa c circlecopyrt2016 acm.
isbn .
.
.
.
introduction the limited software quality assurance sqa resources of software organizations must focus on software modules e.g.
source code files that are likely to be defective in the future.
to that end defect prediction models are trained to identify defect prone software modules using statistical or machine learning classification techniques.
such classification techniques often have configurable parameters that control characteristics of the classifiers that they produce.
for example the number of decision trees of which a random forest classifier is comprised can be configured prior to training the forest.
furthermore the number of non overlapping clusters of which a k nearest neighbours classifier is comprised must be configured prior to using the classification technique.
since the optimal settings for these parameters are not known ahead of time the settings are often left at default values.
prior work suggests that defect prediction models may underperform if they are trained using suboptimal parameter settings.
for example jiang et al.
and tosun et al.
also point out that the default parameter settings of randomforestandnaivebayesareoftensuboptimal.
koru et al.
and mende et al.
show that selecting different parameter settings can impact the performance of defect models.
hall et al.
show that unstable classification techniques may underperform due to the use of default parameter settings.
mittas et al.
and menzies et al.
argue that unstable classification techniques can make replication of defect prediction studies more difficult.
indeed we perform a literature analysis that reveals that of the most commonly used classification techniques require at least one parameter setting.
since such parameter settings may impact the performance of defect prediction models the settings should be carefully selected.
however itisimpracticaltoassessallofthepossiblesettings in the parameter space of a single classification technique .
for example kocaguneli et al.
point out that there are at least possible settings to explore when training k nearest neighbours classifier.
in this paper we investigate the performance of defect prediction models where caret an off the shelf automated parameter optimization technique has been applied.
caretevaluatescandidateparametersettingsandsuggests the optimized setting that achieves the highest performance.
through a case study of datasets from systems that span both proprietary and open source domains we record our observations with respect to two dimensions ieee acm 38th ieee international conference on software engineering table overview of studied parameters.
family family description p arameter name parameter description classification techniques that apply with their default and candidate parameter values.
naive b ayesnaive bayes is a probability model that assumes that predictors are independent of each other.
techniques naive bayes nb .laplace correction laplace correction indicates no correction .nb distribution type true indicates a kernel density est imation while false indicates a normal density estimation.nb true false nearest n eighbournearest neighbour is an algorithm that stores all available observations and classifies new observations based on its similarity to prior observations.
techniques k nearest neighbour knn .
clusters the numbers of non overlapping clusters to produce.knn regressionl ogistic regression is a technique for explaining binary dependent variables.
mars is a non linear regression modelling technique.
techniques glm and mars.degree interaction the maximum degree of interaction friedman s mi .
the default is meaning build an additive model i.e.
no interaction terms .mars partial l east squarespartial least squares regression generalizes and combines features from principal component analysis and multiple regression.
techniques generalized partial least squares gpls .
components the number of pls components.
gpls neural n etworkneural network techniques are used to estimate or approximate functions that can depend on a large number of inputs and are generally unknown.
techniques standard nnet model averaged avnnet feature extraction pcannet radial basis functions rbf multi layer perceptron mlp voted mlp mlpweightdecay and penalized multinomial regression pmr .bagging should each repetition apply baggin?avnnet true false weight decay a penalty factor to be applied to t he errors function.mlpweightdecay pmr avnnet nnet pcannet .
.
.
.
svmlinear hidden units numbers of neurons in the hidden l ayers of the network that are used to produce the prediction.mlp mlpweightdecay avnnet nnet pcannet rbf discriminat ion analysisdiscriminant analysis applies different kernel functions e.g.
linear to classify a set of observations into predefined classes based on a set of predictors.
techniques linear discriminant analysis lda penalized discriminant analysis pda and flexible discriminant analysis fda .product degree the number of degrees of freedom that are available for each term.fda shrinkage penalty c oefficient a shrinkage parameter to be applied to each tree in the expansion a.k.a.
learning rate or step size reduction .pda terms the number of terms in the model.
fda rule basedr ule based techniques transcribe decision trees using a set of rules for classification.
techniques rule based classifier rule and ripper classifier ripper .
optimizations the number of optimization iterations.ripper decision t reesbaseddecision trees use feature values to classify instances.
techniques c4.
like trees j48 logistic model trees lmt and classification and regression trees cart .complexity a penalty factor to be applied to the error rate of the terminal nodes of the tree.cart .
.
.
.
.
confidence the confidence factor used for pruni ng smaller values incur more pruning .j48 .
iterations the numbers of iterations.
lmt svms upport vector machines svms use a hyperplane to separate two classes i.e.
defective or not .
techniques svm with linear kernel svmlinear and svm with radial basis function kernel svmradial .sigma the width of gaussian kernels.
svmradial .
.
.
.
.
cost a penalty factor to be applied to t he number of errors.svmradial .
.
svmlinear baggingb agging methods combine different base learners together to solve one problem.
technique random forest rf bagged cart baggedcart trees the numbers of classification trees.
rf boostingb oosting performs multiple iterations each with different example weights and makes predictions using voting of classifiers.
techniques gradient boosting machine gbm adaptive boosting adaboost generalized linear and additive models boosting gamboost logistic regression boosting logitboost extreme gradient boosting tree xgbtree and c5.
.
boosting iterations the numbers of iterations that are used to construct models.c5.
gamboost logitboost gbm xgbtree trees the numbers of classification trees.
adaboost shrinkage a shrinkage factor to be applied to e ach tree in the expansion a.k.a.
learning rate or step size reduction .gbm .
xgbtree .
max tree depth the maximum depth per tree.
adaboost gbm x gbtree min.
terminal n ode size the minimum terminal nodes in trees.gbm winnow should predictor winnowing i.e feat ure selection be applied?c5.
false true aic prune?
should pruning using stepwise feat ure selection be applied?gamboost false true model type either tree for the predicted class or r ules for model confidence values.c5.
rules tree denotes a numeric value denotes a logical value d enotes a factor value.
the default values are shown in bold typeface and correspond to the default values of the caret r package.
performance improvement c aretimprovestheauc performance of defect prediction models by up to percentage points.
moreover the performance improvement provided by caret is non negligible for of the studied classification techniques .
performance stability caret optimizedclassifiersare at least as stable as classifiers that are trained using the default settings.
moreover the caret optimized classifiers of of the studied classification techniques are more stable than classifiers that are trained using the default values.
sincewefindthatparametersettingscanhavesuchanimpact on model performance we revisit prior analyses thatrank classification techniques by their ability to yield topperforming defect prediction models.
we find that caret increases the likelihood of producing a top performing classifier by as much as suggesting that automated parameter optimization can substantially shift the ranking of classification techniques.
our results lead us to conclude that parameter settings can indeed have a large impact on the performance of defect prediction models suggesting that researchers should experiment with the parameters of the classification techniques.
since automated parameter optimization techniques like caret yield substantially benefits in terms of performance improvement and stability while incurring a manageable additional computational cost they should be included in future defect prediction studies.
322tothebestofourknowledge thisisthefirstpapertostudy a large collection of parameters that are derived from of the most frequently used classification techniques in the context of defect prediction.
the improvement and stability of the performance of defect prediction models when automated parameter optimization is applied.
the ranking of classification techniques for defect prediction when automated parameter optimization is applied.
paper organization.
the remainder of the paper is organized as follows.
section illustrates the importance of parameter settings of classification techniques for defect prediction models.
section positions this paper with respect to the related work.
section presents the design and approach of our case study.
section presents the results of our case study with respect to our two research questions.
section revisits prior analyses that rank classification techniquesbytheirlikelihoodofproducingtop performingdefect prediction models.
section discusses the broader implications of our findings.
section discloses the threats to the validity of our study.
finally section draws conclusions.
.
the relev ance of parameter settings for defect prediction models a variety of classification techniques are used to train defect prediction models.
since some classification techniques do not require parameter settings e.g.
logistic regression we first assess whether the most commonly used classification techniques require parameter settings.
we first start with the families of classification techniques that are used by lessmann et al.
.
based on a recent literature review of laradji et al.
we add additional families of classification techniques that have been recently used in defect prediction studies.
in total we study classification techniques that span classifier families.
table provides an overview of the families of classification techniques.
our literature analysis reveals that of the most commonly used classification techniques require at least one parameter setting.
table provides an overview of the unique parameters that apply to the studied classification techniques.
of the most commonly used classification techniques require at least one parameter setting indicating that selecting an optimal parameter setting for defect prediction models is an important experimental design choice.
.
related work research questions r ecent research has raised concerns about parameter settings of classification techniques when applied to defect prediction models.
for example koru et al.
and mende et al.
point out that selecting different parameter settings can impact the performance of defect models.
jiang et al.
and tosun et al.
also point out that the default parameter settings of research toolkits e.g.
r weka scikit learn matlab are suboptimal.although prior work suggests that defect prediction models may underperform if they are trained using suboptimal parameter settings parameters are often left at their default values.
for example mende et al.
use the default number of decision trees to train a random forest classifier provided by an r package .
weyuker et al.
also train defect models using the default setting of c4.
that is provided by weka.
jiang et al.
and bibi et al.
also use the default value of the k nearest neighbours classification technique k .
in addition the implementations of classification techniques that are provided by different research toolkits often use different default settings.
for example for the number of decision trees of the random forest technique the default settings vary from for the bigrfr package for matlab for weka to for the randomforestr package .
moreover for the number of hidden layers of the neuron networks techniques the default settings vary from for the neuralnet r package for the nnetr package and weka to for matlab .
such a variation of default settings that are provided by different research toolkits may influence conclusions of defect prediction studies .
there are many empirical studies in the area of searchbased software engineering sbse that aim to optimize software engineering tasks e.g.
software testing .
however little sbse research has been applied to optimize the parameters of classification techniques for defect prediction models.
although prior studies have explored the impact of parameter settings they have only explored a few parameter settings.
to more rigorously explore the parameter space of classification techniques for defect prediction models we formulate the following research question rq1 how much does the performance of defect prediction models improve when automated parameter optimization is applied?
recent research voices concerns about the stability of perfo rmanceestimatesthatareobtainedfromclassificationtechniques when applied to defect prediction models.
for example menzies et al.
and mittas et al.
argue that unstable classificationtechniques canmake replicationof defect prediction studies more difficult.
shepperd et al.
and jorgensen et al.
also point out that the unstable performance estimates that are produced by classification techniques may introduce bias which can mislead different research groups to draw erroneous conclusions.
myrtveit et al.
show that high variance in performance estimates from classification techniques is a critical problem in comparative studies of prediction models.
song et al.
also show that applying different settings to instable classification techniques will provide different results.
like any form of classifier optimization automated parameter optimization may increase the risk of overfitting i.e.
producing a classifier that is too specialized for the data from which it was trained to apply to other datasets.
to investigate whether parameter optimization is impacting the stability of defect prediction models we formulate the following research questions rq2 how stable is the performance of defect prediction models when automated parameter optimization is applied?
.
case study approach i n this section we discuss our selection criteria for the studied systems and then describe the design of our case study experiment that we perform in order to address our research questions.
.
studied datasets in selecting the studied datasets we identified three important criteria that needed to be satisfied criterion publicly available defect datasets from different corpora our recent work shows that researcherstendtoreuseexperimentalcomponents e.g.
datasets metrics and classifiers .
song et al.
and ghotra et al.
also show that the performance of defect prediction models can be impacted by the dataset from which they are trained.
to combat potential bias in our conclusions and to foster replication of our experiments we choose to train our defect prediction models using datasets from different corpora and domains that are hosted in publicly available data repositories.
criterion dataset robustness mende et al.
show that models that are trained using small datasets may produce unstable performance estimates.
an influential characteristic in the performance of a classification technique is the number of events per variable epv i.e.
the ratio of the number of occurrences of the least frequently occurring class of the dependent variable i.e.
the events to the number of independent variables that are used to train the model i.e.
the variables .
our recent work shows that defect prediction models that are trained using datasets with a low epv value are especially susceptible to unstable results .
to mitigate this risk we choose to study datasets that have an epv above as suggested by peduzzi et al.
.
criterion sane defect data since it is unlikely that more software modules have defects than are free of defects we choose to study datasets that have a rate of defective modules below .
tosatisfycriterion1 webeganourstudyusing101publiclyavailable defect datasets.
datasets are downloaded from the tera promise repository clean nasa datasets are provided by shepperd et al.
are provided by kimet al.
are provided by d ambros et al.
and are provided by zimmermann et al.
.
to satisfy criterion we exclude the datasets that we found to have epv values below .
to satisfy criterion we exclude an additional datasets because they have a defective rate above .
table2providesanoverviewofthe18datasetsthatsatisfy our criteria for analysis.
to strengthen the generalizability of our results the studied datasets include proprietary and open source systems of varying size and domain.
figure provides an overview of the approach that we apply to each studied system.
we describe each step in the approach below.
an overview of the studied systems.
domain system defective files metrics epv r ate nasa jm112 pc5128 proprietary prop prop prop prop prop apache camel .
xalan .
xalan .
eclipse platform .
platform .
platform .
debug .
swt .
jdt521 mylyn513 pde514 1provided by shepperd et al.
.4provided by kim et al.
.
2provided by jureczko et al.
.5provided by ambros et al.
.
3provided by zimmermann et al.
.
.
generate bootstrap sample in ordertoensurethattheconclusionsthatwedrawabout our models are robust we use the out of sample bootstrap validation technique which leverages aspects of statistical inference .
the out of sample bootstrap is made up of two steps step a bootstrap sample of size nis randomly drawn with replacement from an original dataset which is also of size n. step a model is trained using the bootstrap sample and tested using the rows that do not appear in the bootstrap sample.
on average .
of the rows will not appear in the bootstrap sample since it is drawn with replacement .
theout of samplebootstrapprocessisrepeated100times and the average out of sample performance is reported as the performance estimate.
unlike the ordinary bootstrap the out of sample bootstrap technique fits models using the bootstrap samples but rather than testing the model on the original sample the model is instead tested using the rows that do not appear in the bootstrap sample .
thus the training and testing corpus do not share overlapping observations.
unlikek foldcross validation theout of samplebootstrap technique fits models using a dataset that is of equal length to the original dataset.
cross validation splits the data into kequal parts using k parts for fitting the model setting aside fold for testing.
the process is repeated ktimes using a different part for testing each time.
however mende et al.
pointoutthatthescarcityofdefectivemodulesinthe smalltestingcorporaof10 foldcrossvalidationmayproduce biased and unstable results.
prior studies have also shown that fold cross validation can produce unstable results for small samples .
on the other hand our recent research demonstrates that the out of sample bootstrap tends to produce the least biased and most stable performance estimates .
moreover the use of out of sample bootstrap is recommended for high skewed datasets as is the case in our defect prediction datasets.
324careto ptimized setting default se tting construct d efect models defect mo del defect mo del default pe rformancecaret optimized performance defect d ataset testing corpus training corpus generate b ootstrap sample repeat times step g enerate candidate settings caret parameter optimization step ev aluate candidate settings step i dentify the caretoptimized setting calculate p erformance figure an overview of our case study approach.
.
caret parameter optimization since it is impractical to assess all of the possible parameter settings of the parameter spaces we use the optimized parameter settings suggested by the trainfunction of the caretr package .
caret suggests candidate settings for each of the studied classification techniques which can be checked using the getmodelinfo function of the caretr package .
the optimization process is made up of three steps.
step generate candidate parameter settings the trainfunction will generate candidate parameter settings based on a given budget threshold i.e.
tune length for evaluation.
the budget threshold indicates the number of different values to be evaluated for each parameter.
as suggested by kuhn we use a budget threshold of .
for example the number of boosting iterations of the c5.
classification technique is initialized to and is increased by until the number of candidate settings reaches the budget threshold e.g.
.
table shows the candidate parameter settings for each of the studied parameters.
the default settings are shown in bold typeface.
step evaluate candidate parameter settings caret evaluates all of the potential combinations of the candidate parameter settings.
for example if a classification technique accepts parameters with candidate parameter settings for each caret will explore all potential combinations of parameter settings unless the budget is exceeded .
we use repetitions of the out of sample bootstrap to estimate the performance of classifiers that are trained using each of the candidate parameter settings.
for each candidate parameter setting a classifier is fit to a subsample of the training corpus and we estimate the performance of a model using those rows in the training corpus that do not appear in the subsample that was used to trained the classifier.
step identify the caret optimized setting finally the performance estimates are used to identify which parameter settings are the most optimal.
the caret optimized setting is the one that achieves the highest performance estimate.
.
construct defect models in order to measure the impact that automated parameter optimization has on defect prediction models we train defect models using the caret optimized settings and the default settings.
to ensure that the training and testing corpora have similar characteristics we do not re balance or re sample the training data as suggested by turhan .
normality adjustment.
analysis of the distributions of ourindependentvariablesrevealsthattheyareright skewed.
assuggestedbypreviousresearch wemitigatethisskew by log transforming each independent variable ln x prior to using them to train our models.
.
calculate performance prior studies have argued that threshold dependent performance metrics i.e.
precision and recall are problematic because they depend on an arbitrarily selected threshold and are sensitive to imbalanced data .
instead we use the area under the receiver operator characteristic curve auc tomeasurethediscriminationpower of our models as suggested by recent research .
the auc is a threshold independent performance metric that measures a classifier s ability to discriminate between defective and clean modules i.e.
do the defective modules tend to have higher predicted probabilities than clean modules?
.
auc is computed by measuring the area under the curve that plots the true positive rate against the false positive rate while varying the threshold that is used to determine whether a file is classified as defective or not.
values of auc range between worst performance .
random guessing performance and best performance .
.
case study results in this section we present the results of our case study with respect to our two research questions.
rq1 how much does the performance of defect prediction models improve when automated parameter optimization is applied?
approach.
to address rq1 we start with the auc performance distribution of the classification techniques that require at least one parameter setting see section .
for each classification technique we compute the difference in the performance of classifiers that are trained using default and caret optimized parameter settings.
we then use boxplots to present the distribution of the performance difference for each of the studied datasets.
to quantify the magnitude of the performance improvement we use cohen sdeffect size which is the difference between the two means divided by the standard deviation of the data d x1 x2 s.d.
.
the magnitude is assessed using the thresholds provided by cohen effect size negligible if cohen s d .
small if .
cohen s d .
medium if .
cohen s d .
large if .
cohen s d furthermore understanding the most influential parameters would allow researchers to focus their optimization effort.
to this end we investigate the performance difference 325large medium small negligible .
.
.
.
.
c5.
adaboostavnnetcart pcannetnnetfda mlpweightdecaymlplmtgpls logitboostknn xgbtreegbmnbrbf svmradialgamboostrf ripperpmrpdamars svmlinearj48auc performance improvement figure the performance improvement and its coh en sdeffect size for each of the studied classification techniques.
for each of the studied parameters.
to quantify the individual impact of each parameter we train a classifier with all of the studied parameters set to their default settings except for the parameter whose impact we want to measure which is set to its caret optimized setting.
we estimate the impact of each parameter using the difference of its performance with respect to a classifier that is trained entirely using default parameter settings.
results.
caret improves the auc performance by up to percentage points.
figure shows the performance improvement for each of the studied datasets and for each of the classification techniques.
the boxplots show that caret can improve the auc performance by up to percentage points.
moreover the performance improvement provided by applying caret is non negligible i.e.
d .
for of the studied classification techniques .
this indicates that parameter settings can substantially influence the performance of defect prediction models.
c5.
boosting yields the largest performance improvement when caret is applied.
according to cohen sd the performance improvement provided by applying caret is large for of the studied classification techniques .
on average figure shows that the c5.
boosting classification technique benefits most by applying caret with a median performance improvement of percentage points.
indeed the c5.
boosting classification technique improves from to percentage points.
moreover figure3showsthatthe boosting iterations parameter of the c5.
classification technique is the most influential parameter while the winnowandmodel type parameters tend to have less of an impact.
indeed the default boosting iterations setting that is provided by the c5.
r package is indicating that only one c5.
tree model is used for prediction.
moreover we find that when large datasets of more than modules are analyzed the per boosting iterations of gamboostmax t ree depth of gbm boosting iterations of logitboostmax tree depth of xgbtree hidden units of nnet hidden units of pcannet hidden units of avnnet neighbors of knn hidden units of mlpweightdecay components of gpls iteratons of lmtweight decay of nnet hidden units of mlpweight decay of mlpweightdecay terms of fdamax tree depth of adaboostcomplexity parameter of cartweight decay of avnnetweight decay of pcannet boosting iterations of c5.
.
.
.
.
.
auc performance difference figure the auc performance difference of the t op most sensitive parameters.
formance of c5.
boosting with the default setting tends to underperform.
nevertheless we find that the optimal boosting iterations parameter is suggesting that the default parameter settings of the research toolkits are suboptimal for defect prediction datasets which agrees with the suspicion of prior studies .
in addition to c5.
boosting other classifiers also yield a considerably large benefit.
figure shows that the performance of the adaptive boosting i.e.
adaboost advanced neural networks i.e.
avnnet pcannet nnet mlp and mlpweightdecay cart and flexible discriminant analysis fda classification techniques also have a large effect size with a median performance improvement from percentage points.
indeed figure shows that the fluctuationoftheperformanceoftheadvancedneuralnetworktechniques is largely caused by changing the weight decay but not the hidden units orbagging parameters.
moreover thecomplexity parameter of cart and max tree depth of adaptive boosting classification techniques are also sensitive to parameter optimization.
caret improves the auc performance of defect prediction models by up to percentage points.
moreover the performance improvement provided by caret is non negligible for of the studied classification techniques .
rq2 how stable is the performance of defect pr ediction models when automated parameter optimization is applied?
approach.
to address rq2 we start with the auc performance distribution of the studied classification techniques on each of the studied datasets.
the stability of a classification technique is measured in terms of the variability of the performance estimates that are produced by the iterations of the out of sample bootstrap.
for each classification technique we compute the standard deviation 326large medium small negligible .
.
.
.
.
c5.
adaboostavnnetcart pcannetnnetfda mlpweightdecaymlplmtgpls logitboostknn xgbtreegbmnbrbf svmradialgamboostrf ripperpmrpdamars svmlinearj48stability ratio figure the stability ratio of the classifiers that a re trained using caret optimized settings compared to the classifiers that are trained using default settings for each of the studied classification techniques.
s.d.
of the bootstrap performance estimates of the classifiers where caret optimized settings have been used and the s.d.
of the bootstrapperformanceestimates of the classifiers where the default settings have been used.
to analyze the difference of the stability between two classifiers we present distribution of the stability ratio i.e.
s.d.
of the optimized classifier divided by the s.d.
of the default classifier of the two classifiers when apply to studied datasets.
similar to rq1 we analyze the parameters that have the largest impact on the stability of the performance estimates.
to this end we investigate the stability ratio for each of the studied parameters.
to quantify the individual impact of each parameter we train a classifier with all of the studied parameters set to their default settings except for the parameter whose impact we want to measure which is set to its caret optimized setting.
we estimate the impact of each parameter using the stability ratio of its s.d.
of performance estimates with respect to a classifier that is trained entirely using default settings.
results.
caret optimized classifiers are at least as stable as classifiers that are trained using the default settings.
figure shows that there is a median stability ratio of at least one for all of the studied classification techniques.
indeed we find that the median ratio of one tends to appear for the classification techniques that yield negligible performance improvements in rq1.
these tight stability ratio ranges that are centered at one indicate that the stability of classifiers is not typically impacted by caretoptimized settings.
moreover the caret optimized classifiers of of the studied classification techniques are more stable than classifiers that are trained using the default values.
indeed figure shows that there boosting iterations of gbmmax t ree depth of gbm randomly selected predictors of rf hidden units of pcannet trees of adaboost neighbors of knncost of svmradial hidden units of avnnet hidden units of rbfmax tree depth of adaboost boosting iterations of logitboost hidden units of nnet hidden units of mlpweightdecay hidden units of mlpweight decay of mlpweightdecayweight decay of pcannet boosting iterations of c5.0weight decay of nnetweight decay of avnnet terms of fda .
.
.
.
.
stability ratio figure the stability ratio of the top most sens itive parameters.
is a median stability ratio of .
nnet to .
mlp among the classification techniques where the stability has improved.
thisequatestoa39 stabilityimprovement for these caret optimized classifiers.
indeed figure shows that the stability of the performance of the advanced neural network techniques is largely caused by changing the weight decay but not the hidden units orbagging parameters which consistent with our findings in rq1.
caret optimized classifiers are at least as stable as classifiers that are trained using the default settings.
moreover the caret optimized classifiers of of the studied classification techniques are more stable than classifiers that are trained using the default values.
.
revisiting the ranking of classifica tion techniques for defect prediction models prior studies have ranked classification techniques according to their performance on defect prediction datasets.
for example lessmann et al.
demonstrate that of studiedclassificationtechniquesarestatisticallyindistinguishable.
on the other hand ghotra et al.
argue that classification techniques can have a large impact on the performance of defect prediction models.
however these studies have not taken parameter optimization into account.
since we find that parameter settings can improve the performance of the classifiers that are produced see rq1 we set out to revisit the findings of prior studies when caret optimized settings have been applied.
.
approach as keung et al.
point out dataset selection can be a source of bias in an analysis of top performing classification techniques.
to combat the bias that may be introduced by 327applying the sc ott knott esd testrank technique t2 t1dataset 100xtechnique 100xtechnique n ... auc performance d istributionauc performance d istribution applying the sc ott knott esd testrank technique t1 t2 t3dataset m 100xtechnique 100xtechnique n ... auc performance d istributionauc performance d istribution... lineplotsdataset t1t2t3 step c ompute likelihood repeat timeste chnique ranking for studied datasets step bootstrap analysis step resampling w ith replacementt1 t2 t3 .
.330dataset t1t2t3 2123likelihood ste p ranking generation figure an overview of our statistical comparison over mult iple datasets.
.
.
.
.
.
.
c5.
xgbtree avnnetgbmrf gplspdannetpmr gamboostpcannetmarsfda adaboostsvmradial mlpweightdecaymlprbfnb ripperlmtcart svmlinearj48knn logitboostlikelihood optimized classifier default classifier figure the likelihood of each technique appearing in the to p scott knott esd rank.
circle dots and triangle dots indicate the median likelihood while the error bars indicate the confidence interval of the likelihood of the bootstrap analysis.
a likelihood of indicates that a classification technique appears at the top rank for of the studied datasets.
dataset selection we use a bootstrap based ranking likelihood estimation rle experiment.
figure provides an overview of our rle experiment.
the experiment uses a statistical comparison approach over multiple datasets that leverages both effect size differences and aspects of statistical inference .
the experiment is divided into two steps that we describe below.
step ranking generation.
we first start with the auc performance distribution of the studied classification techniques with the caret optimized parameter settings and the default settings.
to find statistically distinct ranks of classification techniques within each dataset we provide the auc performance distribution of the bootstrap iterations of each classification technique with both parameter settings to a scott knott effect size difference esd test .
.
the scott knott esd test is a variant of the scott knott test that is effect size aware.
the scott knott esd test uses hierarchical cluster analysis to partition the set of treatment means into statistically distinct groups.
unlike the traditional scott knott test the scottknott esd test will merge any two statistically distinctgroups that have a negligible cohen s deffect size into one group.
the scott knott esd test also overcomes the confounding issue of overlapping groups that are produced by several other post hoc tests such as nemenyi s test which were used in prior studies .
we implementthescott knottesdtestbasedontheimplementation of the scott knott test provided by the scottknott r package and the implementation of cohen s dprovided by theeffsize r package .
we use the scott knott esd test in order to control for dataset specificmodelperformance sincesomedatasetsmay have a tendency to produce over or under performing classifiers.
finally for each classification technique we have different scott knott ranks i.e.
one from each dataset .
step bootstrap analysis.
we then perform a bootstrap analysis to approximate the empirical distribution of the likelihood that a technique will appear in the top scottknott esd rank .
the key intuition is that the relationship between the likelihood that is derived from studied datasets and the true likelihood that would be derived from the population of defect datasets is asymptotically equiva328lenttotherelationshipbetweenthelikelihoodthatisderived fro m bootstrap samples and the likelihood that is derived from studied datasets.
we first input the ranking of the studied classification techniques on studied datasets to the bootstrap analysis which is comprised of two steps step a bootstrap sample of datasets is randomly drawnwithreplacementfromtherankingtable which is also of comprised of size studied datasets.
step for each classification technique we compute the likelihood that a technique appears in the topscott knottesdrankinthebootstrapsample.
the bootstrap analysis is repeated times.
we then present the results with its confidence interval which is derived from the bootstrap analysis.
.
results c5.
boosting tends to yield top performing defect prediction models more frequently than the other studied classification techniques.
figure shows the likelihood of each technique appearing in the top scottknott esd rank.
we find that there is a likelihood of c5.
appearing in the top scott knott rank.
furthermore the bootstrap derived confidence interval ranges from to .
on the other hand when default settings are applied c5.
boosting has a likelihood of appearing in the top rank.
this echoes the findings of rq1 where c5.
boosting was found to be the classification technique that is most sensitive to parameter optimization.
unlike prior work in the data mining domain we find that random forest is not the most frequent top performer in our defect prediction datasets.
indeed we find that there is a likelihood of random forest appearing in the top scott knott rank with a bootstrap derived confidence interval that ranges from to .
a one tailed bootstrap t test reveals that the likelihood of c5.
producing a topperformingclassifierissignificantlylargerthanthelikelihood of random forest producing a top performing classifier .
.
this contradicts the conclusions of fernandezdelgado et al.
who found that random forest tends to yield top performing classifiers the most frequently.
the contradictory conclusions indicate that the domain specifics play an important role.
automated parameter optimization increases the likelihood of appearing in the top scott knott esd rank by as much as .
figure shows that automated parameter optimization increases the likelihood of of the studied classification techniques by as much as i.e.
c5.
boosting .
this suggests that automated parameter optimization can substantially shift the ranking of classification techniques.
c5.
boosting tends to yield top performing defect prediction models more frequently than the other studied classification techniques.
this disagrees with prior studies in the data mining domain suggesting that domain specifics play a key role.
furthermore automated parameter optimization increases the likelihood of appearing in the top scottknott esd rank by as much as .
.
discussion .
cross context defect prediction the performance improvement of defect prediction models is estimated using a bootstrap resampling approach see rq1 .
whilethisbootstrapresamplingapproachiscommon in other research areas recent studies in software engineering tend to estimate the performance of defect models using data from different contexts .
hence we perform an additional analysis in order to investigate whether caret still improves auc performance in a cross context setting.
weanalyzetheperformanceofdefectpredictionmodelsthat aretrainedinonecontext buttestedinanothercontext.
we then compute the performance improvement between the models that are trained with caret optimal and default settings.
caret still improves the performance of crosscontext defect prediction models by up to percentage points.
based on an analysis of releases of proprietary systems releases of apache xalan and releases of eclipse platform we find that the performance of crosscontext classifiers that are trained using caret outperform classifiers that are trained using default settings.
for example we find that when neural network classifiers are trained using eclipse platform .
and tested using eclipse platform .
the auc performance improves by percentage points when compared to classifiers that are trained using default settings.
this suggests that automated parameter optimization also yields a large benefit in terms of crosscontext defect prediction.
.
computational cost ourcasestudyapproachiscomputationally intensive i.e.
parameter settings out of sample bootstrap repetitions systems results .
however the results can be computed in parallel.
hence we design our experiment using a high performance computing hpc environment.
our experiments are performed on high performance computing machines with 2x intel xeon .
ghz hyper threads and gb memory i.e.
in total hyper threads machines hyper threads .
each machine connects to a petabyte shared storage array via a dual gigabit fibre channel connection.
for each of the classification techniques we compute the average amount of execution time that was consumed by caret when producing suggested parameter settings for each of the studied datasets.
caret adds less than minutes of additional computation time to of the studied classification techniques.
we find that the optimization cost of of the studied classification techniques is less than minutes.
we find that the c5.
and extreme gradient boostingclassificationtechniques whichyieldtop performingclassifiers more frequently than other classification techniques fall into this category.
this indicates that applying caret tends to improve the performance of defect models while incurring a manageable additional computational cost.
on the other hand of the studied classification techniques require more than additional hours of computation time to apply caret.
only adaboost mlpweightdecay and rbf incur this large overhead.
nonetheless the computation could still be completed if it was run overnight.
since defect prediction models do not need to be built very often in practice this cost should still be manageable.
.
threats to v alidity w e now discuss the threats to the validity of our study.
.
construct validity thedatasetsthatweanalyzearepartofseveralcollections e.g.
nasa and promise which each provide different sets of metrics.
since the metrics vary this is a point of variation between the studied systems that could impact our results.
however our within family datasets analysis shows that the number and type of predictors do not influence our findings.
thus we conclude that the variation of metrics does not pose a threat to our study.
on the other hand the variety of metrics also strengthens the generalization of our results i.e.
our findings are not bound to one specific set of metrics.
the caret budget which controls the number of settings that we evaluate for each parameter limits our exploration of the parameter space.
although our budget setting is selected based on the literature selecting a different budget may yield different results.
however the results of our studyshowthatamodestexplorationoftheparameterspace can already lead to a large change in the performance of defect prediction models.
our results from rq1 show that caret improves the performance of defect prediction models.
however the performance improvement may increase the complexity of defect prediction models.
thus we plan to investigate the relationship between model complexity and performance in future work.
.
internal validity we measure the performance of our classifiers using auc.
other performance measures may yield different results.
we plan to expand the set of measures that we adopt in our future work.
the generalizability of the bootstrap based ranking likelihoodestimation rle isdependentonhowrepresentative our sample is.
to combat potential bias in our samples we analyzedatasetsofdifferentsizesanddomains.
nonetheless a larger sample may yield more robust results.
prior work has shown that dirty data may influence conclusion that are drawn from defect prediction studies .
hence noisy data may be influencing our conclusions.
however we conduct a highly controlled experiment where known to be noisy nasa data has been cleaned.
nonetheless dataset cleanliness should be inspected in future work.
.
external validity westudyalimitednumberofsystemsinthispaper.
thus our results may not generalize to all software systems.
however the goal of this paper is not to show a result that generalizes to all datasets but rather to show that there are datasets where parameter optimization matters.
nonetheless additional replication studies may prove fruitful.
.
conclusions defect prediction models are classifiers that are trained to identify defect prone software modules.
the characteristics of the classifiers that are produced are controlled by configurable parameters.
recent studies point out that classifiers may under perform because they were trained using suboptimal default parameter settings.
however it is impracticalto explore all of the possible settings in the parameter space of a classification technique.
in this paper we investigate the performance of defect prediction models where caret an automated parameter optimization technique has been applied.
through a case study of datasets from systems that span both proprietary and open source domains we make the following observations caret improves the auc performance of defect prediction models by up to percentage points.
moreover the performance improvement provided by caret is non negligible for of the studied classification techniques .
caret optimizedclassifiersareatleastasstableasclassifiersthataretrainedusingthedefaultsettings.
moreover the caret optimizedclassifiersof 9ofthe 26studiedclassificationtechniques aremorestablethan classifiers that are trained using the default values.
caret increases the likelihood of producing a topperforming classifier by as much as suggesting that automated parameter optimization can substantially shift the ranking of classification techniques.
our results lead us to conclude that parameter settings can indeed have a large impact on the performance of defect prediction models suggesting that researchers should experiment with the parameters of the classification techniques.
since automated parameter optimization techniques like caret yield substantial benefits in terms of performance improvementandstability whileincurringamanageableadditional computational cost they should be included in future defect prediction studies.
finally we would like to emphasize that we do not seek to claim the generalization of our results.
instead the key message of our study is that there are datasets where there are statistically significant differences between the performance ofclassificationtechniquesthataretrainedusingdefaultand caret optimized parameter settings.
hence we recommend that software engineering researchers experiment with automated parameter optimization e.g.
caret instead of relying on the default parameter setting of the research toolkits assuming that other parameter settings are not likely to lead to significant improvements.
given the availability of automated parameter optimization in commonly used research toolkits e.g.
caret for r multisearch for weka gridsearch for scikit learn we believe that our recommendation is a rather simple and low cost recommendation to adopt.