are deep neural networks the best choice for modeling source code?
vincent j. hellendoorn computer science dept.
uc davis davis ca usa vhellendoorn ucdavis.edupremkumar devanbu computer science dept.
uc davis davis ca usa ptdevanbu ucdavis.edu abstract current statistical language modeling techniques including deeplearning based models have proven to be quite effective for source code.
we argue here that the special properties of source code can be exploited for further improvements.
in this work we enhance established language modeling approaches to handle the special challenges of modeling source code such as frequent changes larger changing vocabularies deeply nested scopes etc.
we present a fast nested language modeling toolkit specifically designed for software with the ability to add remove text and mix swap out many models.
specifically we improve upon prior cache modeling work and present a model with a much more expansive multi level notion of locality that we show to be well suited for modeling software.
we present results on varying corpora in comparison with traditional n gram as well as rnn and lstm deep learning language models and release all our source code for public use.
our evaluations suggest that carefully adapting n gram models for source code can yield performance that surpasses even rnn and lstm based deep learning models.
ccs concepts software and its engineering software maintenance tools keywords naturalness language models software tools acm reference format vincent j. hellendoorn and premkumar devanbu.
.
are deep neural networks the best choice for modeling source code?.
in proceedings of 11th joint meeting of the european software engineering conference and the acm sigsoft symposium on the foundations of software engineering paderborn germany september esec fse pages.
introduction there has been much interest in the idea of naturalness viz.
modeling and exploiting the repetitive nature of software using statistical techniques from natural language processing nlp .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse september paderborn germany copyright held by the owner author s .
publication rights licensed to association for computing machinery.
acm isbn .
.
.
.
models from nlp estimated over the large volumes of code available in github have led to a wide range of applications in software engineering.
high performance language models are widely used to improve performance on nlp related tasks such as translation speech recognition and query completion similarly better language models for source code are known to improve performance in tasks such as code completion .
developing models that can address and exploit the special properties of source code is central to this enterprise.
language models for nlp have been developed over decades and are highly refined however many of the design decisions baked into modern nlp language models are finely wrought to exploit properties of natural language corpora.
these properties aren t always relevant to source code so that adapting nlp models to the special features of source code can be helpful.
we discuss important issues and their modeling implications in detail below.
unlimited vocabulary code and nl can both have an unbounded vocabulary however in nl corpora the vocabulary usually saturates quickly when scanning through a large nl corpus pretty soon one rarely encounters new words.
new proper nouns people place names do pop up but do so infrequently.
code is different while each language only has a fixed set of keywords and operators new identifier names tend to proliferate .
modeling implications in nlp it s de regeur to limit vocabulary to the most common e.g.
words in a pre processing step before model estimation.
words outside this vocabulary are treated as an unknown word or omitted entirely.
this artificially limits the space of events over which to distribute probability mass.
similarly numerals and strings are replaced with generic tokens.
this works for nlp since words outside the dominant vocabulary are sorare.
virtually all work in modeling of source code borrows this approach.
in source code given the constant vocabulary innovation this approach is not appropriate.
we demonstrate that a closed vocabulary even if large does indeed negatively affect performance section .
and introduce methods to address this.
nested scoped locality while developers do invent new names for variables classes and methods the repeated use of these names tends to be localized.
in java e.g.
local variables parameters and private methods can be introduced used repeatedly in one scope and never used elsewhere.
the package structures in large systems can introduce nesting of such vocabulary scopes with different identifiers going in and out of use as one traverses the package hierarchy .
researchers have even noted applicationand developer specific vocabularies .
modeling implications this type of nested scoped vocabulary innovation is accompanied by corresponding repetition where certain esec fse september paderborn germany vincent j. hellendoorn and premkumar devanbu code structures involving specific local names repeat locally within their own nested scopes.
this requires a nested modeling approach which captures the local repetition within a scope si and then makes it available to scopes si si .
.
.nested within si.
furthermore if such nested models are used within an interactive tool such as an ide the model would need to be rapidly re estimated as the programmer s working context changes.
dynamism evolution is normal for well used software systems bug fixes and new features keep rolling in.
nlp corpora evolve much more slowly.
furthermore during interactive coding software tools must quickly adjust to new localities and contexts in a single coding session a developer may open and close many files.
as she explores the code a language model that works within the ide for code completion defect localization etc.
must rapidly adapt to the working context.
modeling implications traditional nlp models cannot handle rapid re estimation.
deep learning models in particular are not very dynamic and re estimation is very slow.
in response to the observations and concerns raised above we have developed a dynamic hierarchically scoped open vocabulary language model for source code that achieves best in class performance when using non parametric count based language modeling.
we make the following contributions we introduce mixed scoped models to handle arbitrary nesting and mixing of n gram models.
we implement these models using a fast datastructure optimized for dynamic scoped counting of language events.
we compare several popular smoothing techniques in related work and show that a simple approach not typically used works better than others.
finally we evaluate the performance of these models on a large corpus of java code in comparison and combination with implicit deep learning based models.
we find that our model outperforms the rnn and lstm deep learning models achieving unprecedented levels of entropy also performance on the code suggestion task.
we also show that our approach adds value even to lstm models.
our runnable api code and replication details can be found on github.com slp team slp core background we now review language models including explicit or non parametric count based models and implicit here deep learning models.
.
language models performance language models assign a probability to or score an utterance e.g.
a phrase a sentence or just a word .
models are estimated on large corpora of natural text.
a good language model should score an utterance high if it would sound natural to a native speaker and score low the unnatural or wrong sentences.
accurate scoring matters in tasks like machine translation where the output sentence should sound normal and likewise also in speech recognition summarization and spell checkers.
similarly in a source code environment language migration synthesis of code from natural language and vice versa as wellas code suggestion engines all need models that score code fragments accurately.
the usual way to score a code fragment sof length s is to tokenize it into e.g.
t1 t2 .
.
.and score each next token tigiven the previous tokens i.e.
p s s i 1p ti t0 .
.
.
ti this yields both per token probabilities and a single probability for the entire phrase.
more generally each token is predicted from its context c including its preceding tokens and perhaps additional information e.g.
from a previous phrase or the topic of a document .
since the probabilities may vary by orders of magnitude one often uses the typically negated logarithm of the phrase probability to arrive at the information theoretic measure of entropy hp s s log2p s s s i 1log2p ti c entropy reflects the number of bits needed to encode the phrase and analogously a token given the language model.
an alternative metric often seen in nlp literature is perplexity which is simply 2hp s and accentuates differences between higher entropy scores.
in equation the probability of a token in a phrase is calculated given all previous tokens.
in general this isn t practical once the corpus gets big enough.
there are two ways to approach this problem using explicitly defined rules and using implicit state .
.
explicit language models explicit modeling requires a restriction of the relevant context this approach is quite mature.
three prior classes of models are based on n grams on extracted long distance dependencies and on the use ofcaches .
these models generally require smoothing to account for rare or unseen events in the training data that may behave differently in the test data.
we now discuss these ideas.
n gram language models are easily constructed and popular .
these models simplify equation with a markov assumption each token is conditioned on just n 1preceding tokens.
we can then score a token using maximum likelihood of its occurrence in some context.
for instance in source code the score for i given the context for int is very high since the former frequently occurs in the latter context.
capturing this information would require a gram model to count sequences up to for tokens.
dependency models can capture long distance dependencies between tokens rather than dependencies between tokens that are sequential in the text1 which are sometimes more effective than markovian dependencies in left to right parses of natural language .
similar models have been proposed in source code using dependencies extracted by compilers.
researchers have modeled api invocations as a graph prediction problem and code completion by conditioning on identifiers in scope or parent nodes in the ast .
cache models augment n gram model with an additional cache n gram model to track just the local changes.
the two models can 1e.g.the dependency from walked todoginthe girl walked the restless yellow dog.
764are deep neural networks the best choice for modeling source code?
esec fse september paderborn germany be mixed using the cache s confidence at prediction time e.g.
based on how often it has seen the context as in .
these are distinct from dynamic models where the trained model is updated with all information at test time since caches are strictly local they can forget events e.g.
because caches are limited in size or be swapped out depending on the context caches are re initialized when a new file is encountered .
we expand on this notion in this work section .
smoothing n gram models come with a trade off longer n gram contexts are more specific but correspondingly less frequent in the corpus thus may occur in the test corpus but not in the training corpus.
meanwhile shorter contexts especially the empty context occur more often but lose information.
smoothing methods in language modeling provide a well founded way to combine information from various context lengths.
a smoothed n gram model starts at the longest order nfor which the context has been observed in the training corpus.
then the model assigning both a probability pnto the observed event given the context and a confidence nto the context per se.
the latter quantifies the amount of information that is present in the context a total of nprobability mass is divided among events seen in this context while a probability mass nis recursively passed to a shorter context.
generally the recursion halts after the empty context which represents the unconditioned token frequency distribution in the corpus where the left over probability mass is divided equally across the complete vocabulary.
the above method merges information present at all context lengths up to any n. the choice of nhas been well studied the nlp field and many smoothing methods have arisen each with its own way to compute n. in this work we consider four methods jelinek mercer jm smoothing uses a fixed confidence in all contexts we use .
and requires the least parameters.
witten bell wb assigns confidence to a context based on the average frequency of events seen in a context higher is better .
absolute discounting ad subtracts a fixed discount from the count of each event and re distributes this to unseen events thus penalizing rarely seen events most.
kneser ney kn improves upon ad by considering how likely a token is to appear in a new context e.g.
fransisco virtually always appears in the same context san and represents the state of the art in nlp n gram models.
both ad and kn can slightly be improved by using three separate discounts for events seen once twice and more than twice we use these versions and refer to them as mkn and adm m for modified .
we refer the reader to for more details on these techniques.
as we shall see the interpretation of nas confidence in a context according to a model will turn out to be a powerful tool in mixing n gram models that are trained on different corpora.
explicit models code the issues of unlimited vocabulary nested locality and dynamism introduced in section are not fully addressed by explicit models.
n gram models typically close the vocabulary at test time which would tend to diminish performance for locally used terms.
they do not consider scope at all nested or otherwise all text is treated uniformly.
finally traditional modelsassume static estimation use since nl text is generally unchanging.dependenc ybased models also limit vocabulary for reasons similar to n gram models the range of dependencies tend to be small and thus scope outside of the immediate sentence is rarely considered.
so far to date the dependencies considered in models of code are intra procedural and do not capture patterns in a nested scoped fashion.
they are not very good at handling dynamism since changes would typically require re the dependency analysis.
although incremental static analysis has been explored for code to our knowledge no one has used it for dependencybased statistical models thereof.
finally cache models do deal with vocabulary by accounting for alltokens within a single limited un nested typically at a single file level scope.
they do not deal with multiple nested scopes.
there is limited dynamism a single file s ngrams are counted and stored in the cache and this cache is flushed when a new file is opened.
this approach however ignores the nested scoped context of e.g.
the package structure in java and cannot quickly handle browsing if a developer closes a file a.java within a package sales.foo.com and opens another file b.java we would like to a flush the counts of sequences in a.java to the model of sales.foo.com b weight that model more than foo.com since it is more appropriate to b.java and c start tracking the counts in b.java and do all this efficiently at interactive speeds.
no existing model does this but we do.
.
implicit language models the above models all rely on explicit counts of actual n gram frequencies.
neural network models by contrast use an optimized high dimensional real valued parameter space to implicitly represent the co occurrence patterns of tokens in a large corpus.
this parameter space is estimated using gradient descent techniques which propagate loss gradients in entropy like functions over a training corpus.
we compare our approach with two popular techniques recursive neural networks and long short term memory networks.
both have been used to model software .
these models have proven to be quite powerful but are more computationally expensive to estimate this limits their dynamism as discussed later.
in most cases it is best to use them with high performance gpus.
recurrent neural networks rnn maintain a hidden state vector to capture a digested representation of the current context as they scan forward token by token.
learned parameters both read out this vector e.g.
to predict score a token and update this vector upon seeing the next token.
these models are quite effective when trained with sufficient data see for more details in a natural language and source code setting respectively .
long short term memory networks lstm are extensions of rnns which can be trained to selectively forget information from the hidden state thus allowing room to take in more important information .
implicit models code like earlier explicit models deep learning based models also were not designed for the specific vagaries of code.
first they strongly limit the vocabulary larger vocabularies 2for an accessible overview see also 765esec fse september paderborn germany vincent j. hellendoorn and premkumar devanbu substantially increase the number of parameters needed for a model partly because they require larger hidden state sizes which quadratically increases training time and increase the cost of predictions3.
character lstm models deal with this issue by modeling one character at the time instead and may provide a solution to this problem in the long term possibly in a hybrid form since these models at present cannot compete with word level models .
dealing with nested scoped vocabulary and dynamism with implicit models is difficult.
counts are not explicitly stored but are transformed in opaque non linear ways into real valued vectors.
it s not clear how to quickly update these to deal with code changes or interactive developer browsing.
dynamically adding observations to these models can be done with some success but there is no option for removing observations capturing localities or changing contexts.
on the other hand unlike explicit models these models effectively seamlessly and smoothly combine modeling of local and non local patterns.
lstm models are specially capable of capturing relevant long distance contexts which n grams fundamentally cannot do.
thus there is a clear opportunity to combine explicit models together with implicit models.
our approach consider the web app developer in figure who is working in a file named bill.java in package billing .
her development context begins with bill.java and then the proximate packages shipping customer and finally a host of other projects that also include repeating code patterns and variable names e.g.
from github all contexts could be captured in models.
in figure the developer queries the model with customer .
?
which might be an api recommendation task.
for this we could start with the longest context customer .
and ask each model for possible completions with corresponding mle probabilities.
if the global corpus hasn t seen this context it hasn t any say.
suppose the two local models customer ship and the cache have but that the latter has only seen bill in this context and gives it a high probability while the neighboring packages note that ship is a possibility as well without discrediting bill as an option.
these probabilities are mixed then we consider a shorter context .
and repeat here the global model may have a contribution such as the general prevalence of get among api calls.
algorithm describes how we assign probabilities to possible completions while considering all relevant localities assuming simple jelinek mercer interpolation j m .
although this example has only three levels of locality our approach generally improves with increasing project hierarchy depth deep hierarchies are common in our corpus.
when more than two models have seen a context the assigned probabilities get averaged serially from global to local strongly favoring the more local model s predictions.
the global model is entirely static and doesn t have to be an n gram model it could be any combination of models e.g.
lstm or rnn .
however the more local models should be highly responsive e.g.
if the developer suddenly switches to shipment.java we must quickly update the local model hierarchy.
these performance challenges are discussed next.
finally the confidence scores can be 3this applies particularly when using normal softmax output layers but also when using nce or cnn softmax bill.java billingweb appother projects shipment.java shipping customer.java customercustomer .
bill customer .
unk query customer .
?
customer .
ship customer .
bill ....get ... .
bill .
figure illustrative example in which a developer interacts with various localities while working on a file.
derived from various sources such as the language models themselves or even as parameters learned by an lstm.
their values are less important than the ordering of models from global to local so simple interpolation appears a good enough choice.
in fact in the newer implementations4each model just computes one probability and confidence score for the whole sequence after which we mix everything rather than mixing at every context length with no significant loss in accuracy.
.
dynamic scoped counting language modeling packages originally designed for natural languages statically estimate n gram probabilities a priori and store these in model files that are read when needed.
a probability and interpolation parameter is stored with each sequence making lookup relatively fast for known sequences.
however updating these models on line is complex and time consuming.
suppose when a new context is entered or a code change is made the observed frequency of j following for int increases e.g.
because another programmer prefers jover ias a loop counter in one particular file .
now changing the probability of one sequence or adding a novel event must affect many other sequences with the same context.
yet code bases are perpetually evolving and developers frequently switch files to work on this can make fine grained especially the above hierarchical models costly to compute specially in interactive settings.
ideally we would be able to rapidly alter models to fit our needs and construct smaller models on the fly yet not rule out static models e.g.
n gram or rnn models pre estimated on large external corpora when available.
to do so we deviate from conventional approaches that store probabilities and instead store counts.
these allow dynamic updating by incrementing decrementing language event counts.
we use a trie like data structure in which each token is a node in a tree with any number of successors leafs allowed.
at each node we store a number of values such as how frequently 4github.com slp team slp core 766are deep neural networks the best choice for modeling source code?
esec fse september paderborn germany algorithm probability calculation in nested model require global counts on global corpus may be empty require root root of project returns counters data structures holding the frequency of all sequences nested around a file from global to local function get counters file path directories from root inclusive to file counters fori counters do counters counters remove more local counts end for return global counters cache file add global file cache end function returns the j m probability of a sequence of tokens in a file function get jm probability file tokens counters get counters file retrieve nested counters probability forifrom tokens to do seq tokens start with shortest sequence context tokens may be empty p for all c counters do ifcontext cthen continue end if p p c.freq seq c.freq context end for probability probability p end for probability v add vocabulary base rate return probability end function simple j m smoothing amounts to averaging with other counters and context lengths.
technically no averaging is used for the first model that returns an observation omitted for brevity it occurs as a context to other tokens as well as its successors in a sorted array.
all tokens are pre translated using a vocabulary that is either dynamically constructed or pre computed.
as an optimization any sequence of tokens is initially stored as a simple array with an occurrence only if multiple successors are observed for the sequence is it promoted to a full fledged node.
this greatly reduces the memory footprint of our models which generally require more memory than conventional probability based models since many long sequences of tokens are singular appearances in source code similar to how many tokens are rarely seen i.e.
zipf s law .
with nested scopes each scope has it s own set of counts stored in a trie.
this allows quick switching of scopes or creation of new scopes.
in most cases we have between one and nested tries.
it s quite efficient typically to close one file and go to another it takes only a few milliseconds.
these nested tries allow probabilities and confidences to be computed by cumulating the various counts stored.
although this makes modeling slightly slower than using pre computed probabilities in the static case our model is still able to answer queries in microseconds and delivers all the benefits ofdynamicity.
furthermore we memoize the top 2nsuccessors at the most frequently seen nodes for prediction tasks these are re scored to get nranked suggestions in micro seconds at prediction time and the memoization is flushed if a change has been detected to the trie node e.g.
a context switch .
finally this choice of model achieves two additional benefits count based models allow any choice of models after thefact which means we can switch out smoothing methods and many other models that can work on counts of data e.g.
skip gram models perhaps even neural networks this model can represent any change granularity.
recent bug detection work has built models on snapshots at one month apart and tested on the ensuing changes until the next snapshot in order to make the computation tractable the models in this work can be updated with every commit from a project s inception and run in the order of minutes across thousands of commits which may also be beneficial for modeling code changes in code reviews .
evaluation methodology we use allamanis et al.
s giga token corpus which collects over thousand popular java projects from github5 sans forks .
we also use the original partition of this data into train of projects and test of projects sets.
for base lining we focus on a subset of the corpus since some prior models specially deep learning don t scale well.
we took of train and test projects from the original split and further took of the projects from the train split non overlapping to a validation corpus for the neural networks.
the corpus statistics are shown in table table .
table corpus statistics as reproduced from full corpus and train test validation splits.
full corpus train test valid projects files tokens 602m .98m .3m .8m to our knowledge our 16m token training corpus is the largest yet used to train deep learning models of source code.
to make training computationally feasible we fix the vocabulary forthe experiments involvingdeep learn ingmodels but not in our other experiments by removing tokens seen less than times in the training data and replacing these and any novel tokens in the validation and test data with a generic unknown token.
this limits the vocabulary to tokens comparable to prior work using deep learning .
this vocabulary limit is not needed for experiments not involving deep learning models our nested dynamic counting models can easily handle much larger vocabularies.
later on evaluation we relax the vocabulary limits to show that this limit falsely inflates modeling performance of all models.
5retrieved corpus statistics deviate slightly from those reported in the original work.
767esec fse september paderborn germany vincent j. hellendoorn and premkumar devanbu .
metrics we evaluate both intrinsic using entropy section .
and extrinsic using code suggestion performance of each of our models.
in the suggestion task the model provides an ordered list of suggestions for each token in the file given the context we collect the rank of the true next token in the file from this list.
we also collect the top predictions from each model and compute the top kaccuracy the fraction of times the correct suggestions appears in the top ksuggestions for k .
we mainly report mean reciprocal ranking mrr a summary metric which calculates the average over all predication points of the reciprocal of the correct suggestion s rank or if there are no correct suggestions .
this metric balances prediction accuracy across various k and can intuitively be interpreted as the inverse of the average expected position in the rank list.
for instance a mrr value of .
suggests the correct suggestion can be expected at position in the prediction list on average.
evaluation on other extrinsic tasks like bug prediction or variable renamings is left for future work.
partially due to the large number of samples even minor improvements e.g.
.
bits in entropy can be statistically significant in language modeling.
for all of our comparisons we used a paired two tailed t test we report when this was not the case.
the same applies to prediction accuracy scores which are strongly inversely correlated with entropy.
even if small improvements in terms of entropy can help many applications and shed more light on the repetitive nature of source code.
improving prediction accuracy is practically important and in our case also comes with faster implementations.
thus we expect that our mixed scope dynamic models would be the preferred choice when token level language models are needed.
we report effect sizes using cohen s d. .
model configurations tuet al.
s cache model is compared with our cache implementation as our approach to mixing differs both in terms of choice of interpolation parameters and in terms of choice of the models to be mixed.
for consistency with prior work we use modified kneserney gram smoothing for the global model and a gram back off cache model mixed with a dynamically computed concentration parameters as in .
we further vary the n gram order of both components from through .
white et al.
found that recurrent neural network works quite well for modeling source code .
we replicate their experiments using the recurrent neural network language model toolkit rnnlm6 which includes a hidden layer size of and direct connections.
more recent work has demonstrated that lstm networks achieve superior results for source code similar to natural language.
we adapt tensorflow s lstm code for our purposes testing two configurations corresponding to the small 7and medium configurations reported in both tensorflow s implementation8and various other work.
most notably the lstms embed their vocabulary into a statebased high dimensional euclidean space whereas rnnlm simply 7with two changes see next paragraph encodes its vocabulary.
embedding allows faster and potentially more accurate training as words are encoded in lowerdimensional potentially semantically more meaningful vectors.
for this reason we slightly alter the small configuration to closely match white et al.
s rnn configuration reducing it to a single tier and increasing its hidden layer size to neurons.
this allows us to interpret the differences in performance between the rnnlm and the lstm models as the gain achieved by embedding and using lstm units.
the medium configuration is left as is and uses two tiers of hidden layer neurons as well as drop out regularization during training.
this model was the largest that could be trained on our corpus in reasonable time approximately three days requiring passes over the 16m tokens of training data at an average of ca.
words per second on a tesla k40c gpu.
finally for extrinsic evaluation purposes we compare and combine our models with the two lstm configurations the most powerful benchmarks .
here we restrict ourselves to the first one million tokens in the test data storing the top ten suggestions for each model in a file and merging the suggestion lists after the fact.
we encountered slight irregularities in tensorflow s output that led it to occasionally skip predicting a token and accommodated our code accordingly these events accounted for less than of test samples and are unlikely to distort the results.
.
test settings we evaluate the models in different settings static similar to prior work dynamic which favours deep learning and maintenance which arguably most resembles developers normal mix of browsing coding activities.
static tests bystatic we mean training on a fixed training corpus and then evaluating the performance on a separate test dataset.
prior work evaluated models in intra project and cross project settings .
we begin in a setting similar to allamaniset al.
on our sub sets training on one set of projects and testing on a disjoint part of the corpus.
here we demonstrate the performance of our cache model in comparison to tu et al.
and the baseline performance of the recurrent neural networks.
we later refer back to this setting when we discuss the impact of vocabulary constraints that were frequently used in prior work.
dynamic tests while our static tests are strictly cross project in practice source code files are rarely modified in isolation.
our study subject projects comprise many nested directories.
we use a cache to exploit the nested context at test time in a rather limited way if the cache grows too large performance can actually decrease as the cached information becomes less locally relevant.
dynamic models deal with this problem differently they update with all information available at test time without any cache model type forgetting .
this provides considerable gains for neural network models .
this approach does not suffer from the too large cache problem discussed above because they do not mix a local and global model information is added to the global model directly however these models don t truly leverage the full potential of scoped locality as we shall see.
our second test setting models projects in a dynamic fashion allowing the models to observe the testing corpus left to right and absorb information to their training data as encountered.
the 768are deep neural networks the best choice for modeling source code?
esec fse september paderborn germany rnn lstm models are dynamically updated by training once on each new sample with a learning rate fixed to .
for the rnn and .
for the lstm models this yielded slightly better results than .
.
the neural network models do not at present have the ability to cache.
software maintenance tests finally we evaluate the model performance in a software maintenance setting where a developer explores code as she does her work.
in this maintenance setting the full test project is available for training minus only the file to be modeled.
the neural networks cannot take full advantage of this situation without constructing many models neither can traditional language modeling tool kits.
our dynamic models can however quickly update nested counts on the fly.
thus our nested models recursively accumulate the local repetitions from all nested levels of the project down to the target file.
as we shall see performance in this final setting is best in class and has ramifications for code synthesis fault localization code naturalization and many real world code suggestions settings .
results our first experiments clarify our choice of base n gram model design including a replication of the tu et al.
cache.
then we compare the tu et al.
cache with our nested cache models in both an intrinsic and extrinsic setting.
we next compare our static and dynamic models with rnn and lstm models first in a closedvocabulary setting and then following an analysis of the impact of such a constraint in an unbounded vocabulary setting.
we furthermore show mixture results between these classes of models in which we demonstrate the mutual performance and complementarity of lstm and n gram models particularly in terms of entropy.
our lstm models were trained using tensorflow on a tesla k40c gpu with 12gb graphics memory the rnn was trained using a single core of an intel xeon cpu and all other evaluations were run on an intel i7 4710mq processor with up to 24gb of ram.
.
initial experiments we implemented the three smoothing methods described in section .
jm wb and adm also using tu et al.
s code to evaluate mkn.
of these mkn is best suited for natural language followed by adm which we found performed nearly as well as mkn on a one billion token corpus from as well as on several small corpora.
mkn and sometimes wb have been used in most prior work.
we tried various lengths of n grams for our models and the jm model at n 6yielded a durable optimum for both the plain and cache models as shown in figure with higher orders yielding little to no gain past this point.
thus we use jm models in our experiments.
unfortunately the srilm toolkit used by tu et al.
did not work for orders or higher due to problems in the discounting.
nonetheless the trend from through grams suggests that we should expect little to no improvement beyond the gram level.
there are some notable patterns.
for one with natural language cache less n gram models beyond the or gram level achieve minimal gains and only with highly aggressive smoothing to lower orders mkn is typically capped at grams where it can achieve minor improvements over grams with large training corpora.
for jmwb admmknjmadmwb mkn 9entropy bits orderfigure modeling performance entropy obtained with various smoothers for n grams plotted against order of n. models without cache top and with cache bottom largely follow the same pattern with jelinek mercer smoothing outperforming more refined alternatives.
source code mkn behaves likewise with both unbounded as used here or restricted vocabularies as in white et al.
.
cache performance degrades notably with higher orders.
this pattern does notobtain with less aggressive smoothers especially jm.
perhaps mkn is too aggressive for source code where longer contexts seem to contain more useful information.
prior work has used gram kneser ney smoothed models for source code but this model scores about .
bits of entropy worse than jm6 in our experiments .
bits for the cache model .
interestingly jm is very simple smoothing approach perhaps further improvements could arise from more software appropriate smoothing methods.
many previous approaches report reductions in entropy by using some measure of local state hidden explicit or both into their models reporting improvements over simple n gram models.
tu et al.
s cache model provides an elegant way of incorporating local state but has unfortunately not usually been explicitly compared.
we hope that the availability of our tool with implementations can help set the standard for n gram baselines in work on modeling of source code.
finally we briefly note that all these results did replicate in the restricted vocabulary setting that we explore in section section .
which was used in most related work.
.
cache models we now expand the idea of caching extending it to nested scopes mixed as described in section .
nested models only apply when the models have a view of the context of the file to be modeled either incremental or full.
thus we demonstrate results in two settings dynamically updated models which add each file to their training corpus after modeling it and software maintenance models which are allowed to see each file in the project excluding the one to be modeled as well as the nested scoped directory hierarchy in which the test file occurs.
in figure we show the results.
the flat models without and with cache treat all seen files as a single flat corpus.
they show higher entropies of .
.
bits and .
.
respectively mrr performance is around .
without cache and rises to .
with cache.
however hierarchy aware nested models substantially boost model prediction performance entropy decreases by about bits for the cache lessmodel and about bit otherwise with concomitant 769esec fse september paderborn germany vincent j. hellendoorn and premkumar devanbu .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
flat nested flat nested no cache with cache entropy bits accuracy mrr mrr dynamic mrr maintenance entropy dynamic entropy maintenance figure entropy and mrr prediction performance in a dynamic and software maintenance setting for both nonnested and nested models.
each model is shown with and without a cache component.
increases in mrr.
remarkably the nested model without a local file cache outperforms the non nested model with cache in the maintenance setting.
this suggests that information in nearby files is almost as useful as information from the same file.
prediction accuracy also increases with nested models boosting maintenance setting mrr to a best in classs .
and .
in a dynamic setting again with very good timing performance.
again we emphasize that improvements are statistically significant p 1e small effect sizes in general e.g.
.
between cache and nested cache and .
between plain and nested plain .
.
implicit models baseline we compare our baseline jm smoothed n gram models with and without cache with the rnnlm and lstm implementations in table table .
please note inthissectiononly we restrict the vocabulary to words as specified in section .
consequently the overall entropy scores are deceptively lower than in previous sections even for the n gram model we shall revisit this result later.
the rnn lstm models outperform the plain n gram model in the static setting by upto a bit and see section section .
the kn baseline used in most prior work by well over a bit .
furthermore the lstm model outperforms the rnn baseline despite their similar configuration which can be attributed to the use of lstm units and word embeddings in this class of model.
this is in line with prior work .
finally the largest lstm model which was stacked and trained with drop out achieves best results in this cross project setting.
nonetheless the non nested cache model outperforms all of these by a margin by only incorporating a small amount of local information p 1e cohen s d .
compared with lstm .
the comparison between the lstms and cache model is complicated since both model local information at test time the cache model does so explicitly while the lstm represents local state implicitly in its hidden state vector.
in the second setting dynamic column both models are allowed to dynamically update with any content seen after modeling making a more fair comparison.
in this setting the plain cache model is somewhat worse than thebest lstm .
the nested cache model however decisively outperforms all other models p 1e cohen s d .
compared lstm .
finally in the maintenance setting our best model nested cache scores .
bits p .
cohen s d .
compared with dynamic lstm the best dynamic lstm .
the deep learning models are not applicable in the maintenance setting testing on each file given all other files in the nested directories of tested project would be intractable even on this small corpus.
table modeling results for our various baselines.
the nested models are not applicable in a cross project setting and the deep models cannot at present accommodate software maintenance settings in any tractable manner.
modelsettingstatic dynamic maintenance plain .
.
.
cache .
.
.
nested plain .
.
nested cache .
.
rnn .
.
lstm .
.
lstm .
.
mixing results we now report prediction accuracy for our nested n gram models and lstms in isolation and importantly when mixed together.
first as in prior work we credit models for predicting out of vocabulary words as unknown since we use a restricted vocabulary and then in the next section report an alternative.
the mixing procedure is a simple interpolation of probability scores since the lstms do not report a confidence score with their predictions future work may investigate methods to do so .
we compare with the lstms as these performed best in our previous experiment focusing specifically on the first one million tokens in the test set for timing purposes.
each lstm model required on average hours of run time on a tesla k40c to generate top predictions for these test sets our models are much much faster.
as expected from the entropy results see table in the static setting the two lstm models beat the plain n gram model but are beaten by the cache model.
mixing these models yields small but significant gain for the cache model in both intrinsic and extrinsic terms.
our best performing models jm cache and lstm mix best yielding the best entropy score in a static setting .
bits and boosting the cache s mrr from to p 1e 10either way cohen s d .
vs. cache .
vs lstm .
mixing lstms and plain models all yielded significant improvements in terms of entropy but not in terms of prediction accuracy suggesting the plain model is too weak to complement the lstms.
in the dynamic setting mixing the best performing models dynamic lstm .
bits and nested cache n gram .
bits gives the best combined score of .
bits of entropy p 1e either way cohen s d .
vs. nested cache .
vs dynamic lstm yielding a best in class token level model for source 9the lstm model proved more amenable to dynamic updating possibly because the larger model would need several passes over each new observation to update its larger number of parameters.
770are deep neural networks the best choice for modeling source code?
esec fse september paderborn germany table isolated first row and column and mixture intersection of models mrr prediction results for various crosscombinations of models from two categories deep learning networks and n gram networks.
static setting lstmn gram plain cache .
.
lstm .
.
.
lstm .
.
.
dynamic setting lstmn gram plain cache .
.
lstm .
.
.
lstm .
.
.
code suppressing the information content of source code to around bit per token!
notably unlike our approach is language agnostic.
however the prediction scores don t improve as expected the cache model appears to dominate the lstm so that simple mixing does not help.
in the next section we will see how a more realistic scenario actually shows beneficial mixing.
.
vocabulary use for source code the excellent model combinations above with single bit entropy scores and unprecedented prediction accuracy come with a big caveat also applicable to prior work this is due to closed limited vocabularies .
this arises from a convention in natural language to train using a finite vocabulary on a training corpus and close this vocabulary at test time.
oddly enough the term open vocabulary refers to the practice of closing the vocabulary at test time but treating new tokens as unknown tokens rather than skipping them entirely.
in source code limiting vocabulary on training data is arguably inappropriate.
developers introduce new vocabulary in new scopes and use them in contained scopes.
code models must capture this.
consider figure showing the misleading lift in performance y axis with artificially limited vocabularies which cuts off events seen just a given number of times x axis in the training data solid lines and are closed at test time replacing all unseen events in the test data with the same generic unknown token compared to the best estimate of the true performance flat dashed lines in which the vocabulary is never closed .
for many settings including code suggestion only the latter curve matters predicting tokens as unknown at test time is unhelpful.
several patterns stand out from figure the plain model jumps substantially in performance when the vocabulary size decreases even just a little.
a similar effect can be observed for the cache model from just closing a vocabulary at test time.
crucially however the plain model cannot attain the performance of the dotted cache baseline even with a closed vocabulary and a count cut off that reduces the vocabulary size by two thirds i.e.
cutoff .
we thus conclude two things .
reducing vocabulary size and or closing it 5entropy bits unknown cutoff baseline plain baseline cache closed plain closed cache 5mrr vocabulary cut off open plain open cache closed plain closed cachefigure vocabulary cut off minimum number of times an event must be seen in training data to be included in the vocabulary vs. performance on two mainstream metrics solid lines as well as truly open vocabulary at test time dashed lines .
at test time causes substantial but misleading inflation in modeling and prediction performance and .
a cache component elegantly deal with the vocabulary innovation problem while requiring no artificial limits.
ramifications for deep learning the lstm and rnn models struggle with vocabularies larger than those we used.
a recent investigation of natural language which heavily optimized performance of lstm models required over one tesla k40c gpu year to achieve optimal performance with a vocabulary of less than 800k although our subset has a vocabulary of 200k the full dataset s vocabulary exceeds 12m.
training our biggest lstm models with just 76k vocabulary took many days compared to ca.
seconds for the explicit models .
furthermore without using character level models which so far don t work as well as word level models in nlp opening the vocabulary remains impractical.
next we repeat the experiments in section .
but without giving credit to the lstm models for predicting an unknown token.
for entropy the lstm models are assigned the vocabulary baserate score corresponding to .
bits of entropy in this setting for each encountered unknown token exactly as the plain n gram model with our smoothing approach .
the n gram models here are trained with no vocabulary constraints and tested similarly thus never predicting an unknown token.
this setting can be interpreted as one in which the lstm models are trained as an aide to ngram models where the former have the upper hand in terms of static modeling capacity the latter contribute dynamic insights that can be rapidly obtained and integrated at test time as well as greater vocabulary range at training time to account for less common events.
the mixture works as before only defaulting to then gram model when the lstm predicts the unknown token.
the prediction results are shown in table .
as can be seen the lstm model s numbers are much worse as are the plain n gram 771esec fse september paderborn germany vincent j. hellendoorn and premkumar devanbu model s. this does notapply to the cache based n gram model in the static setting or to the nested model in the dynamic setting the nested cache model loses only .
points still achieving an mrr of .
and top accuracy of .
!
interestingly the mixtures no longer improve each others performance in all but the static plain model.
in terms of entropy performance however we still find substantial gain the best models prove remarkably complementary decreasing each other s entropy scores from .
nested cache and .
dynamic lstm to .
bits per token p 1e either way cohen s d .
vs. nested cache .
vs dynamic lstm .
this is not only the best ever reported entropy score without vocabulary limit but also astonishingly close to the mixture of these same models with vocabulary limits!
this due to a high degree of complimentarity while the lstm predicts the unknown token costing it .
bits the nested cache has an average entropy of about .
bits well below its own average.
contrariwise on the other tokens the lstm outperforms the nested cache by .
bits and by over bit on tokens that the nested cache assigns bits or less .
these models excel in different contexts and contribute mutual robustness.
this complementary potential is reflected in the standard deviations of entropy scores for the lstm .
bits nested cache s.d.
.
bits and the mixture .
bits which has the most narrow range of all.
thus we see substantial complementarity in terms of intrinsic modeling performance.
table isolated first row and column and mixture intersection of models mrr prediction results for various crosscombinations of models from two categories deep learning networks and n gram networks.
static setting lstmn gram plain cache .
.
lstm .
.
.
lstm .
.
.
dynamic setting lstmn gram plain cache .
.
lstm .
.
.
lstm .
.
.
discussion deep learning vs. count models dl models are now used everywhere including for modeling of source code .
however dl models use a great many parameters require extensive configuration and are also often heavily tuned on the task at hand.
in addition they are often compared with rather simple baslines which casts doubt upon any often minor improvements observed.
in the nlp community omer et al.
conducted various investigations into word embeddings semantic representations of words and found that state of the art neural network models perform similarly to simple matrix factorization models provided the latter 10see also for a discussion of this phenomenon.were enriched with just a few hyper parameters .
our work paints a similar picture rnn lstm fail to beat a well calibrated cache model even sans tuning on validation data .
we do find however that rnn lstm complement our dynamic mixed scope model!
while high training costs and the difficulty of incorporating local information make dl insufficient on its own it can provide semantic insights beyond the reach of simpler models.
we encourage future work to focus on fair comparisons between deep and simpler models and optimize both in conjunction.
lstm probabilities we found lstm model predictions to be often quite polarized the correct next tokens often scored very high or very low.
this might arise from the softmax output layer which can accentuate differences.
this partially explains the positive mixture results the n gram models are more conservative in their estimates rarely either ruling out events entirely or having high confidence in an observation.
we studied the ca.
.
of cases in which the lstm assigned probability to its prediction and found that .
of these were correct.
this may help when applying neural networks to study program properties in this line of work soundness is often a strong requirement and neural networks cannot as yet guarantee this.
however the lstm s inclination to assign high probabilities only in cases of great certainty can prove a very helpful property in generating samples for search based sbse methods.
training corpus size current lstm models can t manage the largest corpus size exceeding 1b tokens but our n gram models can do so handily.
we trained a gram model on the full training data of the corpus from table requiring ca.
hours and 12gb of ram and tested it on the same test data.
the nested cache model on this corpus shows a small but significant gain of .
bits and .
mrr.
conclusion we have made the following contributions.
we introduce a dynamically updatable nested scope unlimited vocabulary count based n gram model that significantly outperforms all existing token level models including very powerful ones based on deep learning.
our model is far faster than the deep learning models.
our nested cache model achieves an mrr performance of .
with unlimited vocabulary .
with limited vocabulary which isbest in class.
.
our work illustrates that traditional approaches with some careful engineering can beat deep learning models .
we show that our count based approach plays well with lstm models and yields even better performance in combination particularly in terms of entropy scores where the best mixture achieving .
bits of entropy per token without constraining the vocabulary .
our detailed evaluations reveal some new observations jelinek mercer smoothing outperforms smoothing approaches used in prior work.
limiting vocabularies artificially and misleadingly boosts intrinsic performance without boosting actual performance on the suggestion task.
772are deep neural networks the best choice for modeling source code?
esec fse september paderborn germany