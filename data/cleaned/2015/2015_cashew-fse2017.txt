constraint normalization and parameterized caching for quantitative program analysis tegan brennan nestan tsiskaridze nicol s rosner abdulbaki aydin and tevfik bultan department of computer science university of california santa barbara ca tegan nestan rosner baki bultan cs.ucsb.edu abstract we present a constraint caching framework to expedite potentially expensive satisfiability and model counting queries.
integral to this framework is our new constraint normalization procedure under which the cardinality of the solution set of a constraint but not necessarily the solution set itself is preserved.
we extend these constraint normalization techniques to string constraints in order to support analysis of string manipulating code.
we use a grouptheoretic framework which generalizes earlier results to express our normalization techniques.
we also present a parameterized caching approach where in addition to storing the result of a modelcounting query we store a model counter object that allows us to efficiently recount the number of satisfying models for different bounds.
we implement these techniques in our tool cashew which is built as an extension of the green caching framework and integrate it with the symbolic execution tool symbolic pathfinder spf and the model counting constraint solver abc.
our experiments show that constraint caching can significantly improve the performance of symbolic and quantitative program analyses.
for instance cashew can normalize the unique constraints in the smc kaluza benchmark down to normal forms achieve a 10x speedup on the smc kaluza big dataset and an average 3x speedup in our spf based side channel analysis experiments.
ccs concepts software and its engineering formal software verification keywords constraint caching quantitative program analysis model counting string constraints acm reference format tegan brennan nestan tsiskaridze nicol s rosner abdulbaki aydin and tevfik bultan.
.
constraint normalization and parameterized caching this material is based on research sponsored by nsf under grant ccf and by darpa under the agreement number fa8750 .
the u.s. government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation thereon.
the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements either expressed or implied of darpa or the u.s. government.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse september paderborn germany association for computing machinery.
acm isbn .
.
.
.
quantitative program analysis.
in proceedings of 11th joint meeting of the european software engineering conference and the acm sigsoft symposium on the foundations of software engineering paderborn germany september esec fse pages.
introduction improvements in the area of satisfiability modulo theories and powerful smt solvers have been key technological developments enabling the rise of effective symbolic program analysis and testing techniques in the last decade .
performing symbolic analysis via satisfiability checking however is not sufficient for quantitative program analysis which is an important problem that arises in many contexts such as probabilistic analysis reliability analysis and quantitative information flow .
the enabling technology for quantitative program analysis is model counting constraint solvers.
a model counting constraint solver returns the number of solutions for a given constraint within a given bound .
since constraint solving and model counting are heavily used in program analysis improving performance of these tasks is of critical importance.
in this paper we present a new approach for constraint normalization and constraint caching with the goal of improving the performance of quantitative program analyses.
the key step in constraint caching is normalization of constraints i.e.
reducing constraints to a normal form where two constraints are reduced to the same form only if they are equivalent w.r.t.
satisfiability or model counting .
using the normal form of a constraint as a key we can recover results of previous satisfiability or model counting queries without recomputing them.
earlier techniques for constraint caching focus only on numeric constraints and do not handle string constraints use normalization techniques that preserve the exact solution set of a constraint which reduces cache hits for model counting queries and always produce cache misses for model counting queries if a different bound is used even if the queried constraint remains the same.
in this paper we extend earlier results in multiple directions we present constraint normalization techniques for model counting under which the solution set of the constraint may not be preserved but its cardinality is.
we extend constraint caching to string constraints which is crucial for analyzing string manipulating code.
we present a parameterized caching approach where in addition to the result of a model counting query we also cache a counter object in the constraint cache that allows us to efficiently recount the models for different bounds.
we formalize our normalization scheme using an extensible group theoretic framework for constraint normalization that 535esec fse september paderborn germany t. brennan n. tsiskaridze n. rosner s. aydin and t. bultan generalizes earlier results on constraint normalization for caching.
we implemented these techniques in our tool cashew which is built as an extension of the green caching framework .
we integrated cashew with symbolic pathfinder spf and the abc model counting constraint solver.
our experiments demonstrate that constraint caching can improve the performance of quantitative program analysis significantly.
the paper is organized as follows in section we provide some motivating examples for constraint caching.
in section we give an overview of our constraint caching framework.
in section we discuss our group theoretic normalization scheme.
in section we describe the constraint language we support.
in sections and we present the constraint normalization procedure.
in section we present our experiments.
in section we discuss related work.
in section we present our conclusions.
in section we describe how to obtain and use the implementation.
motivation the amount of string manipulating code in modern software applications has been increasing.
common uses of string manipulation include input sanitization and validation in web applications query generation for back end databases generation of data formats such as xml and html dynamic code generation dynamic class loading and method invocation.
in order to analyze programs that use string manipulation it is necessary to develop techniques for efficient manipulation of string constraints.
recently there has been significant amount of work in string constraint solving to address this problem .
one of our contributions in this paper is a constraint normalization and caching framework that can handle string constraints.
consider the following string constraint f b https prefix of b url c ?
contains c url w index of w url the solution set of fis the set of values that can be assigned to the string variables b c w and urlfor which fevaluates to true.
constraints such as fcommonly arise in symbolic program analysis.
for example fmight correspond to a path constraint generated during symbolic execution of a string manipulating program.
a fundamental question about a constraint fgenerated during program analysis is its satisfiability.
symbolic program analysis techniques generate numerous satisfiability queries while analyzing programs.
given that satisfiability checking is computationally expensive it is crucial to answer satisfiability queries efficiently in order to build scalable symbolic program analysis tools.
on the other hand quantitative program analysis techniques ask another type of question while analyzing programs.
assume that we bound the length of the string variables b c w and urlin constraint fto .
how many different string values are there for the variable bsuch that the constraint fis satisfiable within the given bound?
these types of queries can be answered by modelcounting constraint solvers.
again due to the high complexity of model counting answering model counting queries efficiently is crucial for quantitative program analysis.now consider the following string constraint g k w http contains k var0 z index of z var0 prefix of w var0 constraint gis a constraint on string variables k z w andvar0.
assume that constraints fandgare generated during program analysis and it is necessary to check the number of satisfying solutions and satisfiability of each.
can we avoid making redundant calls to the constraint solver?
note that the solution sets of fand gare different since different string constants appear in these two constraints.
however the satisfiability and the cardinality of the solution sets for these two constraints are identical.
hence if we were able to detect the relationship between the number of satisfying models of fandgand had stored the result of a model counting query on f then when we see gwe do not have to call the modelcounting constraint solver again.
same for satisfiability queries.
the problem of reusing information about fto answer our questions about ghas now been reduced to finding a fast way to determine that fandgare equivalent with respect to satisfiability and model counting.
in this paper we present a constraint normalization scheme to determine this type of equivalence.
based on our scheme the normalized form of fandgare identical v0 a v1 bccde contains v0 v2 prefix of v1 v2 v3 f index of v3 v2 hence given a constraint to determine if an equivalent constraint has already been encountered we normalize it and check if its normal form was seen previously.
using a constraint store to cache the results of prior queries to the solver we avoid redundant queries for constraints that have the same normalized form.
for both satisfiability and model counting queries we can cache the result of the query in a constraint store use normalization to determine equivalence of constraints and then reuse the query results from the store when we get a cache hit.
however since model counting queries come with a bound parameter in order for the query to match the bound also has to match.
while this limits our ability to reuse results in the most general case there is a class of model counting constraint solvers whose results can be reused even is the case of mismatched bounds.
parameterized model counting techniques not only count the number of solutions for a constraint within a given bound but also generate a model counter that can count the number of solutions for any given bound.
note that counting the number of solutions with different bounds may be necessary during program analysis.
for example consider the following constraint which has no solutions for bounds less than but has satisfying solutions for higher bounds contains x abcde y x y ab in this paper we present a parameterized caching approach that utilizes parameterized model counting constraint solvers.
we assume that in response to a model counting query parameterized model counting constraint solvers return a model counter object that can be used to count the number of models for any given bound.
by storing the model counter object we are able to reuse modelcounting query results even for queries with different bounds.
536constraint normalization and parameterized caching for quantitative program analysis esec fse september paderborn germany conjunct sortervariable renamerstring alphabet renamerconstant shifterstore solvertranslatorclient reusemgrmodel counter object evaluator f v b f v b f v b m c object b normalization procedure f v b f v b f v figure architecture of cashew constraint caching our tool cashew depicted in figure is designed to work with a wide range of model counting solvers to support quantitative program analyses.
algorithm outlines how cashew handles modelcounting queries.
cashew expects a query of the form f v b where fis a well formed formula vis a set of variables in f and b is a bound.
the answer to the query denoted as f v b is the number of satisfying solutions for ffor the variables in vwithin the bound b. we normalize the formula variable s and bound using our normalization procedure normalize query which is described in the following sections.
the resulting normalized query is denoted as f v b normalize query f v b .
depending on the capabilities of the selected model counting constraint solver f v b is queried differently.
algorithm outlines the normalization and query process.
typical model counting constraint solvers return a single count value f v b after receiving a query of the form f v b .
for such solvers our caching algorithm first sends the query f v b to the cache store.
if there is a cache hit the result is returned to the client.
if not the normalized query is sent to the model counting solver and the result is stored under f v b and returned to the client.
we call a model counting constraint solver paramaterized if it returns a model counter object that can be used to compute the number of satisfying solutions for an arbitrary bound.
abc is a parameterized model counting constraint solver where the model counter object is the transfer matrix of an automaton that accepts all satisfying models of the given constraint.
smc and barvinok are also parameterized model counting constraint solvers where the model counter object is a generating function.
for parameterized solvers the store is queried as follows first f v b is queried.
on a hit the result f v b is returned to the client.
in the case of a miss an additional query for f v is made.
if this results in a hit the model counter object for f v is recovered from the store.
this model counter object is sent to the model counter evaluator which evaluates f v b based on b .
the result returned by the model counter evaluator is stored under f v b and is returned to the client.
if both queries are misses the selected solver is called the model counter object is computed and cached under the key f v and f v b is evaluated based on b stored under f v b and returned to the client.in order to use cashew s parameterized caching functionality and reuse cached model counter objects a service that is able to take a model counter object such as a transfer matrix or a generating function and evaluate it for a particular bound is required.
this service is referred to as the model counter evaluator.
algorithm constraint caching f v b input a query f v b .
output the number of satisfying solutions of vinfunder the length bound b. f v b normalize query f v b ifhit on f v b then return f v b end if ifhit on f v then evaluate the model counter object for bound b using the model counter evaluator store the result under f v b return f v b end if translate f v b andsend it to the selected model counting solver store the returned model counter object under f v store f v b under f v b return f v b group theoretic framework the goal of normalization is to reduce constraints equivalent under some property to the same form.
this objective is shared by work in constraint programming where detecting symmetries in constraints leads to a more efficient search .
symmetrybreaking for constraint programming is expressed using concepts from group theory a formalization we find fitting and intuitive for our purposes and adopt.
our framework provides a means for constructing normal forms of constraints based on groups of property preserving transformations.
for different analysis problems it might be necessary to preserve the entire solution set the cardinality of the solution set or only the satisfiability of constraints each corresponding to a different level of normalization.
our framework is equally applicable regardless of the desired level of normalization.
our framework is also not restricted to a constraint language but is equally applicable to any background theory on which a group of property preserving transformations can be defined.
symmetry groups.
a group g op is a set of elements together with a binary operator that satisfies the four group axioms closure associativity identity and invertibility.
for example the set of all transpositions on the natural numbers n under the binary operator function composition form a group.
the transposition fromnto itself defined by the relation is an example of an element of this group which maps to to and all other elements of nto themselves.
a subset of a group is called a subgroup if it also forms a group under the same binary operator.
we construct the group of cardinalitypreserving transformations under composition gcard by introducing its generating subgroups.
as composition is the only binary operator we consider we simply refer to this group as gcard throughout the remainder of the paper.
solution set preserving subgroups of gcard.a solution set preserving transformation is one under which the solution set is mapped 537esec fse september paderborn germany t. brennan n. tsiskaridze n. rosner s. aydin and t. bultan to itself.
any solution set preserving transformation is trivially cardinality preserving.
each generating subgroup acts on a particular domain related to some feature of a constraint.
the first generating subgroup we introduce acts on i the domain of all possible indices of conjuncts in a constraint.
here we consider the index of a conjunct to be its position in that constraint when read from left to right making isimply the set of natural numbers.
the subgroup acting on iis the group mentioned previously that of all transpositions on nor identically the permutation group whose elements fix all but a finite number of numbers.
intuitively this subgroup captures our understanding that the solution set of a constraint is independent of the order of the conjuncts in it.
under the transposition for example the formula x y 0is mapped to y x making the two orderings equivalent modulo the action of this group.
our second solution set preserving subgroup is the transposition group acting on v the infinite domain of all allowable variable names.
since the solution set of a constraint is independent of the choice of variable names two constraints that are equivalent modulo the action of this group have the same solution set.
as a simple example realize that both wandxare elements of vand that the number of solutions for x x 2is the same as that of its mapping under the relation x w w x w w .
cardinality preserving subgroups of gcard.preserving only the cardinality of the solution set of a constraint enables the use of subgroups with less constrained group actions.
under these groups the solution set of a constraint is bijectively mapped to the solution set of another constraint leaving the number of solutions unchanged.
our first family of cardinality preserving subgroups are given by the euclidean groups e n symmetry groups on euclidean space acting on the solution space of linear integer arithmetic constraints.
the elements of these groups are euclidean motions such as translations rotations and indirect isometries such as reflection.
under these symmetries of euclidean space the volume captured by the corresponding polytope remains unchanged.
though this volume is preserved under any action of the euclidean group some actions impact the number of lattice points in the polytope.
because we are often interested in the number of integer solutions to a constraint we limit ourselves to considering only those transformations that preserve the number of lattice points as well as those that can be easily reflected through changes in the syntax of the constraint.
in particular our normalization scheme uses the subgroup of integral translations in euclidean space as a generating subgroup for gcard.
integral translations can be reflected syntactically in integer constraints through changes in the constant terms of each conjunct.
each constant term must be identically shifted by an integral amount.
for example shifting each constant term of the constraint x y x y 0by results in the constraint x y x y 2which has the same number of integer solutions as the original.
for any arithmetic constraint f the shift off denoted shift f is the vector composed of the constant terms of each of its conjuncts.
sh denotes the domain of all possible shifts.
the subgroup of integral translations thus acts on sh.
for string or mixed constraints we do not apply transformations from this subgroup and we say thatshift f of such constraints is .our second cardinality preserving subgroup is given by the permutation group on the string alphabet .
the solution set of a string constraint can be canonically represented by an automaton that accepts exactly the set of solutions to that constraint.
transitions between states are made based on a set of allowed alphabet symbols.
permuting the alphabet symbols thus changes the strings accepted by that automaton but not the cardinality of the accepted set.
as a simple example the number of solutions of the constraints f x.contains ac andf x.contains bd is the same.
orbits under the symmetry group gcard.these subgroups generategcardin the following sense the domain of any element of gcardis the union of the domains of the subgroups making it the cartesian product i v sh .
every element of a subgroup acts as an element of gcardby acting as the identity on every domain element on which it is not defined.
any element of gcardcan be written as a composition of elements from these subgroups.
for a constraint f the orbit offundergcardis the set of constraints obtained by applying any element g cardtof.
the problem of choosing a normalized form for fcan now be formulated as choosing a representative constraint from the orbit of fundergcard.
we do this by defining a strict ordering on constraints and choosing the well defined lowest ordered constraint within the orbit as the representative for all constraints within the orbit.
while we have spoken generally about cardinality preserving group actions our application of interest is in parameterized modelcounting which involves finding the number of satisfying solutions to a constraint for any given bound.
while most of the group actions defined above preserve the number of solutions for a given bound the elements of the euclidean group may not.
for example x y x y 0has solutions given a bound of but x y x y which is in the same orbit under gcard has only one solution for the same bound.
in order to preserve the parameterized model count the bound is translated according to the same group action as the constraint.
in the example above bound is translated to bound by the same integer translation that translated the shift resulting in satisfying models.
in a similar vein it s interesting to note that though not all of our transformations preserve the solution set of a constraint all of them are invertible.
this means that the solution set of a constraint can be obtained from the solution set of its normal form by applying the inverse transformations of those applied to andsh when normalizing the constraint to the solution set of its normal form.
this enables our transformations to be used even for analyses that require the solution sets of constraints.
constraint language we focus on constraints over strings and linear integer arithmetic.
we define three types of terms string terms ts regular expression termstr and lia termsta as described in figure .
we consider three types of constraints over these terms which we call conjuncts throughout this paper string conjuncts s regular membership conjuncts r and lia conjuncts a. the conjuncts are built using comparators as described in figure .
l is a language defined over these conjuncts.
input constraints to our normalization procedure are assumed to be in conjunctive form with each conjunct from l. 538constraint normalization and parameterized caching for quantitative program analysis esec fse september paderborn germany tsbc vs ts ts char at ts ta int to str ta replace ts ts ts substr ts ta ta trb s tr tr tr tr tr t r tabn va t a ta ta ta ta ta ta n n ta ts index of ts ts str to int ts figure here c n z s vsand vadenote an unbounded string variable and an integer variable resp.
lb s r a sbts ts ts ts contains ts ts prefix of ts ts suffix of ts ts not contains ts ts not prefix of ts ts not suffix of ts ts rbts tr ts tr abta ta ta ta ta ta ta ta figure the language l conjuncts of string s regular expression r and lia a types.
letsopbe the set of string operators i.e.
the operators used to build thetsterms.
let scomp be the set of string comparators i.e.
the comparator used to build sconjuncts.
similarly let ropbe the set of regular expression operators used in tr rcomp the set of regular expression comparators used in r aop the set of the lia operators used inta and acomp the set of lia comparators used ina.
let a function type s r a scomp rcomp acomp be a function that takes in a conjunct and returns the comparator of this conjunct.
constraint ordering assume a strict total ordering on constraints .
a constraint fis a normal form if for every other constraint f in its orbit undergcard f f .
there are many ways to impose an ordering on constraints.
we present one possible ordering below.
our ordering is produced compositionally with strict orders defined over various components of our language which are composed to yield an ordering on constraints.
to start we define an ordering on each element of the domain of gcard.
the ordering on both vand is lexicographical.
the ordering on iis that induced by the natural numbers.
we define the ordering on sh the domain of constant shifts after we introduce an ordering on vectors.
we consider vectors over strict totally ordered sets and denote by vecan order on such vectors.
letxbe a strict totally ordered set and xbe a strict total order onx.
letv v0 .
.
.
vn andu u0 .
.
.
um be two vectors over x then vecis defined as v vecu m n or m n i j j i n vj uj vi xui.
this defines ordering on sh since shift vectors are built over integer constants.
our normalization procedure relies on the following auxiliary functions that given a constraint return as vectors various structural and syntactic components characterizing the constraint.
these vectors are built over the domains of v andz i.e.
over strict totally ordered sets.vi f returns a vector of the indices of variables as they occur infrelative to other variables constants and operators.
the indices are compared according to the operator over z. int f returns a vector of integer constants occurring in ffrom left to right ignoring all elements of shift f .
the vectors are compared according to the operator on z. vvv f returns a vector of variable names occurring in ffrom left to right.
these vectors are compared according to the lexicographical order on v. f returns a vector of string characters occurring in ffrom left to right.
the vectors are compared according to the lexicographical order on .
next we define strict total orderings on operators and separately on comparators listing them in the order of the increasing precedence.
both operators and comparators are ordered with precedence to s then r and a. sop the rest of the string operators in the lexicographic order according to their names in figure rop ordered according to the standard precedence order on regular expression operators aop the rest of the lia operators in the lexicographic order according to their names in figure .
scomp the rest of the string comparators in the lexicographic order according to their names in figure rcomp acomp the ordering on comparators allows to define an order typeon the types of the conjuncts type based on the type of the comparator occurring in the conjuncts.
the strict total ordering on operators allows to introduce vectors of operators of constraints and compare them with vec op f a vector of string regular and lia operators occurring inffrom left to right.
note all auxiliary vectors and their orderings introduced in this section are defined for constraints and are naturally applicable to conjuncts as to a special type of constraints with a single conjunct.
in the future when we compare two elements of the same type we will drop the subscript notation and use to represent comparison between them.
we are now ready to build a strict total order on conjuncts.
we define the ordering hierarchically the structural or syntactic aspects of the conjuncts are compared one at a time in a fixed order until a tie breaking aspect is found.
this order can be selected in any way.
we present one intuitive order below to distinguish conjuncts with more significant differences as early as possible.
the conjuncts are first compared based on their type type then based on their length then the total number of variables var then their vectors of operators op followed by the vectors of indices of variables vi their vectors of integer coefficients int their vectors of variable names vvv then vectors of string constants and finally based on their constant shifts shift .
this order is described in algorithm .
this order is strict and total.
two conjuncts are equal if and only if they are the same conjunct.
this allows us to extend the ordering to constraints as follows i order constraints based on their total number of conjuncts.
539esec fse september paderborn germany t. brennan n. tsiskaridze n. rosner s. aydin and t. bultan algorithm c lessthan c1 c2 conjunct comparison input two conjuncts c1 c2 l output true ifc1 c2 otherwise false for each f do iff c1 f c2 then return true end if end for return false ii then order constraints by comparing their conjuncts elementwise according to the order imposed on i. this is equivalent to comparing conjuncts pairwise from first to last.
iii a constraint fis lower ordered than a constraint gif the first differing conjunct of fis lower ordered than that of g. this order is described in algorithm .
algorithm f lessthan f g constraint comparison input two constraints f f1 .
.
.
fmandg g1 .
.
.
gn output true iff g otherwise false ifm nthen fori ndo ifc lessthan fi gi then return true end if end for end if return m n normalization procedure the normal form of a constraint fis the lowest constraint in the orbit of fundergcard.
in this section we present a normalization procedure to find the normal form of a constraint.
given a transformation g card we define the action of onf as a composition of elements of four categories corresponding to each of the components of the domain of gcard i i gives the constraint resulting from re ordering the conjuncts of faccording to i. v v gives the constraint resulting from renaming the variables of faccording to v gives the constraint resulting from permuting the alphabet constants in faccording to sh sh gives the constraint resulting from shifting each element of f s shift according to sh.
we first present an expensive but complete procedure for normalization in algorithm and give guarantees for its termination and correctness.
given a constraint f this procedure probes each permutation f of conjuncts in f building and applying a composite from transformations specific to the domains v andsh which reduces f until the only transformations that can reduce it further involve an action on i. the results among all permutations offare compared and the lowest ordered result is chosen as the normal form of f. the procedure uses auxiliary functions to build the minimizing domain specific transformations min v f constructs vcompositionally it proceeds through the conjuncts of f from left to right renaming the variables of f in order of appearance.
each time a new variable is encountered a transposition is added to the composition that permutes the name of the encountered variable and the lowest ordered variable name that no other variable of f has been renamed to yet.
at the start of the procedure vis initialized to the identity transposition on v. min f similarly constructs it proceeds through the conjuncts of f from left to right this time permuting string characters.
each time a new string character is encountered a transposition is added to the composition that permutes the encountered string character with the lowest ordered character that no other character in f has been mapped to yet.
is initialized as the identity transposition on .
min sh f returns sh the transformation on shift f that translates the constant coefficient of the first appearing from left to right linear integer arithmetic conjunct in f to .
if fcontains variables that are shared between string and lia constraints shis the identity transformation.
algorithm complete normalization f input a constraint f output the normalized form of f fmin f for each permutation f of conjuncts in fdo v min v f min f sh min sh f f v sh iff lessthan f fmin then fmin f end if end for return fmin theorem .
.
algorithm terminates.
proof.
given a constraint f there are finitely many permutations of conjuncts f .
consequently there are finitely many executions of the for each loop.
construction of each permutation f is linear in the length of f. construction of each of the domainspecific transformations within a single for each call is performed in a single pass through the conjuncts of f thus is linear in the length of f too.
the final transformation on f is also linear in the length of f. thus complete normalization terminates.
theorem .
.
algorithm returns the normal form of f. proof.
assume g complete normalization f is not the normal form of f. then either gis not in the orbit of fundergcard or there is some constraint hin the orbit of fsuch that h fand f h. we show that both result in a contradiction.
assume gis not in the orbit of f.gis the result of permuting the conjuncts of f the action of some i composed with domain specific transformations.
each domain specific transformation has an inverse ingcardas does any permutation of the conjuncts of f. therefore there exists some ingcardsuch that f. now assume that there is some hin the orbit of fsuch that h gandg h. the order of conjuncts in his given by some transposition of the indices of f. this means that there is some iteration of the for loop of algorithm in which the conjuncts of 540constraint normalization and parameterized caching for quantitative program analysis esec fse september paderborn germany the considered permutation of fare ordered identically to those ofh.
by construction our choices of v and shreduce this constraint to the lowest ordered constraint that maintains the same ordering of conjuncts.
therefore either g horg h. algorithm gives a normalization procedure which is sound each orbit has at least one fixed point and complete there is exactly one fixed point for each orbit .
in practice however such a brute force exploration is very expensive.
for our implementation we use a sound but not complete normalization procedure given in algorithm .
given f normalization f returns the semi normal form on f a constraint within the orbit of fwhich though not necessarily the lowest in the orbit is not higher ordered than f. algorithm simplifies complete normalization procedure in that instead of brute forcing all permutations of conjuncts in f it inexpensively chooses a permutation by ordering the conjuncts of f according to c lessthan up to the point when further refinement involves comparison over the domains v orsh.
in other words the conjuncts are not compared according to their variable names string constants or shifts.
it is possible that two conjuncts in f are equal by this comparison in which case their initial order in fis preserved.
the resulting permutation of conjuncts defines a transposition oni.
we apply this transposition to f resulting in a constraint f .
v and share generated by the same auxiliary functions as in algorithm composed and applied to f .
the result is the semi normal form of f. algorithm normalization f input a constraint f output a semi normal form of f f permute conjuncts of faccording to algorithm up until vvv v min v f min f sh min sh f f v sh return f theorem .
.
algorithm is sound.
proof.
each action on fis the action of an element of gcard.
by definition the resulting formula is in the orbit of fundergcard.
the procedure given in algorithm is not complete.
there are orbits for which not every constraint is reduced to the same form.
though this potentially increases the number of misses to the cache our experimental results demonstrate the large number of formulas mapped to the same semi normal form by algorithm .
queries to cashew are of the form f v b where vis the set of variables on which to count and bis the maximum length of a satisfying solution.
to ensure that the cardinality of the solution set is preserved after normalizing f both vandbmust be normalized according to the same transformations applied to fduring algorithm .
algorithm normalize query f v b implements this query normalization.
experimental evaluation we implemented our tool cashew as an extension of the green caching framework.
this allows cashew to use any of the existing green services and it allows green users to benefit from our normalization procedure.
we experiment with cashew enabled satisfiability and model counting services which support stringalgorithm normalize query f v b input a query f v b output a normalized query f v b f normalization f the transformation used to normalize f v b return f v b constraints and linear integer arithmetic.
they also support mixed constraints i.e.
those involving both string and arithmetic operations.
in this evaluation we used abc as our constraint solver.
as we explained in section other model counting constraint solvers can be integrated instead of abc.
all the experiments were run on an intel core i7 .
ghz computer running linux .
.
.
the machine has gb ram of which gb were allocated for the java vm.
.
model counting over the smc kaluza string constraint dataset the kaluza dataset is a well known benchmark of string constraints that are generated by dynamic symbolic execution of real world javascript applications .
the authors of the smc solver translated the satisfiable constraints to their input format one contains big while the other contains small where big and small classification is done based on the constraint sizes in the kaluza dataset.
we shall refer to the former as the original smc big and to the latter as the original smc small.
duplicate constraints.
while inspecting the results of our normalization we found out that many of the files within each dataset are identical indistinguishable by diff .
due to the presence of duplicates even trivial caching without any normalization will yield some benefit on the original datasets.
after removing all duplicate files only of the constraints in smc big and of the constraints in smc small were found to be unique.
as we discuss below our normalization procedure allows further reductions in this dataset increasing the benefits of caching well beyond what can be achieved with trivial caching.
model counting.
since these constraints correspond to path conditions from symbolic execution counting the number of satisfying models of each one could be necessary for quantitative analysis.
we model counted all constraints in each set as a simple way to emulate the behavioral pattern w.r.t.
caching of one or more users performing quantitative analyses on the original programs.
when counting the models of a constraint over strings to avoid infinite counts one needs to set a bound on the length of strings.
in this experiment we set the bound to characters for both sets.
we model counted each constraint in the dataset.
we first did this without normalization or caching and then again with cashew normalization and caching.
in non caching mode each constraint was sent unmodified to the model counting solver.
in caching mode the cache was cleared before running smc big and again before running smc small.
since these path constraints were produced by an external symbolic executor in this experiment we did not use spf.
note that since all constraints were model counted the order in which we traverse the datasets does not matter each normalized 541esec fse september paderborn germany t. brennan n. tsiskaridze n. rosner s. aydin and t. bultan smc big smc small figure orbit sizes for the original smc datasets.
table model counting smc big and smc small.
without caching with caching speedup big average .
s .
s .90x no dups maximum .
s .
s .03x total time .
s .
s .94x small average .
s .
s .40x no dups maximum .
s .
s .97x total time .
s .
s .19x big average .
s .
s .70x original maximum .
s .
s .03x total time .
s .
s .38x small average .
s .
s .60x original maximum .
s .
s .97x total time .
s .
s .29x constraint will fall within some orbit and for each orbit the full cost will be paid exactly once first cache miss .
results.
table shows the total maximum and average modelcounting time as well as the speedups obtained by cashew on each of these metrics for the two datasets with and without duplicates.
on the smc big set cashew achieved a speedup over 10x.
on the smc small set which is a rather bad case for the caching tradeoff because it contains a large number of very small constraints cashew still achieved a .19x speedup.
for the original datasets these numbers e.g.
a 87x speedup are largely due to the presence of duplicates which makes even caching with no normalization very effective.
we report the results because the original datasets are widely used and because the duplicates might indeed have been genuinely generated by symbolic execution of various different yet similar javascript programs.
figure depicts the effect of our normalization procedure on the original benchmarks.
the area of each orbit is proportional to its size.
labels indicating orbit size are shown only when they fit in the available space.
for the original smc small set the original constraints are reduced to orbits.
for the smc big set the original constraints are reduced to just orbits.
we do not compare cashew with green because the original green without cashew cannot handle string constraints.
note that the largest constraint in smc small takes slightly more time after normalization.
we cannot infer much from this because the largest constraint in smc small barely takes one second the small difference about msec could be due to noise.
however the maximum time for smc big decreased by 3x with caching enabled from to seconds.
this is due to normalization.
the constraint that without normalization requires maximum time to be model counted falls within some orbit.
it does not matter which constraint in that orbit will be the one to cause a cache miss oncetable effect of transformations on orbit refinement.
transformations enabled orbits smc big orbits smc small none all transformations all except i all except v all except all except removevar all except removeconj caching is enabled only one of them will and as they are all normalized to the same normal form any of them would take the same model counting time.
what is interesting is that said time can be significantly smaller than the maximum pre normalization time.
table shows the number of orbits that are achieved by different subsets of the transformations in our normalization procedure.
since some transformations can benefit from others instead of considering them in isolation we measured the effect of disabling each one.
we did not include shas it doesn t apply to the string domain.
the removevar and removeconj transformations are preprocessing steps that remove redundant variables and conjuncts respectively.
these results indicate that all transformations yield some benefit and that vis the most beneficial transformation.
for smc small removing more than doubles the number of orbits.
the same is true of ifor smc small.
this shows that different transformations can be more effective for different datasets.
.
spf analysis of string handling code in this second part of the experimental evaluation we use symbolic pathfinder with cashew to symbolically execute java programs that operate on strings.
in order to support model countingbased quantitative analyses we are interested in obtaining a model count for each leaf path constraint.
as an example of quantitative information flow analysis we study some possible applications of cashew to side channel analysis.
we consider four java programs in which a side channel can allow an attacker to gain information about a hidden secret.
passwordcheck1 contains a method that checks whether or not a user given string matches a secret password.
due to the way the program is written the attacker can deduce that the longer the program executes the longer a prefix of the hidden password was matched.
passwordcheck2 is another variant that attempts to mitigate that vulnerability by requiring a certain number of characters to be compared before returning even if a mismatch has already been found.
this yields a more interesting side channel which can still be exploited but is much noisier and less predictable.
obscure is a java translation of the obscure.c program used in which is a password change authorizer.
given an old password and a new one obscure performs a series of tests to determine whether the new password is different enough from the old one.
crime is a java version of a well known attack compression ratio info leak made easy .
this is a side channel in space the secret is concatenated with a string that can be controlled by the attacker and both are compressed together before encryption.
thus the attacker can try various strings and observe the changes in the size of the compressed payload to infer from the compression rate the level of similarity between the secret and the injected content.
542constraint normalization and parameterized caching for quantitative program analysis esec fse september paderborn germany table spf based quantitative analyses of string programs.
program caching total time speedup hits misses h m password1 none s no norm s .15x .
cashew s .80x .
password2 none s no norm s .99x .
cashew s .71x .
obscure none s no norm s .10x .
cashew s .54x .
crime none s no norm s .02x .
cashew s .82x .
in symbolic execution it is not always desirable to make allarguments of a method symbolic.
this is often the case due to scalability issues.
it can also be due to the need to explore a nonstandard distribution of some parameter.
consider for instance a situation where a list of passwords from a website is unwillingly disclosed to the public.
as a consequence users are strongly encouraged to change their passwords and an algorithm similar to the obscure program is employed to ensure that they are sufficiently different from the stolen ones.
we might be interested in measuring the amount of leakage of the algorithm over that particular list of passwords.
by running spf on obscure with the new password as a symbolic string and the old password as a concrete string we can measure the leakage for that particular stolen password.
by repeating this for various passwords from the list we can quantify the algorithm s leakage for that list s particular distribution.
so requires running spf repeatedly on the same code but with different secret strings.
this will affect many path conditions in fundamental ways but others might be unaffected or changed in such a way that cashew can still normalize them down to a previously seen one.
rockyou.
the rockyou1k dataset is a sample of real world passwords taken from the rockyou leak without duplicates.
the sample consists of unique passwords that cover all lengths between and characters and can include any ascii symbols.
results.
for each of the four programs under analysis we ran symbolic execution based side channel analyses using as the secret each of the passwords in the rockyou1k dataset.
for passwordcheck1 andpasswordcheck2 the secret is the password which is concrete and the user s guess is a symbolic string.
for obscure the roles are reversed what we made concrete is the old password which is no longer secret whereas the user chosen new password which is secret is symbolic.
for crime we used a concrete secret session id and a symbolic user injected payload.
table shows execution time hits and misses for three execution modes.
the first mode uses neither normalization nor caching.
in the second mode only caching without normalization is performed which measures the extent to which syntactically identical path conditions akin to the duplicates mentioned in section .
are generated.
in the third mode cashew s normalization is enabled.
note that each symbolic execution generates many path conditions.
the tables show the aggregated results over all path conditions of each execution and the executions of each mode.
as in the previous section we do not compare cashew with green intable cashew normalization and caching costs.
total total total total average total time time norm.
norm.
norm.
cache no cache cashew time calls time size smc big s s s ms kb smc small s s s ms kb password1 s s s s .
mb password2 s s s s mb obscure s s s s .
mb crime s s s s .
mb these experiments because the original green without cashew cannot handle string constraints.
the results show that for these experimental subjects cashew achieved an average speedup of nearly 3x while caching without normalization only achieved .06x and for passwordcheck2 was in fact slower than no caching .
the hit miss ratios improve dramatically when switching to cashew .
costs of caching and normalization.
a caching scheme involves overheads and space time trade offs.
normalization overhead must be kept low since its cost must be paid not only for each hit but also for each miss.
cache size must also be kept within reasonable limits.
cashew is implemented on top of green and like green uses the in memory redis database by default.
this allows extremely fast queries but competes with the client application for available ram.
as table shows the average time to normalize a constraint in our spf symbolic execution experiments was only a few hundred microseconds.
it was about milliseconds for the largest formulas the smc big set with an average size before normalization of about kb of text per constraint .
finally as shown in table the total cache memory usage was very reasonable for these experiments.
.
parameterized caching in this last part of the experimental evaluation we present some experiments for evaluating parameterized caching that is caching that leverages parameterized model counting solvers.
the motivation behind these experiments is that users of cashew who are targeting quantitative information analysis techniques often perform their analyses with various different bound values.
for example in side channel analysis one may want to compute the amount of information leakage for different lengths of a symbolic secret or input.
this requires using different bounds when counting models.
in scenarios where we have reason to believe that there is potential for reusing already created model counting objects for multiple values of b we can try to amortize the time required to construct them by caching them.
string constraints smc kaluza.
recall the smc big and smcsmall datasets from section .
.
we ran these two datasets several more times starting with the string length bound bat characters and raising it up to characters.
since our goal was to evaluate the usefulness of caching model counting objects we did not clear the cache between successive values of b. again we did not compare with green in these experiments because green without cashew does not support constraints over strings.
figure shows the cumulative time spent running smc big and smc small respectively for b .
.
.
characters.
we did this twice for each dataset.
the upper lines red correspond to 543esec fse september paderborn germany t. brennan n. tsiskaridze n. rosner s. aydin and t. bultan figure smc datasets for increasing bounds.
cashew nonparameterized red and parameterized blue caching.
figure symbolic execution of sorting searching programs for increasing bounds.
green green vs. cashew blue .
cashew with parameterized caching disabled model counting objects are not cached .
the lower lines blue correspond to cashew with parameterized caching enabled.
in this mode an extra cost is paid to cache the model counting objects but so enables the possibility of reusing them later on.
the left chart shows that caching model counting objects is indeed beneficial for smc big.
this is an idealized amortization scenario since all stored modelcounting objects are reused on each successive bound value.
nevertheless it is useful to confirm that for this dataset running even one additional bound is profitable and that this profit becomes larger each time we run the dataset for an additional value of b. the right chart shows a similar phenomenon for smc small but although the gap does increase the lines are so close together that caching model counting objects would probably not be worth its cost.
this is consistent with a large number of small problems.
arithmetic constraints.
the goal of these experiments is to evaluate the usefulness of cashew s parameterized caching when symbolically executing java code whose branch conditions involve linear integer arithmetic operations.
green can handle arithmetic constraints so we can use it as the baseline for these experiments.
one well known class of algorithms that involve integer arithmetic constraints and give rise to nontrivial path conditions are classical sorting algorithms.
for these experiments we ran an spf based quantitative analysis symbolic execution and model counting on complete path conditions on the following algorithms bubblesort insertionsort selectionsort quicksort heapsort and mergesort.
figure shows the cumulative time spent in the analysis of each of the seven java programs for b .
.
.
.
sincewe are counting over the integers the bound bnow denotes the maximum number of bits that may be used to represent an integer.
we ran each series twice.
the upper curve green corresponds to green with caching enabled using its normalization procedure for integer arithmetic constraints.
the lower curve blue corresponds tocashew with parameterized caching enabled.
the magnitude of the gap between both curves varies for different programs.
in most cases the initial run on an empty cache forb is slightly more costly for cashew due to the overhead of having to store all the model counting objects in the cache.
this is compensated as soon as they are reused at least once and in all cases we see that the gap between the curves grows as the model counting objects are reused further.
this confirms that parameterized caching is beneficial for these programs if there is a reasonable chance that the model counting objects may be reused.
related work our work builds on top of green an external framework for caching the results of calls to satisfiability solvers or model counters developed by visser et al.
other caching frameworks include greentrie an extension of green and recal both of which are able to detect some implications between constraints.
recal transforms a lia constraint to a matrix canonizes it and uses the result as the constraint s normal form.
a different approach is taken by the tool utopia which identifies past satisfying solutions likely to be shared with new formulas.
this enables results to be reused across formulas that share at least one solution regardless of their structural resemblance.
cashew differs notably from these previous caching frameworks.
first we present a parameterized model counting approach for quantitative program analysis which allows us to cache and reuse a model counter object in addition to the results of model counting queries.
this allows us to reuse results for model counting queries across different bounds.
cashew also exploits more expressive normalization techniques with reductions that preserve only thenumber of solutions of a constraint instead of their solution set.
this allows us to reuse information that the above caching frameworks can not.
cashew is also able to handle string constraints a unique contribution amongst the above mentioned tools.
conclusions we presented a caching framework for quantitative program analysis built on constraint normalization techniques that preserve the cardinality of the solution set for a given constraint but not necessarily the solution set itself.
we augmented our framework with parameterized constraint caching techniques that can reuse the result of a previous model counting query even if the bounds of the queries do not match and extended our framework to support string constraints and combinations of string and arithmetic constraints.
our experiments exemplify how when supplemented with our constraint normalization techniques constraint caching can significantly improve the performance of quantitative program analyses.
544constraint normalization and parameterized caching for quantitative program analysis esec fse september paderborn germany table classes implementing each transformation transformation class name i orderingservice v variablerenamer alphabetrenamer removevar variableremover removeconj redundantconstraintremover implementation cashew is built on top of green and like green it acts as a wrapper around constraint solvers and model counters.
in other words it sits between the client application typically a verification tool based on constraint satisfiability and or model counting and the actual constraint solver and or model counter.
besides the caching strategy and mechanism it provides a standard interface for the client application to create constraints and request solving counting services this enables switching solvers without modifying the client.
constraints for cashew are constructed using the same class hierarchy as in green the main change is that we extended that hierarchy with new objects that represent operations on strings.
cashew also inherits the storage abstraction from green.
the default store is a redis database but other back ends can be used.
.
obtaining cashew cashew can be obtained from the github repository located at .
configuring cashew cashew can be configured via key value pairs using the standard java properties mechanism.
each of the transformations shown in table is implemented by a class in the service.canonizer package as shown in table .
layers can be enabled and disabled for both constraint satisfiability and model counting by means of the green.service.sat and the green.service.count properties respectively.
as in the green framework these properties can specify a composition of services using a parenthesized syntax.
examples of this can be found in the sample configuration files included in the cashew repository.
.
cashew integration examples a model counting tool for smc kaluza benchmark problems.
we used cashew to implement a simple model counter for smc kaluza constraints as shown in section .
expressed using a subset of the smt lib format .
this tool is essentially a translator that parses that input syntax maps it to cashew s constraint representation and then invokes the model counter service.
this implementation is included in the cashew repository as runcashewkaluza.java .
cashew as a back end for spf.
we also used cashew as the back end for symbolic pathfinder as shown in section .
.
the integration of spf with cashew was done much in the same way as it was with green the spf codebase is modified in the few places where it calls the constraint solver or model counter so that it calls cashew instead.
a translator class is added in order to translatespf s constraint language i.e.
abstract syntax tree class hierarchy tocashew s before making the call.
thecashew repository includes a sample working version of spf modified in this way.
also cashew option configuration properties are read through the jpf configuration property space which allows the user to control cashew specific behavior from the same .jpf configuration file used by jpf and spf.
examples can be found in the sample configuration files included in the cashew repository.
cashew as a back end for your own tool.
cashew can be integrated with your client application just like green see the green documentation and examples for more details.
if you would like to use string operations in your constraints please see the operation stringconstant and stringvariable classes in the green.expr package.
adding your own solver to cashew.
cashew has already been integrated with the abc constraint solver and model counter.
you can add a new smt solver to cashew in the same way as you would add one to green.
at a minimum you will need to write a translator from cashew s constraint representation to your solver s input format.
the simplest way to do this is to have your translator class extend the visitor class in the green.expr package.
adding a new model counter is done similarly as well but in this case we augmented the interface to allow for storing model counter objects.
besides the getmodelcount method cashew also supports thegetmodelcounter method which does not require a bound and returns a model counter object rather than a number and thegetmodelcountusingcounter method which takes a modelcounter object and a bound and returns a number.
you may want to implement these methods if your solver supports parameterized model counting.
model counter objects are stored by cashew as raw binary objects byte so they can contain anything.