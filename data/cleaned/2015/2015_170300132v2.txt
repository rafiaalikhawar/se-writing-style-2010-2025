revisiting unsupervised learning for defect prediction wei fu tim menzies com.
sci.
nc state usa wfu ncsu.edu tim.menzies gmail.com abstract collecting quality data from so f tware projects can be time consuming and expensive.
hence some researchers explore unsupervised approaches to quality prediction that does not require labelled data.
an alternate technique is to use supervised approaches that learn models from project data labelled with say defective or notdefective .
most researchers use these supervised models since it is argued they can exploit more knowledge of the projects.
at fse yang et al.
reported startling results where unsupervised defect predictors outperformed supervised predictors for effort aware just in time defect prediction.
if con f irmed these results would lead to a dramatic simpli f ication of a seemingly complex task data mining that is widely explored in the so f tware engineering literature.
t his paper repeats and refutes those results as follows.
t here is much variability in the efficacy of the yang et al.
predictors so even with their approach some supervised data is required to prune weaker predictors away.
t heir f indings were grouped across n projects.
when we repeat their analysis on a project by project basis supervised predictors are seen to work be t ter.
even though this paper rejects the speci f ic conclusions of yang et al.
we still endorse their general goal.
in our our experiments supervised predictors did not perform outstandingly be t ter than unsupervised ones for effort aware just in time defect prediction.
hence they may indeed be some combination of unsupervised learners to achieve comparable performance to supervised ones.
we therefore encourage others to work in this promising area.
keywords data analytics for so f tware engineering so f tware repository mining empirical studies defect prediction acm reference format wei fu tim menzies.
.
revisiting unsupervised learning for defect prediction.
in proceedings of 11th joint meeting of the european so f tware engineering conference and the acm sigsoft symposium on the foundations of so f tware engineering paderborn germany september esec fse pages.
.
.
introduction t his paper repeats and refutes recent results from yang et al.
published at fse .
t he task explored by yang et al.
was effort ware just in time jit so f tware defect predictors .
jit defect predictors are built on code change level and could be used to conduct defect prediction right before developers commit the current change.
t hey report an unsupervised so f tware quality prediction method that esec fse paderborn germany .
.
.
.
.
.
.3106257achieved be t ter results than standard supervised methods.
we repeated their study since if their results were con f irmed this would imply that decades of research into defect prediction had needlessly complicated an inherently simple task.
t he standard method for so f tware defect prediction is learning from labelled data.
in this approach the historical log of known defects is learned by a data miner.
note that this approach requires waiting until a historical log of defects is available i.e.
until a f ter the code has been used for a while.
another approach explored by yang et al.
uses general background knowledge to sort the code then inspect the code in that sorted order.
in their study they assumed that more defects can be found faster by f irst looking over all the smaller modules an idea initially proposed by koru et al.
.
a f ter exploring various methods of de f ining smaller they report their approach f inds more defects sooner than supervised methods.
t hese results are highly remarkable t his approach does not require access to labelled data i.e.
it can be applied just as soon as the code is wri t ten.
it is extremely simple no data pre processing no data mining just simple sorting.
because of the remakrable nature of these results this paper takes a second look at the yang et al.
results.
we ask three questions rq1 do all unsupervised predictors perform better than supervised predictors?
t he reason we ask this question is that if the answer is yes then we can simply select any unsupervised predictor built from the the change metrics as yang et al suggested without using any supervised data if the answer is no then we must apply some techniques to select best predictors and remove the worst ones.
however our results show that when projects are explored separately the majority of the unsupervised predictors learned by yang et al.
perform worse than supervised predictors.
results of rq1 suggest that a f ter building multiple predictors using unsupervised methods it is required to prune the worst predictors and only be t ter ones should be used for future prediction.
however with yang et al.
approach there is no way to tell which unsupervised predictors will perform be t ter without access to the labels of testing data.
to test that speculation we built a new learner oneway that uses supervised training data to remove all but one of the yang et al.
predictors.
using this learner we asked rq2 is it bene f icial to use supervised data to prune away all but one of the yang et al.
predictors?
our results showed that oneway nearly always outperforms the unsupervised predictors found by yang et al.
t he success of oneway leads to one last question rq3 does oneway perform better than more complex standard supervised learners?
such standard supervised learners include random forests linear regression j48 and ibk these learners were selected based onarxiv .00132v2 jun 2017esec fse september paderborn germany wei fu tim menzies prior results by .
we f ind that in terms of recall andpopt the metric preferred by yang et al.
oneway performed be t ter than standard supervised predictors.
yet measured in terms ofprecision there was no advantage to oneway .
from the above we make an opposite conclusion to yang et al.
i.e.
there are clear advantages to use supervised approaches over unsupervised ones.
we explain the difference between our results and their results as follows yang et al.
reported averaged results across all projects we offer a more detailed analysis on a project by project basis.
t he rest of this paper is organized as follows.
section is a commentary on yang et al.
study and the implication of this paper.
section describes the background and related work on defect prediction.
section explains the effort ware jit defect prediction methods investigated in this study.
section describes the experimental se t tings of our study including research questions that motivate our study data sets and experimental design.
section presents the results.
section discusses the threats to the validity of our study.
section presents the conclusion and future work.
note one terminological convention in the following we treat predictors and learners as synonyms.
science in the 21st cenury while this paper is speci f ic about effort aware jit defect prediction and the yang et al.
result at another level this paper is also about science in the 21st century.
in the so f tware analytics community now has the tools data sets experience to explore a bold wider range of options.
t here are practical problems in exploring all those possibilities speci f ically too many options.
for example in section .
of kocaguneli et al.
list different ways of estimation by analogy.
we have had some recent successes with exploring this space of options but only a f ter the total space of options is reduced by some initial study to a manageable set of possibilities.
hence what is needed are initial studies to rule our methods that are generally unpromising e.g.
this paper before we apply second level hyper parameter optimization study that takes the reduced set of options.
another aspect of 21st century science that is highlighted by this paper is the nature of repeatability.
while this paper disagrees the conclusions of yang et al.
it is important to stress that their paper is an excellent example of good science that should be emulated in future work.
firstly they tried something new.
t here are many papers in the se literature about defect prediction.
however compared to most of those the yang et al.
paper is bold and stunningly original.
secondly they made all their work freely available.
using the r code they placed online we could reproduce their result including all their graphical output in a ma t ter of days.
further using that code as a starting point we could rapidly conduct the extensive experimentation that leads to this paper.
t his is an excellent example of the value of open science.
t hirdly while we assert their answers were wrong the question they asked is important and should be treated as an open and urgent issue by the so f tware analytics community.
in our experiments supervised predictors performed be t ter than unsupervised but not outstandingly be t ter than unsupervised.
hence they may indeed besome combination of unsupervised learners to achieve comparable performance to supervised.
t herefore even though we reject the speci f ic conclusions of yang et al.
we still endorse the question they asked strongly and encourage others to work in this area.
background and related work .
defect prediction as soon as people started programming it became apparent that programming was an inherently buggy process.
as recalled by maurice wilkes speaking of his programming experiences from the early 1950s it was on one of my journeys between the edsac room and the punching equipment that hesitating at the angles of stairs the realization came over me with full force that a good part of the remainder of my life was going to be spent in f inding errors in my own programs.
it took decades to gather the experience required to quantify the size defect relationship.
in fumio akiyama described the f irst known size law saying the number of defects d was a function of the number of loc where d loc.
in t homas mccabe argued that the number of loc was less important than the complexity of that code .
he argued that code is more likely to be defective when his cyclomatic complexity measure was over .
later work used data miners to build defect predictors that proposed thresholds on multiple measures .
subsequent research showed that so f tware bugs are not distributed evenly across a system.
rather they seem to clump in small corners of the code.
for example hamill et al.
report studies with a the gnu c compiler where half of the f iles were never implicated in issue reports while of the f iles were mentioned in half of the issues.
also ostrand et al.
studied b at t data and reported that of the bugs reside in of the f iles.
similar results have been observed in c nasa systems as well as d open source so f tware and e so f tware from turkey .
given this skewed distribution of bugs a cost effective quality assurance approach is to sample across a so f tware system then focus on regions reporting some bugs.
so f tware defect predictors built from data miners are one way to implement such a sampling policy.
while their conclusions are never correct they can be used to suggest where to focus more expensive methods such as elaborate manual review of source code symbolic execution checking etc.
for example misirli et al.
report studies where the guidance offered by defect predictors reduced the effort required for so f tware inspections in some turkish so f tware companies by while at the same time still being able to f ind the of the f iles that contain of the defects.
not only do static code defect predictors perform well compared to manual methods they also are competitive with certain automatic methods.
a recent study at icse rahman et al.
compared a static code analysis tools findbugs jlint and pmd and b static code defect predictors which they called statistical defect prediction built using logistic regression.
t hey found no signi f icant differences in the cost effectiveness of these approaches.
given this equivalence it is signi f icant to note that static code defect prediction can be quickly adapted to new languages by buildingrevisiting unsupervised learning for defect prediction esec fse september paderborn germany lightweight parsers that extract static code metrics.
t he same is not true for static code analyzers these need extensive modi f ication before they can be used on new languages.
to build such defect predictors we measure the complexity of so f tware projects using mccabe metrics halstead s effort metrics and ck object oriented code mertics at a coarse granularity like f ile or package level.
with the collected data instances along with the corresponding labels defective or non defective we can build defect prediction models using supervised learners such as decision tree random forests svm naive bayes and logistic regression .
a f ter that such trained defect predictor can be applied to predict the defects of future projects.
.
just in time defect prediction traditional defect prediction has some drawbacks such as prediction at a coarse granularity and started at very late stage of so f tware development circle whereas in jit defect prediction paradigm the defect predictors are built on code change level which could easily help developers narrow down the code for inspection and jit defect prediction could be conducted right before developers commit the current change.
jit defect prediction becomes a more practical method for practitioners to carry out.
mockus et al.
conducted the f irst study to predict so f tware failures on a telecommunication so f tware project 5ess by using logistic regression on data sets consisted of change metrics of the project.
kim et al.
further evaluated the effectiveness of change metrics on open source projects.
in their study they proposed to apply support vector machine to build a defect predictor based on so f tware change metrics where on average they achieved accuracy and recall.
since training data might not be available when building the defect predictor fukushima et al.
introduced cross project paradigm into jit defect prediction.
t heir results showed that using data from other projects to build jit defect predictor is feasible.
most of the research into defect prediction does not consider the effort1required to inspect the code predicted to be defective.
exceptions to this rule include the work of arishom and briand koru et al.
and kamei et al.
.
kamei et al.
conducted a large scale study on the effectiveness of jit defect prediction where they claimed that using of efforts required to inspect all changes their modi f ied linear regression model ealr could detect defect introducing changes.
inspired by menzies et al.
s manualup model i.e.
small size of modules inspected f irst yang et al.
proposed to build unsupervised defect predictors by sorting the reciprocal values of different change metrics on each testing data set in descending order.
t hey reported that with efforts many unsupervised predictors perform be t ter than state of the art supervised predictors.
method .
unsupervised predictors in this section we describe the effort aware just in time unsupervised defect predictors proposed by yang et al.
which serves as a baseline method in this study.
as described by yang et al.
1effort means time labor required to inspect total number of f iles code predicted as defective.their simple unsupervised defect predictor is built on change metrics as shown in table .
t hese different change metrics can be divided into dimensions diffusion ns nd nf and entropy.
size la ld and lt. purpose fix.
history ndev age and nuc.
experience exp rexp and sexp.
table change metrics used in our data sets.
metric description ns number of modi f ied subsystems .
nd number of modi f ied directories .
nf number of modi f ied f iles .
entropy distribution of the modi f ied code across each f ile .
la lines of code added .
ld lines of code deleted .
lt lines of code in a f ile before the current change .
fix whether or not the change is a defect f ix .
ndev number of developers that changed the modi f ied f iles .
age t he average time interval between the last and the current change .
nuc t he number of unique changes to the modi f ied f iles .
exp t he developer experience in terms of number of changes .
rexp recent developer experience .
sexp developer experience on a subsystem .
t hediffusion dimension characterizes how a change is distributed at different levels of granularity.
as discussed by kamei et al.
a highly distributed change is harder to keep track and more likely to introduce defects.
t he sizedimension characterizes the size of a change and it is believed that the so f tware size is related to defect proneness .
yin et al.
report that the bug f ixing process can also introduce new bugs.
t herefore the fixmetric could be used as a defect evaluation metric.
t he history dimension includes some historical information about the change which has been proven to be a good defect indicator .
for example matsumoto et al.
f ind that the f iles previously touched by many developers are likely to contain more defects.
t he experience dimension describes the experience of so f tware programmers for the current change because mockus et al.
show that more experienced developers are less likely to introduce a defect.
more details about these metrics can be found in kamei et al s study .
in yang et al.
s study for each change metric mof testing data they build an unsupervised predictor that ranks all the changes based on the corresponding value of1 m1coin descending order where m1cois the value of the selected change metric for each change c. t herefore the changes with smaller change metric values will ranked higher.
in all for each project yang et al.
de f ine simple unsupervised predictors la and ld are excluded as yang et al.
.
.
supervised predictors to further evaluate the unsupervised predictor we selected some supervised predictors that already used in yang et al.
s work.
as reported in both yang et al.
s and kamei et al.
s work ealr outperforms all other supervised predictors for effortaware jit defect prediction.
ealr is a modi f ied linear regressionesec fse september paderborn germany wei fu tim menzies model and it predictsy1xo effort1xoinstead of predicting y1xo where y1xoindicates whether this change is a defect or not or and effort1xorepresents the effort required to inspect this change.
note that this is the same method to build ealr as kamei et al.
.
in defect prediction literature ibk knn j48 and random forests methods are simple yet widely used as defect learners and have been proven to perform if not best quite well for defect prediction .
t hese three learners are also used in yang et al s study.
for these supervised predictors y1xowas used as the dependant variable.
for knn method we set k according to yang et al.
.
.
oneway learner based on our preliminary experiment results shown in the following section for the six projects investigated by yang et al some of unsupervised predictors do perform worse than supervised predictors and there is no one predictor constantly working best on all project data.
t his means we can not simply say which unsupervised predictor works for the new project before predicting on the testing data.
in this case we need a technique to select the proper metrics to build defect predictors.
we propose oneway learner which is a supervised predictor built on the implication of yang et al s simple unsupervised predictors.
t he pseudocode for oneway is shown in algorithm .
in the following description we use the superscript numbers to denote the line number in pseudocode.
t he general idea of oneway is to use supervised training data to remove all but one of the yang et al.
predictors and then apply this trained learner on the testing data.
speci f ically oneway f irstly builds simple unsupervised predictors from each metric on training datal4 then evaluates each of those learners in terms of evaluation metricsl5 like popt recall precision and f1.
a f ter that if the desirable evaluation goal is set the metric which performs best on the corresponding evaluation goal is returned as the best metric otherwise the metric which gets the highest mean score over all evaluation metrics is returnedl9 in this study we use the la t ter one .
finally a simple predictor is built only on such best metricl10 with the help of training data.
t herefore oneway builds only one supervised predictor for each project using the local data instead of predictors directly on testing data as yang et al .
experimental settings .
research q uestions using the above methods we explore three questions do all unsupervised predictors perform be t ter than supervised predictors?
is it bene f icial to use supervised data to prune all but one of the yang et al.
unsupervised predictors?
does oneway perform be t ter than more complex standard supervised predictors?
when reading the results from yang et al.
we f ind that they aggregate performance scores of each learner on six projects which might miss some information about how learners perform on each project.
are these unsupervised predictors working consistentlyalgorithm pseudocode for oneway input data train data test eval goal2ff1 popt recall precision g output result function o n.sc e.scw a.sc y.sc data train data test eval goal allscores null formetric indata train do learner buildunsupervisedlearner data train metric scores evaluate learner scores include all evaluation goals e.g.
popt f1 allscores .append scores end for best metric prunefeature allscores eval goal result buildunsupervisedlearner data test best metric return result end function function p.sc r.sc u.sc n.sc e.scf e.sc a.sc t.sc u.sc r.sc e.sc allscores eval goal ifeval goal null then mean scores getmeanscoresforeachmetric allscores best metric getmetric max mean scores return best metric else best metric getmetric max allscores eval goal return best metric end if end function across all the project data?
if not how would it look like?
t herefore in rq1 we report results for each project separately.
another observation is that even though yang et al.
propose that simple unsupervised predictors could work be t ter than supervised predictors for effort aware jit defect prediction one missing aspect of their report is how to select the most promising metric to build a defect predictor.
t his is not an issue when all unsupervised predictors perform well but as we shall see this is not the case.
as demonstrated below given munsupervised predictors only a small subset can be recommended.
t herefore it is vital to have some mechanism by which we can down select from mmodels to the l mthat are useful.
based on this fact we propose a new method oneway which is the missing link in yang et al.
s study and the missing f inal step they do not explore.
t herefore in rq2 and rq3 we want to evaluate how well our proposed oneway method performs compared to the unsupervised predictors and supervised predictors.
considering our goals and questions we reproduce yang et al s results and report for each project to answer rq1.
for rq2 and rq3 we implement our oneway method and compare it with unsupervised predictors and supervised predictors on different projects in terms of various evaluation metrics.
.
data sets in this study we conduct our experiment using the same data sets as yang et al.
which are six well known open source projects bugzilla columba eclipse jdt eclipse platform mozilla and postgresql.
t hese data sets are shared by kamei et al.
.
t he statistics of the data sets are listed in table .
from table we know that all these six data sets cover at least years historical information and the longest one is postgresql which includes years of data.revisiting unsupervised learning for defect prediction esec fse september paderborn germany table statistics of the studied data sets project periodtotal change of defectsavg loc per change modi f ied files per change bugzilla .
.
platform .
.
mozilla .
.
jdt .
.
columba .
.
postgresql .
.
t he total changes for these six data sets are from to which are sufficient for us to conduct an empirical study.
in this study if a change introduces one or more defects then this change is considered as defect introducing change.
t he percentage of defectintroducing changes ranges from to .
all the data and code used in this paper is available online2.
.
experimental design t he following principle guides the design of these experiments whenever there is a choice between methods data etc.
we will always prefer the techniques used in yang et al.
.
by applying this principle we can ensure that our experimental setup is the same as yang et al.
.
t his will increase the validity of our comparisons with that prior work.
when applying data mining algorithms to build predictive models one important principle is not to test on the data used in training.
to avoid that we used time wise cross validation method which is also used by yang et al.
.
t he important aspect of the following experiment is that it ensures that all testing data was created a f ter training data.
firstly we sort all the changes in each project based on the commit date.
t hen all the changes that were submi t ted in the same month are grouped together.
for a given project data set that covers totally nmonths history when building a defect predictor consider a sliding window size of t he f irst two consecutive months data in the sliding window ith and i 1th are used as the training data to build supervised predictors and oneway learner.
t he last two months data in the sliding window i 4th and i 5th which are two months later than the training data are used as the testing data to test the supervised predictors oneway learner and unsupervised predictors.
a f ter one experiment the window slides by one month data.
by using this method each training and testing data set has two months data which will include sufficient positive and negative instances for the supervised predictors to learn.
for any project that includes nmonths data we can perform n different experiments to evaluate our learners when nis greater than .
for all the unsupervised predictors only the testing data is used to build the model and evaluate the performance.
to statistically compare the differences between oneway with supervised and unsupervised predictors we use wilcoxon single ranked test to compare the performance scores of the learners in this study the same as yang et al.
.
to control the false discover rate the benjamini hochberg bh adjusted p value is used to 2h t tps github.com weifoo revisitunsupervisedtest whether two distributions are statistically signi f icant at the level of .
to measure the effect size of performance scores among oneway and supervised unsupervised predictors we compute cliff s that is a non parametric effect size measure .
as romano et al.
suggested we evaluate the magnitude of the effect size as follows negligible j j small j j medium j j and large .
j j .
.
evaluation measures for effort aware jit defect prediction in addition to evaluate how learners correctly predict a defect introducing change we have to take account the efforts that are required to inspect prediction.
ostrand et al.
report that given a project of the f iles contain on average of all defects in the project.
although there is nothing magical about the number it has been used as a cutoff value to set the efforts required for the defect inspection when evaluating the defect learners .
t hat is given effort how many defects can be detected by the learner.
to be consistent with yang et al in this study we restrict our efforts to of total efforts.
to evaluate the performance of effort aware jit defect prediction learners in our study we used the following metrics precision recall f1 andpopt which are widely used in defect prediction literature .
precision true positi v.alte true positi v.alte false positi v.alte recall true positi v.alte true positi v.alte false ne afii10069.italati v.alte f1 precision recall recall precision where precision denotes the percentage of actual defective changes to all the predicted changes and recall is the percentage of predicted defective changes to all actual defective changes.
f1is a measure that combines both precision andrecall which is the harmonic mean ofprecision andrecall .
figure example of an effort based cumulative li f t chart .
t he last evaluation metric used in this study is popt which is de f ined as opt where optis the area between the effort codechurn based cumulative li f t charts of the optimal model and the prediction model as shown in figure .
in this chart the x axis isesec fse september paderborn germany wei fu tim menzies considered as the percentage of required effort to inspect the change and the y axis is the percentage of defect introducing change found in the selected change.
in the optimal model all the changes are sorted by the actual defect density in descending order while for the predicted model all the changes are sorted by the actual predicted value in descending order.
according to kamei et al.
and xu et al.
poptcan be normalized as follows popt1mo s1optimalo s1mo s1optimalo s1worsto where s1optimalo s1moands1worstorepresent the area of curve under the optimal model predicted model and worst model respectively.
note that the worst model is built by sorting all the changes according to the actual defect density in ascending order.
for any learner it performs be t ter than random predictor only if thepoptis greater than .
.
note that following the practices of yang et al.
we measure precision recall f1andpoptat the effort point.
in this study in addition to poptandacc i.e.
recall that is used in yang et al s work we include precision andf1measures and they provide more insights about all the learners evaluated in the study from very different perspectives which will be shown in the next section.
empirical results in this section we present the experimental results to investigate how simple unsupervised predictors work in practice and evaluate the performance of the proposed method oneway compared with supervised and unsupervised predictors.
before we start off we need a sanity check to see if we can fully reproduce yang et al.
s results.
yang et al.
provide the median values of poptandrecall for the ealr model and the best two unsupervised models lt and age from the time wise cross evaluation experiment.
t herefore we use those numbers to check our results.
as shown in table and table for unsupervised predictors lt and age we get the exact same performance scores on all projects in terms of recall andpopt.
t his is reasonable because unsupervised predictors are very straightforward and easy to implement.
for the supervised predictor ealr these two implementations do not have differences in popt while the maximum difference in recall is only .
since the differences are quite small then we believe that our implementation re f lects the details about ealr and unsupervised learners in yang et al.
.
for other supervised predictors used in this study like j48 ibk and random forests we use the same algorithms from weka package and set the same parameters as used in yang et al.
.
rq1 do all unsupervised predictors perform better than supervised predictors?
to answer this question we build four supervised predictors and twelve unsupervised predictors on the six project data sets using incremental learning method as described in section .
.
figure shows the boxplot of recall popt f1andprecision for supervised predictors and unsupervised predictors on all data sets.
for each predictor the boxplot shows the 25th percentile median and percentile values for one data set.
t he horizontal dashed lines indicate the median of the best supervised predictor whichtable comparison in popt yang s method a vs. our implementation b projectealr lt age a b a b a b bugzilla .
.
.
.
.
.
platform .
.
.
.
.
.
mozilla .
.
.
.
.
.
jdt .
.
.
.
.
.
columba .
.
.
.
.
.
postgresql .
.
.
.
.
.
average .
.
.
.
.
.
table comparison in recall yang s method a vs. our implementation b projectealr lt age a b a b a b bugzilla .
.
.
.
.
.
platform .
.
.
.
.
.
mozilla .
.
.
.
.
.
jdt .
.
.
.
.
.
columba .
.
.
.
.
.
postgresql .
.
.
.
.
.
average .
.
.
.
.
.
is to help visualize the median differences between unsupervised predictors and supervised predictors.
t he colors of the boxes within figure indicate the signi f icant difference between learners t heblue color represents that the corresponding unsupervised predictor is signi f icantly be t ter than the best supervised predictor according to wilcoxon signed rank where the bh corrected p value is less than .
andthe magnitude of the difference between these two learners is not trivial according to cliff s delta wherej j .
t heblack color represents that the corresponding unsupervised predictor is not signi f icantly be t ter than the best supervised predictor orthe magnitude of the difference between these two learners is trivial where j j .
t hered color represents that the corresponding unsupervised predictor is signi f icantly worse than the best supervised predictor andthe magnitude of the difference between these two learners is not trivial.
from figure we can clearly see that not all unsupervised predictors perform statistically be t ter than the best supervised predictor across all different evaluation metrics.
speci f ically for recall on one hand there are only2 12and2 12of all unsupervised predictors that perform statistically be t ter than the best supervised predictor on six data sets respectively.
on the other hand there are6 12and6 12of all unsupervised predictors perform statistically worse than the best supervised predictor on the six data sets respectively.
t his indicates that about of the unsupervised predictors perform worse than the best supervised predictor on any data set without any prior knowledge we can not know which unsupervised predictor s works adequately on the testing data.revisiting unsupervised learning for defect prediction esec fse september paderborn germany .
.6supervised unsupervisedrecall0.
.
.
.
.
.
.
.
.
.
ealrrfj48ibk nsndnf entropyltfixndevageexprexpsexpnucrecall .
.6supervised unsupervisedpopt0.
.
.
.
.
.
.
.
.
.
ealrrfj48ibk nsndnf entropyltfixndevageexprexpsexpnucpopt .
.6supervised unsupervisedf10.
.
.
.
.
.
.
.
.
.
ealrrfj48ibk nsndnf entropyltfixndevageexprexpsexpnucf1 .
.6supervised unsupervisedprecision0.
.
.
.
.
.
.
.
.
.
ealrrfj48ibk nsndnf entropyltfixndevageexprexpsexpnucprecision figure performance comparisons between supervised and unsupervised predictors over six projects from top to bottom are bugzilla platform mozilla jdt columba postgresql .
note that the above two points from recall also hold for popt.
forf1 we see that only lt on bugzilla and age on postgresql perform statistically be t ter than the best supervised predictor.
other than that no unsupervised predictor performs be t ter on any data set.
furthermore surprisingly no unsupervised predictor works signi f icantly be t ter than the best supervised predictor on any data sets in terms of precision .
as we can see random forests performs well on all six data sets.
t his suggests that unsupervised predictors have very low precision for effort aware defect prediction and can not be deployed to any business situation where precision is critical.
overall for a given data set no one speci f ic unsupervised predictor works be t ter than the best supervised predictor across all evaluation metrics.
for a given measure most unsupervised predictors did not perform be t ter across all data sets.
in summary not all unsupervised predictors perform be t ter than supervised predictors for each project and for different evaluation measures.note the implications of this f inding some extra knowledge is required to prune the worse unsupervised models such as the knowledge that can come from labelled data.
hence we must conclude the opposite to yang et al.
i.e.
some supervised labelled data must be applied before we can reliably deploy unsupervised defect predictors on testing data.
rq2 is it bene f icial to use supervised data to prune away all but one of the yang et al.
predictors?
to answer this question we compare the oneway learner with all twelve unsupervised predictors.
all these predictors are tested on the six project data sets using the same experiment scheme as we did in rq1.
figure shows the boxplot for the performance distribution of unsupervised predictors and the proposed oneway learner on six data sets across four evaluation measures.
t he horizontal dashed line denotes the median value of oneway .
note that in figure blue means this learner is statistically be t ter than oneway red means worse and black means no difference.
as we can see in recall onlyesec fse september paderborn germany wei fu tim menzies .
.6unsupervised proposedrecall0.
.
.
.
.
.
.
.
.
.
nsndnf entropyltfixndevageexprexpsexpnuc oneway .
.6unsupervised proposedpopt0.
.
.
.
.
.
.
.
.
.
nsndnf entropyltfixndevageexprexpsexpnuc oneway .
.6unsupervised proposedf10.
.
.
.
.
.
.
.
.
.
nsndnf entropyltfixndevageexprexpsexpnuc oneway .
.6unsupervised proposedprecision0.
.
.
.
.
.
.
.
.
.
nsndnf entropyltfixndevageexprexpsexpnuc oneway figure performance comparisons between the proposed oneway learner and unsupervised predictors over six projects from top to bottom are bugzilla platform mozilla jdt columba postgresql .
one unsupervised predictor lt outperforms oneway in4 6data sets.
however oneway signi f icantly outperform9 12and 12of total unsupervised predictors on six data sets respectively.
t his observation indicates that oneway works signi f icantly be t ter than almost all learners on all data sets in terms of recall .
similarly we observe that only lt predictor works be t ter than oneway in3 6data sets in terms of poptand age outperforms oneway only on the platform data set.
for the remaining experiments oneway performs be t ter than all the other predictors on average out of predictors .
in addition according to f1 only three unsupervised predictors exp rexp sexp perform be t ter than oneway on the mozilla data set and lt predictor just performs as well as oneway and has no advantage over oneway .
we note that similar f indings can be observed in precision measure.
table provides the median values of the best unsupervised predictor compared with oneway for each evaluation measure onall data sets.
note that in practice we can not know which unsupervised predictor is the best out of the unsupervised predictors by yang et al.
s method before we access to the labels of testing data.
in other words to aid our analysis the best unsupervised ones in table are selected when referring to the true labels of testing data which are not available in practice.
in that table for each evaluation measure the number in green cell indicates that the best unsupervised predictor has a large advantage over oneway according to the cliff s similarly the yellow cell means medium advantage and the gray cell means small advantage.
from table we observe that out of experiments on all evaluation measures none of these best unsupervised predictors outperform oneway with a large advantage according to the cliff s .
speci f ically according to recall andpopt even though the best unsupervised predictor lt outperforms oneway on four and three data sets all of these advantage are small.
meanwhile rexp and exp have a medium improvement over oneway on one and two data sets for f1andprecision respectively.
in terms of the averagerevisiting unsupervised learning for defect prediction esec fse september paderborn germany .
.6supervised proposedrecall0.
.
.
.
.
.
.
.
.
.
ealrrfj48ibk onewayrecall .
.6supervised proposedpopt0.
.
.
.
.
.
.
.
.
.
ealrrfj48ibk onewaypopt .
.6supervised proposedf10.
.
.
.
.
.
.
.
.
.
ealrrfj48ibk onewayf1 .
.6supervised proposedprecision0.
.
.
.
.
.
.
.
.
.
ealrrfj48ibk onewayprecision figure performance comparisons between the proposed oneway learner and supervised predictors over six projects from top to bottom are bugzilla platform mozilla jdt columba postgresql .
table best unsupervised predictor a vs. oneway b .
t he colorful cell indicates the size effect green for large yellow for medium gray for small.
projectrecall popt f1 precision a lt b a lt b a rexp b a exp b bugzilla .
.
.
.
.
.
.
.
platform .
.
.
.
.
.
.
.
mozilla .
.
.
.
.
.
.
.
jdt .
.
.
.
.
.
.
.
columba .
.
.
.
.
.
.
.
postgresql .
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
scores the maximum magnitude of the difference between the best unsupervised learner and oneway is .
in other words oneway is comparable with the best unsupervised predictors on all data sets for all evaluation measures even though the best unsupervised predictors might not be known before testing.
overall we f ind that no one unsupervised predictor signi f icantly outperforms oneway on all data sets for a given evaluation measure mostly oneway works as well as the best unsupervised predictor and has signi f icant be t ter performance than almost all unsupervised predictors on all data sets for all evaluation measures.
t herefore the above results suggest as a simple supervised predictor oneway has competitive performance and it performs be t ter than most unsupervised predictors for effort aware jit defect prediction.
note the implications of this f inding the supervised learning utilized in oneway can signi f icantly outperform the unsupervised models.
rq3 does oneway perform better than more complex standard supervised predictors?
to answer this question we compare oneway learner with four supervised predictors including ealr random forests j48 andtable best supervised predictor a vs. oneway b .
t he colorful cell indicates the size effect green for large yellow for medium gray for small.
projectrecall popt f1 precision a ealr b a ealr b a ibk b a rf b bugzilla .
.
.
.
.
.
.
.
platform .
.
.
.
.
.
.
.
mozilla .
.
.
.
.
.
.
.
jdt .
.
.
.
.
.
.
.
columba .
.
.
.
.
.
.
.
postgresql .
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
ibk.
ealr is considered to be state of the art learner for effortaware jit defect prediction and all the other three learners are widely used in defect prediction literature over past years .
we evaluate all these learners on the six project data sets using the same experiment scheme as we did in rq1.
from figure we have the following observations.
firstly the performance of oneway is signi f icantly be t ter than all these four supervised predictors in terms of recall andpopton all six data sets.
also ealr works be t ter than random forests j48 and ibk which is consistent with kamei et al s f inding .
secondly according to f1 random forests and ibk perform slightly be t ter than oneway in two out of six data sets.
for most cases oneway has a similar performance to these supervised predictors and there is not much difference between them.
however when reading precision scores we f ind that in most cases supervised learners perform signi f icantly be t ter than oneway .
speci f ically random forests j48 and ibk outperform oneway on all data sets and ealr is be t ter on three data sets.
t his f inding is consistent with the observation in rq1 where all unsupervised predictors perform worse than supervised predictors for precision .
from table we have the following observation.
first of all in terms of recall andpopt the maximum difference in median values between ealr and oneway are and respectively whichesec fse september paderborn germany wei fu tim menzies are and improvements over and on mozilla and postgresql data sets.
for both measures oneway improves the average scores by and which are and improvement over ealr.
secondly according to f1 ibk outperforms oneway on three data sets with a large medium and small advantage respectively.
t he largest difference in median is .
finally as we discussed before the best supervised predictor for precision random forests has a very large advantage over oneway on all data sets.
t he largest difference is on postgresql data set.
overall according to the above analysis we conclude that oneway performs signi f icantly be t ter than all four supervised learners in terms of recall andpopt it performs just as well as other learners for f1.
as for precision other supervised predictors outperform oneway .
note the implications of this f inding simple tools like oneway perform adequately but for all around performance more sophisticated learners are recommended.
as to when to use oneway or supervised predictors like random forests that is an open question.
according to no free lunch t heorems no method is always best and we show unsupervised predictors are o f ten worse on a project by project basis.
so best predictor selection is a ma t ter of local assessment requiring labelled training data an issue ignored by yang et al .
threats to validity internal validity .
t he internal validity is related to uncontrolled aspects that may affect the experimental results.
one threat to the internal validity is how well our implementation of unsupervised predictors could represent the yang et al.
s method.
to mitigate this threat based on yang et al r code we strictly follow the approach described in yang et al s work and test our implementation on the same data sets as in yang et al.
.
by comparing the performance scores we f ind that our implementation can generate the same results.
t herefore we believe we can avoid this threat.
external validity .
t he external validity is related to the possibility to generalize our results.
our observations and conclusions from this study may not be generalized to other so f tware projects.
in this study we use six widely used open source so f tware project data as the subject.
as all these so f tware projects are wri t ten in java we can not guarantee that our f indings can be directly generalized to other projects speci f ically to the so f tware that implemented in other programming languages.
t herefore the future work might include to verify our f indings on other so f tware project.
in this work we used the data sets from where totally change metrics were extracted from the so f tware projects.
we build and test the oneway learner on those metrics as well.
however there might be some other metrics that not measured in these data sets that work well as indicators for defect prediction.
for example when the change was commi t ted e.g.
morning a f ternoon or evening functionality of the the f iles modi f ied in this change e.g.
core functionality or not .
t hose new metrics that are not explored in this study might improve the performance of our oneway learner.
conclusion and future work t his paper replicated and refutes yang et al.
s results on unsupervised predictors for effort ware just in time defect prediction.
not all unsupervised predictors work be t ter than supervised predictors on all six data sets for different evaluation measures .
t his suggests that we can not randomly pick an unsupervised predictor to perform effort ware jit defect prediction.
rather it is necessary to use supervised methods to pick best models before deploying them to a project.
for that task supervised predictors like oneway are useful to automatically select the potential best model.
in the above oneway peformed very well for recall poptandf1.
hence it must be asked is defect prediction inherently simple?
and does it need anything other than oneway ?
.
in this context it is useful to recall that oneway s results for precision were not competitive.
hence we say that if learners are to be deployed in domains where precision is critical then oneway is too simple.
t his study opens the new research direction of applying simple supervised techniques to perform defect prediction.
as shown in this study as well as yang et al.
s work instead of using traditional machine learning algorithms like j48 and random forests simply sorting data according to one metric can be a good defect predictor model at least for effort aware just in time defect prediction.
t herefore we recommend the future defect prediction research should focus more on simple techniques.
for the future work we plan to extend this study on other so f tware projects especially those developed by the other programming languages.
a f ter that we plan to investigate new change metrics to see if that helps improve oneway s performance.
addendum as this paper was going to press we learned of new papers that updated the yang et al.
study liu et al.
at emse and huang et al.
at icsmse .
we thank these authors for the courtesy of sharing a pre print of those new results.
we also thank them for using concepts from a pre print of our paper in their work3.
regretfully we have yet to return those favors due to deadline pressure we have not been able to con f irm their results.
as to technical speci f ics liu et al.
use a single churn measure sum of number of lines added and deleted to build an unsupervised predictors that does remarkably be t ter than oneway and earl where the la t ter could access all the variables .
while this result is currently uncon f irmed it could well have raised the bar for unsupervised defect prediction.
clearly more experiments are needed in this area.
for example when comparing the liu et al.
methods to oneway and standard supervised learners we could a give all learners access to the churn variable b apply the yang transform of1 m1coto all variables prior to learning c use more elaborate supervised methods including synthetic minority oversampling and automatic hyper parameter optimization .