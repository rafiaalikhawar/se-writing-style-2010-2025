tracking static analysis violations over time to capture developer characteristics pavel avgustinov arthur i. baars anders s. henriksen greg lavender galen menzel oege de moor max sch fer julian tibble semmle ltd. oxford united kingdom publications semmle.com abstract many interesting questions about the software quality of a code base can only be answered adequately if finegrained information about the evolution of quality metrics over time and the contributions of individual developers is known.
we present an approach for tracking static analysis violations which are often indicative of defects over the revision history of a program and for precisely attributing the introduction and elimination of these violations to individual developers.
as one application we demonstrate how this information can be used to compute fingerprints of developers that reflect which kinds of violations they tend to introduce or to fix.
we have performed an experimental study on several large open source projects written in different languages providing evidence that these fingerprints are well defined and capture characteristic information about the coding habits of individual developers.
i. i ntroduction static code analysis has become an integral part of the modern software developer s toolbox for assessing and maintaining software quality.
there is a wide variety of static analysis tools particularly for java and c c which examine code for potential bugs or performance bottlenecks flag violations of best practices and compute software metrics.
it has been shown that static analysis warnings and sub par metric scores below collectively referred to as violations are indicative of software defects .
some static analysis tools are tightly integrated into the development cycle and examine code as it is being written while others run in batch mode to produce detailed reports about an entire code base .
the latter approach is particularly useful for obtaining a high level overview of software quality.
moreover most modern tools can aggregate violations at different levels of detail thus making it easy for instance to identify modules that appear to be of worse quality than others and should thus receive special attention.
however this view of software quality is both static and coarse grained it is static because it only concerns a single snapshot of the code base at one point in time and it is coarse grained because it does not differentiate contributions by individual developers.
most software is in a constant state of flux where developers implement new features fix bugs clean up and refactor code add tests or write documentation.
while existing tools can certainly analyse multiple versions of a code base separately they cannot easily assess changes between revisions.
forinstance we might want to know what violations were fixed in a given revision or which new violations were introduced.
in short analysing individual snapshots cannot provide an accurate picture of the evolution of software quality over time.
similarly most code bases have many authors there may be seasoned developers and novice programmers prolific core hackers and occasional bugfix contributors.
by just looking at a single snapshot it is impossible to understand the quantity and quality of contributions of individual developers.
yet there are many situations where more dynamic and fine grained information is desirable.
we briefly outline three example scenarios alice wants to delegate a crucial subtask of her program to a third party library and is looking for an open source library that fits her needs.
as is usually the case with open source projects many different implementations are available and alice has to carefully choose one that is not only of high quality but also actively maintained.
she could use an off the shelf static analysis tool to gain insight into the quality of the latest version of the library.
in addition however she might want to know how its quality has evolved over time and how different developers have contributed to the code.
for instance she may want to avoid a library where a core developer who has contributed a lot of new code and bug fixes for many years has recently become inactive.
bob is managing a team of developers and would like to better understand their strengths and weaknesses.
if he knew for instance that developer dave often introduces static analysis violations related to concurrency he could arrange for dave to receive additional training in concurrency.
if there is another developer on the team who often fixes such violations bob could team them up for code reviews.
in an effort to improve software quality bob s colleague carol uses a static analyser to find modules that have a particularly high violation density and hence are likely to be of bad quality.
in deciding which developer to put to work on this code it again helps if she understands their expertise.
if for instance the code in question has many violations related to possible null pointer exceptions it may be a good idea to assign a developer who has a strong track record of fixing such violations.all three scenarios rely on being able to precisely attribute new and fixed violations to individual developers.
this can be achieved by integrating static analysis with revision control information if a violation appears or disappears in a revision authored by developer d then this suggests that dwas responsible for introducing or fixing this violation and it can justifiably be attributed to them.
for this method to work however we need a reliable way of tracking violations between revisions if both revision n and revision n exhibit a violation of the same type we need to determine whether this is in fact the same violation or whether the violation in nwas fixed and a new one of the same type just happened to be introduced in n .
further challenges to be handled include merge commits which have more than one parent revision and un analysable revisions in almost any real world code base there is bound to be an occasional bad commit that is not compilable or cannot meaningfully be analysed for other reasons.
such commits require special care in order not to wrongly attribute violations to authors of later commits.
finally there is an implicit assumption behind the scenarios outlined above namely that individual developers have a distinctive fingerprint of violations that they introduce or fix.
if all developers tend on average to make the same mistakes then attribution information would not be very useful.
in this paper we present team insight a tool for finegrained tracking of software quality over time.
at its core is a method for tracking violations across revisions based on a combination of diff based location matching and hashbased context matching.
this approach is robust in the face of unrelated code changes or code moves and fast enough to work on large real world code bases.
team insight integrates with many popular revision control systems to attribute violations to developers.
it uses a simple distributed approach for efficiently analysing and attributing a large number of revisions in parallel.
we also present an approach for computing fingerprints from attribution data which compactly represent a summary of which violations a developer tends to introduce or fix.
we have used team insight to analyse the complete revision history of several large open source projects written in java c scala and javascript.
we performed experiments to verify that our violation tracking algorithm does not spuriously match up unrelated revisions and to gauge the relative importance of the different matching strategies.
finally we used the information about new and introduced violations to test the robustness of our developer fingerprints selecting a training set and a test set from the analysed snapshots we computed fingerprints from both sets of snapshots and compared them.
we found that fingerprints were both stable 1many version control systems provide a way to determine which developer last changed a given line of code.
this information however is not usually enough to determine who introduced a violation the last developer to touch the relevant source code may have been making an entirely unrelated edit.
also many violations are non local in that the code that causes them is far removed from the code that is flagged by the static analysis.
finally this approach does not provide any information about fixed violations.
set string revs ... for irevision rev new arraylist irevision keep if !revs.contains rev ... fig.
.
example of a violation i.e.
the fingerprints computed for a single developer from the test and the training set are very similar and characteristic of individual developers i.e.
the fingerprints computed for different developers are quite different .
hence our fingerprints could be useful in scenarios such as the ones outlined above.
we now turn to a brief exposition of some basic concepts underlying team insight section ii .
we then explain our violation matching technique in more detail section iii and discuss how it is used to attribute new and fixed violations to developers section iv .
next we describe our approach for computing developer fingerprints in section v. these developments are then pulled together in section vi which reports on our experimental evaluation.
finally section vii discusses related work and section viii concludes.
ii.
b ackground we start by motivating the concept of violation matching with a real world example from a java project we analysed.
at one point the file deletesnapshots.java in the code base contained the code fragment shown in figure .
on line the variable revsis declared to be of type set string yet the contains test on line checks whether rev which is of type irevision is contained in it.
this is highly suspicious unless unsafe casts were used to deliberately break type safety every element in revsmust be of type string which revis not.
thus the test must always return false which is probably not what the programmer intended.
in this case the code should have checked whether rev.getid which is a string representation of rev is contained in revs.
arguably this code should be rejected by the type checker but for historical reasons the contains method in java s collections framework is declared to have parameter type object so any object can be tested for membership in any collection regardless of the collection s type.
this problem is common enough that many static analysis tools for java including our own static analysis tool project insight check for it.
to trace the life cycle of this violation we consider seven revisions of the code base which we refer to as revisions to .
figure gives a brief summary of the relevant changes in each revision and the source location associated with the violation here the call to contains .
in each case the violation location is given as a pair of a file name and a line number.
for now we assume that these revisions were committed in sequence as shown in figure a .
2in practice of course locations are more precise they include a start and an end position and specify both line and column number.revision change summary violation location deletesnapshots.java created n a violation introduced deletesnapshots.java code added before violation deletesnapshots.java code added before violation deletesnapshots.java containing file renamed findobsoletesnapshots.java code added after violation findobsoletesnapshots.java violation fixed n a fig.
.
relevant revisions of the code in figure m n a b fig.
.
two commit graphs for the revisions in figure revisions where the violation is present are shaded.
the file containing the violation was first created in revision and the violation itself was introduced in revision at this point it was located at line of file deletesnapshots.java .
in revision the violation was still present but some code had been inserted before it so it had moved to line .
in revision it moved to line .
in revision its line number number did not change but the enclosing class was renamed to findobsoletesnapshots and the enclosing file to findobsoletesnapshots.java .
in revision the file was again changed but since the changes were textually after the violation its location did not change.
finally the violation was fixed in revision .
if we want to automatically and precisely attribute violations to developers we have to carefully keep track of violations across revisions.
for instance a violation tracking approach based purely on source locations would consider the violation in revision to be different from the violation in revision since they have different source locations.
consequently the author of revision would erroneously be considered to have fixed a violation on line and introduced a violation of the same type on line .
a more lenient location based approach might try to identify violations based on the name of the method and class enclosingits source location.
this however would fail in revision where the enclosing class is renamed along with the file .
ignoring source locations entirely one could attempt to match up violations based on the similarity of their surrounding code if two subsequent revisions contain two violations of the same type that appear in identical or very similar fragments of code then there is a good chance that both are occurrences of the same violation.
in revision for instance there were only two minor textual changes in addition to the file renaming so a similarity based approach could easily determine that the violation is still present and has simply moved to another file.
note however that neither location based matching nor similarity based matching is strictly superior to the other while the former cannot deal with code movement or file renaming the latter can become confused by unrelated changes in code close to the violation.
in such a case a similarity based matcher may not be able to identify violations even if their location has not changed.
team insight uses a combined approach detailed in the next section first it tries to match up as many violations as possible based on their source location.
for those violations that could not be matched up it computes a hash of the surrounding tokens similar to techniques used in clone detection and then matches up violations with the same hash.
additional care has to be taken when considering nonlinear commit graphs with branches and merges.
assume for instance that the revisions of figure were committed as shown in the commit graph of figure b .
here revision and revision are committed independently on separate branches and then merged together by a merge commit m. similarly revisions and are committed independently and merged by n. note in particular that the violation is now no longer present in revision which branched off before the violation was introduced in revision .
in the merge commit m however the violation is merged in with the changes from revision .
the author of the merge commit m should clearly not be blamed for introducing the violation since it is already present in revision .
in general we can only consider a merge commit to introduce a violation if that violation is absent in allits parent revisions but not the merge commit itself .
similarly the author of the merge commit n should not be considered to have fixed the violation since it was alreadyabsent in revision .
again a merge commit can only be considered to fix a violation if that violation is present in all its parent revisions but not the merge commit itself.
finally it should be noted that in any non trivial code base there are revisions that for one reason or another cannot be analysed.
for example this could be due to a partial or erroneous commits that results in uncompilable code or simply because some external libraries required by a very old revision are no longer available.
such un analysable revisions have to be treated with care when attributing violations.
if say revision of our example was not analysable then it would not be possible to decide whether the violation present in revision was introduced in revision itself or was already present in revision .
looking further back we can of course note that the revision was already present before revision so it is likely that neither nor introduced the violation but this is at best an educated guess.
this concludes our informal discussion.
we will now describe the team insight attribution algorithm in more detail.
iii.
m atching violations we start by establishing some terminology.
aproject is a code base that can be subjected to static analysis.
a snapshot is a version of a project at a particular point in time for instance if the project s source code is stored in a version control system every revision is a snapshot.
we assume that there is a parent of relation between snapshots.
there may be multiple snapshots with the same parent due to branching and conversely a snapshot may have multiple parents due to merging .
we do not assume a particular underlying static analysis system.
all we require is that the static analysis can for a given snapshot s produce a set of violations where each violation is uniquely identified by a source location land a violation type t. the source location in turn is assumed to be given by a start position and an end position delimiting the piece of code that gives rise to the violation.
thus the violation can be modelled as a triple s l t .
we explicitly allow for the case that the static analysis may not be able to analyse some snapshots in which case there will be no violation triples for that snapshot.
with this terminology in place the violation matching problem can be stated succinctly as follows given two snapshots spandscof the same project wherespis a parent snapshot of sc and two violation triplesvp sp lp tp andvc sc lc tc dovpandvcindicate the same underlying defect?
we will not attempt to give a precise semantic definition of when two violations indicate the same defect.
previous studies have shown that even developers familiar with a code base often disagree about the origin of code snippets so there may not be a single correct definition anyway.
instead 3in practice the static analysis may be able to analyse parts of a snapshot but we do not use this partial information since it is very hard to compare between snapshots.we describe the syntactic violation matching approach taken by team insight and leave it to our experimental evaluation to provide empirical support for its validity.
to decide whether two violations sp lp tp and sc lc tc match we employ a combination of three different matching strategies a location based strategy that only takes the violation locations lpandlcinto account a snippet based strategy that considers the program text causing the violations and a hash based strategy that also considers the program text around the violations.
we will now explore these strategies in detail.
note that clearly two violations can only match if they are of the same type so in the following we implicitly assume that tp tc.
a. location based violation matching the idea of location based violation matching is to use a diffing algorithm to derive a mapping from source positions inspto source positions in sc and then match up violations if they have the same or almost the same start position under this mapping.
our implementation uses the well known diffing algorithms of myers and of hunt and szymanski to derive source position mappings for individual file in the snapshot.
specifically we use the former algorithm for dense diffs where there is a lot of overlap between the two files and the latter for sparse diffs.
clearly computing pairwise diffs for all the files in the parent and child snapshots would be much too expensive hence we only compute diffs for files with the same path in both snapshots.
in particular if a file is renamed or moved to a different directory the location based violation matching will not be able to match up any violations occurring in it.
we rely on the hash based matching explained below to catch such cases.
we will use the java code snippets shown in figure as our running example in this section.
the code on the left which we assume to be from the parent snapshot contains three violations v1is an instance of the contains type mismatch problem mentioned in section ii v2flags a reader object that we assume for this example is not closed thus potentially leading to a resource leak v3flags a call to system.gc which is generally bad practice.
these three violations reappear in the child snapshot on the right but the statements containing them have been rearranged and new code has been inserted.
we will now show how our matching strategies match up these violations.
when given two files fpandfc wherefpis from the parent snapshot and fcfrom the child snapshot the diffing algorithms essentially partition fpandfcinto sequences of line rangesrp rp n andrc rc n respectively.
each pair of line ranges rp i rc i is either a matching pair meaning that the ranges are textually the same or a diff pair meaning that they are not the same.
every line in fpandfcbelongs to exactly one line range.
for a location l we write to mean the line range its start line belongs to and l to mean the distance in lines from lto the start of its line range.if apples.contains orange count fr new filereader tst system.gc fr new filereader tst if debug system.gc if apples.contains orange count parent snapshot child snapshotv1 v2 v3v0 v0 v0 fig.
.
examples of location based violation matching in general there is no unique way of performing this partitioning we simply rely on whatever partitioning the underlying diff algorithm produces.
in our example there are four line ranges as indicated by the boxes.
we draw lines between corresponding line ranges which are labelled with for matching pairs and with for diff pairs.
v0 and v0 form diff pairs while v0 is the single matching pair.
all violations have a distance of zero from the start of their region except for v0 which has v0 .
consider now a violation vcin the child snapshot at location lc and a violation vpin the parent snapshot at location lp where the files containing lcandlphave the same path.
if their ranges correspond we try to match them up as follows if is a matching pair then the violations are triggered by code that did not change between spand sc so we only match them up if their positions within the region are exactly the same i.e.
lc lp .
in our example there is only one matching pair the one containingv2andv0 respectively.
since their positions in the region are the same they are matched up.
if on the other hand is a diff pair we allow the locations to vary slightly vcandvpare considered to match ifj lc lp j for some threshold value .
in our implementation we use .
for example consider v3on the left and v0 3on the right.
their regions form a diff pair and v3 whereas v0 since their distance is less than three lines we match them up.
b. snippet based violation matching since location based matching requires violation locations to belong to corresponding line ranges this matching strategy will fail for violations like v1whose location has changed significantly between snapshots.
we use an additional strategy to catch simple cases where a violation has moved in the same file but is triggered by exactly the same snippet of code if lc andlpbelong to the same file and the source text of lcandlp i.e.
the text between the respective start and end positions is the same we match them up.
in the example this allows us to also match up v1andv0 .
c. hash based violation matching neither location based matching nor snippet based matching apply to violations in files that are renamed or moved between snapshots.
to match up those violations we employa hash based strategy that tries to match violations based on the similarity of their surrounding code.
specifically for a violation v s l t we compute two hash values h v andh v the former is computed from thentokens up to and including the first token in l and the latter is computed from the ntokens starting with the first token inl wherenis a fixed threshold value by default we usen .
two violations vcandvpare considered to match ifh vc h vp orh vc h vp .
if the location lstarts less than ntokens into the file h v is undefined similarly h v is only defined if l starts at least ntokens before the end of the file.
this avoids spurious matches due to short token sequences but it makes it necessary to use two hashes since otherwise violations near the beginning or the end of a file could never be matched.
iv.
a ttributing violations now that we have discussed how to match violations between two snapshots let us consider the problem of attributing a snapshots that is computing the sets of new and fixed violations.
clearly a snapshot is only attributable if both it and all its parent revisions can be analysed.
as a special case if sdoes not have any parents we do not consider it attributable since it may contain code copied from other sources and we know nothing about the history of the violations in this code.
let us first consider the case where shas precisely one parent snapshot p. we write v s andv p for the sets of violations of sandp respectively.
using the matching strategies described earlier we can determine for any two violations whether they match.
in general a single violation in v s may match more than one violation in v p and vice versa.
to decrease the chance of accidentally matching up unrelated violations we prioritise our matching strategies as follows apply location based matching first.
if v2v s is located in a diff range and may match more than one violation in v p choose the closest one.
only use snippet based matching if diff based matching fails.
if more than one violation in v p has the same source text as v choose the closest one.
only use hash based matching if location based matching and snippet based matching both fail.
if there aretwo or more violations in v s orv p that have the same hash exclude them from the matching.
furthermore once a violation from v p has been matched up with a violation from v s we exclude it from further consideration.
in this way we obtain a matching relation that matches every violation in v s with at most one violation in v p and vice versa.
now the new violations in sare simply those for which there is no matching violation in p n s fv2v s j 9v02v p v v0g dually the set of fixed violations in sare those violations inpfor which there is no matching violation in s f s fv02v p j 9v2v s v v0g note that according to this definition simply deleting a piece of code fixes all the violations in it.
merge commits have more than one parent.
generalising the definition of n s to this case poses no problems but generalising f s is not so straightforward.5thus we choose not to define any new or fixed violations for merge commits.
finally let us consider how to efficiently attribute multiple snapshots.
a common use case would be to attribute every snapshot in the entire revision history of a project or every snapshot within a given time window.
since all parents of a snapshot need to be analysed before attribution is possible we could perform a breadth first traversal of the revision graph start by analysing the first snapshot and all its children assuming that all of them are analysable the child snapshots can then be attributed.
now analyse all of their children for which all parents have been analysed in turn and attribute them and so on.
in practice however later revisions are often more interesting users normally first want to understand recent changes in code quality before they turn to historic data.
also older snapshots are more likely to be unanalysable due to missing dependencies so attribution may not even be possible in many cases.
this is why team insight attributes snapshots in reverse chronological order.
since analysing and attributing a large number of revisions can take a very long time the analysis and attribution tasks need to be parallelised as much as possible.
to this end team insight splits up all analysable snapshots into attribution chunks an attribution chunk consists of a set aof snapshots to attribute together with a set uof supporting snapshots such 4note that this case only arises if the violation has disappeared from its original file since both location based matching and diff based matching fail and multiple new copies of the violation have appeared elsewhere.
this is most commonly caused by a piece of code containing the violation being cut from its own file and pasted into multiple other files.
by avoiding a match we force this to be considered as a single fixed violation and multiple introduced violations which seems like the most appropriate way to model it.
5f s consists of violations in the parent snapshot and so for merge commits we would have to identify corresponding violations across all parent snapshots.
this is made more difficult by the fact that our definition of hashbased matching is not transitive since only one hash needs to be equal to establish a match so all pairs of parent snapshots would have to be compared against each other.that for each snapshot in aall of its parents are contained in eitheraoru.
clearly once all snapshots in a uhave been analysed all the snapshots in acan be attributed without referring to any other snapshots.
thus different attribution chunks can be processed in parallel.
v. d eveloper fingerprinting we now discuss an important application of attribution information computing violation fingerprints to characterise which kinds of violations developers tend to introduce or fix.
we represent the violation fingerprint for a developer by two vectors one containing information about introduced violations and one about fixed violations.
each vector has one component per violation type where the component corresponding to some violation type tells us how often the developer introduces or fixes violations of that type.
given a set of attributed snapshots we can compute a fingerprint for every developer by simply summing up the number of introduced and fixed violations of every type over all the snapshots authored by the given developer.
however such uncalibrated fingerprints are difficult to compare between developers a developer who has changed more lines of code is in general expected to have introduced and fixed more violations than a developer with fewer contributions.
thus it makes sense to scale the components of each vector to account for such differences in productivity.
we consider two scaling factors scaling by churn where the violation counts are divided by the total number of lines of code changed by the developer across the set of considered snapshots.
scaling by total number of violations where the violation counts are divided by the total number of new fixed violations of the developer.
this provides an overview of what portion of the violations a developer introduces or fixes are of a certain type.
as an example assume we have two developers d1and d2 and we have a set of snapshots where violations of two types have been attributed.
if d1has introduced a total of violations of the first type and fixed none while introducing violations of the second type and fixing her uncalibrated violation fingerprint is h i. similarly if the fingerprint ofd2ish i then this means that he introduced four violations of the first type and one of the second type while fixing four violations of the second type but none of the first type.
given these uncalibrated fingerprints it may be tempting to deduce that d1introduces more violations of the first type thand2.
if however d1contributed lines of churn whiled2only touched lines of code this conclusion is unwarranted d1contributed times as much code than d2 but only introduced about twice as many violations of the first type.
scaling the fingerprints by the amount of churn in thousands of lines of code d1 s fingerprint becomesh i whiled2 s fingerprint is still h i this shows that relatively speaking d2is muchname language snapshots size kloc churn kloc total new total fixed hadoop common java mongodb c spark scala gaia javascript table i projects used in the evaluation more likely to introduce violations of both types but is also more likely to fix violations of the second type.
if instead we want to compare how many of the fixed introduced violations of a developer are of a certain type we can scale by the total number of fixed introduced violations overall developer d1introduced violations and fixed two so her fingerprint becomes h i developerd2introduced five and fixed four giving the very similar fingerprint h i. which kind of fingerprint is more useful depends on the application area.
fingerprints scaled by churn are useful to compare different developers and could for instance be used to find out which team member is most adept at fixing a given kind of violation.
fingerprints scaled by the number of violations on the other hand compare a single developer s performance in different areas and could hence be used to select appropriate training.
vi.
e valuation we now report on an evaluation of our violation matching approach and the developer fingerprinting on four large opensource projects with significant revision histories.
for the violation matching we investigate the quality of the matchings produced and the relative importance of the different matching strategies.
for the fingerprinting we assess whether fingerprints are in fact characteristic of individual developers.
a. evaluation subjects as our evaluation subjects we chose the four open source projects hadoop common mongodb spark and gaia as shown in table i. for each of our subjects the table shows the language they are implemented in the total number of snapshots that were attributed the approximate size in thousand lines of code of the latest attributed snapshot the total amount of churn across all attributed snapshots and the total number of new and fixed violations.
to find violations we used the default analysis suites of our tool project insight comprising analyses for java for c for scala and for javascript.
the raw analysis results our experiments are based on are available from .
b. evaluating violation matching we evaluate our violation matching algorithm with respect to two evaluation criteria 6note that for gaia there are more fixed than new violations this is because some violations were introduced in unattributable revisions.project exact fuzzy snippet hash hadoop .
.
.
.
mongodb .
.
.
.
spark .
.
.
.
gaia .
.
.
.
table ii violation matchings contributed by individual algorithms ec1 how many violation matchings are contributed by the different matching algorithms?
ec2 do these violation matchings match up violations that actually refer to the same underlying defect?
to answer ec1 we randomly selected snapshots from each of our subject programs and counted how many violation matchings were contributed by each of the algorithms.
the results are shown in table ii for the location based violation matching we distinguish between exact matches column exact and matches in diff regions that are no further than three lines apart column fuzzy columns snippet and hash refer to the snippet based matching and hash based matching algorithms respectively.
as one might expect the overwhelming majority of matchings are exact usually a single commit only touches a small number of files so almost all violations remain unaffected.
most of the remaining matchings are found by the fuzzy matching algorithm which applies if there were changes within the same file that do not affect the violation itself but only shift it by a few lines.
snippet based matching only applies in a few cases while the contribution of the hash based algorithm varies considerably between projects it contributes very little on hadoop but is more important than the fuzzy matching algorithm on gaia.
this suggests that exact matching may in practice be enough for many applications.
for our use case of attributing violations to developers however this is not so on mongodb for instance hash based matching contributes matchings.
this number is vanishingly small when compared to the total number of matched violations but it is ten times the total number of fixed violations identified across all attributed snapshots.
without hash based matching each missing matchings would give rise to one new violation and one fixed violation dramatically influencing the overall result.
7recall that the different algorithms are applied in stages where more sophisticated algorithms are only run for those violations that could not be matched using the simpler algorithms.as for ec2 it can ultimately only be answered by domain experts manually examining a large number of violation matchings and deciding whether they are reasonable or not.
in lieu of a large scale experiment the outcome of which would in the light of be of doubtful significance we manually inspected randomly selected hash based matchings from each of our four subject programs.
most of these matchings corresponded to file renames or moves that were confirmed either by the snapshot s commit message or by version control metadata.
the remainder were due to code being copied between files or moved within a file with minor changes being performed at the same time thus preventing the violations from being matched up by the snippet based algorithm.
none of the matchings were obviously wrong.
we have not yet performed a comprehensive performance evaluation of our violation matching approach.
however we observed during our experiments that the location based and snippet based algorithms each take about three to four milliseconds to compare two files.
the hash based algorithm is global and hence more expensive taking around four seconds to compute matchings for a pair of snapshots.
c. evaluating fingerprinting the other main focus of our evaluation is to examine how meaningful our violation fingerprints are.
recall that fingerprints are computed from a set of attributed snapshots.
if fingerprints for the same developer vary wildly across different sets of snapshots we would have to conclude that they are not well defined.
conversely if different developers are assigned very similar fingerprints this would mean that fingerprints are not characteristic.
finally even if violation fingerprints are characteristic we have to show that they do not simply reflect more basic characteristics of a developer such as average churn per commit or average number of violations per commit.
we distill these considerations into three evaluation criteria ec3 are fingerprints stable across different snapshot sets?
ec4 is there a measurable difference between the fingerprints computed for different developers?
ec5 are violation fingerprints independent of churn and total number of new and fixed violations?
to answer these questions we designed an experiment in which we take two sets aandbof snapshots and compute for every developer da violation fingerprint f d a based on the snapshots in set a the training set and a violation fingerprint f d b based on the snapshots in set b the test set .
now we compare f d a against the fingerprints f d0 b ofall developers including d as computed from set band rank them by their euclidean distance kf d a f d0 b kfrom the fingerprint of d. if fingerprints are highly dependent on the set of snapshots used to compute them ec3 we would expect the outcome of this ranking to be mostly random.
similarly if all developers 8recall that fingerprints are pairs of vectors to determine their distance we simply concatenate the two constituent vectors into one larger vector and then compute their euclidean distance.tend to have similar fingerprints ec4 a random outcome would be expected.
to address ec5 we perform our experiments with two kinds of fingerprints violation density fingerprints and violation vector fingerprints scaled by total violations.
the former are two element vectors containing the total number of new violations and the total number of fixed violations scaled by the number of lines changed.
the latter are vectors with one element per violation type scaled by the total number of fixed new violations as described in section v. if developers on average introduce and fix the same number of violations per line of code they touch the ranking using violation density fingerprints should be random since these fingerprints are scaled by churn.
similarly if developers introduce and fix different kinds of violations with the same frequency the violation vector fingerprints would produce random rankings since they are scaled by the total number of violations.
care has to be taken in selecting the snapshot sets aandb and in choosing which developers to compute fingerprints for.
we exclude commits with less than lines of churn since they most likely have too few fixed or new violations to be interesting and commits with more than lines of churn since they are not likely to actually be the work of a single developer .
we also do not include commits by developers who have contributed fewer than commits or less than lines of churn since their contributions are likely too small to derive a significant fingerprint from.
overall our experiment comprises the following steps randomly partition the set of all considered snapshots into two halves aandb.
compute fingerprints for all developers on aandb and compute ranks as explained above.
for every kind of fingerprint count how often a developer s fingerprint was ranked as being closest to themselves second closest to themselves and so on.
to enhance statistical significance we perform these steps times on every test subject and aggregate the counts.
the results of these experiments are shown in figure figure figure and figure .
every figure contains two histograms showing the results for the violation density fingerprint on the left and for the violation vector fingerprint on the right.
the individual bars show how often over the runs of the experiment a developer was ranked as being closest to themselves second closest to themselves and so on.
we note that none of the experiments yield a random distribution.
instead both kinds of fingerprints consistently rank developers as similar to themselves and dissimilar to others thus suggesting a positive answer to ec3 andec4 .
since this is in particular true for the violation density fingerprints on all four subject programs we conclude that the ratio of introduced and fixed violations per lines of changed code is not the same for all developers.
the results are even more striking for the violation vector fingerprints our experiments give a strong indication that each developer has 9note that for readability the two graphs are not on the same scale.
violation density fingerprints violation vector fingerprints fig.
.
ranking results for hadoop common violation density fingerprints violation vector fingerprints fig.
.
ranking results for mongodb violation density fingerprints violation vector fingerprints fig.
.
ranking results for spark violation density fingerprints violation vector fingerprints fig.
.
ranking results for gaiaa very characteristic signature in terms of the violations they introduce and fix as a proportion of their overall number of violations.
ec5 can hence also be answered in the affirmative.
these results hold for all our subject programs.
the comparatively weak results for mongodb and spark are most likely due to the relatively sparse data across the snapshots we analysed there were only developers on mongodb and on spark who accumulated enough churn and commits to be considered in our experiments compared to on hadoop and on gaia which makes the results less stable.
d. threats to validity our static analysis examines source code as it is seen by the compiler during a build including generated code.
attributing violations in generated code code is however not straightforward so we manually excluded it from consideration on a best effort basis.
given the size and complexity of our subject programs we may have missed some files which could affect our results.
another difficulty are bulk imports of third party code.
our experiments account for this by excluding revisions with large amounts of churn but this is only a heuristic.
furthermore we ran our static analysis with its standard rule set.
using different rules might conceivably lead to a different outcome but preliminary experiments with subsets of the standard rule set yielded similar conclusions.
in computing fingerprints we only consider developers who have contributed at least commits and lines of churn.
we have not yet experimented with varying these thresholds.
we used violation density fingerprints and violation vector fingerprints in our experiments.
many other ways of computing fingerprinting could be devised and some of them may well be even more characteristic of individual developers than the ones we used.
for example a syntactic fingerprint based on preferred indentation style would probably be highly characteristic.
however such shallow fingerprints do not yield much insight into the contributions a developer makes to the software quality of a code base which is our main interest.
finally we note that care has to be taken in generalising our results to other projects.
we deliberately chose diverse projects utilising different programming languages.
although they are open source all four projects have core developers from large software companies.
therefore we expect our results to generalise to both commercial and open source projects.
vii.
r elated work spacco et al.
discuss two location based violation matching techniques used in findbugs and fortify .
to allow for code movement they relax their location matching in various ways for example by matching violations at different locations within the same method and by only considering the name but not the full path of the enclosing file.
as in our approach they prioritise stricter matching criteria over more relaxed ones.
however their approach cannot match violations that have moved between methods or even classes.
diff based matching of code snippets has been used to track violations forward and fixed bugs backward throughrevision history as well as for tracking code clones .
sliwerski et al.
directly use revision control metadata to pair up fix inducing commits with later bug fixes.
kim et al.
improve upon this by adding a form of diff based matching similar to our approach.
hash based matching does not seem to have been used for tracking static analysis violations before but the technique itself is well established in multi version analysis .
clone detectors in particular often use hashes to identify potential code clones within a given code base.
our hashes are tokenbased similar to the one used by pmd s code clone detector .
other systems use more advanced similarity metrics based on the ast or the pdg which are less scalable.
in origin analysis whole functions are hashed based on attributes such as cyclomatic complexity as well as call graph information.
our methods for violation matching strive to be language and analysis independent and hence cannot directly employ such advanced hash functions.
violation fingerprints as defined in this paper appear to be novel.
previous work has considered other developer characteristics such as code ownership that is how much experience a developer has with a piece of code .
typically these characteristics are computed entirely from source control information without any static analysis.
developer fingerprints based on layout information lexical characteristics and software metrics have been employed in software forensics to identify authors of un attributed code .
spafford et al.
suggest considering typical bugs as well.
judging from our dataset this seems difficult since most revisions introduce or fix at most one or two violations which is insufficient to derive a meaningful fingerprint.
viii.
c onclusion we have motivated the need for enriching static analysis results with revision information to track changes in code quality over time and attribute changes to individual developers.
the main enabling technique for such an integration is violation tracking which determines new and fixed violations in a revision relative to its parent revisions.
we have discussed one approach for implementing violation tracking and validated it on several substantial open source projects written in different programming languages.
we furthermore demonstrated that developers have characteristic fingerprints of violations they tend to introduce and fix.
it has been observed that static analysis violations are often ignored by developers .
some organisations have tried to address this by imposing a commit gate that enforces adherence to coding rules but experience with our clients has shown that this is counter productive sometimes business circumstances necessitate the introduction of technical debt which manifests itself through an increased number of violations .
fingerprints on the other hand allow each individual to keep track of their own coding habits of where they are well and where they can improve thus helping to establish an esprit de corps among a team of developers.