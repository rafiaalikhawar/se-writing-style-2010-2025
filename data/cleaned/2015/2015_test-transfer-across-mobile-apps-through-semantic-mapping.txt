test transfer across mobile apps through semantic mapping jun wei lin reyhaneh jabbarvand and sam malek school of information and computer sciences university of california irvine usa junwel1 jabbarvr malek uci.edu abstract gui based testing has been primarily used to examine the functionality and usability of mobile apps.
despite the numerous gui based test input generation techniques proposed in the literature these techniques are still limited by lack of context aware text inputs failing to generate expressive tests and absence of test oracles.
to address these limitations we propose c raft droid a framework that leverages information retrieval along with static and dynamic analysis techniques to extract the human knowledge from an existing test suite for one app and transfer the test cases and oracles to be used for testing other apps with the similar functionalities.
evaluation of craft droid on real world commercial android apps corroborates its effectiveness by achieving precision and recall on average for transferring both the gui events and oracles.
in addition of the attempted transfers successfully generated valid and feature based tests for popular features among apps in the same category.
index t erms test transfer test migration gui testing natural language processing semantic similarity i. i ntroduction gui testing is the primary way of examining the functionality and usability of mobile apps.
to reduce the cost of manual gui testing many automated test input generation techniques have been proposed in the literature over the past years .
these techniques follow different exploration strategies such as random model based stochastic or searchbased for generating inputs in order to achieve a pre defined testing goal e.g.
maximizing code coverage or finding more crashes.
despite all these efforts to automate the gui test input generation several studies indicate that they are not widely adopted in practice and majority of the mobile app s testing is still manual .
there are three main reasons that limit the viability of these techniques lack of context aware text inputs.
most of the stateof the art techniques use random input values or rely on the manual configurations for text inputs.
however contextual text inputs are critical to thoroughly test majority of the apps e.g.
city names for a navigation app correct urls for a browser app and valid username password for a mail client app.
without such meaningful inputs exploration of the app under test aut may get stuck at the very beginning and gui states deep in the testing flow may never been exercised.
failing to generate expressive tests.
majority of the automated testing techniques aim to maximize the code coverage or reveal as many crashes as possible.
the generated tests by such techniques are typically feature irrelevant or unrepresentative of the canonical usages of apps .
this lack of expressiveness makes debugging cumbersome as such tests do not include the reproduction steps that can be organized by use cases or features .
absence of test oracles.
despite a few efforts for automatic generation of test oracles for mobile apps majority of the existing test generation tools are unable to identify failures other than crashes or run time exceptions.
without automated test oracles such tests cannot thoroughly verify correct behavior of the aut.
to address these limitations we propose c raft droid a framework to reuse an existing test suite for one app to test other similar apps.
c raft droid is inspired by recent work from behrang and orso and rau et al.
which provided initial evidence of the feasibility of test transfer for mobile apps and web applications respectively.
like their works our proposed technique transfers available test cases corresponding to a specific feature or use case scenario of one app to other apps with similar functionality.
however unlike their work c raft droid is also able to transfer the test oracles if they exist.
to enable context awareness for text inputs c raft droid relies on information retrieval techniques to extract the human knowledge from an existing test suite and reuse it for other apps.
since test transfer is across apps with similar functionalities features the generated tests using c raft droid are inherently feature relevant and expressive.
as c raft droid not only transfers test inputs but also oracles assertions it is able to thoroughly verify correct behavior of the aut.
two insights from the prior literature form the foundation of our work.
first apps within the same category share similar functionalities.
for example shopping apps should implement user registration and authentication to provide personalized services.
as another example web browsers should implement common features such as browsing adding removing tabs or bookmarking urls despite different strategies they take for enabling privacy.
second gui interfaces for the same functionality are usually semantically similar even if they belong to different apps with different looks and styles.
by semantic similarity we mean the conceptual relation between the textual information e.g.
the text adjacent labels or variable names which can be retrieved from actionable gui widgets such as buttons input fields or checkboxes.
for instance a button to start the registration process on an app can appear with text join sign up or create account .
even if the texts are syntactically different they are semantically related.
as another example a confirm and pay button on a shopping app for checkout can be a 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
place order button on another shopping app.
extensive evaluation of c raft droid on real world commercial and open source android apps collected from various categories on google play including popular apps such aswish y elp and firefox focus confirms effectiveness of theproposed approach.
in fact of the attempted transfers by c raft droid successfully generated valid and feature based test cases with precision and recall on average forthe transferred gui events and oracles.
this paper makes thefollowing contributions a novel technique for transferring both test inputs andoracles across mobile apps through semantic mapping ofactionable gui widgets.
an implementation of the proposed framework for an droid apps which is publicly available .
empirical evaluation on real world apps demonstratingthe utility of c raft droid to successfully transfer tests across mobile apps.
the remainder of this paper is organized as follows.
section ii introduces a motivating example that is used to describeour research.
section iii provides an overview of our frame work and sections iv vi describe the details of the proposedtechnique.
section vii presents the evaluation results.
thepaper concludes with a discussion of the related research andavenues for future work.
ii.
m otiv a ting example to illustrate how c raft droid works consider rainbow shops a shopping app for women clothing and y elp a local search app for services and restaurants.
figures 2and show the registration process on rainbow shops andy elp respectively.
to register a new account on rainbowshops user starts by clicking on the join button figure a which directs the user to a registration form figure b .by filling the required fields of registration form and clickingon the create account button rainbow shops creates anaccount for the user and moves to the profile page figure c which shows information about user such as her username i.e.
sealbot.
to initiate the registration process on y elp the testing flow starts by clicking on the profile tab denoted by me in figure3 a. then the user should navigate through several screens toprovide required registration information figures d to e .finally by clicking on the sign up button figure f theregistration process is complete and y elp moves to the profilepage where user can see her username i.e.
sealbot figure3 g .
while the overall registration process in these two apps follows the same steps clicking on a button to start registration filling the registration form and submitting information adirect copy of the test steps from rainbow shops to y elp is notpossible due to the following reasons the mapping of teststeps between the two apps is not one to one.
for example toreach the registration form rainbow shops requires only oneclick figure a while it takes three clicks in y elp to reachthe registration form figures a b and c .
as anotherexample a user provides personal information using two formsin y elp compared to the one form in rainbow shops.
the fig.
overview of c raft droid mapping of gui widgets is challenging especially if they aresyntactically different but semantically similar.
for example the clicked buttons in these two test flows are different in termsof their label i.e.
join in rainbow shops and sign up iny elp.
despite these challenges c raft droid is able to transfer a test case that verifies the registration process in rainbowshops to y elp by semantically mapping their gui widgets.in the following sections we describe the details of how c raft droid identifies the matches and transfers gui oracle events from rainbow shops to y elp.
iii.
a pproach overview figure provides an overview of c raft droid consisting of three major components test augmentation component that augments test cases available for an existing app i.e.
source app with the information extracted from its guiwidgets that are exercised during test execution model extraction component that uses program analysis techniques to retrieve the gui widgets and identify transitions betweenactivity components of a target app and test generation component that leverages natural language processing nlp techniques to compute similarity between gui widgets of thesource and target apps to transfer tests.
more specifically c raft droid takes an existing mobile app and its test case as input.
it then instruments executes and augments the source test with textual information retrievedfrom the gui widgets it exercised during its execution.
af terwards c raft droid statically analyzes the target app to extract its ui transition graph uitg .
finally c raft droid uses uitg of the target app to search for widgets that aresimilar to those found in the source app to generate a newtest.
it leverages nlp techniques such as word embedding tocompute the similarity between gui widgets in the source andtarget apps.
regarding the transfer of oracle c raft droid is able to deal with several types of oracles that are commonlyused in practice including negative ones such as nonexistencecheck of text.
we will describe these three components in moredetail in the following sections.
iv .
t est augment a tion algorithm shows how test augmentation component works.
it takes the source app srcapp with an existing test case t as input and generates an augmented test case t prime which contains textual meta data related to the gui widgets authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
excerpted registration process on rainbow shops fig.
excerpted registration process on y elp that are exercised by t. we formulate test case tas a set of gui events w1 a1 w2 a2 ... wherewiis a gui widget e.g.
button and aiis the action performed on wi e.g.
click .a n actionaican be a single operation such as click or an operation with arguments such as swipe that contains starting and ending coordinates.
if a test comes with an oracle c raft droid identifies it as a special type of event wi ai whereaiis an assertion e.g.
assertequal .
if the assertion is widget specific e.g.
existence check of a widget widenotes the widget to be checked.
on the other hand if the assertion is widget irrelevant e.g.
existence of certain text on the screen wiis set to be empty.
algorithm starts by initializing variables line and launching the source app line .
for each gui or oracle event wi ai int test augmentation component dynamically analyzes current screen to retrieve required textual information ofwi such as the resource id text and content desc .
to that end it uses adb tool to dump current screen i.e.
an xml file of current ui hierarchy line and parses the xml file line .
algorithm updates wiwith textual information to authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm test augmentation input a source app srcapp a test case t w1 a1 w2 a2 ... forsrcapp output an augmented test case t prime w prime a1 w prime a2 ... t prime launchapp srcapp for each wi ai tdo screen dumpcurrentstate info getextrainfo wi screen w prime i augment wi info t prime t prime w prime i ai execute wi ai end for returnt prime fig.
excerpted ui transition graph for y elp produce augmented widget w prime iand adds it to the augmented test line .
finally it executes wi line to move to the next widget.
v. m odel extraction model extraction component statically analyzes the target app targetapp to generate a model called ui transition graph uitg .
this model represents how activities fragments of an app interact with each other through invoking gui widgets event handlers.
uitg will later be used by test generation component to search for a match for a given widget of source app in the target app.
at a high level uitg represents activity components comprising the target app as nodes and gui events as transitions among the nodes.
each node of uitg in turn contains a list of widgets that can be rendered directly through the activity or indirectly through fragments comprising the activity.
it is important to consider fragments since an activity may consist of several fragments which can be called from different activities.
figure shows a partial uitg for y elp.
as demonstrated by this uitg clicking on the me widget transfers users from the home activity to the userprofilel oggedout activity .
model extraction constructs the uitg in two steps extracting activities fragments and their corresponding gui widgets.
model extraction parses manifest fileof the target app to collect a list of activity components.
for each identified activity it then extracts all of the gui widgets e.g.
button edittext and textview that it renders during execution of the app.
these widgets are either implemented by the activity itself or inside fragments within the activity.
to extract widget list model extraction analyzes xmlbased meta data resource files for statically defined gui widgets as well as the source code for dynamically defined ones.
more specifically to get the list of statically defined gui widgets model extraction first refers to the source code of each activity fragment and looks for specific methods such assetcontentview and findviewbyid to identify resource files corresponding to widgets.
it then adds all the widgets identified in the resource file to the widget list of the activity.
to get the list of dynamically defined gui widgets model extraction analyzes the source code of activity fragment components to identify initialization of gui widget elements in them and adds the corresponding widgets to the widget list.
during extraction of widgets model extraction also retrieves and stores their corresponding textual information.
identifying transitions between activities.
model extraction starts from the launcher activity which is specifically identified in the manifest file.
for each activity it analyzes the event handlers of all the gui widgets in the activity s widget list e.g.
onclick for a button or oncheckedchanged for a check box.
if the event handler of a widget invokes specific methods that result in transition to another activity e.g.
startactivity or fragment e.g.
begintransaction model extraction includes a transition between the two activity components.
we identify two types of transitions in uitg inter component transition .
the method call results in a transfer of control between two distinct activities.
for example when user clicks on the me tab in figure 3a the onclick handler of this widget initiates an intent message and invokes startactivity method to transfer the control to userprofilel oggedout activity.
intra component transition .
the method call to a gui event handler results in a transition back to the same acti vity.
such transitions happen when an activity consists of multiple fragments and performing an action on one fragment results in transition to another fragment within the same activity.
for instance figures d and e represent two fragments related to the createaccount activity .
clicking on the next button on the first fragment moves the control to the second fragment.
thereby this transition causes a loop in the uitg as shown in figure .
after generation of uitg model extraction component combines the widgets collected for all nodes into an associative array denoted as map .
this construct maps all the gui widgets in targetapp to the corresponding activity fragment that can render them during execution of app.
vi.
t est genera tion algorithm demonstrates how the test generation component of c raft droid works.
this component takes the targetapp its corresponding uitg and widget map and an augmented test for the srcapp t prime as input and generates a authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm test generation input targetapp uitg oftargetapp map widgets on each activity frag.
in the targetapp t prime w prime a1 w prime a2 ... fromsrcapp output tnew wn1 an1 wn2 an2 ... fortargetapp while true do tnew for each w prime i ai t primedo candidates getcandidates w prime i map uitg for each wn candidates do leadingevents getleadingevents wn uitg map t new ifleadingevents negationslash null then an generateaction w prime i ai wn tnew tnew leadingevents wn an break end if end for end for if fitness tnew threshold or timeout break end if end while returntnew function get leading events wn uitg map t new execute tnew srcact getcurrentactivity destact getactivity wn map paths getpaths srcact destact uitg sort paths for each path paths do isvalid validate wn path map ifisvalid true then returnpath end if end for returnnull end function new test case tnew fortargetapp by transferring the gui and oracle events of t prime.
to that end it iterates over every gui or oracle event wi ai int primeand collects a list of candidate widgets in targetapp candidates which are ranked based on their similarity to wi line details in section vi a .
for each gui widget wnincandidates algorithm checks to see if it is reachable and if so identifies a sequence of events leading events that should be executed to reach wn line details in section vi b .
for a reachable candidate wn algorithm identifies the appropriate action an line details in section vi c adds wn an along with the leading events to tnew line andmoves to the next w prime ito find its match line .
once all the widgets in t primeare checked for a match in targetapp algorithm checks the termination criteria line details in section vi d .
if termination criteria are met it terminates line .
otherwise it repeats the whole process of transfer.
the reason for repeating the test generation process is that test generation component relies on uitg to identify reachability of the candidate widgets.
since uitg is derived through static analysis it is an over approximation of the app s runtime behavior.
in addition static analysis is not able to realize dynamically generated contents such as pop up dialogues or buttons in android s webview.
to overcome these limitations c raft droid executes targetapp to determine reachability and updates uitg based on runtime information.
thereby test generation repeats transfer with an updated uitg to increase the chance of successful transfer.
in the remainder of this section we describe the key components of test generation in more detail.
a. computing similarity score in the getcandidates function line test generation considers two factors to compute the similarity between widgets their corresponding textual information and their location in uitg .
more specifically to determine the similarity of a candidate widget wnto source widget w prime i test generation first computes score t a measure of how similar are the textual information of wnto that of w prime i. it then normalizes score tbased on how close wnis to the current activity by leveraging uitg to compute the final similarity value.
computing textual similarity score score t craft droid collects the textual information of a gui widget from multiple sources such as widget s attributes the name of activity fragment that renders it and its immediate parent and siblings.
c raft droid follows a two step process to measure the textual similarity.
it first retrieves raw textual data from different sources and processes them.
it then utilizes the processed data to measure the similarity in a weighted scheme among all sources.
text processing.
test generation processes the collected textual information by test augmentation and model extraction through applying a series of common practices in nlp including tokenization and stopword removal.
the result of this step is a set of word lists for every textual information.
for example textual information for the button sign up in figure b can have three word lists from its label from its resource id of sign upbutton and from its activity name of userprofilel oggedout .
computing textual similarity.
to determine score tbetween two gui widgets wnandw prime i test generation computes the similarity score for each information source and then calculates a weighted sum of the individual scores.
since the previous step produces a set of word lists for each gui widget the problem of determining the textual similarity between two gui widgets is dual to the problem of computing the similarity score between word lists.
craft droid leverages word2v ec a model that captures the linguistic contexts of words to compute the simi46 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
larity score between two word lists.
that is it first computes the cosine similarity for all possible word pairs in the word lists.
next it identifies the best match among pairs based on two criteria the pair has the highest cosine similarity and every word is only matched once.
for instance consider the create account button in figure b and sign up button in figure b from the motivating example.
the two word lists corresponding to these buttons are and .
to compute the similarity score between them the pairwise cosine similarity is calculated as follows bracketleftbiggcreate account sign .
.
up .
.
bracketrightbigg in this example the word pairs that match the mentioned criteria are create sign and account up with cosine similarity of .
and .
respectively.
thereby the final similarity score between these two word lists is calculated as .
.
.
which is the score for the text of these two buttons.
similarly the two word lists corresponding to the resource id of these two widgets are button sign up and .
the cosine similarity for these lists are as follows button sign up sign .
.
.
up .
.
.
button .
.
.
based on these values the score for resource id is calculated as .
.
.
.
.
if only these two information sources are considered to compute the similarity score the final textual similarity between these two buttons is .
.
.
.
computing final similarity score to compute the final similarity score between wnandwi test generation normalizesscore tbased on the distance of wnfrom current screen.
test generation consults uitg to get the shortest distance d i.e.
number of gui events from the current screen to the activity to which wnbelongs.
it computes the final similarity as follows similarity wn w prime i score t ifd score t l o g2d otherwise this adjustment assigns a higher priority to candidate gui widgets that are closer to the current screen.
this is because intuitively the steps or events to test the same functionality should not be significantly different even in different apps.
for example consider the join button from figure a in rainbow shops.
the most semantically similar widget in y elp app to this button is the sign up button which appears in multiple uis e.g.
figures b c and f. to identify which one of these buttons is the best match for join c raft droid starts from the launcher activity of y elp home activity and finds the closest node in its uitg figure that contains a sign up button userprofilel oggedout activity which is shown in figure b.b.
reachability check the function getleadingevents in algorithm checks the reachability of wn a candidate widget in targetapp that can be matched to w prime i. if reachable the function returns the gui events leading to the activity holding wn.
to that end firsttnew series of gui events successfully transferred so far is executed and the last activity srcact executed by tnew is identified line .
the widget map is then used to pinpoint the activity destact that holds wn line .
next all the potential paths in uitg fromsrcact todestact are explored to derive sequences of gui events leading events that execute each path line .
the identified paths are sorted based on their length line .
this way shorter paths have a higher chance of being selected thereby making the length of final transferred test shorter which is generally desirable for debugging purposes.
the function validate then verifies whether wnis reachable by executing actions corresponding to each path on targetapp line .
the first path that verifies reachability ofdestact fromsrcact is returned as output lines .
if no path is found or could be verified null is returned line indicating that wnis not reachable.
finally it is worth mentioning that in addition to verifying the reachability of each path function validate updates uitg by removing invalid paths i.e.
unreachable paths updates the widget map by adding new gui widgets that are encountered at runtime i.e.
those that are loaded dynamically and determines the correct screen for asserting negative oracles details in section vi c .
c. actions for the transferred gui and oracle events once a widget match wnis found algorithm determines the proper action anfor it to successfully transfer w prime i ai line .
based on the type of event i.e.
gui or oracle event algorithm identifies anas follows gui event.
even when the type of matched gui widgets insrcapp andtargetapp are the same their corresponding action might be different.
for example removing an item in a to do list app can be performed by a swipe while the same task in another to do list app might be performed by a long click.
to overcome this challenge c raft droid considers a series of possible actions for wnand finds the one that properly works on wnintargetapp .
to that end it first analyzes the source code of targetapp to find a specific event listener such as onswiped orsetonlongclicklistener registered for the matched widget wn and returns anas the action corresponding to such an event listener.
if no specific action can be identified it reuses the same action in srcapp i.e.
assignan ai.
oracle event.
for oracle events w prime i ai insrcapp where aiis an assertion c raft droid generates anfortargetapp based on whether aiis widget specific e.g.
existence check of a widget or widget irrelevant e.g.
existence check of text.
table i lists the types of oracle events supported by the current version of c raft droid .
for widget specific assertions test generation modifies the assertion so that it matches the target widget wn.
for example when aichecks if the resource id ofw prime imatches a specific value the generated an authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
t able i types of oracle supported by c raft droid .
w prime i ai the source oracle event.
wn an the transferred target oracle event.
ai an widget specific?
assertequal va l ue i attr w prime i assertequal va l ue n attr wn y elementp resense w prime i elementp resense wn y elementinvisible w prime i elementinvisible wn y textp resense string textp resense string n textinvisible string textinvisible string n should also check if the resource id ofwnmatches a specific value first row in table i .
on the other hand if aiis widgetirrelevant it can be directly transferred to targetapp .
transferring negative oracle events e.g.
nonexistence of text is challenging as they can make the transferred test pass regardless of the successful transfer of tests.
for example consider testing the functionality of removing a task from todo list.
to ensure that an item has been successfully deleted the oracle could be a negative assertion of textinvisible to check non existence of item s text.
suppose that we have a source app that removes a task without confirmation while target app requires one additional step to get confirmation of removal from user before removing the task.
an unsuccessful transfer of test that does not consider user confirmation in target app can still pass since the negative assertion will be checked at the confirmation step where the text of item is not visible yet item is not deleted.
thereby the main challenge of transferring negative oracles is to identify the correct screen for them to be executed.
a heuristic that allowed us to overcome this challenges is as follows a negative oracle is likely to be asserted on the proper screen when its negation i.e.
positive oracle is also asserted on that same screen albeit with different content displayed on the screen.
to find the correct screen for a negative oracle c raft droid uses anchor widget an actionable widget that appears in the screen where both a negative oracle and negation of the negative oracle i.e.
positive oracle should be asserted.
the anchor widget serves as a reference to the correct screen.
to identify an anchor widget c raft droid first negates the assertion of negative oracle and then searches for a screen where that assertion can be verified.
any actionable widget in that screen can be considered as the anchor widget.
to that end c raft droid analyzes the source test t prime i before transfer and determines the negate of negative oracle if one exists.
during test transfer it examines the negated assertion on all screens and selects an actionable widget in a screen that the negated assertion passes as an anchor widget1.
in the example of to do list apps c raft droid negates the negative oracle of text non existence to existence i.e.
checks if the text of an item exists in the current screen.
the anchor widget in this example could be an add widget that is used to add items to a list.
this is because existence of the text of a to do item should be checked when that item is being added.
thereby a widget for adding always exists in the screen that list items exist.
later for transfer of oracle event 1craft droid uses anchor widget instead of activity names since activity might have multiple fragments.
thereby just getting back to the activity does not guarantee the screen is correct.craft droid leverages uitg to first navigate back to the screen where the add exists and then transfers the oracle.
d. termination criteria algorithm iteratively improves the quality of test transfer through updating uitg and the widget map .
it terminates once the fitness of a transferred test cannot be improved any further or a timeout value is reached.
the fitness of a transferred test is the average of similarity values section vi a computed for its corresponding events.
vii.
e v alua tion we investigate the following research questions in our experimental evaluation of c raft droid rq1.
how effective is c raft droid in terms of the number of successful transfers compared to total attempted transfers?
what are the precision and recall for attempted gui and oracle transfers?
rq2.
what are the main reasons yielding transfer failure?
rq3.
how efficient is c raft droid in terms of the running time to transfer tests from one app to another?
rq4.
what are the factors impacting the efficiency of c raft droid ?
a. experimental setup we implemented c raft droid with python and java for test cases written using appium which is an open source and cross platform testing framework.
existing test cases for the subject apps are written using appium s python client and the augmented generated test cases are stored in json format.
the model extraction component is built on top of soot a static analysis framework for java .
for our experiments we used a nexus 5x emulators running android .
api installed on a windows laptop with .
ghz intel core i7 cpu and gb ram.
subject apps.
we evaluated the proposed technique using both open source and commercial android apps.
c raft droid is able to transfer tests for similar functionalities implemented differently on separate apps.
thereby we performed test transfers among apps within the same category and for each category identified main functionalities to be tested.
to that end we selected five categories that have large number of apps on google play namely browser to do list shopping mail client and tip calculator .
these five categories are often used in prior research that either studied common functionalities across mobile web apps or proposed android gui testing solutions .
table ii shows the list of subjects and their categories.
for each category we identified two main functionalities and the corresponding test steps.
the test steps for each authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
t able ii subject apps.
cate gory app version source a1 bro wsera11 lightning .
.
f droid a12 bro wser for android .
google play a13 pri vacy browser .
f droid a14 foss browser .
f droid a15 firefox focus .
google play a2 t o do lista21 minimal .
f droid a22 clear list .
.
f droid a23 t o do list .
f droid a24 simply do .
.
f droid a25 shopping list .
.
f droid a3 shoppinga31 geek .
.
google play a32 w ish .
.
google play a33 rainbo w shops .
.
google play a34 etsy .
.
google play a35 y elp .
.
google play a4 mail clienta41 k .
google play a42 email mail box fast mail .
.
google play a43 mail.ru .
.
google play a44 mymail .
.
google play a45 email app for any mail .
.
google play a5 t ip calculatora51 t ip calculator .
google play a52 t ip calc .
google play a53 simple tip calculator .
google play a54 t ip calculator plus .
google play a55 free tip calculator .
.
.
google play t able iii test cases for the proposed functionalities.
functionality test casesavg total eventsavg oracle events b11 access website by url .
b12 back button .
b21 add task b22 remo ve task .
b31 re gistration .
b32 login with valid credentials b41 search email by keywords b42 send email with valid data b51 calculate total bill with tip .
b52 split bill .
total .
.
functionality which are listed in table iv include at least one oracle step.
the oracle steps are implemented as assertion and wait until statements.
test cases.
to construct tests suites we first collected tests for each subject app2 if there were any and then augmented the test suites with the test cases corresponding to the steps described in table iv.
the number of events for tests among different categories varies from to with an average of .
events including .
oracle events3.
attempted transfers.
for each test case validating a functionality of an app c raft droid transfers the test case to the other four apps under the same category.
thereby the number of attempted transfers for each category are test cases transfers making the total number of attempted transfers for evaluating c raft droid to be .
after each transfer we manually examined the test and its execution to identify false positive false negative and true positive cases as follows false positive occurs when the target widget of manual transfer is different from wnidentified by craft droid false negative occurs when c raft droid fails to find a target widget while manual transfer can and true 2test suites for geek wish and etsy apps are from 3the number of actual gui and oracle events in the test cases may be more than the number of steps shown in table iv since table iv only provides general instructions for testing the functionalitiespositive occurs when the target widget from manual transfer matches wnidentified by c raft droid .
based on these metrics we measured the precision as the number of generated target events that are correct.
additionally recall measures how many of the source events are correctly transferred.
our experimental data is publicly available .
b. rq1 effectiveness table v demonstrates the effectiveness of c raft droid in terms of successful transfers for each functionality listed in table iv.
these results demonstrate that on average .
of the attempted transfers by c raft droid are successful with an overall precision and recall considering all the transferred gui and oracle events.
thereby c raft droid is substantially effective in identifying correct gui widgets and successfully transferring tests across mobile apps.
the results shown in table v also confirm that finding a match for all the widgets in source test is not necessary to successfully transfer a test.
as an instance for such cases consider the functionality b11 where its corresponding precision for transferring gui events is while it successfully transfers all tests success rate .
that is transfer of events from source app to target app in b11 has been accompanied by false positives i.e.
the target widget is identified incorrectly.
however these false positives are not harmful since different apps might implement common functionalities in different ways.
for example while one app may require the user to confirm the provided password during registration and before an account is created this confirmation may not be required in another app thereby can be skipped.
while false positive may be acceptable false negative is not as it prevents the examination of the desired functionality.
in other words high recall is more important than high precision in test transfer as false negatives typically have more adverse affect compared to false positives.
c raft droid s high recall of for transferring gui and oracle events makes it suitable for test transfer.
another important observation from the results in table v is that the success rate varies significantly among different categories of apps ranging from success rate for the apps under browser and mail client categories to for shopping apps.
even within the same category the success rate varies for different functionalities.
in the next research question we investigate the attributes that impact the success rate of test transfer.
c. rq2 factors impacting effectiveness to identify the factors that impact effectiveness of a test transfer we manually investigated all of the attempted transfers including both successful and failed ones.
we identified the following reasons for transfer failure length of test.
intuitively transfer of a long test is more challenging compared to a shorter one since more gui and oracle events should be transferred.
thereby more false positives and false negatives might be generated.
to identify how the length of tests impact effectiveness of a test transfer we calculated the pearson correlation coefficient between the average length of tests i.e.
number of total events and the effectiveness metrics in our experiments.
table vi represents authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
t able iv identified main functionalities for subject apps.
category functionality test steps a1 browserb11 access website by url1.
locate the address search bar .
input a valid url and press enter .
specific content about the url should appear b12 back button1.
locate the address search bar .
input valid url1 and press enter .
specific content about url1 should appear .
input valid url2 and press enter .
specific content about url2 should appear .
click the back button .
specific content about url1 should appear a2 to do listb21 add task1.
click the add task button .
fill the task title .
click the add confirm button .
the task title should appear in the task list b22 remove task1.
add a new task .
click long click swipe the task in the task list to remove the task .
click the confirm button if exists .
the task should not appear in the task list a3 shoppingb31 registration1.
click the register signup button .
fill out necessary personal data .
click the submit signup button to confirm registration .
personal data should appear in the profile page b32 login with valid credentials1.
click the login signin button .
fill out valid credentials .
click the submit signin button to login .
personal data should appear in the profile page a4 mail clientb41 search email by keywords1.
start the inbox activity .
click the search button .
input keywords for search and press enter .
specific email related to the keywords should appear b42 send email with valid data1.
start the inbox activitiy .
click compose button .
input an unique id for the subject .
input a valid email address for the recipient .
click send button .
the unique id should appear in the inbox a5 tip calculatorb51 calculate total bill with tip1.
start the tip calculation activity .
input bill amount and tip percentange .
total amount of bill should appear based on the values in step b52 split bill1.
start the tip calculation activity .
input bill amount and tip percentage .
input number of people .
total amount of bill per person should appear based on the values in step and t able v effectiveness and efficiency evaluation of c raft droid .
functionalitygui event oracle event successful transferavg.
transfer time sec precision recall precision recall b11 b12 b21 b22 b31 b32 b41 b42 b51 b52 total .
t able vi pearson correlation coefficient between average test length and effectiveness.
gui event oracle event successful transfer precision recall precision recall avg.
test length .
.
.
.
.
the computed correlation coefficients.
these results indicate a strong and negative correlation between the length of tests and success of a test transfer.
complexity of app.
complexity of subject apps in terms oftheir interface and functionality also impacts the effectiveness of c raft droid .
some categories of apps have standard or de facto design guidelines such as arrangement of gui widgets to follow which makes the transfer of test cases across such apps easier.
for example the design guideline for browser apps is to have a simple main screen that only contains a search bar and few actionable gui widgets.
this relatively simple design for the browser apps makes the transfer of the gui events across them easier since there are fewer candidate widgets on a screen to be analyzed for proper mapping.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
on the other hand apps without uniform design guidelines such as shopping apps are flexible to determine the number of functionalities contained on a screen and the number of required steps for a functionality.
this flexibility makes the search for finding correct matches more complicated.
as demonstrated by the results in table v while c raft droid successfully transfers all the tests under the browser category the success rate of transfer among shopping apps is not as high as other categories.
d. rq3 efficiency table v shows the average running time of c raft droid to transfer a test from one app to another when executed sequentially.
on average a test transfer takes less than .
hours ranging from minutes to .
hours among different functionalities.
performance evaluation of different components of c raft droid shows that validating reachability of a candidate widget is the most time consuming part of test transfer.
that is the function getleadingevents in algorithm dominates the execution time as it frequently restarts the target app to validate the potential paths for a candidate widget.
fortunately this function can be easily parallelized.
multiple devices or emulators can be used to drastically cut down the execution time by performing the reachability check in parallel.
for example in our experiments c raft droid verifies .
paths on average to transfer a source event.
as a result by using emulators we can speed up the transfer approximately times to reduce the average running time to minutes.
we believe this is a reasonable amount of time to produce a feature based test consisting of both inputs and oracles.
e. rq4 factors impacting efficiency by analyzing efficiency of test transfer among different apps and functionalities we identified three factors that impact the efficiency of c raft droid length of tests transfer success and size of target app.
intuitively the longer is a test it takes more time to transfer its events.
in fact the average test length and average transfer time are strongly and positively correlated as the pearson correlation coefficient between them is .
in our experiments.
in addition we observed that unsuccessful transfers take more time compared to successful ones.
that is an unsuccessful transfer often needs to examine and validate more candidate widgets during transfer.
in our experiments the average running time of the successful transfers is seconds while this number for the remaining 51unsuccessful transfers is seconds meaning unsuccessful transfers are3xslower.
finally the size of uitg is positively correlated to the size of app with correlation coefficient .
.
since craft droid heavily relies on the uitg to search and validate the correct widget it requires more time to transfer a test for a larger target app.
f .
threats to v alidity the major external threat to validity of our results is the generalization to other mobile apps and test cases.
to mitigate this threat we collected commercial and open source apps from google play and f droid under various categories.
themain internal threat to validity of the proposed approach is the possible mistakes involved in our implementation and experiments.
we manually inspected all of our results to increase our confidence in their correctness.
the experimental data is also publicly available for external inspection.
in terms of the construct validity c raft droid assumes that the source test is transferable i.e.
the source and the target apps share similar functionalities which is not always true.
however c raft droid is not designed to generate test cases for every possible or app specific features.
it aims at reducing the manual effort of implementing tests for common or popular functionalities across apps.
our evaluation shows that this assumption does hold for apps under different categories.
viii.
r ela ted work common functionalities across gui based apps several prior works have discussed common functionalities across desktop software application agnostic features across mobile apps and common gui patterns used in web app testing .
augusto studies common functionalities such as authentication and saving a file in desktop software and proposes an automated technique to generate gui tests for them with pre defined gui structures and formal pre post conditions.
zaeem et al.
introduce several applicationagnostic ui interactions which can serve as oracles for mobile testing.
ermuth and pradel propose that sequences of lowlevel ui events which correspond to high level logical steps can be inferred from test traces and to be further used for test generation.
moreira et al.
develop a domain specific language to assist modeling and testing of ui patterns.
similar to the above work c raft droid exploits the existence of commonality across gui based apps.
however unlike them craft droid aims to generate feature based tests for an app from existing tests for apps within the same category similar features .
semantic mapping of gui widgets in recent years researchers have proposed approaches for transferring or reusing tests on different platforms.
rau et al.
proposed a technique for mapping of gui widgets among web applications .
behrang and orso proposed an approach to transfer test cases by mapping the gui widgets to support assessment of mobile app coding assignments.
hu et al.
presented a framework that leverages machine learning to synthesize reusable ui tests for mobile apps.
qin et al.
recently proposed to migrate gui events for the different instances of the same app running on different operating systems.
unlike all prior work c raft droid is able to transfer test oracles.
in fact and only discuss gui element mapping and focuses on generating gui tests from high level manually written test cases.
additionally c raft droid utilizes an unsupervised and data driven model i.e.
word2v ec to compute the similarity score between gui widgets hence it is fully automated.
on the other hand uses a lexical database such as wordnet and adopts supervised machine learning both of which require human effort in database maintenance or data annotation for this purpose.
finally craft droid leverages both static and dynamic analyses to transfer tests while only adopts dynamic analysis.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
concurrent to our development of c raft droid behrang and orso developed a pptest migra tor to migrate gui tests including oracles for mobile apps with similar functionality.
while at a high level both works adopt similar techniques e.g.
using word2v ec models and combination of static and dynamic analyses c raft droid is different from apptest migra tor in terms of several algorithmic details such as the ways it leverages the statically extracted model of app and computes similarity between gui widgets.
since both approaches have similar goals an empirical comparison between them in future may provide more insights into their relative strengths and weaknesses.
ix.
c onclusion and future work in this paper we presented c raft droid a framework for transferring tests across mobile apps through semantic mapping of actionable gui widgets.
we evaluated c raft droid using 25real world apps from 5categories.
our experimental results show that of the attempted transfers are successful with precision and recall for the transferred gui and oracle events.
we also discussed the factors impacting the effectiveness and efficiency of c raft droid which can be used as a guideline by researchers to improve test transfer techniques.
for the future work we are planning to conduct empirical study with more apps and incorporate techniques such as crowd sourcing to improve the effectiveness of c raft droid .
we share the vision of behrang and orso toward the establishment of a centralized repository similar to app store but for test cases.
this test store will be able to generate feature based test cases for newly developed apps.
the knowledge mined from existing tests and apps which we use for test transfer can also have applications beyond testing such as suggesting missing features and improving gui layouts flows for new apps.
acknowledgment this work was supported in part by awards ccf and cns from the national science foundation.