ctras crowdsourced test report aggregation and summarization rui hao1 yang feng2 james a. jones2 y uying li1 zhenyu chen1 1state key laboratory for novel software technology nanjing university nanjing china 2department of informatics university of california irvine usa ruihao smail.nju.edu.cn yang.feng jajones uci.edu yuyingli smail.nju.edu.cn zychen nju.edu.cn abstract crowdsourced testing has been widely adopted to improve the quality of various software products.
crowdsourced workers typically perform testing tasks and report their experiences through test reports.
while the crowdsourced test reports provide feedbacks from real usage scenarios inspecting such a large number of reports becomes a time consuming yet inevitable task.
to improve the efficiency of this task existing widely used issue tracking systems such as jira bugzilla and mantis have provided keyword search based methods to assist users in identifying duplicate test reports.
however on mobile devices such as mobile phones where the crowdsourced test reports often contain insufficient text descriptions but instead rich screenshots these text analysis based methods become less effective because the data has fundamentally changed.
in this paper instead of focusing on only detecting duplicates based on textual descriptions we present ctras a novel approach to leveraging duplicates to enrich the content of bug descriptions and improve the efficiency of inspecting these reports.
ctras is capable of automatically aggregating duplicates based on both textual information and screenshots and further summarizes the duplicate test reports into a comprehensive and comprehensible report.
to validate ctras we conducted quantitative studies using more than test reports collected from industrial crowdsourced projects.
the experimental results reveal that ctras can r each an accuracy of .
on average regarding automatically detecting duplicate reports and it outperforms the classic max coverage based and mmr summarization methods under jensen shannon divergence metric.
moreover we conducted a task based user study with participants whose result indicates that ctras can save nearly time cost on average without loss of correctness.
i. i ntroduction crowdsourced testing has become a popular mobile application testing method because it can provide feedback from diverse settings such as devices location information network environments user behaviors and operating systems.
even though crowdsourced testing has been widely adopted in many commercial testing platforms such as utest testin baidu crowd test alibaba crowd test mooctest and testio and is effective for improving the quality of various software products inspecting and triaging the overwhelming number of test reports are a time consuming yet inevitable task in practice.
in the past decades to improve the efficiency of processing test reports software engineering researchers have presented many techniques to detect and summarize duplicates.
yang feng james a. jones and zhenyu chen are corresponding authors.duplicate detection techniques aim at assisting developers in identifying duplicate submissions from the repository .
prior research primarily focuses on two kinds of information text descriptions and execution traces to reach this goal.
further conventional and widely used issue tracking systems such as bugzilla jira and mantis have provided keyword search based features for reporters to query similar reports to reduce duplicates .
also rastkar et al.presented a test report summarization technique to assist developers to identify key sentences from test reports to reduce inspection efforts .
however the settings and inherent features of mobile crowdsourced testing bring challenges into applying these techniques.
zhang et al.
found that mobile test reports often contain insufficient text descriptions and rich screenshots in comparison with desktop software .
under this situation while text analysis based methods become less effective because of short and inaccurate text descriptions automatically identifying information from screenshots becomes critical for developers to understand reports.
further while all of these techniques are built on the assumption that duplicate reports are harmful to software maintenance and aim at filtering out this information zimmermann et al.
and bettenburg et al.
empirically found that duplicate reports are helpful for report comprehension and debugging .
thus in this paper we propose an approach named ctras which is capable of leveraging the information of duplicate test reports to assist developers in comprehending test reports.
different from the conventional bug test reportprocessing techniques instead of discouraging developers from submitting duplicates and filtering them out our technique aims at leveraging the additional information provided by them and summarizing both the textual and image information from the grouped duplicates to a comprehensive and comprehensible report.
ctras automatically detects and aggregates the duplicate reports by measuring the similarity of both the text description and screenshots.
based on the aggregation results for each duplicate report cluster it identifies the most informative report which we call the master report and summarizes the supplementary text and screenshots.
these supplementaries are sorted by their weight and ctras generates the final ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
summarized report by combining the master report and supplementaries to provide the developer with a comprehensible overview of each test report duplicate group.
to validate ctras we conducted both quantitative and qualitative experiments using more than test reports collected from mobile applications.
the results show that ctras can accurately detect and aggregate duplicate reports by utilizing both text description and screenshot information.
it improves the duplicate report detection and aggregation accuracy by and by when compared to only text based screenshot based methods.
meanwhile based on our evaluation result ctras generates more descriptive summaries when compared to classic mcb and mmr summarization methods.
furthermore we conducted a task based evaluation involving participants whose result indicates that ctras can save time costs on average without losing correctness.
this paper makes the following contributions we present ctras a mobile crowdsourced test report processing technique to assist developers.
to the best of our knowledge ctras is the first work that aims at aggregating multiple duplicate test reports into an enriched summarized report.
the implementation of ctras is publicly available1.
based on ctras we present a method to detect duplicate test reports that leverages image information.
the experiment result shows that the image information can improve the performance of both duplicate detection and aggregation.
we conducted comprehensive studies on over test reports from industrial crowdsourced projects to validate ctras and found that ctras performed well in terms of clustering accuracy summarization efficiency of use and usability.
ii.
b ackground a. mobile crowdsourced test reports in contrast to conventional bug repositories of desktop software applications bug report repositories of mobile crowdsourced testing often have higher duplicate ratios briefer text descriptions and richer screenshots .
high duplicate ratio.
crowdsourced testing is popular in mobile application testing because it enables developers to evaluate the performance of their software products under real usage scenarios.
however in practice crowd workers are often required to finish crowdsourcing tasks in a given short time and the number of completed tasks influences the rewards for the crowd workers.
as such crowd workers are less apt to actively filter out duplicates and they are incentivized to submit as many reports as possible.
these factors contribute to crowdsourced testing to contain a higher duplicate ratio than conventional testing.
short t ext and rich screenshots.
in addition on almost all mobile devices images have played a crucial role in sharing and exchanging information.
end users can easily take a screenshot and the crowd workers tend to describe bugs with a direct screenshot and short descriptions rather than verbose and complex text descriptions largely due to the ease of taking screenshots and the relative difficulty in typing longer descriptions on mobile virtual keyboards .
on mobile platforms screenshots usually capture well defined application views and do not suffer as many of the difficulties of desktopapplication screenshots such as varying resolutions scaling occlusion and window sizes.
in this context the above issues are ameliorated and the main problems are the prompting of the error message dialogs shifting of gui elements or the fact that some elements are not displayed at all.
b. motivation the aforementioned features of mobile crowdsourced test reports i.e.
high duplicate ratio short text descriptions and rich screenshots motivate us to propose an approach to leveraging both the text and image information from duplicate reports to enhance developers understanding of bugs.
moreover a common and conventional belief in softwaredevelopment practice is that the reporting of duplicate test reports is a bad practice and therefore considered harmful.
the long and frequent arguments against duplicates are that they strain issue tracking systems and waste efforts of software maintainers.
thus based on this argument prior researchers have proposed many techniques to assist developers in avoiding wasting time on duplicates.
however there are also arguments to the contrary.
zimmerman et al.
claim that the missing information such as reproduction steps and environment settings is one of the most serious problems of test reports of open source projects.
they find that developers often need to spend extra time to interact with reporters to identify the missing information and gain enough understanding of the bug.
bettenburg et al.
present empirical evidence to show that duplicates provide additional information for describing bugs and this information is helpful for fault localization and fixing.
these findings fit the situation of mobile crowdsourced testing which has been widely adopted in the quality assurance of modern mobile applications.
iii.
g oals and preliminary definitions in this section we highlight our main goals and provide definitions and notations that will be used subsequently.
our overall goal is to aggregate duplicate test reports and provide a comprehensible overview of the group.
specifically for mobile software we seek to provide a technique that is robust and effective in clustering test reports that may contain short text descriptions but may include screenshots that exhibit failure symptoms.
for a software project with submitted crowdsourced test reportsr r r si ti i ...n in which sdenotes the screenshots i.e.
images containing the views that may capture symptoms of the bug being reported and tdenotes the text describing the buggy behavior.
note that each the text description consists of multiple sentences thus we have ti tij j ...m in which tijdenotes the jth sentence authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
!
fig.
process flow of ctras in the test report ri.
similarly we employ sijto denote the jth screenshot in the test report ri.
summary.
the goal of ctras is to cluster duplicate reports into groups each group of duplicate reports is a subset of r and then to generate a summary sfor each group in g. in our formulation we define a summary for a group of duplicate test reports as a master report and a list of supplementaries .
master report.
in contrast to traditional testing crowd workers are often inexperienced and unfamiliar with software testing.
they may describe a bug from different aspects and in various ways.
this fact leads the quality writing style and content of these reports to varying widely .
hence we seek to find one report that provides a relatively comprehensive description of the issue for the group of duplicates.
a master report is defined as an individual test report r g that is identified as providing the most information.
supplementaries.
even though presenting the most informative one or its summary is helpful for developers to build a high level understanding of the bug the supplementary information such as different software and hardware settings diverse inputs and various triggers is critical for developers to gain enough knowledge of debugging and fixing.
these supplementary details from other test reports in the duplicate group can provide additional insights to developers in understanding the varying conditions the lead to the issue.
hence we seek to identify the useful information from the redundant information and summarize them into the comprehensible supplementaries.
a supplementary is defined as the representative information item i.e.
either text or screenshot which is taken from g r .
textual supplementaries are small snippets of text that present information that is not included in the master report and thus can provide more information and greater context for developers during debugging and triaging.
likewise image supplementaries are screenshots that differ from those included in the master report.iv .
a pproach we present the process flow of ctras in the fig.
.
ctras is composed of two main components aggregator and summarizer .
the aggregator is designed for computing the distance between test reports and aggregating the duplicates into group g. we use hierarchical clustering to accomplish this task.
to assist developers understanding the content of the duplicate groups quickly the summarizer first picks a single test report that best exemplifies the group of aggregated test reports then it supplements the information by gradually extracting supplementary information which contains topics or features uncovered by the master report .
a. aggregator the aggregator first computes the distance of test reports and outputs the distance matrix.
in the distance computation we adopt the hybrid strategy proposed by feng et al.
to measure the distance between test reports by combining both the image similarity and text similarity.
for the textual descriptions natural language process nlp techniques including tokenization synonym identification and keyword vector building are applied to calculate the text similarity.
for the screenshots the spatial pyramid matching spm technique is adopted to extract the image features and calculate screenshot similarity.
the hybrid distance matrix among all reports is built upon the weighted harmonic mean of these two similarities.
applying the spm algorithm to extract features from one screenshot typically only costs seconds and computing the distance between two screenshots costs milliseconds.
based on the distance matrix the aggregator is capable of measuring the similarity between test reports and further grouping the duplicates.
considering that in practice the number of groups cannot be predicted we adopt hierarchical agglomerative clustering hac which can determine the number of groups based on a threshold distance value to group the duplicates .
b. summarizer the summarizer is the core component of ctras.
for each duplicate test report group g it performs the following three steps to generate a summary to assist developers in forming a comprehensive understanding over all reports in g identifying the master report r generating supplementaries and forming and generating final summaries.
to ease explanation we take a real group containing six test reports in our empirical study as an example and we illustrate each substep in fig.
.
we list the six similar reports in the exemplary group in fig.
a. note that all of these reports describe the same bug that involves a problem logging into the app via a third party tool.
each report of this group consists of its basic attributes such as report names creation time as well as its textual description and several screenshots.
master report identification to help the developers concisely understand the topic of the test reports within the group we identify the master report r in the first step.
we authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
!
!
fig.
a running example of ctras abstract each test report group into a graph within which each node represents an individual report.
the weight of edges between two nodes indicates the similarity between these two reports.
thus we can apply the pagerank algorithm which can compute a numerical rank score and measure the importance for each node within a weighted graph on each test report group.
ctras i dentifies the test report having the greatest page rank score as the master report for the group.
example the graph representing the exemplary group is illustrated in fig b and the table shows the hybrid similarity between each pair of these reports.
we compute the weight of each node by applying the pagerank algorithm.
we find that report has the highest weight and thus it is selected as the master report labeled with r of these six reports.
through reading the contents of report developers can reach a high level understanding of the whole test report group i.e.
there is a bug that users can t login the app through qq social login service as it fails the authentication and is regarded as non official software.
supplementary generation even though we have identified the master report r and helped the developer get the most informative report within the group g describing the same bug from other perspectives and providing supplementary materials is critical for developers in fixing the bug properly.
thus we further analyze the other reports and identify the content that is shared among them as the supplementary points.
in this procedure we perform two substeps for identifying the supplementaries identifying candidate items from g r grouping the candidate items to form a supplementary .
candidate item identification.
because the sentence is considered to be the immediate integral unit in linguistic theory a number of prior research efforts that aimed at analyzing test bug reports to assist developers in understanding the bug descriptions have selected the sentence as the basic unit .
we thus also do so accordingly and measure the similarity between two sentences by computing the jaccard distance between their keyword vectors.
jaccard distance isa useful metric to compare the similarity and diversity of two sets which is shown in equation .
in this equation tand t primedenote the keyword sets of two sentences t t prime denotes the number of words in the intersection of both sets and t t prime denotes the number of words in the union of both sets.
regarding the screenshot as we discussed in section iv a we adopt feng et al.
s method and corresponding parameters to identify different screenshots .
given a test report group g and its master report r to generate supplementaries for r the first step is to identify candidate items i.e.
sentences and screenshots which are not included in r from g r .
from the set g r we extract all singleton items i.e.
individual sentences and screenshots to get the set of sentences t tij and the set of screenshots s sij of g r .
similarly we can get the set of sentences and set of screenshots from r and we denote them t t j and s s j respectively.
for each sentence whose keyword set is t int if not existing any element in s having thej t t is smaller than the predefined threshold value we consider it is a candidate sentence for r .
similarly given s ands we can identify the candidate screenshot for r .
j t t prime t t prime t t prime example as shown in fig.
c we label the text items with rectangles and the screenshot items with diamonds.
ctras identified eight candidate sentences and four candidate screenshots from the exemplary group.
for example t0 2is a candidate sentence as it supplements the expected result information i.e.
errorcode for r i.e.
report t5 0expresses a special case that without qq installed and s5 0is a candidate screenshot for this case.
candidate item refinement.
through candidate item identification all candidate items which are not similar to any item in the master report are identified.
however some of these items may be too brief to understand or they are similar with each other.
therefore we refine them into concise and representative supplementaries .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the refinement process of ctras consists of three substeps step clusters similar candidate items step provides an additional clustering of the candidate clusters and step weights the candidates within the clusters to identify the most representative among them.
in step we group similar candidate items to remove redundancy and improve the conciseness of the supplementary.
in this step we apply hierarchical agglomerative clustering on the set of candidate sentences to form some candidate sentence clusters.
similarly we can apply the same strategy on the candidate screenshot set and get the candidate screenshot clusters using approach spatial pyramid matching described in section iv a. moreover we record the origin information for each candidate so that we can map candidates to reports and vice versa.
this information is not only useful for further aggregating candidate sentence clusters in the next sub step but also helpful for developers to track back original reports in practice.
in step we further group the candidate clusters from step .
the purpose of this step is to restore context.
this extra level of grouping is useful particularly for singleton clusters clusters containing only a single candidate item due to the fact it contains distinct information .
we merge the clusters based on the origin information clusters that contain candidates originating from much of the same test reports are clustered.
we define the distance between two candidate item clusters tandsas equation in which each cluster can be either candidate sentence cluster or candidate screen cluster and the t represents the set of test reports that contributed to the candidate item cluster t. based on the equation candidate item clusters are aggregated into supplementaries when the distance between them is smaller than the threshold value .
raw supplementaries should not be presented to the end user because they contain too much redundancy.
d t s t s t s in step we identify the most representative candidate items in each supplementary cluster.
based on our definition of sentence similarity and screenshot similarity we abstract all sentences and screenshots within a supplementary into a weighted graph respectively and employ the similarity value as the weight of edges.
given these two weighted graphs we apply the pagerank algorithm and obtain the pagerank score for each of the node i.e.
sentences and screenshots.
these weights will be used within the next phase of content extraction to highlight the most relevant and representative information for each supplementary.
example fig.
d displays the refinement result of all candidate items three candidate sentences i.e.
t0 t2 1andt4 are grouped together because they contain error code candidate clusters t5 t5 and s5 are grouped because they belong to report .
particularly the size of supplementary is as its content comes from three reports.
content extractor based on master report and supplementaries we can further refine them and generate a concise final summary.in many textual summarization techniques e.g.
the compression ratio kcontrols the conciseness of the final summary.
in previous works compression ratio is computed as the ratio of the number of selected keywords to the number of total keyword within the original document.
however because ctras aims at generating summary over both text and screenshots we extend the classic definition.
for the text we define the compression ratio as the ratio of the number of unique selected word to the total number of unique word within the supplementary.
similarly for the screenshot the compression ratio is the ratio of the number of selected screenshots to the total number of screenshots within the supplementary.
we weight text and screenshot equally and thus utilize the mean value of these two compression ratios as the compression ratio for the whole summary.
to generate the final summary we first include the master report and then list all supplementaries sorted by the number of test reports that contributed to them in descending order.
for each supplementary we iteratively select the sentence or screenshot based on the weights computed in step of the candidate refinement phase and include them into the summary until reaching the summary compression ratio set by the user.
example the detail summarizing process is shown in fig.
e. we take the supplementary as sample.
it contains keywords and screenshots.
at the beginning of summarization the sentence t0 2is selected as it has the highest pagerank score then the summary contains keywords and the compression ratio has reached the limit i.e.
the compression ratio is .
the summarization process ends.
c. implementation we have implemented a web based test report management tool which not only provides test report summarization but also extends a number of classic test report management functions such as automatic duplicate detection a project report dashboard keyword searching bug triaging and best fixer recommendation.
we present the screenshot of its summary visualization page in fig.
f which shows the visualization result of the final summary from the exemplary duplicate group.
at the top of this page we present attribute information about the aggregate report such as the set of all crowd testers who submitted reports in this set bug category severity .
and then we show information from the master report which aims at assisting users to get an overview understanding of all duplicates.
below the master report pane topics of supplementary are listed.
we highlight the representative sentences phrases and screenshots of each supplementary which enables end users to understand the main topic of this supplementary at a glance.
further end users can view the details including all sentences screenshots and original reports contributed the supplementary by clicking these supplementary topics.
using this tool we conducted comprehensive studies to validate our approach described in the next section.
v. e xperiment to assess the performance of ctras in achieving its goals i.e.
to assist developers in processing test reports authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
providing comprehensive and comprehensible summaries and saving efforts we conduct mixed evaluations to answer the following three research questions rq1.
effectiveness of duplicate aggregator.
can the aggregator accurately group duplicates?
to what extent do the screenshots improve the accuracy of detecting and aggregating duplicate reports?
rq2.
effectiveness of summarizer.
to what extent can the summarizer refine the informative and non redundant content from the duplicates?
rq3.
effectiveness of ctras.
can ctras help developers save time costs in inspecting mobile crowdsourced test reports without loss of accuracy?
both rq1 and rq2 are designed to evaluate the effectiveness of the aggregator and summarizer components of ctras.
because identifying and aggregating duplicate reports are foundational steps of correctly leveraging the information from test reports rq1 aims at evaluating the accuracy of ctras in detecting duplicates and revealing the effectiveness of employing the screenshot information to aggregate the duplicates.
also identifying the critical complementary and non redundant information from the large volume of information plays an important role in helping developers to understand and fix the bugs.
thus rq2 aims at evaluating the effectiveness of the summarizer regarding the metrics of information theory.
further although rq1 and rq2 present quantitative and theoretical evaluations understanding the practical performance of ctras is critical.
thus we design rq3 which is a task based user study to investigate the efficiency improvement as well as reporting any accuracy loss.
as discussed in section iv several fundamental parameters may influence the performance of ctras.
w e pro vide our parameterizations for all three experiments to assist verifiability and repeatability.
feng et al.
suggested that the should be adjusted based on different tasks and given previous researchers bettenburg et al.
found that the textual information e.g.
description observed and expected behavior reproduction steps is capable of providing more accurate description than screenshots for developers in debugging we set to weight textual descriptions more.
also there are two fundamental parameters of the hac algorithm the linkage type which defines the method of calculating the distance between clusters and the threshold for terminating the clustering.
we choose the single linkage type because it makes the hac to solely focus on the area where the two clusters come closest to each other and ignore distant parts which fits the goal of ctras well.
we set the to .
that is the medium value of the scale of the distance between test reports.
further we apply strict combination strategy by setting .2that we defined in section iv b2 to group candidate item clusters.
in the study of rq1 and rq3 we set the compression ratio k .
which is considered to be a proper value in the paper .table i statistical information of testing applications name v ersion category r s rs d p1 cloudmusic .
.
music p2 game .
games p3 hw health .
.
health p4 hj normandy .
.
education p5 mylistening .
.
education p6 ishopping .
.
shopping p7 jayme .
.
social p8 justforfun .
.
photo p9 kimiss .
.
beauty p10 slife .
.
health p11 tuniu .
.
travel p12 ubook .
.
books total a. dataset description to produce the dataset for our evaluation we utilized the results of the national software testing contest2 which simulated crowdsourced testing of several popular mobile applications across multiple domains including games education social media and so on .
the contestants of the contest were required to test the applications and report bugs in four hours.
they could write descriptions and take screenshots to document their testing procedures and the unexpected behavior of applications.
this contest attracted participants and generated over test reports.
more than professional testers and members of the organizational committee manually labeled and evaluated the quality of these reports.
the detailed information of the dataset is shown in table i in which r denotes the number of reports s denotes the number of screenshots rs denotes the number of reports that contain at least one screenshot and d denotes the number of duplicates.
b. rq1.
effectiveness of duplicate aggregator .
methods while a number of classic duplicate testreport detection methods only focus on the textual description to measure the similarity between reports ctras employs both textual description as well as screenshots to assist detecting duplicates.
thus to answer the rq1 we have the following three methods ctras.
our duplicate detection method which employs both textual information and screenshots.
in this method the distance between two reports is calculated based on the balanced distance equation.
ctras txt.
the duplicate detection method employs only textual information.
in this method the distance is calculated based on only textual distance.
ctras img.
the duplicate detection method employs only screenshot information.
in this method the distance is calculated based on only screenshot distance.
evaluation metrics to measure the performance of these three methods we employ three classic metrics for evaluating clustering homogeneity completeness v measure .
taking the classes set cand clusters set kas reference we define the contingency table a aij i ... n j ... m whereaijdenotes the number of test reports that belongs to bothciandkj.
en.html authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
homogeneity reflects the extent to which each cluster contains only members of a single class.
it can be calculated viah h c k h c where h c k k summationdisplay j c summationdisplay i 1aij n logaij summationtext c k 1akj h c c summationdisplay i summationtext k j 1aij n log summationtext k j 1aij n completeness is a symmetrical criterion of homogeneity which measures the extent to which all members of a given class are assigned to the same cluster.
it can be calculated via c h k c h k where h k c c summationdisplay i k summationdisplay j 1aij n logaij summationtext k k 1aik h k k summationdisplay j summationtext c i 1aij n log summationtext c i 1aij n v measure is the harmonic mean of homogeneity and completeness.
it is widely used as the measure of the distance from a perfect clustering.
in our paper a higher v measure score indicates a better duplicate detection and aggregation result which is calculated by the equation .
v h c h c c. rq2.
effectiveness of summarizer methods to investigate the theoretical effectiveness of the summarizer of ctras we compared its performance with two classic summarization methods max coveragebased mcb and maximal marginal relevance mmr .
ctras.
our summarization method that generates the summarized report under ratio k. mcb.
the max coverage based method is a greedy algorithm.
mcb iteratively selects the test report with maximal coverage score and inserts it into the summarization until kis met.
the original definition of coverage score in paper refers the ratio of the number of selected conceptual units to the total number.
in this experiment we have two kinds of conceptual units i.e.
keywords and screenshots.
thus we define the coverage score as the mean value of the keyword coverage score and screenshot coverage score.
mmr.
the mmr method is a typical method for summarizing multiple topically related documents which employs keywords that have the highest frequency to build a query .
this query is used to select the document from a set based on the maximum marginal relevance strategy which selects the one having the largest distance from the selected set while being relevant to query in each step.
in our implementation we adopt the same idea and construct the query with keyword and screenshot having the highest frequency.
we employ the distance between test reports which we defined in section iv a as the distance measurement to implement the maximummarginal relevance strategy.
note that we define the compression ratio of the final summary as the mean value of the text compression ratio and screenshot compression ratio see section iv b3 this strategy is also applied in this experiment.
evaluation metrics we adopt a fully automatic evaluation method for content selection the jensen shannon divergence js divergence which has been shown to be highly correlated with manual evaluations and sometimes even outperforms standard recall oriented understudy for gisting evaluation rouge scores .
js divergence employs the probability distribution of words to measure the distance between documents.
a good summary is expected to have low divergence with the original document.
in this paper we calculate the js divergence of textual information and screenshots respectively.
the js divergence is represented in equation in which pandqdenote the probability distributions of word gand summary s respectively.
we entirely adopt the recommended parameter settings in i.e.
a p q 2denotes the mean distribution of pandq cdenotes the frequency of keyword nis the sum of frequencies of all keywords b .
v wherevdenotes the text corpus and is assigned to .
to perform a small smoothing.
jssis defined in a similar manner.
jst p q d p a d q a where d p q summationtext pp log2pp pq p c n b afterjstandjssare calculated we utilize their harmonic mean as the measure of these summarization methods.
d. rq3.
effectiveness of ctras although rq2 evaluates the theoretical performance of ctras we also seek to understand its practical performance for real users.
in rastkar et al.
designed a task based user study to investigate whether the generated summaries can help developers in processing test reports.
in their study participants were asked to read a new test report and a list of potential duplicated reports which were presented under their original orsummary format and then determine for each whether it was duplicated with the new test report or not.
considering both rastkar et al.
s work and ours share the same goal we adopt the task based user study to answer the rq3.
however because their work is designed to produce a summary for a single test report while ctras generates a summary for multiple test reports we adjust the duplicate testreport detection task into duplicate test report clusterization tasks in our study.
for our study we utilized a modified version of the webbased ctras tool to assist participants to cluster test reports.
our participants are given a set of test reports see table i and authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
optionally a set of summaries and asked to group duplicate test reports i.e.
test reports describing the same bugs .
our hypothesis is that ctras can help developers reach a multiperspective understanding of the bug and thus identify the duplicates more efficiently without loss of accuracy.
the rationale of this study design is straightforward if the summary generated by ctras failed to pro vide sufficient and correct knowledge for participants to understand the bug it cannot help participants and further improve their efficiency in grouping the duplicates that are describing the same bug.
study setting we recruited second year master students majoring in computer science or software engineering as participants of this study.
all of these participants have at least years programming experience but have no experience using any of these experimental applications.
we select applications i.e.
mylistening p5 ishopping p6 kimiss p9 tuniu p11 ubook p12 as subject programs.
the categories of these applications are diverse and the number of reports varies from to thus we believe these applications are representative.
we randomly divide the participants into groups.
in the study these three groups are provided with different reference materials group a control group the participants of this group are only provided with the original test report.
there are no supportive materials for the participants of this group.
group b ctras the participants of group b are provided with the original test report and the summary that is generated by ctras.
note that for this group the summarizer works on the fully automated aggregator which groups the test reports based on both image and text similarity.
group c golden the participants of group c are provided with the original test report and summaries that are generated by the summarizer of ctras w orking on the ground truth clustering results as manually determined by the professional developers described in section v a .
because the quality of summaries is influenced not only by the summarization algorithm but also by the duplicate aggregation algorithm we set up this group to evaluate the gap between the performance of ctras and the perfect situation.
within each group participants each every subject application software applications is assigned to two participants.
participants are required to manually cluster these original test reports independently without any collaboration.
the modified version of ctras shows summaries without showing any information that reveal test report identities simply showing the summarized sentences and screenshots that describe bugs.
this version of ctras also provides keyword search and keyword filtering.
we employ ctras to generate summaries under the predefined condition of group b and c and then provide these summaries to the participant of corresponding groups as reference material.table ii details of the summarization result in rq3 p5 p6 p9 p11 p12 bcbcbcbc b c summary sentencesmean .
.
.
.
.
.
.
.
.
.
std .
.
.
.
.
.
.
.
.
.
screenshotsmean .
.
.
.
.
.
.
.
.
.
std .
.
.
.
.
.
.
.
.
.
table ii illustrates summarization results.
for each subject application and group b c we show the number of summaries and their mean number of sentences and screenshots.
evaluation metrics we evaluate ctras based on three aspects efficiency accuracy and satisfaction.
efficiency.
we adopt the average completion time for each report as the evaluation metric of efficiency.
accuracy.
using the ground truth data described in section v a we determined the accuracy of the participant s inspection by utilizing v measure metric see definition in section v b2 .
satisfaction.
the satisfaction of summary is measured upon the qualitative feedback from the questionnaire which is shown in table iii.
particularly we present the questionnaire only to participants of group b and c. vi.
r esult analysis in this section we present the experimental results to answer the three research questions.
a. answering rq1 effectiveness of the duplicate aggregator we present the homogeneity h completeness c and v measure v results of ctras and the two baseline techniques in table iv.
on average ctras achieves .
v measure score while these two baseline techniques i.e.
ctras txt and ctras img obtain .
and .
respectively.
further ctras outperforms these two baseline techniques over all subject projects except the slife p10 .
we investigated the content of test reports of subject projects.
we found the application slife is a daily activity tracker which is designed for tracking the health data of users daily activity.
even though its operation is simple the testing procedure which requires a number of activities beyond the regular operations becomes relatively difficult.
given the fact that our test reports come from the contest which requires participants to finish the tasks in a short time hours we speculate that the test reports of slife could only reveal simple bugs and as such and their text descriptions were accurate.
thus ctras txt obtains the highest homogeneity score which results in the relatively higher v measure score in comparison with ctras.
summary the high v measure score indicates that the duplicate aggregator is capable of accurately detecting and aggregating duplicate reports.
in comparison with the classic text only based strategies the screenshot information is able to improve the performance of detecting duplicates in out of subject applications .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii interview questions in rq3 .
on a scale of with being the most positive how would you describe the overall performance of the summary in assisting your clusterization task?
.
does the master report reflect the topic of the summary if yes how does it reflect it?
.
is there additional information in the supplementaries that helped you cluster test reports?
if so describe it.
.
which type of information is more important for you in inspecting reports textual descriptions or screenshots?
why?
table iv evaluation results for the duplicate aggregator rq1 p1p2p3p4p5p6p7p8p9p10p11p12avg h ctras .
.
.
.
.
.
.
.
.
.
.
.
.
txt .
.
.
.
.
.
.
.
.
.
.
.
.
img .
.
.
.
.
.
.
.
.
.
.
.
.
c ctras .
.
.
.
.
.
.
.
.
.
.
.
.
txt .
.
.
.
.
.
.
.
.
.
.
.
.
img .
.
.
.
.
.
.
.
.
.
.
.
.
v ctras .
.
.
.
.
.
.
.
.
.
.
.
.
txt .
.
.
.
.
.
.
.
.
.
.
.
.
img .
.
.
.
.
.
.
.
.
.
.
.
.
b. answering rq2 effectiveness of the summarizer the results of rq2 are shown in fig.
for all subject application with varying compression ratios.
we note that regardless of the compression ratio ctras generally outperforms mcb and mmr methods in all projects except justforfun.
through further investigation we found that justforfun is an image editing and sharing application.
thus the screenshots are largely composed of user content i.e.
their photos instead of more standardized activity views so most screenshots have a large distance from each other which causes them to be categorized as independent supplementaries.
this causes a decrease in js divergence.
in addition as the summarization ratio increases the score of the js divergence decreases except for the ctras result on project game which is caused by the fact that there is only one gaming interface.
that is to say the overwhelming majority of screenshots are similar.
few screenshots can represent the whole corpus thus the js divergence is smaller under lower summarization ratio.
summary for most projects ctras is more effective than classic summarization methods mcb and mmr.
c. answering rq3 effectiveness of ctras efficiency accuracy table v shows the average test report inspection time cost per test report for group a i.e.
control group b i.e.
ctras and c i.e.
golden .
according to table v the average completion time cost of each report are .
and .
seconds respectively for group b and c which saves .
and .
compared with group a .
seconds and the average v measure scores of group a b and c are .
.
and .
respectively which shows that group b and group c improve .
and .
accuracy compared with group a. this result indicates that with the help of summarization people can substantially save their time in duplicate test report clustering work not only without loss of accuracy but even with slight improvement.
in addition surprisingly group b cost less time than group c on average.
we investigated the details of the summarization result that is presented in table ii.
we find that that ctras performs a more strict duplicate aggregation than the professional developers which leads the number of clusters for group b to be smaller than group c. as such participants of group b generally have fewer summaries to reference in the inspectiontable v task evaluation results rq3 ab c completion time s .
.
.
.
.
v measure .
.
.
.
.
procedure.
thus in comparison with group c group b can save some time cost at the expense of loss of accuracy.
satisfaction the participants satisfaction rating on average was high .
on a point scale.
more subjectively the semi structured interviews produced qualitative results.
all participants thought that the master report can reflect the topic of summary and it helps them get a general idea of the summary instructs the granularity of clustering participants mentioned supplementaries contained additional information which is clear and coherent can be used as valuable reference when it comes to uncertain condition and provides detailed operation steps .
many participants mentioned text was more useful which supports our strategy of setting distance calculation parameters described in section v. some participants stated that screenshots are open to various interpretations and can t tell where s the problem .
moreover some suggestions for improvement were proposed such as the description is not well structured and highlighting important parts of screenshots .
d. threats to v alidity subject program selection the first threat is related to the generality of ctras.
w e ev aluated our approach on projects all of which are android applications thus it is unclear whether ctras can achieve similar results on other projects from android and other mobile platforms e.g.
ios .
however the categories of our projects vary widely such as music audio games health fitness education travel local and so on.
therefore we believe the experiment result can indicate the usefulness of our method.
natural language selection all the crowdsourced test reports utilized in this paper are written in chinese which may affect the generalization of ctras.
however the purpose of our method is to generate comprehensive summaries by leveraging the information in duplicated reports and its key part is to measure the similarities between reports utilizing textual description and screenshots.
in the aspect of textual descriptions the similarities are effected by the keyword corpus authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
jsdivergence compression ratiocloudmusic jsdivergence compression ratiogame jsdivergence compression ratiohw health jsdivergence compression ratiohj normandyjsdivergence compression ratiomylistening jsdivergence compression ratioishopping jsdivergence compression ratiojayme jsdivergence compression ratiojustforfunjs divergence compression ratiokimiss js divergence compression ratio mcb ctras mmrslife js divergence compression ratiotuniu js divergence compression ratioubook fig.
evaluation results for the summarizer rq2 lower is better extraction methods and nlp researchers have proposed many relatively mature approaches targeting different languages.
student participant in our user study students were recruited to complete the task although the participants are students none have used the subject applications which means that diagnosing for these applications is a new task for them.
based on salman et al.
s research under this situation students and professionals often perform similarly.
thus we argue that the result can be generalizable to professionals.
vii.
r ela ted work a. duplicate report detection duplicate report detection is the problem of verifying whether a new test report is a duplicate of existing reports which has been well studied by many researchers.
previous duplicate software visualization detecting approaches are mainly based on natural language processing techniques machine learning techniques and information retrieval techniques .
lately jiang et al.
applied a clustering technique on crowdsourced test reports.
they proposed terfur to aggregate multiple redundant test reports into clusters to reduce the number of reports that need to be inspected manually.
however none of these approaches leverages the screenshot information on duplicate report detection problem and the goal of these works is to filter out the duplicates whereas our goal is to take advantage of duplicates to help people understand bugs more comprehensively.
b. bug report summarization there are several works discussing the problem of bug report summarization which are resolved in either a supervised or unsupervised way.
rastkar et al.
developed a supervised learning classifier to judge whether a sentence should be included in the summary and found that the classifier trainedspecifically on bug reports outperformed existing conversationbased classifiers.
jiang et al.
improved the summarization performance by employing authorship characteristics and information in associated duplicate reports .
to address the problem that supervised approaches require manually annotated corpora and generated summaries may be biased towards training data mani et al.
applied four well known unsupervised summarization algorithms to bug report summarization.
lotufo et al.
proposed a hypothetical model of users bug report reading processes utilized it to rank sentences by their probability of being read and include sentences with the highest probabilities into the summary.
in contrast instead of summarizing individual bug reports our approach utilizes the information from all duplicate reports to provide a summary view of a set of related reports.
viii.
c onclusion the problem of diagnosing the overwhelming number of test reports has been a fundamental challenge for crowdsourced testing.
to alleviate this problem in this paper we present ctras a novel approach for aggregating and summarizing crowdsourced test reports.
ctras lev erages the duplicate reports to assist professional testers to manage and understand crowdsourced test reports.
it overcomes several shortcomings by aggregating duplicates to enable batch processing summarizing the supplementary topics from duplicates to facilitate developer comprehension of the reports.
the evaluation result reveals that ctras is capable of assisting people s detecting and triaging crowdsourced test reports.
acknowledgment this work was supported by the national science foundation under award career ccf the national natural science foundation of china and china scholarship council csc .
thanks for the data support of mooctest.net.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.