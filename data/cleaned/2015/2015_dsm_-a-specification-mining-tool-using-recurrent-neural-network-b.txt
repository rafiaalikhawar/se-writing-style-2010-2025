singapor e management univ ersity singapor e management univ ersity institutional k nowledge at singapor e management univ ersity institutional k nowledge at singapor e management univ ersity resear ch collection school of computing and information systems school of computing and information systems dsm a specification mining t ool using r ecurr ent neur al network dsm a specification mining t ool using r ecurr ent neur al network based language model based language model tien duy b. le singapor e management univ ersity btdle smu.edu.sg lingf eng ba o zhejiang univ ersity david l o singapor e management univ ersity davidlo smu.edu.sg follow this and additional works at https ink.libr ary.smu.edu.sg sis r esear ch part of the programming languages and compilers commons and the softwar e engineering commons citation citation le tien duy b. ba o lingf eng and l o david.
dsm a specification mining t ool using r ecurr ent neur al network based language model.
.
esec fse pr oceedings of the 26th a cm joint meeting on e uropean softwar e engineering conf erence and symposium on the f oundations of softwar e engineering no vember lak e buena vista fl .
.
available at available at https ink.libr ary.smu.edu.sg sis r esear ch this conf erence pr oceeding ar ticle is br ought t o you for fr ee and open access b y the school of computing and information systems at institutional k nowledge at singapor e management univ ersity .
it has been accepted for inclusion in resear ch collection school of computing and information systems b y an authoriz ed administr ator of institutional k nowledge at singapor e management univ ersity .
for mor e information please email cher ylds smu.edu.sg .
dsm a specification mining tool using recurrent neural network based language model tien duy b. le singapore management university singaporelingfeng bao zhejiang university city college chinadavid lo singapore management university singapore abstract formal specifications are important but often unavailable.
furthermore writing these specifications is time consuming and requires skills from developers.
in this work we present deep specification miner dsm an automated tool that applies deep learning to mine finite state automaton fsa based specifications.
dsm accepts as input a set of execution traces to train a recurrent neural network language model rnnlm .
from the input traces dsm creates a prefix tree acceptor pta and leverages the inferred rnnlm to extract many features.
these features are then forwarded to clustering algorithms for merging similar automata states in the pta for assembling a number of fsas.
next our tool performs a model selection heuristic to approximate f measure of fsas and outputs the one with the highest estimated f measure.
noticeably our implementation of dsm provides several options that allows users to optimize quality of resultant fsas.
our video demonstration on the performance of dsm is publicly available at .
ccs concepts software and its engineering dynamic analysis keywords specification mining deep learning acm reference format tien duy b. le lingfeng bao and david lo.
.
dsm a specification mining tool using recurrent neural network based language model.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
introduction formal specifications play important roles to the reliability and maintainability of software systems.
manually writing formal specifications can be very expensive and difficult as developers must lingfeng bao is a join first author.
he was affiliated with singapore management university singapore when this work was performed.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa association for computing machinery.
acm isbn .
.
.
.
requisite skills and experiences.
furthermore specifications can be quickly outdated due to rapid evolution of software systems.
thus there is a need of automated solutions to infer specifications.
to help developers reduce the cost of manually drafting formal specifications many automated approaches have been prosed .
one of the popular specification mining algorithms is to infer finite state automaton fsa based specifications from execution traces .
nevertheless the quality of mined specifications is not perfect yet and more works need to be done to make specification mining better.
for example fsas inferred using the k tail algorithm are usually inaccurate for execution traces containing methods that frequently co occur in particular orders but are not required to occur exactly in these orders.
in this work we implement a prototype tool based on our new specification mining algorithm that performs deep learning on execution traces.
we name the tool dsm which stands for deep specification miner.
this tool takes as input a set of execution traces and performs several processing steps that eventually result in one fsa.
first dsm performs deep learning on the input execution traces to train a recurrent neural network language model rnnlm .
then dsm constructs a prefix tree acceptor pta from the execution traces and leverage the learned language model to extract a number of interesting features from pta s nodes.
these features are then input to clustering algorithms for merging similar states i.e.
pta s nodes .
the output of an application of a clustering algorithm is a simpler and more generalized fsa that reflects the training execution traces.
finally based on the predicted values of f measure of constructed fsas dsm selects the one with highest predicted value of f measure as output.
our evaluation on target library classes shows that dsm has a good performance i.e an average f measure of .
.
the remainder of this paper is structured as follows.
section highlights the bird s eye view design of dsm.
sections illustrates how dsm works with specific examples as well as demonstrates command line options provided by dsm.
we presents our evaluation of dsm and discuss related works in section and section respectively.
finally we conclude and mention future work in section .
overall design figure shows the design of dsm.
our tool accepts as input a set of execution traces which are sequences of methods.
the output of dsm is a finite state automaton fsa that reflects interactions between program methods in the input sequences.
in our design there are five processes in dsm recurrent neural network based language model rnnlm learning trace sampling feature engineering clustering and model selection.
896esec fse november lake buena vista fl usa tien duy b. le lingfeng bao and david lo execution traces trace sampling rnnlm learning prefix tree acceptor rnnlm feature engineering feature s clustering fsa candidates model selectionfsa figure overall design of dsm s9s1 start s2 init s6update s7 s8initverify end s12 s10 initverify end s11initverifys5 s3 s4verify end ... ... figure an example prefix tree acceptor pta rnnlm learning dsm accepts as input execution traces in the form of method sequences.
each of these sequences begins and terminates with two special symbols start and end respectively.
these symbols are used for separating two distinct sequences.
dsm provides utilities to select the underlying neural network architecture and configure learning parameters for training rnnlms from execution traces.
dsm supports the following three architectures standard recurrent neural network long short term memory lstm and gated recurrent units gru .
trace sampling since it is expensive to use all sequences in the training data to construct fsas dsm leverages a heuristic to select a subset of method sequences.
the goal here is to create a smaller subset that is likely to represent the whole set of all traces reasonably well.
in particular dsm s heuristic tries to find a subset of traces that covers all co occurrence pairs1of methods in all training traces.
more detail of dsm s trace sampling heuristic is available in our research paper .
feature extraction dsm constructs a prefix tree acceptor pta from method sequences of the sampled execution traces.
a pta is a tree like deterministic finite automaton dfa created by putting all the prefixes of sequences as states and a pta only accepts the sequences that it is built from.
the final states of our constructed ptas are the ones have incoming edges with end labels see the step of rnnlm learning .
figure shows an example of a prefix tree acceptor pta .
next dsm extracts two different types of features based on pta.
the goal of this step is to provide sufficient information for clustering algorithms in the subsequent process to better merge pta nodes.
the following are the two feature types type i this type of features captures information of previously invoked methods before the state sis reached.
the values of type i features for state sis the occurrences of methods on the path between the starting state i.e.
the root of the pta ands.
for example according to figure the values of type i features corresponding to node s3are f start f init finitverify 1andfupdate fverify f end .
m1 m2 is a co occurrence pair if m1andm2appear together in at least one trace.
type ii this type of features captures the likely methods to be immediately called after a state is reached.
values of these features are computed by the inferred rnnlm in the deep learning step.
for example at node s3in figure close and end have higher probabilities than the other methods to be called afterward.
examples of type ii features and their values for node s3output by a rnnlm are as follows pinitverify p end .4andp start p init pverify pupdate .
.
clustering dsm executes k means and hierarchical clustering algorithms on the pta s states with their extracted features.
the goal of this step is to create a simpler and more generalized automaton that captures specifications of a target library class.
since both k means and hierarchical clustering require the predefined inputcfor number of clusters dsm tries with many values of cfrom tomax cluster2to search for the best fsa.
overall the execution of clustering algorithms results in max cluster fsas.
model selection dsm employs a heuristic to select the best fsa among the ones output by the clustering algorithms using the input execution traces.
it estimates precision by first constructing a set picontaining all pairs m1 m2 where m1andm2appear consecutively i.e.
m1is called right before m2 in the input execution traces.
then dsm constructs another set pmcontaining all pairs m1 m2 that appear consecutively in a trace generated by an automaton moutputted by the clustering algorithms.
the estimated precision is pi pi pm.
next dsm estimates the values of recall by computing the percentage of all execution traces accepted by a given automaton m. once all precision and recall of fsa models are estimated dsm computes the expected value of f measure i.e.
harmonic mean of precision and recall for each of the automata.
finally dsm returns the fsa with highest expected f measure.
more details of dsm s model selection heuristic are is available in our research paper .
usage scenarios in this section we illustrate how dsm operates via several usage scenarios.
furthermore we describe a number of dsm s key options to tune the accuracy of output fsa.
.
scenario i simple command dsm is implemented in python .
with tensorflow .
.
and scikit learn .
.
.
we choose python due to its simplicity and 2by default max cluster .
897dsm a specification mining tool using recurrent neural network ... esec fse november lake buena vista fl usa c3 c1signaturec2 c0endupdateverify falseinitverifyend initverify figure resultant fsa for java.security.signature .
c3 c0stringtokenizerc2endhasmoretokens truenexttoken c1hasmoretokens falseendhasmoretokens false figure resultant fsa for java.util.stringtokenizer .
convenience when deploying machine learning libraries.
in a nutshell dsm is a command line application that allows users to input the path of data folder i.e.
data dir where execution traces are stored in input.txt file in the folder.
dsm accepts traces that contains sequences of methods for example input traces of java.security.signature is available at these sequences can be collected from execution of software systems that are known to utilize the target api libraries or apis.
additionally test case generation tools e.g.
randoop etc.
can also be used to generate a richer set of test cases that capture many behaviors of the target libraries apis.
users can run the following simple command to mine fsas python3 dsm .py data dir after this command is executed dsm performs several processing steps including learning a recurrent neural network language model rnnl using its default parameters.
depending on learning configuration of rnnlm e.g.
number of hidden layers size of each rnn layer etc.
dsm might takes longer than minutes to output a fsa.
the resultant model mined by dsm is shown in two formats text and image.
note that graphviz3 i.e.
graph visualization software must be installed to enable dsm to produce dot files.
figure shows one simple fsa mined by dsm.
in our implementation dsm.py is the program main point that contains code portions calling five major modules of dsm see section .
advanced users who are interested in adapting our tool can investigate dsm s source code starting from dsm.py .
the latest version of dsm is developed and tested on linux platform.
we believe dsm can be employed on other platforms such as windows and mac os as long as dependency libraries especially tensorflow are properly installed.
details of dsm s installation instructions can be found at .
scenario ii tuning dsm s parameters assuming we are interested to infer a fsa for an input execution traces of java.util.stringtokenizer which is available at since it is possible that execution time and accuracy of resultant fsas are not as expectation of users dsm provides several parameters to tune its performance.
table highlights a number of key parameters of dsm.
in order to adjust the learning of rnnlm users can change the configuration of the learned rnnlm such as number of hidden layers i.e.
num layers dsm s key parameters parameter description rnn size number of rnn states in each hidden layer num layers number of hidden layers model rnn type i.e.
rnn lstm gru batch size minibatch size seq length rnn sequence length num epochs number of epochs min cluster minimum number of clustering settings max cluster maximum number of clustering settings max cpu maximum number of cpus can be used table target library classes.
m represents the number of class methods that are analyzed generated test cases is the number of test cases generated by randoop recorded method calls is the number of recorded method calls in the execution traces nfst stands for numberformatstringtokenizer .
target library m generated recorded class test cases method calls arraylist hashmap hashtable hashset linkedlist nfst signature socket stringtokenizer stackar zipoutputstream number of states in each hidden layer i.e.
rnn size or type of rnn architecture i.e.
model .
in dsm s implementationm we leverage the multi cores of a cpu to speed up tasks that can execute in parallel such as feature engineering and clustering processes.
to maximize the degree of parallelism in dsm users can change the values of max cpu to indicate the number of processes that can be used by dsm.
the more cpus are assigned to dsm the faster the specification mining process is.
dsm also provides min cluster and max cluster options to optimize the clustering process see section .
these options regulate the number of clusters considered by the clustering algorithms.
by default the range is .
the following is an example command that executes dsm with custom setting python3 dsm .py data dir num layers rnn size model lstm min cluster max cluster max cpu figure shows the resultant fsa inferred by dsm for java.util.stringtokenizer s input traces available at https goo.gl myqlxo using the above setting.
evaluation in our experiments we select target library classes as the benchmark to evaluate the effectiveness of our proposed approach.
898esec fse november lake buena vista fl usa tien duy b. le lingfeng bao and david lo table effectiveness of dsm.
f is f measure.
class f class f arraylist .
signature .
hashmap .
socket .
hashset .
stackar .
hashtable .
stringtokenizer .
linkedlist .
zipoutputstream .
nfst .
average .
these library classes were also investigated by previous research works .
table shows further details of the selected library classes including information of collected execution traces.
among these library classes out of are from java development kit jdk the other two library classes are datastructure.stackar from daikon project and numberformatstringtokenizer from apache xalan .
table shows the f measure of dsm for the eleven target library classes.
from the table our approach achieves an average f measure of .
.
noticeably for stringtokenizer and signature dsm infers models that exactly match ground truth models i.e.
f measure of .
there are other out of the library classes where our approach achieves f measure of or higher.
related work k tails is a classic algorithm proposed by biermann and feldman to infer a fsa from execution traces.
the algorithm takes as input a set of execution traces and a parameter k.k tails first builds a prefix tree acceptor pta and then merges every two states of the pta that have identical sequences of the next kmethod invocations i.e.
k tails .
the effectiveness of k tails depends the choice of kand the quality of its input traces.
krka et al.
propose a number of specification miners that are capable of inferring a fsa from execution traces and likely invariants .
these miners are contractor state enhanced k tails sekt and trace enhanced mts inference temi .
among the above three approaches contractor only utilizes valuebased program invariants inferred by daikon to construct fsas.
on the other hand state enhanced k tails sekt and traceenhanced mts modal transition system inference temi analyze both execution traces and daikon s likely invariants to infer fsa based specifications.
specforge is built on the top of krka et al s proposed miners and k tails .
there are two important processes in specforge model fission and model fusion.
specforge employs model fission to extract many temporal rules from input fsas and select a number of interesting rules that frequently appear in input fsas.
then model fusion is utilized to construct a new automata that satisfies that selected temporal rules.
additionally lo et al.
propose smartic that mines a fsa from a set of execution traces using a variant of k tails to construct a probabilistic fsa.
walkinshaw and bogdanov propose an approach that allows users to input temporal properties to support a specification miner to construct a fsa from execution traces .
mariani et al.
propose k behavior that creates an automaton by inspecting one single trace at a time.
synoptic infers three kinds of temporal invariants from execution traces and uses them to generate a concise fsa .different from dsm none of the above mentioned studies leverage deep learning to mine specifications from execution traces.
we have also shown in our research paper that dsm substantially outperforms a number of baselines in terms of quality of mined specifications and ability to detect malicious behaviors of android apps.
conclusion in this work we present dsm which is an automated tool that applies deep learning to mine finite state automaton fsa based specifications from execution traces.
in particular we describe the design usage scenarios and performance of dsm.
dsm is opensource and can be run from command line with simple options.
our dataset dsm s implementation and a technical report that includes additional results are publicly available at com lebuitienduy dsm .
acknowledgment this research was supported by the singapore national research foundation s national cybersecurity research development programme award number nrf2016ncr ncr001 .