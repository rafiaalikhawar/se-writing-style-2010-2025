a learning based approach for automatic construction of domain glossary from source code and documentation chong wang fudan university chinaxin peng fudan university chinamingwei liu fudan university china zhenchang xing australian national university australiaxuefang bai fudan university chinabing xie peking university china tuo wang fudan university china abstract a domain glossary that organizes domain specific concepts and their aliases and relations is essential for knowledge acquisition and software development.
existing approaches use linguistic heuristics or term frequency based statistics to identify domain specific terms from software documentation and thus the accuracy is often low.
in this paper we propose a learning based approach for automatic construction of domain glossary from source code and software documentation.
the approach uses a set of high quality seed terms identified from code identifiers and natural language concept definitions to train a domain specific prediction model to recognize glossary terms based on the lexical and semantic context of the sentences mentioning domain specific concepts.
it then merges the aliases of the same concepts to their canonical names selects a set of explanation sentences for each concept and identifies is a has a and related to relations between the concepts.
we apply our approach to deep learning domain and hadoop domain and harvest and concepts together with and relations respectively.
our evaluation validates the accuracy of the extracted domain glossary and its usefulness for the fusion and acquisition of knowledge from different documents of different projects.
ccs concepts software and its engineering documentation computing methodologies information extraction .
c. wang x. peng m. liu x. bai and t. wang are with the school of computer science and shanghai key laboratory of data science fudan university china.
x. peng is the corresponding author pengxin fudan.edu.cn .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august tallinn estonia association for computing machinery.
acm isbn .
.
.
.
documentation domain glossary concept knowledge learning acm reference format chong wang xin peng mingwei liu zhenchang xing xuefang bai bing xie and tuo wang.
.
a learning based approach for automatic construction of domain glossary from source code and documentation.
in proceedings of the 27th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august tallinn estonia.
acm new york ny usa pages.
introduction software projects usually belong to specific domains for example tensorflow and pytorch are deep learning libraries hadoop and hbase are distributed database systems.
each domain has a set of business or technical concepts.
these concepts are frequently mentioned in source code and documentation of software projects in a domain.
they can be mentioned in full names as well as various aliases e.g.
abbreviations and other morphological variants .
many concepts are correlated such as hypernymhyponym whole part etc.
organizing domain specific concepts their aliases and relations in a domain glossary is essential for knowledge acquisition and software development.
moreover based on a domain glossary we can semantically link the elements e.g.
classes methods apis q a posts document fragments from different artifacts.
these links can facilitate many software engineering tasks such as developer question answering traceability recovery and maintenance feature location api recommendation and code search .
due to the complexity and fast development of a domain it is often laborious and expensive to manually construct a comprehensive domain glossary.
therefore researches have been advocated to investigate automatic extraction of glossary terms or domain concepts from different kinds of software artifacts.
some researches extract glossary terms from requirements documents.
these approaches use linguistic heuristics or term frequency based statistics to identify domain specific terms from noun phrases and thus the accuracy is often low.
moreover these approaches only cluster relevant terms together and cannot identify the aliases andesec fse august tallinn estonia chong wang xin peng mingwei liu zhenchang xing xuefang bai bing xie and tuo wang relations of a specific concept.
some researches extract domain concepts and relations from q a websites such as stack overflow but rely on stack overflow tags and human labelled data for the estimation of domain relevance.
in this paper we aim at extracting domain specific concepts their aliases and relations from both source code and documentation of software projects in the same domain in an unsupervised way.
the challenges for this task lie in several aspects see examples in section .
first knowledge about domain concepts and their relations is often scattered in different documents or even different projects.
second the same concepts are often mentioned in different aliases in different places.
third the relevance of concepts to a specific domain cannot be reliably determined by lexical heuristics or term frequency.
in this work we propose a learning based approach for automatic construction of domain glossary from source code and software documentation.
the basic idea of our approach is twofold.
first domain specific concepts in documentation are often used as identifiers in source code and the relations between concepts may be inferred from structural relations between corresponding code elements.
second using the sentences that mention the high quality seed terms identified from code identifiers and natural language concept definitions as training data we can learn a domain specific prediction model to recognize more glossary terms based on the lexical and semantic context of the sentences mentioning domainspecific concepts.
based on these two ideas our approach first extracts a set of candidate terms from the source code and documentation of different projects of the target domain and then merges the aliases of the same concepts to their canonical names.
after that our approach selects a set of explanation sentences for each concept and further identifies is a hypernym hyponym has a whole part related to relations between the concepts.
we implement the approach and conduct an empirical study with two technical domains deep learning and hadoop .
the study leads to the following findings.
first our approach outperforms a state of the art approach in documentation based domain glossary extraction.
second domain glossary extraction provides an effective way to fuse the domain knowledge from different documents of different projects.
third the extracted domain glossary complements general knowledge bases such as wikipedia.
fourth the extracted domain glossary can help developers to acquire the required knowledge from documents more efficiently.
this work makes the following contributions.
we propose a learning based approach for automatic construction of domain glossary from source code and documentation.
we apply the approach to the deep learning domain and the hadoop domain and harvest and concepts together with and relations respectively.
we evaluate the effectiveness of the approach for domain glossary extraction and the usefulness of the extracted domain glossary for knowledge fusion and software knowledge acquisition.
motivation domain concepts are often mentioned in various aliases.
when encountering an alias in code or documentation that developers do not understand the first question to ask is what this alias represents.for example nas is the abbreviations of neural architecture search in deep learning domain but it is hard to find explanations for it by searching google.
in this situation just knowing the full name of the alias corresponding domain concept is already very helpful for developers.
knowing the name of a domain concept is of course not enough.
because of the lack of domain knowledge developers further need to know the domain specific meaning of the concept.
for example in deep learning domain lstm is the abbreviation of long short term memory which is an advanced neural network structure that many developers may not know.
an explanation of the concept will greatly help the developers understand it.
in addition to understand the meaning of domain specific concepts it is also very important for the developers to establish an understanding of the relationships between domain concepts such as hypernym hyponym whole part or semantically correlated.
knowing the relationships between domain concepts can help developers understand domain knowledge in source code and documentation and help them make informed decisions in adopting certain techniques.
for example a developer is learning to use seq2seq model.
when she knows the relation seq2seq has a rnn rnn has a rnn cell and lstm cell is a rnn cell she can naturally think of using lstm cell to replace the rnn cell in the seq2seq model.
furthermore when she knows more variants of rnn cell e.g.
gru cell she will soon be able to build up her understanding of seq2seq.
this knowledge may change the developer s choice of more advanced model in her work.
therefore a solution to build domain glossary automatically is needed.
this solution can exploit the domain concepts mentioned in source code and documentation.
however the following challenges must be addressed.
first domain concepts and their explanations and relationships are often scattered in many documents of different projects.
for example pytorch has a package nn but we cannot find a sentence for explaining what nnis in its tutorials or reference documentation.
meanwhile tensorflow also has a package nnand we can find a sentence in its api reference documentation that explains the purpose of the package wrappers for primitive neural net nn operations .
from this sentence we may infer that nnin pytorch also stands for neural net.
thus it is important to fuse the same concepts may be in different aliases mentioned in different projects of the same domain.
second the same concepts are often mentioned in various aliases in different parts of the documents.
for example neural network is mentioned in different forms in deeplearning4j tensorflow and pytorch e.g.
nn neural net neural nets neural networks nn .
the alias can be different abbreviations of the full names or different morphological variants.
we need to identify these aliases and resolve them to canonical forms before we can reliably extract explanation sentences and relationships of domain concepts.
third the relevance of a term to a specific domain cannot be reliably determined by simple lexicon heuristics or term frequency metrics.
for example some lexical heuristics may assume that acronyms are glossary terms but many acronyms such as use support and note in deep learning domain are not.
term frequency metrics are not reliable either because some frequent noun phrases are irrelevant to the target domain while some relevant concepts are only mentioned a few times.
the lexical and semantic context of the sentences in whicha learning based approach for automatic construction of domain glossary from source code and documentation esec fse august tallinn estonia figure approach overview domain specific concepts are mentioned must be considered.
moreover source code can be used as an auxiliary resource for example many class names and package names represent the domain concepts.
approach an overview of the approach is presented in figure .
it includes three phases i.e.
preprocessing concept extraction and relation explanation identification.
the purpose of preprocessing is to extract useful information from the input corpus and prepare the required data for the subsequent phases.
it includes two steps i.e.
code analysis and document analysis.
code analysis parses the source code and extracts code elements e.g.
packages and classes and their relations e.g.
containment inheritance and aggregation from the code.
document analysis parses the documents and extracts sentences from them.
the purpose of concept extraction is to extract domain concepts from source code and documents.
it includes three steps i.e.
seed term extraction extended term extraction and alias merging.
seed term extraction uses heuristics to identify a set of initial seed terms from the sentences with the aid of code elements.
by treating the seed terms as the initial labelled data extended term extraction uses a semi supervised learning method to iteratively identify more candidate terms from the sentences.
note that these candidate terms may be the aliases of the same concepts.
therefore alias merging identifies and merges concept aliases e.g.
morphological synonyms and abbreviations that are extracted from different sentences.
the purpose of relation and explanation identification is to provide relations and explanations for the extracted domain concepts.
it includes two steps i.e.
relation identification and explanation extraction.
relation identification identifies is a has a and related to relations between concepts with the aid of code elements.
explanation extraction selects a set of explanation sentences for each concept.
we implement our approach in python and the natural language processing tool used in our implementation is spacy .
the remaining of the section details the steps of the approach and corresponding implementation.
all the examples used in this section are from the deep learning domain and its projects deeplearning4j tensorflow and pytorch.
.
preprocessing our implementation includes code analyzers for java and python which are implemented based on javalang a java parser implemented in python and the build in module astof python respectively.
the code analyzers extract the packages modules classes and various relations between them from the source code.
the technical documents of many projects are web pages that are available online.
therefore we implement a crawler based on scrapy to obtain the documents.
we use beautifulsoup apython parser for html and xml files to parse the obtained web pages.
we then clean the obtained web page content by handling different kinds of special html elements for example replacing code fragments with code replacing tables with table and recovering the sentences that are broken by tags like p and li .
finally we extract the plain text from the content and split the text into sentences based on punctuations.
.
seed term extraction seed term extraction automatically identifies a small set of terms with high confidence using heuristics.
to find an optimal set of heuristics we try different combinations of heuristics e.g.
extracting all caps words in text and local variable parameter names in code as seed terms and adjust the heuristics based on the results.
finally we choose the following two heuristic rules that can be used to extract high quality terms from source code and documents.
.
acronym appears with its full name in document if an acronym appears together with its full name in a document the acronym is likely a term.
this rule is embodied by the following three sentence patterns.
for example by design the output of arecurrent neuralnetwork rnn depends on arbitrarily distant inputs.
for example feed forward neural networks are comprised of dense layers while recurrent neural networks can include graves lstm long short term mem ory layers.
.
for example enumeration used to select the type of regression statistics to optimize on with the various regression score functions mse mean squared error mae mean absoluteerror rmse root mean squared error rse relativesquared error corrcoeff correlation coefficient.
.
to enforce this rule we first match a sentence with the above three patterns and then identify the phrase for the full name based on the acronym.
for example based on the acronym rnn we can identify the three word phrase recurrent neural network before the bracket as its full name.
for each matched acronym both its full name and itself are recognized as seed terms.
.
package class name appears in document if the name of a package or class appears as ordinary text i.e.
not code elements in a document the name is likely a term.
for example in the sentence during nn training each x iterations executors will send encoded dense updates with lower threshold from deeplearning4j the term nn is the name of the package org.deeplearning4j.nn .
the term nn here means neural network thus can be recognized as a seed term.
considering finer grained code elements such as parameters and local variables may produce vast terms but most of them are false ones.
to ensure the quality of seed terms we only consider course grained code elements i.e.
package and class in seed term extraction.
to enforce this rule we obtain the names of all the packages and classes in the code elements and split them by camel case we then search for the split words or phrases in all the sentences case insensitive and treat all the matched ones as seed terms.
seed terms extracted by the above two rules may be ordinary phrases that happen to be used as a package class name or general technical terms that are not specific to the target domain.
for example in the sentence computes the aggregate score as a sum of all ofesec fse august tallinn estonia chong wang xin peng mingwei liu zhenchang xing xuefang bai bing xie and tuo wang the individual scores of each of the labels against each of the outputs of the network from deeplearning4j the phrase a sum happens to be the name of the class org.nd4j.linalg.api.ops.impl.accum.asum .
but obviously the phrase is not a term.
examples of general technical terms include those related to file or string operations.
these ordinary phrases and general technical terms need to be eliminated to ensure the quality of the extracted seed terms.
to this end we use the following three rules to further eliminate irrelevant terms.
the seed terms that satisfy any condition are eliminated.
the term only appears in the documents of one project the term includes a single word and the word is a common word included in wordnet the term includes a stop word at the beginning or end.
the stop words we use in our implementation are the english stop words defined by nltk a natural language toolkit plus some special stop words related to programs including get set return test util .
.
extended term extraction extended term extraction uses machine learning to identify more terms from the sentences.
it treats the term identification as a sequence tagging task which is to predict the corresponding tag sequence of an observation sequence.
there are many nlp problems that are solved as sequence annotations such as part of speech tagging pos chunking and named entity recognition ner .
the tagging scheme we use for this task is the iobes scheme which is widely used for sequence tagging tasks.
in the schema b beginning i inside and e end respectively indicate that the current token is the beginning middle and end of a term s single indicates that the current token itself constitutes a term and o outside indicates a normal token.
for example the tagging of a sentence that includes two terms i.e.
recurrent neural network and rnn is presented below.
o byo design o o theo outputo ofo ab recurrent i neurale network o s rnn o o depends o ono arbitrarily o distant o inputs o .
our machine learning model for term identification is the widely used lstm crf model which combines lstm long short term memory networks and a crf conditional random fields layer.
it is an end to end learning model that does not require handcrafted features.
we use the implementation of the lstm crf model provided by guillaume et al.
.
to train the model we generate a sequence pair an input token sequence and its corresponding tag sequence from each sentence in the labelled data and use these sequence pairs as the training data.
when used for term identification i.e.
prediction the model takes as input a token sequence generated from an input sentence and produces as output a tag sequence.
the words that are tagged with s and the phrases that are tagged with b i and e in the output tag sequence are identified as glossary terms.
in both the training and prediction we use pretrained word vectors to represent the tokens in the input token sequence.
the word vectors are pretrained by fine tuning the glove word vectors.
our implementation is based on glove.840b.300d which has billion tokens .
million cased vocabulary and vector dimensions.
and the word2vec implementation we use is gensim .we use a semi supervised method to train the model and use an iterative process to identify more terms.
the process starts with the seed terms by treating the sentences that include the seed terms as the labelled data and all the other sentences as unlabelled data.
after each iteration a term identification model is trained and used to identify more terms from the sentences.
similar to seed term extraction this iterative term extraction process also uses the three rules i.e.
term appearing in only one project common word in wordnet or including stop words to filter out irrelevant terms.
all the unlabelled sentences with newly identified terms are then added into the labelled data to start the next iteration.
this filtering strategy ensures the quality of the sentences that are iteratively added into the labelled data.
the whole iterative process ends when any of the following conditions is met no new terms are identified by the trained model the number of iterations reaches a predefined limit in our implementation there are no unlabelled sentences.
in the last iteration all the newly identified terms are accepted except those including stop words at the beginning or end of their names.
for example in this step activation layer is identified as a term from the sentence imports an activation layer from keras.
from deeplearning4j.
to ensure the quality of the extracted terms we further refine the results of seed and extended term extraction to produce the candidate terms.
first eliminate general technical terms.
we use the following equation to estimate the generality of a term where f req t f req d t and t n t t nd t are the frequency and occurrence number of a term trespectively and n nd is the token number in the general technical corpus domain corpus .
we eliminate the terms whose generality is higher than a threshold .
in our implementation which is chosen based on trial term refinement using different thresholds.
the general technical corpus used in our implementation includes the reference documentations of jdk and python .
.
for example acm app and number values are eliminated using this rule.
generalit y t f req t f req d t t n t n t nd t nd second for each term that includes an acronym add the acronym into candidate terms if it is not included.
for example ais data is a candidate term thus ais which means automatic identification system is also added as a candidate term.
.
alias merging the purpose of this step is to identify the aliases of the same concept from the candidate terms and merge them together to form a concept.
this step handles two kinds of concept aliases that widely exist in technical documents i.e.
morphological synonym and abbreviation.
we first lemmatize all the candidate terms then identify and merge morphological synonyms and finally identify and merge abbreviations.
.
.
identification and merging of morphological synonyms.
typical morphological synonyms in technical documents include words or phrases that are only different in cases or singular plural forms e.g.
deeplearning4j and deeplearning4j rnn and rnns a learning based approach for automatic construction of domain glossary from source code and documentation esec fse august tallinn estonia words or phrases that have different spelling or misspelling e.g.
deeplearning4j and deeplearinng4j words or phrases that use hyphens in different ways e.g.
t sne and tsne .
the first type of morphological synonyms can be directly identified after lemmatization.
for the other two types we use edit distance to determine whether two words or phrases are morphological synonyms.
in particular we use damerau levenshtein distance dl distance to measure the similarity of two words or phrases.
the dl distance is the minimum number of operations insert delete or substitute a single character or transpose two adjacent characters required to convert one word or phrase into the other.
considering words or phrases of different lengths we also calculate the relative distance between two words or phrases s1ands2as the following equation where distance s1 s2 is the dl distance of s1ands2 len th s is the length of a string s. rdistance s1 s2 distance s1 s2 max len th s1 len th s2 for example deeplearinng4j can be converted into deeplearning4j by a transposition operation of i and n thus the dl distance of these two words is similarly the dl distance of rnn and nn is also .
these two pairs of words have the same dl distance but the relative distance of rnn and nn is much bigger than that of deeplearinng4j and deeplearning4j .
for two words or phrases if both of their dl distance and relative dl distance are lower than predefined thresholds respectively in our implementation we regard them as a pair of morphological synonyms.
the thresholds are chosen based on trail selection of morphological synonyms we first fix the dl distance threshold to and experimentally search for an optimal threshold for relative dl distance we then search for an optimal threshold for dl distance by fixing the relative dl distance threshold.
based on the identified morphological synonyms we construct alias sets of domain concepts by calculating transitive closures based on morphological synonyms i.e.
iteratively merging morphological synonyms together.
for any candidate term that has no morphological synonyms we construct an alias set that includes only the term itself.
.
.
identification and merging of abbreviation relationships.
typical abbreviation relationships in technical documents include a word is the acronym of a phrase e.g.
nn and neural network a word is the prefix of another one e.g.
net and network two phrases satisfy one of the above two relationships after eliminating their common head or tail words e.g.
neural net and neural network .
any two words or phrases that satisfy one of the above three conditions are treated as a candidate abbreviation relationship.
thus we can further merge the alias sets based on the identified candidate abbreviation relationships.
a difficulty here is that a candidate term may have abbreviation relationships with multiple other candidate terms but it is usually an abbreviation of only one other candidate term.
for example rnn is a candidate abbreviation for both recurrent neural network and recursive neural network but it is usually the abbreviation of the former in deep learning.table hearst patterns c1such as c2 c2 is a c1 suchc1asc2 c2 is a c1 c2 or other c1 c2 is a c1 c2 and other c1 c2 is a c1 c1 including c2 c2 is a c1 c1 especially c2c2 is a c1 therefore we need to determine only one full name for an abbreviation in alias merging.
for an alias set asthat has candidate abbreviation relationships with a set of other alias sets asset we determine an only alias set in asset as the full name of asbased on context similarity in the following way.
we calculate the context similarity between asand each alias set in asset as the cosine similarity of their vectors.
the vector of an alias set is obtained by averaging the vectors of all the sentences that include a candidate term from the alias set while the vector of a sentence is obtained by averaging the pretrained vectors of all the words in the sentence.
based on the context similarities we select an alias set from asset that has the highest context similarity with asas the full name of as.
based on the finally determined abbreviation relationships we iteratively merge the alias sets that have abbreviation relationships together.
after alias merging each of the alias set is treated as a domain concept with all the candidate terms in the set as its aliases.
for example we can obtain the following domain concepts and their alias rnn rnns recurrent neural networks rnn recurrent neural network recurrent neural networks rnn neural network nn neural net neural net neural network neural networks neural net neural network neural networks neural nets neural networks .
.
relation identification relations between the extracted domain concepts are often implied in specific sentence patterns in the documents and structural relations between code elements.
based on the document and code analysis we identify is a has a and related to relations between the extracted concepts.
for two concepts c1andc2 we identify is a and has a relations between them from the following aspects.
first identify is a relations from the sentences that mention c1andc2based on hearst patterns which are widely used in the automatic acquisition of hyponymy lexical relations from unrestricted text.
the patterns used in our current implementation are shown in table where c1andc2can be any alias of the corresponding concepts.
second identify is a and has a relations based on the structural relations between packages and classes.
for two packages or classes e1ande2whose names are the aliases of c1andc2 respectively ife1contains e2or aggregates e2as a property add a has a relation from c1toc2 ife1inherits e2 add an is a relation from c1toc2.
third identify is a and has a relations based on prefix suffix relations between concept aliases if an alias of c1is the prefix of an alias of c2 add a has a relation from c1toc2 if an alias of c1is the suffix of an alias of c2 add an is a relation from c2toc1 esec fse august tallinn estonia chong wang xin peng mingwei liu zhenchang xing xuefang bai bing xie and tuo wang to provide richer concept relations we further identify related to relations between the extracted concepts which are weaker than is a and has a relations.
the identification of related to relations is based on the similarity between two concepts if the similarity between two concepts is higher than a predefined threshold add a bidirectional related to relation between them.
the context similarity is calculated as the following equation which combines the lexical similarity simlex and the context similarity simcontext of two concepts w1andw2are two weights satisfying w1 w2 .
sim c1 c2 w1 simlex c1 c2 w2 simcontext c1 c2 the lexical similarity of c1andc2is the jaccard similarity between the token sets of their aliases i.e.
token c1 andtoken c2 .
simlex c1 c2 token c1 token c2 token c1 token c2 the context similarity of c1andc2is calculated in a similar way to the context similarity used in alias merging generate a vector for each concept by averaging the vectors of all the sentences that mention the concept then calculate the cosine similarity between concept vectors.
in our implementation the similarity threshold is set to .
and w1andw2are set to .
and .
respectively.
the threshold and weights are determined based on trail identification of related to relations we first fix the threshold to .
and experimentally search for an optimal value for w1 w2 w1 we then search for an optimal threshold by fixing w1.
some examples of concept relations extracted from different aspects are as follows.
sgd is a optimizer extracted from the sentence torch.optim contains optimizers such as sgd.
based on hearst patterns nn has a activation layer extracted based on the package containment relation between org.deeplearning4j.nn and org.deeplearning4j.nn.layers.activationlayer rnn has a rnn layer and recurrent neural network is a neural network extracted based on the prefix suffix relations between concept names l1 related to regularizer extracted based on context similarity of concepts.
note that some related to relations may be is a or has a relations.
for example l1 is a common method for regularization to avoid overfitting in machine learning but only a related to relation is extracted for them because there lack supports from sentences and code elements for the is a relation between them.
there may be more than one relation between two concepts.
for example there are both is a and has a relations between rnn and neural network .
.
explanation extraction concept extraction provides for each extracted domain concept a list of sentences that mention the concept.
the purpose of explanation extraction is to select for each domain concept a set of sentences that can help users to understand the concept.
these sentences usually can be classified into the following categories.
concept definition provide definition for a concept and related techniques for example taxonomy explanation of the meaning technical comment comment on the characteristics of a concept and related techniques for example benefits and drawbacks comparisons with other techniques usage guidance suggest the proper way to use a concept and related techniques for example applicable scenarios guidance on related settings common problem solutions.
we need to filter out sentences that mention a concept but are useless for the understanding of the concept.
these sentences may be low quality sentences that are incomplete or sentences talking about low level implementation e.g.
using the concepts to explain the functionalities of code elements .
we treat all the sentences that mention a domain concept as the candidate sentences for the concept and use the following heuristic rules to filter out useless sentences.
candidate sentences that satisfy any of the rules are filtered out.
incomplete sentences the sentence has no subject or predicate or has incomplete punctuations e.g.
the right parenthesis is missing code elements the sentence includes code elements question the sentence is a question subordinate clause the sentence mentions the concept in its subordinate clauses.
for example for the concept activation layer the following two sentences are selected as its explanation sentences activation layer is a simple layer that applies the specified activation function to the input activations.
activation layer used to apply activation on input and corresponding derivative on epsilon.
.
while the following two sentences are filtered out imports an activation layer from keras.
advanced activation layers.
.
empirical study we conduct an empirical study with two technical domains deep learning and hadoop .
based on the results we evaluate the effectiveness of our approach for domain concept extraction and the usefulness of the extracted domain concepts for software development tasks.
all the data and results of the study have been included in our replication package .
.
study design the two subject domains that we choose for the study represent two kinds of technical domains.
deep learning domain includes software libraries that are written in different languages and provide similar functionalities for the development of deep learning applications.
hadoop domain is a software ecosystem that consists of interdependent software systems.
the subject projects we choose for the deep learning domain are three popular deep learning libraries deeplearning4j .
.0beta3 tensorflow .
pytorch .
.
.
among them deeplearning4j is written in java tensorflow and pytorch are written in python.
the subject projects we choose for the hadoop domain are hadoop .
.
hbase .
.
hive .
.
.
hadoop is a distributed data storage and processing framework based on the mapreduce model.
hbase is a distributed database that runs on top of hdfs hadoop distributed file system and provides bigtable like capabilities for hadoop.
hive provides a sql like interface to querya learning based approach for automatic construction of domain glossary from source code and documentation esec fse august tallinn estonia figure concepts and relations extracted for deep learning domain data stored in various databases and file systems that integrate with hadoop.
all these three projects are written in java.
all these six projects are open source.
we crawl the source code of these projects and the documents available at their official websites.
the documents include official project introductions user guides manuals tutorials api