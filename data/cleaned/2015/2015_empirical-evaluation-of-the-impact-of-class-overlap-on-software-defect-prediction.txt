empirical evaluation of the impact of class overlap on software defect prediction lina gong school of computer science and technology china university of mining and technology xuzhou china mine digitization engineering research center of ministry of education xuzhou china department of information science and engineering zaozhuang university zaozhuang china email linagong cumt.edu.cnshujuan jiang rongcun wang and li jiang school of computer science and technology china university of mining and technology xuzhou china mine digitization engineering research center of ministry of education xuzhou china email shjjiang cumt.edu.cn email rcwang cumt.edu.cn email lijiang cumt.edu.cn abstract software defect prediction sdp utilizes the learning models to detect the defective modules in project andtheir performance depends on the quality of training data.
the previous researches mainly focus on the quality problems of class imbalance and feature redundancy.
however training data oftencontains some instances that belong to different class but have similar values on features and this leads to class overlap to affect the quality of training data.
our goal is to investigatethe impact of class overlap on software defect prediction.
at the same time we propose an improved k means clustering cleaning approach ikmcca to solve both the class overlap and class imbalance problems.
specifically we check whether kmeans clustering cleaning approach kmcca or neighborhoodcleaning learning ncl or ikmcca is feasible to improve defect detection performance for two cases i within project defect prediction wpdp ii cross project defect prediction cpdp .
to have an objective estimate of class overlap we carry out our investigations on open source projects and compare the performance of state of the art learning models for the above mentioned cases by using ikmcca or kmcca or ncl vs. without cleaning data.
the experimental results make clear that learning models obtain significantly better performance in termsof balance recall and auc for both wpdp and cpdp when the overlapping instances are removed.
moreover it is better to consider both class overlap and class imbalance.
index t erms class overlap software defect prediction kmeans clustering machine learning i. i ntroduction defect prediction technology is one of the research topics among academic and industrial organizations .
with the development of machine learning methods more and more classification models areapplied to software defect prediction sdp to detect as manydefective modules as possible with minimal cost.
the qualityof training dataset from software projects seriously affects theprediction capabilities of learning models.
previous studies inthe data quality of sdp mainly focus on class imbalance andfeature redundancy and have proposed lots of models to dealwith these quality problems.
however in practical data collection due to the objective condition some instances with different classes may havefig.
.
the distribution of pc4 dataset in nasa.
figure denotes the pc4 has high overlapping instances these overlapping areas are diffcult to classifier.
similar values on some features which results in overlap in feature space.
these instances are called overlapping instances triggering the class overlap problem.
class overlap is one of the bottlenecks in data mining and machine learning and sdp is no exception.
many instances from nasa aeeem relink softlab and morph are overlapped as shown in figure many instances arelocated in overlapping space where learning models oftenfail to classify which hinders the prediction performance oflearning models.
in the past few years many studies have encountered class overlap problem in practical fields such as credit card fraud detection text classification .
they found the overlapping instances involve the performance of learning models which often becomes a thorny problem when combined with other factors such as class imbalance.
in sdp researchersmainly focus on data preprocessing including class imbalance and noise cleaning few studies have paid attention on classoverlap problem.
chen et al.
attempted to apply the neigh borhood cleaning learning ncl rule to remove overlappinginstances for sdp.
however the ncl method only removed the conflicting non defective instances to eliminate the classoverlap and the defective instances located in non defective ui .
oufsobujpobm pogfsfodf po vupnbufe 4pguxb sf ohjoffsjoh authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
overlapping space were not considered.
in this study we are committed to identify and process overlapping instances and propose an improved k meansclustering cleaning approach ikmcca to remove the over lapping instances.
our contribution is to investigate the impactof class overlap on software defect prediction and excavate how class overlap influence the prediction performance for both wpdp and cpdp.
so we have structured our workaccording to the following two research questions rq1 how does class overlap influence the prediction performance of within project defect prediction approaches?
rq2 how does class overlap influence the prediction performance of cross project defect prediction approaches?
we conduct experiments to investigate the impact of class overlap for wpdp and cpdp using overlapping data fromnasa aeeem relink softlab and morph.
the restof this study is organized as follows the next section iiprovides a review of related work on sdp.
section iii provides the methodology containing study questions methods to build models data and model evaluation.
section iv lists the resultsof study.
the discussions of our work are described in section v and the threats to validity of our work are analyzed in section vi.
we conclude the paper in section vii.
ii.
r elated work in this section we mainly discuss the related researches on the sdp and class overlap.
a. software defect prediction software defect prediction technology has been one of the most concerned research topics in software engineeringsince 1970s .
in recent years with the rapid development of machine learning various machine learning methods have been widely applied to improve the performanceof sdp.
according to whether the training and testing data come from the same project the sdp is divided into within projectdefect prediction wpdp and cross project defect prediction cpdp .
wpdp is to directly apply classifiers such as naivebayes nb random forests rf k nearestneighbor knn support vector machine svm and logistic regression lr to detect the defective modules.
cpdp is to find as many defective modules as possible in one project based on the learning model trained by otherprojects.
the most important problem of cpdp is the different distribution of training and testing data.
many researchers have presented a comparative research on cpdp methods .
zhou et al.
found that the simple module size model method had comparable or better predictive performance thanmost cross project defect prediction methods.
in our work we focus our discussions on six empirical studies which could achieve better performance.
detailed descriptions are shown as follows.
turhan et al.
chose similar instances as training set from different projects and using these similar instances totrain k nearest neighbor knn classifier.
nam et al.
used transfer component analysis tca method to maptraining and testing data into the common feature space.they conducted experiments on relink and aeeem datasets and proposed tca method for automatic data selectionstandardization.
chen et al.
integrated two levels of data transfer and proposed double transfer boosting dtb approach.
first they reshaped the whole distribution of cross company cc dataset by data gravitation to fit wp data.
then they eliminated negative instances in cc dataset by transferboosting with nb based classifier.
turhan et al.
foundmixed project prediction was reasonable in early phases of de velopment though studying the impact of mixed project dataseton binary defect prediction.
ryu et al.
discussed the classimbalance for the mixed project defect prediction mpdp and put forward the value cognitive boosting with support vector machine vcb svm approach.
their experimental resultsindicated that vcb svm could achieve better results when training data only contained fewer label instances of testing project.
xia et al.
proposed the hydra hybrid modelreconstruction approach including two phases of genetic and ensemble learning.
the phase of genetic algorithm was tobuild the optimal weight combination of multiple classifiers by genetic algorithm.
the phase of ensemble learning was to produce a classifier with good performance by adaboostmethod.
b. class overlap class overlap is that some instances in training data are close to or even overlap in the distribution space but havedifferent class.
these instances often lead to poor class boundary and hinder to build a good learning model with good performance .
in the field of sdp researchers consideredclass overlap problem as the data quality or noise detection.
tang et al.
proposed a clustering based noise detection method to remove noisy instances identified by the noise fac tor.
they conducted experiments on nasa with c4.
learner and the results indicated that removing noisy instances couldimprove the accuracy of classifier.
kim et al.
evaluated the effect of the dataset containing both false positive and false negative noise and proposed the closest list noise identification clni method to identify noise instances.
based on clni chen et al.
proposed neighborhood cleaning learning ncl to deal with both class overlap and class imbalance.
theexperimental results indicated that the new learning models could get best values in terms of g mean and auc compared with state of the art methods.
however they do not provide a comparison with cpdp models.
in our work we evaluate the impact of class overlapon both wpdp and cpdp.
iii.
m ethodology in this section we firstly provide the rationale to our studying questions.
then we report the experimental data.at last we introduce the methods under study and model authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
g55 g85 g68 g76 g81 g76 g81 g74 g3 g71 g68 g87 g68 g55 g72 g86 g87 g76 g81 g74 g71 g68 g87 g68 g53 g72 g80 g82 g89 g76 g81 g74 g3 g82 g89 g72 g85 g79 g68 g83 g3 g76 g81 g86 g87 g68 g81 g70 g72 g86 g55 g85 g68 g76 g81 g76 g81 g74 g3 g71 g68 g87 g68 g55 g72 g86 g87 g76 g81 g74 g71 g68 g87 g68 g54 g39 g51 g3 g80 g82 g71 g72 g79 g11 g53 g52 g20 g12 g3 g58 g51 g39 g51 g3 g80 g82 g71 g72 g79 g86 g11 g53 g52 g21 g12 g3 g38 g51 g39 g51 g3 g80 g82 g71 g72 g79 g86 g51 g85 g72 g71 g76 g70 g87 g51 g85 g72 g71 g76 g70 g87 g37 g68 g79 g68 g81 g70 g72 g36 g56 g38 g53 g72 g70 g68 g79 g79 g37 g68 g79 g68 g81 g70 g72 g36 g56 g38 g53 g72 g70 g68 g79 g79 g48 g82 g71 g72 g79 g3 g40 g89 g68 g79 g88 g68 g87 g76 g82 g81 g48 g82 g71 g72 g79 g3 g40 g89 g68 g79 g88 g68 g87 g76 g82 g81 g48 g82 g71 g72 g79 g3 g40 g89 g68 g79 g88 g68 g87 g76 g82 g81 g53 g72 g86 g88 g79 g87 g86 fig.
.
the overview of our model construction and evaluation approach.
evaluation.
figure provides an overview of the steps in our study.
a. research questions our study is to evaluate the effect of overlapping instances on sdp.
to this end our work checks the following two questions.
rq1 how does class overlap influence the prediction performance of existing within project defect prediction models?
rq2 how does class overlap influence the prediction performance of existing cross project defect prediction models?
the aim of rq1 and rq2 are to compare the performance of the existing state of the art learning models by removing overlapping instances against without removing under twoprediction cases wpdp and cpdp .
for rq1 we study nb rf svm lr and knn classifiers for wpdp models.
for rq2 we study nn filter tca vcb svm dtb mnb and hydra models for cpdp.
noted that all the above learning models were conducted on python .
and scikit learn .
.
.
since our works are to investigate the impactof class overlap on sdp the parameters of nb rf svm lr and knn classifiers are set as default parameters on scikitlearn .
.
and the parameters of cpdp learning models are set as the same as their studies.
if the learning models under removing overlapping instances are much better than these without removing overlappinginstances it would be a good option for practitioners to employ removing overlapping instances before building the models.in addition if the performance ranking of learning model ischanged it would be indicated that class overlap problem hasa greater impact on this learning model.
b. datasets in order to build and validate the effect of class overlap on software defect prediction we use projects fromnasa aeeem relink softlab and morph groups.
these datasets collected from these projects includes static code metrics and process metrics and the defect matchingwas relied on szz algorithm.
the metrics along with label information of these datasets are at different level function orclass or file granularity .
projects from nasa systems are nasa aerospace projects which were gathered as part of the metric data program mdp .
these projects are all only one version and their metricfor sdp are function granularity.
we only apply cm1 mw1 pc1 pc3 and pc4 projects as they were developed by clanguage and they have the common metrics.
projects from softlab systems are embedded controllers for household appliances from turkish software company.
these projects include ar1 ar3 ar4 ar5 and ar6developed by c language.
the metrics for these projects arealso function granularity.
projects from morph systems are collected by jureczko and spinellis including releases of open sourceprojects and student projects.
the metrics of these projects are class granularity and developed by java language.
in our experiment we choose one version for multi version project which were used in reference .
projects from aeeem and relink are developed by java language.
aeeem are collected by d ambros et al.
and relink are collected by wu et al.
.
the metrics in aeeem are class granularity and the metrics of relink arefile granularity.
table i shows the testing projects used in our experiment.
from this table we can observe that i the test projects are from different fields and are developed by different languages c or java ii the number of instances in the test projectsvary greatly and the percentage of defective instances are verylow leading to class imbalance.
these observations concludethat these test projects could be used to provide a fairevaluation of the impact of class overlap on software defect prediction.
c. methods to build the models in the field of sdp class overlap problem often is considered as the data quality or noise detection.
in our experiment we use kmcca ncl and our proposed ikmcca methods to remove the overlapping instances.
clni approach was presented by kim and used by chen simultaneously .
chen et al.
considered both classimbalance and class overlap in the same time.
so in ourexperiment we used the ncl proposed by chen.
they used authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i the experimental datasets .
group project langeage granularity number of metricsnumber of total instances o f defective instances nasacm1 c function37327 .
mw1 .
pc1 .
pc3 .
pc4 .
softlabar1 .
ar3 .
ar4 .
ar5 .
ar6 .
aeeemequinox framework eq javaclass .
eclipse jdt core jdt .
apache lucence lc .
mylyn ml .
eclipse pde ui pde .
relinkapache http server file .
openintents safe .
zxing .
morphant1.
class camel1.
.
poi1.
.
tomcat .
velocity1.
xalan2.
.
xerces1.
.
arc .
redktor .
skarbonka the neighborhood cleaning learn ncl rule to remove the overlapping instances.
it is noted that the non defective in stances only be removed to stress the class imbalance problem.
this is to say they searched the nearest neighbors of each defective instance and removed the nearest neighbors whoseclass were non defective.
kmcca approach was presented by tang and this was a clustering based noise detection approach based on kmeans.
firstly the k means algorithm was used to cluster the data into kclusters.
then for each cluster the noise factor values of each instance are computed.
finally the top p o f instances were removed.
in order to the same as the ncl we improved the kmcca method called ikmcca that considers the class imbalance at the same time.
in step of the removing overlapping instances the rule is based on the percentage of defective instances.
ifthe percentage of defective instances in the i thcluster is below the p the defective instance in this cluster are removed.
on the contrary the non defective instances in this clusterare removed.
pseudo code for ikmcca 1is presented in algorithm .
noted that we employ the log transformation before using the datasets which was studied by previous researches .the pseudo code for the simulation experiments are providedin algorithm for evaluating the impact of class overlap toanswer rq1 and rq2.
ii pseudo code for ikmcca.
algorithm pseudo code for ikmcca inputs traning data d the parameter m n the number of instances in d d the number of defective instances in d p d n k floorleftbign m floorrightbig using k means algorithm to divide d into k clusters fori k do compute the ratio r of defective instances to all instances in cluster i ifr p delete the non defective instances in cluster i else delete the defective instances in cluster i end if end for combine the remaining instances in each cluster d. model evaluation in order to investigate the impact of class overlap on learning models we use three performance measures including balance recall and area under the receiver operating characteristic curve auc .
balance bal is the balance between pdand pf.
the bigger the balvalue is the better the performance of learning model authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii pseudo code for the experimental setup .
algorithm pseudo code for the experimental setup inputs data1 cm1 mw1 pc1 pc3 pc4 data2 ar1 ar3 ar4 ar5 ar6 data3 eq jdt lc ml pde data4 apache safe zxing data5 ant1.
camel1.
poi1.
tomcat velocity1.
xalan2.
xerces1.
arc redktor skarbonka learning model wpdp nb svm knn rf lr cpdp nn filter tca vcb svm dtb mnb hydra d a t ai sd a t a 1o rd a t a 2o rd a t a 3o rd a t a or data5 methods kmcca ikmcca ncl fordata in data do fori 20do ifwpdp wptrain select of data test data wptrain end if ifcpdp ifmixed project wp select of a project test project wp cptrain data project wp else test a project in data cptrain data project end if end if employ log trainsform on wptrain cptrain test employ methods kmcca ikmcca ncl on wptrain and cptrain to remove overlapping instances formodel in learning model apply the learning model to train on wptrain or cptrain apply the model to predict the class on test and report the perftomance balance recall auc on test for each learning model end for end for end for is.
it is defined as bal radicalbig pd pf2 .
recall is the ratio of the number of correctly predicted defective instances to the total number of defective instances.the bigger the recall value is the better the performance of learning model is.
it is defined as recall tp tp fn.
auc is the area under a receiver operating characteristic roc curve.
the x axis of roc is pfand y axis of roc ispd.
the bigger the auc value is the better the performance of learning model is.
in these measures pd probability of detection is the ratio of correctly predicted defective instances to total defective instances.
pfis the fraction of incorrectly predicted defective instances to total non defective instances.
in order to make a statistical evaluation of the detailed prediction results the non parametric friedman test with thenemenyi test is used to compare multiple runs over the projects.
the confidence level is .
they are widely used in software defect prediction sdp .
firstly the fried man test is used to determine whether there are statistically significant differences among compared methods.
if there are significant differences in statistical data the differences willbe test by post hoc nemenyi test.
in addition to evaluate the degree of difference among the compared methods in terms of bal recall and auc results we apply cliff s delta to measure the effect size which don t need the compared vectors meet normality assumption.
this is calculated as followed cliff primesd e l t a x1 x x1 x n1n2.
wherex1andx2are scores within group performance values achieved by one compared method and group performance values achieved by another compared method and n1andn2are the sizes of the instances groups.
the cardinality symbol indicates the number.
the effect size of cliff s delta is divided into four levels d .
negligible n .
d .
small s .
d .
medium m and d .
large l .
iv .
e xpermental results in this section we report the experimental results for the comparison of removing overlapping instances with without removing overlapping instances to answer rq1 and rq2.
a. rq1 how does class overlap influence the prediction performance of existing within project defect prediction models?
for answering the question we replicate the experiments on the projects by nb rf svm knn and lr classifiers using ikmcca vs. ncl vs. kmcca vs. without removing data respectively.
in ikmcca method the percentage p i s set as the percentage of defective instances in the training data and the parameter mis set as that will be discussed in section v .
parameters of kmcca and ncl methods are setthe same as the