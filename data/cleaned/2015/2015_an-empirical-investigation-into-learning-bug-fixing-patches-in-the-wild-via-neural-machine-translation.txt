an empirical investigation into learning bug fixing patches in the wild via neural machine translation michele tufano college of william and mary williamsburg va usacody watson college of william and mary williamsburg va usagabriele bavota universit della svizzera italiana usi lugano switzerland massimiliano di penta university of sannio benevento italymartin white college of william and mary williamsburg va usadenys poshyvanyk college of william and mary williamsburg va usa abstract millionsofopen sourceprojectswithnumerousbugfixesareavailableincoderepositories.thisproliferationofsoftwaredevelopment histories can be leveraged to learn how to fix common programmingbugs.toexploresuchapotential weperformanempirical study to assess the feasibility of using neural machine translation techniquesforlearningbug fixingpatchesforrealdefects.wemine millionsofbug fixesfromthechangehistoriesofgithubrepositories to extract meaningful examples of such bug fixes.
then we abstract the buggy and corresponding fixed code and use them to train an encoder decoder model able to translate buggy code into itsfixedversion.ourmodelisabletofixhundredsofuniquebuggy methods in the wild.
overall this model is capable of predicting fixed patches generated by developers in of the cases.
ccs concepts software and its engineering software maintenance tools keywords neural machine translation bug fixes acm reference format michele tufano cody watson gabriele bavota massimiliano di penta martinwhite anddenysposhyvanyk.
.anempiricalinvestigationinto learningbug fixingpatchesinthewildvianeuralmachinetranslation.inproceedingsofthe201833rdacm ieeeinternationalconferenceonautomated software engineering ase september montpellier france.
acm newyork ny usa 6pages.
introduction localizingandfixingbugsisknowntobeaneffort proneandtimeconsuming task for software developers .
to support programmers in this activity researchers have proposed a number of approaches aimed at automatically repairing programs.
the proposedtechniqueseitheruseagenerate and validateapproach whichconsistsofgeneratingmanyrepairs e.g.
throughgenetic permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september montpellier france association for computing machinery.
acm isbn ... .
like genprog or an approach that produces asinglefix .whileautomatedprogramrepairtechniques stillfacemanychallengestobeappliedinpractice existingwork has made strides to be effective in specific cases.
these approaches given the right circumstances substantially contribute in reducing the cost of bug fixes for developers .
two major problems automated repair approaches have are producing patches acceptable for programmers and especially for generate and validate techniques over fitting patches to test cases.
tocopewiththisproblem le etal.
leveragethepasthistory ofexistingprojects intermsofbug fixpatches andcompare automatically generatedpatcheswithexistingones.patchesthat are similar to the ones found in the past history of mined projects are considered to be more relevant.
another approach that identifies patches from past fixes is prophet which after having localized the likely faulty code by running test cases generates patches from correct code using a probabilistic model.
our work is motivated by the following three considerations.
first automatedrepairapproachesarebasedonarelativelylimited andmanually crafted withsubstantialeffortandexpertise setof transformations or fixing patterns.
second the work done by leet al.
shows that the past history of existing projects can be successfullyleveragedtounderstandwhata meaningful program repairpatchis.third severalworkshaverecentlydemonstratedthe capability of advanced machine learning techniques such as deep learning to learn from large software engineering se datasets.
someexamplesofrecentmodelsthatcanbeusedinanumberof se tasks include code completion defect prediction bug localization clone detection code search learning api sequences or recommending method names .
forges like github provide a plethora of change history and bug fixing commits from a large number of software projects.
a machine learningbasedapproachcanleveragethisdatatolearn aboutbug fixingactivitiesinthewild.inthiswork weevaluatethesuitability ofa neural machine translation nmt based approach for the task of automatically generating patches for buggy code.
automatically learning from bug fixes in the wild provides the ability to emulate real patches written by developers.
additionally we harness the power of nmt to translate buggy code into fixed code thereby emulating the combination of ast operations performedinthedeveloperwrittenpatches.furtherbenefitsinclude thestaticnatureofnmtwhenidentifyingcandidatepatches since unlikesome generate and validateapproaches wedonot needtoexecute test cases during patch generation .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france m. tufano c. watson g. bavota m. di penta m. white d. poshyvanyk westart by mining a large set of 787k bug fixing commits fromgithub.fromthesecommits weextractmethod levelast edit operations using fine grained source code differencing .
we identifymultiplemethod leveldifferencesperbug fixingcommit andindependentlyconsidereachone yieldingto .3mbug fix pairs bfps .
after that the code of the bfps is abstracted to make it more suitable for the nmt model.
finally an encoder decoder modelisusedtounderstandhowthebuggycodeistransformedinto fixed code.
once the model has been trained it is used to generate patches for unseen code.
we empirically investigate the potential of nmt to generate candidate patches that are identical to the ones implemented by developers.
the results indicate that trained nmt model is able to successfullypredict thefixedcode given thebuggycode in of the cases.
approach fig.1showsanoverviewofthenmtapproachthatweexperiment with.
the black boxes represent the main phases the arrows indicatedataflows andthedashedarrowsdenotedependencieson externaltoolsordata.weminebug fixingcommitsfromthousands of github repos using github archive sec.
.
.
from the bug fixes weextract method levelpairs of buggyand correspondingfixedcode named bug fix pairs bfps sec.
.
.
.
bfps are the examplesthatweusetolearnhowtofixcodefrombug fixes buggy fixed .weusegumtree toidentifythelistofeditactions a performed between the buggy and fixed code.
then we use a java lexer and parser to abstract the source code of the bfps sec.
.
.
into a representation better suited for learning.
during the abstraction we keep frequent identifiers and literals we call idiomswithinthe representation.theoutput ofthisphase arethe abstractedbfpsandtheircorrespondingmapping m whichallows reconstructing the original source code.
next we filter out long methods sec.
.
.
andweusetheobtainedsettotrainanencoderdecoder model able to learn how to transform a buggycode into a corresponding fixedversion sec.
.
.
the trained model can be used to generate a patch for unseen buggy code.
.
bug fixes mining we downloaded from github archive every public github event between march and october and we used the google bigquery apis to identify all commits having a message containingthepatterns fix or solve and bug or issue or problem or error .weidentified 10m bug fixing commits.
as the content of commit messages and issue trackersmight imprecisely identify bug fixing commits two authors in dependently analyzed a statistically significant sample confidence level confidence interval for a total size of of identified commits to check whether they were actually bug fixes.
aftersolving13casesofdisagreement theyconcludedthat97.
of the identified bug fixing commits were true positive.
details about this evaluation are in our online appendix .
foreachbug fixingcommit weextractedthesourcecodebefore and after the bug fix using the github compare api .
this allowedustocollectthebuggy pre commit andthefixed postcommit code.wediscardedcommitsrelatedtonon javafiles aswellasfilesthatwerecreatedinthebug fixingcommit sincethere would be no buggy version to learn from.
moreover we discarded commits impacting more than five java files since we aim to learn focused bug fixes that are not spread across the system.
the result of this process was the buggy and fixed code of bug fixing commits.
.
bug fix pairs analysis a bfp bug fixing pair is a pair mb mf wherembrepresents a buggy code component and mfrepresents the corresponding fixedcode.wewillusethesebfpstotrainthenmtmodel makeit learningthetranslationfrombuggy mb tofixed mf code thus being able of generating patches.
.
.
extraction.
given fb ff apairofbuggyandfixedfilefrom a bug fix bf we used the gumtree spoon ast diff t o o lt o computetheastdifferencingbetween fbandff.thiscomputes the sequence ofedit actions performed at theast levelthat allows to transform the fb s ast into the ff s ast.
since the file level granularity could be too large to learn patterns of transformation we separate the code into method level fragments that will constitute our bfps.
the rationale for choosing method level bfps is supported by several reasons.
first methods represent a reasonable target for fixing activities since they are likely to implement a single task or functionality.
second methods provide enough meaningful context for learning fixes such as variables parameters andmethodcallsusedinthemethod.this choice is justified by recent empirical studies which indicated how the large majority of fixing patches consist of single line single churnor worstcases churnsseparatedbyasingleline .smaller snippets of code lack the necessary context and hence they could not be considered.
finally considering arbitrarily long snippets of code such as hunks in diffs makes learning more difficult given the variability in size and context .
we first rely on gumtree to establish the mapping between the nodes of fbandff.
then we extract the list of mapped pairs of methods l m1b m1f ... mnb mnf .
each pair mib mif contains the method mib from the buggy file fb and the correspondingmethod mif fromthefixedfile ff .next foreachpair ofmappedmethods weextractasequenceofeditactionsusingthe gumtree algorithm.
we then consider only those method pairs for whichthereisatleastoneeditaction i.e.
wedisregardmethods thathavenotbeenmodifiedduringthefix .therefore theoutputof this phase is a list of bfps bfp1 ... bfp k where each bfp is a tripletbfp mb mf a wherembis the buggy method mfis thecorrespondingfixedmethod and aisasequenceofeditactions that transforms mbinmf.
we exclude methods created deleted during the fixing since we cannot learn fixing operations from them.
overall we extracted .3m bfps.
itshouldbenotedthattheprocessweusetoextractthebfps i does not capture changes performed outside methods e.g.
class signature attributes etc.
and ii considers each bfp as an independent bug fix meaning that multiple methods modified in thesame bug fixing activity are considered independently from one another.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
an empirical investigation into learning bug fixing patches in the wild via nmt ase september montpellier france recurrent neural network rnn encoder decoder h1 h2 hn rnn cell gru x1 x2 end start y ymci s1 s2 sm rnn cell gru softmax abstractfabstractbencoder rnn attention decoder rnn .. .... ...... 787k bug fixesbug fixes mining githubarchive buggy code fixed codepublic mylist checklist mylist l if l.size populatelist l return l public mylist checklist mylist l if l.size populatelist l return l buggy code fixed codepublic mylist checklist mylist l if l.size populatelist l return l public mylist checklist mylist l if l.size populatelist l return l buggy code fixed codepublic mylist checklist mylist l if l.size populatelist l return l public mylist checklist mylist l if l.size populatelist l return l gumtree buggy code fixed codepublic mylist checklist mylist l if l.size populatelist l return l public mylist checklist mylist l if l.size populatelist l return l edit actions a delete literal at if insert literal at ifbuggy code fixed codepublic mylist checklist mylist l if l.size populatelist l return l public mylist checklist mylist l if l.size populatelist l return l edit actions a delete literal at if insert literal at ifbuggy code fixed codepublic mylist checklist mylist l if l.size populatelist l return l public mylist checklist mylist l if l.size populatelist l return l edit actions a update literal at if .3m bug fix pairsabstraction antlr lexer buggy code fixed codepublic mylist checklist mylist l if l.size populatelist l return l public mylist checklist mylist l if l.size populatelist l return l edit actions a delete literal at if insert literal at if var 2 tmp method 1 isvalid mapping m buggy code fixed codepublic mylist checklist mylist l if l.size populatelist l return l public mylist checklist mylist l if l.size populatelist l return l edit actions a delete literal at if insert literal at if var 2 tmp method 1 isvalid mapping m abstract buggy code abstract fixed codepublic type 1 method 1 type 1 var 1 if var 1 .
size method 2 var 1 return var 1 public type 1 method 1 type 1 var 1 if var 1 .
size method 2 var 1 return var 1 edit actions a update literal at if var 1 l method 1 checklistmapping m .3m abstract bug fix pairsjavaparser parser recurrent neural network rnn encoder decoder h1 h2 hn rnn cell gru x1 x2 end start y ymci s1 s2 sm rnn cell gru softmax abstractb buggy abstractf fixed encoder rnn attention decoder rnn .. .... ...... 6idioms ij size ... transformation pairs analysis datasets small bug fix pairs7 figure overview of the process used to experiment with an nmt based approach.
.
.
abstraction.
learning bug fixing patterns is extremely challengingbyworkingatthelevelofrawsourcecode.thisisespecially due to the huge vocabulary of terms used in the identifiers and literals of the 2m mined projects.
such a large vocabulary would hinder our goal of learning transformations of code as a neural machine translation task.
for this reason we abstract the code and generateanexpressiveyetvocabulary limitedrepresentation.we use a java lexer and a parser to represent each buggy and fixed method within a bfp as a stream of tokens.
the lexer built on top ofantlr tokenizestherawcodeintoastreamoftokens that isthen fedinto ajava parser which discernsthe roleof each identifier i.e.
whether it represents a variable method or type name and the type of a literal.
each bfp is abstracted in isolation.
given a bfp bfp mb mf a we first consider the source code of mb.
the source code is fed to a java lexer producing the stream of tokens.
the stream of tokens is then fed to a java parser which recognizes the identifiers and literals in the stream.
the parser generates and substitutes a unique id for each identifier literal within the tokenizedstream.ifanidentifierorliteralappearsmultipletimesin the stream it will be replaced with the same id.
the mapping of identifiers literalswiththeircorrespondingidsissavedinamap m .
the final output of the java parser is the abstracted method abstract b .
then we consider the source code of mf.
the java lexerproducesastreamoftokens whichisthenfedtotheparser.
the parser continues to use a map mwhen abstracting mf.
the parser generates new ids only for novel identifiers literals notalreadycontainedin m meaning theyexistin mfbutnotin mb.
then itreplaces allthe identifiers literalswith thecorresponding ids generating the abstracted method abstract f .
the abstracted bfp is now a tuple bfpa abstract b abstract f a m where mistheidmappingforthatparticularbfp.theprocesscontinues considering the next bfp generating a new mapping m. note that we first analyze the buggy code mband then the corresponding fixed code mfof a bfp since this is the direction of the learning process.
ids are assigned to identifiers and literals in a sequential and positionalfashion thefirstmethodnamefoundwillbeassigned the id of method 1 likewise the second method name will receive the id of method 2 .
this process continues for all the method and variable names var x as well as the literals string x int x float x .atthispoint abstract bandabstract fofabfpareastreamoftokensconsistingoflanguagekeywords e.g.
for if separators e.g.
and ids representing identifiers and literals.
comments and annotations have been removed from the code representation.
some identifiers and literals appear so often in the code that for the purpose of our abstraction they can almost be treated as keywords of the language.
this is the case for the variables i j o r index thatareoftenusedinloops orforliteralssuchas oftenusedinconditionalstatementsandreturnvalues.similarly method names such as sizeoradd appear several times in our codebase sincetheyrepresentcommonconcepts.theseidentifiersand literals are often referred to as idioms .
we include idioms inourrepresentationanddonotreplaceidiomswithagenerated id but rather keep the original text when abstracting the code.
todefinethelistofidioms wefirstrandomlysampled300kbfps andconsideredalltheiroriginalsourcecode.then weextracted the frequency of each identifier literal used in the code discarding keywords separators and comments.
next we analyzed the distributionofthefrequenciesandfocusedonthetop0 .
frequent words outliersofthedistribution .twoauthorsmanuallyanalyzed thislistandcuratedasetof272idiomsalsoincludingstandardjava types such as string integer common exceptions etc.the list of idioms is available in the online appendix .
thisrepresentationprovidesenoughcontextandinformation toeffectivelylearncodetransformations whilekeepingalimited vocabulary v .
the abstracted code can be mapped back to the real source code using the mapping m .
buggy code fixed code bug fix abstracted buggy code abstracted fixed code abstracted buggy code with idioms abstracted fixed code with idioms learningpublic integer getminelement list mylist if mylist.size return listmanager.getfirst mylist return public integer getminelement list mylist if mylist.size return listmanager.min mylist return null public type 1 method 1 type 2 var 1 if var 1 .
method 2 int 1 return type 3 .
method 3 var 1 return int 1 public type 1 method 1 list var 1 if var 1 .
size return type 2 .
method 3 var 1 return public type 1 method 1 type 2 var 1 if var 1 .
method 2 int 2 return type 3 .
method 4 var 1 return null public type 1 method 1 list var 1 if var 1 .
size return type 2 .
min var 1 return null figure code abstraction example.
to better understand our representation let us consider the example in fig.
where we see a bug fix related to finding the minimum value in a list of integers.
the buggy method contains threeerrors which thefixedcoderectifies.the firstbugiswithin authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france m. tufano c. watson g. bavota m. di penta m. white d. poshyvanyk theif condition wherethebuggymethodchecksifthelistsizeis greaterthanorequalto .thisisproblematicsincealistwithout anyvaluescannothaveaminimumvaluetoreturn.thesecondbug is in the method call getfirst this will return the first element in thelist whichmayormaynotbetheminimumvalue.lastly ifthe if conditionfailsinthebuggymethodthenthemethodreturns returning 0whentheminimumisunabletobeidentifiedisincorrect asitindicatesthatoneoftheelementswithinthelistis .thefixed code changes the if condition to compare against a list size of ratherthan usesthe minmethodtoreturntheminimumvalue and changes the return value to nullwhen the if condition fails.
usingthebuggyandfixedcodefortraining althoughaviable andrealisticbug fix presentssomeissues.whenwefeedthebuggy piece ofcode tothe java parserand lexer weidentify someproblems with the mapping.
for example the abstracted fixed code contains int 2andmethod 4 whicharenotcontainedintheabstractedversionofthebuggycodeoritsmapping.sincethemappingoftokenstocodeissolelyreliantonthebuggymethod this example would require the synthesis of new values for int 2and method 4 .however themethodologytakesadvantageofidioms allowingtostillconsiderthisbfp.whenusingtheabstractionwith idioms weareabletoreplacetokenswiththevaluestheyrepresent.
now when looking at the abstracted code with idioms for bothbuggy and fixed code there are no abstract tokens found in thefixed code that are not in the buggy code.
previously we needed to synthesize values for int 2andmethod 4 however int 2was replaced with idiom 1andmethod 4 with idiom min.
with the use of idioms we are capable of keeping this bfp while maintaining the integrity of learning real developer inspired patches.
.
.
filtering.
wefilteroutbfpsthat i containlexicalorsyntactic errors i.e.
either the lexer or parser fails to process them in either the buggy or fixed code ii their buggy and fixed abstractedcode abstract b abstract f resultedinequalstrings iii performed more than100 atomicast actions a betweenthe buggy andfixedversion.therationalebehindthelatterdecisionwasto eliminate outliers of the distribution the 3rd quartile of the dis tribution is actions which could hinder the learning process.
moreover we do not aim to learn such large bug fixing patches.
next we filter the bfps based on their size measured in the numberoftokens.wedecidedtodisregardlongmethods longer than50tokens andfocusedonsmallsizebfps.we therefore create the dataset bfp small bfp .
.
.
synthesis of identifiers and literals.
bfpsaretheexamples we use to make our model learn how to fix source code.
given a bfp mb mf a we first abstract its code obtaining bfpa abstract b abstract f a m .thebuggycode abstract bisusedas inputtothemodel whichistrainedtooutputthecorresponding fixed code abstract f. this output can then be mapped back to real source code using m. intherealusagescenario whenthemodelisdeployed wedo nothaveaccesstotheoracle i.e.
fixedcode abstract f butonlyto the input code.
this source code can then be abstracted and fed to the model which generates as output a predicted code abstract p .
the ids that the abstract pcontains can be mapped back to real valuesonlyiftheyalsoappearintheinputcode.ifthefixedcode suggests to introduce a method call method 6 which is not foundin the input code we cannot automatically map method 6 to an actual method name.
this inability to map back source code exists foranynewlycreatedidgeneratedforidentifiersorliterals which are absent in the input code.
therefore it appears that the abstraction process which allows ustolimitthevocabularysizeandfacilitatethetrainingprocess confinesustoonlylearningfixesthatre arrangekeywords identifiers and literals already available in the context of the buggy method.thisistheprimaryreasonwedecidedtoincorporateidioms in our code representation and treat them as keywords of the language.
idioms help retaining bfps that otherwise would be discarded because of theinability tosynthesize newidentifiers or literals.thisallows themodeltolearnhow toreplaceanabstract identifier literalwithanidiomoranidiomwithanotheridiom e.g.
bottom part of fig.
.
after these filtering phases the datasets bfp smallis comprised of 58k bug fixes.
.
learning patches .
.
dataset preparation.
given a set of bfps i.e.
bfp small we use the instances to train an encoder decoder model.
given abfpa abstract b abstract f a m we use only the pair abstract b abstract f of buggy and fixed abstracted code for learning.
no additional information about the possible fixing actions a is provided during the learning process to the model.
the given set of bfps is randomly partitioned into training validation and test sets.
before the partitioning we make sure to removeanyduplicatedpairs abstract f abstract b tonotbiasthe results i.e.
same pair both in training and test set.
.
.
nmt.
theexperimentedmodelisbasedonanrnnencoderdecoder architecture commonly adopted in nmt .
this model consists of two major components an rnn encoder which encodesa sequence of terms xinto a vector representation and an rnn decoder which decodesthe representation into another sequenceofterms y.themodellearnsaconditionaldistribution over a output sequence conditioned on another input sequence of terms p y1 .. ym x1 .. xn wherenandmmay differ.
in our case givenaninputsequence x abstract b x1 .. xn andatarget sequence y abstract f y1 .. ym the model is trained to learn the conditional distribution p abstract f abstract b p y1 .. ym x1 .. xn wherexiandyjareabstractedsourcetokens java keywords separators ids and idioms.
fig.
1shows the architectureoftheencoder decodermodelwithattentionmechanism .
the encoder takes as input a sequence x x1 .. xn and produces a sequence of states h h1 .. hn .
we rely on a bi directional rnn encoder which is formed by a backward andaforwardrnn whichareabletocreaterepresentationstaking into account both past and future inputs .
that is each state hi represents the concatenation dashed box in fig.
of the states producedbythetwornnswhenreadingthesequenceinaforward and backward fashion hi .
the rnn decoder predicts the probability of a target sequence y y1 .. ym given h. specifically the probability of each outputterm yiiscomputedbasedon i therecurrentstate siinthe decoder ii the previous i terms y1 .. yi and iii a context vector ci.
the latter constitutes the attention mechanism.
the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
an empirical investigation into learning bug fixing patches in the wild via nmt ase september montpellier france vectorciis computed as a weighted average of the states in h ci summationtext.1n t 1aithtwhere the weights aitallow the model to pay moreattention to different parts of the input sequence.
specifically the weight aitdefines how much the term xishould be taken into account when predicting the target term yt.
the entire model is trained end to end encoder and decoder jointly by minimizing the negative log likelihood of the target terms using stochastic gradient descent.
.
.
hyperparameter search.
forthemodelbuiltonthe bfp small dataset i.e.
msmall we performed hyperparameter search by testing ten configurations of the encoder decoder architecture.
the configurations testeddifferentcombinations of rnn cells lstm and gru number of layers and units fortheencoder decoder andtheembeddingsize .bucketing and padding was used to deal with the variable length of the sequences.wetrainedourmodelsforamaximumof60kepochs andselectedthemodel scheckpointbeforeover fittingthetraining data.to guidetheselectionof thebestconfiguration weusedthe lossfunctioncomputedonthe validation set notonthetestset whiletheresultsarecomputedonthe testset.alldataareavailable in our online appendix .
experimental design thegoalof this study is to empirically assess whether nmt can beused tolearn fixesinthe wild.the contextconsistsof adataset of bug fixes sec.
section and aims at answering the following research question.
.
rq is neural machine translation a viable approach to learn how to fix code?
weaimtoempiricallyassessingwhethernmtisaviableapproach tolearntransformationsofthecodefromabuggytoafixedstate.
to this end we use the dataset bfp smallto train and evaluate the nmt model msmall.
precisely given a bfp dataset we train differentconfigurationsoftheencoder decodermodels thenselect thebestperformingconfigurationonthevalidationset.wethen evaluate the validity of the model with the unseen instances of the test set.
the evaluation is performed as follows let mbe the trained model and tbe the test set of bfps bfp small we evaluate the modelmfor eachbfp abstract b abstract f t. specifically wefeedthebuggycode abstract btothemodel m whichwillgenerateasinglepotentialpatch abstract p.wesaythatthemodelgeneratedasuccessfulfixforthecodeifandonlyif abstract p abstract f. we report the raw count and percentage of successfully fixed bfps in the test set.
results .
rq is neural machine translation a viable approach to learn how to fix code?
whenperformingthehyperparametersearch wefoundthatthe configuration which achieved the best results on the validation set was the one with layer bi directional encoder layer attention decoderbothwith256units embeddingsizeof512 andlstm rnn cells.
we trained the msmallmodel for 50k epochs.the model was able to successfully generate a fix for out of cases .
of the bfps in the test set by translating the buggy code in the corresponding fixed code.
while the number of successful fixes might appear relatively small it is important to note that these fixes are generated with a single guessof the model asopposedtopreviousapproachesthatgeneratemanypotential patches.moreover itisworthnotingthatallbfpsinthetestsets areuniqueandhaveneverbeenseenbeforebythemodelduringthe trainingorvalidationsteps.allthepatchesgeneratedbythemodel can be mapped to concretesource code by replacing the ids in the abstractcodetotheactualidentifiersandliteralsvaluesstoredin the mapping m. threats to validity construct validity.
to have enough training data we mined bugfixes in github repositories rather than using curated bug fixdatasets such as defects4j or introclass useful but very limited in size.
to mitigate imprecisions in our datasets we manuallyanalyzedasampleoftheextractedcommitsandverifiedthat they were related to bug fixes.
internal.
it is possible that the performance of our model depends on the hyperparameter configuration.
we explain in section2.
.3how hyperparameter search has been performed.
external.
wedidnotcomparenmtmodelswithstate of thearttechniquessupportingautomaticprogramrepairsinceourmain goal was not to propose a novel approach for automated program repair butrathertoexecutealarge scaleempiricalstudyinvestigating the suitability of nmt for generating patches.
additional stepsareneededtoconvertthemethodologyweadoptedintoan end to end working tool such as the automatic implementation of the patch the running of the test cases checking its suitability etc.
this is a part of our future work agenda.
weonlyfocusedonjavaprograms.however thelearningprocessislanguage independent andthewholeinfrastructurecanbe instantiated for different programming languages by replacing the lexer parser and ast differencing tools.
weonlyfocused onsmall sizedmethods.we reachedthisdecisionafteranalyzingthedistributionoftheextractedbfps balancingtheamountoftrainingdataavailableandthevariabilityinsentence length.
conclusions we presented an empirical investigation into the applicability of nmt for the purpose of learning how to fix code from real bugfixes.wefirstdevisedanddetailedaprocesstomine extract and abstract the source code of bug fixes available in the wild in order toobtain method levelexamples ofbug fixes which wecallbfp.
then wesetup trained andtunednmtmodelsto translatebuggy code into fixed code.
we found that our model is able to fix a large number of unique bug fixes accounting for of the used bfps.
this study constitutes a solid empirical foundation upon which other researchers could build and appropriately evaluate program repair techniques based on nmt.