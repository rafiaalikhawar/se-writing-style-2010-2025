pattern based mining of opinions in q a websites bin lin fiorella zampetti gabriele bavota massimiliano di penta and michele lanza software institute universit della svizzera italiana usi switzerland university of sannio italy abstract informal documentation contained in resources such as q a websites e.g.
stack overflow is a precious resource for developers who can find there examples on how to use certain apis as well as opinions about pros and cons of such apis.
automatically identifying and classifying such opinions can alleviate developers burden in performing manual searches and can be used to recommend apis that are good from some points of view e.g.
performance or highlight those less ideal from other perspectives e.g.
compatibility .
we propose pome pattern based opinion miner an approach that leverages natural language parsing and pattern matching to classify stack overflow sentences referring to apis according to seven aspects e.g.
performance usability and to determine their polarity positive vsnegative .
the patterns have been inferred by manually analyzing sentences from stack overflow linked to a total of apis.
we evaluated pome by i comparing the pattern matching approach with machine learners leveraging the patterns themselves as well as n grams extracted from stack overflow posts ii assessing the ability of pome to detect the polarity of sentences as compared to sentiment analysis tools iii comparing pome with the state of the art stack overflow opinion mining approach opiner through a study involving human evaluators.
our study shows that pome exhibits a higher precision than a state of the art technique opiner in terms of both opinion aspect identification and polarity assessment.
i. i ntroduction online discussions among software developers through various communication channels e.g.
mailing lists issue trackers and above all question answer q a forums such as stack overflow are playing a major and increasing role in software development.
such sources bring various pieces of information including examples of how to use programming language constructs apis or frameworks and discussions about design choices or algorithmic solutions to certain development problems.
to cope with the limited search capabilities of q a forums and other forges and to alleviate developers burden of manually searching for relevant information researchers have proposed a wide variety of recommender systems.
such recommending systems can for example link stack overflow discussions to code snippets produce documentation enhance existing documentation by mining stack overflow discussions or identify insights about apis .
naturally developers discussions contain opinions e.g.
whether a certain api is suitable for solving a given problem or what the pros and cons of a given framework are.
for example some developers might recommend an api for its rich functionality while others may warn about its performance.
recommenders could therefore exploit such opinions i.e.
perform opinion mining and suggest apis that best satisfy the developers needs which can be better functionality better performance increased compatibility ease of use etc.in natural language processing nlp opinion mining has been used in various contexts e.g.
e commerce movie streaming to analyze users moods and feelings about a given product expressed in a review written in natural language.
sentiment analysis is a frequently used opinion mining technique.
its goal is to identify affective states and subjective opinions reported in sentences.
in its basic usage scenario sentiment analysis is used to classify customers written opinions as negative neutral or positive.
sentiment analysis has been used in software engineering for various purposes such as assessing the polarity of apps reviews developers distress or happiness or identifying negative opinions about apis .
recent work has shown that out of the box sentiment analysis tools are particularly unreliable and very often in disagreement when applied to software engineering corpora .
even customized tools e.g.
sentistrength se or retrained tools produced results inadequate in practice for tasks such as api opinion mining.
a recent work by uddin and khomh dealt with api opinion mining by relying on a keyword matching approach with a customized sentiment orientation algorithm .
stemming from the positive and negative results highlighted in previous attempt to automatically mine api opinions and from the seminal work by uddin and khomh in this field we propose a novel approach named pome which leverages linguistic patterns contained in stack overflow sentences referring to apis and classify whether i a sentence refers to a particular api aspect functional documentation community compatibility performance reliability or usability and ii it has a positive or negative polarity.
the main idea behind pome is to identify whether an api relevant sentence from stack overflow discussions matches any of the manually defined patterns.
each pattern consists of a natural language parse tree where each leaf can either be a generic part of speech e.g.
a noun or in some cases a specific part of speech taken from a thesaurus we have built characterizing an aspect positively or negatively.
we have evaluated our approach along three dimensions we assess the precision and recall of pome in identifying api related opinions in stack overflow on a manually labeled dataset of sentences.
we compare different variants of pome based on simple pattern matching as well as on machine learning algorithms finding that its best configuration achieves a precision ranging between .
and .
and a recall ranging between .
and .
depending on the quality aspect subject of the opinion.
ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
we compare the performance of the opinion polarity assessment when using pattern matching with six sentiment analysis tools finding that the defined patterns help in achieving higher values of precision recall both for positive .
precision and .
recall and for negative .
precision and .
recall opinions.
we conducted a survey with cs students and professional developers to collect their assessment about the precision of the opinions mined by pome and by the state of the art opinion mining tool opiner for four popular apis.
the achieved results show that for most of the quality aspect categories e.g.
usability pome is able to mine opinions with a higher precision than opiner.
we release pome s source code the web app used to label patterns and the list of patterns we manually defined and all the data used in our evaluations in a replication package .
ii.
r ela ted work we detail related work concerning i recommendation of documentation and ii applications of sentiment analysis and opinion mining for software engineering problems.
a. recommendation of formal and informal documentation treude and robillard proposed an approach to identify api library insights in stack overflow.
we detail this work in section iii since pome uses a slightly modified reimplementation of this approach for tracing stack overflow sentences onto apis.
however pome also provides positive or negative opinions about a specific aspect of the library.
wong et al.
proposed a uto comment to mine comments from stack overflow and automatically describe source code similar to snippets discussed on stack overflow.
differently from a uto comment we focus on mining opinions about apis rather than code snippet descriptions.
chatri and robillard developed an approach for identifying relevant portions of documentation to support developers seeking information.
other work focused on suggesting relevant documents discussions and code samples from the web to fill the gap between the ide and the web browser.
subramanian et al.
proposed b aker to enhance api documentation by linking to it relevant code examples.
among the various resources available on the web q a websites and in particular stack overflow have been the basis of many recommender systems .
our approach is an add on to these recommenders as it could be used to tag apis in recommended snippets with quality badges mined from developers opinions.
b. sentiment analysis opinion tools and their application to software engineering problems sentiment analysis has been used in various areas of se for example to analyze the sentiment of commit comments in github the correlation between the sentiment in 560k jira comments and the time to fix a jira issue or howthe sentiment of developers is affected by the result of a build process in continuous integration .
besides the analysis of developers behaviors sentiment analysis has been used to analyze users or developers opinions about software applications.
specifically guzman et al.
used sentiment analysis to classify tweets related to software projects .
several authors have used sentiment analysis to support the evolution of mobile applications by prioritizing user reviews based on the expressed sentiment.
the most adopted sentiment analysis tool in se is s entistrength which is based on a sentiment word strength list plus some heuristics including spell checking and negation handling.
its word lists are based on comments taken from myspace.com making it unsuitable for se applications.
nltk is a lexicon and rule based sentiment analysis tool leveraging v ader v alence aware dictionary and sentiment reasoner which is tuned to social media text especially micro blogging .
differently s t anford core nlp leverages a recursive neural network rnn and is able to compute the sentiment of a sentence based on how words compose the meaning of the sentence and not by summing up the sentiment of individual words.
however s tanford core nlp was trained on movie reviews a non se domain.
since most existing sentiment analysis tools were not conceived to be applied on se artifacts researchers posed questions on their applicability in this domain.
tourani et al.
found that s enti strength achieves a very low precision when analyzing mailing lists.
novielli et al.
also highlighted the challenges of employing sentiment analysis techniques to assess the affective load of text containing technical lexicon.
jongeling et al.
conducted a comparison of four widely used sentiment analysis tools and found none of them can provide accurate predictions in the se domain.
to overcome limitations of existing tools islam and zibran developed s enti strength se which includes word lists and heuristics specific for se documents.
calefato et al.
proposed e motxt to recognize specific emotions in se datasets such as stack overflow and jira.
also calefato et al.
developed s enti 4sd a sentiment analysis tool trained on stack overflow and leveraging lexicon and keywordbased features as well as word embedding.
de la mora and nadi proposed the use of metric based comparison to support developers in selecting the library to use.
our work aims at mining api opinions from stack overflow thus being complementary to the quantitative metrics used in .
the closest work to ours is opiner by uddin and khomh able to detect the polarity of sentences related to apis by customizing the sentiment orientation algorithm .
the algorithm was originally developed to mine customers opinions on computer products.
uddin and khomh customized the tool with words specific to api reviews.
opiner classifies the mined opinions into aspects by exploiting machine learning classifiers using the frequencies of single words and n grams appearing in the sentences as predictor variables.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
stackoverflow developer10 front endmaven api miner database2fine grained linker3 45polarity analyzer aspect classi fier67 fig.
.
the pome architecture.
we took inspiration from this work.
indeed the goal of pome is the same of opiner.
we aim at automatically mine opinions about apis in stack overflow discussions and classify them based on their aspect and sentiment polarity.
however we show that a simpler pattern based approach can provide a substantial step ahead in the accuracy of the opinions mined from stack overflow.
we also show that pattern matching is more precise than sentiment analysis tools for identifying the polarity of opinionated sentences.
iii.
pome fig.
depicts pome s architecture.
the dashed arrows represent dependencies while full arrows indicate flows of information pushed from one component to another.
arrows depicted in red indicate operations performed at the beginning and then refreshed periodically with the goal of storing crowdsourced opinions about apis the black arrows represent actions triggered by a pome user from the front end.
the api miner extracts all available java apis from the maven central repository 1in fig.
.
we select for each api its i name ii description iii link to the jar of the latest version and iv release date of the jar .
we collected this information for a total of apis between may and june storing it in our database .
the fine grained linker mines stack overflow discussions to establish links between the apis stored in the database and relevant sentences in stack overflow discussions .f o r example the sentence apache commons io is the straightforward solution to programmatically copy files is linked to the commons io library by using an approach built on top of the one proposed by treude and robillard .
knowing the sentences related to an api the aspect classifier categorizes the semantic content of each sentence in one of the eight aspects described in table iii and adds this information to the database .
the sentences not classified as none i.e.
those discussing quality aspects relevant to mined opinions about apis are then analyzed by the polarity analyzer that identifies the sentiment they express and consequently their polarity i.e.
positive ornegative we ignore sentences having a neutral sentiment since they are not of interest when mining opinions and stores this information in the database .
t o use pome a dev eloper interested in accessing opinions about an api can submit a textual query through the web based front end .
she can search for a specific api or if she does not know which api to use the query can be used to describe the task she wants to perform e.g.
reading json files in java .
this information is provided to a web service to identify the most relevant apis for the given query and provide as output the opinions mined for them.
in the following we detail the main pome components.
the api miner is not described since it is a simple python web scraper to mine the maven central repository.
a. fine grained linker this component retrieves sentences from stack overflow posts related to a given api.
given an api e.g.
googlegson we use the information collected by the api miner to download itsjar .
using java reflection we extract the complete list of its classes and methods and link sentences in stack overflow discussions to apis using a reimplementation of the linker by treude and robillard .
there are two differences between our approach and the one by treude and robillard .
first while they use the stack overflow api to retrieve the stack overflow discussions we rely on the december official stack overflow data dump to avoid issues related to usage limitations of the api.
second they use the first three regular expressions reported in table i to identify sentences including i the fully qualified api type e.g.
com.google .code .gson ii the non qualified api type e.g.
gson and iii the link to the official api documentation e.g.
in our approach we also retrieve stack overflow sentences matching the fourth regular expression shown in table i. we decided to include this fourth regular expression since we observed that many sentences on stack overflow discuss issues related to apis by referring to specific apis rather than to the api type i.e.
name or to its documentation.
while this additional regular expression might introduce false positives matching both the class name and the method name mitigates this risk.
we discuss the precision of this additional regular expression in section vi.
we use the fine grained linker to identify all relevant sentences for a given api only from stack overflow answers i.e.
we do not consider questions because opinions are unlikely to reside in the questions where users mostly ask for help.
also we discard sentences belonging to questions posted before the release date of the library jar under analysis to reduce the risk of mining opinions referring to old releases of the library.
the sentences identified by the fine grained linker along with the link to the respective api are stored in the pome s database for all previously mined apis.
b. aspect classifier the aspect classifier analyzes the stored sentences to identify the quality aspect s discussed in them.
in the following we discuss different ways to perform this task while in section iv we explain how we identified the best solution.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
t able i regular expressions for extracting api rela ted sentences in stac k overflow answers .
regular expression description is case sensitive?
?i .
bpackagename .typename b. fully qualified api type .
typename .
non qualified api type check .
a .
href.
packagename typename .html.
.
a .
link to the api official documentation check .
classname .methodname reference to a method of a specific class check t able ii da t aset used for p a tterns definition and training of the machine learning algorithms .
category sentences sentencesurllinked validated bytecode apis goo.gl rzoqc7 embedded sql db goo.gl kknzvd http clients goo.gl b8vgqn json apis goo.gl 9cas1c reflection apis goo.gl 6935xc ssh apis goo.gl 2ih4h6 overall pattern matching the conjecture is that users providing opinions about apis on stack overflow tend to use repetitive discourse patterns that can be encoded to capture both the quality aspect s and the sentiment of the opinion thus pattern matching can be used in the context of the polarity analyzer .
to identify the patterns we manually analyzed stack overflow sentences identified by the fine grained linker as related to apis belonging to the six categories of popular apis provided by maven central reported in table ii.
table ii reports the name of the category the number of apirelated sentences extracted from stack overflow discussions the number of sentences we manually analyzed and the link to maven central listing the apis belonging to the specific category.
from each category we only extracted sentences related to the five most used apis listed on for categories having more than linked sentences we manually analyzed only a randomly selected subset to avoid bias in the definition of the patterns i.e.
extract patterns that are very specific to one predominant api category in our dataset .
the sentences have been manually analyzed by four of the authors from now on evaluators to categorize each one as expressing or not an opinion about the linked api.
each sentence was randomly assigned to two of the four evaluators resulting in similarequal2 sentences per evaluator.
in case a sentence did not report any opinion we assigned the none label.
if an opinion was identified the evaluator firstly selected the part of the sentence reporting the opinion.
then she classified the selected part of the sentence in terms of the quality aspect s the opinion refers to e.g.
compatibility .
no predefined list of quality aspects was provided.
however every time the evaluator had to analyze a sentence the web application showed the list of quality aspects created so far allowing the evaluator to select one of the already defined aspects.
in a context like the one encountered in this work where the number of possible quality aspects might be large such a choice helps using consistent naming without introducing a substantial bias.
table iii presentsthe list of aspects obtained during the labeling process.
the evaluator also assigned a negative orpositive sentiment to the reported opinion this information is used in the context of the polarity analyzer and finally she identified in the selected part of the sentence the parts of speech pos referring to the linked api and the quality aspect s i.e.
noun adjective etc.
to better understand the process let us discuss an example of manual analysis.
consider the sentence based on my personal experience gson is the fastest library out there .
first the evaluator selects the part reporting the opinion in this case gson is the fastest library .
then she assigns the performance quality aspect and a positive sentiment to it.
finally she marks gson as a proper noun referring to the library and fastest as an adjective related to the quality aspect assigned to the opinion i.e.
performance .
once each sentence was manually analyzed by any two of the evaluators we solved all conflicts by adding a third evaluator not previously involved in the analysis of that sentence.
a conflict could be related to i the part of the sentence selected as opinion ii the sentiment assigned to the opinion and iii the quality aspect s identified.
the output includes sentences classified as reporting an opinion and referring to seven different quality aspects and discarded as not discussing quality aspects .
table iii reports the number of positive and negative opinions identified for each of them.
about of the linked sentences explicitly report negative or positive opinions related to one of the quality aspects.
while the percentage might look low if we consider the number of posts on stack overflow 50m at the date of the writing the amount of opinions is still impressive.
the api related sentences manually annotated have been exploited to identify recurrent patterns used in stack overflow discussions for expressing opinions about apis.
with patterns we refer to lexical rules that capture the syntax and semantic of the opinionated sentences.
one of the evaluators conducted a pilot study using api related sentences including opinions about performance .
since we wanted to define patterns considering both the syntax and the semantic of the api related sentences the evaluator working on the patterns extraction not only had the quality aspect and the sentiment assigned to each sentence as information but also the parts of speech related to each token i.e.
noun verb adjective adverb etc.
as well as their syntactic dependencies.
to reduce the number of patterns belonging to the same quality aspect the evaluator could also create a bag of words related to verbs adjectives and adverbs and use them for defining patterns.
a positive pattern belonging to the performance category is shown in the following.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
t able iii aspects used by the aspect classifier and sentences identified for each of them during manual anal ysis .
quality aspect the sentence talks about... opinions negative positive community the activities of the community maintaining the api e.g.
is the api actively maintained?
compatibility the compatibility of the api with respect to specific platforms programming languages or other apis documentation the content quality of the api documentation functional the features offered not offered by the api performance the performance of the api e.g.
speed memory footprint reliability the reliability of the api e.g.
whether it is buggy or not usability the usability of the api in terms of how easy is to use adapt it and evolve maintain the code using it none none of the above listed aspects quality aspect sentiment performance positive rule dependency requirement should be the first parent node of with a pos tag of verb.
example gson the library out there.
parsed syntactic dependencies gson propnis verbthe detfastest adjlibrary nounout advthere advnsubj amoddetattr advmodadvmod the pos adjective performance includes positive adjectives linkable to performance such as fastest performant etc.
once the pilot study was completed the evaluator trained other three evaluators in a minute session that involved discussing the results and some ambiguous sentences.
the api related sentences belonging to the other six quality aspects were randomly distributed among the four evaluators.
for each quality aspect all the api related sentences were coded by the same evaluator.
the same api related sentence can fall into more than one quality aspect.
for this reason it is possible to infer more than one pattern from the same sentence.
at the end of the patterns extraction all the evaluators created a catalog of inferred patterns to merge similar patterns into a more general pattern.
each decision taken at this stage was representative of the opinion of all evaluators.
in the end we obtained a list of patterns each one representative of a specific quality aspect expressing a specific sentiment.
given a sentence sas input the aspect classifier can then be used to check whether smatches one of the defined patterns.
to do this the aspect classifier uses the spacy nlp library to build a dependency tree of s. the tree reports i the pos in s and ii the dependency relations between the tokens composing s. this allows to i easily verify whether s matches a given pattern and ii identify negated terms needed to correctly assess the sentiment polarity of the matched pattern e.g.
if a positive pattern for performance is matched but a positive performance adjective is negated then the sentiment polarity is inverted to negative .
machine learning another possibility to implement the aspect classifier is to use a machine learning algorithm trained on a set of manually labeled sentences.
we exploit previously labeled sentences table ii to train machine learners to classify a given sentence into eightcategories the seven quality aspects we consider plus none .
specifically we used all the sentences with opinions and randomly selected same amount of sentences without opinions for training to avoid bias.
we used the scikit learn python library to experiment with different machine learners.
as predictor variables we used the terms contained in the sentences.
for preprocessing we remove stop words and punctuations and performed word stemming.
we considered each term as a predictor variable.
besides analyzing the single words contained in each sentence we extract the set of n grams composing it considering n .
we consider as features for the machine learner the presence absence of the patterns i.e.
whether a sentence matches each of the patterns we previously defined.
there is a key difference between the pattern matching approach and employing patterns as a feature of a machine learner.
in the first case patterns are used as rules and therefore sentences matching a given pattern are automatically classified into an aspect and sentiment polarity.
in the second case the presence of a pattern may or may not contribute towards a classification along with other features.
we experimented each machine learner with seven different combinations of features i bow only bag of words only considering single terms ii n grams only iii patterns only iv bow ngrams v bow patterns vi n grams patterns and vii bow n grams patterns.
a possible problem is that some categories are rarer than others.
a machine learning algorithm tends to assign sentences to more frequent categories because an error in under represented categories is more acceptable than an error in other categories to achieve a better overall accuracy.
to prevent this we re balanced our training set using smote an oversampling method which creates synthetic samples from the minor class.
we experimented each algorithm both with and without smote.
c. polarity analyzer the polarity analyzer analyzes the sentences classified as relevant by the aspect classifier to identify the sentiment polarity of the opinions.
we investigated two different options for the implementation of the polarity analyzer and we evaluate their performance to pick the best one see section iv .
pattern matching the set of patterns we extracted for the aspect classifier can be used also to assess the sentiment polarity of the opinions.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
information and opinions about the gson api presented by pome.
indeed each pattern is related to an aspect and to a sentiment polarity.
thus the first possibility is to use pattern matching to identify the sentiment of opinions.
sentiment analysis t ools a second possibility to determine a sentence s sentiment polarity is to exploit one of the many sentiment analysis tools existing in the literature.
we experimented with six of them with their default settings s entistrength s enti strength se nltk senti cr s enti 4sd and s t anford core nlp .
d. pome in action we implemented pome as an online application.
pome implements a java api search engine.
a developer who needs to parse json files without prior knowledge of any relevant api can search with a query parse json .
pome uses information retrieval ir techniques to list the apis in the database having a textual description relevant for the query.
the developer can select an api for example gson to assess what the users opinions about this api are.
pome will then present relevant information about gson as shown in fig.
and including basic information.
the api group id artifact id link to the jar file license and description .
opinions on the api classified by aspect.
pome analyzes the polarity of the mined opinions and presents the results with a bar chart where the green and orange depict the percentages of positive and negative opinions respectively.
each bar in the chart stands for one aspect while the top bar summarizes the overall polarity of all opinions that are listed in the table below .
by clicking a bar in the chart pome only shows in the table opinions related to the aspect of interest.
opinions on related apis.
pome also presents a bar chart summarizing opinions of related apis i.e.
same similar functionality identified as the ones having a high textual similarity in terms of description or belongingt able iv da t aset used to answer rq1 r q .
category apis sentences url configuration apis goo.gl gnqr51 mocking goo.gl 6itv eq v alidation frameworks goo.gl sq15rp xml processing goo.gl twptgd jdbc pools goo.gl yduwq1 overall t able v da t aset used to answer rq3 aspectpome opinions opiner opinions pos neg sum pos neg sum community compatibility documentation functional performance reliability usability total to same categories in maven.
each bar stands for one api and bars are ordered by decreasing ratio of positive opinions.
users can open the information pages of related apis by clicking the bars.
iv .
s tudy design the goal of this study is to evaluate the accuracy of pome in mining opinions from stack overflow discussions and classifying these opinions according to the quality aspects they refer to e.g.
performance usability and their sentiment polarity i.e.
negative or positive .
the context of the study consists of sentences extracted from stack overflow discussions related to apis from the maven central repository.
the material used in this evaluation along with its working data set is available in our replication package .
a. research questions we aim at answering the following research questions rqs rq how does a rule based aspect classifier for stack overflow perform compared to machine learning approaches?
this rq compares the performance of different implementations of the aspect classifier i.e.
the pattern matching approach and the machine learning approaches.
rq how does the rule based polarity analyzer perform compared to state of the art sentiment analysis tools?
this rq evaluates the accuracy of the polarity analyzer when using i a pattern matching approach or ii six state ofthe art sentiment analysis tools.
rq how does pome perform compared to opiner a stateof the art tool for mining opinions from stack overflow?
this rq compares pome with opiner .
b. context selection data collection table iv and table v present the datasets we used.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
rq 1and rq we considered a set of sentences from stack overflow discussions mined from the official stack overflow dump dated dec identified using our finegrained linker as relevant to one of the apis belonging to the five popular categories of apis from maven central listed in table iv.
sentences were mined as relevant to at least one of the subject apis.
the apis used in the context of rq 1and rq 2have not been used to define the patterns exploited by our approach for the opinions detection and classification.
we performed a manual analysis to categorize each of the sentences as expressing or not an opinion about the linked api.
in case the sentence did not report any opinion we assigned it to a no opinion label.
instead if an opinion was identified the sentence was further classified in terms of the quality aspect s the opinion refers to i.e.
one or more among community compatibility documentation functional performance reliability and usability .
finally the sentiment of the reported opinions was manually assessed by assigning a value between negative and positive .
the manual analysis was performed by three of the authors and also in this case was supported by a web application ensuring that two authors were assigned to each sentence.
all sentences were labeled by two authors.
the cohen s kappa coefficient is .
for sentiment and is .
for aspect which demonstrates a substantial agreement.
a fourth author not involved in the manual analysis then solved conflicts.
a conflict can concern the sentiment of a sentence as well as the quality aspects assigned to it.
overall sentences were classified as reporting opinions related to one aspect to two aspects community compatibility documentation functional performance reliability and usability .
this manual process was performed before the definition of the patterns catalog to avoid the authors being influenced during the process.
also in rq we involved external evaluators in the judgment of the opinions mined by pome and by opiner to have an external and unbiased view on the quality of the mined opinions.
to answer rq we ran different pome implementations on the dataset of sentences to assess their accuracy in identifying opinion aspects.
the implementations include the pattern matching approach and machine learning approaches in all variations presented in section iii.
concerning rq we compared the accuracy of pome in assessing the sentiment of opinions with the six sentiment analysis tools mentioned in the previous section.
we only conducted the comparison on the subset of sentences for which the best configuration of the aspect classifier output of rq can detect the existence of opinions.
indeed when envisioning pome as a tool deployed to mine opinions and assign a polarity to them our priority was to identify the polarity analyzer implementation better suited for the sentences identified by the aspect classifier as opinions since the discarded ones are not shown to the pome user.
rq to compare with opiner we collected the opinions mined by the two tools for four apis including springframework glassfish.jersey mongodb and google.gwt and asked developers and cs students to assess their accuracy.
those apis are listed in the top ten most reviewed apis in opiner and were not used in the pome s pattern definition nor in rq 1and rq .
once the best aspect classifier rq and polarity analyzer rq were identified we ran pome on the stack overflow data dump to identify opinionated sentences related to the four apis collecting in total opinions.
to compare with opiner we performed the following steps.
first we collected the opinions mined by opiner for the subject apis from the original implementation of the authors .
second we only considered the opinions mined by opiner for the same apis that are related to the same aspects used in pome.
third opiner uses a set of heuristics to link stack overflow sentences onto apis.
one of the heuristics it uses is the explicit mention of the library in the sentence similar to what we also do .
other heuristics focus on increasing the number of collected opinions i.e.
higher recall at the expense of precision.
for example the same conversation association links an opinionated sentence to the nearest library mentioned in a stack overflow conversation.
since inrq 3we evaluate the precision of the mined opinions we did not want to penalize opiner by considering for pome sentences linked with an approach designed to ensure high precision like the one implemented in our fine grained linker and for opiner sentences linked with heuristics possibly introducing imprecisions.
therefore among all opinionated sentences mined by opiner we only considered those explicitly mentioning the subject library.
finally since opiner identified more sentences than pome we tried to balance the number of sentences to be evaluated by participants for the two tools if the number of sentences identified by opiner for a specific aspect was lower or equal than we kept all sentences related to that aspect.
this applied to community compatibility and performance .
otherwise for a given aspect ai we compute the percentage paiof sentences identified by opiner for ai e.g.
if out of overall opinions mined by opiner are related to ai then such a percentage is .
then we randomly select pai npome where npome is the total number of opinions identified by pome among those identified by opiner for ai.
we invited developers cs students bsc msc phd and postdoc to evaluate the accuracy of the opinions mined by pome and by opiner for the subject apis.
participants had an average of .
years of java development median .
each participant was asked to use a web app to label the aspect and sentiment polarity positive neutral negative expressed in the sentences.
while the tools automatically classify the sentiment polarity into positive or negative we gave to the annotators the option to select neutral to identify false positives in the sentiment identification.
the sentences were randomly selected from the considered apis and shown in random order.
participants were not aware that the opinions were extracted from different tools to avoid any type of bias.
each sentence was labeled by two participants and participants were required to label at least sentences.
on average participants labeled .
sentences median .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
each sentence was firstly labeled by two participants.
if two participants did not agree with each other on either aspect or sentiment a third participant would be asked to solve conflicts related to the aspect classification and to the sentiment polarity again through the web app.
for sentences identified by opiner the participants solving the conflict were not able to assign an aspect sentiment with a high confidence.
thus we preferred to exclude these sentences from our dataset as they are characterized by a high degree of subjectivity three humans were not able to agree on the aspect and or sentiment polarity .
the final number of opinions evaluated in rq 3for each tool is reported in table v. c. data analysis to answer rq 1we compare the precision and recall of each experimented approach in classifying sentences as belonging or not to one of the seven aspects for the dataset of sentences.
to answer rq 2we compare the precision and recall of the sentiment analysis classification performed by the pattern matching approach and the six sentiment analysis tools.
to answer rq 3we compare the precision of the opinions mined by pome and opiner both in terms of aspects they identify and sentiment assigned to the opinions.
we report the percentage of correctly identified aspects and sentiment for both tools.
to compare the precision of pome and opiner we use fisher s exact test which statistically compare proportions.
since we perform multiple comparisons one for each aspect we adjust p values using holm s correction .
we also report for the overall dataset the odds ratio or i.e.
the ratio between the chance odd pome has to correctly classify aspect and sentiment vs. odd achieved by opiner.
v. r esul ts discussion rq how does a rule based aspect classifier for stack overflow perform compared to machine learning approaches?
table vi reports the precision and recall in detecting each of the seven quality aspects discussed in api related sentences.
table vi compares the performance obtained using the pattern matching approach black row and the best performing machine learner i.e.
linearsvm .
we show the results when using smote to balance the training set since it ensured a boost in performance.
also we do not show the results when using n grams only as this approach obtained poor accuracy.
the complete results including all machine learning approaches are in the replication package .
while a reasonable recall is useful to get enough recommendations in the context of opinion mining a high precision is preferable to avoid misleading recommendations to developers.
using bow for training the machine learner guarantees a relatively high precision for two of the seven quality aspects namely usability .
and performance .
with a recall floating around .
.
adding n grams does not significantly improve the performance of pome with respect to bow only.
the only exception is for reliability for which the linearsvm is able to reach a precision equals to but with a very lowrecall .
.
the limited contribution of n grams is in line with the findings of uddin and khomh .
when patterns are included as features gray rows in table vi the performance substantially improves especially for precision.
moreover training the linearsvm with patterns only is sufficient to obtain the similar performance ensured by the combination of all features bow n grams patterns .
this confirms the pivotal role of patterns in the classification.
finally the last row of table vi reports results obtained using the patterns as rules i.e.
plain pattern matching without any learning algorithm.
the precision for all aspect categories is comparable to the one obtained using patterns as features for training linearsvm with the exception that other approaches failed to detect sentences with community aspect.
it is worth noting that the recall is significantly higher.
the approach using pattern matching is able to obtain for each quality aspect a precision varying in the range with a recall varying in .
the api related sentences belonging to documentation orperformance are the ones better identified in terms of both precision .
and .
and recall .
and .
.
for both reliability and community the precision is high .
and .
respectively with a low recall .
and .
.
given the above results our decision was to implement the aspect classifier of pome using the pattern matching approach given its simplicity and performance.
rq how does the rule based polarity analyzer perform compared to state of the art sentiment analysis tools?
to answer rq we use the api related sentences identified as containing opinions when running the implementation of the aspect classifier chosen in rq .
table vii reports the precision and recall of i six state of the art sentiment analysis tools and ii the pattern based approach in identifying the sentiment expressed in the sentences.
as also highlighted in previous literature sentiment analysis tools show poor performance in identifying the sentiment positive or negative reported in software engineering datasets.
our results tend to confirm the above statement and most importantly underline how the pattern based approach outperforms the state of the art tools for both positive and negative opinions.
this is expected since i the patterns have been properly determined looking at apirelated sentences mined from stack overflow and ii the sentences considered for evaluation have been selected using the approach that verifies the presence of at least one of the patterns.
specifically for both positive and negative opinions the pattern based approach has a precision .
.
the recall is higher for positive opinions than for negative ones .
and .
respectively .
to sum up the pattern based approach has good performance in terms of both precision and recall while for sentiment analysis tools a high precision comes at the expense of low recall.
the only exception to this trend is standford corenlp that however exhibit a very low precision for the negative opinions.
looking more in depth at the low recall of sentiment analysis tools it is possible to state that the big challenge resides in the presence of many sentences wrongly classified as neutral.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
t able vi performance of the best machine learning approach using seven different set of fea tures and the pa ttern ma tching approach .
community compatibility documentation functional performance reliability usability pr rc pr rc pr rc pr rc pr rc pr rc pr rc bow only .
.
.
.
.
.
.
.
.
.
.
.
.
.
bow n grams .
.
.
.
.
.
.
.
.
.
.
.
.
.
patterns only .
.
.
.
.
.
.
.
.
.
.
.
.
.
bow patterns .
.
.
.
.
.
.
.
.
.
.
.
.
.
n grams patterns .
.
.
.
.
.
.
.
.
.
.
.
.
.
bow n grams patterns .
.
.
.
.
.
.
.
.
.
.
.
.
.
pattern matching .
.
.
.
.
.
.
.
.
.
.
.
.
.
t able vii ev alua tion resul ts for sentiment anal ysis tools .
tool correctpositive precisionpositive recallnegative precisionnegative recall sentistrength .
.
.
.
sentistrength se .
.
.
.
nltk .
.
.
.
senticr .
.
.
.
senti4sd .
.
.
.
stanford corenlp .
.
.
.
pattern matching .
.
.
.
t able viii precision for pome and opiner in aspect sentiment prediction .
predicted aspectaspect prediction sentiment prediction pome opiner pome opiner community .
.
.
.
compatibility .
.
.
.
documentation .
.
.
.
functional .
.
.
.
performance .
.
.
.
reliability .
.
.
.
usability .
.
.
.
overall .
.
.
.
as an example when a sentence clearly reports that the library provides some useful features the commons configuration project from apache will do the job it will allow you to write and read properties files the patternbased approach is able to correctly identify it as a positive opinion while all the sentiment analysis tools label it as neutral.
the same happens for the sentence as already stated above there is a compatibility issue with mockito all in which the pattern based approach is able to recognize the presence of a negative feeling from the compatibility point of view while the sentiment analysis tools classify the sentence as neutral.
note that this is a limitation of these tools in the specific context in which we are using them.
however this does not mean that they do not work when assessing the sentiment polarity in other contexts e.g.
users happiness on stack overflow .
given the above results in pome we rely on the patternmatching approach to identify sentiment polarity rather than using existing sentiment analysis tools.
rq 3how does pome perform compared to opiner a state of the art tool for mining opinions from stack overflow?
to answer rq we compare the results of both aspect detection and sentiment analysis achieved by pome and opiner on the sentences they extracted from stack overflow.
results shown in table viii indicate that pome achieves an overall better precision.
that is when pome identifiesan aspect from a discussion the chance of it being correct is higher than that identified by opiner .
vs0.
.
according to fisher s exact test the difference is statistically significant p value .
with an or .
i.e.
pome has .
times more chances of providing a correct aspect classification than opiner.
the same trend holds for each aspect except compatibility where both opiner and pome e xhibit low performance.
one example of misclassification by pome in this category is it did not work for me with my spring boot version classified by pome as compatibility related due to the pattern did not work version .
the study participants labeled the sentence as not reporting any opinion probably because it is not clear whether the problem experienced by the user is an actual compatibility issue as opposed e.g.
to a misuse of the api by the user .
pome significantly outperforms opiner when identifying opinions related to usability and functional aspects with the fisher s exact test indicating that differences are statistically significant adjusted p value .
.
in other cases differences are not statistically significant on single categories because of the small number of samples.
however the ors are always in favor of pome ranging from .
for compatibility to .
for functional .
we can conclude that pome performs better than opiner in aspect identification.
since for most aspects pome can achieve a precision greater than .
we can say that the opinions mined by pome are generally reliable considering that a random assignment of aspect would result in a precision of .
.
we qualitatively discuss some examples related to functionalrelated sentences in which pome obtains a .
precision as compared to the .
achieved by opiner.
examples of sentences correctly classified in this aspect by pome are you can do most of this config using application.properties if you are using spring boot and the guava library has an ordering.greatestof method that returns the greatest k elements from an iterable .
concerning the misclassifications related to the functional aspects one of the pome s patterns causing false positives is see for an explanation of this pattern that matches for example the sentence if you are using mongo java driver then you can have a look at this so answer .
this pattern was responsible for out of the false positives in the functional aspect.
however it also helped in identifying true positives thus posing the usual recall vsprecision dilemma.
as for opiner its precision in identifying opinions about functional aspects is quite low.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
misclassifications here include i am working on a jersey web service o r an important architectural difference is that gwt rpc operates at a more functional level .
probably this is due to the features words used by the machine learner to classify the aspects.
indeed service and functional are likely to be keywords characterizing feature related sentences.
when comparing the results of sentiment prediction pome almost doubles the precision of opiner .
vs0.
and performs better in all categories.
fisher s exact test indicates that the observed differences are again statistically significant for functional and usability adjusted p value .
in both cases .
in other cases the test did not report significant differences again because of the limited number of samples.
the ors are always in favor of pome ranging from .
of compatibility to .
of functional .
on the overall dataset we have a statistically significant difference p value .
and an or .
i.e.
pome has four times more chances of opiner in indicating the correct sentiment polarity.
also for what concerns the sentiment prediction the strongest difference between the two approaches is observed in the functionalrelated sentences.
since we already discussed this category for the aspect identification we focus our qualitative analysis on the compatibility related sentences the ones exhibiting the smaller difference in sentiment prediction precision among the two approaches .
vs0.
.
here the pome s misclassifications are mostly due to the wrong handling of negations often caused by misspelling typing issues.
for example pome misclassifies as positive the sentiment of the sentence the problem is that framelayout.layoutparams constructor doesn t support another framelayout as a parameter until the api .
due to the use of the backtick instead of an apostrophe which caused the negation handling failure.
other examples are typos like cann t instead of can t .
integrating a spell checker could solve the problem although it must cope with having source code words not being correct english words.
concerning opiner the main problem is represented by sentences considered by the participants as do not actually reporting an opinion and thus being neutral in terms of sentiment while classified as positive negative by the tool.
this is the case for btw i m working with spring mvc classified as a positive compatibility sentence by opiner and as non opinionated by participants.
despite the better results achieved by pome opiner identifies a higher number of opinions for these apis times higher than that identified by pome thus very likely exhibiting a higher recall .
pome has been designed to favor precision over recall and in rq 3we are only focusing on the precision of the mined opinions since assessing the recall would require the analysis of the entire stack overflow.
the precision reported in rq 3is not as high as for the other rqs.
this might depend on the specific dataset and or on whom performed the labeling.
the datasets used in the previous rqs have been created by the authors having a deeper knowledge of the problem.
also they discussed cases where there was a disagreement while this did not happen for rq 3participants.
although instructions were given for evaluators of rq theannotation task remains highly subjective.
in spite of these concerns pome adv ances the current state of the art in aspect and sentiment identification.
also the difficulty annotators had in their task highlights once more that grasping api opinions from stack overflow sentences is not an easy task and therefore recommenders such as pome and opiner are valuable.
vi.
t hrea ts to validity construct validity .
this affects the creation of the labeled dataset used in rq 1and rq .
the threat has been mitigated by having multiple evaluators classifying aspects and sentiments.as for the slightly modified approach by treude and robillard we manually validated all the sentences extracted with the fourth regular expressions introduced by us in order to discriminate between sentences referring to apis.
among the sentences extracted by our fine grained linker have been identified using the fourth rule in table i. one of the authors manually analyzed all of them classifying of the sentences correctly linked to the api .
internal validity .
it is possible that a different calibration of the machine learners produce better results.
therefore results reported in table vi and table v represent a lower bound for the different configurations of pome.
conclusion validity .
where needed we supported our claims through appropriate statistical procedures.
as for the aspectspecific comparison it is possible that type ii errors occurred failed to reject hypothesis due to limited sample however we showed how the differences were statistically significant on the overall dataset.
external validity .
while we have validated pome and compared it with opiner on unseen data it is possible that a different dataset would exhibit different results.
also another dataset could exhibit different distributions of the identified aspects and possibly further aspects we did not consider.
however this still makes the approach applicable possibly by augmenting the set of identified patterns.
pome is suitable for popular apis due to the large availability of opinions to mine.
however this applies to any recommender based on historical data mining.
vii.
c onclusion mining opinions on apis helps developers take better decisions during software development.
we proposed a patternbased approach named pome which achieves high accuracy in identifying opinion aspects.
pome also outperforms a stateof the art tool opiner .
pome aids developers to quickly gain an understandings of the overall quality pros and cons of apis.
as opinions are embedded in many other kinds of sources our future work is given.