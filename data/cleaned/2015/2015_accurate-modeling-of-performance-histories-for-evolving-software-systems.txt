accurate modeling of performance histories for evolving software systems stefan m uhlbauer bauhaus university weimar germanysven apel saarland university germanynorbert siegmund bauhaus university weimar germany abstract learning from the history of a software system s performance behavior does not only help discovering and locating performance bugs but also identifying evolutionary performance patterns and general trends such as when technical debt accumulates.
exhaustive regression testing is usually impractical because rigorous performance benchmarking requires executing a realistic workload per revision which results in large execution times.
in this paper we propose a novel active revision sampling approach which aims at tracking and understanding a system s performance history by approximating the performance behavior of a software system across all of its revisions.
in a nutshell we iteratively sample and measure the performance of specific revisions that help us building an exact performance evolution model and we use gaussian process models to assess in which revision ranges our model is most uncertain with the goal to sample further revisions for measurement.
we have conducted an empirical analysis of the evolutionary performance behavior modeled as a time series of the histories of six real world software systems.
our evaluation demonstrates that gaussian process models are able to accurately estimate the performanceevolution history of real world software systems with only few measurements and to reveal interesting behaviors and trends.
i. i ntroduction for most software systems performance e.g.
response time is a key success factor .
performance is not only critical in real time applications but performance bottlenecks may render any kind of software unusable.
despite its relevance for software quality and user experience performance assessment is all to often postponed and exercised too little and too late in the development process .
performance is shaped by the software system s architecture and evolves along with code added removed and refactored in individual revisions1.
the performance influence of evolutionary changes to a system can emerge cumulatively such as when software evolves with little respect to its original architecture.
this pattern of undisciplined architectural evolution has been described as architectural erosion resulting in technical debt .
it often implies degrading performance also known asperformance regression .
performance related problems have been addressed from different perspectives.
most work on performance anomalies aims at identifying performance regression and at pinpointing the cause to an individual revision of the source code.
the detection criteria for performance regression are manifold and comprise statistical significance tests correlation 1we consider each commit to a repository as a new revision of the system.analyses among performance metrics and absolute or relative deviation thresholds that when exceeded indicate a possible regression.
performance regression is determined by the performance measurements of either performance benchmarks or unit tests .
all this work is able to detect performance regressions with high accuracy.
work by heger et al.
complements this task with a consecutive root cause analysis it bisects the evolution history recursively until the revision that introduced the performance anomaly is isolated.
the scope of this analysis is limited to the local performance evolution around a potential performance anomaly.
while this is effective for understanding a single performance anomaly it tells not much about whether there are notable long term regression trends patterns in performance evolution and possible indicators for future performance degradation.
these can be learned only from the global performance evolution of a software system and they are the basis to take proactive measures to direct maintenance tasks apply refactorings and postpone addition of new functionality.
the naive approach to obtain a model of the global performance evolution history requires exhaustive performance assessment of every single revision.
take for example the highly influential python package numpy .
its performance is constantly assessed for every code change by means of a microbenchmark suite2.
clearly this huge effort is justified since the package is widely used as an integral part of most industrial relevant python libraries such as scipy or tensorflow .
however not every project exercises this level of performance assessment or has the resources to do so.
to obtain a good approximation of the performance evolution history of a software project assessing all of its revisions is only rarely an option in practice.
if the revision history is sampled along time such as sampling all milestones releases or flagged commits this might indeed expose performance changes but prohibits pinpointing interesting trends and anomalies to a distinct revision rater than a segment of revisions in an unbiased manner.
an assumption that underlies our approach is that a revision sampling strategy should be adaptive in that it actively searches for performance changes and that it should be agnostic of a software system s properties.
for instance it should not rely on the developers commit message discipline.
clearly the goal 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
is to model a software system s performance evolution history with as few revision measurements as possible .
to this end we propose active revision sampling an active learning approach to model and estimate performance of arbitrary revisions of a software system.
starting with an initial sample of revisions our approach interpolates the performance evolution history and iteratively expands the sample set with new revisions.
the exploration of new sample revisions is guided by the uncertainty of the current interpolation model.
by means of an empirical analysis of the history of six real world software systems we demonstrate that performance evolution histories can be approximated with only a small fraction of all available revisions and that interesting performance behavior and trends can be revealed this way.
the main contributions of this paper are an analysis of the presence and frequency of notable performance changes of a selection of six real world software systems the analysis reveals common performance evolution characteristics and guides us in the choice of methods for estimating performance across arbitrary revisions cf.
section ii an approach to efficiently learn and estimate the performance of a software system across arbitrary revisions with only few measurements cf.
section iii along with an evaluation of feasibility and accuracy on six real world systems cf.
section iv .
ii.
c haracterizing performance evolution as software evolves with every incremental modification of the code performance can change as well.
performance changes may result from purposeful optimizations or as a byproduct of functional modifications of the software.
our goal is to provide an efficient method to identify substantial performance changes over the life time of a software which can be the basis of a root cause analysis of performance degradation or a method to quantify the technical debt and development process of the software system with respect to performance.
before we present our method in the next section we first require a better understanding of how performance evolves that is what are common characteristics of time series of performance measurements across multiple commits.
identifying time series properties of performance evolution is key for developing an appropriate modeling approach and provides insights into how performance changes over a system s life time.
for time series data there exists a variety of prediction methods especially in the category of auto regressive models .
most traditional methods however require knowledge of what characteristics the process to be modeled has.
these characteristics usually include constant behavior stationarity continuous or gradual changes trends and periodic changes seasonality .
moreover the time series can show abrupt changes also called change points which suggest to model the time series signal piecewise.
in what follows we analyze the performance evolution of two configurable real world software systems as well as four pythonlibraries with respect to change points.
knowing whether performance changes abruptly or continuously allows us to understand the limitations of traditional time series prediction techniques.
a. change point detection with cusum our first step in assessing the characteristics of performance evolution is to systematically test our corpus of software systems for the presence of change points.
the corresponding problem is often referred to as change point detection and there is a variety of statistical methods available for this task.
for our analysis we use cusum a method from statistical quality control .
cusum is a sequential algorithm that uses the cumulative sum of deviations between successive measurements.
the basic idea is to maintain a cumulative sum of changes in positive and negative direction.
a change point is indicated when the cumulative sum exceeds a given threshold.
in this case the cumulative sum of the respective positive or negative change is reset to zero.
the algorithm behind cusum is defined by page et al.
as follows first it initializes the positiveg 0and negative g 0cumulative sum with zero g g next for each data point stof the time series at time t the corresponding cumulative sums g tandg t are defined as g t max g t st v g t max g t st v that is the deviation of the current measurement stfrom a target value v drift is added subtracted to the positive and negative cumulative sum.
for every point in time the cumulative sums are compared against a threshold value h which when exceeded indicates a possible change point g t braceleftbigg iffg t h g telse the cumulative sum g tis then reset to zero to enable detection of following change points.
b. parameterization of cusum the threshold hdescribes a budget of change that when exceeded indicates a change point.
changes in the data set can emerge gradually or abruptly.
the cusum method does not discriminate between these two types as change can accumulate either way.
this is an important property for our analysis not to be biased toward a certain expectation.
however along with the change points reported by cusum one can consider only change points for which the absolute change compared to the preceding data point was substantial.
the cusum algorithm can be configured with two parameters the drift vand the threshold h. a guideline of how to tune the parameters for a given data set is given by gustafsson .
the guideline suggests to choose a drift vof one half of the expected change .
in our analysis we employ a drift of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
time .
.
.
.
.
.
.050performanc performance observation change point low threshold h change point high threshold h h figure influence of threshold parameter hon the detection of change points for a synthetic time series.
percent of the range of the time series since we consider a change of percent in performance substantial.
v .
max s min s next we estimate and tune the threshold parameter h. the guideline suggests starting with a large value for h which is then continuously decreased until the algorithm reports sufficiently few false positives.
we adopt this procedure by using the average standard deviation of the time series or more precisely five times the average standard deviation over a sliding window.
instead of directly changing the threshold parameter directly we tweak the size of the sliding window as described in equation .
the rationale behind this approach is to capture both global and local changes.
for a small sliding window the average standard deviation corresponds to a shorter time frame and hence is more sensitive to local changes.
by contrast for a large sliding window the standard deviation incorporates a wider time frame and the threshold becomes less sensitive to local changes as these are diluted by large scale effects.
we exemplify this influence of the sliding window size in figure where for a smaller sliding window more small change point candidates are reported in total whereas a larger sliding window size results in a more robust and large scale view of possible change points in total .
for an initial window size equal to the total number of data points the threshold parameter corresponds to the standard deviation of the whole time series and draws a global picture of variation over the entire performance evolution history.
we start with this initial case and decrease the window size iteratively.
as we approach smaller values of the sliding window sizew the threshold parameter happroaches a more local measure of variation.
hence with a decreasing window size the threshold becomes more sensitive to change points with smaller amplitudes but also false positives.
we summarize our definition of threshold h wheresdenotes the overall number of revisions wthe sliding window size h s ws w summationdisplay p 0p w summationdisplay i p radicalbig var c. change points in performance data in figure we report our findings of using cusum on six different software systems.
we provide a detailed description of how we selected our subject systems and respective benchmarks in section iv.
in a nutshell we have selected two configurable software systems xzand lrzip because recent findings suggest that performance bugs are often due to issues related to configurability and occur only in certain configurations .
that is for each configuration we derive a dedicated variant as discussed in section iv with possibly different performance histories.
for xz lrzip we use and different variants with identical workload.
for the remaining four software systems performance measurement was conducted at a finer grain with method level microbenchmarks.
again per method we derived a specific performance history for these software systems.
by untangling the selected systems in this manner we obtain a more generalizable picture which includes whole system performance evolution and micro system performance evolution.
we have aggregated the change point analysis results in two different ways.
for configurable systems we report the change points across all variants for microbenchmarks we treat the sum of all method execution times as one single benchmark.
both decisions are due to space limitations and affect only the reporting in this paper.
we have also analyzed all undiscussed benchmarks and provide the corresponding results online3.
we consider the more condensed visualization of the sum of all microbenchmarks as representative for the entire software systems because after analyzing all results the same conclusions can be drawn from this.
for each subject system we varied the sliding window size as a percentage of the total number of revisions and measured both the number of reported change points in red as well as the distribution of change point amplitudes as box plots.
the range of the sliding window size presented in figure is chosen such that it includes a range where the number of reported change points is constant.
for this range the number of change points is least sensitive to the influence of the sliding window size.
each box plot was normalized to the mean of the time series to provide a context for interpreting this measure of effect size .
that is the amplitude of a change point puts it in relation to the mean performance of the time series.
as expected the number of change points decreases for a greater window size.
for smaller window sizes the reported change points include also smaller amplitudes.
the rationale is that there should be fewer changes points on a global i.e.
whole history scale than local changes points and we can confirm this.
however our subject systems show substantial change points for different ranges of sliding window sizes e.g.
for sliding window size change points are reported for xz but not for scipy .
this is due to the difference in sizes of the data sets xz lrzip and ultrajson do not contain as many revisions as the remaining systems as shown in table i. the results of figure revision sampling authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
sliding window size win percent of number of commits0.
.
.
.
.
.
.14change point amplitudes signal mean0255075100125150 number of reported change pointsxz time series sliding window size win percent of number of commits0.
.
.
.
.0change point amplitudes signal mean050100150200250300 number of reported change pointslrzip time series sliding window size win percent of number of commits0.
.
.
.105change point amplitudes signal mean0.
.
.
.
.
.
.
number of reported change pointsultrajson all time series .
.
.
sliding window size win percent of number of commits0.
.
.
.
.30change point amplitudes signal mean0.
.
.
.
.
.
.
.
number of reported change pointsnumpy all time series .
.
.
sliding window size win percent of number of commits0.
.
.
.
.035change point amplitudes signal mean0246810 number of reported change pointsscipy all time series .
.
.
.
.
.
sliding window size win percent of number of commits0.
.
.
.
.0change point amplitudes signal mean0123456 number of reported change pointspillow all time series figure frequency of reported change points vs sliding window size for threshold h. we present the analysis results for xz lrzip ultrajson numpy scipy and pillow from top left to bottom right .
suggest that we are able to show and quantify the presence of substantial changes in performance introduced abruptly by single change points.
d. results the question we address in this section is whether performance histories can be modeled with traditional time series prediction techniques .
across the configurable software systems of our sample we have identified various change points.
about one in each variant on average with an effect size between and percent for xzand around percent for lrzip .
the accumulated time series of the microbenchmarks for the remaining five subject systems exhibit larger numbers of change points with similar effect size ranges which can be attributed to the larger revision history with the exception of ultrajson which has a comparably smaller revision history .
that is we can confirm the presence of substantial change points in all subject systems.
given this it is important to note that the presence of change points poses an obstacle in modelingperformance evolution since the time series data are segmented and possible trends or other patterns are superimposed by abrupt changes.
we argue that missing knowledge of location and amplitude of these change points hinders prediction of performance evolution using traditional models such as trend or seasonality extrapolation from the realm of auto regressive models .
since our intention is to efficiently estimate performance for arbitrary versions we conclude that any such approach has not to only estimate performance but especially search and pinpoint abrupt changes in performance evolution.iii.
a ctive performance approximation our empirical findings on the ground truth data of performance evolution measurements of six software systems in section ii suggests that software performance evolutionexhibits abrupt changes that are hard to pinpoint withoutexhaustive measurements.
while one could employ simplesearch heuristics such as binary search to locate abruptchanges this modeling strategy is neither able to pinpointpossible performance patterns nor an arbitrary number of change points.
to analyze performance time series in a way that incorporates possible patterns as well as an arbitrary number of abrupt changes we propose an adaptive revision sampling approach to obtain accurate performance estimation models.
the workflow in figure outlines its key ingredients and procedure which we will discuss next in detail.
figure workflow of adaptive revision sampling a. active learning a key feature in the workflow presented in figure is the iterative cycle of actions.
in that sense our approachfollows a design strategy and family of machine learning authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithms called active learning .
unlike ordinary machine learning algorithms which are trained once and then evaluated active learning trains repeatedly with gradually augmented training sets.
this family of algorithms is intended to choose by itself which data are included in the training set.
for instance if an estimation model performs poorly on the influence of a specific parameter it will suggest obtaining additional training samples with emphasis on variance of this very parameter.
the motivation for this strategy is to minimize measurement effort by continuously querying specific data to be measured .
in our context the pool of unseen observations are performance measurements.
we opted for an active learning strategy since it minimizes the size of the training data set and guides the process of obtaining new observations.
as performance measurement is typically an expensive task we emphasize the incentive of minimizing costs.
b. gaussian process regression we use gaussian processes gp to learn a model that can accurately estimate performance for unobserved revisions step in figure .
the main motivation is that a gp makes estimations in the form of multivariate gaussian distributions rather than scalar values.
these distributions provide a measure of variance or more importantly a measure of confidence about our estimation accuracy for each revision of our time line .
that is given a small confidence interval the model is confident in its estimation whereas a large confidence interval suggests to use the estimation with precaution and to refine the model.
this facet of gps is especially beneficial in the context of active learning as we will explain in section iv in more detail.
formally a gp assumes that for a target function f x each valuey f x can be expressed as a gaussian distribution with a mean function x rand a covariance function k x x r f x n x k x x .
the covariance function k commonly called kernel encodes the relationship between response values depending on the distance of the input values.
that is for a pair of input values x1 x2 x the covariance between f x1 andf x2 is defined as k x1 x2 .
although we do not require any internal knowledge about the system studied to build a gp model we can incorporate domain knowledge such as in the choice or construction of the used kernel function which is an important success factor for our approach.
for instance for modeling time series with seasonality a periodic kernel function can be used.
for an overview on different kernel functions we refer the interested reader to the literature .
once we have build a gp model for a set of given observations f x1 f x2 ... we can predict the posterior distribution that is the mean m and variance 2 for an unobserved data point x as m x k x x k x x y x 2 k x x k x x k x x k x x .
with these two equations we can obtain performance estimates of unseen revisions and obtain a measure of uncertainty about this estimate .
c. active data selection the key component of our active learning approach is the repeated augmentation of the training set.
the procedure that is used to determine which observation to add to the training set cf.
step in figure is defined by an acquisition function .
generally a data point is selected such that it is most likely to increase the prediction accuracy compared to the model built in the previous iteration.
for different purposes a variety of acquisition functions have been proposed yet we follow an uncertainty aware approach and let the data acquisition be guided by the prediction variance of the gp model directly.
to minimize prediction uncertainty and hopefully increase prediction accuracy in the next iteration we augment the training set with the data point xnextexhibiting the maximum prediction uncertainty 2in the current iteration.
xnext arg max x x 2 x d. termination criterion the last piece of the puzzle is to decide in each iteration whether our cycle yields a model sufficiently confident in its estimations or whether we need to continue refinement.
much like for data acquisition we employ the prediction uncertainty.
we consider a model to be sufficiently confident if the uncertainty measure for every revision does not exceed a threshold specified by the user.
this threshold tstopcan be estimated empirically for instance using the inherent measurement bias of the performance measurement setup.
in other words we can measure a single revision a couple times and report the resulting variance as the threshold beyond which we cannot obtain more precise results.
thus in each iteration we evaluate whether the following formula holds or further refinement is necessary.
max x x 2 x tstop e. approach summary the workflow of our approach is as follows initialize training set t f x1 f x2 ... with performance measurements for a small number of revisions xt x1 x2 ... and select a kernel function k. train a gp model m t k and estimate the performance f x n x 2 x for the remaining unobserved revisions x x xt.
check whether equation holds for the gp model.
if so return the prediction model m t k .
if not continue.
determine xnextaccording to equation as arg max x x xt 2 x. measuref xnext addxnexttoxt f xnext to the training sett and go back to step .
we illustrate our approach in figure where we attempt to approximate a performance time series with about authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
100020304050sample size gp estimate ground truth next point training set uncertainty 100020253035404550sample size 100020253035404550sample size 100020253035404550sample size time revisions performance figure four iterations of our adaptive approximation approach with to revisions as training sets.
the black curve depicts the ground truth performance the blue curve depicts the gp estimate with its respective uncertainty.
the orange line highlights the revision which is next to be included in to the training set.
revisions.
in the top left subfigure we initially train a gp model with a training set of size five i.e.
.
of all revisions .
in all four subfigures the black line depicts the ground truth performance the blue line the gp estimation and the blueshaded area the corresponding confidence interval.
in the initial iteration we are not satisfied with the maximum uncertainty and instead select the revision with the maximum uncertainty depicted by an orange vertical line to be included into the training set in the following iteration.
we proceed in this manner with six and seven training samples.
finally the gp estimation in the bottom right subfigure with eight training samples in total i.e.
.
now clearly approaches the ground truth.
let us summarize the benefits of our approach instead of finding a single change point as for bisect search we can approximate the performance history of a software system as a whole.
we can use different kernel functions at different time intervals to account for different time series properties as found in section ii in an appropriate way.
iv .
e v aluation next we evaluate our approach with respect to efficacy and efficiency.
a. subject systems software system selection we have selected six realworld actively maintained software systems from different domains including file compression xzand lrzip json parsing ultrajson image manipulation pillow and scientific computing numpy and scipy .
the selection along with project metrics is presented in table i. the subjecttable i meta data for our subject systems.
we report the number of different variants and methods per subject respectively.
in total we consider revisions in our analysis.
software system methods or variants revs.
sloc xz variants lrzip variants pillow methods ultrajson methods numpy methods scipy methods sloc source lines of code were collected using the tool cloc .
systems can be divided into two categories xzand lrzip are software systems that can be configured at load time to obtain functionally differing variants.
we analyze multiple variants because performance bugs are often configurationrelated and emerge only under certain configurations .
the second category includes python libraries for various tasks.
here we obtained performance history data for individual methods as these subjects represent libraries that are embedded in other applications.
considering both stand alone systems and libraries we obtain a more general picture about performance evolution.
deriving performance histories for the two load time configurable software systems we assess performance for and different configurations and at least repeated runs resulting in more than measurements.
the selection of system variants follows known feature wise sampling for binary options and plackett burman sampling for numeric options .
each configuration results in a different execution for which performance can differ and evolve independently.
we selected benchmarks such that each represents a realworld execution scenario.
for xzand lrzip we use the silesia compression corpus4 which contains about mb of files of different types.
for the python libraries except ultrajson we reused performance benchmarks at method level provided and used by the respective software projects.
these microbenchmarks are used for optimizing the respective libraries so they reflect performance behavior relevant for maintenance.
based on these microbenchmarks we derive a number of performance histories by considering each microbenchmark as a variant of the whole software system.
this way we consider macro and microbenchmarks in our evaluation avoiding bias e.g.
the controversy of whether microbenchmarks are representative for the performance of a whole system .
except pillow the python projects measure track and publish performance measurements for their microbenchmark using the tool airspeed velocity5.
we extracted the performance data for the three python libraries from their respective github 4the silesia corpus can be found at authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
repositories6 whereas for p illow ultrajson xz and lrzip we conducted the performance measurements on our own.
for ultrajson we assembled four tasks on our own which consist of parsing and serializing a small size 617kb and a large size .4mb json file respectively as this is the main purpose and most performance relevant task of a json library.
all measurements were conducted for all revisions on different commercial off the shelf compute clusters.
however each software system was assessed with an identical hard and software setup.
xzand lrzip have been assessed on machines with a core quad cpu .
ghz gb ram p illow on machines with an i7 cpu .
ghz gb ram and ultrajson on machines with an xeon cpu .
ghz gb ram.
per revision we repeated each experiment five times and reported the median measurement to mitigate measurement bias.
the coefficient of variation reported on all machines was well below ten percent.
b. research questions our approach can be customized via the choice of the kernel function.
different kernel functions result in possibly different shapes of the estimation of the gp model.
for the use of gp regression models there exists a wide variety of kernel functions which can be used or composed to obtain more complex ones .
for our evaluation we selected the five kernel functions presented in table ii which are commonly provided by or can be added to gp libraries such as scikit learn or gp yfor python.
four of the five kernels are parametric and stationary since they represent functions of the distance between the input vectors whereas the brownian kernel is inspired by a stochastic process whose growth is normally distributed so that it is not stationary .
this selection is by no means exhaustive but we selected kernels that are widely used in practice.
in our implementation we use the gp ylibrary7for gaussian processes in python and the respective kernels.
the kernel hyperparameters were not explicitly tuned per subject system yet the default optimizer selects the optimal fit of at most iterations.
in what follows we motivate our research questions and describe how we operationalized them.
rq1 which choice of a kernel function performs best at estimating performance histories?
to start our evaluation for our subject system corpus we compare two indicators for a setup of the five different kernel functions maximum uncertainty and performance estimation error.
the first indicator serves as an upper bound for model confidence whereas the mean absolute percentage error mape is a measure of how well an estimation fits the ground truth observations on a global scale.
as these two indicators change with every iteration and due to the lack of space we have decided to incorporate these dynamics in our visualization and present it in an animation provided online.
6the measurements can be obtained for numpy at numpy bench scipy at addition we evaluate our results on both macrobenchmarks and microbenchmarks.
the four python libraries represent microbenchmarks as the performance has been measured using the method execution time.
for macrobenchmarks we use a the whole systems of xzand lrzip including their variants and b the sum of method execution times of all methods of a respective library.
the rationale behind this is to obtain an overview of the performance behavior of a whole library one would need to create a benchmark executing all methods.
hence we interpret the performance value of a revision as the sum of all performance values of all method execution times of this revision.
rq2 does active revision sampling improve modeling performance evolution histories with gp models?
to evaluate whether selecting the next sample i.e.
revision based on the uncertainty of the performance estimate is reasonable we compare active revision sampling against random sampling.
in particular we compare how many iterations are required to obtain accurate estimation models and whether with each iteration the estimation error converges robustly.
for this comparison we evaluate on the best performing kernel of rq1 on the micro and macrobenchmarks.
rq3 can we estimate global change points in a performance evolution history?
reporting mape for a gp estimation provides a good picture of how accurate global effects such as trends have been modeled.
however even a small average error could mean that abrupt changes might not be spotted in the model as the high error at change points can be diluted by small error of the vast number of remaining revisions.
since change points represent interesting events in the performance history of a software system we quantify whether our model is capable of detecting them at the macrobenchmark level.
to assess whether we can derive the locations of global change points we apply two different off line change point detection algorithms to the ground truth as well as our estimations.
in this experiment the independent variables are the kernel size of the sample set and in addition the choice of the change point algorithm.
we decided not to reuse the cusum algorithm from section ii as it is sensitive to its parametrization.
this was useful for exploring the presence of change points of different magnitudes yet in this context we focus on global change points.
instead we employ a top down binary segmentation strategy as well as a bottom up merge strategy.
the first algorithm recursively segments the given time series at the point with the highest variation whereas the second algorithm merges smaller segments to larger segments as long as the constituent segments exhibit little variation.
for both algortihms we refer to respective implementations and parameterization from the python library ruptures8 where a more detailed description of both algorithms can be found.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii selection of five widely used kernel functions.
the hyperparameters describes the lengthscale of the kernel function 2describes the variance.
name definition radial basis function rbf k x y 2exp parenleftbigg x y 2 2 parenrightbigg rational quadratic kernel rqf k x y 2 parenleftbigg x y 2 parenrightbigg brownian kernel bk k x y 2min x y mat ern kernel mk3 k x y 2 parenleftbig x y parenrightbig exp parenleftbig x y parenrightbig mat ern kernel mk5 k x y 2 parenleftbigg x y x y 3 2 parenrightbigg exp parenleftbig x y parenrightbig c. results kernel selection efficacy in table iii we report for different sample sizes both the uncertainty of the different kernels as well as the respective mape.
for the configurable systems we average the uncertainties and errors rates over all variants for the libraries we average over all methods.
we highlight for each sample size the best kernel with respect to mape in green.
as can be seen in table iii the brownian kernel bk achieves the lowest mape for nearly all subjects.
only for ultajason we observe that the radial basis function rbf and rational quadratic kernel rqf perform slightly better.
nevertheless for most systems the difference is substantial partially resulting in a one or to two orders of magnitude lower mape.
interestingly macrobenchmarks seem to be easier to model than microbenchmarks which might be because macrobenchmarks do not fluctuate in the performance behavior over the history so much.
the rationale might be that changes at method level will have a more severe effect on execution time at microbenchmark level than for the whole system.
a further observation is that prediction accuracy increases when the sample grows.
although this is plausible we argue that suboptimally selected samples might lead to false approximations of the gp model e.g.
in the case of scipy when comparing of revisions vs. of revisions .
since bk is vastly superior we use this kernel for the next research question.
summary the brownian kernel bk produces the most accurate performance history estimations with single digit prediction error rates except for numpy mape .
.
revision sampling efficiency to learn whether actively selecting samples improves over random sampling we compare the mape of the bk learned with active sampling against random sampling of revisions.
we repeated random sampling ten times and use the mean.
table iv shows the corresponding mape values highlighting better values in green.
the presented numbers are aggregated mapes over multiple variants and methods compare table i .
to verify whether active revision sampling is more efficient than random sampling we conducted a significance test.
first we need to calculate the weighted mean for xz and lrzip because these values represent mean values frommultiple variants cf.
table i .
then we use a shapiro wilk normality test to determine which kind of significance test is applicable.
since the data are not normally distributed we use a one sided non parametric paired wilcoxon signed rank test.
the p value is .
which is not significant at level .
.
in other words there seems to be a favor toward active sampling but the difference is not significant.
summary active sampling performed slightly better than random sampling but the effect is not statistically significant p value .
.
global change point estimation although the estimations with a bk kernel yielded the most accurate estimation in this analysis we consider estimations with all kernels since the kernels exhibit different properties9which can influence the shape of the gp estimation.
for all combinations of algorithms and kernels we compare the time series ground truth against gp estimations trained with and of all revisions.
we consider a change point a true positive if a change point inferred from a gp estimation matches a change point reported in the corresponding ground truth within a range of revisions.
in figure we report the f1 score a measure that summarizes precision and recall of a binary classificator for all subject systems kernels training set sizes and change point detection algorithms.
following the results from rq1 the accuracy of pinpointing global change points for the majority of experiment configurations is greater for larger training sets.
the most accurate estimations of change point locations are achieved again using the bk.
ultrajson exhibits only global few change points resulting in high accuracy.
while the bk outperformed the other kernels for rq1 the effect here is less pronounced.
summary from our gp estimates we are able to accurately derive the locations of global change points.
9unlike the other four kernels the brownian kernel is non stationary i.e.
corresponding estimated do not exhibit mean reverting dynamics.
moreover the two variants of the mat ern kernel are only differentiable one or twice respectively authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.60lrzip .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.575xz .
.
.
.
.
.
.
.
.
.
.
.
.
.04ultrajson .
.
.
.
.
.
.
.
.
.
.
.
.
.970numpy .
.
.
.
.
.
.
.
.
.
.
.
.
.
.990scipy .
.
.
.
.
.
.
.
.
.
.
.
.
.
.72pillow kernel brownian estimator bottomup kernel matern32 estimator bottomupkernel matern52 estimator bottomup kernel ratquad estimator bottomupkernel rbf estimator bottomup kernel brownian estimator binarykernel matern32 estimator binary kernel matern52 estimator binarykernel ratquad estimator binary kernel rbf estimator binarytraining sample size f1 score the f1 score is the harmonic mean of precision p and recall r. these are defined as p tp tp fp a n dr tp tp fn respectively where tp true positives fp false positives and fn false negatives.
figure comparing change points reported by two different algorithms binary segmentation and bottom up merging from the observed ground thruth and from gp estimations for five different kernels and varying training set sizes.
the training sample size refers to the percentage of total commits per software system.
v. d iscussion a. kernel selection efficacy among our selection of kernel functions estimations using the brownian kernel bk outperformed the others in terms of the lowest error rate in most scenarios only exceeding a mape of for numpy .
the error rates reported for the bk across our selection of subject system range from below to and the highest among them are reported for microbenchmarks.
this discrepancy suggests that macrobenchmarks are easier to learn than microbenchmarks.
the estimations obtained using the bk kernel show the applicability of this kernel to obtain accurate estimation models using gp regression with relatively small training sample sizes.
this high accuracy was achieved by measuring only a tiny portion of the performance history i.e.
to of all revisions .
we attribute the accuracy of estimates using bk partly to their general shape.
estimations obtained using bk resemble a piece wise linear function.
considering a segment of observations between two revisions t1andt2 witht1 t the covariance for any pair of revisions between t1and a point t primeof the segment where t1 t prime t is constant and equal tot1sincekbk x y min x y .
therefore each segment between observations is estimated with a linear function.
given the presence of change points the estimates obtained using bk provide a good fit of the observed performance histories that are segmented by change points since those can be approximated by narrow segments with a high slope.b.
revision sampling efficiency the comparison of models trained with our active sampling strategy with randomly trained models has shown only a slight yet not statistically significant improvement in terms of reported error in different scenarios.
random sampling is usually be the best sampling strategy but we provide a more systematic approach which works slightly better.
the work of roberts et al.
on using gp for time series data does not compare active data selection with a random baseline so our findings complement the understanding of the combination of active learning and gp regression.
in addition since we let gp s uncertainty guide the acquisition of new observations the training sample is uniformly distributed rather than being concentrated around change points.
therefore a balanced acquisition strategy might be a better extension because it explores the revisions guided by uncertainty and exploits the gradient of the gp estimate to accurately approach and pinpoint change points.
c. global change point estimation we were able to extract most change points from the estimated models which we validated against an established approach to locate change points in time series.
we acknowledge that our change point analysis is more exploratory nature as this is not our main objective we thrive for accurate modeling of whole performance histories .
overall our results suggest that using adaptive revision sampling extensive performance analyses over large performance histories with just a few authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii for each kernel and subject system udenotes the mean uncertainty of a kernel over all revisions and mape denotes the mean average percentage error over all revisions.
and are the sizes of the sample set with respect to the whole performance history.
for all subjects we report the average over all variants and methods respectively.
kernels u mape u mape u mapemacrobenchmarks xzrbf .
.
.
.
.
.
rqf .
.
.
.
.
.
bk .
.
.
.
.
.
mk3 .
.
.
.
.
.
mk5 .
.
.
.
.
.881lrziprbf .
.
.
.
.
.
rqf .
.
.
.
.
.
bk .
.
.
.
.
.
mk3 .
.
.
.
.
.
mk5 .
.
.
.
.
.469pillowrbf .
.
.
.
.
.
rqf .
.
.
.
.
.
bk .
.
.
.
.
.
mk3 .
.
.
.
.
.
mk5 .
.
.
.
.
.747ultrajasonrbf .
.
.
.
.
.
rqf .
.
.
.
.
.
bk .
.
.
.
.
.
mk3 .
.
.
.
.
.
mk5 .
.
.
.
.
.795numpyrbf .
.
.
.
.
.
rqf .
.
.
.
.
.
bk .
.
.
.
.
.
mk3 .
.
.
.
.
.
mk5 .
.
.
.
.
.36scipyrbf .
.
.
.
.
.
rqf .
.
.
.
.
.
bk .
.
.
.
.
.
mk3 .
.
.
.
.
.
mk5 .
.
.
.
.
.07microbenchmarks pillowrbf .
.
.
.
.
.
rqf .
.
.
.
.
.
bk .
.
.
.
.
.
mk3 .
.
.
.
.
.
mk5 .
.
.
.
.
.153ultrajasonrbf .
.
.
.
.
.
rqf .
.
.
.
.
.
bk .
.
.
.
.
.
mk3 .
.
.
.
.
.
mk5 .
.
.
.
.
.492numpyrbf .
.
.
.
.
.
rqf .
.
.
.
.
.
bk .
.
.
.
.
.
mk3 .
.
.
.
.
.
mk5 .
.
.
.
.
.986scipyrbf .
.
.
.
.
.
rqf .
.
.
.
.
.
bk .
.
.
.
.
.
mk3 .
.
.
.
.
.
mk5 .
.
.
.
.
.
measurements is possible.
to the best of our knowledge this could not have been done before.
our approach enables a widetable iv for the brownian kernel we report the mean average percentage error over all revisions and compare actively act.
and randomly rand.
sampled setups.
act.
rand.
act.
rand.
act.
rand.macrobenchmarksxz .
.
.
.
.
.
lrzip .
.
.
.
.
.
pillow .
.
.
.
.
.
ultrajson .
.
.
.
.
.
numpy .
.
.
.
.
.
scipy .
.
.
.
.
.
range of further applications such as tracking accumulated technical debt and performance evolution.
d. threats to v alidity threats to internal validity include measurement noise that may leak into when learning performance models.
we mitigate this threat by using the average over five repeated runs.
we quantified the deviation of measurements and made sure with additional measurements that the standard deviation is below with respect to the mean performance of the repeated runs.
the data sets we obtained from the library developers have been repeated times for numpy and scipy and for ultrajson and pillow about thousand times on multiple machines.
hence we are confident that our raw data are robust against measurement outliers.
moreover our learning and estimation pipeline could contain implementation errors.
we carefully reviewed all intermediate results to mitigate this threat.
also we make all scripts available at our repository.
regarding external validity we cannot claim that our approach works for all kinds of software systems.
with our evaluation we selected systems from different domains and made sure that we consider both macro and microbenchmarks.
moreover we selected popular real world systems to test our approach on realistic data.
nevertheless there are still types of system that we have not considered so far we did not include client server systems nor other performance metrics than response time.
vi.
r elated work a. performance anomaly detection the question of what is a relevant performance change is crucial for both applications in industry and academia.
we revisit a selection of approaches to identify or pinpoint performance anomalies.
most prior work focuses on a single performance indicator such as execution time.
nguyen et al.
malik et al.
and lee et al.
use statistical process control spc charts a technique derived from quality management to detect possible performance regressions.
spc charts dynamically provide a simple definition of thresholds that when exceeded indicate quality regression.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a more rigorous statistical approach is to formalize performance regression as a hypothesis testing problem applying statistical tests to ask whether a revision s performance is significantly different from a preceeding one as proposed by reichelt et al.
.
their work assesses a set of different statistical tests regarding the applicability to this problem.
heger et al.
employ continuous measurement of unit tests as performance benchmarks .
they use analysis of variance anov a as an rigorous alternative to statistical testing and compare performance distributions of different revisions.
a more sophisticated notion of performance is to consider complex indicators that are aggregated from single indicator.
foo et al.
use the correlation among performance indicators and malik et al.
use performance signatures to aggregate multiple performance indicators for instance by first performing dimensionality reduction on a vector of performance indicators with pca and tracking the evolution of pca weights with spc charts .
our approach incorporates only single metrics but aggregated performance indicators provide a possible extension.
cito et al.
apply on line change point detection to detect performance degradation in web applications and pinpoint possible root causes .
while our work uses fractions of a batch of performance observations their strategy uses a stream of continuous performance measurements at runtime.
the application of off line change point detection is a promising further research direction to infer performance anomalies in performance evolution histories.
b. gaussian process for time series analysis the application of gaussian process models to learn realworld time series data is extensively but not exclusively discussed by roberts et al.
.
they employ a similar active learning approach of uncertainty guided active sampling to minimize observation effort.
their proposed algorithm not only includes new actively sampled data points with each iteration but in contrast to our approach has the opportunity to exclude data points for which the model is highly confident.
garnett et al.
and osbourne et al.
approach the problem of learning time series data with change points by proposing a kernel which includes the location of a single change point as a hyperparameter .
we cannot resort to such kernels since for a performance evolution history we do not know if and if so how many change points there are.
c. performance of configurable software systems another area of research aims at learning performance models on configurable software systems.
the idea is to model configuration options as features in a machine learning setting and learn a corresponding prediction function.
there are different learning techniques such as classification and regression trees multi variable regression and deep neural networks .
although we consider also configurable software systems we do not model features but revisions and predict performance not for variants but revisions which is an orthogonal task.
nevertheless there are some ideas related to ours.
siegmund et al.
propose an active learning approach basedon heuristics to select variants that exhibit certain interactions of configuration options .
nair et al.
use also the idea of gp models acquisition function to sample the configuration first which point to the fasted configuration .
oh et al.
use active sampling to find an optimal configuration without building a performance model .
they do so by recursively shrinking the search space towards the fastest configurations.
although these techniques are orthogonal learning and sampling in space to ours in time we see potential in combining both toward a combined learning and sampling approach.
vii.
c onclusion we propose an approach to accurately estimate the performance evolution history for software systems using gp regression with an active sampling strategy.
guided by the uncertainty provided along estimations we iteratively expand the training set until a desired model confidence is reached.
in an exploratory analysis we confirm the presence of abrupt and substantial performance changes for six real world software systems.
we investigate the choice of five different kernels for learning gp models compare our active sampling against a random sampling baseline and estimate the locations of abrupt changes for six different software systems.
our work has confirmed the presence of global change points and we were to able to pinpoint those with high accuracy.
as global change points contribute to performance evolution we believe this aspect of performance evolution in particular deserves further attention.
it is an avenue of further research to investigate the causes of change points and triangulate inferred change point locations with insights from development and documentation artifacts.
in addition the search for change points in performance evolution time series can be disassociated from the framework of gaussian processes provided in this work.
viii.
a cknoledgements apel s work has been supported by the deutsche forschungsgemeinschaft dfg under the contract ap .
siegmund s work has been supported by the dfg under the contracts si and si .