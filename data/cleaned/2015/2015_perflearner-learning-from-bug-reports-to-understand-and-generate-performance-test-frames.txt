perflearner learning from bug reports to understand and generate performance test frames xue han department of computer science university of kentucky lexington ky usa xha225 g.uky.edutingting yu department of computer science university of kentucky lexington ky usa tyu cs.uky.edudavid lo school of information systems singapore management university singapore davidlo smu.edu.sg abstract software performance is important for ensuring the quality of softwareproducts.performancebugs definedasprogrammingerrors that cause significant performance degradation can lead to slow systems and poor user experience.
while there has been some researchonautomatedperformancetestingsuchastestcasegeneration themainideaistoselectworkloadvaluestoincreasethe programexecutiontimes.thesetechniquesoftenassumetheinitialtestcaseshavetherightcombinationofinputparametersandfocus on evolving values of certain input parameters.
however such an assumption may not hold for highly configurable real word applications inwhichthecombinationsofinputparameterscanbeverylarge.inthispaper wemanuallyanalyze300bugreportsfromthree large open source projects apache http server mysql and mozillafirefox.wefoundthat exposingperformance bugsoften requires combinations of multiple input parameters and certain input parameters are frequently involved in exposing performance bugs.guidedbythesefindings wedesignedandevaluatedanautomatedapproach perflearner toextractexecutioncommandsand inputparametersfromdescriptionsofperformancebugreportsand use them to generate test frames for guiding actual performance test case generation.
ccs concepts software and its engineering software testing and debugging keywords software testing performance bugs software mining acm reference format xue han tingting yu and david lo.
.
perflearner learning from bug reports to understand and generate performance test frames.
in proceedingsofthe201833rdacm ieeeinternationalconferenceonautomated software engineering ase september montpellier france.
acm newyork ny usa 12pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september montpellier france association for computing machinery.
acm isbn ... .
introduction softwareperformanceiscriticaltothequalityofadeployedsystem.
a performance bug can cause significant performance degradation leading to problems such as poor user experience long response time and low system throughput .
compared to functional bugs that typically cause system crashes or incorrect results performance bugs are substantially more difficult tohandle becausetheyoftenmanifestthemselvesbyspecial inputs and in specific execution environments .
over the past decade numerous research efforts have been made to analyze detect andfixperformancebugs .forexample manyprofilingtechniques havebeenproposedtodynamically determinewhatprogramentities e.g.
methods areresponsiblefor theexcessiveexecutiontimeandresourceconsumptiongivenan input.
profiling methods depend on the chosen set of input values which is a known weakness for successfully detecting performancebugsinthesubjectundertest.toaddressthisproblem several test case generation techniques have been proposed to generate large workload test inputs for increasing the chance of exposing performance bugs .
however there are several limitationsinexistingperformancetestgenerationtechniques manytechniquesfocusonevolvingthevaluesofcertaininputparameters whilekeepingtheotherparametersasdefault.forexample burnim et al.
focus on increasing the workload values of data inputs while keeping the values of configuration options as default.
these techniquesmaybeineffectiveatdetectingperformancebugsdue to combinatorial effects of different input parameters.
for example in apachebug theperformance bug isexposed onlywhen the configuration options keepalive andrequestreadtimeout are specified.
otherwise by using the default configuration this performancebugcannotbetriggeredeveniftheworkload e.g.
the number of requests is increased.
while a full performance testing with all combinations of input parameterscanaddresstheaboveproblem itisinfeasibledueto theenormouscombinationspace.forexample thelatestversion ofapache httpserverhas 618inputparameters configurationoptionsand8typesofdatainputs .itisimpracticaltotryall combinationsofvaluesfortheseinputparameters.toreducethe costofperformancetesting shenetal.
useageneticalgorithm ga asasearchheuristicforobtainingcombinationsofinputparameter values that maximize the execution time.
however thistechnique evolves all input parameters which can be inefficientbecause many parameters may not provide contributions to the application s performance.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france xue han tingting yu and david lo thegoalofourresearchistwofold.first wewanttounderstand towhatextentperformancebugsarerelatedtothecombinations of input parameters.
a study on performance bug reports from bug tracking systems such as bugzilla can help us understand thecharacteristicsofinputparametersandtheircontributionsto performancebugs.second weaimtodevelopaframeworktoautomaticallygeneratecombinationsofinputparameters alsocalledtestframes discussedinsection forguidingthegenerationof actualperformancetestcases1.tothebestofourknowledge no existing research achieves the same goal.
our main idea is to mine information from the application s bug reports to identify commands i.e.
commands for executing the program and input parameters i.e.
configuration options and datainputs thathavecausedperformancebugsandusethemto generatetestframesfortestingnewerversionsoftheapplication.
perflearner is used during software maintenance and evolution wheretheprojects issuetrackingsystemshavebeenestablished.
specifically we extract and rank commands and input parameters fromeachbugreport.wethengeneratetestframes acombination of the commands and input parameters for each bug report and prioritize the most frequently generated test frames among all bug reports.
ourhypothesis includes bug reports containa specific set of vocabulary relatedto commands and input parameters that canmaketheautomatedtextextractionpossible commandsand input parameters appearing frequently in performance bug reports maybemorelikelytotriggerperformancebugsthantheinfrequent ones.perflearnerisapplicablesoftwareprojectswithestablished issue tracking systems.
inthisresearch wemanuallyidentifiedandanalyzed300performancebugreportsfromthreepopularopensourceprojects.
we discovered that it is possible to leverage information retrieval and naturallanguageprocessingtechniques toextractcommandsand inputparametersfrombugreports.wefoundthatsomeinputparameters are more likely to cause performance bugs and should be used with higher priority in performance testing.
based on our findings we develop perflearner an approach that combines natural language processing and information retrieval to automatically extract relevant commands and input parameters from bug reports and use them to generate performance test frames for guiding performance testing.
in summary our paper makes the following contributions we develop a tool perflearner that can automatically extract performance related commands and input parameters and generate performance test frames from the bug reports.
tothebestofourknowledge thisisthefirstworkthatautomatically generatestest frames frombug reports written in natural language.
weimplement perflearner andconduct anempirical study to demonstrate its effectiveness and efficiency in generating performance test frames and detecting real performance bugs.
weenvisiontheapproachtobeappliedtoatleasttwoscenarios.first givenaperformancebugreport adeveloperwhowants to know the commands and input parameters that have caused 1an actual test case is built from a test frame by specifying a concrete value for each input parameter .thisbug mayanalyzethebugreportwithperflearner.second a testingengineercanuseperflearnertogenerateandprioritizeperformance test frames from the historical performance bug reports.
the test frames can be converted into actual test cases by giving input parameters with concrete values.
note that perflearner isorthogonal to existing performance testing tools.
existing tools focus on increasing the values of certain workload sensitive input parameterswhileassumingthetestframes i.e.
thecombination ofinputparameters exist.therefore perflearnercanbeusedto enhancetheeffectivenessandefficiencyofexistingperformance testing tools.
toevaluatetheapproach weapplyperflearnerto300bugreports collected from apache http server mysql and firefoxbug tracking systems.
our results show that perflearner is able to extract commands and input parameters from performance bug reports with a high accuracy.
when using perflearner to generate test frames compared to a state of the art combinatorial testing ct technique it generates significantly less .
test frames on average to get the ground truth test frame.
when combiningperflearner with an existing performance test input generationtool to detect randomly selected performance bugs perflearner detects out of bugs within a reasonable time whereas when using the test input generation tool alone failed to detect all bugs.
background the concept of test frame was first introduced in the categorypartitionmethodwithtestspecificationlanguage tsl .tsl wascreatedtodefinecombinationsofprograminputparameters and environment factors.
each combination is a test frame that canbeconvertedintoactualtestcases.aperformancetestframe consists of three input categories command configuration and datainput.atestframecanhaveonecommandinthecommand category zeroormoreconfigurationoptionsintheconfiguration category andzeroormoredatainputsinthedatainputcategory.
eachcommand configurationoption anddatainputinatestframe is generally referred to as a test frame element or frame element.
wedefinea command asanactiontoexecuteafunctionalunit of the program.
for example the mysql server has several datamanipulationcommands including select update and insert.
these commands correspond to three different functional units retrieve modify and add data records.
we define input parameters as explicit input points along with the command.
an input parameter can be a configuration option or adata input.
configuration options refer to a set of predefined options e.g.
command line optionsordirectivesinaconfigurationfile.datainputsrefertothe user supplied data that is processed by the command.
for example thedatainputassociatedwiththecommand updateisthenameof a table column.
figure1shows a performance bug report snippet with the associated test frame and a test case.
the test frame for manifesting thisperformancebuginvolvesthreeframeelements acommand update aconfigurationoption innodb fill factor andadata input column.aframeelementcanbe workload sensitive.inthisexample the updatecommand is workload sensitive because a large numberof updatequeriesisrequiredtotriggertheperformance authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
perflearner ase september montpellier france bug description updatestoindexed columnmuchslowerin5.
.
.repeatingthetest doneforheapengineoninnodbshowsabigregressionfor updates toanindexedcolumn.innodbismorethan2xslowerthan5.
.
and4xslowerthan5.
.
.... innodb fill factor whosedefaultvalue is ... test frame update innodb fill factor column test case update fo oi i 2where i innodb fill factor mysqlslap number of queries figure mysql bug bug.
in mysql a workload can be simulated by benchmark tools2 such as mysqlslap .
since many performance test generation techniqueshavebeenfocusingonidentifyingtheworkload sensitive inputs pinpointing the workload from a bug report may speed up this pr ocess for performance test case generation techniques.
the actual test case is created by assigning concrete values to frame elements.
performance bug study before designing our approach we wish to understand to what extentperformancebugsarerelatedtocertaincommandsandinput parameters.
.
data collection we chose three large open source software projects apache http server mysqldatabaseserver andmozillafirefoxbrowser.with publicly accessible sourcecode andwell maintained bugtracking systems these projects have been widely used as subject programs by existing bug characteristic studies .
we collected performance bugs from bug tracking systems of apache mysql andfirefox.wesearchedthesesystemsusinga set of commonly used general keywords and phrases to describe the symptoms of performance bugs such as slow latency and lowthroughput .wealsosearchedtermsthatattributetoa specific aspect of the performance problems such as cpu spikes cache hit and memory leak to identify performance bugs.
next we selected reports with the bug status field marked as either resolved verified or closed and the resolution field marked as fixed .
the whole process yielded a total of bugs.
with a large amountofthereturnedbugreports wecalculatetheneededsample sizeis300 givenaconfidencelevelof95 andaconfidenceinterval of5.thissamplingstrategyhasbeencommonlyusedbyexisting work .
we manually examined bugs in a random order and during the manual inspection we follow those reports that have sufficient bug description details and discussions posted by commentators.foreachbugreport wetrytoidentifycommands configuration options datainputs andworkloadthatcausetheperformancebug.
to ensure the correctness of our results the manual inspection was performed independently by two inspectors graduate studentswhohave2 4yearsofindustrialwebdevelopmentexperience 2abenchmarktoolisusedtomeasuretheperformanceoftheprogramundertestwith synthesized workload.table subjects and their characteristics application searched bugs sampled bugs o fc m d o fc o o fd i apache mysql firefox total withapache mysql andfirefox.weholdtwotrainingsessions of minutes each to explain to inspectors the test frame elements to be extracted from the bug report.
each inspector is given the samesetofbugseachweektowritedownwhattheyconsiderto be the command configuration options data inputs and workload thattriggerthebuginthereport.inspectorsmettwiceaweekto compare and consolidate their findings.
a bug report is selectedonly when both inspectors agree on the outcome of the manualinspection.
we refer to the consensus outcome as ground truthframe elements for the bug reports.
this process terminates for eachsubjectafter100bugreportshavebeenincludedinthesample dataset.
thenumberofbugssampledissimilartorecentworksonperformance bug study .
while a larger number of bug reportsmayyieldabetterevaluation thecostofthemanualprocess is high our data collection process took a total of to hours spanning across more than weeks.
columns of table1listthe subject programs the number ofbugs returned by the keyword search and the number of performance bugs sampledaftermanualinspection.columns4 7listthenumberofcommands configurationoptions anddatainputsavailableinallthreesubjects.
the full lists of the three categories are saved in separate frame elementdatabases includingcommanddatabase configurationdatabase and data input database.
we collected such information by studying all artifacts that are publicly available to users including documents e.g.
usermanualsandonlinehelppages configuration files and source code.
each database can be updated separately to accommodate changes in different application versions.
.
results analysis aftermanuallyanalyzing300bugreports wesummarizethefollowing findings a majority to of studied performance bugs involvesmorethanoneinputparameters i.e.
configuration options and data inputs in apache in mysql and89 infirefox.theseresultsimplythatcombinatorial effectsamonginputparametersshouldbeconsideredinperformance testing.
a significant number of performance bugs are related toconfigurations inapache inmysql and25 in firefox.
these results are consistent with a recent performance bug study .
only23 of bugsrequire specificworkload valuesto manifest inapache inmysql and19 infirefox.these results imply that workload is only part of the requirement forexposingperformancebugs otherfactors suchasconfigurationoptions shouldalso beconsidered forperformance testing.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france xue han tingting yu and david lo !
figure overview of the perflearner framework among all commands and inputs for the three software projects of them appear more than three times in the studied bug reports.
among all configuration options for the three software projects only of them are relatedtothestudiedperformancebugreportsand57 of them appear more than one time.
these results suggest that asmallsubsetofconfigurationoptionstendtoaffectapplication s performance so performance testing might focus mainly on such options to improve the efficiency of testing.
inaddition testframeelementsthatappearmultipletimesin performance bug reports might be more likely to cause performancebugsthantheothersandshouldbegivenhigher priority in performance testing.
perflearner approach guided by the findings in section we design and develop perflearner anautomatedapproachforextractingperformancetest frames from bug reports.
figure 2shows an overview of the perflearner framework.
perflearner consists of three steps frame element extraction test frame generation and performance testcase generation.
the shaded boxes indicate the information supplied by users.
frame element extraction.
given a performance bug report perflearnerautomaticallyextractsframeelementsandtheirassociated workloadfromthereport.perflearnerassumesthatabugreport hasalreadybeenlabeledas performancebug althoughexisting techniquesonclassifyingbugreports canbeadoptedto automaticallyclassifyperformancebugs.thelistofframeelementsare application and domain specific e.g.
each application is associated with a list of different configuration options.
the bug corpora foreachapplicationisbuiltfromsourcesdescribedinsection .
.
the output of this step is a list of ranked frame elements and their associated workload if any under each input category for each bug report.
performance test frame generation.
perflearner utilizes ranked frame elements a strength file and a constraint file to generate performancetestframes.thestrengthfile whichisusedtorestrict the number of test frames specifies the strength3of interaction among elements within each input category.
the constraint file 3combinatorialtestingofstrength t t requiresthateacht wisetupleofvalues of the different system input parameters is covered by at least one test case .specifies the constraints among frame elements to ensure their combinationsarevalid.bothfilesaredefinedoncebydevelopers for each application and generic to all bug reports in the sameapplication.
next perflearner generates a set of test frames for eachperformancebugreportbycombiningtheselectedcommands andinputparameterswithrespecttothestrengthandconstraint files.
these test frames are closely related to the performance bug describedinthereport.
finally perflearnercounts thefrequency of test frames generated from all bug reports and ranks them ina descending order.
the top ranked test frames are used first to generate performance test cases.
performance test case generation.
perflearner iteratively selects a testframefromtherankedtestframesandconvertsitintoactual performance test cases by assigning frame elements with concrete values.
perflearner can be combined with existing performance testingtools suchasprofilingandtestgenerationtools.thecurrent version of perflearner is combined with a performance testinput generation tool that uses a search based algorithm to automatically generate input values to expose performance bugs.
.
test frame element extraction for each bug report that is labeled as a performance bug report perflearnerextractscommands configurationoptions datainputs and the associated workload.
a straightforward approach is tomatch frame element databases against each bug report using a grep likemethod.thematchedelementscanthenberankedby countingtheiroccurrences theelementwiththehighestcount is more likely to be the ground truth frame element for the performancebug.
however in abugreportwritten innaturallanguage manywordscanbeambiguousintheirmeaning thesameword canrefertoacommandoraconfigurationoptiondependingonthe context.
for example in the apache bug the word token timeoutcanbematchedaseitheracommandoraconfiguration option.
in addition simply counting the occurrence of a token may result in false positives.
in the apache bug both startand requestappearinthebugreport sobothtokenswouldbematched ascommandsofthisbugreport.incidentally thecountof start is actually higher than the count of request although the ground truth command is request.
perflearneremploystwostrategiestoaddresstheaboveproblems.
first perflearner uses natural language processing and in formation retrieval together with user manuals to address the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
perflearner ase september montpellier france table number of patterns to detect frame elements application of matched patterns commands data inputs workload apache mysql firefox definitions drop create ... slow long ... pattern description command verbs appear in the same sentence that symptoms exist.
example cmdtable on very large tables can be very symp.
figure a common pattern to identify command definitions adp to on ... noun update insert ... pattern adp noun description datainputisidentifiedasthesubjectofthecommand.
example cmd adpindexed nounmuch slower in .
.
figure a common pattern to identify data inputs definitions file html ... verb contain has ... adj long large ... pattern verb adj noun description workload details the content of data inputs.example atext input verbavery adj noun figure a common pattern to determine a workload mismatch problem between the frame elements query and bug reports documents .
second wesummarize 18linguisticpatterns that are commonly used to describe commands eight patterns input parameters four patterns and workload in bug reports six patterns .
while the frame elements are application specific the linguisticpatternsaregenericandhencecanbereusedfordifferent applications.
toavoidoverfitting thefirstauthorsummarizedthelinguistic patterns from the bug reports excluding the sampled bug reportsinthedataset .intheexperiment thesepatternsareapplied tothe300bugreports.wecanautomaticallydetectthepresence ofthesepatternstolocatesentencesdescribingaparticularinput category and identify the frame element under that category more accurately.table 2showsthenumberofpatternsweidentifiedin all sentences from the bug reports.
while there has been some research on using linguistic patterns in other software activities suchasanalyzingdeveloperintention anddetectingmissing information littleworkisknownonusinglinguisticpatterns to identify commands and input parameters.
.
.
commands.
weobservethatacommandoftenappearswith the bug symptom in one sentence.
for example the sentence describing the symptom of apache bug is i could reproduce the100 cpuwithpostrequests wherethesymptomis cpu andthecommandis request.ifweidentifysentencescontainingbugsymptoms itnarrowsdownthesearchandimproves the accuracy of finding the performance bug triggering commands.we have defined six linguistic patterns using the part of speech tag for detecting one or more sentences containing symptoms.
if such sentences are detected perflearner matches the command againstthesesentencesandcountstheiroccurrences.theidentified kcommands are ranked at the top kposition in a descending order with respect to their occurrences.
our patterns can precisely identify commands in performancebugreports i.e.
rankedatthetop comparedtothe78 precision rate by the grep like method.
the most frequently used pattern as seen in figure illustrates a pattern that uses a verb andaphrase wheretheverbreferstothecommandelementand the phrase refers to the predefined list of phrases indicating per formance bug symptoms.
if any of the symptoms appear in the sentence the verb is identified as a candidate of the bug triggering command.ifnosymptomsentencesaredetected perflearnerprioritizes sentences that appeared in the bug report title as well as the first post and uses the grep like method to count the occurrences of commands.
if no command sentences are detected the same approach is applied to the entire bug corpus.
.
.
data inputs.
perflearner ranks data inputs in a similar way ascommandsbecausesimplymatchingabugreportagainsttheele mentsindatainputisimprecise.perflearnerdefinesfourlinguistic patterns to detect sentences that contain data inputs and rank data inputswithinthesesentences.figure 4showsoneofthecommonly usedpatterns.thispatternindicatesthatdatainputscoexistwith commandsinthesamesentence.specifically thesentencestarts with a command i.e.
update followed by a preposition i.e.
to on and the data input i.e.
column .
.
.
configuration options.
unlikecommandsanddatainputs we observe that many configuration options cannot be directly searched from bug reports.
one solution is to leverage information retrieval ir algorithms such as tf idf and cosine similarity based on the vector space model vsm to rank configuration options in terms of their relevance to the bug report.
a straightforwardmethod istosplitthe configurationnameintotokenstocalculateitscosinesimilaritytothebugreport.however weobservethatmanyconfigurationoptionssharewiththesame tokens.
since a configuration option name is often short this approachmayresultinmanyequallyrankedconfigurationoptions.
for example in figure innodb buffer pool instances and innodb buffer pool size wouldberankedequallyif innodb buffer and pool are the three word tokens appearing in the report.
toimprovetheaccuracyofranking weleveragemanualsthat describe configuration options to bridge the lexical gap between configuration option names and bug reports.
in the example of figure1 themanualdescriptionof innodb fill factor figure7 contains words such as b tree index and space which also appear in the bug report can be used to link the configuration option to the bug report effectively.
to compute the similarity between a configuration option oand a bug report br we first concatenate owith its textual description where o o o.desc.perflearnerthenprocesses obystandardnlp pre processing steps word tokenization andstop word removal.
the tokenizationconvertsbugreportsintoa bagofwords usingwhite spaces.
we then remove punctuation numbers and standard stop authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france xue han tingting yu and david lo words.compoundwords suchastheconfigurationoptionname browser.chrome.image icons.max size can be split by camel case dots or underlines into tokens.
next all words are reduced to their base form using lemmatization.
unlike stemming that simply chops off the ending of a word lemmatizationinvolvesacomplexwordanalysisandgenerallyprovides better results.
finally we also remove repeated text sections asquotationsofthepreviouscommentatorinthebugreporthappenveryfrequentlyandtherepeatedtextwouldaffecttheaccuracy oftexttokendistribution.eachbugtrackingsystemmayhavetheir ownmechanismtomarkquotations.forexample bugzillabased bugtrackingsystems quotationstartswiththegreatersign symboloneachnewlineandthequotationblockhasacssclass of quote .
developers can design their own match patterns for removing quotations and plug it into perflearner.
after processing o the combinedconfiguration optionand its description let vbethevocabularyofalltexttokensfromboththe bugreport brando.letr ando be thevsmrepresentationsofthebugreport brandtheconfiguration option o. the term weights wt brandwt oare computed using the classical tf idf method described in existing literature .
after thevectorspacerepresentationsarecomputed thetextualsimilarity score between oandbrcan be calculated using the standard cosine similarity between their corresponding vectors sim br o cos br o r o r o the score is computed by the inner product of the two vectors divided by their euclidean distance.
for mysql bug fig ure1 by utilizing the configuration api description figure perflearner ranks innodb fill factor at the top.
.
.
identifying workload.
in performance testing we need to know which frame elements are workload sensitive so testingcan focus on generating workload values for these elements.
we havedefinedsixlinguisticpatternstoidentifysuchframeelements.
the most frequently used pattern is to locate sentences containing benchmark tool names.
benchmark tools are often used to simu late workload in performance bug reports.
for instance mysqlbug report uses benchmark tool mysqlslap to generate a large number of database updates.
therefore by searching forthe benchmark name mysqlslap we can detect that updateis workload sensitive.thispatternappliesto44.
ofperformance bug reports involving specific workload.
the second commonly used linguistic pattern detects sentences describingworkloadinformationofdatainputs figure .inthis pattern the data input i.e.
a text file is followed by a verb i.e.
containing thatdetailsthecontentofinputdata i.e.averylong line .
once this pattern is detected the corresponding data input is considered to be workload sensitive.
.
performance test frame generation perflearner generates performance test frames from the ranked frameelements theworkloadspecification astrengthfile anda constraint file.
the strength file specifies top nframe elements undereachinputcategorytobeusedfortestframegeneration.the constraint fileis used toenforce constraints ofinteraction among frame elements which can limit the number of invalid frames !
!
!
!
!
!
!
!
!
!
!
!
!
.
!
!
.
!
.
figure an example of performance test frame generation innodbperformsabulkloadwhencreatingorrebuildingindexes.
this method of index creation is known as a sorted index build .
innodb fill factor defines the percentage of space on each b tree pagethatisfilledduringasortedindexbuild withtheremaining space reserved for future index growth.
figure api description for innodb fill factor to be generated.
the constraints are manually derived from user manuals.
both files are provided by users and generic to all bug reports within the same application.
figure6shows a partial constraint file of mysql.
the data definition command dropin the sql works with database and table but not with column.
we use ifto enforce conditions on which frame elements can be combined.
to enforce the rule that updateworks withtable but notdatabase condition if cmdupdate isaddedfordatainputscolumnandtable.condition is added for data inputs database.
therefore when updateis chosen it can only be combined with column and table.
ourexperiment indicatesthat addingconstraints can reduce of test frames.
in the example of figure the strength file indicates that top2 commands nc top configuration options po and top datainputs nd areselectedtogeneratetestframes.becausethe numberofoptionsisoftenlarge weuseapercentageofthetotal numberofconfigurationoptionstoindicatetheselectednumber ofconfiguration options.the threesymbols tc to and tdindicate theinteractionstrengthsforcommands configurationoptions anddatainputsrespectively.infigure apair wisecombination to is applied to the configuration options and no combinations are used forthe command tc anddata inputs td .
figure 6also showsthedefaultstrengthfileusedbyperflearner.thesestrength valuesarechosenbasedonourempiricalevaluationastheyarethe minimum requirements for generating test frames achieving up to accuracy.wealsoevaluatedthesensitivityofthesevaluesin section7.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
perflearner ase september montpellier france algorithm perflearner test frame generation require strenf consf bu reports lptn ensure tfprio forbr bu reports do rlcmd rankcmd br dbcmd lptn .cmd rlco rankconfig br dbco rldi rankdata br dbdi dbdi lptn .di lwl getworkload br dbdi lptn .wl rl selectelements rlcmd rlco rldi strenf tfbr generateframes rl lwl consf tf tfbr tf end for tfprio rankfreq tf algorithm combining perflearner with testing tools require tfprio ensure testresults fortf tfpriodo fore tfdo cate ory getinputcate ory e ifhasworkload e then e inte rateworkload cate ory e end if tc.xml updatetestcase e end for testresults runperftesttool tc.xml end for algorithm 1describes the process of generating performance test frames.
the algorithm takes as input a list of bug reports from an application a strength file and a constraint file.
for each bug report the algorithm obtains a ranked list for each input category lines and a list of workload line .
it then selects frame elements from the ranked lists with respect to the strengths.
next a list of candidate test frames is generated given the selected frame elements and the constraints line .
finally the algorithm ranks test frames collected from all bug reports line in terms of the frequency of their appearance.
test frames ranked higher indicate theymaybemorelikelytocauseperformancebugs.thelastcolumn of figure 6shows an example of the five test frames generated.
.
performance test case generation algorithm 2outlines the process of generating performance test casesfromtestframes.first perflearneriterativelyselectsatest frame from the prioritized list output by algorithm .
for each frameelement thealgorithmchecksforitsinputcategory.ifthe frame element is workload sensitive depending on the input category the algorithm applies workload in two ways line .
for the commandcategory thebenchmark optionthat controlsworkload isincludedinthetestcasegeneration.forotherinputcategories theinputsizeisincludedinthetestcasegeneration.thealgorithm updates the test case as it gets more information from frame elements line7 .specifically thetestframeisconvertedintoanxml file tc.xml ofwhichstructureisknowntothetestcasegeneration tools.
finally the test input tc.xml is supplied to the performance testingtool.itisuptotheperformancetestingtooltodeterminehow to assign input values and execute the subject under test to detect performance bugs.
implementation weimplementedawebcrawlerusingthepythonbeautifulsoup library to collect raw bug reports and api documentations.
we then leveragedpython natural language toolkit nltk to parse the description of the bug reports and match linguistic patternsagainstthe newbugreportswithregular expressionson part of speec htags.fortheinformationretrievalcomponent we utilized the python machine learning library scikit learn t o getthetf idfmatrixandcosinesimilarityscores.lastly weimplemented python programs to handle the performance test frame generation.
evaluation of perflearner we evaluated perflearner on three open source projects with characteristicsdescribedinsection .
.weaimtoanswerthefollowing research questions rq1 howaccurateisperflearneratdetectingperformancebugtriggering frame elements and workload?
rq2 how effective and efficient is perflearner at generating performance test frames?
rq3 can perflearner enhance existing performance testing tools for detecting performance bugs?
.
techniques and metrics rq1 accuracyofbugreportsanalysis.
toanswerrq1 weevaluatetheaccuracyofperflearnerinextractingframeelementsand workload.
the techniques for extracting commands configuration options data inputs and workload are denoted as cd co di wl respectively.eachtechniqueiscomparedtoabaselinemethodto evaluatetheeffectsofusingadvancedtechniquessuchaslinguistic patterns andinformation retrieval tf idf cosine similarityetc.
.
specifically we compare cd co di to three baseline techniques cds cos anddis.thesebaselinetechniquesuseakeywordmatch and count the occurrence of each frame element appearing in a bug report.
to evaluate the usefulness of configuration manuals inextractingconfigurationoptions wealsocomparecotoco a. coauses only tokens in the configuration option name without configurationmanualstomakethesimilaritycomparison.sincethe workload describes whether a frame element is workload sensitive thekeywordcountingisnotapplicableinthiscase.nevertheless toevaluatetheusefulnessoflinguisticpatternsinidentifyingthe workload the baseline technique wl rrandomly selects a frame element and treats the element as workload sensitive.
weusetwometricstoevaluatetheeffectivenessofranking.the firstmetricisthetop nsuccessrate whichiscomputedbyranksofgroundtruthswithintopnitemsoverallbugreports.forexample if20outof100performancebugreportsrankthegroundtruthof configurationoptionsinthetop5 ofall600configurationoptions the top n n success rate is .
when there are multiple elementsspecifiedasthegroundtruth weonlyconsiderthefirst one that perflearner can find.
since workload is directly identified without ranking we examine the percentage of bug reports in which ground truth workload is found.
forthesecondmetric weusemap meanaverageprecision .
map is a single figure measure of ranked retrieval results independentofthesizeofthetoplist .itisdesignedforgeneralranked retrieval problems where a query can have multiple relevant documents.
to compute map it first calculates the average precision authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france xue han tingting yu and david lo ap for each individual query qi and then calculates the mean of aps on the set of queries q map q summationdisplay qi qap qi toillustratethemapcalculation supposetherearetwoconfigurationoptions o1and o2associatedwithabugreport.iftechnique i ranksthetwooptionsatthe1stand2ndpositionsamongall500 options andtechnique ii ranksthe two optionsat the1st and 3rd positions then the map of technique i is and the map of technique ii is .
.
rq2 effectiveness and efficiency of generating performance test frames.to answer rq2 ideally the comparison should be done with existing approaches that generate performance test frames.
however we cannot find an existing approach with this specific goal.intheabsenceofsuchapproaches weinsteadcompareperflearner to a combinatorial testing ct strategy that employs the category partition method t wise testing and the random testing approach.
specifically ct generates test frames by combining elements under each input category with respect to theconstraints.thefirstdifferencebetweenperflearnerandct is that ct does not analyze bug reports or rank frame elements in terms of their relevance to the report instead ct ranks the frame elements in a random order.
the second difference is that in ct the workload is randomly assigned to a frame element.
to make a faircomparison theinteractionstrengthofconfigurationoptions and that of data inputs are the same as those used in perflearner.
to evaluate the cost effectiveness of perflearner and ct in generating performance test frames we wish to know whether frame elements frequently appeared in historical bug reports can be used togeneratetestframe fortestingfutureversionsoftheprograms.
for each bug report used for evaluation we manually inspect and derivethetestframethattriggerstheperformancebugdescribed inthereport section .
.werefertothistestframeastheground truth test frame.
since test frames cannot be executed directly we consideranapproachdetectsthebugifthegroundtruthtestframe is included in the generated test frames.
to do this we first list the bug reports from each program in ascending order by the bug creationdate.
wethen selectthe first90bug reports training set andapplytechniques perflearnerandct describedinsection .
to generate testframes.
wecompare the test frames generated by eachtechniqueagainsttheremaining10bugreports testset from eachsubject.specifically weexamineatwhichiterationtheground truth test frame of the test set bug report is generated by the technique.toevaluatetheefficiencyofthetwotechniques weevaluate the time they take to generate the ground truth test frames.
rq3 detectingperformancebugs.
besidesevaluatingperflearneron generatingperformancetestframes wewouldliketoknowwhether thegeneratedframesareusefulfordetectingactualperformance bugs.
perflearner is orthogonal to existing performance testingtools.
it aims to improve the efficiency of testing by focusing on selectingcommands andinput parametersthataremorelikely to exposeperformancebugs.toanswerrq3 wecombineperflearner withga prof aperformancetestinputgenerationtooltodetect performancebugs .wechoosega profbecauseitistheonly toolthatcanevolvebothconfigurationoptionanddatainputvalues.
ga prof employs a genetic algorithm to explore the space of inputtable rq1 test frame extraction accuracy app.metriccommand data input config.
optionmetricworkload cdcdsdidiscocospla wlwlr ap.top n91 acc.
map0.
.
.
.
.
.
.
my.top n83 acc.
map0.
.
.
.
.
.
.
fi.top n82 acc.
map0.
.
.
.
.
.
.
combinations among allinput parameters.
we re implemented the genetic algorithm part of ga prof to handle c c applications.
we compare two settingsof ga prof a default setting denoted byga inwhichthecombinationsareevolvedforallcommands andinputparameters and2 anenhancedtechnique denotedby gpplwhereitutilizestestframesgeneratedfromperflearnerto iteratively select and evolve input values to generate performance test cases.
toevaluatewhetherthetwotechniquesareabletodetectperformance bugs within a reasonable time limit we select real performancebugsthatwecanreproduce.weiterativelyselectabug reportfromthe1083performancebugreports excludingthe300 sampledbugreportsinthedataset andtrytoreproducethebug.because reproducing performance bugs is challenging and expensive we stop this process after we have bugs successfully reproduced this process took approximately work hours.
next we apply the two techniques to the program versions correspondingtothe10performancebugs.weevaluatewhether theperformancebugdescribedinthebugreportcanbedetectedand recordthetimeittakes.specifically weconducttestexperimentson high performance computer hpc clusters.
thebasic hpc node is equipped with a core .
ghz intel xeon x5650 westmere gbmemory and 256gb harddrive.
thisenvironment enablesus to run multiple experiments simultaneously without interruption.
each experiment is repeated10 times and we report the mean to reduce the bias due to randomness.
we default the time limit to hours before terminating the experiment and set the maximum number of ga iterations in each run to be .
.
results and analysis rq1 accuracy of bug reports analysis.
table3shows the effectiveness of different techniques at ranking frame elements andextracting workload.
the success rates are based on the default valuesspecifiedinthestrengthfile.theresultsindicatethatcommands appear in the top positions for of bug reports the correctdatainputappearsinthetop 2positionsfor83 ofthe bug reports the correct configuration option appears in the top returnedresultsfor71 ofthereports.additionally theworkloadisidentifiedwith56 accuracy.comparedtothebaseline approaches the success rate is higher in each category over all programs.
where the map scores are concerned perflearner is more effectivethanthebaselinetechniquesoverallthreetypesofframe elementsacrossallsubjectprograms.theimprovementsrangefrom to40 .theseresultssuggestthat heuristicsusedbyperflearner is effective in boosting accuracy.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
perflearner ase september montpellier france table rq2 performance test frame generation application of const.perflearner ct spacecount avg.
spacecount avg.
apache 445k 64m 10k mysql .4m .9b 10k firefox 443k .9b 10k of const.
the number of constraints.
space the number of total configurations w.r.t.
constraints and the default weight.
count avg.
the average number of test frames generated by the test method before reaching the ground truth.
table rq3 performance testing with ga application bug ideffectiveness efficiency gagaplgacount gpplcountpl apache54852 noyes24h8297 .2h noyes24h942910.1h nono24h8790 24h noyes24h882220.2h nono24h9037 .7h mysql21727 noyes24h861414.8h noyes24h925911.7h noyes24h845811.3h noyes24h7446 .3h nono24h9793 24h rq2 effectiveness and efficiency of performance test frame generation.table4showstheresultsofperflearnerandctingenerating performancetestframes.sincectdoesnotranktestframes we allowcttogeneratetestframesamongrandomlysampledinput spaceforeachinputcategory.welimitthenumberoftestframes to .
the threshold number is based on practical considerations as tests may take considerable executing time.
withthe default ct method all three subjects failed to generate theground truth test frame before the frame limit threshold.
these resultssuggestthat perflearnerismorecost effectiveatgenerating performance test frames than the traditional combinatorial testing approach.
figure8shows the distribution of test frames generated ineachsubjectforbothperflearner pl andct.firefoxhasthe worst performance of all this is largely due to firefox bugs require multiple steps to trigger.
firefox also has the largest number of commandsandlowestcommandextractionaccuracy.asaresult the ranking of test frames does not work as effectively as the other two subjects.
figure test frame generation rq3 enhancingperformancebugdetection.
table5showstheresults of ga and gp pl ga enhanced with perflearner .
ga failed todetectall10performancebugs.likeothertestcasegeneration techniques the genetic algorithm for generating input values is ap pliedonlyafteratestframeissele cted.however withoutknowing which frame element is more likely to cause a performance bug a random method is used to allow frame elements in each input category to have an equal chance to be selected.
as a result many low quality test frames are generated.
the ground truth test frame often fails to be generated within the time limit hours .
our results show that the gp plapproach can detect out of performance bugs within an average of .
hours.
these results suggestthat perflearner canpotentially enhance existingperformancetestingtools.
forthethreebugsgp plfailedtodetect apachebug 37680requirestwoentriesofthe listen option.when selecting configuration options we do not allow duplications of configurationoptionsincemultipleappearancesofthesameoption normally overwrites one another.
apache bug executes a testframe servergracefulstop thatcausesalongresponsetime.
thistestframeisconsideredtotriggeraperformancebug however thegroundtruthtestframeofthisbugisrelatedtocacheutilization.
thisistheonlyfalsepositivecaseappearedinourexperiment.
formysqlbug the profile commandisrequiredtotrigger this bug.
however none of the bug reports used to generate test framesincludesthecommand profile .weconjecturefalsenegative cases can be reduced as more bug reports are used for mining test frames.
figure weight sensitivity analysis discussion sensitivityofstrength.
bydefault perflearnerusesstrengths nc no nd nw t .theselectedvaluesarebasedon theempiricalstudythatachievesbesttestframeelementextracting results.
to understand the influence of selecting different sets of strengths we evaluate perflearner on two other sets of strengths w1 nc no nd nw and t andw2 nc no nd nw a n d t .
figure 9reports the results of test frame generation using the three sets of strengths on the test set bug reports for each of the three subjects.
the verticalaxis indicates the number of frames generated before reaching the groundtruth.theresultsindicatethat ingeneral defaultstrengths outperform the other two sets.
in apache w1 outperforms the default strengths in terms of the average frames generated but w1 exhibitsalargerstandarddeviation.theweightsensitivityanalysis implies that the strengths should not be set too low or too high.
lowstrengthvaluesmaycauseperflearnertomisscertainrelevant frames whereas high strength values may result in generating too many performance test frames and thus reduce the efficiency of perflearner.
threatstovalidity.
theprimarythreattotheexternalvalidityof thisstudyinvolvestherepresentativeness ofoursubjectsandbug reports.wedoreducethisthreattosomeextentbyusingseveral varieties of well studied open source projects and bug tracking authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france xue han tingting yu and david lo systems for our study.
combining keyword search and manual inspectionisaneffectivetechniquetoidentifybugsofaspecifictype fromalargepoolofgenericbugsandhasbeenusedsuccessfully inpriorstudies .wecannotclaimthatourresultscan begeneralizedtoallsystemsofalldomainsthough.theprimary threat to the internal validity involves the manual inspection to identifythegroundtruthtestframefromabugreport.tominimizetheriskofincorrectresultsgivenbymanualinspection theanalysis process was done independently by two trained inspectors.
limitations.
thetextualqualityofabugreporthassubstantial impact on the effectiveness of the proposed approach.
for example abugreportmaynotusethestandardnamesoftheframeelements.
this can be addressed by integrating advanced nlp techniques suchas word2vec .the incompletenessofbugreports isalso amajorobstacleforperflearnertoworkwell likeformanybug report analysis techniques.
one strategy is to filter out bug reports containingmissinginformationusinganautomatedapproach and apply perflearner only to complete bug reports to improveaccuracy.
other classification techniques can be integrated with perflearner as well such as detecting reproducible and duplicate bug reports.
perflearner takes only labeled performance bug reports.
one extensionpointistobuildapredictionmodelthatcanautomatically predict whether a new bug report is related to performance or not.
there has been some research on using text mining to classify bug reports which can be easily tuned to handle performance bug reports.
in addition when a performance bug requires aspecificsystemstate e.g.
networkingevents tobetriggered the current approach cannot extract such information.
for example a state may be associated with the topology of the target system e.g.
the firewall setup may negatively affect the performance of a system .nevertheless webelieveperflearnercanbeextendedto handle system level triggering events by defining additional frame databases and linguistic patterns.
related work there has been a great deal of research on analyzing detecting and fixing performance bugs .
burnim et al.
designedatechniquetogenerateworse caseinputs largerinput sizes to find performance bugs.
as discussed in section these techniques often rely on initial test cases and do not address the challenges of finding the right combination of input parameters tocreateeffectiveinitialtestcases.asourempiricalstudyshows workload only helps to trigger some but not all performance bugs.
although perflearner also takes workload into consideration it focuses more on the combination of elements to be used in the test frame.ourmethodisorthogonaltothetestcasegenerationtools as our experiment shows perflearner can be integrated into existing performancetestingtechniquestoimprovetheeffectivenessand efficiency of bug detection.
a great body of work has been conducted on applying combinatorial testing ct to address the problem of large input space in complexandconfigurablesystems .ctsystematically samplestheinputspaceandtestsonlytheselectedinputparameterscombinations.zhangetal.
proposedamethodtooptimize combinatorialtestingtogeneratetestcasestofindabalancedpointof coverage without pressuring on achieving the maximum cov erage.
dumlu et al.
proposed a feedback driven approach to detectandavoidmaskingeffectresultedfromct.thesetechniquesfocusonsamplingcombinationsfromtheentireinputspace.therefore it is often inevitable to result in a large sampling space.
inthe contrast perflearner detects and uses only the error pronecommands and input parameters from the historical bug reports.
empirical results show that our approach can significantly reduce thesamplingspacewhengeneratingtestframesforperformance bugs.
therehasbeenconsiderableworkonusingnaturallanguageand informationretrievaltechniquestoimprovecodedocumentation and understanding and to create code traceability links .while ourworkappliessome ofthesesamebasic techniques suchastokenization lemmatization vectorspacemodel with term frequency inverse document frequency weighting the prior work has not applied these techniques to performance bug reports and has not considered or extracted input parameters to generate test frames.
therehasbeenalargebodyofworkthatdemonstratestheneed for configuration aware testing techniques and proposes methods to sample and prioritize the configuration space to reduce the cost of testing.
for example jamshidi et al.
conductanempiricalstudytoevaluatethefeasibilityofapplyingthe transfer learning technique to reduce the dimensionality of the configuration space when constructing performance models.
nair et al.
use inexpensive and inaccurate models to find optimal configurations with less cost compared to the state of the art samplingtechniques.unliketheabovetechnique ourapproachfocuses on creating test frames to aim performance testing for finding performance bugs instead of performance modeling.
conclusions performancebugsaredifficulttoexposebecausetheyoftenmanifest under special input conditions and system configurations.in this paper we studied real world performance bugs fromthree popular open source projects.
our findings indicate that combinationsofinputparameters especiallyconfigurations can playanimportantroleinexposingperformancebugs.guidedby thesefindings wedesignedperflearner anautomatedapproach to extract test frame elements and to generate test frames for performance testing.
we evaluated perflearner on bug reports and the results show that perflearner extracts test frame elementswithhighaccuracy.perflearnerisalsoeffectiveingenerating performance bug triggering test frames.
our evaluation oncombiningperflearnerwithga proftodetectreal worldperformance bugs indicates that perflearner can enhance existing performancetestingtoolsforgeneratingtestcasesanddetecting performance bugs.
for reproducibility and further research perflearnerandallthedatafromtheexperimentsarepubliclyavailable at