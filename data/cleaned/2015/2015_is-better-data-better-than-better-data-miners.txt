is better data better than better data miners ?
on the benefits of tuning smote for defect prediction amritanshu agrawal department of computer science north carolina state university raleigh nc usa aagrawa8 ncsu.edutim menzies department of computer science north carolina state university raleigh nc usa tim menzies.us abstract we report and fix an important systematic error in prior studies that ranked classifiers for software analytics.
those studies did not a assess classifiers on multiple criteria and they did not b study how variations in the data affect the results.
hence this paper applies a multi performancecriteriawhile b fixingtheweaker regions ofthe training data usingsmotuned whichis an autotuning version of smote .
this approach leads to dramatically large increases in software defect predictions when applied in a 5cross validationstudyfor3 681javaclasses containingover a million lines of code from open source systems smotuned increasedaucandrecallby60 and20 respectively.theseimprovements are independent of the classifier used to predict for defects.
same kind of pattern improvement was observed when a comparativeanalysisofsmoteandsmotunedwasdoneagainst the most recent class imbalance technique.
inconclusion forsoftware analytictasks likedefect prediction datapre processingcanbemoreimportantthanclassifierchoice ranking studies are incomplete without such pre processing and smotuned is a promising candidate for pre processing.
keywords search based se defect prediction classification data analytics for software engineering smote imbalanced data preprocessing acm reference format amritanshu agrawal and tim menzies.
.
is better data better than better data miners ?
on the benefits of tuning smote for defect prediction .
in icse icse 40th international conference on software engineering may june gothenburg sweden.
acm new york ny usa pages.
introduction softwarequalitymethodscostmoneyandbetterqualitycostsexponentially more money .
given finite budgets quality assuranceresourcesareusuallyskewedtowardsareasknowntobemost safety critical or mission critical .
this leaves blind spots regions of the system that may contain defects which may be missed.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may june gothenburg sweden association for computing machinery.
acm isbn ... .
in addition to rigorously assessing critical areas a parallel activity should be to sample the blind spots .
to sample those blind spots many researchers use static code defectpredictors.sourcecodeisdividedintosectionsandresearchers annotatethecodewiththenumberofissuesknownforeachsection.
classification algorithms are then applied to learn what static code attributesdistinguishbetweensectionswithfew manyissues.such static code measures can be automatically extracted from the code base with little effort even for very large software systems .
one perennial problem is what classifier should be used to build predictors?
many papers report ranking studies where a quality measure is collected from classifiers when they are applied to data sets .theseranking studies report which classifiers generate best predictors.
research of this paper began with the question would the use of datapre processorchangetherankingsofclassifiers?
sedatasetsare oftenimbalanced i.e.
thedatainthetargetclassisoverwhelmedbyanover abundanceofinformationabouteverythingelseexceptthe target .asshownintheliteraturereviewofthispaper inthe overwhelming majority of papers se research uses smote to fix data imbalance but smote is controlled by numerous parameters which usually are tuned using engineering expertiseor left at their default values.
this paper proposes smotuned an automatic method for setting those parameters which whenassessed on defect data from classes over a million lines ofcode takenfromopensourcejavasystems smotunedoutperformed both the original smote as well as state of the art method .
to assess we ask four questions rq1 are the default off the shelf parameters for smoteappropriate for all data sets?
result smotuned learneddifferentparametersforeachdataset allof which were very different from default smote.
rq2 is there any benefit in tuning the default parameters of smotefor each new data set?
result performance improvements using smotuned are dramatically large e.g.
improvements in auc up to against smote.
in those results we see that while no learner was best across all data sets and performance criteria smotuned was most often seen in the best results.
that is creating better training data might be more important than the subsequent choice of classifiers.
acm ieee 40th international conference on software engineering authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden amritanshu agrawal and tim menzies rq3 in terms of runtimes is the cost of running smotuned worth the performance improvement?
result smotuned terminates in under two minutes i.e.
fast enough to recommend its widespread use.
rq4 how does smotuned perform against the recent class imbalance technique?
result smotuned performsbetterthanaveryrecentimbalancehandling technique proposed by bennin et al.
.
in summary the contributions of this paper are thediscoveryofanimportantsystematicerrorinmanyprior ranking studies i.e.
all of .
a novel application of search based se smotuned to handle class imbalance that out performs the prior state of the art.
dramatically large improvements in defect predictors.
potentially for any other software analytics task that uses classifiers a way to improve those learners as well.
amethodologyforassessingthevalueofpre processingdata sets in software analytics.
a reproduction package to reproduce our results then perhaps to improve or refute our results available to download from the rest of this paper is structured as follows section .
gives an overview on software defect prediction.
section .
talks about all the performance criteria used in this paper.
section .
explains the problem of class imbalance in defect prediction.
assessment ofthepreviousrankingstudiesisdoneinsection2.
.section2.
introduces smote and discusses how smote has been used in literature.section2.6providesthedefinitionofsmotuned.section3describestheexperimentalsetupofthispaperandaboveresearch questions are answered in section .
lastly we discuss the validity of our results and a section describing our conclusions.
note that the experiments of this paper only make conclusions aboutsoftwareanalyticsfordefectprediction.thatsaid manyother software analytics tasks use the same classifiers explored here for non parametricsensitivityanalysis asapre processortobuild the tree used to infer quality improvement plans to predict githubissueclosetime andmanymore.thatis potentially smotuned is a sub routine that could improve many software analyticstasks.thiscouldbeahighlyfruitfuldirectionforfuture research.
background and motivation .
defect prediction softwareprogrammersare intelligent butbusypeople.
suchbusy peopleoftenintroducedefectsintothecodetheywrite .testing software for defects is expensive and most software assessment budgets arefinite.
meanwhile assessmenteffectiveness increases exponentiallywithassessmenteffort .suchexponentialcosts exhaustfiniteresourcessosoftwaredevelopersmustcarefullydecide what parts of their code need most testing.avarietyofapproacheshavebeenproposedtorecognizedefectprone software components using code metrics lines of code complexity orprocess metrics numberofchanges recent activity .
other work such as that of bird et al.
indicatedthatitispossibletopredictwhichcomponents fore.g.
modules arelikelylocationsofdefectoccurrenceusingacomponent sdevelopmenthistoryanddependencystructure.prediction models based on the topological properties of components within them have also proven to be accurate .
thelessonofalltheaboveisthattheprobablelocationoffuture defects can be guessed using logs of past defects .
these logs mightsummarizesoftwarecomponentsusingstaticcodemetrics such as mccabes cyclomatic complexity briands coupling metrics dependencies between binaries or the ck metrics which is described in table .
oneadvantage with ck metrics is that they aresimpletocomputeandhence theyarewidelyused.radjenovi et al.
reported that in the static code defect prediction the ck metrics are used twice as much as more traditional source code metrics such as mccabes or process metrics .
the staticcodemeasuresthatcanbeextractedfromasoftwareisshown in table .
note that such attributes can be automatically collected even for very large systems .
other methods like manual code reviews are far slower and far more labor intensive.
static code defect predictors are remarkably fast and effective.
giventhecurrentgenerationofdataminingtools itcanbeamatter ofjustafewsecondstolearnadefectpredictor seetheruntimes in table of reference .
further in a recent study by rahman et al.
found no significant differences in the cost effectiveness of a static code analysis tools findbugs and jlint and b static code defect predictors.
this is an interesting result since it is much slower to adapt static code analyzers to new languages than defect predictors sincethelatterjustrequireshackingtogethersomenew static code metrics extractors .
.
performance criteria formally defectpredictionis abinary classificationproblem.the performanceof adefect predictorcan beassessed viaa confusion matrixliketable 2wherea positive outputis thedefectiveclass under study and a negative output is the non defective one.
table results matrix actual prediction falsetrue defect free tnfn defective fptpfurther false means the learner got it wrong and true means the learner correctlyidentifiedafaultornonfault module.
hence table hasfourquadrantscontaining e.g.
fpwhich denotes false positive .
from this matrix we can define performance measures like recall pd tp tp fn precision prec tp tp fp false alarm pf fp fp tn area under curve auc which is the area covered by an roccurve inwhichthex axisrepresents falsepositive rate and the y axis represents true positive rate.
as shown in figure a typical predictor must trade off betweenfalse alarmandrecall.this isbecausethe moresensitivethe detector the more often it triggers and the higher its recall.
if a authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
is better data better than better data miners ?
icse may june gothenburg sweden table oo ck code metrics used for all studies in this paper.
the last line shown denotes the dependent variable.
amc average method complexity e.g.
number of java byte codes avg cc average mccabe average mccabe s cyclomatic complexity seen in class ca afferent couplings how many other classes use the specific class.
cam cohesion amongst classes summationofnumberofdifferenttypesofmethodparametersineverymethoddividedbyamultiplicationof number of different method parameter types in whole class and number of methods.
cbm coupling between methods total number of new redefined methods to which all the inherited methods are coupled cbo coupling between objects increased when the methods of one class access services of another.
ce efferent couplings how many other classes is used by the specific class.
dam data access ratio of the number of private protected attributes to the total number of attributes dit depth of inheritance tree ic inheritance coupling number of parent classes to which a given class is coupled lcom lack of cohesion in methods number of pairs of methods that do not share a reference to an case variable.
locm3 another lack of cohesion measure ifm aarethenumberof methods attributes inaclassnumberand a isthenumberofmethodsaccessing an attribute then lcom3 a summationtext ja a j m m .
loc lines of code max cc maximum mccabe maximum mccabe s cyclomatic complexity seen in class mfa functional abstraction no.
of methods inherited by a class plus no.
of methods accessible by member methods of the class moa aggregation count of the number of data declarations class fields whose types are user defined classes noc number of children npm number of public methods rfc response for a class number of methods invoked in response to a message to the object.
wmc weighted methods per class ndefects raw defect counts numeric number of defects found in post release bug tracking systems.
defects present?
boolean ifndefects 0thentrueelsefalse detectortriggersmoreoften italsoraisesmorefalsealarms.hence when increasing recall we should expect the false alarm rate to increase ideally not by very much .
figure trade offs false alarm vs recall probability of detection .there are many more ways to eval uate defect predictorsbesidesthefour listed above.
previously menziesetal.
catalogued dozensof them see table23.
of and even several novel ones were proposed balance g measure .
butnoevaluationcriteriais best sincedifferentcriteriaareappropriate in different business contexts.
for e.g.
as shown in figure whendealingwithsafety criticalapplications managementmaybe risk adverse and hence many elect to maximize recall regardless ofthetimewastedexploringfalsealarm.similarly whenrushing somenon safetycriticalapplicationtomarket managementmay be costadverse andelectto minimizefalsealarm sincethisavoids distractions to the developers.
in summary there are numerous evaluation criteria and numerousbusinesscontextswheredifferentcriteriamightbepreferred bydifferentlocalbusinessusers.inresponsetothecornucopiaof evaluation criteria we make the following recommendations a doevaluatelearnersonmorethanonecriteria b donotevaluate learners on all criteria there are too many and instead apply the criteria widely seen in the literature.
applying this advice this paperevaluatesthedefectpredictorsusingthefourcriteriamentioned above since these are widely reported in the literature butnotothercriteriathathaveyettogainawideacceptance i.e.
balance and g measure .
.
defect prediction and class imbalance class imbalance is concerned with the situation in where someclasses of data are highly under represented compared to otherclasses .byconvention theunder representedclassiscalled theminority class and correspondingly the class which is overrepresentediscalledthe majorityclass.inthispaper wesaythat classimbalanceis worsewhentheratioofminorityclasstomajority increases thatis class imbalanceof5 isworsethan .menzies et al.
reported se data sets often contain class imbalance.
in their examples they showed static code defect prediction data sets with class imbalances of .
the problem of class imbalance is sometimes discussed in the software analytics community.
hall et al.
found that models basedonc4.5under performiftheyhaveimbalanceddatawhile naivebayesandlogisticregressionperformrelativelybetter.their general recommendation is to not use imbalanced data.
some researchersofferpreliminaryexplorationsintomethodsthatmight mitigate for class imbalance.
wang et al.
and yu et al.
validatedthehalletal.resultsandconcludedthattheperformance ofc4.5isunstableonimbalanceddatasetswhilerandomforest and naive bayes are more stable.
yan et al.
performed fuzzy logic and rules to overcome the imbalance problem but they only explored one kind of learner support vector machines .
pelayo et al.
studiedtheeffectsofthepercentageofoversamplingand undersamplingdone.theyfoundoutthatdifferentpercentageof eachhelpsimprovetheaccuraciesofdecisiontreelearnerfordefect prediction using ck metrics.
menzies et al.
undersampled the non defectclasstobalancetrainingdataandreportedhowlittlein formationwasrequiredtolearnadefectpredictor.theyfoundthat throwing away data does not degrade the performance of naive bayesandc4.5decisiontrees.otherpapers haveshown the usefulness of resampling based on different learners.
wenotethatmanyresearchersinthisarea referto the smote method explored in this paper but only in the context of future work.
one rare exception to this general pattern is the recentpaperbybenninetal.
whichweexploredaspartofrq4.
.
ranking studies a constant problem in defect prediction is what classifier should beappliedtobuildthedefectpredictors?toaddressthisproblem many researchers run ranking studies where performance scores authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden amritanshu agrawal and tim menzies table classifiers used in this study.
rankings from ghotra et al.
.
rank learner notes best rf random forest random forest of entropy based decision trees.
lr logistic regression a generalized linear regression model.
knn k means classify a new instance by finding k examples of similar instances.
ghortra et al.
suggested k .
nb naive bayes classify a new instance by a collecting mean and standard deviations of attributes in old instances of different classes b return the class whose attributes are statistically most similar to the new instance.
dt decision trees recursively divide data by selecting attribute splits that reduce the entropy of the class distribution.
worst svm support vector machines map the raw data into a higher dimensional space where it is easier to distinguish the examples.
table highly cited software defect prediction studies.
refyearcitationsranked classifiers?evaluated using multiple criteria?considered data imbalance?
are collected from many classifiers executed on many software defectdatasets .
this section assesses those ranking studies.
we will say a ranking study is good if it compares multiple learners using multiple data sets and multiple evaluation criteria while at the same time something to address the data imbalance problem.
figure summary of table .injuly2017 wesearched scholar.google.com for the conjunction of software and defectprediction and oo and ck published in the lastdecade.
this returned231 results.
we only selected oo and ck keywordssinceckmetrics are more popular and better than process metrics for software defect prediction .
from that list we selected highly cited papers which we defined as having more than citations per year.
this reduced our population of papers down to107.afterreadingthetitlesandabstractsofthosepapers andskimming the contents of the potentially interesting papers we found22papersoftable4thateitherperformedrankingstudies asdefinedabove orstudiedtheeffectsofclassimbalanceondefect prediction.inthecolumn evaluatedusingmultiplecriteria papersscoredmorethan iftheyusedmultipleperformancescoresof the kind listed at the end of section .
.
wefindthat inthose22papersfromtable4 numerousclassifiershaveusedaucasthemeasuretoevaluatethesoftwaredefect predictor studies.
we also found that majority of papers from last column of table in se community has used smote to fix the data imbalance .
this also made us to propose smotuned.
as noted in no single classification techniquealwaysdominates.
thatsaid tableix ofarecentstudy by ghotra et al.
ranks numerous classifiers using data similar to what we use here i.e.
oo java systems described using ck metrics .usingtheirwork wecanselectarangeofclassifiersfor this study ranking from best to worst see table .
the key observation to be made from this survey is that as showninfigure2 theoverwhelmingmajorityofpriorpapersin oursample donotsatisfy ourdefinitionofa good project thesole exception is the recent bennin et al.
which we explore in rq4 .
accordingly the rest of this paper defines and executes a good ranking study with an additional unique feature of an auto tuning version of smote.
.
handling data imbalance with smote smote handles class imbalance by changing the frequency ofdifferent classes of the training data .
the algorithm s name isshortfor syntheticminorityover samplingtechnique .when appliedtodata smotesub samplesthemajorityclass i.e.
deletes someexamples whilesuper samplingtheminorityclassuntilall classeshavethesamefrequency.inthecaseofsoftwaredefectdata the minority class is usually the defective class.
defsmote k m r defaults whilemajority mdo deleteanymajority item random whileminority mdo add something like any minority item defsomething like x0 relevant emptyset k while k1 andsize found k all k1 nearest neighbors relevant items in all of x0class z anyof found y interpolate x0 z returny defminkowski distance a b r return iabs ai bi r r figure pseudocode of smotefigure shows howsmoteworks.
duringsuper sampling a member of the minority class finds k nearestneighbors.it builds an artificial member of the minority class at some pointin betweenitself and one of its randomnearestneighbors.
during thatprocess some distancefunctionisrequired which is the minkowski distance function.
smote s control parameters are a kthat selects how many neighborstouse defaultsto k b mishowmanyexamplesof authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
is better data better than better data miners ?
icse may june gothenburg sweden each class which need to be generated defaults to m of the totaltrainingsamples and rwhichselectsthedistancefunction default is r i.e.
use euclidean distance .
in the software analytics literature there are contradictory findingsonthevalueofapplyingsmoteforsoftwaredefectprediction.
van et al.
pears et al.
and tan et al.
found smote to be advantageous while others such as pelayo et al.
did not.
further someresearchersreportthatsomelearnersrespondbetterthanotherstosmote.kameietal.
evaluatedtheeffectsof smoteappliedtofourfault pronenessmodels lineardiscriminant analysis logistic regression neural network and decision tree by using two module sets of industry legacy software.
they reported thatsmoteimprovedthepredictionperformanceofthelinearand logisticmodels butnotneuralnetworkanddecisiontreemodels.
similar results that the value of smote was dependent on the learner was also reported by van et al.
.
recently bennin et al.
proposed a new method based on the chromosomal theory of inheritance.
their mahakil algorithm interpretstwodistinctsub classesasparentsandgeneratesanew synthetic instance that inherits different traits from each parent and contributesto thediversity withinthe datadistribution.
they report that mahakil usually performs as well as smote but doesmuchbetterthanallotherclassbalancingtechniquesinterms of recall.
please note that work did not consider the impact of parameter tuning of a preprocessor so in our rq4 we will compare smotuned to mahakil.
.
smotuned auto tuning smote one possibleexplanation for the variabilityin the smote results is that the default parameters of this algorithm are not suited to all data sets.
to test this we designed smotuned which is an auto tuning version of smote.
smotuned uses different control parameters for different data sets.
smotuned uses de differential evolution to explore the parameter space of table .
de is an optimizer useful for functions that may not be smooth or linear.
vesterstrom et al.
find de s optimizations to be competitive with other optimizers like particle swarm optimization or genetic algorithms.
des have been usedbefore for parameter tuning but this paper is thefirstattempttodode basedclassre balancingforsedataby studying multiple learners for multiple evaluation criteria.
in figure de evolves a frontierof candidates from an initialpopulationwhichisdrivenbyagoal likemaximizingrecall evaluated using a fitness function shown in line .
in the caseof smotuned each candidate is a randomly selected value for smote s k mandrparameters.toevolvethefrontier withineach generation decompareseachitemtoa newcandidategenerated bycombining threeotherfrontier items andbetter newcandidates replace older items .
to compare them the betterfunction line calls smot efunction fromfigure3 usingtheproposed new parameter settings.
this pre processed training data is then fed into a classifier to find a particular measure like recall .
when our de terminates it returns the best candidate ever seen in the entire run.
table6providesimportanttermsofsmotunedwhenexploring smote s parameter ranges shown in table .
to define theparameters we found the range of used settings for smote and1 defde n cf .
f .
default settings frontier sets of guesses n best frontier.
any value at all lives while lives tmp empty fori 1t o frontier size of frontier old frontier i x y z anythreefromfrontier picked at random new copy old forj 1t o new for all attributes ifrand cf at probability cf... new.j x.j f z.j y.j ...change item j end for n e w n e wif better new old elseold tmp i n e w ifbetter new best then be s t n e w lives enable one more generation end end for frontier tmp end while returnbest figure smotuned uses de differential evolution .
table smote parameters paradefaults used by smotetuning range explored by smotuned description k number of neighbors m number of synthetic examples to create.expressedasapercentof final training data.
r power parameter for the minkowski distance metric.
table important terms of smotuned algorithm keywords description differential weight f .
mutation power crossover probability cf .
survival of the candidate population size n frontier size in a generation livesnumber of generations fitness function better driving factor of de rand function returns between to best or output optimal configuration for smote distance functions in the se and machine learning literature.
toavoidintroducingnoisebyoverpopulatingtheminoritysamples we are not using mas percentage rather than number of examples to create.
aggarawal et al.
argue that with data being highly dimensional rshould shrink to some fraction less than one hence the bound of r .
in table .
experimental design this experiment reports the effects on defect prediction after using mahakil or smotuned or smote.
using some data di d performance measure mi m and classifier ci c this experimentconductsthe5 5cross validationstudy definedbelow.our datasets dareshownintable7.theseareallopensourcejava oo systems described in terms of the ck metrics.
since we are comparingtheseresultsforimbalancedclass onlyimbalancedclass data sets were selected from seacraft our performance measures mwere introduced in section .
which includes auc precision recall and the false alarm.
our authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden amritanshu agrawal and tim menzies classifiers ccomefromarecentstudy andwerelistedintable3.
for implementations of these learners we used the open source tool scikit learn .
our cross validation study is defined as follows we randomized the order of the data set difive times.
this reduces the probability that some random ordering of examples in the data will conflate our results.
each time we divided the data diinto five bins for each bin the test we trained on four bins the rest and then tested on the test bin as follows.
a the training set is pre filtered using either no smote i.e.
do nothing or smote or smotuned.
b when using smotuned we further divide those four bins oftrainingdata.3binsareusedfortrainingthemodel and bin is used for validation in de.
de is run to improve the performance measure miseen when the classifier ci was applied to the training data.
important point we only usedsmoteon the training data leaving the testing data unchanged.
c after pre filtering a classifier cilearns a predictor.
d themodelisappliedtothetest datatocollectperformance measure mi.
e we print the relative performance delta between this miand another migeneratedfromapplying citotherawdata di i.e.
compare the learner without any filtering .
we finally report median on the repeats.
note that the above rig tunes smote but not the control parametersoftheclassifiers.wedothissince inthispaper weaimto documentthebenefitsoftuningsmotesinceasshownbelow they are very large indeed.
also it would be very useful if we can show thatasinglealgorithm smotuned improvestheperformanceof defectprediction.thiswouldallowsubsequentworktofocuson the task of optimizing smotuned which would be a far easier task than optimizing the tuning of a wide range of classifiers .
.
within vs cross measure assessment we call the above rig as the within measure assessment rig since it isbiasedin itsevaluationmeasures.
specifically inthis rig when smotuned is optimized for e.g.
auc we do not explore theeffects on e.g.
the false alarm.
this is less than ideal since it is table7 datasetstatistics.datasetsaresortedfromlowpercentageofdefectiveclasstohighdefectiveclass.datacomesfrom the seacraft repository .version dataset name defect no.
of classes lines of code .
jedit .
camel .
.
tomcat .
ivy .
arcilook .
.
redaktor .7apache ant .
synapse .
.
.
velocity total 314known that our performance measures are inter connected viathe zhang s equation .
hence increasing e.g.
recall might potentially have the adverse effect of driving up e.g the false alarmrate.toavoidthisproblem wealsoapplythefollowing crossmeasure assessment rig.
at the conclusion of the within measure assessment rig we will observe that the auc performance measure will show the largest improv ements.
using that best performer we will re apply steps abcde listed above but this time in step 3b we will tell smotuned to optimize for auc instep 3d 3ewe willcollect theperformancedelta onaucas well as precision recall and false alarm.
inthisapproach steps3dand3ecollecttheinformationrequiredto checkifsucceedingaccordingtooneperformancecriteriaresults indamagetoanother.wewouldalsowanttomakesurethatour model is not over fitted based on one evaluation measure.
and sincesmotunedisatimeexpensivetask wedonotwanttotune for each measure which will quadruple the time.
the results of within vs cross measure assessment is shown in section .
.
statistical analysis whencomparingtheresultsofsmotunedtoothertreatments we useastatisticalsignificancetestandaneffectsizetest.significance test are useful for detecting if two populations differ merely byrandom noise.
also effect sizes are useful for checking that two populations differ by more than just a trivial amount.
for the significance test we used the scott knott procedure .
this technique recursively bi clusters a sorted set of numbers.
ifanytwoclustersarestatisticallyindistinguishable scott knott reports them both as one group.
scott knott first looks for a break inthesequencethatmaximizestheexpectedvaluesinthedifference in the means before and after the break.
more specifically it splits lvaluesintosub lists mandninordertomaximizetheexpected value of differences in the observed performances before and after divisions.
for e.g.
lists l mandnof size ls msandnswhere l m n scott knottdividesthesequenceatthebreakthatmaximizes e ms ls abs m. l. ns ls abs n. l. scott knottthenappliessomestatisticalhypothesistest htocheck ifmandnare significantly different.
if so scott knott then recursesoneachdivision.forthisstudy ourhypothesistest hwasa conjunctionofthea12effectsizetest endorsedby andnonparametric bootstrap sampling i.e.
our scott knott divided the data if bothbootstrapping and an effect size test agreed that the division was statistically significant confidence and not a small effect a12 .
.
results rq1 arethedefault off the shelf parametersforsmoteappropriate for all data sets?
as discussed above the default parameters for smote k m andrare and .
figure shows the range of parameters found by smotuned across nine data sets for the repeatsof our cross validation procedure.
all the results in this figure arewithin measureassessment results i.e.
here wesmotuned on a particular performance measure and then we only collect performance for that performance measure on the test set.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
is better data better than better data miners ?
icse may june gothenburg sweden figure 5a tuned values for k default k .
figure 5b tuned values for m default m .
figure 5c tuned values for r default r .
figure data sets vs parameter variation when optimized for recall and results reported on recall.
median denotes 50th percentile values seen in the cross validations and iqr shows the intra quartile range i.e.
th percentiles.
figure smotuned improvements over smote.
within measure assessment i.e.
for eachof these charts optimize for performance measure mi then test for performance measure mi .
for most charts largervalues are better but for false alarm smallervalues are better.
note that the corresponding percentage of minority class in this case defective class is written beside each data set.
infigure5 the medianisthe50thpercentilevalueand iqristhe th percentile variance .
as can be seen in figure most of thelearnedparametersarefarfromthedefaultvalues median k isneverlessthan11 median mdiffersaccordingtoeachdataset andquitefarfromtheactual the rusedinthedistancefunction was never rather it was usually .
hence our answer to rq1is no the use of off the shelf smote should be deprecated.
we note that many of the settings in figure are very similar for e.g.
median values of k and r seems to be a commonresultirrespectiveofdataimbalancepercentageamong the datasets.
nevertheless we do notrecommend replacing the defaults of smote with the findings of figure .
also iqr bars are very large.
clearly smotuned s decisions vary dramatically depending on what data is being processed.
hence we strongly recommend that smotuned be applied to each new data set.
rq2 is there any benefit in tuning the default parameters of smote for each new data set?figure shows the performance delta of the within measure assessmentrig.pleaserecallthatwhenthisrigappliessmotuned itoptimizesforperformancemeasure mi recall precision fa l s e alarm au c after which it uses the sameperformance measure miwhenevaluatingthetestdata.infigure6 eachsubfigureshows that de is optimized for each m iand results are reported against thesame m i.fromthefigure6 itisobservedthatsmotuned achieves large auc about and recall about improvementsrelativelywithoutdamagingprecisionandwithonlyminimal changestofalsealarm.anotherkeyobservationherethatcanbe made is thatimprovements in auc with smotuned is constant whether imbalance is of or .
another note should be taken oftheaucimprovements thatthesearethelargestimprovements we have yet seen for any prior treatment of defect prediction data.
also for the raw auc values please see figure offers a statistical analysis of different results achieved after applying our three data pre filtering methods no d o nothing s1 use default smote and s2 use smotuned.
forany learner there arethreesuchtreatmentsand darkerthecell authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden amritanshu agrawal and tim menzies figure7 scottknottanalysisofno smote smoteandsmotuned.thecolumnheadersaredenotedasnoforno smote s1 for smote and s2 for smotuned.
mark represents the best learner combined with its techniques.
betterthe performance.
in that figure cells with the same color are eithernotstatisticallysignificantlydifferentoraredifferentonly via asmall effect as judged by the statistical methods described in section .
.
as to what combination of pre filter learner works better for anydataset thatismarkedbya .sincewehavethreepre filtering methods and six learners providing us with in total treatments and representsthebestlearnerpickedwithhighestmedianvalue.
in the auc and recall results the best cell always appears in thes2 smotunedcolumn i.e.
smotunedisalwaysusedby the best combination of pre filter learner .
as to precision results at first glance the results in figure lookbadforsmotunedsince lessthanhalfthetimes thebest happens in s2 smotuned column.
but recall from figure thattheabsolutesizeoftheprecisiondeltasisverysmall.hence even though smotuned losses in this statistical analysis thepragmatic impact of that result is negligible.
but if we can get feedbackfromdomain expert wecanchangebetweensmoteand smotuned dynamically based on the measures and data miners.
astothefalsealarmresultsfromfigure7 asdiscussedabove in section .
the cost of increased recall is to also increase the false alarm rate.
for e.g.
the greatest increasein the recall was .
seen in the jeditresults.
this increase comes at a cost of increasing thefalsealarmrateby0.
.apartfromthisonelargeoutlier the overall pattern is that the recall improvements range from .
to .
median to max and these come at the cost of much smaller false alarm increaseof .
to .
median to max .
insummary theanswerto rq2isthatouraucandrecallresults stronglyendorsetheuseofsmotunedwhiletheprecisionand false alarm rates show there is little harm in using smotuned.
before moving to the next research question we note that these results offer an interesting insight on prior ranking studies.
based on the ghotra et al.
results of table our expectation was that random forests rf would yield the best results across this defect data.figure7reportsthat aspredictedbyghotraetal.
rfearns more stars thananyotherlearner i.e.
itisseentobe best more often than anything else.
that said rf was only best in of those results i.e.
even our best learner rf fails over half the time.
it is significant to note that smotuned was consistently used by whatever learner was found to be best in recall and auc .hence we conclude prior ranking study results that only assessed differentlearners havemissedamuchmoregeneraleffect i.e.it canbemoreusefultoreflectondatapre processorsthanalgorithm selection.
to say that another way at least for defect prediction better data might be better than better data miners .
rq3 intermsofruntimes isthecostofrunningsmotuned worth the performance improvement?
figure shows the mean runtimes for running a crossvalidation study forsix learners for each data set.these runtimes werecollectedfromonemachinerunningcentos7 with16cores.
notethattheydonotincreasemonotonicallywiththesizeofthe datasets aresultwecanexplainwithrespecttotheinternalstructure of the data.
our version of smote uses ball trees to optimize the nearest neighbor calculations.
hence the runtime of that algorithm is dominated by the internal topology of the data sets rather thanthenumberofclasses.also asshowninfigure3 smotuned explores the local space until it finds kneighbors of the same class.
this can take a variable amount of time to terminate.
figure data sets vs runtimes.
note that the numbersshown here are the mean times seen across repeats of a5 cross validation study.
as expected smotuned is an order of magnitude slower than smotesinceithastorunsmotemanytimestoassessdifferentpa rametersettings.thatsaid thoseruntimesarenotexcessivelyslow.
smotunedusuallyterminatesinundertwominutesandnever more than half an hour.
hence in our opinion we answer rq3 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
is better data better than better data miners ?
icse may june gothenburg sweden figure smotuned improvements over mahakil .
within measure assessment i.e.
for each of these charts optimize for performance measure mi then test for performance measure mi .
same format as figure .
as yes since the performance increment seen in figure is more than to compensate for the extra cpu required for smotuned.
rq4 how does smotuned perform against more recent class imbalance technique?
all the above work is based on tuning the original smote paper .
while that version of smote is widely used in the se literature it is prudent to compare smotuned with more recent work.ourreadingoftheliteratureisthatthemahakilalgorithm ofbenninetal.
representsthemostrecentworkinseonhandling class imbalance.
at the time of writing of this paper early august2017 therewasnoreproductionpackageavailableformahakilsowewroteourownversionbasedonthedescriptioninthat paper available on we verified our implementationontheirdatasets andachievedclosetotheirvalues .
.
the difference could be due to different random seed.
figure9comparesresultsfrommahakilwiththosefromsmotuned.theseresultsweregeneratedusingthesameexperimental methods as used for figure those methods were described in section3.
.thefollowingtablerepeatsthestatisticalanalysisof figure7toreporthow oftensmote smotuned ormahakil achieves best results across nine data sets.
note that in this following table largervalues are better number of wins treatments aucrecall precision false alarm mahakil smote smotuned these statistical tests tell us that the differences seen in figure are large enough to be significant.
looking at figure there are9datasetsonx axis andthedifferencesinprecisionaresosmallin out of those data sets that the pragmatic impact of those differencesissmall.astoaucandrecall weseethatsmotuned generatedlarger andbetter resultsthanmahakil especiallyfor recall .smotunedgeneratesslightlylargerfalsealarmsbut in data sets the increase in the false alarm rate is very small.accordingtoitsauthors mahakilwasdevelopedtoreduce the false alarm rates on smote and on that criteria it succeeds as seeninfigure9 sincesmotuneddoesleadtoslightlyhigherfalsealarmrates .but asdiscussedaboveinsection2.
thedownsideonminimizingfalsealarmsisalsominimizingourabilitytofinddefects whichismeasuredintermsofaucandrecall smotuneddoes best.
hence if this paper was acomparative assessment ofsmotuned vs mahakil we would conclude that by recommending smotuned.
however thegoal of this paperis to defend theclaim that better data could be better than better data miners i.e.
data preprocessingismoreeffective thanswitchingtoanotherdataminer.
in this regard there is something insightful to conclude if we combine the results of bothmahakil and smotuned.
in the mahakilexperiments theresearchersspentsometimeontuningthelearner sparameters.thatis figure9isreallyacomparisonoftwo treatments tuned data miners adjust data against just using smotuned to adjust the data.
note that smotuned still achieves better results even though the mahakil treatment adjusted both dataanddataminers.sincesmotunedperformedsowellwithout tuningthedataminers wecanconcludefromtheconjunctionof these experiments that better data is better than using better data miners .
of course there needs to be further studies done in other se applications to make the above claim.
there is also one more treatmentnotdiscussedinthepaper tuning boththedatapre processor andthedataminers.thisisavery verylargesearchspacesowhile we have experiments running to explorethis task atthis time we have not definitive conclusions to report.
threats to validity as with any empirical study biases can affect the final results.
therefore anyconclusionsmadefromthisworkmustconsiderthe following issues in mind.
orderbias witheachdatasethowdatasamplesaredistributed in training and testing set is completely random.
though there authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden amritanshu agrawal and tim menzies figure10 smotunedimprovementsoversmote.cross measureassessment i.e.
foreachofthesecharts optimizeforauc then test for performance measure mi .
same format as figure .
could be times when all good samples are binned into training and testingset.tomitigatethisorderbias weruntheexperiment25 times by randomly changing the order of the data samples each time.
samplingbias threatensanyclassificationexperiment i.e.
what matterstheremaynotbetruehere.fore.g.
thedatasetsusedhere comesfromtheseacraftrepositoryandweresuppliedbyone individual.
these data sets have used in various case studies by various researchers i.e.
our results are not more biasedthanmanyotherstudiesinthisarena.thatsaid ournine open source data sets are mostly from apache.
hence it is an open issueifourresultsholdforproprietaryprojectsandopensource projects from other sources.
evaluation bias in terms of evaluation bias our study is far less biased than many other ranking studies.
as shown by our sampleof22rankingstudiesintable4 22ofthosepriorstudies usedfewerevaluation criteria than the four reported here auc recall precision and false alarm .
theanalysisdoneinrq4couldbeaffectedbysomeothersettings whichwemightnothaveconsideredsincethereproductionpackage was not available from the original paper .
that said there is anothermoresubtleevaluationbiasarisesinthefigure6.thefour plots of that figure are four differentruns of our within measure assessment rig defined in section .
.
hence it is reasonable to check what happens when a one evaluation criteria is used to control smotuned and b the results are assessed using all four evaluation criteria.
figure shows the results of such a crossmeasureassessmentrig whereaucwasusedtocontrolsmotuned.
wenotethattheresultsinthisfigureareverysimilartofigure6 e.g.
theprecisiondeltasaverusuallytiny andfalsealarmincreases areusuallysmallerthantheassociatedrecallimprovements.but there are some larger improvements in figure than figure .hence we recommend cross measure assessment only if cpu iscritically restricted.
otherwise we think smotuned should becontrolledbywhateveristhedownstreamevaluationcriteria as done in the within measure assessment rig of figure .
conclusion prior work on ranking studies tried to improve software analytics by selecting better learners.
our results show that there may bemorebenefits in exploring data pre processors like smotuned because we found that no learner was usually best across all data sets and all evaluation criteria.
on one hand across the same data sets smotunedwasconsistently usedby whateverlearner was found to be best in the auc recall results.
on the other hand fortheprecisionandfalsealarmresults therewaslittleevidence against the use of smotuned.
that is creating better training data usingtechniques like smotuned maybe more important than the subsequent choice of a classifier.
to say that another way at least for defect prediction better data is better than better data miners .
asto specificrecommendations we suggestthat anyprior ranking study which did not study the effects of data pre processing needs to be analyzed again.
any future such ranking study should includeasmote likepre processor.smoteshouldnotbeused with its default parameters.
for each new data set smote should be used with some automatic parameter tuning tool in order tofind the best parameters for that data set.
smotuned is one of the examplesof parametertuning.
ideally smotuned should be tunedusingtheevaluationcriteriausedtoassessthefinalpredictors.
however if there is not enough cpu to run smotuned for eachnewevaluationcriteria smotunedcanbetunedusingauc.