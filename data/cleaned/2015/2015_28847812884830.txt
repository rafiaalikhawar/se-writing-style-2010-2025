multi objective software effort estimation federica sarro alessio petrozzielloyand mark harman university college london london united kingdom university of portsmouth portsmouth uky f.sarro ucl.ac.uk alessio.petrozziello port.ac.uk mark.harman ucl.ac.uk abstract we introduce a bi objective e ort estimation algorithm that combines con dence interval analysis and assessment of mean absolute error.
we evaluate our proposed algorithm on three di erent alternative formulations baseline comparators and current state of the art e ort estimators applied to ve real world datasets from the promise repository involving di erent software projects in total.
the results reveal that our algorithm outperforms the baseline state of the art and all three alternative formulations statistically signi cantly p and with large e ect size a12 over all ve datasets.
we also provide evidence that our algorithm creates a new state of the art which lies within currently claimed industrial human expert based thresholds thereby demonstrating that our ndings have actionable conclusions for practicing software engineers.
categories and subject descriptors d. .
management keywords software e ort estimation multi objective evolutionary algorithm con dence interval estimates uncertainty.
.
introduction e ort estimation is a critical activity for planning and monitoring software project development in order to deliver the product on time and within budget .
the competitiveness and occasionally the survival of software organisations depends on their ability to accurately predict the e ort required for developing software systems both over or under estimates can negatively a ect the outcome of software projects .
several algorithmic approaches have been proposed in literature to support software engineers in improving the accuracy of their estimations.
these methods often produce a point estimate of the e ort required to develop a new project.
permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for third party components of this work must be honored.
icse may austin tx usa c copyright held by the owner author s .
acm isbn .
few previous studies have accounted for the inherent uncertainty of the estimates produced .
some previous work has instead investigated the overcon dence and or under con dence of the prediction given by expert judgement .
existing surveys on estimation practice suggest that human e ort estimates are over optimistic and there is a strong over con dence in their accuracy.
we introduce a multi objective evolutionary approach that seeks to build a robust estimation model by simultaneously maximising the estimation accuracy and minimising the uncertainty associated with the estimation model itself.
we named this approach con dence guided e ort estimator cogee .
we use the familiar sum of absolute error abs real e ort estimated e ort as one objective to guide our algorithm combining this with the less widely known and less widely used con dence interval.
the con dence interval is an estimated range of values that are likely according to the chosen interval range to include the estimated e ort.
we report the results of four sets of experiments on ve publicly available datasets to compare and evaluate our approach against candidate competitors baseline estimators state of the art estimators alternative single and multi objective formulations and currently claimed best industrial practice based on human judgment .
in our evaluation we follow recent best practice to assess prediction systems and evolutionary approaches .
our new bi objective e ort estimation algorithm outperforms baseline estimators a sanity check state of the art techniques case based reasoning linear regression regression trees and also three alternatives that we implemented in order to assess the degree to which multi objectivity plays a role in the performance of our algorithm.
these claims have been tested using a non parametric wilcoxon test for statistical signi cance which reveals that the results are signi cant p after applying the bonferroni correction for multiple statistical testing the most conservatively cautious of all corrections .
furthermore in all cases our bi objective algorithm outperforms these candidate competitors with a large e ect size as measured using the varghadelaney non parametric e ect size measure a12 .
we also compare both the estimation error produced by our algorithm and the current state of the art and the budget overruns that would accrue from using them against two claimed thresholds for industrial best estimation practice.
the results are very encouraging suggesting that cogee moves median expected state of the art performance within at least one and sometimes both thresholds.
ieee acm 38th ieee international conference on software engineering the rest of the paper is organised as follows.
section gives some background on software development e ort estimation.
section describes our proposal cogee for multi objective e ort estimation.
the research questions and experimental method we used to address them are described in section .
the results of the empirical study are reported in section .
the study validity is discussed in section .
section reports on related work while nal remarks are presented in section .
.
software effort estimation software development e ort estimation is the process of predicting the most realistic amount of e ort usually expressed in terms of person hours or person month required to develop or maintain a software project based on information collected in the early stage of a software project.
expert estimation remains the dominant strategy when estimating software development e ort in practice .
research results have focused on the construction of formal software e ort estimation models to support the engineers in the estimation process.
the rst estimation models were based on regression analysis.
since then di erent model building approaches have been investigated including approaches based on analogy based techniques e.g.
case based reasoning machine learning techniques e.g.
support vector regression bayesian network searchbased approaches e.g.
genetic programming tabu search and combinations of two or more of these models e.g.
.
formal estimation approaches usually exploit training data about past projects to build an estimation model which is then used to predict the e ort for a new project.
such a model takes as input a set of predictors e.g.
manager experience team experience and returns a scalar value that represents the e ort estimated to develop a new software system having the characteristics captured by the predictors.
this model can be described by the following equation estimatede ort c1op1f1 cnop2n 1fnop2nc wherefirepresents the value of the ithproject feature and ci its coe cient crepresents a constant while opirepresents theithmathematical operator of the model.
the way in which the predictors are used is speci c to the prediction approach.
for example when using a linear regression technique the predictors are combined through a linear combination while case based reasoning exploits the predictors to identify the most similar past projects.
several measures have been proposed to evaluate the accuracy of a prediction model.
generally they are based on the absolute error i.e jreale ort estimatede ortj .
the most popular are mmre and pred .
the former is the mean of magnitude of relative error mre where mre is de ned as mre jreale ort estimatede ortj reale ort mre is calculated for each project whose e ort has to be estimated and mmre is used to have a cumulative measure of the error.
the prediction at level l pred l measures the percentage of the estimates whose error is less than l and lis usually set at .
it is de ned as follows pred k n wherenis the total number of projects and kis the number of observations whose mre is less than or equal to .
.
these measures have been criticised as being biased towards underestimations and can behave very di erently when comparing prediction models thereby motivating the use of other more standardised measures such as the mean absolute error mae and the standardized accuracy sa recently recommended to compare the performance of prediction models.
mae is unbiased towards over or underestimation and is de ned as mae nnx i 1jrei eeij wherenis the number of projects used for evaluating the performance and reiandeeiare the actual and estimated e ort respectively for the project i. sa is based on mae and it can be de ned as follows sa mae pj mae rguess wheremae pjis the mae of the approach pjbeing evaluated andmae rguess is the mae of a large number e.g.
runs of random guesses.
thus sa represents how much better pjis than random guessing.
a value close to zero means that the prediction model pjis practically useless performing little better than a mere random guess .
e ort estimation techniques seek to minimise mae whereas they seek to maximise sa.
.
bi objective estimation our approach to software e ort estimation uses search based software engineering sbse an increasingly prevalent approach to software engineering in which software engineering problems are reformulated as search problems within the search space that can be explored using computational search algorithms.
in the sbse literature there has been more than years of work on the use of search based approaches for software e ort estimation e.g.
.
a recent survey of work on sbse for e ort prediction in particular and software project management in general can be found in the work of ferrucci et al.
.
however all previous search based e ort estimation approaches seek to produce point estimates.
furthermore only two previous studies concerned multi objective formulations of effort estimation.
both of these two previous multi objective studies focused on point estimates and aimed to analyse the tradeo among di erent accuracy measures for the single overall objective of producing the most accurate point estimate.
by contrast our multi objective approach treats the accuracy of the point estimate as one of the two objectives and the con dence interval as the other.
in the following we describe our proposed approach using the standard three key ingredients of any sbse approach representation tness function and computational search algorithm.
since our formulation is a bi objective formulation we also describe how we handle multi objective search.
representation feasible solutions to the problem de ned in section are estimation models described by equation .
such a model can be encoded as a genetic algorithm individual using an expression syntax tree and randomly choosing the coe cients ci2randopi2f g while the factors values depend on the training data and do not 620change during the evolution process.
it is worth noting that the equations feasible for the e ort estimation problem are those providing positive value for estimatede ort .
the initial population was generated by building random trees of xed depth.
fitness to evaluate the tness of each chromosome we employed a multi objective function to simultaneously minimise the estimates accuracy and the estimates distribution uncertainty.
many di erent indicators can be used to evaluate the accuracy of the estimates see section .
previous work showed that the use of di erent measures can impact both the tting and the predictive performance of the models built by ga relative measures e.g.
mmre memre often a ect negatively the overall model accuracy while absolute measures e.g.
sae seem to not have any detrimental e ect.
thus in this paper we choose to employ the sum of absolute error sae as tness function.
on the other hand fewer measures have been suggested in the literature to assess the uncertainty of the e ort estimates and no previous work has been carried out so far to investigate their e ectiveness as a tness function.
therefore in this paper we measure the estimate uncertainty by employing the con dence interval1associated with the estimation model to assess the uncertainty of the mean value of the distribution of absolute errors produced by the model as follows p df std absoluteerrors pn wherenis the size of the sample std absoluteerrors is the standard deviation of the distribution of absolute errors produced by the estimation model and p df is the quantile function .
this function returns a threshold value x below which random draws from the given cumulative distribution function c.d.f.
would fall ppercent of the time q p inffx2r p f x g for a probability p .
con dence intervals are usually calculated so that this percentage is while the degree of freedom df depends on the number of parameters we are estimating.
in regression models an n sized sample usually leads ton kdegrees of freedom where kis the number of parameters to be estimated.
handling multiple objectives in our case the two objectives are measured on orthogonal scales so we use pareto optimality which states a solution x1is said to dominate another solution x2 ifx1is no worse than x2in all objectives andx1is strictly better than x2in at least one objective.
using pareto optimality we can plot the set of solutions found to be non dominating and therefore equally viable .
computational search as a ranking method we employed a widely used multi objective evolutionary algorithm namely nsgaii .
we experimented with two widely used selection operators i.e.
roulette wheel selector and 1please note that an important di erence between con dence interval ci and prediction interval pi is that pi refers to the uncertainty of an estimate while ci usually refers to the uncertainty associated with the parameters of an estimation model or distribution.
the con dence level of a pi refers to the expected or subjective probability that the real value is within the predicted interval while the condence level of a model assesses the uncertainty of the mean value of a distribution of e ort values.tournament selector whereas the crossover and mutation operators are speci c for our solution encoding.
we used the roulette wheel selector to choose the individuals for reproduction while we employed the tournament selector to determine the individuals that are included in the next generation i.e.
survivals .
the former assigns a roulette slice to each chromosome according to its tness value.
in this way even if candidate solutions with a higher tness have more chances to be selected there is still a chance that they may remain unselected.
on the contrary using the tournament selector only the best nsolutions usuallyn2 are copied straight into the next generation.
crossover and mutation operators were de ned to preserve well formed equations in all o spring .
to this end we used a single point crossover which randomly selects the same point in each predictive model expression tree and swaps the subtrees corresponding to the selected node.
since both trees are cut at the same point the trees resulting after the swapping have the same depth as compared to those of parent trees.
we used a mutation operator that selects a node of the tree and randomly changes the associated value.
mutation can a ect internal nodes i.e.
operators or leaves i.e.
coe cients of the tree.
in particular when mutation involves internal nodes a new operator op0 i2ff gnopigis randomly generated and assigned to the node while if the mutation involves a leaf a new coe cientc0 i2ris assigned to the node.
it is worth noting that also the mutation operator we used preserves the syntactic structure of the equation.
crossover and mutation rate were xed to .
and .
respectively since in previous work recommended crossover rates ranged from .
to .
and mutation rates ranged from .
to .
.
the evolutionary process terminates after generations.
.
empirical study design this section presents the design of the empirical study we carried out to get an insight into the use of multi objective e ort estimation.
we rst present the research questions we aim to answer and then the data and techniques we experimented with and the evaluation criteria we used to assess the results.
.
research questions a novel e ort estimation approach must outperform baseline methods.
thus the rst research question we aim to answer represents a sanity check of our proposal rq1 sanity check is the proposed approach cogee suitable for e ort estimation?
to answer this question we compare our algorithm with three common baseline benchmarks used in the context of e ort estimation i.e.
mean and median e ort and random guessing and described in section .
.
if the investigated estimation method does not outperform the results achieved using these baselines it cannot be transferred to industry .
if a proposed estimation method outperforms the benchmark then the next natural question to ask is whether it outperforms state of the art techniques currently proposed in the e ort estimation literature.
if not then there would be no reason to adopt it since it could not be guaranteed to advance the state of the art.
this motivates rq2 rq2.
state of the art benchmark does cogee provide more accurate and robust estimates than currently used e ort estimation methods?
621to answer rq2 we compared our proposal cogee to three widely used and well studied state of the art e ort estimation methods namely lr cart and cbr as detailed in section .
.
we compare against three di erent approaches in order to improve the scienti c evidence that our proposed approach does indeed advance the state of the art.
for di erent datasets one or other of these techniques may produce the best e ort estimates but among the three of them they typically can be expected to perform better than other techniques in the literature .
since cbr is con gurable depending on the number of cases used and this can in uence performance we also report results for three di erent choices.
if we nd that our multi objective algorithm can outperform the state of the art then we have scienti c evidence to suggest that it should be adopted.
however this would not provide scienti c evidence that it is the multi objective nature of our approach that confers the improvements in estimation accuracy we observe.
therefore in rq3 we investigate whether there is evidence that the use of the two objectives we have chosen leads to any improvements we might have observed in answering rq2.
there is evidence in the literature on multi objective optimisation that multi objective formulations can outperform single objective formulations even when compared against the speci c single objective targeted by a single objective formulation.
when this happens it provides evidence that the multiple objectives are in a sense sympathetic to the targeted single objective they help guide the search towards desired single objective even better and focusing solely on the single objective itself.
this arises because search spaces are non monotonic and therefore disimproving moves may be required in order to arrive at overall results that lie closer to global optima.
hitherto this possibility has not been investigated for software project e ort estimation thereby motivating rq3 rq3.
bene ts from multi objective formulation does cogee provide more accurate and robust estimates than alternative single and multi objective approaches?
first we seek to establish whether the two objectives we consider together outperform each when considered individually.
therefore we compare two variants of the e ort estimation formulated as a single objective problem in which each optimise one of the two objectives used by our approach i.e.
ga sae where the goal is to minimize the sum of absolute error and ga ci where the goal is to minimize the con dence interval rq3.
.
does cogee provide more accurate and robust estimates than ga sae and ga ci?
when we optimise the sum of absolute error sae we are implicitly searching for a compromise between underestimates and overestimates because the sum of absolute error is the sum of underestimates and over estimates.
however these are clearly two contrasting goals that our formulation combines to give sae.
for completeness we therefore separate out these two components of sae to investigate whether they should be separately optimised using an implementation we call nsgaii uo or whether it is su cient to combine them as a single objective sae rq3.
.
does cogee provide better results than nsgaiiuo?
if our proposed approach cogee satis es all of the evaluation criteria covered by rqs and then we will havestrong scienti c evidence that it outperforms the state ofthe art and also other candidate alternative formulations.
however in order to have real world impact it will also be necessary to outperform current industrial practice.
there is little reliably consistent scienti c evidence concerning the actual estimate accuracy of current industrial practice.
therefore our cogee technique may have to outperform current beliefs about industrial practice as reported by practising software engineers in order to promote wider industrial uptake of our approach and of automated e ort estimation more generally .
this motivates our nal research question in which we evaluate the performance of our proposed estimation technique against the claims for the performance of current industry best practice in e ort estimation rq4.
comparison to industrial practices does our cogee provide more accurate and robust estimates than the ones claimed for current industrial best practice?
to answer rq4 we compare the performance of our cogee and other state of the art estimators against claims made for best human expert based results currently achievable in industry .
we investigate the magnitude of relative error compared to claimed industrial best practice and because industrialists tend to be more concerned with under estimated results rather than overestimated results we also evaluate the budget overrun that would accrue from using our technique compared to these claimed for industrial best practice and the state of the art.
.
datasets to carry out the empirical study we exploited ve publicly available datasets included in the promise repository namely china desharnais finnish maxwell and miyazaki.
these datasets represent an interesting sample of industrial software projects collected from a single company or several software companies.
the datasets cover a diversity of application domains and projects characteristics.
in particular they di er for observation number from to projects number and type of features from to features including a variety of features describing the software projects such as number of developers involved in the project and their experience technologies used size in terms of function points etc.
technical characteristics software projects developed in di erent programming languages and for di erent application domains ranging from telecommunications to commercial information systems involved companies the desharnais dataset is within company wc the others are cross company cc geographical locations software projects coming from china canada finland .
furthermore all these datasets have been widely used in previous research work to evaluate e ort estimation methods.
table summarises the descriptive statistics of the features of the datasets we considered while further details are provided in appendix a to allow readers to assess whether the results we gathered can scale up to their own contexts.
.
validation and evaluation to verify whether a method gives useful estimations of the actual development e ort a validation process is required.
to this end we performed a multiple fold cross validation partitioning the whole dataset into training sets for model building and test sets for model evaluation.
indeed when the accuracy of the model is computed using the same dataset employed to build the prediction model the accu622da taset type variable min max mean std.
dev.
ch ina cc input .
.
projects output .
.
inquiry .
.
file .
.
interface .
.
e ort des harnais wc teamexp .
.
projects managerexp .
.
entities .
.
transactions .
.
adjustedfps .
.
e ort .
.
f innish cc hw .
.
projects ar .
.
fp .
.
co .
.
e ort .
.
m iyazaki cc scrn .
.
projects form .
.
file .
.
e ort .
m axwell cc sizefp .
.
projects nlan .
.
t01 .
.
t02 .
.
t03 .
.
t04 .
.
t05 .
.
t06 .
.
t07 .
.
t08 .
.
t09 .
.
t10 .
.
t11 .
.
t12 .
.
t13 .
.
t14 .
.
t15 .
.
e ort .
table descriptive statistics of the dataset.
racy evaluation is considered optimistic .
to apply the multiple fold cross validation we partitioned a dataset in three test sets the observations were sampled uniformly at random without replacement and then for each test set we considered the remaining observations as training set.
this procedure was applied to each dataset thus obtaining for china a test set of observations and two of for desharnais a test set of observations and two of for finnish a test set of observations and two of for maxwell a test set of observations and two of for miyazaki three test sets of observations.
concerning the evaluation of the estimates obtained with the analysed estimation methods we used the mean absolute error and the standardized accuracy see section .
to establish if the estimations of one method were signi cantly better than the estimations provided by another method we tested the statistical signi cance of the absolute errors achieved with di erent estimation methods .
to check for statistical signi cance we used the wilcoxon signed rank test since the shapiro test showed that many of our samples came from non normally distributed populations making the t test unsuitable.
the wilcoxon test is a safe test to apply even for normally distributed data since it raises the bar for signi cance by making no assumptions about underlying data distributions.
in particular we tested the following null hypothesis the absolute errors provided by the prediction model piare signi cantly less that those provided by the prediction model pj.
and set the con dence limit at and applied the standard bonferroni correction k wherekis the number of hypotheses when multiple hypotheses were tested.as it has been previously noted in advice on statistical testing of randomised algorithms it is inadequate to merely show statistical signi cance alone we also need to know whether the e ect size is worthy of interest.
to assess whether the e ect size is worthy of interest we employed a non parametric e ect size measure namely the vargha and delaney s a12statistic since not all samples were normally distributed.
indeed as suggested in recent best practice it is better in cases such as ours to use a standardised measure rather than a pooled measure such as the cohen s de ect size.
given a performance measure m thea12statistic measures the probability that running algorithm ayields better m values than running another algorithm b based on the following formula a12 r m m n where r1is the rank sum of the rst data group we are comparing and mandn are the number of observations in the rst and second data sample respectively.
if the two algorithms are equivalent then a12 .
given the rst algorithm performing better than the second a12is considered small for a12 medium for .
a12 and large for a12 although these thresholds are somewhat arbitrary.
in this case we are always interested in anyimprovement in predictive performance so no transformation of the a12metric is needed .
to assess the performance of the multi objective optimisation algorithms we carried out a quantitative assessment by employing three solution set quality indicators namely contributions ic hypervolume ihv and generational distance igd .
to compute these indicators we normalised the tness values to avoid unwanted scaling e ects and we used the set of non dominated solutions found by the union of all the approaches compared as reference set rs .
theicquality indicator is the simplest measure.
it measures the proportion of solutions given by an algorithm a that lie on the reference front i.e.
rs .
the higher this proportion the more acontributes to the best solutions found by the approaches compared and so the better is the quality of its solutions.
icis a simple and intuitive measure but it is a ected by the number of solutions produced unfavourably penalising algorithms that might produce few but excellent solutions.
this is why we also consider two other measures of solution quality ihvandigd.
theihvquality indicator calculates the volume in the objective space covered by members of a non dominated set of solutions from an algorithm of interest.
the larger this volume the better the algorithm because the more it captures of the non dominated solution space.
zitzler demonstrates that this hypervolume measure is also strictly pareto compliant .
that is the hypervolume of ais higher thanbif the pareto set of adominates that of b. by using a volume rather than a count this measure is also less susceptible to bias when the numbers of points on the compared fronts are very di erent.
theigdquality indicator computes the average distance between the set of solutions s from the algorithm measured and the reference set rs.
the distance between sandrsin ann objective space is computed as the averagen dimensional euclidean distance between each point in sand its nearest neighbouring point in rs.
we can think ofigdas the distance between the front sand the reference frontrsin then dimensional problem objective space.
623due to the stochastic nature of evolutionary algorithms best practice requires the use of careful deployment of inferential statistical testing to assess the di erences in the performance of the algorithms used .
we therefore performed independent runs per algorithm per tness function measure per project to allow for such statistical testing correcting for multiple statistical tests.
.
benchmarks random guessing .
random guessing is a na ve benchmark suggested to assess the usefulness of a prediction system .
it randomly assigns the yvalue of another case to the target case.
more formally it is de ned as predict ayfor the target case tby randomly sampling with equal probability over all the remaining n cases and take y rwhereris drawn randomly from nr t .
any prediction system should outperform random guessing since an inability to predict better than random implies that the prediction system is not using any target case information.
mean median e ort .
these are two baseline benchmarks commonly used for e ort estimation techniques.
specifically the mean median of the past project e orts is used as predicted e ort for a new project.
linear regression.
we used the automatically transformed linear model atlm recently proposed as a suitable approach for comparison against novel software e ort estimation methods .
despite its simplicity atlm performs well over a range of di erent project types and requires no parameter tuning it is also deterministic meaning that results obtained are amenable to replication .
case based reasoning.
cbr is a branch of arti cial intelligence that has been successfully used in software engineering for prediction and reuse type applications .
given a new software project i.e.
target project characterised by its set of features the past projects relevant to solve it are retrieved from a case base of past projects.
these relevant cases are identi ed by using a similarity function that measures the distance between the target case and the other cases based on the values for the nfeatures of these projects.
the e ort values of the kmost similar projects i.e.
analogies are then used as nal prediction for the new project.
the choice of kis left to the user and has been a matter of some debate .
we used angel to obtain cbr predictions.
it is a tool introduced by shepperd and scho eld to estimate the development e ort of a software project.
it supports the euclidean distance measure between vectors and we used this metric to compute project similarity while the nal estimation was computed as the mean e ort of the knearest analogies.
we report results of each of the choices of k betweenk andk analogies.
classi cation and regression trees .
cart are machine learning methods to build prediction models by recursively partitioning the data and tting a simple prediction model within each partition .
the partitioning can be represented graphically with a decision tree.
decision trees where the dependent variable takes a nite set of values are called classi cation trees while decision trees where the dependent variable takes continuous values are called regression trees.
in our work regression trees were generated using the r package rpart2.
genetic algorithms .
we considered two variants of the e ort estimation formulated as a single objective prob2 ina sa desharnais sa finnish sa maxwell sa miyazaki sa co gee .
cogee .
cart .
cogee .
cogee .
ga sae .
lr .
cogee .
ga sae .
lr .
ga ci .
ga sae .
ga ci .
cart .
ga ci .
cart .
cart .
lr .
cbr3 .
ga sae .
cbr3 .
cbr3 .
cbr3 .
ga ci .
nsgaii uo .
median .
median .
ga sae .
cbr2 .
cbr2 .
cbr2 .
cbr2 .
cbr2 .
nsgaii uo .
cbr3 .
cbr1 .
cbr1 .
cbr1 .
lr .
cbr1 .
mean .
mean .
nsgaii uo .
median .
median .
lr .
ga sae .
mean .
mean .
cart .
nsgaii uo .
nsgaii uo .
median .
cbr1 .
mean .
table rqs1 standard accuracy sa values achieved by our approach cogee the baseline mean and median e ort and state of the art cbr1 lr and cart techniques over the ve datasets.
for completeness sa results are also included for the other three alternative evolutionary algorithms considered later in answer to rq3 ga ci ga sae and nsgaii uo.
lem and one formulated as a multi objective problem.
these variants di er only in the objective function ga sae where the goal is to minimize the sum of absolute errors ga ic where the goal is to minimize the con dence interval associated to the mean of the absolute errors nsgaii uo where the goal is to simultaneously minimise over and under estimates.
the single objective gas were implemented by using the r package ga3.
the multiobjective algorithms nsgaii uo and cogee were implemented using the r package nsga2r4.
all these algorithms use the same solution encoding and setting see section .
.
results this section presents our results in answer to rqs1 .
.
rq1.
sanity check the analysis of sa see table suggests that the estimations obtained with cogee are better than those achieved by using mean median and random estimates.
table shows the results of the wilcoxon test together with the corresponding a12e ect size to compare the statistical signi cance and e ect size of the improvements over the baseline due to cogee.
the rst row of the table for each dataset presents the results that compare our proposed approach cogee with the accuracy provided by mean median and random baseline estimates.
for completeness table also compares the performance of the other evolutionary approaches we investigate subsequently in rq3.
table reveals that the improvements over the baseline we observed in table for our proposed approach are signi cant p and with large e ect size a12 .
the results remain signi cant after correcting for multiple statistical testing.
this inferential statistical analysis con rms that our approach signi cantly outperforms the baseline thereby passing the sanity check set by rq1.
.
rq2.
comparison to state of the art the analysis of the sa measure see table reveals that our proposed algorithm cogee not only outperforms the baseline but also the di erent state of the art techniques against which we compare it.
indeed the sa values provided by cogee are always higher than those provided by cbr and lr on all the datasets we considered and higher than those provided by cart on out datasets.
these 624da taset technique mean median random ch ina cogee .
.
.
.
.
.
nsgaii uo .
.
.
.
.
.
ga ci .
.
.
.
.
.
ga sae .
.
.
.
.
.
desharnais cogee .
.
.
.
.
.
nsgaii uo .
.
.
.
.
.
ga ci .
.
.
.
.
.
ga sae .
.
.
.
.
.
finnish cogee .
.
.
.
.
.
nsgaii uo .
.
.
.
.
.
ga ci .
.
.
.
.
.
ga sae .
.
.
.
.
.
maxwell cogee .
.
.
.
.
.
nsgaii uo .
.
.
.
.
.
ga ci .
.
.
.
.
.
ga sae .
.
.
.
.
.
miazaky cogee .
.
.
.
.
.
nsgaii uo .
.
.
.
.
.
ga ci .
.
.
.
.
.
ga sae .
.
.
.
.
.
table rq1.
results of the wilcoxon test a12effect size in brackets performed on the mean of absolute errors provided by our algorithm cogee compared to baseline comparators mean median and random estimates.
for completeness sanity check comparators are also included for the other three alternative algorithms considered later in answer to rq3 ga ci ga sae and nsgaii uo.
co gee vs. cbr1 cbr2 cbr3 lr cart ch ina .
.
.
.
.
.
.
.
.
.
desharnais .
.
.
.
.
.
.
.
.
.
finnish .
.
.
.
.
.
.
.
.
.
maxwell .
.
.
.
.
.
.
.
.
.
miyazaki .
.
.
.
.
.
.
.
.
.
table rq2.
results of the wilcoxon test with a12e ect sizes in brackets which compare the mean and median of the absolute errors for our algorithm cogee to those for the state of the art techniques cbr1 lr and cart.
observations are con rmed by the inferential statistical analysis the results of which are presented in table the improvement of our algorithm over the three state of the art techniques is signi cant p and the e ect size is large a12 in out cases.
the results remain signi cant after correcting for multiple statistical testing.
we also veri ed that cogee remains the best algorithm when we use the median of absolute error usually less sensitive to extreme values than the mean as evaluation criterion5.
.
rq3.
does multi objectivity help?
table shows the quality indicators of the pareto fronts obtained from the runs by using the proposed bi objective algorithm i.e.
cogee and the single objective algorithms ga ci and ga sae.
we can observe that for the three data sets china finnish and miyazaki our proposed approach produces comfortably better results according to any quality indicator such as hypervolume or contribution to or distance from the reference pareto front typically used in such experiments .
for the remaining two datasets desharnais and maxwell the two single objective competitors occasionally provide solutions on or close to the pareto front produced by our algorithm.
the results of the wilcoxon test see table executed to 5all our results including these are available at http www0.cs.ucl.ac.uk sta f.sarro projects cogee da taset technique igdihvic ch ina cogee .
.
.
ga sae .
.
.
ga ci .
.
.
desharnais cogee .
.
.
ga sae .
.
.
ga ci .
.
.
finnish cogee .
.
.
ga sae .
.
.
ga ci .
.
.
maxwell cogee .
.
.
ga sae .
.
.
ga ci .
.
.
miyazaki cogee .
.
.
ga sae .
.
.
ga ci .
.
.
ch ina cogee .
.
.
nsgaii uo .
.
.
desharnais cogee .
.
.
nsgaii uo .
.
.
finnish cogee .
.
.
nsgaii uo .
.
.
maxwell cogee .
.
.
nsgaii uo .
.
.
miyazaki cogee .
.
.
nsgaii uo .
.
.
table rq3.
mean of the quality indicators igd ihv ic computed on the pareto fronts of the considered evolutionary approaches over runs.
compare the quality indicators of the pareto fronts obtained by cogee are signi cantly better than the ones provided by ga ci and ga sae in out of the comparisons and never worse in cases often with medium or large e ect size.
these observations remain after bonferroni correction for multiple statistical tests.
thus we conclude that our proposed algorithm cogee outperforms the single objective competitors ga sae and ga ci rq3.
.
to answer rq3.
we compared the pareto fronts obtained by the proposed algorithm cogee and a multi objective algorithm that minimises under and over estimates nsgaiiuo .
from the pareto fronts we observe that cogee produces better results than nsagaii uo .
in fact the competitor nsgaii uo never produces a solution that dominates any solution on the pareto front produced by cogee.
the wilcoxon test see table executed to compare the quality indicators of the pareto fronts obtained by cogee and nsgaii uo con rms that cogee signi cantly outperforms nsgaii uo in out cases and never worse in the other often with medium or large e ect size.
these observations remain after bonferroni correction.
as a nal check we con rmed that cogee remains the best performing algorithm when we use under and over estimation as the evaluation criterion though space does not permit us to include the corresponding pareto fronts5 .
.
rq4.
comparison to industrial practices in figure a we show box plots for the magnitude of relative error mre obtained in the prediction for each project in a given dataset by using our proposed multiobjective approach cogee and the three state of the art techniques i.e.
cbr lr and cart .
on the gure we plot two dotted lines that denote desirable thresholds within which we would like to see these errors lie.
these two thresholds set at .
and .
denote predictions of project e ort which lie within and of the true value.
the reason for choosing these two thresholds derives from evidence that industrial practice based on human judgment hopes claims 625da taset technique igd ihv ic ch ina cogee vs. ga sae .
.
.
.
.
.
cogee vs. ga ci .
.
.
.
.
.
cogee vs. nsgaii uo .
.
.
.
.
.
desharnais cogee vs. ga sae .
.
.
.
.
.
cogee vs. ga ci .
.
.
.
.
.
cogee vs. nsgaii uo .
.
.
.
.
.
finnish cogee vs. ga sae .
.
.
.
.
.
cogee vs. ga ci .
.
.
.
.
.
cogee vs. nsgaii uo .
.
.
.
.
.
maxwell cogee vs. ga sae .
.
.
.
.
.
cogee vs. ga ci .
.
.
.
.
.
cogee vs. nsgaii uo .
.
.
.
.
.
miazaky cogee vs. ga sae .
.
.
.
.
.
cogee vs. ga ci .
.
.
.
.
.
cogee vs. nsgaii uo .
.
.
.
.
.
table rq3.
results of the wilcoxon test with a12e ect sizes in brackets which compare the quality indicators igd ihv ic of our algorithm cogee to the ones obtained by the other evolutionary approaches over runs.
to produce predictions within these tolerances.
the evidence for these thresholds comes from a survey of current industry practices by molkken and j rgensen .
as can be seen from figure a the magnitude of relative error lies comfortably within both thresholds.
indeed as the box plots show in all but one case the entire distribution of estimation errors for our proposed approach cogee lies within both thresholds.
the same cannot be said for case based reasoning one of the state of the art techniques.
although these box plots show the median of magnitude relative error mdmre this value should not be used to compare techniques against one another as like mmre it can be misleading .
we present the box plots simply to depict the distribution of relative errors for each technique and their relationship to these two industry thresholds.
there is also evidence from industry that managers are far more concerned about underestimated project e ort and thereby underestimated project duration than they are about overestimates.
while an overestimate may give rise to missed opportunities an underestimate and consequent project overrun can have far more pernicious consequences.
a project manager may therefore be interested to see the distribution of the magnitude of underestimated predictions for each technique.
the industrial thresholds we used of .
and .
are in fact derived from the current industrial claims concerning project overrun.
as such they better indicate the threshold within which the set of all underestimates must lie in order to be competitive and actionable for industrial uptake than they indicate a threshold for the magnitude of relative error.
perhaps the risk aversion and reticence to risk overrun might be a contributory factor to the current lack of takeup of e ort estimation within the industry.
therefore in order to address the managers natural disinclination for underestimates and consequent budget overrun we report box plots for the distribution of overrun project budgets that would be expected from each e ort prediction approach in figure b .
as can be seen from the results the median expected budget overrun for our approach lies within the claimed best results obtained from industrial practice for all data sets except the china where it is very close to the upper bound .
in two of the ve datasets the entire distribution of overrun values expected from our estimation algorithm lie within the upper bound while in the other two the vast majority ofthe distribution of overruns lies within this bound.
by contrast the current state of the art techniques yield median expected overrun values that lie outside the currentlyclaimed industrial upper abound.
we therefore nd evidence to support the claim that our proposed estimation algorithm cogee moves the stateof the art that can be expected from automated estimators within the bounds of current claims for industrial best practice.
this may prove to be an important nding because it provides evidence that our new multi objective approach can advance the claimed state of best practice as well as the known scienti c state of the art.
.
threats to validity several factors can bias the validity of empirical studies.
in this section we discuss the validity of our empirical study based on three types of threats namely construct conclusion and external validity.
to satisfy construct validity a study has to establish correct operational measures for the concepts being studied .
this means that the study should represent to what extent the predictor and response variables precisely measure the concepts they claim to measure .
thus the choice of the features and how to collect them represents a crucial aspect.
we tried to mitigate such a threat by using real world data coming from ve publicly available datasets widely used to empirically evaluate e ort estimation methods.
with regards to the conclusion validity we carefully applied the statistical tests verifying all the required assumptions and correcting for multiple statistical testing.
we also used datasets of di erent sizes to mitigate the threats related to the number of observations in each dataset.
to reduce conclusion instability we followed recent best practice to assess prediction systems and evolutionary approaches .
to mitigate threats to external validity we used datasets containing projects related to di erent contexts that might be characterised by some speci c project and human factors such as development process developer experience tools and technologies used time and budget constraints .
despite we used a set of subjects that has such a degree of diversity we cannot claim that our results generalise beyond the subjects studied.
moreover the industrial prediction thresholds used in our study come from a survey of industry practices carried out in thus they may not generalise to other periods.
.
related work a comprehensive review of work exploiting evolutionary approaches for e ort estimation can be found elsewhere .
in this section we summarise the main work investigating robust e ort estimates with con dence intervals by highlighting the di erence with the approach we proposed herein.
these studies can be classi ed into two broad categories i those that produce con dence intervals for point estimates during the estimation process and ii those that produce probabilities of prede ned intervals before the estimation process.
our approach cogee falls in the rst category since it builds estimation models that optimise both the accuracy of the point estimates and the con dence intervals during its evolutionary process.
angelis and stamelos report the rst empirical investigation of estimation models based on prediction intervals cogeecbr1 cbr2 cbr3lr cartcogeecbr1 cbr2 cbr3lr cartcogeecbr1 cbr2 cbr3lr cartcogeecbr1 cbr2 cbr3lr cartcogeecbr1 cbr2 cbr3lr cart china desharnais finnish maxwell miy azakimre a magnitude of relative error cogeecbr1 cbr2 cbr3lr cartcogeecbr1 cbr2 cbr3lr cartcogeecbr1 cbr2 cbr3lr cartcogeecbr1 cbr2 cbr3lr cartcogeecbr1 cbr2 cbr3lr cart china desharnais finnish maxwell miy azakioverrun b overrun figure rq4.
comparison with claimed optimal industrial practice when the magnitude of relative error a and overrun b of each project in a given dataset are considered.
these results provide evidence that our multi objective approach can move the current state of the art for automated e ort estimation within current claims for human expert based industrial best practice.
in the context of software development e ort estimation.
they compared the e ort prediction intervals derived from a bootstrap based model with the ones obtained by using regression based e ort estimation models.
however this study displays a confusion of terms and a critique was consequently made by j rgensen to clarify the ambiguity.
the same authors also investigated statistical simulation techniques for calculating con dence intervals for project portfolios .
in a subsequent study j rgensen described the uncertainty of the estimate through an e ort prediction interval pi and introduced an approach that is based on the assumption that the estimation accuracy of earlier software projects predicts the e ort pi of new projects.
the approach has been evaluated with two empirical studies to provide insight into when to use the proposed approach regression based approaches or software professionals judgment.
braga et al.
introduced a method based on machine learning which gives the estimation of the e ort together with a con dence interval for it.
they used robust con dence intervals which do not depend on the form of probability distribution of the errors in the training set.
they evaluated the proposed approach on two datasets.
the results showed that the proposed method was able to build robust con dence intervals.
the rst study that falls in the second category used multinomial logistic regression for modeling productivity intervals .
in this study sentas et al.
also investigated prede ned intervals of productivity in a bayesian belief network to support expert opinion.
subsequently the same authors investigated ordinal regression to model the probabil ities of both e ort and productivity intervals.
bibi et al.
also provided an empirical comparison between models producing point estimates and models producing prede ned interval estimates.
bakir et al.
proposed a new approach that converts e ort estimation into a classi cation problem to classify new software projects in one of the e ort classes each of which corresponds to an e ort interval.
differently from the previous studies the e ort intervals are not prede ned manually but determined by clustering analysis.
moreover bakir et al.
used classi cation algorithms instead of regression based methods.
the approach evaluated on public datasets provided point estimations comparable to those in literature but estimation hit around which is higher than those obtained in previous studies.
.
conclusion this paper has introduced and evaluated a bi objective software project e ort estimation algorithm.
the primary novelty of the algorithm lies in its incorporation of con dence intervals to guide a multi objective evolutionary algorithm.
our results indicate that the new algorithm outperforms the state of the art moving it to within current claimed thresholds for industrial human expert based best practice in e ort estimation.
our results also provide evidence that it is the multi objective nature of our approach that conveys this signi cantly improved performance.
as well as the inherently attractive performance improvements we believe developers and their managers may also nd the provision of con dence intervals on e ort estimations useful since they bound the uncertainty of the estimation.
627acknowledgement the research is funded by the dynamic adaptive automated software engineering programme grant ep j017515 and supported by two microsoft azure research grants sarro petrozziello .
appendix a datasets in this appendix we provide details on the datasets used in our study descriptive statistics are shown in table .
the china dataset includes data of projects.
we employed as independent variables the elements used to calculate function points i.e.
input output inquiry file interface and effort as dependent variable.
desharnais is an industrial dataset comprising software projects derived from a canadian software company.
we considered the total e ort as dependent variable but not the length of the code.
we also excluded from our analysis the categorical variables i.e.
language and yearend and four projects that have missing values as done in previous works e.g.
.
therefore we used the following independent variables teamexp i.e.
the team experience measured in years managerexp i.e the manager experience measured in years entities i.e.
the number of the entities in the system data model transactions i.e.
the number of basic logical transactions in the system adjustedfps i.e.
the adjusted function points .
the finnish dataset contains data from projects developed by di erent finnish companies.
each project is described by a dependent variable the e ort expressed in person hours and ve independent variables.
among them we decided to not consider the prod variable since it represents the productivity expressed in terms of e ort and size.
the independent variables we employed are hw i.e.
the type of harware fp i.e.
function points arand co. the maxwell dataset contains projects developed for one of the biggest commercial banks in finland.
we employed features function points sizefp and ordinal variables i.e.
number of di erent development languages used nlan customer participation t01 development environment adequacy t02 sta availability t03 standards used t04 methods used t05 tools used t06 software s logical complexity t07 requirements volatility t08 quality requirements t09 e ciency requirements t10 installation requirements t11 sta analysis skills t12 sta application knowledge t13 sta tool skills t14 and sta team skills t15 .
as with the desharnais dataset we did not use categorical variables.
the miyazaki dataset is composed by projects developed by di erent software companies of the fujitsu large systems users group.
for this dataset we considered the following independent variables scrn i.e.
the number of di erent input or output screens form i.e.
the number of di erent report forms and file i.e.
the number of di erent record format .
the dependent variable is effort dened as the number of person hours needed from system design to system test including indirect e ort such as project management.
.