f ast approaches to scalable similarity based t est case prioritization breno miranda federal university of pernambuco recife brazil bafm cin.ufpe.bremilio cruciani gran sasso science institute l aquila italy emilio.cruciani gssi.itroberto verdecchia gran sasso science institute l aquila italy roberto.verdecchia gssi.itantonia bertolino isti cnr pisa italy antonia.bertolino isti.cnr.it abstract many test case prioritization criteria have been proposed for speeding up fault detection.
among them similarity based approaches give priority to the test cases that arethe most dissimilar from those already selected.
however the proposed criteria do not scale up to handle the manythousands or even some millions test suite sizes of modernindustrial systems and simple heuristics are used instead.
we introduce the f ast family of test case prioritization techniques that radically changes this landscape by borrowingalgorithms commonly exploited in the big data domain to find similar items.
f ast techniques provide scalable similaritybased test case prioritization in both white box and black box fashion.
the results from experimentation on real world c and java subjects show that the fastest members of the family outperform other black box approaches in efficiency with no significant impact on effectiveness and also outperform whitebox approaches including greedy ones if preparation time is not counted.
a simulation study of scalability shows that one f ast technique can prioritize a million test cases in less than minutes.
ccs concepts software and its engineering software testing and debugging general and reference verification metrics information systems similarity measures acm reference format breno miranda emilio cruciani roberto verdecchia and antonia bertolino.
.
fast approaches to scalable similarity based test case prioritization.
in proceedings of icse 40th international conference on software engineering gothenburg sweden may june icse pages.
.
.
also with vrije universiteit amsterdam 1081hv the netherlands.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistributeto lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse gothenburg sweden copyright held by the owner author s .
publication rights licensed to acm.
.
.
.
.
.
.
introduction test case prioritization tcp is a very active topic in software testing research .
the goal of tcp is to speed up fault detection it re orders a test suite so that those test cases that are most likely to fail are executed first.
tcp is typically applied in regression testing as changes are introduced into a software system previously saved test cases need to be re executed to ensure quality and stability.
by herzig answering the question what to test next is one of the five top things that require automation in industrial software testing.
tcp can help to detect faults more quickly and to provide confidence that should testing be stopped before all test cases are run the ones executed are the most effective.
in the years researchers have proposed many prioritization criteria that exploit different information related to the test cases early approaches were based on code coverage more recently black box criteria based on system models on requirements or on historical failure data are investigated as with the growing scale of regression testing coverage measures can hardly be afforded.
since the effectiveness of tcp techniques is evaluated by their average percentage of faults detected apfd .
this measure captures how fast a re ordered test suite detects faults which is certainly an important concern in test prioritization.
however apfd does not consider how fastthe prioritization approach itself is.
as said by henard et al.
if prioritization takes too long then it eats into the time available to run the prioritized test suite .
as also noticed in for real world software the size of a test suite can often exceed the size of the system under test.
in contrast the time available for test execution cycles decreases especially with companies migrating towards rapid release or continuous integration practices.
memon et al.
report that every day at google an amount of 800k builds and 150m test runs are performed on more than 13k code projects.
in line with we notice that most tcp approaches in the literature cannot handle such scale.
our experimental results show that some tcp approaches become soon inefficient even for small medium size benchmarks.
in this paper we look at tcp from a novel perspective we acknowledge that test suite sizes grow at fast pace and existing techniques and tools for regression testing become inadequate.
making the appropriate scale distinctions the management of test suites in large scale industrial projects is becoming a big data problem.
big data are datasets whose size is beyond the ability of typical database software tools acm ieee 40th international conference on software engineering authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden b. miranda e. cruciani r. verdecchia and a. bertolino to capture store manage and analyze .
in a similar way we coin the term big testsets to denote sets of test cases whose dimensions go beyond the capacity of existing testing tools.
a testset dimension can be big due to either the number of test cases e.g.
millions of tests or the size of test case related information e.g.
coverage measures for huge programs or the combination of both.
we borrow well established data mining techniques for handling big testsets.
indeed finding similar items is a fundamental problem in data mining and very efficient algorithms exist to address this problem.
in software testing diversity has been shown to be an effective measure for both selecting and prioritizing test cases and several similarity based approaches have been proposed .
however such approaches do not scale up to big testsets.
in this paper we present f ast a family of similarity based tcp techniques that employs minhashing andlocality sensitive hashing algorithms for quickly finding diverse test cases within a big set.
f ast can perform white box wb or black box bb prioritization and can be tuned to yield higher or lower efficiency by trading off with precision in similarity estimation.
we assessed the effectiveness efficiency and scalability of five techniques from the f ast family applied to both wb function statement and branch coverage and bb testsets i.e.
techniques in total against several competing approaches techniques in total .
the results on commonly used benchmarks both c and java showed that the fastest techniques from the f ast family outperformed the bb competitors in efficiency with no significant impact on effectiveness.
in comparison with wb approaches they required the shortest prioritization time net of preparation time even against greedy approaches.
indeed our simulation study shows that after preparation is done fa s t efficiency is independent from the size of the test cases and one f ast technique can prioritize 1m size testsets in less than minutes whereas even greedy total performance degrades when dealing with big dimensions.
summarizing the contributions of this work include the first proposal of exploiting data mining algorithms for similarity based testing of big testsets.
the definition and implementation of the f ast family of similarity based tcp techniques.
a large scale experimentation of f ast techniques compared for effectiveness efficiency and scalability against competing approaches.
the release of an automated framework and all the data used for the experiments in order to support replicability and follow up studies.
the paper is structured as follows.
related work is overviewed in the next section.
background information behind similarity based tcp and the employed algorithms is provided in section .
the f ast approaches the performed experiments and the results achieved are described in sections and respectively.
section concludes the paper.
1fa s t is a recursive acronym for fa s t approaches to similarity based testing.
related work this work proposes a family of novel tcp approaches based on similarity that explicitly addresses scalability.
related work on test case prioritization.
the literature on tcp is huge testifying the great interest in the topic from both academic and industrial perspectives.
for an overview of existing work we refer to .
catal and mishra present a systematic mapping study of tcp over the pe riod .
the work from yoo and harman i sa broad survey on regression testing.
concerning tcp they categorize techniques based on the information used for ordering test cases including coverage based interaction testing which considers different combinations of components and others including distribution based human based probabilistic requirement based model based .
hao et al.
provide an overview of tcp research considering five aspects test adequacy criteria algorithms used measures adopted constraints considered and application scenarios.
notably they observe that efficiency is important and tcp becomes unbearable when the prioritization time gets close to test execution time.
this work fully shares such consideration.
related work on similarity based tcp.
both jiang et al.
and zhou et al.
have proposed art based prioritization techniques guided by code coverage.
art adaptive random testing is a variant of random test generation that tries to spread as evenly as possible the test inputs in the input domain.
in our experimentation we compare the f ast approaches against both which are further described in section .
also the approach proposed by fang et al.
is based on code coverage information from which they exploit the execution frequency profiles.
among black box approaches ledru et al.
propose a similarity based approach solely considering the strings that express the test cases i.e.
the input data or the junit test cases.
we also compare f ast against this approach see section .
noor and hemmati develop a history based approach in which among new or modified test cases those that are the most similar to failing ones are prioritized.
fa s t does not currently use history data.
related work on scaling up test prioritization.
it is important to consider the applicability of proposed tcp approaches to real world testing environments.
busjaeger and xie identify heterogeneity scale and cost as the practical realitiesto address in tcp and propose to rank the test cases by using machine learning techniques trained on five features codecoverage text path and content similarity failure history and test age .
the rocket approach by marijan et al.
implements an automated tcp approach considering failure history and test execution time and compares it against manual approach.
elbaum et al.
propose a regression test strategy for continuous integration environments based on execution history data that combines techniques of test case selection and prioritization.
we observe that approaches conceived for handling huge test suites generally embed specific heuristics conceived on the basis of the studied industrial authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fast approaches to scalable similarity based test case prioritization icse may june gothenburg sweden process.
in contrast fa s t is a generally applicable approach as it does not embed any ad hoc heuristic.
background .
similarity based tcp the tcp problem can be defined as follows given a test suite s the set pof permutations of s and an award function f p r then tcp consists in finding a t p such that f t f t prime for all t prime pwith t prime negationslash t. ideally the award function frefers to the rate at which faults are detected in the given ordering.
in reality tcp approaches can only be based on surrogate criteria and as discussed in the previous section fairly different techniques have been suggested.
in particular the idea at the basis of similarity based approaches for test prioritization stp in short is to reward the diversity between test cases.
in figure we provide the scheme of a generic stp process.
it consists of three main activities i encoding of test related information ii evaluating similarity iii picking out the next test case s .
the similarity between two test cases can be evaluated in many different ways also depending on the adopted test strategy.
for example in coverage based testing similarity between test cases is evaluated by considering set similarity measures among their respective coverage as in in model based testing instead by considering the overlap between the traces covered by the tests over a state based model of the system under test as in .
several other features related to test cases have been taken into account e.g.
historical data failure the test input string etc.
therefore before applying any stp a preparation phase is needed step in which the information related to the test cases is collected and encoded other processing of such data may also be needed depending on the approach .
such data is then processed to calculate the similarity with respect to the already picked test cases step .
in fact stp proceeds in iterations at each iteration there exists a set of already picked test cases denoted as so far ordered tests to which the test cases yet to be ordered are compared.
followingstep we obtain a ranking of the coded test information.
from this one or more test cases are picked and added to the set of so far ordered tests step .
the process terminates when the whole testset is ordered compatibly with available resources .
.
algorithms for similarity estimation finding similar items is a fundamental problem in data mining and very efficient techniques have been developed to solve it .
typical tasks that face such problem include finding plagiarized documents detecting mirror pages identifying articles coming from the same source.
as the number of test cases to prioritize grows in size up to millions the idea of applying such efficient techniques in stp seems the natural way to go.
the naive approach for similarity computation between nitems needs to perform all the pairwise comparisons and becomes inefficient as ngrows.
in this work we measure thecoded test informationsimilarity ranked test information so far ordered testsnext test s encoding test informationevaluating similarity picking out next test s add to ordered testspick initial test s yes no finished?test related information1 figure overview of stp.
similarity of two sets aand bas their jaccard similarity js a b a b a b and the distance between them as their jaccard distance jd a b js a b .
this choice allows us to leverage and combine techniques able to drastically reduce the cost of computing similarity.
the first technique called shingling is used to represent items as sets of shingles out of which the similarityis computed.
given a string of characters a k shingle of that string is the set of its substrings of length k. for example the shingles set of the string gzip is gzi zip .a n y document can be represented by a set of k shingles and if two documents are similar they will have many shingles in common.
we use k shingles in bb prioritization to make a set out of the string representation of the input test cases.
sets of shingles though are larger than the original data and for huge datasets it is not practical to use them directly.
the second technique we leverage is minhashing which derives compact representations of sets called signatures .
minhash signatures have the nice property of preservingthe jaccard similarity between the sets they represent.
aminhash of a set sis computed as follows a given hash function gis used to hash all the elements in sand the minimum resulting value becomes the minhash of s. this process is repeated multiple times e.g.
using hdifferent hash functions to generate a sequence of minhashes which forms the signature of s.2the estimation of the jaccard similarity between two sets can be computed by counting the fraction of minhashes that agree in the signatures of the sets.
for example the signatures and have an estimated jaccard similarity of since they agree in the first and fourth minhash.
minhashing helps to compress large items into small signatures but the signature pairs to be compared can still be beyond feasibility if the number of items is high.
the third technique we use is locality sensitive hashing lsh which reduces the scope of comparison to only a subset of items that are likely to be similar the candidate set .
lsh works on the signature matrix i.e.
the matrix that has minhash signatures as columns by dividing it into bbands of rrows each and applying a hash function to them.
in particular for each band the vectors of rintegers located in the columns are hashed into several buckets if it happens 2a signature of length hguarantees an expected error of o h i n the similarity estimation.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden b. miranda e. cruciani r. verdecchia and a. bertolino that two of these vectors hash into the same bucket then it means that a portion of their signatures agree and the pair is added to the candidate set.
in the end the candidate set will contain all the pairs of sets which are likely to be similar i.e.
all the pairs that have a jaccard similarity over a certain threshold s. such threshold depends on the choice of the values bandrand a good approximation is s b r .
approach algorithm provides a pseudocode description of f ast .
in the preparation phase lines and f ast uses the coded test information tto create the minhash signatures m. note that this is the only operation where f ast handles bb and wb inputs differently.
while for wb the code coverage information can be directly represented as sets regardless of the coverage criterion for bb the string representation of the test cases needs to be preprocessed into k shingles.
we used k to have a suitable set representation.3once the minhash signatures are computed tis not required anymore and only mis used during the prioritization process.
we used a number of hash functions h which guarantees an expected error not greater than .
in the estimation of the jaccard similarity and distance between two signatures.
even if the error in the estimation is high the choice of the next test case is performed over a subset of tests that are all dissimilar from the so far prioritized ones.
algorithm f ast prioritization.
input coded test suite info t optional selection function f. output prioritized test suite p. 1p emptylist 2i gettestcaseids t 3m mhsignatures t no need of tfrom here on 4b lshbuckets m m v cumulative signature of so far ordered test cases 5m v mhsignature 6while p negationslash i do cs lshcandidates b m v ifcs then m v mhsignature cs lshcandidates b m v cd i p cs complement of cs s select m v m cd f m v updatemhsignature m v m s m remove m s p append p s return p function select m v m c f ifno fthen f ast pw return arg max c c braceleftbig estimatejd m v m c bracerightbig else f ast f return randomsample c f in line the collection of lsh buckets is computed.
basically bcontains bbuckets one for each band and 3the typical bb input is smaller than 505characters which is the number of all the possible shingles considering an average of characters letters and symbols .each bucket keeps track of all the test cases colliding there.
we defined the number of bands b and rows r such that the number of rows in the signature matrix is equal to the signature size i.e.
h r b. these values guarantee a similarity threshold s .
for the candidate set.
notice that while in finding the most similar items a higher similarity threshold would be better for the context of stp we want to select the test cases that are dissimilar from the so far prioritized ones.
intuitively with a similarity threshold s .
the candidate set will contain almost all the test cases but the ones that are dissimilar to m v i.e.
having jaccard distance greater than .
.
in fact the actual candidate set cdused by f ast is computed line as the complement of cs excluding the so far prioritized test cases.
the candidate sets are created inside the while loop lines to where the actual prioritization happens as follows m v is divided into bbands each band is hashed and if there is a collision with the corresponding bucket in b then the test cases of that bucket are added to the candidate set cs.
m v is initialized in line and is updated whenever new test cases are selected line to keep track of the cumulative signature of the so far ordered test cases.
whenever csis empty we reset m v lines to and recompute cs.f o r f ast such operation is analogous to what is done by some tcp approaches that reset the coverage vector when of the achievable coverage is accomplished.
the function select line is where the f ast approaches differentiate from each other.
f ast pw computes the estimated jaccard distance between m v and each test case in the candidate set cdusing minhash signatures and selects the candidate that is the farthest away from m v .
the other approaches instead use a function fthat is provided as input to the algorithm to select a random subset of cdof size f cd .
in line the newly selected test case s are appended to the prioritized test suite p. for the experiments in this work we considered the following functions thatprogressively increase the efficiency of the prioritization one log sqrt all.
in general fis a generic function that can be tuned to achieve the right balance between the efficiency and the accuracy required in a particular context.
experiments we describe the experiments conducted to assess the effectiveness efficiency and scalability of f ast in comparison with existing prioritization techniques.
.
research questions the ultimate goal of any tcp approach is to reveal faults as quickly as possible.
therefore our first and second research questions investigate the effectiveness and efficiency of f ast rq1 how does f ast compare with other existing prioritization approaches in terms of fault detection rate ?
rq2 how does f ast compare with other existing prioritization approaches in terms of time required for the prioritization?
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fast approaches to scalable similarity based test case prioritization icse may june gothenburg sweden effectiveness and efficiency of tcp have been extensively explored by researchers in previous work e.g.
.
however one dimension that is usually not explicitly considered is the one of scalability.
a given tcp approach might be effective and efficient for small sized programs but to what extent does it scale to big real world sized programs?
we address this concern in our third research question rq3 how does f ast compare with other existing prioritization approaches in terms of scalability ?
.
compared tcp approaches in order to conduct our experiments we had to decide which tcp approaches to consider for the comparison with f ast .
we limited the scope of our study to tcp approaches that require only test cases and or coverage information as input .
this decision is justified by the fact that other types of inputs used by some tcp techniques e.g.
models or requirements are not easily available for experimentation.
moreover as these are the only inputs that f ast techniques require this decision supports a fair comparison.
because f ast is based on similarity we started by searching the literature for state of the art stp techniques that would meet our selection criterion.
at this phase the following approaches were selected in bold within brackets we introduce the acronym used in the experiment description jiang et al.
proposed a family of art based tcp techniques guided by coverage information.
at each iteration a candidate set is dynamically4created by randomly picking test cases from the set of not yet prioritized tests as long as they can increase coverage.
the test case within the so built candidate set that is the farthest away from the set of already prioritized tests is selected.
the authors proposed and assessed different set distance functions for our experiments we implemented the version that performed better i.e.
maxmin .
zhou et al.
the essence of this tcp approach is the same of art d. the main differences are inthe way the candidate set is created and in the distance metric adopted.
while in the candidate set has a flexible size zhou et al.
proposed a fixed5size i.e.
for the candidate set.
besides the authors used manhattan distance instead of the jaccard distance adopted by jiang et al.
.
ledru et al.
as said in related work this approach only uses test input strings.
a greedy algorithm is applied that at each iteration picks the test case that is themost distant from the set of already prioritized ones.
several distance functions are evaluated and manhattan distance is the one recommended.
considering other non similarity based approaches two natural choices are the well known greedy total and greedy additional which pick as the next test case the one that covers the largest number of entities in total or among those yet uncovered respectively.
4this is why the d in the name label.
5this is why the f in the name label.we finally looked at the results from some recent studies comparing tcp approaches with the intent of selecting those emerging as the best ones.
excluding among the best tcp techniques indicated in those already included in our list or using models in input we could add more competitors one wb and one bb described below.
additional spanning is a variant of ga that at each iteration picks the test case that covers the largest number of not yet covered entities among those in the spanning set .
in coverage testing an element subsumes another if covering the former guarantees also covering the latter the notion of a spanning set was introduced in to denote the subset of non subsumed entities.
feldt et al.
this approach proposes to use the normalized compression distance between multisets introduced by to measure the diversity of sets of test cases.
while the measure is originally proposed as universally applicable to any test related feature the version performing better in considers test inputs.
we also looked at the approaches compared in .
in this case excluding the ones requiring additional information would yield to no new competing technique with respect to those already selected.
thus summarizing we collected as competing approaches bb ones i.e.
str and i tsd and wb ones i.e.
art d art f ga gt ga s. for each wb approach we implemented three variants addressing function statement and branch coverage totalizing competitors techniques.
.
evaluation metrics in order to assess prioritization effectiveness rq1 we use the average percentage of faults detected apfd .
apfd is calculated according to equation in which given a test suite tcontaining ntest cases and a set fof mfaults revealed by t for each ordering of twe denote as tfithe position of the first test case that reveals fault i. apfd tf1 tf2 ... tfm nm 2n.
to answer our research questions on efficiency rq2 and scalability rq3 we assess the investigated tcp approaches in terms of preparation time and prioritization time .
the preparation time considers the time spent by each tcp approach on tasks other than the prioritization itself e.g.
precomputing pairwise similarity between test cases whereasthe prioritization time considers only the time to process the already prepared test information and order the test suite.in reporting our results we also refer to total time which is simply the sum of preparation and prioritization times.
all times refer to the actually spent cpu time which we measured by using python s time.clock function.
.
study subjects to answer research questions rq1 and rq2 we conducted experiments on c and java programs.
the c programs 6this is the function recommended by the python software foundation for benchmarking or timing algorithms.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden b. miranda e. cruciani r. verdecchia and a. bertolino table study subjects details.
subject loc test loc tm tc fault type faults flex seeded grep seeded gzip seeded sed seeded make seeded closure compiler real commons lang real commons math real jfreechart real joda time real total loc lines of code tm test methods tc test classes.
namely flex grep gzip sed and make were collected from the software artifact infrastructure repository sir .
these programs are available in sequential versions each containing a different number of seeded faults.
the number of faults that can be revealed by the accompanying test suite varies greatly considering the extremes in some cases no faults can be revealed whereas in other cases multiple faults could be revealed by the vast majority of the test cases.
to minimize the influence of these characteristics in our study we selected from each program the version that contains the highest number of hard to find faults i.e.
faults that could not be detected by more than of the test cases.
the open source java programs investigated in our study namely closure compiler commons lang commons math jfreechart and joda time are integrated in the defects4j framework .
for a given program multiple versions are available each containing a single fault.
for our investigations we used as input for the tcp approaches the artifacts i.e.
coverage traces and test cases from the first version of the program only.
the effectiveness of the prioritized test suites are then assessed in the subsequent versions.
for jfreechart and joda time all the faults could be triggered by the test suite of the reference version.
thus we could use all the available versions for these subjects.
for the otherprograms however some faults could be revealed only by test cases that were not available in the reference test suite.
for those programs we considered the versions v1tovn with nbeing the first version for which the reference test suite could not trigger the existing fault.
the number of versions we considered for each java program is displayed in table column faults .
in total we used versions of different programs.
.
experiment procedure to answer rq1 and rq2 we applied the investigated tcp approaches to each experimental subject and measured i the preparation time ii the prioritization time iii the apfd of the prioritized test suites.
this process was repeated times to account for the stochastic nature of the tcp approaches considered in our study e.g.
when more than one test case would be equally ranked by the tcp approach a random choice is made to solve the tie .
for the java subjects the apfd of the prioritized test suites is computed for each of the subsequent versions considered for a given program.
to answer rq3 we considered two dimensions that might hinder tcp scalability i the size of the test suite ii the size of the test case representation.
item i is important as itdefines how many test cases need to be evaluated.
while this might have little influence for some tcp approaches such as e.g.
gt it might forbid the adoption of stp approachesthat would depend on pairwise similarity computation.
item ii on its turn is important as tcp approaches may use different test related information whose size can vary greatly from just a few characters for command line programs to thousands of words in the case of junit tests.
for coverage traces the size of test representation may change depending on both the coverage criteria and the program size.
to control these two dimensions in our experiments on scalability we used synthetic data.
with respect to the first dimension we considered test suite sizes that assume discrete values in the range from 1k to 1m as follows from 1k to 10k in increments of 1k i.e.
1k 2k 3k ... from 10k to 100k in increments of 10k and from 100k to 1m in increments of 100k yielding different test suite sizes .
with respect to the second dimension we considered three different sizes for a test case representation small for an average length of medium for 1k large for 10k elements.
in all three cases we allowed for a variance of .
we refer to generic elements for the size of test cases to be agnostic with respect to test criteria.
thus a small test case can be interpreted as a coverage trace from a test that covers functions in the same way that it can be understood as the textual representation of a command line test case containing words.
likewise a large test case could be either a coverage trace for the branch coverage criterion of a big program or a large junit test method or even a junit test suite .
all combinations of the different test case and test suite sizes account for different dimensions of testsets.
in order to collect data to answer rq3 for each test case size small medium large we applied the investigated tcp approaches to the synthetic test suites from the smallest 1k to the largest 1m one and measured the preparation time and prioritization time taken by each approach.
all the experiments were performed on an intel core tm i7 5960x with fixed .
ghz cpu 20m cache 32gb ram running linux opensuse .
.
results in this section we report and discuss the results.
note that with the aim of supporting the independent verification and replication we make available the artifacts produced as part of this work.7the replication package includes among others the implementation of the algorithms details on the hash function used 8input data raw data used for the statistical analyses and additional results.
8xxhash is the fastest non cryptographic hash function available to the best of our knowledge.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fast approaches to scalable similarity based test case prioritization icse may june gothenburg sweden .
rq1 effectiveness the apfd results achieved by the considered bb and wb tcp approaches are displayed as box plots in figure for the sake of space for wb we display only the box plots for branch coverage and in more detail in table .
note that results for the same f ast techniques are reported among both wb and bb approaches.
as explained in section for f ast techniques the only difference between wb or bb versions concerns the coding of test info after that the same algorithm can be identically applied.
a c bb b c wb branch c java bb d java wb branch the y axis displays the average percentage of faults detected apfd and the numbers in the x axis represent the prioritization approaches f ast pw f ast f ast log f ast sqrt f ast all str i tsd art d art f gt ga ga s figure apfd for bb and wb tcp approaches.
since the c and java programs contain different types of faults see section .
we analyze them separately to gather the effectiveness results.
as we could not assume our data to be normally distributed we adopted a non parametric statistical hypothesis test the kruskal wallis rank sum test to assess at a significance level of the null hypothesis that the differences in the apfd values for the different tcp approaches are not statistically significant.
for the particular case of c programs when considering the bb approaches the resulting p value for the test was .
meaning that we cannot reject the null hypothesis i.e.
no significant difference in effectiveness was observed.
for all the other cases the resulting p values were smaller than .2e meaning that the observed differences in effectiveness are statistically significant at least at the confidence level.
a significant kruskal wallis test indicates that at least one tcp approach stochastically dominates another one but does not identify the dominance relationship amongpairs of techniques.
to determine which tcp approachesare different we performed pairwise comparisons after thekruskal wallis test.
the results are displayed in table inside the parenthesis.
if two approaches have different letters they are significantly different .
.
if on the other hand they share the same letter s the difference between the means is not statistically significant.
an approach canhave more than one letter assigned to it.
as an example looking at the results for bb approaches and c subjects in table we can tell that f ast pw ab is not different from f ast log a and it is also not different from i tsd b even though f ast log a is different from i tsd b .
among the wb approaches the results confirm the ones reported in regarding the effectiveness of ga and ga s as both appear always in the first group.
concerning f ast the best results are achieved for function coverage it goes in the first group although with different members of the family for the c and java subjects.
among the bb approaches the f ast family consistently achieves the best group both for c and java with the exception of the f ast pw technique that for java programs performed worse than str.
these results hint that f ast performs better in terms of effectiveness for coarse grained criteria which is the most natural target when considering big testsets.
answer apfd results vary across techniques and subject programs.
a trend can be observed that f ast provides better effectiveness with more coarse grained techniques bb and function coverage .
.
rq2 efficiency to answer rq2 we compared the investigated tcp approaches in terms of time required to perform the prioritization both including and excluding the preparation time.
we report the results in table note that we do not discriminate between c and java as efficiency results are only impacted by the size of test cases and coverage traces.
we applied the kruskal wallis rank sum test to verify at a significance level of the null hypothesis that the observed differences in the total time and in the prioritization time are not statistically significant.
in both cases total time and prioritization time only regardless of the coverage criteria considered the resulting p values were always smaller than .2e leading us to reject the null hypothesis.
to identify which tcp approach dominates the others in terms of efficiency we proceeded with a pairwise comparisonafter the kruskal wallis test.
when considering bb tcp the f ast family outperformed all the competitors.
for wb tcp the results vary between total or only prioritization time and among the coverage criteria.
overall when considering total time gt outperformed the competitor approaches for all the criteria.
this result is aligned with our expectation as gt does not require any kind of preparation and simply performs a sorting of the test cases based on how many entities they cover.
nevertheless such approach was always followed by at least one member of the f ast family which outperformed even the other greedy competitors ga and ga s .
for the particular case of statement coverage gt is followed by the whole f ast family before the other tcp approaches.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden b. miranda e. cruciani r. verdecchia and a. bertolino table apfd results for the investigated bb and wb tcp approaches.
bb approachwbapproachfunction statement branch c j ava c j ava c j ava c j ava fast pw .
.
ab .
.
c fast pw .
.
a .
.
f .
.
d .
.
e .
.
bc .
.
de fast .
.
ab .
.
a fast .
.
cd .
.
abc .
.
e .
.
bc .
.
c .
.
de fast log .
.
a .
.
a fast log .
.
cd .
.
ab .
.
de .
.
c .
.
bc .
.
cd fast sqrt .
.
a .
.
a fast sqrt .
.
cd .
.
abc .
.
de .
.
b .
.
c .
.
de fast all .
.
ab .
.
a fast all .
.
bc .
.
de .
.
de .
.
d .
.
c .
.
ef str .
.
ab .
.
b art d .
.
a .
.
de .
.
c .
.
b .
.
b .
.
def i tsd .
.
b .
.
d art f .
.
ab .
.
e .
.
c .
.
bc .
.
bc .
.
f gt .
.
e .
.
bc .
.
f .
.
a .
.
d .
.
bc ga .
.
d .
.
a .
.
a .
.
c .
.
a .
.
a ga s .
.
a .
.
cd .
.
b .
.
a .
.
a .
.
b results are displayed in the format m being m the median apfd the standard deviation and the group for the pairwise comparisons after the kruskal wallis test.
different letters indicate significant differences between the approaches .
.
table prioritization times including and excluding preparation time for the investigated tcp approaches.
bb approachwbapproachfunction statement branch tot.
time prio.
time tot.
time prio.
time tot.
time prio.
time tot.
time prio.
time fast pw .
.
e .
.
e fast pw .
.
g .
.
g .
.
f .
.
f .
.
f .
.
g fast .
.
d .
.
d fast .
.
f .
.
e .
.
e .
.
d .
.
e .
.
e fast log .
.
b .
.
b fast log .
.
e .
.
d .
.
d .
.
c .
.
d .
.
c fast sqrt .
.
c .
.
c fast sqrt .
.
d .
.
b .
.
c .
.
b .
.
c .
.
b fast all .
.
a .
.
a fast all .
.
b .
.
a .
.
b .
.
a .
.
b .
.
a str .
.
f .
.
f art d .
.
h .
.
i .
.
g .
.
i .
.
g .
.
i i tsd .
.
g .
.
g art f .
.
i .
.
j .
.
h .
.
j .
.
h .
.
j gt .
.
a .
.
c .
.
a .
.
e .
.
a .
.
d ga .
.
c .
.
h .
.
f .
.
h .
.
c .
.
h ga s .
.
j .
.
f .
.
i .
.
g .
.
i .
.
f results are displayed in the format m being m the median time total or prioritization the standard deviation and the group for the pairwise comparisons after the kruskal wallis test.
different letters indicate significant differences between the approaches .
.
for str the preparation time considers the time required for computing the pairwise similarity matrix for ga s it refers to the time required to extract the spanning entities.
by taking into account exclusively the prioritization time f ast all and f ast sqrt had the best performance for the three criteria surpassing even gt.
for statement coverage the most demanding criteria in terms of time required the best result achieved by a competitor was gt at fifth place.
a trend emerging from table is that the more demanding a criteria the better the performance of the f ast approaches especially by considering exclusively prioritization time.
the reasons for this will be discussed in the following section while answering rq3.
answer the fastest members of f ast family outperform the bb approaches in terms of total time and all competitors when we consider only prioritization time.
.
rq3 scalability to answer rq3 we assessed the tcp approaches with respect to the time required to prioritize synthetic test suites representing both wb and bb with sizes from 1k to 1m and for three test case dimensions small medium and large .
in figure we provide the line plots for the total time figures 3a to 3c and for the prioritization time figures 3d to 3f required by different tcp approaches.
although we have allowed all the approaches to proceed even further for aclearer visualization the plots in figure report only the results for the testset sizes that could be prioritized within two hours.
in fact we considered that a tcp approach that can perform the task within two hours or less would allow developers to run the prioritization on their own machine in thetimeframe of a meeting or lunch break.
hence we considered executions exceeding the two hours less interesting.
we do not include in the plots the results for the stp approaches as they performed poorly and their results could not be easily visualized due to their long execution times.
we summarize the results below in the format small medium large where for each test case size we report the largest testset size whichcould be completed within the two hours while indicatesthat the approach was not able to complete even the smallest testset str 4k 2k when considering total time and 6k 5k 5k when considering prioritization time i tsd art d 6k 2k art f 6k 1k .
the plots show that when considering total time gt outperforms all the competitors and can prioritize 1m size testset for the three sizes of test cases.
the f ast family on its turn clearly outperformed ga and ga s. two members of the family f ast all and f ast sqrt prioritized the 1m set for small and medium test cases within the two hours limit while f ast log prioritized the 600k and 400k test suites for small and medium test cases respectively.
as expected f ast approaches are affected by the dimension of the test cases the signature of each test case can be computed in linear time on the size of the test representation thus for bigger test cases this step will take longer.
however once the preparation phase is completed the only dimension that matters is the one of the test suite size see the strong similarity of the line plots of the fa s t approaches in figures 3d to 3f .
this answers our open question from the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fast approaches to scalable similarity based test case prioritization icse may june gothenburg sweden .2m .4m .6m .8m 1m01000200030004000500060007000 hour a total time small test cases .2m .4m .6m .8m 1m01000200030004000500060007000 hour b total time medium test cases .2m .4m .6m .8m 1m01000200030004000500060007000 hour c total time large test cases .2m .4m .6m .8m 1m01000200030004000500060007000 hour d prioritization time small test cases .2m .4m .6m .8m 1m01000200030004000500060007000 hour e prioritization time medium test cases .2m .4m .6m .8m 1m01000200030004000500060007000 hour f prioritization time large test cases figure total prioritization time y axis seconds to prioritize testsets of varying dimension x axis number of test cases .
previous section the relative performance of the f ast family improves with respect to the competitors as the criteria becomes more demanding in terms of test case information used because when we consider only the prioritization time the size of the test representation does not affect the f ast approaches in the same way that it affects the competitors.
considering only prioritization time gt still outperforms most f ast approaches but f ast all that outruns gt for all the testsets for medium and large test cases.
besides that f ast sqrt outperforms gt for all the testsets with large test cases and also for some of the medium testsets.
answer considering total time all f ast techniques were second only to gt.
if only prioritization time is counted f ast all surpass gt for medium and large testsets and f ast sqrt outruns it for some of the medium and all of the large testsets.
overall except for f ast pw the bigger the testset the better f ast s relative performance.
.
on the costs and benefits of f ast as it can be noticed from our results for rq2 and rq3 most of the cost associated with f ast lies in the preparation phase.
bb input is first mapped into shingles in linear time.
then the preprocessing involves going through all the test cases computing their signatures and storing them for futureuse.
in particular the time complexity of computing a single signature is o h with hbeing the signature length making the entire preprocessing phase costing o hn with a test suite of size n. the cost for computing jaccard similarity of two sets is o l where lis the size of the biggest of the two sets while estimating it through the minhash signatures costs o h .
lsh on its turn is able to create a candidate set in o n .
concerning space costs the overall space required by fa s t iso shn where sis the size of a hashed value.
in contrast the cost of storing a distance matrix for a generic stp approach with all the pairwise similarities e.g.
ledru et al.
iso bn2 where bis the size of the float representing a distance between two test cases.
in some settings e.g.
regression testing the cost of updating existing information e.g.
signatures distance matrix is of interest.
as discussed before f ast can simply add information regarding a new test case by computing its signature.
in contrast the time required by a stp approach to update an existing distance matrix is that of comparing the newly introduced test case with all the existing ones not only this operation is slower in terms of time complexity but it also requires the entire test suite for the comparison.
it is also possible even though we have not yet exploited this possibility to use parallelization to reduce the running authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden b. miranda e. cruciani r. verdecchia and a. bertolino time of f ast .
in fact the preprocessing phase i.e.
the computations of the signatures of the buckets and of the candidate set are all parallelizable.
to decide which variant of f ast to use one should consider the time available for the prioritization task.
f ast pw is the most precise in the ranking because it guarantees that the candidate test case that is the most dissimilar from the already prioritized ones will be chosen as the next test but it is also the most time consuming.
when the time required by f ast pw cannot be afforded other members of the family can be chosen.
if the objective of the prioritization is toincrease diversity then the order of choice should be pw one log sqrt and all.
f ast can provide greater benefits if it is adopted in an environment where the test case signatures can be reused as new test cases are added.
a regression testing environment combined with the use of black box representation for the test cases seems to be a perfect fit for f ast .
.
threats to validity the results reported must be considered in light of potential threats to validity of the experiments.
internal validity concerns factors different from the treatment that could have affected the observed behavior .
one common threat is the selection of experimental subjects.
in assessing effectiveness and efficiency we opted for benchmark programs that have been made available and used in similar studies to favorreplicability of results but they could not be good representa tive of actual regression test scenarios.
however as the same subjects have been used for all approaches possible threats apply to all.
a similar argument can be done concerning the assessment of the time taken by the various techniques.
to mitigate potential threats related to this point we have i implemented all the algorithms in the same programming language ii captured the process cpu time with checkpoints at the same places iii performed all the experiments in the same machine.
concerning scalability assessment our simulated scenarios could bias the results because the randomly generated test information could reproduce unusual situations.
we have created multiple testsets to try to reduce possible bias but only many real world big testsetscan prevent this threats.
concerning possible errors in the study implementation all developed code has been rigorously inspected all experiments have been repeated more than once and all code and data are made available.
external validity concerns whether the results are generalizable beyond the experiment subjects.
about effectiveness andefficiency our results may suffer from the limited number and the specific characteristics of the chosen subjects although to mitigate this potential threat we covered two differentlanguages.
about scalability our chosen modeling for testsets dimension could not be valid in real contexts.
besides different parameter settings in the f ast algorithm might produce different results.
this is not really a validity threat though it only implies that other f ast implementations can be done and have to be evaluated.
conclusions and future work we have introduced the f ast family of approaches for fast test case prioritization.
the simple yet powerful idea behind f ast is that of managing the big testsets of modern software development processes through the use of well establishedtechniques for big data.
the results from our experiments both on real test subjects and on synthetic data support our idea.
they showed that in comparison with bb techniques we can significantly improve prioritization efficiency with no impact in effectiveness.
the fastest members of f ast defeated all competitors even greedy total if we only count the prioritization time after preparation.
more importantly theresults show that f ast can scale up to industrial demands we prioritize one million test cases in less than minutes.
overall the f ast family offers different effectiveness and efficiency results allowing for a range of techniques spanning over bb and wb criteria and for fine tuning the selection of the next test cases depending on size and time.
the approach worked nicely on commonly used benchmarks yet its most attractive target is obviously the realm of what we called big testsets to which sophisticated or fine grained techniques cannot be applied.
some recent works have addressed regression testing at google scale by applying heuristicsthat use historical or dependency information.
indeed a gap exists between academic fine grained and practical coarse approaches.
our results hint that f ast can help reduce such gap and allows to use more refined criteria to test prioritization even for big testsets.
to the best of our knowledge this is the first proposal to apply lsh techniques to address the growth of testing problem dimensions.
we expanded here the idea for tcp but we prospect a great potential for application of the sametechniques to other testing tasks.
there are several problems in testing that can leverage similarity computations.
for example an immediate follow up study will address test case selection that we did not include here for lack of space and time.
we expect that variants of f ast can allow for efficiently selecting the most dissimilar test cases among thousands or millions of test cases.
another possibility is that of handling product lines where huge numbers of product variations need to be tested and f ast could help in quickly finding the most diverse configurations among a large set.
moreover adopting multi objective prioritization for fa s t could be a good point for future work the f ast algorithm could be adapted to consider other objective functions in addition to dissimilarity although it is to be evaluated how this could affect the approach efficiency.