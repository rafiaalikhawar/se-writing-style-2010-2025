neural machine translation based commit message generation how farare we?
zhongxin liu zhejiang university china liu zx zju .edu.cn david lo singapore management university singapore davidlo smu.edu.sg abstractxin xia monash university australia xin.xia monash .edu zhenchang xing australian national university australia zhenchang.xing anu.edu.au keywordsahmed e.hassan queen suniversity canada ahmed cs.queensu.ca xinyu wang zhejiang university china wangxinyu zju.edu.cn commit messages can be regarded as the documentation ofsoft ware changes .these messages describe thecontent andpurposes ofchan ges hence areuseful forprogram comprehension andsoft ware maintenance.
however due to the lack oftime and direct motivation commit messages sometimes areneglected by develop ers.
to address thisproblem jiang et al.
proposed anapproach we refer to it as nmt which leverages aneural machine translation algorithm toautomatically generate short commit messages from code.
the reported performance oftheir approach ispromisin g however they didnotexplore why their approach performs well.
thus in this paper we first perform anin depth analysis oftheir experimental results.
we find that most of the test diffs from which nmt can generate high quality messages are similar to one ormore training diffsat the token level.
about ofthe commit messages injiang etal.s dataset arenoisy due to being automatically generated or due to them describing repetitive trivial changes .
the performance ofnmt declines by a large amount after removing such noisy commit messages .
inaddition nmt is complicated and time consuming .inspired by our first finding we proposed asimpler and faster approach named nngen nearest neighbor generator togenerate concise commit messages using thenearest neighbor algorithm.
our experimental results show that nngen is over times faster than nmt andoutperforms nmt interms ofbleu anaccuracy measure that iswidely used toevaluate machine translation systems by .finally we also discuss some observations for the road ahead forautomated commit message generation toinspire other researchers.
ccs concepts software and itsengineering software maintenance tools permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise.
or republish.
to post on servers or to redistribute to lists.
requires prior specific permission and or a fee.
reque st permi ssion s from p enni ssions acm.or g. ase september montpellie r france 2018association for computing machinery.
acm isbn ... .
http s dol org l o .
.
373commit message generation nearest neighbor algorithm neural machine translation acm reference format zhongxin liu xin xia ahm ede.hassan david lo zhenchang xing and xinyu wan g. .
neural ma chine translation based com m it mes sage g eneration ho w far are we ?
in proceedings ofthe 33rd acm ieee international confe rence on automated softwar e engineering ase sep tember montpellier france.
acm new york ny usa pages.
http s d ol org .
.
1introduction insoftware projects version control systems arewidely used to manage theevolving code base.
while committing achange to a version control system developers document their changes using acommit message .
acommit message is afree form textual de scription of its corresponding change.
the message may summarize what happened in the change and orexplain why thechange was made .
there isempirical evidence that the use ofcommit messages iscommonplace in code that ismanaged with version control systems .
documentation plays an important role in program comprehen sion andsoftware maintenance .
as the documentation of changes commit messages can help developers understand the ra tionales behind changes before they dig into details .
commit messages also provide information tounderstand the evo lution ofsoftware .
however due to the lack of direct moti vation and time pressure writing high quality commit messages remains aneglected issue.
dyer et al.
report that around of the commit messages in 23k java sourceforge projects were com pletely empty .
many tools have been proposed togenerate commit messages automatically .
the commit messages created bythem canassist developers inwriting high quality commit messages or replace empty commit messages.
given the dif fof a change most ofthese tools e.g.
deltadoc andchangescribe are able to produce detailed messages which cananswer what was changed and where this change happened.
but their generated messages areverbose and fail to reveal therationale behind a change.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france itishard toautomatically generate high quality commit mes sages since answering why a change happened usually requires syn thesis of different kinds of knowledge and context.
however recent studies report that commit messages follow some patterns andit ispossible tolearn patterns ofsoftware artifacts from large datasets .based on these findings jiang et al.
proposed the adap tion ofaneural machine translation nmt technique togenerate commit messages from chan gediffs .in the remainder of the paper forsimplicity sake we refer to their approach asnmt.
nmt aims to learn how to write commit messages from prior changes and their commit messages.
different from prior work nmt focuses on producing short messages which canreveal therationales behind chan ges.jiang etal.reported theperformance oftheir approach using adataset built from the top ikjava projects ingithub .
nmt hasmany advantages in contrast to existing commit message g eneration methods nmt produ cesshort summaries in stead ofexhaustive descriptions ofchanges.
nmt does notre quire manually defined templates asneeded bymany prior tools e.g.
deltadoc andchangescribe .
nmt cangenerate commit messages forchanges tomany types ofsoftware artifacts not only source code changes .
however jiang et al.
did notexplore why nmt performs so well in their paper.
understanding theapplicable scenario ofan approach can help us apply it in practice.
so in this paper we first investigate therationale fornmt s good performance .
additionally nmt isquite complex and its tr aining process is very slow .jianget al.
spent 38hours training their nmt model on an nvidia geforce gtx .however according to the suggestions of fu and menzies it is a good practice to explore simple and fast techniques before applying deep learning methods on se t asks.
therefore wewish toinvestigate theconstruction of amuch simpler and faster approach toaddress the same problem that issolved bynmt.
our study aims to answer thefollowing research questions rq1 why does nmt perform sowell?
weconduct ananalysis ofthegenerated commit messages by nmt using thedata published onjiangetal.s website .we randomly read 200commits injiang et al.stestresults andmanually identify those high quality generated commit messages bynmt we call them good messages .then those identified good messages andtheir corresponding commits arefurther analy zedby us.
from the analysis we find the codediffs of most of the good messa ges aresimilar to one or more training dif fsat the token level.
we also find that jiang et al.
s dataset contains noisy commit messages like messa gesthat areautomatically produced byother development tools e.g.
acontinuous integration ci bot named liferay continuous integration ormessages that arewritten by hu man butcontain little and redundant information e.g.
update readme.md .such amessage describes neither what was changed in the readme filenorwhy thechan gehappened hence contains little information .inaddition since the information can be obtained easily bylooking at the list ofchanged files it is also redundant.
itmakes little sense totrain and test approaches e.g.
nmt for automated commit message generation onsuch noisy messages .
therefore wemanually derive thepatterns ofsuch noisy commit messages and build a new dataset bydeleting such noisy commit 374liu xia hassan l o xing and wang messages and their corresponding diffs from jiang et al .sdataset.
we re train nmt on this cleaned dataset andobtain anew model.
compared to the model trained on the original dataset theper formance ofthenew model declines by a large amount and the bleuscore anaccuracy measure that is widely used to evalu atemachine translation systems decreases by .
from .
to .
.
rq2 can asimpler and faster approach outperform nmt?
inrql we found that the code dif fsof most of the good me s sages are lexically similar to one or more dif fsin the training set.
inspired by this finding wepropose asimpler and faster approach named nngen nearest neighbor generator toautomatically gen erate commit messages from dif fs.
our approach is based on the nearest neighbor algorithm and does not require atraining process .
togenerate a commit message for a newdiff nngen first finds thediff which is most similar to the newdiff i.e.
thenearest neighbor from the training set.then it simply outputs thecommit message of the nearest neighbor as the generated commit message .
our experimental results show that nngen outperforms nmt onjiang et al.
s dataset and thecleaned dataset interms ofbleu by a substantial margin.
moreover it only takes 23to30seconds to run nngen on a cpu instead of24to38hours on a gpu which means that nng enisover times faster than nmt .we also perform ahuman evaluation to compare nng enand nmt .our evaluation shows that nng enperforms better than nmt and the improvement issignificant.
finally weconduct afurther analysis ofautomated commit message generation.
wepoint outthat only diffs and commit messages arenotenough forthis task.
by answering theabove research questions wejust move one step further butthere is still a lon gway to go.
the main contributions ofthiswork are as follows we perform anin depth analysis of the experimental results in jiang et al.swork and analyze the reasons ofnmt s good performance.
we propose asimpler andfaster appro achcalled nng ento generate short commit messages.
nngen is over times faster than nmt andsignificantly outperforms nmt.
the remainder ofthispaper isorganized as follows.
section introduces thebackground ofourstudy.
section 3describes our experimental settin gs including research questions and dataset.
section 5details our experiments and the experimental results of each research question respectively.
section 6discusses thereason behind nngen sbetter performance thecases where nmt out performs nngen theimplications ofourstudy and threats to the validity of our reported findings.
section 7discusses some observa tions for the road ahead forautomated commit message generation .
section 8surveys therelated work.
section 9concludes thepaper.
2background .
commit diff commit messages jiang et al.sdataset isextracted from git repositories .git is one of the most popular version control systems.
each time a developer commits achange git will create a commit for this change and allow thedeveloper towrite atextual message called a commit message to describe the change.
a commit in git contains achan ge and acommit message whi chmay beempty .
a ch ange can be authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
neural machine translation based commit message generation represented by adiff which captures the difference between two program versions and can be generated using the gitdiffcommand in git.
in this work bymentioning acommit we are referring to thepair ofa code diff and itscorresponding commit message .
given acommit werefer to its original commit message which isextracted from the git repository as the reference message the produced commit message bynmt as the nmt message and the produced commit message bynngen as the nngen message .
.
jiang etal s nmt approach the nmt model adapted byjiang et al.
is the attentional rnn encoder decoder model which is an extension to the rnn encoder decoder model .the rnn encoder decoder model was originally designed fortranslating between natural languages.
there aretwo parts in this model the encoder and thedecoder each of which is arecurrent neural network rnn .
given asource sen tence i.e.
a sentence written in the source language theencoder reads andencodes it into a fixed length vector.
this vector can be regarded as the intermediate representation of the source sentence andcontains theneeded information fortranslation .the decoder outputs thetarget sentence from the encoded vector .the encoder and the decoder arejointly trained using a large number of pairs of source sentences and target sentences .in the machine translation community this kind ofdataset isreferred to as a parallel corpora.
compared to the rnn encoder decoder model theattentional rnn encoder decoder model introduces theattention mechanism to cope with long source sentences .
injiang et al.swork thesource sentences arediffs and the target sentences arereference messages.
the parallel corpora are collected from github which contains pairs ofhistorical dif fs and thecorresponding reference messages.
after training on the special parallel corpora jiang et al.
s model can translate a new dif finto ashort textual description which may summarize the corresponding change.
.
bleu to align with jiang et al.
we use the bleu 4score toevaluate theperformance ofnngen .the bleuscore is an accuracy measure that iswidely used toassess thequality ofmachine translation systems .
the score first calculates themodified n gram for bleu n i precisions of acandidate sequence to the reference message then measures theaverage modified n gram precision with apenalty foroverly short sentences.
inour case we regard agenerated commit message annmt message or annngen message as acandidate.
considering the fact that bleu aims tomatch human judgment at a corpus level andjiang et al.
use a corpus level bleu score toevaluate nmt we also calculate the bleu score at the corpus level.
inaddition nngen leverages the bleu score internally to measure thesimilarity between two di ffs.however it calculates bleu 4score at the sentence level.
3experimental setup .
research questions jiang et al.
did notinvestigate why their approach performs so well.
understanding thereasons isimportant forapplying nmt in practice.
so first of all we want toinvestigate 375ase september montpellier france rql why does nmt perform so well ?
additionally nmt isquite complicated and its training process isvery slow and costly e.g.
requiring specialized and dedicated hardware .
simple and fast methods areusually easier to be adopted inpractice.
fuand menzies also recommend theexploration of simple methods first while dealing with setasks .therefore wewould like to know rq2 can a simpler and faster approach outperform nmt?
.
dataset since we wish toinvestigate thereason behind nmt s good per formance and compare theperformance ofnngen and nmt we simply usejiang et al.
s dataset toconduct ourexperiments.
jiang et al.
have gratefully published their dataset .to make our paper self contained we briefly describe thebuilding process ofjiang et al.sdataset infollowing paragraphs.
collecting data jiang et al.
collected 2mcommits from the most starred lkjava projects in github.
preprocessing they first extracted the first sentence of each col lected commit message .next to reduce their vocabulary sizeand improve theperformance ofnmt they removed commit ids from dif fs andremoved issue ids from reference messages .then they removed merge commits rollback commits and commits with a dif fthat islarger than 1mb.
finally they broke reference mes sages and diff sinto tokens .but they didnotconvert tokens into lowercase and nor did they split the camelcase tokens.
after preprocessing .8m commits remained .
filtering to apply the nmt algorithm jiang et al.
needed to filter commits bylength i.e.
the number oftokens in asequence .
they only kept commits with adi f f length ofnomore than 100and areference message length ofno more than .only 75k commits meet these length requirements .inaddition jiang et al.
introduced the verb direct object v do filter for the reference messages.
they did so because the nmt algorithm performs better on such pattern ofmessages .their v do filter identifies theverb direct object pattern e.g.
delete a method through the dobj dependency in thestanford corenlp library .they removed theextracted sentences which do not begin with a dobj dependency.
jiang et al.
only preserved 32kfrom the 75k messages that begin with a dobj dependency.
after preprocessing andfiltering jiang et al.
randomly divided the32kcommits into sets i.e.
training set validation set and test set.
the training setcontains 26k commits.
the validation setand thetestset each contain 3kcommits.
4rq1 why does nmt perform sowell?
.
analyzing nmt messages toinvestigate rql we closely analyze thegenerated commit mes sages bynmt i.e.
nmt messages.
we first randomly select commits from jiang et al.stest set.then the first author and a master student independently evaluate thenmt messages ofthese 200commits .
injiang et al.swork they conducted ahuman study toevaluate thequality ofnmt messages.
given a commit human experts were authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase sept emb er montp ellier fran ce liu xia hassan lo xing and w a ng o i changelog .
3i messag e upua recnanqeloq message generated by nm t updated c hangelogi!
!ichangelog mdid e letedifigure anexample ofabotmessage3 cm dver liferay com m i t 2f03e545085c 159d922 fbgeac9 b166ee821 com m i t c3d68dbcaaa 18c18e76bb46697c52e4d8el mode push 8i .. ...........message .. .1a te modules apps foundation porta l .
... .... .93enerated by nm t ignore update modules apps foundation portal .
import as json unittest time shutil sys sys .
pa t h .extend import h2o h2o cmd o ssage moduleslappslfo undat ion portal l.gitrepo ichanged i00 figure asimilar training commit tofigu re2 canseethat without con siderin gthe cas e messaqe is identical to messaqei andmessage j sdi f f is similar t othatofmessaqe at the t oken level.
thisfindin gis sur prising since it me ansthateven using s uch a complicated nmt algor ithm nmt is still n obetter than a nearest neighbor based recommend er.
byreading the commits ofthesegood mess ages we a lso obs erve that many oftheirref erence messages arenoisy.
weidentif y two ca tego riesforsuch n oisy messages .one category isnam ed byus as botmessages whichreferstoreference m essages that are automatically gene ra ted b y otherdevelopm enttools.the othe r category whichwe calltrivia l m essages repr e sentsref erence mes sages that are w rittenbyhum an sbutcontain little and redundant inform ation thatone caneasilyinfer for example b yjustlooking at the li stof changed file s. figure anexample ofatrivial message figure 4shows an e xampl eofabot message.
the commit in figure4is co llected fr omthe repository ofliferay portal .we notethatthenmt message isnearly the sameastheref erence message.
however after s earchin gingithub w efind that this com mitis pushed b y a co ntinuous int egrati on c i botnamed liferay continuous i ntegratio n whichinturn automatically genera tes thi s ref erence message.
therefore theref erence message in fi gure4is a bot m essage.
anexample ofatrivia l message is presented in figure5.nmt also ge nerates anearly i dentical commit m essage totheref erence26.
jgo r a i0 import unitt est random sy s tim e sys.
pa t h .extend import h2o host s l ssage message genera tedby nm t add h2o hostso g scores figure distribution ofthescores ofnmt message s py testd ir singlejvml tescp layers na.py ichanged i figure the commit ofagood message asked t oreaditsrefere nce m essage andnmt message andgivea score between to 7tomeasurethe sema ntic similarity b etween thetwomessages .a score of m eans thatthetwo m essages have noth ing i ncommon andascore of 7means thatthey are id enti cal in meaning.
to g rade th e 200nmt messages thetworaters care fully read the sco ring examples p rovided byjian g e tal.
andrated eachnmt messages following ji an g e tal.sevalua tion criterion.
wefind ahighlevel o fagree ment b etween thetworater s witha cohen s k app a coe fficient of0.
which shows a substantial agreement among the d ifferent rater s.afterrating the two r aters discussed theirdisagreem entsto reach co nsensus.
f igure1shows thefinaldistributi onof th e sco res.
next we identif ythosehigh qu ality co mmit m essages ge nerated bynmt.
for si mplicity sake w e refer tothosemessages as good messages .to align with jian g e tal.
weregard thenmt messages that arescored 6or7 asgood messages .wefind .
good messages fromthe200nmt messages whichisclosetothe res ultsof jiang e t al.shum an study .
good messages onadifferenttestset.
afterpickin g outgood messages wecarefully r eadthesemes sages and th eir cor respo nding commits again to t ryto rec ognize somesimple p attern sin th em.but w edonotfind any ob vious tex tualpattern s w hich areshared among allgood messages.
itappears thatnmt canprodu ce va rious t ypes o f high quality messages .con sidering th atmachine learningmethod slearn from th etrainin g set before predictin g wethen sea rch th etrain ing se t tofind similar commit m essages for eac hgood messages through i ts keywords and tr yto gainsome in sightsfrom the se similar tr aining commit messages.
wefind th at fo r nearly eve rygood messages 70out of71 w e can find outoneor mo retrainin g co mmit m essages that are nearly identic altothegood message and th ediffsof thesetrain ing messages ar esimilar to th atofthegood message at thetoken level.
for exa mple figure 2presents atest co mmit of which th enmt message is agood message .werefertothisgood message asmessaqei figure 3shows a commit which i sfound in thetrainin g set by sea rching th eref erence messages that co ntain h2o hosts .
w e call theref erence message in figure 3messaqe i.we authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
neural machine translation based commit message generation ase september montpellier france table our trivial message patterns table bleu scores oforiginal and new nmt models update changelog gitignore readme prepare version bump version dataset original cleanedbleu .
.
.
.8pz .
.6p3 .
.
.
.
modify dockerfile makefile update submodule means optional refers to or and version nu mber refers to th e version number introdu cedinachange.
message .however both messages only mention that thechangelog file was updated and fail to describe what waschanged in detail nor why thischange occurred.
therefore such a message contains little information.
since a programmer with rudimentary knowledge of version control systems is able to obtain theinformation byglancing thename ofthechanged file the information isoflittle value .
moreover this kind of messages can be automatically produced by some rule based tools.
for example we can write ascript toparse a new change .if the change only modified the changelog file of the project ourscript will first extract thefilename of the changed file then simply output update filename .
from these examples we can see that there is little useful infor mation involved inthese two categories ofmessages .moreover both botmessages and trivial messages can be generated through rule based methods e.g.
liferay continuous integration or a simple script .
therefore itmakes little sense tolearn from orproduce these two kinds ofmessages through machine learning methods.
.
evaluating nmt onthecleaned dataset based on the discovery of noisy messages a question emerges in our mind if we deleted such noisy messages and their corresponding diffs i.e.
thenoisy commits from jiang et al.
s dataset and re trained nmt on the new dataset how much would the performance ofnmt beaffected ?
inorder toanswer theabove question we first build anew dataset byremoving thenoisy commits from jiang et al.
s dataset.
to delete such noisy commits we need to automatically identify bot messages and trivial messages.
forbot messages weonly find the messages that aregenerated bylijeray continuous integration.
since all such messages follow the same pattern which is ignore update filename it is easy to identify them through aregular expression .
however there aremore than one types oftrivial messages.
to identify them we manually derive some common patterns oftrivial messages byskimming thecommits in jiang et al.
s dataset.
table presents ourtrivial message patterns .the exact regular expressions are available in our online appendix .table 2shows thepropor tions of ouridentified bot messages and trivial messages injiang et al.sdataset.
we can see that bot messages and trivial messages are common injiang et ai.
s dataset.
after identifying bot messages and trivial messages their corre sponding commits areregarded as noisy commits .we build the new training set validation setand testset by deleting noisy commits from thetraining set validation setand test setofjiang et al.
sdataset respectively .please note we do notclaim that wehave found anddeleted allnoisy commits injiang et ai.
sdataset.
only those commits ofwhich thereference messages match ourextracted patterns arecleaned by us.
then were train and test nmt on the cleaned dataset the bleu score is computed toevaluate the new model just likejiang et al.
the experimental results areshown in table .since the dataset theimplementation ofnmt and the training and testscripts used by us are provided by jiang et ai.
we simply use the results that are reported intheir work forperformance comparison .we can see from table 3that theperformance ofthenew model declines by a large amount.
the bleu score ofthenew model is .
lower than theoriginal model.
these results show that thegood performance ofnmt mainly comes from those noisy commits in jiang et al.
s dataset.
insummary after an in depth analysis ofnmt messages we find that thediffs of most out of71 in our randomly selected test set good messages aresimilar to one or more training diffs at the token level.
about ofjiang et al.
s commits arenoisy commits .
the performance ofnmt declines by a large amount after removing such noisy commits.
nngen leverages thenearest neighbor nn algorithm toproduce commit messages.
the nnalgorithm is a lazy learning method which issimple and does notrequire atraining phase .just like nmt ourapproach takes as input anew diffand a training set and outputs aone sentence commit message for the new diff .
our approach first extracts dif fsfrom the training set.
next the training di f f sand the new dif farerepresented asvectors in the form of bags of words .in abag of words model the grammar and theword order ofadiffareignored only term frequencies are kept.
we refer to this kind of vector as adiffvector.
then nngen calculates thecosine similarity between thenew diffvector and each training diffvector and selects the top k training dif fswith highest similarity scores.
after that the bleu 4score between the new di f f and each ofthetop k training diffsarecomputed.
the training diffwith thehighest bleu score isregarded as thenearest neighbor of the new diff.finally ourapproach simply outputs thereference message ofthenearest neighbor as the final result.
in summary given a new diff ourapproach will first find5.
nngennmt leverages thecomplex slow and resource consuming nmt algorithm togenerate commit messages.
inspired by our first finding in rq1 we propose anearest neighbor based approach named nngen which is simpler and faster than nmt while outperforming nmt interms ofbleu score onjiang etal.s dataset and the cleaned dataset.
rq2 can asimpler and faster approach outperform nmt?
driginal r efers tothe original data setprovided byjiangeta1.
s. cleane d refer s totheclean ed data set.
the ble u sco res ar ecalculated on the whole t est se t.pn refers tothemodified n gram pre cision.
.
.
.
all trivial messages .
.
.
bot messages .
.
.
table proportions ofidentified messages.
dataset original validation original testoriginal training authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase septe mbe r mon tpel lier france table4 bleu 4scores ofnmt and nngen dataset approac hbleu pi pz p3 p4 orig ina lnmt .
.
.
.
.
nnge n .
.
.
.
.
cleanednmt .
.
.
.
.
nngen .
.
.
.
.
ihebleu 4scoresarecalculate donthetest se ts.pn referstothemodified n gramprecision.
table time costs ofnmt and nngen dataset appro achdevice training time testing time nmt gtx1070 38hours .5mins original nmt gtx 34hours 17mins nngen cpu n a 30sees cleanednmt gtx 24hours 13mins nngen cpu n a 23sees gtx1070 an dgtx1080 referto nv idiageforce gtx 1070and1080 respectivel y. cpu i s intel coreis2.5ghz.
itsnearest neighbor inthetraining set then reuse thereference message oftheneares tneighbor as thegenera tedmessage forthe newdif f asdescrib edin sec tion2.
bleu score is apopular and automated metric forevaluating thequalityof mac hine t ranslation.
itcan also be used to meas ure t he sim ilarity between twosente nces.
compared tocosine similar ity thebleu score takes into account theorder ofwords .however thecompu tatio nofbleu score is relative ly slow.
to speed up o urapproach we do not find t heneares t neigh bor by calculating bleu sco resfor al l thetraining di.ffs.
instead we firs t usethecosine similarity between diffvectorsto find the k nearest neig hbor candidates.
then thebest candida teis selected accor dingto bleu scores .this strategy balances the time cost andaccuracy ofnngen .bydefau lt we se tk as .
.2automati cevaluation we eva luate thenngen onjiang et al.sdataset andthecleaned dataset described in4.
using the corp us level bleu 4score and compare the test res ultsofnngen with th ose o fnmt.
the test results ofnmt on jia ngetal.sdatase t arerepor tedin and we directly usethem for performance comparison .to eva lua te nmt on thecleaned datase t we use theimplement ation ofnmt and thetraining and test scripts which areprovided onjiang et al.s websi te thentrain an d testnmt usingan nv idiageforce gtx wi th8gb memory .
table 4prese ntstheevaluation resultsfornmt and nngen we can seethatour nnge napproac houtperforms nmt interms of bleu 4score oneach dataset.
thebleu improvemen tsachieved byourapproa chrange from to .moreover allthe modified n gram precis ions pi p4in ta ble4 ofourapproac harehigher thanthose o fnmt.
to investigate whe thernngen isfaster thannmt we meas ure thetime costs ofnng enonjiang etal.s datasetand thecleaned dataset and compare them withthetime cos tsofnmt .the time costs ofnmt on jiang et al.sdatase tisprovided by jia ngetal.
in .jiangetal.conductedthetrainingand testofnmt on a n nvidia geforce gtx 1070gpu wit h8gb memory .whileevaluating nmt wecond uctthetraining and testing processes on an nvidia geforce gtx gpu wi th8gb memory .however nngen is evaluated onacpu intel core is .5gh z with8gb ram .table5 378liu xia hassa n l o xing andwang jooq src main jav a org jooq impi currentuser.java ichangeoi ix i la s c r e ntuser extends abstractfunction string case postgres message rtforthefirebird da tabase fixe dcurrent user generated message addsupport f orthefirebird database fixedmulti record insert genera tedmessage add implement ation to shadow application .checkpermis sion score o fgenerated message scoreofgenerated m essage figure aquestion inoursurvey score definition t womessages have so mesimilarinformatio n buteachofthem co ntai ns some informa tion w hich isnotmen tioned bythe ot her.
example reference m essage reduce the heap to for je nkins generated m essage increased h ea p explanat ion the two messages alltalkabout th emodification o f heap size but one mentions thedecrease o f heap s ize th e other m en tions t heincrease.
figure apart ofourscoring criterion shows thetime costs o fnmt and nnge n.forcomparison we a lso presen tthetime costsofnmt on jiang et al .sdatase tusing our server .we can see that ittakes to hour s to train nmt and .
to17 minutes t otest on t hetwo datasets .since nnge ndoes not need training itstraining time is marked as n a .
the time cost of itstesting processes isonly 23to seconds .thismeans nnge nis cons iderab ly more than times fas ter th annmt onthe two datasets .
.3human evaluation we also co nduct ahum an eva luation toevaluate nngen andcom pare nngen withnmt .we inv ite6 ph .d.studen ts toparticipa te in oursurvey allofwhom arenotco au thors majorincompu ter science and haveindustria lexperience injava pro grammi ng rang ingfrom 4years .eachpartic ipan tisasked toread 100commits and assess thesemantic similari tiesbetween reference messages and each of com mit m essagesgene ra ted bynng enandnmt .
.
.7procedure.
we ra ndomly select200commits from the cleaned datase t described in sec tion .
divide them even lyintotwo groupsand make a questionnaire foreach group.inourquestion naires each questio nfirstprese nts t he info rma tion of onecommit i.e.
itsdif f itsrefere ncemessage itsnmt message and i tsnng en message thenasks participa ntsto givetwo sco resbetween0 to to measure thesemantic similarities between therefer ence mes sage and thetwogenera tedmessages.
score 0means thereis no similari tybetwee n the two messages and score4means two mes sages are identica linmeaning.
figure 6shows one q uestion in our survey .participants aretoldthat the first message isthereference message buttheorder of the nmt message andthenngen message is random lydecided.
so participa ntsdonotknow which message isgenera tedbywhic happroa ch and they are asked to en terto score each generated message separately .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
neural machin e tran slati on based co mmit m essage ge nera tion ase se pte mber montp ellier fran ce 89o .iginal testdiffs random test ditt s repart level low saurce sets s au rcesets.mainltaalversian .
.
effort lmax l25 l26 l27 128l28 mes sage l29 gson main o nly messagegeneratedby nm t add ultim ateandroiduicomponent proje ct in d emoofui findbu9s ignarefailures false saurcesets s au rcesets.
mai nl messa ge q4 qe rdbuq sfor main isubprojects figure10 thenearest neighbor found bynngen compute th e cos inesimilarities betwee neach testdiffvector and each trainingdi.
fvector .finally for eac h testdiff we se lect th e trainin gdiff with thehighest c osine similarity score and record this sco re.there are3 co mmits in ji an g e t al.
stest se t hen ce we o btain a setof3 000cosine similari ty scores.
toreducethepotentialvariance cause d b yhowjian g e tal.s dataset isdivided we a lsoperform a10 fold c ross validation.
specif ically wefirst shuffle th e 208commits in jian g e tai.
sdat aset and divide these c ommits i ntoto fold s.then we runthe same p ro cedureasthe above e xperim enttotime s. eac h time o ne foldof forthelastrun commits i sused as thetest se t and theremainin gfolds o f28 forthelastrun commits i s used as the tr aining se t. we obtain a setof cosinesimilarity scores from th eto fold c ross va lidation.
we visualize th edistribut ion o feachsetofcosinesimilarity scores u sing a v iolin pl ot as show n in f igur e .theleft p lot is thedistributi on o f thescores ofjiang etal.
stest se t and th e right oneisthedistribution o f the sco resobtained from th e10 fold c ross validation .the visualization r esults show th atthedistribution o fmessage generat edby nngen only run findbu gsformain figure a test commit eibuild.grad leich an g e di build.gradleichanged i 1lfindbugs figure8 distribution ofthecosine similarities between test diffs andtheirneare stneighbor s0.
.
.
!
.
.5e vi0.
c .
.
unnge n .
.
.
.46approac h lo w me dium h igh mea nscore nmt .
.
.
.34table the results ofour userstudy 6discussion .
why does nngen perform better?
given anewdiff nnge nfirst find sthediff which is most similar t oitatthetoken level fro mthetrainin g set then simply outputs th e commit m essage o f thetrainin gdiff asthe generated commit m essage .hence we s pec ulate th atthe reas onofnnge n s betterperforman ce is that given atest co mmit there is a high chance th atthere w illalways exista verysimilar tr ainin g commit to it.
to ve rify ourconjecture we co nduct anexpe rime nt on jian g etal.
sdataset.
we firstconve rt alldiffs in thetrainin g setand thetest se tintodi.
fvectors descr ibed insection .
.then we low medium and high refer tolow qualit y medium qualit yand hi gh qualitymessages respective ly.
eachcommit group i s eva lua ted b y 3participants.ourscoring criterion islisted i n thebeginn ing o feach que stionnaire to guide participants.figure7presents apartofourscoring criterion.
our complete scoring criterion can be found in ouronlin e appendix .
inadditio n participa nts areallowe d t o sea rch th einternet for re lated inform ation .
different fromjiang e tal.shum an study fir st we selectcommits from thecleaned dataset i nstea d ofjiang e tal.
sdataset si nceit is meanin gless toevalua te the commit me ssages ge nera ted for noisy commits.
seco nd th e score ra ngeofoursur vey i s instead o f .
alarge score range re quires our parti cipant s tospend m ore e fforts disting uishing subtle sema ntic differe nces butinthiswork we care more a bou t th e rough qu alit y o fgenera ted m essages instead of subtle semantic d ifferences.
inaddition a point scale is widely used i nprior softwareeng in eeringstudies .third we a lso p rovide diff sinourquestionnaires tohelppart icip ants maketheir judgments .
.2results.we obtain 600pairsofscores from ourhum an eva l uation.
eachpair co ntains a sco refor the nmt message anda score for the correspo nding nnge n message.
weregard a sc oreof a nd aslowquality a score o f2asmedium quality a nda sco reof3and ashigh quality.t able 6present s the res ults of our user study.
w e can seethattheproportion of high quality nnge nmessages is slightly higher thanthatof hi gh quality nmt messages.
.
of thennge n messages areofmedium quality while fornmt messages thepro portion is only8.
.thenumber oflow qu alit y nnge nmess ages is smallerthanthat of low qualit ynmt messages.
moreover the mean sco reofnngen mess ages ishigher thanthat of nmt messages.
these results sh ow t ha tnnge noutpe rforms nmt.we also u se a wilc oxon s igned rank t est ata significance l evel to check whe the r th eperfor ma nce differe nces between nngen andnmt aresignifican t. the p value is0.
which me an stheimpr ovement achieve d b ynnge nis significant.
insummary nnge nismuch simpler andfaster th annmt.our experimental r esult s s how th atnngen outperforms nmtin terms of bleu score on jian g e tal.sdataset a nd th eclean ed dataset .
inaddition ourhuman evaluation shows th atnngen outpe rforms nmt and th eperform ance impr ovement is statistically significant.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france liu xia hassan l o xing and wang pom .xmi ichangeo ifigure anexample where nmt performs bettermessage generated by nngen remo veddoublecheckedlocking froq .
checkst yle.xml.
m odu lename e m pt yst at em e nt m odu lename a r raytypest yl e m odu lename m i ss i ngo ve r ri de groupl d com .
zaxxe r groupl d art i fac t l d h i ka r i c p art ifact l d ve rs i on .
.
sn a ps ho t ve rs i on 49 message .
.
0final izechecksty lerule message generated bynmt added defa ultcomeslast checkstyle rule figure another example where nmt performs better are similar in the meaning while compared to this nngen message this nmt message ismore specific andaccurate.
therefore nmt performs better in this case.
after further searching in the training set we find that there exist some training commit messages which share the same pattern as this nmt message .for example a training commit message is prepare release hikaricp .
.
.this means nmt has the ability togeneralize but its generalization ability is very limited sothat its overall performance is worse than our simple approach i.e.
nngen.
due to the space constraint more details of the aforementioned examples can be found inouronline appendix .
ve rs i on .
.
ve rsi on .
message ease hikaricp .
.
message generated by nmt prepare release hikaricp .
.
message generateo oynng en repa refor next develo m1be nt it e ra tio n. ..... build checkst yle.xml lchangeo i !
l .
implications from this work we distill some general suggestions which isbeyond the specific task andapproaches .
clean upthedata carefully.
insoftware repositories there may exist some noisy data e.g .
the noisy commits injiang et al.
s dataset.
itmakes little sense totrain and test our models on the noisy data.
worse still wemay getmisleading results if we conduct experiments ondataset with noisy examples .therefore we recommend researchers toalways clean up their datasets carefully before training and testing their models .
consider simple approaches first.
our study shows that it is worth trying simple and fast methods before applying complicated and time consuming techniques onsoftware engineering tasks.
this try with simpler practice is also recommended by fu and menzies .implementing and applying simple methods only costs a little effort but may bring a deep understanding of the data .
moreover for some se tasks simple approaches are able to achieve equal or even better performance in less time e.g.
nngen vsnmt .each setofscores ishighly skewed.
specifically foreach set the cosine similarities between most test dif fsandtheir most similar training dif fsarehigher than .
.hence themajority oftest commits arevery similar toanother training commit at the token level.
we also manual examine some examples from jiang et al .s dataset toexplain thebetter performance ofnngen.
figure 9and10 show oneofsuch examples .figure 9presents atest commit its nmt message and its nngen message.
itsnearest neighbor found bynngen isshown infigure .
we can see that thediffs of thetwo commits aresimilar atthetoken level and their refer ence messages areidentical inmeaning.
by finding out this nearest neighbor nngen produces ahigh quality commit message for this testcommit.
however thecommit message generated bynmt is notrelevant to this testcommit at all which means nmt fails to generate agood commit message.
insummary weargue that given atestcommit there is ahigh chance that there will exist a training commit which is very similar to it and forthose testcommits that arevery similar toanother training commit nngen cangenerate better commit messages than nmt.
.
where does nmt perform better?
although theoverall performance ofnngen isbetter than that ofnmt on the two datasets in some cases nmt messages obtain higher scores than their corresponding nngen messages.
to figure outsuch cases we compare theaverage scores ofeach nmt mes sage anditscorresponding nngen message and find there are commits where nmt performs better than nngen .
wemanually analyze these 30commits and their generated commit messages.
we find that for outofthe commits the nmt messages and nngen messages are all oflow quality i.e.
score or .
in each case themeanings ofthereference message and the two generated messages aredifferent butnmt generates theright verb at the beginning ofthenmt message.
therefore theaverage scores ofthe nmt messages are all and those ofthecorresponding nngen messages are all o.forexample the reference message ofatestcommit is add abstractprocessingfil ter.getauthenticationdetailssourceo itsnmt message is added getter forauthoritiespopulator and its nngen message is properly handle empty layout ingetfirstvisiblepositiono .we can see that themeanings of the three messages are different butnmtgenerates thecorrect verb add at the beginning.
thus thisnmt message is better than thisnngen message .
as for the other 10commits we find that without considering the case thenmt messages of9commits areidentical to one or more training commit messages.
in these cases the reason ofnmt s better performance may be that nmt captures better nearest neigh bors than nngen.
figure 11shows anexample.
after comparing thetraining diffsofthenearest neighbors captured bynmt andnngen with this test dif f we find that although thetraining dif fselected bynngen is lexically more similar to this test diff themeaning and thewriting style ofthis nmt message iscloser to this reference message.
thus this nmt message obtains higher average score.
there is also a special case where thenmt message can not be found in the training set as shown in figure .
the three messages authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
neural machine translation based commit message generation ase september montpellier france figure anexample ofaunique commit .
src!com google javascri pvjscom p com piler.java lcflangeoi ..!
public class compiler extends abstractcompiler library srclandroidtest assetslt sisample.ts.o.dump ich angedi 00track encoderpadding subsampleoffsetus selection flags language null language und54 message .
.ctortests1049 l50 l511051 .
.1 5 message move