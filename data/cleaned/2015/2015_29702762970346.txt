automatic microbenchmark generation to prevent dead code elimination and constant folding marcelino rodriguez cancio university of rennes france marcelino.rodriguezcancio irisa.frbenoit combemale university of rennes inria france benoit.combemale irisa.frbenoit baudry inria france benoit.baudry inria.fr abstract microbenchmarking evaluates in isolation the execution time of small code segments that play a critical role in large applications.
the accuracy of a microbenchmark depends on two critical tasks wrap the code segment into a payload that faithfully recreates the execution conditions of the large application build a sca old that runs the payload a large number of times to get a statistical estimate of the execution time.
while recent frameworks such as the java microbenchmark harness jmh address the sca old challenge developers have very limited support to build a correct payload.
this work focuses on the automatic generation of payloads starting from a code segment selected in a large application.
our generative technique prevents two of the most common mistakes made in microbenchmarks dead code elimination and constant folding.
a microbenchmark is such a small program that can be over optimized by the jit and result in distorted time measures if not designed carefully.
our technique automatically extracts the segment into a compilable payload and generates additional code to prevent the risks of over optimization .
the whole approach is embedded in a tool called autojmh which generates payloads for jmh sca olds.
we validate the capabilities autojmh showing that the tool is able to process a large percentage of segments in real programs.
we also show that autojmh can match the quality of payloads handwritten by performance experts and outperform those written by professional java developers without experience in microbenchmarking.
ccs concepts software and its engineering !software performance keywords performace evaluation microbencharking text tagging .
introduction microbenchmarks allow for the nest grain performance testing e.g.
test the performance of a single loop .
this kind of test has been consistently used by developers in highly dependable areas such as operating systems virtual machines data structures databases and more recently in computer graphics and high performance computing .
however the development of microbenchmarks is still very much a craft that only a few experts master .
in particular the lack of tool support prevents a wider adoption of microbenchmarking.
microbenchmarking consists in identifying a code segment that is critical for performance a.k.a segment under analysis sua in this paper wrapping this segment in an independent program the payload and having the segment executed a large number of times by the sca old in order to evaluate its execution time.
the amount of technical knowledge needed to design both the sca old and the payload hinder engineers from e ectively exploiting microbenchmarks .
recent frameworks such as jmh address the generation of the sca old .
yet the construction of the payload is still an extremely challenging craft.
engineers who design microbenchmark payloads very commonly make two mistakes they forget to design the payload in a way that prevents the jit from performing dead code elimination and constant folds propagations cf cp .
consequently the payload runs under different optimizations than the original segment and the time measured does not re ect the time the sua will take in the larger application.
for example click found dead code in the ca einemark and sci mark benchmarks resulting in in nite speed up of the test.
ponge also described how the design of a popular set of microbenchmarks that compare json engines1was prone to over optimization through dead code elimination and cf cp.
in addition to these common mistakes there are other pitfalls for payload design such as choosing irrelevant initialization values or reaching an unrealistic steady state.
in this work we propose to automatically generate payloads for java microbenchmarks starting from a speci c segment inside a java application.
the generated payloads are guaranteed to be free of dead code and prevent cf cp.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
copyright is held by the owner author s .
publication rights licensed to acm.
ase september singapore singapore acm.
... .
our technique statically slices the application to automatically extract the sua and all its dependencies in a compilable payload.
second we generate additional code to i prevent the jit from over optimizing the payload using dead code elimination dce and constand folding constant propagation cf cp ii initialize payload s input with relevant values and iii keep the payload in steady state.
we run a novel transformation called sink maximization to prevent dead code elimination.
we turn some sua s local variables into elds in the payload to mitigate cf cp .
finally we maintain the payload in stable state by smart reseting variables to their initial value.
we have implemented the whole approach in a tool called autojmh .
starting from code segment identi ed with a speci c annotation it automatically generates a payload for the java microbenchmark harness jmh .
jmh is the de facto standard for microbenchmarking.
it addresses the common pitfalls when building sca olds such as loop hoisting and strength reduction optimizations that can make the jit reduce the number of times the payload is executed.
we run autojmh on the loops present in mature java projects to assess its ability to generate payloads out of large real world programs.
our technique extracts sua into microbenchmarks of all loops and correctly generates complete payloads for of the loops .
we also evaluate the quality of the generated microbenchmarks.
we use autojmh to re generate microbenchmarks handwritten by performance experts.
automatically generated microbenchmarks measure the same times as the microbenchmarks written by the jmh experts.
finally we ask professional java engineers to build microbenchmarks.
all these benchmarks result in distorted measurements due to naive decisions when designing benchmarks while autojmh prevents all these mistakes by construction.
to sum up the contributions of the paper are a static analysis to automatically extract a code segment and all its dependencies code generation strategies that prevent arti cial runtime optimizations when running the microbenchmark an empirical evaluation of the generated microbenchmarks a publicly available tool and dataset to replicate all our experiments2 in section we discuss and illustrate the challenges for microbenchmark design which motivate our contribution.
in section we introduce our technical contribution for the automatic generation of microbenchmarks in java.
in section we present a qualitative and quantitative evaluation of our tool and discuss the results.
section outlines the related work and section concludes.
.
payload challenges in this section we elaborate on some of the challenges that software engineers face when designing payloads.
these challenges form the core motivation for our work.
in this work we use the java microbenchmark harness jmh as to generate sca olds.
this allows us to focus on payload generation and to reuse existing e orts from the community in order to build an e cient sca old.
dead code elimination dead code elimination dce is one of the most common optimizations engineers fail to detect in their microbenchmarks .
during the design of microbenchmarks engineers extract the segment they want to test but usually leave out the code consuming the segment s computations the sink allowing the jit to apply dce.
it is not always easy to detect dead code and it has been found in popular benchmarks .
for example listing displays a microbenchmark where the call to math.log is dead code while the call to m.put is not.
the reason is that m.put modi es a public eld but the results of the math.log are not consumed afterwards.
consequently the jit will apply dce when running the microbenchmark which will distort the time measured.
map string double m maputils .
buildrandommap benchmark public void hiddendce math .log m. put ten listing an example of dead code a key feature of the technique we propose in this work is to automatically analyze the mircrobenchmark in order to generate code that will prevent the jit from running dce on this kind of benchmark.
.
constant folding constant propagation in a microbenchmarks more variables has to be explicitly initialized than in the program.
a quick naive solution is to initialize these elds using constants allowing the compiler to use constant folding and constant propagation cf cp to remove computations that can be inferred.
while mostly considered prejudicial for measurements in some punctual cases a clever engineer may want to actually pass a constant to a method in a microbenchmark to see if cf cp kicks in since it is good for performance that a method can be constant folded.
however when not expected the optimizations causes microbenchmarks to return deceitfully good performance times.
good examples of both dce and cf cp optimizations and their impact on the measurements can be found in literature .
concrete evidence can also be found in the jmh examples repository3.
.
non representative data another source of errors when designing payloads is to run a microbenchmark with data not representing the actual conditions in which the system being measured works.
for example suppose a maintenance being done over an old java project and that di erent sort methods are being compared to improve performance one of them being the collections.sort method.
suppose that the system consistently uses vector t but the engineer fails to see this and uses linkedlis t in the benchmarks concluding that collections.sort is faster when given as input an already sorted list.
however as the system uses vector lists the actual case in production is the opposite sorted lists will result in longer execution times as shown in table making the conclusions drawn from the benchmark useless.
le tip jmh samples src main java org openjdk jmh samples 133table execution times of collections.sort using a sorted list using an unsorted list linkedlist ns ns vector ns ns .
reaching wrong stable state the microbenchmark sca old executes the payload many times warming up the code until it reaches a stable state and is not optimized anymore.
a usual pitfall is to build microbenchmarks that reach stable state in conditions unexpected by the engineer.
for example if we were to observe the execution time of the collection.sort while sorting a list one could build the following wrong microbenchmark linkedlist double m listutils .
buildrandomlist benchmark public void dosort collections .
sort m listing sorting a sorted list in each run unfortunately after the rst execution the list gets sorted.
in consecutive executions the list is already sorted and consequently we end up measuring the performance of sorting an already sorted list which is not the situation we initially wanted to measure.
.
autojmh autojmh automatically extracts a code segment and generates a complete payload with inputs that re ect the behavior of the segment in the original application.
the generation process not only wraps the segment in an independent program it also mitigates the risks of unexpected dce and cf cp optimizations and ensures that it will reach stable state in the same state executed by the sua during the unit tests.
test suiteapplicationinstrument and executepayload0.
check snippet1.
extract segment and its dependencies2.
sink maximization .
cf cp prevention4.
smart reset5.
generate test cases6.
generate initialization valuesreg.
test cases figure global process of autojmh for payload generation.
figure illustrates the di erent steps of this process.
if the sua satis es a set of preconditions detailed in section .
autojmh extracts the segment into a wrapper method.
then the payload is re ned to prevent dead code elimination constant folding and constant propagation steps as well as unintended stable state step when the payload is executed many times.
the last steps consistin running the test suite on the original program to produce two additional elements a set of data inputs to initialize variables in the payload a set of regression tests that ensure that the segment has the same functional behavior in the payload and in the original application.
in the rest of this section we go into the details of each step.
we illustrate the process through the creation of a microbenchmark for the return statement inside the enumerateddistribution value method of apache common math shown in listing .
the listing also illustrates that a user identi es a sua by placing the javadoc like comment bench this on top of it.
this comment is speci c to autojmh and can be put on top of every statement.
the resulting payload is shown listing .
double value double x double ... param throws dimensionmismatchexception nullargumentexception validateparameters param bench this return sigmoid .
value x param param listing an illustrating example a sua in commons.math in listing we can see that autojmh has wrapped the return statement into a method annotated with benchmark.
this annotation is used to indicate the wrapper method that is going to be executed many times by the jmh scaffold.
the private static method sigmoid.value has been extracted also into the payload since it is needed by the sua.
autojmh has turned variables xand params into elds and provides initialization code from them loading values from a le which is part of our strategy to avoid cf cp.
finally autojmh ensures that some value is returned in the wrapper method to avoid dce.
class mybenchmark double params double x setup void setup loader l new loader data sigmoid 160 .dat x l. loaddouble params l. loaddoublearray1 double sigmoid value double x double lo double hi return lo hi lo fastmath .exp x benchmark public double payloadwrapper return sigmoid value x params params listing an illustrating example the payload generated by autojmh .
preconditions the segment extraction is based on a static analysis and focuses on suas that meet the following conditions.
these preconditions ensure that the payload can reproduce the same conditions than those in which the sua is executed in the original program.
.
initialized variables used by the sua are of the following types primitive int double boolean their class counterparts integer double boolean string types implementing the serializable interface or collections and arrays of all the above.
non initialized variables used by the sua can be of any public type.
134this condition ensures that autojmh can store the values of all variables used by the sua .
none of the methods invoked inside the sua can have a target not supported in item .
this ensures that autojmh is able to extract all methods used by the sua.
.
all private or protected methods used by the sua can be resolved statically.
dynamically resolved methods have a di erent performance behavior than statically resolved ones .
using dynamic slicing we could make available to the microbenchmark a non public dynamic method but we would distort its performance behavior.
.
the call graph of all methods used by the sua cannot be more than a user de ned number of levels deep before reaching a point in which all used methods are public.
this sets a stopping criterion for the exploration of the call graph.
.
sua extraction autojmh starts by extracting the segment under analysis sua to create a compilable payload using a custom forward slicing method over the abstract syntax tree ast of the large application which includes the source code of the sua.
the segment s location is marked with the benchthis javadoc like comment introduced by autojmh to select the segments to be benchmarked.
if the sua satises the preconditions autojmh statically slices the source code of the sua and its dependencies methods variables and constants from the original application into the payload.
non public eld declarations and method bodies used by the sua are copied to the payload their modi ers static nal volatile are preserved.
some transformations may be needed in order to achieve a compilable payload.
non public methods copied into the payload are modi ed to receive their original target in the sua as the rst parameter e.g.
data.dosomething becomes dosomething data .
variable and method may be renamed to avoid name collision and to avoid serializing complex objects.
for example if a segment uses both variable data and a eld myobject.data autojmh declares two public elds data and myobject data .
when method renaming is required autojmh uses the fully quali ed name.
at the end of the extraction phase autojmh has sliced the sua code into the payload s wrapper method.
this relieves the developer from a very mechanical task and its automation reduces the risks of errors when copying and renaming pieces of code.
yet the produced payload still needs to be re ned in order to prevent the jit from overoptimizing this small program.
preserving the original performance conditions.
we aim at generating a payload that recreates the execution conditions of the sua in the original application.
hence we are conservative in our preconditions before slicing.
we also performed extensive testing to be sure that the code modi cations explained above do not distort the original performance of the sua.
these tests are publicly available4.
then all the additional code generated by autojmh to avoid dce initialize values mitigate cf cp keep stable state is inserted before or after the wrapped sua.
.
preventing dce with sink maximization during the extraction of the sua we may leave out the code consuming its computations the sink giving the jit an opportunity for dead code elimination dce which would distort the time measurement.
autojmh handles this potential problem featuring a novel transformation that we callsink maximization .
the transformation appends code to the payload which consumes the computations.
this is done to maximize the number of computations consumed while minimizing the performance impact in the resulting payload.
there are three possible strategies to consume the results inside the payload make the payload wrapper method return a result.
this is a safe and time e cient way of preventing dce but not always applicable e.g.
when the sua returns void .
store the result in a public eld .
this is a time e cient way of consuming a value yet less safe than the previous solution.
for example two consecutive writes to the same eld can make the rst write to be marked as dead code.
it can also happen that the payload will read from the public eld with a new value modifying its state.
jmh black hole methods .
this is the safest solution which does not modify the microbenchmark s state.
black holes bh are methods provided by jmh to make the jit believe their parameters are used therefore preventing dce.
yet black holes have a small impact on performance.
a naive solution is to consume all local variables live at the end of the method with bhs.
yet the accumulation of bh method calls can be a considerable overhead when the execution time of the payload is small.
therefore we rst use the return statement at the end of the method taking into consideration that values stored in elds are already sinked and therefore do not need to be consumed.
then we look for the minimal set of variables covering the whole sink of the payload to minimize the number of bh methods needed.
sink maximization performs the following steps to generate the sink code .
determine if it is possible to use a return statement.
.
determine the minimal set of variables vmincovering the sink of the sua.
.
when the use of return is possible consume one variable from vminusing one return and use bhs for the rest.
if no return is possible use bhs to consume all local variables in vmin.
.
if a return is required to satisfy that all branches return a value and there is no variables left in vmin return a eld.
to determine the minimal set vmin the autojmh converts the sua code into static single assignment ssa form and builds a value dependency graph vdg .
in the vdg nodes represent variables and edges represent direct value dependencies between variables.
for example if the 135value of variable adirectly depends on b there is an edge frombtoa.
an edge going from one variable node to a phi node merging two values of the same variable is a back edge .
in this graph sink nodes are nodes without ingoing edges.
initially we put all nodes of the vdg in vmin except those representing elds values.
then we remove all variables that can be reached from sink nodes fromvmin.
after this if there are still variables in vminother than the ones represented by sink nodes we remove the back edges and repeat the process.
int d a b c i f a d a h a b a listing a few lines of code to exemplify sink maximization to exemplify the process of nding vminwithin sink maximization let us consider listing .
the resulting vdg graph is represented in gure .
sink nodes are nodes dand b1 which are represented as rounded nodes.
the links go from variables to their dependencies.
for example ddepends on a0andh.
since it is not possible to arrive to all nodes from a single sink dorb1 in the example vmin fd b1g.
consequently both dand bmust be consumed in the payload.
figure vdg of listing .
cf cp mitigation since all sua are part of a larger method they most often use variables de ned upfront in the method.
these variables must be declared in the payload.
yet naively declaring these variables might let the jit infer the value of the variables at compile time and use constant folding to replace the variables with a constant.
meanwhile if this was possible in the original system it should also be possible in the payload.
the challenge is then to detect when cf cp must be avoided and when it must be allowed to declare variables and elds accordingly.
autojmh implements the following rules to declare and initialize a variable in the payload constants static final elds are initialized using the same literal as in the original program.
fields are declared as elds keeping their modi ers static nal volatile and initialized in the setup method of the microbenchmark.
their initial values are probed through dynamic analysis and logged in a le for reuse in the payload cf.
section .
for details about this probing process .
local variables are declared as elds and initialized in the same way except when a they are declared by assigning a constant in the original method and b all possible paths from the sua to the beginning of the parent method include the variable declaration i.e.
the variable declaration dominates the sua in which case their original declaration is copied into the payload wrapper method.
we determine whether the declaration of the variable dominates the sua by analyzing the control ow graph of the parent method of the sua.
listing shows how the variables xandparams are turned into elds and initialized in the setup method of the payload.
the setup method is executed before all the executions of the wrapper method and its computation time is not measured by the sca old.
.
keep stable state with smart reset in section we discussed the risk for the payload to reach an unintended stable state.
this happens when the payload modi es the data over which it operates.
for example listing shows that variable sumis auto incremented.
eventually sumwill be always bigger than randomvalue and the payload will stop to execute the return statement.
public t sample final double randomvalue random .
nextdouble double sum bench this for int i i probabilities .
length i sum probabilities i f randomvalue sum return singletons .get i return singletons .get singletons .
size listing variable sumneeds to be reset to stay in the same state autojmh assumes that the computation performed in the rst execution of the payload is the intended one.
hence it automatically generates code that resets the data to this initial state for each run of the sua.
yet we implement this feature of autojmh carefully to bring the reset code overhead to a minimum.
in particular we reset only the variables in uencing the control ow of the payload.
in listing autojmh determined that summust be reset and it generates the code to do so.
benchmark public double dobenchmark sum sum reset smart reset here !
for int i i probabilities .
length i sum probabilities i f randomvalue sum return singletons .get i return sum listing variable sumis reset by code appended to the microbenchmark to determine which variables must be reset autojmh reuses the vdg built to determine the sinks in the sink maximization phase.
we run tarjan s strongly connected components algorithm to locate cycles in the vdg and all variables inside a cycle are considered as potential candidates for reset.
in a second step we build a control flow graph cfg and we traverse the vdg trying to nd paths from variables found in the branching nodes of the cfg to those found in the cycles of the vdg.
all of the variables that we succesfully reach are marked for reset.
.
retrieving inputs for the payload the last part of the microbenchmark generation process consists in retrieving input values observed in the original application s execution steps and of gure .
to retrieve these values we instrument the original program to log the variables just before and after the sua.
then we run once the test cases that cover the sua in order to get actual values.
the user may also con gure the tool to use any program executing the sua.
to make the collected values available to the payload autojmh generates a speci c jmh method marked with the setup annotation which executes only once before the measurements containing all the initialization code for the extracted variables.
listing shows an example where the initial values of variables xand params are read from le.
test public void testmicrobench loader l new loader get values recorded before execution l. openstream data sigmoid 160 .
dat mybenchmark m new mybenchmark m.x l. readdouble m. params l. readdoublearray1 double mresult m. payloadwrapper check sua s output is equal to payload s output l. openstream data sigmoid 160 after .dat assertequals m.x l. readdouble assertarrdblequals m.params l. readdoublearray1 assertequals mresult m. payloadwrapper listing generated unit test to ensure that the microbenchmark has the same functional behavior than the sua .
verifying functional behavior to check that the wrapper method has the same functional behavior as the sua in the original application i.e.
produces the same output given the same input autojmh generates a unit test for each microbenchmark where the outputs produced by the microbenchmark are required to be equal to the output values recorded at the output of the sua.
these tests serve to ensure that no optimization applied on the benchmark interferes with the expected functional behavior of the benchmarked code.
in the test the benchmark method is executed twice to verify that the results are consistent within two executions of the benchmark and signal any transient state.
listing shows a unit test generated for the microbenchmark of listing .
.
evaluation we perform a set of experiments on large java programs to evaluate the e ectiveness of our approach.
the purpose of the evaluation is twofold.
first a quantitative assessment ofautojmh aims at evaluating the scope of our program analysis looking at how many situations autojmh is able to handle for automatic microbenchmark generation.
second two qualitative assessments compare the quality of autojmh s generated microbenchmarks with those written by experts and with those built by expert java developers who have little experience in microbenchmarking.
we investigate these two aspects of autojmh through the following research questions rq1 how many loops can autojmh automatically extract from programs into microbenchmarks?in addition to the generation of accurate microbenchmarks it is important to have a clear understanding of the reach ofautojmh s analysis capacities.
remember that autojmh can only handle those segments that meet certain preconditions.
therefore we need to quantify the impact of these conditions when analyzing real world code.
rq2 how does the quality of autojmh s generated microbenchmarks compare with those written by experts?
our motivation is to embed expert knowledge into autojmh to support java developers who have little knowledge about performance evaluation and who want to get accurate microbenchmark.
this research question aims at evaluating whether our technique can indeed produce microbenchmarks that are as good as the ones written by an expert.
rq3 does autojmh generate better microbenchmarks than those written by engineers without experience in microbenchmarking?
here we want to understand to what extent autojmh can assist java developers wanting to use microbenchmarks.
.
rq1 automatic segment extraction we automatically annotate all the loops of real java projects with the bench this annotation to nd out to what extent the tool is able to automatically extract loops and generate corresponding payloads.
we focus on the generation of benchmarks for loops since they are often a performance bottleneck and they stress autojmh s capacities to deal with transient states although the only limitations to the slicing procedure are the ones described in section .
.
we selected the following projects for our experiments because their authors have a special interest in performance apache math is the apache library for mathematics and statistics vectorz is a vector and matrix library based around the concept of n dimentional arrays.
apache common lang provides a set of utility methods to handle java core objects jsyn is a well known library for the generation of music software synthesizers.
imagelib2 is the core library for the popular java scienti c image processing tool imagej.
exact versions of these projects can be found in autojmh s repository5.
table sumarizes our ndings one column for each project and the last column shows totals.
the row payloads generated shows the number of loops that autojmh succesfully analyzed and extracted in a payload code.
the row payloads generated initialized re nes the previous number indicating those payloads for which autojmh was able to generate code and initialization values i.e.
they were covered with at least one unit test .
the row microbenchmarks generated further re nes the previous numbers indicating the amount of loops for which autojmh was able to generate and initialize a payload that behaves functionally the same as the sua i.e.
equal inputs produce equal results .
the rows below detail the speci c reason why some loops could not be extracted.
we distinguish between variables unsupported or invocations unsupported .
as we can see the main reason for rejection are unsupported variables.
finally row test failed shows the number of microbenchmarks that failed to pass the generated regressions tests.
the percentages are overall percentages.
the key result here is that out of the loops found in all projects autojmh correctly analyzed extracted and 137table reach of autojmh property math vect lang jsyn img2 total total loops payloads generated payloads generated initialized microbenchmarks generated rejected variables unsupported unsupported type collection type is not public type is not storable invocations unsupported target unssuported levels too deep .
private constructor .
protected abstract method .
test failed .
wrapped loops into valid microbenchmarks.
these microbenchmarks resulted from payloads for which autojmh was able to generate and nd initialization values and who s regression test did not fail.
in total autojmh generated the code for payloads.
the tool rejected loops because they did not meet the preconditions.
looking into the details we observe that vectorz and apache lang contain relatively more loops that satisfy the preconditions.
the main reason for this is that most types and classes in vectorz are primitives and serializables while apache lang extensively uses strings and collections.
apache math also extensively uses primitives.
the worst results are to jsyn the reason for this seems to be that the parameters to the synthesizers are objects instead of numbers as we initially expected.
the results vary with the quality of the test suite of the original project.
in all the apache projects almost all loops that satisfy the precondition nally turn into a microbenchmark while only half of the loops of vectorz andjsyn that can be processed by autojmh are covered by one test case at least.
consequently many payloads cannot be initialized by autojmh because it cannot perform the dynamic analysis that would provide valid initializations.
outer for int i i cslen i final char ch cs.
charat i bench this for int j j searchlen j i f searchchars ch i f i cslast j searchlast character .
ishighsurrogate ch i f searchchars cs.
charat i continue outer else continue outer listing the sua depends on outer code to work properly table also shows that some microbenchmarks fail regression tests.
a good example is the inner loop of listing extracted from apache common lang .
this loop depends on the chvariable obtained in its outer loop.
in this case autojmh generates a payload that compiles and can run but that does not integrate the outer loop.
so the payload s behavior is di erent from the sua and the regression tests fails.it is worth mentioning that while autojmh failed to generate the inner loop it did generate a microbenchmark for the outer one.
answer to rq1 autojmh was able to generate microbenchmarks out of loops found in realword java programs and only of the analyzed loops did not satisfy the tool s preconditions.
.
rq2 microbenchmarks generated by autojmh vs handwritten by experts to answer rq2 we automatically re generate mircrobenchmarks that were manually designed by expert performance engineers.
we assess the quality of the automatically generated microbenchmarks by checking that the times they measure are similar to the times measured by the handwritten microbenchmarks.
.
.
microbenchmarks dataset we re generate jmh microbenchmarks that were used to nd documented performance regression bugs in projects by oracle6and sowatec ag .
we selected microbenchmarks from oracle since this company is in charge of the development of hotspot and jmh.
the agship product of sowatec ag arregulo7 has reported great performance results using microbenchmarks.
the microbenchmarks in our dataset contained several elements of java such as conditionals loops method calls elds.
they where aimed at variety of purposes and met the autojmh preconditions.
follows a small description of each one of the microbenchmarks mb in our dataset mb and measure the di erences between the two methods arraylist.add andarraylist.addall when adding multiple elements.
mb to compare di erent strategies of creating objects using re ection using as baseline the operator new.
mb measure the time to retrieve elds using re ection.
mb to compare strategies to retrieve data from maps when the key is required to be a lower case string.
bugs ids and 138mb and compare the concurrenthashmap.get method vs.thenonblockinghashmaplong.get method.
mb to see whether biginteger.value can be constant folded when given as input a number literal.
mb and contrasts the performance of math.max given two numbers vs.a greater than a b comparison.
mb evaluate the performance of the matcher.reset method.
mb to evaluate the performance of the string.format method using several types of input double long string .
.
.
statistical tests we use the statistical methodology for performance evaluation introduced by george to determine the similarity between the times measured by the automatically generated microbenchmarks and the handwritten ones.
this consists in nding the con dence interval for the series of execution times of both programs and to check whether they overlap in which case there is no statistical reason to say they are different.
we run the experiment following the recommended methodology considering virtual machine invocations of which run for microbenchmarks and warm up iterations to reach steady state.
we select a con dence level of .
to determine whether autojmh actually de es the pitfalls shown in section we also generate three other sets of microbenchmarks.
each set of microbenchmark is prone to the following pitfall dce cf cp and wrong initial values.
dce was provoked by turning o sink maximization .
cf cp was provoked by inverting the rules of variable declaration where constants static nal elds are declared as regular elds and initialized from le elds are redeclared as constants static nal eld and initialized using literals zero .14f local variables are always declared as local variables and initialized using literals.
in the third set we feed random data as input to observe di erences in measurements caused by using di erent data.
using these di erent sets of microbenchmarks we performed the pairwise comparison again between them and the handwritten microbenchmarks.
table comparison of generated vs handwritten benchmarks set successful tests 1generated with autojmh dce cf cp bad initialization table shows the results of this experiment.
column successful tests shows for how many of the automatically generated microbenchmarks measured the same times as the ones written by experts.
row shows the set generated with all features of autojmh .
rows and the ones generated with induced errors.
.
.
analysis of the results the key result of this set of experiments is that all the microbenchmarks that we re generated using all the features ofautojmh measure times that are statistically similar to those measured by the ones handwritten by experts while microbenchmarks with induced errors consistently drift away from this baseline.
for us this is an strong indication that autojmh actually de es the pitfalls of section .row of table shows the strong impact of dce on the accuracy of microbenchmarks of microbenchmarks that we generate without sink maximization measure signi cantly di erent times from the times of handwritten microbenchmarks.
the inverted rules for cf cp take a toll on microbenchmarks for example the result of a comparison between two constants is also a constant mb and therefore there is no need to perform the comparison.
eleven microbenchmarks generated with wrong variable declarations still measure similar times because some sua cannot be constant folded e.g.
the map.get method in in mb .
finally line shows that passing wrong initial values produces di erent results since adding elements to a list takes less time than adding mb or converting pi to string is certainly slower than an integer such as mb to .
the three cases that measured correct times occur when the elds initialized in the payload are not used as is the case in mb .
the code for all the microbenchmarks used in this experiment as well as the program and the unit test used to rebuild them can be found in the website of autojmh8.
answer to rq2 microbenchmarks automatically generated by autojmh systematically perform as good as benchmarks built by a jmh experts with a con dence level of .
.
the code generated to prevent dce cf cp and initialize the payload plays a signi cant role in the quality of the generated microbenchmarks.
.
rq3 autojmh vs engineers without microbenchmarking experience for this research question we consider code segments all contained in a single class and we ask professional java developers with little experience in performance evaluation to build a microbenchmark for each segment.
this simulates the case of software engineers looking to evaluate the performance of their code without speci c experience in time measurement.
this is a realistic scenario as many engineers arrive to microbenchmarking due to an eventual need gathering the knowledge they require by themselves using available resources as internet tutorials and conferences.
we provided all participants a short tutorial about jmh.
all participants had full access to internet during the experiment and we individually answered all questions relative to better microbenchmarking.
participants were also reminded that code segments may have multiple performance behaviors and that otherwise noticed they should microbenchmark all behaviors they could nd.
.
.
segments under analysis each of the code segments is meant to test one di erent feature of autojmh .
sua in listing participants were requested to evaluate the execution time of the forloop.
here we evaluate a segment which execution time depends on the different input s types.
the parameter cofaddfunction is of type myfunction which is inherited by two subclasses both overriding the calc method.
the calculations performed by both subclass are di erent which required several microbenchmarks to evaluate all possibilities.
139sua and in listing participants were requested to evaluate the time it takes to add one element into an array list and the time it takes to sort a list of elements.
here we wanted to test the participant s ability at using di erent reset strategies to force the microbenchmark reach stable state measuring the desired case.
the payload for sua must constrain the list size otherwise the jvm runs out of memory.
for sua it is necessary to reset the list into an unordered state.
sua and in listing participants were requested to estimate how long takes the expression to execute.
the segments consist of simple mathematical expressions meant to investigate if participants are able to avoid dce and constant folding when transplanting a sua into a payload.
all microbenchmarks used in this experiment are publicly available in the github repository of autojmh .
.
resulting microbenchmarks figure shows the execution times measured by all microbenchmarks.
the y axis shows execution times in milliseconds log scale .
on the x axis we show clusters mb1a and mb1b for the two performance behaviors of sua and mb2 to mb5 for all other segments.
each cluster includes the time measured by the microbenchmarks designed by the java developers.
in each cluster we add two microbenchmarks one generated by autojmh and one designed manually by us and that has been reviewed by the main developer of jmh.
the latter microbenchmark for short the expert is used as the baseline for comparison.
we use the similiraty of execution times for comparison the closest to the baseline the better.
first we observe that the times for the autojmh and the baseline microbenchmarks are consistently very close to each other.
the main di erences we can see are located in suas and .
this is because autojmh uses a generic reset strategy consisting in clearing the list and adding the values which is robust and performs well in most cases.
however the expert microbenchmarks and the one made by engineer for sua featured speci c reset strategies with less overhead.
the best strategy to reset in sua is to reset only after several calls to the addmethod have been made distributing the reset overhead and reducing the estimation error.
in the expert benchmark for sua each element is set to a constant value.
a clever trick was used by engineer in sua the sort method was called twice with two different comparison functions with equivalent performance changing the correct order in every call.
this removes the need to reset the list since every consecutive call to sort is considered unordered.
second we observe that java developers build microbenchmarks that measure times that are very di erent from the baseline.
in order to understand the root cause of these differences we manually review all the microbenchmark.
here we observe that the participants did encounter the pitfalls we expected for each kind of segment participants fail to distinguish performance behaviors in mb1 participants made mistakes when initializing mb2 and mb3 we found multiple issues in mb4 and mb5 where engineers did not realize that their microbenchmark was optimized by blob master eng6 src main java fr inria diverse autojmh validation eng6 transientstatelistsorteng6.java figure execution times comparison between microbenchmarks generated by autojmh those manually built by java developers and one jmh expert dce engineer allowed parts of its microbenchmark to be constant folded and participants bloated to some extend their microbenchmark with overhead.
an interesting fact was that engineer was aware of constant folding since he asked about it meaning that a trained eye is needed to detect optimizations even when one knows about them.
answer to rq3 microbenchmarks generated by autojmh prevent mistakes commonly made by java developers without experience in microbenchmarking.
.
threats to validity the rst threat is related to the generalizablity of observations.
our qualitative evaluation was performed only with segments and participants.
yet segments were designed to be as di erent as possible and to cover di erent kinds of potential pitfalls.
the quantitative experiment also allowed us to test autojmh on a realistic code base representative of a large number of situations that can be encountered in java applications.
autojmh is a complex tool chain which combines code instrumentation static and dynamic analysis and code generation.
we did extensive testing of our the whole infrastructure and used it to generate a large number of microbenchmarks for a signi cant number of di erent applications.
however as for any large scale experimental infrastructure there are surely bugs in this software.
we hope that they only change marginal quantitative things and not the qualitative essence of our ndings.
our infrastructure is publicly available on github.
.
related work we are not aware of any other tool that automatically generates the payload of a microbenchmark.
however there are works related to many aspect of autojmh .
performance analysis.
the proper evaluation of performance is the subject of a large number of papers .
they all point out non determinism as the main barrier to obtain repeatable measurements.
sources of non determinism arise in the 140addfunction myfunction c i f c null c new funa sua for int i i i sinsum c. calc i listing sua .
di erents inputs in c de ne performanceappendsorted arraylist integer a int value sua a.add value sua a. sort new comparator integer compare integer o1 integer o2 return o1 o2 listing segments and sua angle math .abs math .
sin y pi sua double c x y listing suas and data the code the compiler the virtual machine the operating system and even in the hardware .
various tools and techniques aim at minimizing the e ect of non determinism at each level of abstraction .
jmh stands at the frontier between code and the jvm by carefully studying how code triggers jvm optimizations .
autojmh is at the top of the stack automatically generating code for the jmh payload avoiding unwanted optimizations that may skew the measurements.
microbenchmarking determines with high precision the execution time of a single point.
this is complementary to other techniques that use pro ling and trace analysis that cover larger portions of the program at the cost of reducing the measurement precision.
symbolic execution is also used to analyze performance however symbolic execution alone cannot provide execution times.
finally several existing tools are speci c for one type of bug or even for one given class of software like the one by zhang which generates load test for sql servers.
autojmh is a tool that sits between pro ling trace analysis and microbenchmarking providing execution times for many individuals points of the program with high precision.
performance testing in isolation.
specially close to our work are the approaches of hork y kuperberg and pradel .
microbenchmarking and therefore autojmh evaluate performance by executing one segment of code in isolation.
a simpler alternative favored by industry are performance unit tests which consist in measuring the time a unit test takes to run.
hork y et.al.
proposes methodologies and tools to improve the measurements that can be obtained using performance unit tests uses unlike autojmh which uses unit tests only to collect initialization data.
kuperberg creates microbenchmarks for java apis using the compiled bytecode.
finally pradel proposes a test generator tailored for classes with high level of concurrency while autojmh uses the jmh built in support for concurrency.
all these approaches warm up the code and recognize the intrinsic non determinism of the executions.
the main distinctive feature of autojmh over these similar approaches is its unique capability to measure at the statement level .
these other approaches generate test execution for whole methods at once.
baudry shows that some methods use code living as far as levels deep in the call stack which gives us an idea of how coarse can be executing a whole test method.
autojmh is able to measure both complete methods and statements as atomic as a single assignment.
during the warm up phase the generated jhm payload wrapper method gets in lined and therefore the microbenchmark loop do actually execute statements.
another important distinction if that autojmh uses data extractedfrom an expected usage of the code i.e.
the unit tests .
pradel uses randomly generated synthetic data which may produce unrealistic performance cases.
for example jit in lining is a very common optimization that improves performance in the usual case while reducing it in less usual cases.
the performance improvement of this well known optimization is hard to detect assuming that all inputs have the same probability of occurrence.
program slicing.
autojmh creates a compilable slice of a program which can be executed stays in stable state and is not a ected unwanted optimizations.
program slicing is a well established eld .
however to the best of our knowledge no other tool creates compilable slices with the speci c purpose of microbenchmarking.
.
conclusion and future work in this paper we propose a combination of static and dynamic analysis along with code generation to automatically build jmh microbenchmarks.
we present a set of code generation strategies to prevent runtime optimizations on the payload and instrumentation to record relevant input values for the sua.
the main goal of this work is to support java developers who want to develop microbenchmarks.
our experiments show that autojmh does generate microbenchmarks as accurate as those handwritten by performance engineers and better than the ones built by professional java developers without experience in performance assessment.
we also show that autojmh is able to analyze and extract thousands of loops present mature java applications in order to generate correct microbenchmarks.
even when have addressed the most common pitfalls found in the current microbenchmarks today we are far from being able to handle all possible optimizations and situations detrimental for microbenchmark design therefore our future work will consist in further improve autojmh to address these situations.
.