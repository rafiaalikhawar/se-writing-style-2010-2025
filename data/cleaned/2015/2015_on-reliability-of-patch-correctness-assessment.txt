on reliability of patch correctness assessment xuan bach d. le1 lingfeng bao2 david lo3 xin xia4 shanping li5 and corina pasareanu1 1carnegie mellon university usa bach.le corina.pasareanu west.cmu.edu 2zhejiang university city college china lingfengbao zju.edu.cn 3singapore management university singapore davidlo smu.edu.sg 4monash university australia xin.xia monash.edu 5zhejiang university china shan zju.edu.cn 6nasa ames research center usa corina.s.pasareanu nasa.gov abstract current state of the art automatic software repair asr techniques rely heavily on incomplete specifications or test suites to generate repairs.
this however may cause asr tools to generate repairs that are incorrect and hard to generalize.
to assess patch correctness researchers have been following two methods separately automated annotation wherein patches are automatically labeled by an independent test suite its a patch passing the its is regarded as correct or generalizable and incorrect otherwise author annotation wherein authors of asr techniques manually annotate the correctness labels of patches generated by their and competing tools.
while automated annotation cannot ascertain that a patch is actually correct author annotation is prone to subjectivity.
this concern has caused an on going debate on the appropriate ways to assess the effectiveness of numerous asr techniques proposed recently.
in this work we propose to assess reliability of author and automated annotations on patch correctness assessment.
we do this by first constructing a gold set of correctness labels for randomly selected patches generated by state of the art asr techniques through a user study involving professional developers as independent annotators.
by measuring inter rater agreement as a proxy for annotation quality as commonly done in the literature we demonstrate that our constructed gold set is on par with other high quality gold sets.
we then compare labels generated by author and automated annotations with this gold set to assess reliability of the patch assessment methodologies.
we subsequently report several findings and highlight implications for future studies.
i. i ntroduction bug fixing is notoriously difficult time consuming and costly .
hence effective automatic software repair asr techniques that can help reduce the onerous burden of this task would be of tremendous value.
interest in asr has intensified in recent years as demonstrated by substantial work devoted to the area bringing the futuristic idea of asr closer to reality.
asr can be divided into two main families heuristics vs. semantics based approaches based on the way they generate and traverse the search space for repairs.
ideally complete specifications should be used for assessing correctness of patches generated by asr.
it is however very hard to obtain complete specifications in practice.
asr techniques thus typically resort to using test cases as the primary criteria for correctness judgment of machine generated patches a patch is considered correct if it passes all the tests used for repair .
this assessment methodology however has beenshown to be ineffective.
there could be multiple patches that pass all the tests but are still incorrect causing the so called patch overfitting .
this happens because the search space is often very large and contains many plausible repairs which unduly pass all tests but fail to generalize.
this thus motivates the need of new methodologies to assess patch correctness.
the new methodologies need to rely on additional criteria instead of using the test suite used for generating repair candidates aka.
repair test suite alone.
to address this concern recent works have been following two methods for patch correctness assessment separately automated annotation by independent test suite.
independent test suites obtained via an automatic test case generation tool are used to determine correctness label of a patch see for example .
following this method a patch is deemed as correct orgeneralizable if it passes both the repair and independent test suites and incorrect otherwise.
author annotation.
authors of asr techniques manually check correctness labels of patches generated by their own and competing tools see for example .
following this method a patch is deemed as correct if authors perceive a semantic equivalence between the generated patches and the original developer patches.
while the former is incomplete in the sense that it fails to prove that a patch is actually correct the latter is prone to author bias.
in fact these inherent disadvantages of the methods have caused an on going debate in the program repair community as to which method is better for assessing the effectiveness of various asr techniques being proposed recently.
unfortunately there has been no extensive study that objectively assesses the two patch validation methods and provides insights into how the evaluation of asr s effectiveness should be conducted in the future.
in this work we conduct a study that addresses this gap in research.
we start by creating a gold set of correctness labels for a collection of asr generated patches and subsequently use it to assess reliability of labels created through author and automated annotations.
we study a total of patches generated by popular asr techniques acs kali genprog nopol s3 angelix and enumera5242019 ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
tive and cvc4 embedded in jfix .
these patches are for buggy versions of real world projects of which six projects are from defects4j math lang chart closure mockito and time and seven projects are from s3 s dataset jflex fyodor natty molgenis rtree simpleflatmapper graphhoper .
to determine correctness of each patch we follow best practice by involving multiple independent annotators in a user study.
our user study involves professional developers each asr generated patch is labeled by five developers by comparing the patch with its corresponding ground truth patch created by the original developer s who fixed the bug.
we then analyze the reliability of created gold set and compare it with labels generated by three groups of asr tool authors and two automatic test case generation tools such as d ifftg en that has been used in prior study and randoop that we use in this study.
we answer three research questions rq1 can independent annotators agree on patch correctness?
rq2 how reliable are patch correctness labels generated by author annotation?
rq3 how reliable are patch correctness labels inferred through automatically generated independent test suite?
in rq1 by measuring inter rater agreement as a proxy of annotation quality as commonly done in the literature we demonstrate that our gold set has substantial interrater scores and thus is on par with other high quality gold sets.
in the subsequent two rqs we investigate the strengths and deficiencies of author and automated patch correctness annotation.
we summarize our contributions below we are the first to investigate the reliability of author and automated annotation for assessing patch correctness.
to perform such assessment we have created a gold set of labelled patches created by a user study involving professional developers.
by means of this gold set we highlight strengths and deficiencies in popular assessment methods employed by existing asr studies.
based on the implications of our findings we provide several recommendations for future asr studies to better deal with patch correctness validation.
specifically we find that automated annotation despite being less effective as compared to author annotation can be used to augment author annotation and reduce the cost of manual patch correctness assessment.
the rest of the paper is organized as follows.
section ii describes background for this work.
section iii describes how we collect gold set of patch correctness labels.
we answer rqs to assess the quality of our gold set author annotation and automated annotation in section iv v and vi respectively.
section vii discusses our findings post study survey threats to validity and future extensions.
section ix concludes.ii.
b ackground in this section we describe automated software repair asr techniques used in our experiments.
we subsequently describe popular patch validation methods used in asr research.
finally we discuss best practices in building gold sets.
asr techniques genprog is one of the first techniques that sparked interests in asr.
given a buggy program and a set of test cases at least one of which is failing genprog uses a number of mutation operators such as statement delete insert and append to create a large pool of repair candidates.
it then uses genetic programming to apply the mutations and evolve the buggy program until a candidate passing all the tests is found.
kali is a naive asr technique which just blindly deletes any statements that are identified as potentially buggy.
despite being very simple kali has been shown to be as effective and efficient as genprog.
nopol is a recently developed asr technique that focuses on only repairing defective if conditions .
nopol attempts to synthesize an if condition expression that renders all the tests to pass by using program synthesis.
in a similar vein acs also focuses on synthesizing repairs for buggy if conditions.
like nopol acs also uses program synthesis to create repairs.
unlike nopol acs attempts to rank the repair candidates using various ranking functions.
angelix s3 and jfix use symbolic execution and constraint solving to infer specifications and various program synthesis techniques to synthesize repairs conforming to the inferred specifications.
angelix uses component based synthesis while s3 and jfix use syntax guided synthesis .
evaluation of asr generated patches initially in asr research test cases were used as the sole criteria for judging correctness of machine generated patches.
by relying on the assumption that a patch that passes the repair test suite is regarded as correct early repair techniques such as genprog ae and rsrepair reported to produce many such correct patches.
however it has been shown in recent studies that this assumption does not hold true in practice since such patches that pass the repair test suite are actually still incorrect .
this shows that using a repair test suite alone is a weak proxy for assessing patch correctness.
motivated by the above serious concern researchers have employed new methods to assess patch correctness author annotation in which authors of repair techniques manually check the correctness of patches generated by their and competing tools by themselves see for example automated annotation by independent test suite its generated by automatic test case generation tool see for example .
both methods assume that a reference correct implementation of the buggy program which is used as a basis for comparison is available.
since most asr techniques try to fix buggy versions of real programs the reference implementations can be found in the version control systems of the corresponding projects.
early work that uses annotation by automatically generated authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
its e.g.
uses general purpose automatic test generation tools such as klee to generate an its that maximizes the coverage of the reference implementation written in the c programming language.
test cases generated on the reference correct implementation are then used to assess correctness of machine generated patches i.e.
a machine generated patch is regarded as incorrect if there exists a test case exposing behavioral differences in correct and machine patched code.
recently xin et al.
proposed d ifftg en a test generation tool for java programs specifically designed to generate tests that can identify incorrect patches generated by asr tools .
d ifftg en attempts to generate test cases that cover the syntactic and semantic differences between the machine patched and human patched programs.
if there are any such test cases that expose the differences in outputs of the programs the machine generated patch is deemed as incorrect since it results in a different output as compared to the corresponding ground truth human patched program.
difftg en has been shown to be able to identify incorrect patches produced by various state of the art asr tools such as genprog kali nopol and hdrepair .
best practices in building gold sets to build gold sets objectively a common approach is to employ many independent annotators and measure inter rater agreement as proxy for annotation quality .
the information retrieval ir community especially through the text retrieval conference trec has employed many annotators through a large scale collaborative effort to annotate many document corpora for various retrieval tasks.
many past software engineering studies have also involved independent annotators to construct gold sets.
based on the nature of various tasks annotators include non authors who could be undergraduate graduate students or professional developers .
iii.
u ser study we conducted a user study with professional developers to collect correctness labels of patches.
in this study every developer is required to complete several tasks by judging whether patches generated by asr tools are semantically equivalent to ground truth human patches.
patch dataset.
since the eventual goal of our study is to assess the reliability of author and automated annotations we need a set of patches that have been labeled before by asr tool authors and can be used as input to automated test case generation tools designed for program repair.
we find the sets of patches recently released by xiong et al.
martinez et al.
and le et al.
to be suitable.
xiong et al.
and martinez et al.
labelled a set of patches generated by asr tools designed by their research groups i.e.
acs and nopol and their competitors i.e.
genprog kali .
le et al.
labelled a set of patches generated by their asr tool i.e.
s3 and its competitors i.e.
angelix and enumerative and cvc4 embedded in jfix .
the authors i selected pa tches and their author labels genprog kali nopol acs s3 angelix enum cvc4 incorrect correct unknown total labelled these patches by manually comparing them with ground truth patches obtained from version control systems of the corresponding buggy subject programs.
these patches can be used as input to d ifftg en which is a state of theart test generation tool specifically designed to evaluate patch correctness and r andoop a popular general purpose test case generation tool .
due to resource constraints only professional developers agreed to spend an hour of their time in this user study we cut down the dataset to patches by randomly selecting these patches from the original datasets.
details of the dataset of patches are shown in table i. task design.
at the start of the experiment every participant was required to read a tutorial that briefly explains automated program repair and what they need to do to complete the tasks.
afterwards they can complete the tasks one by one through a web interface.
figure shows a sample task that we give to our user study participants via our web interface.
for each task we provide a ground truth patch taken from the version control system of the corresponding buggy subject program along with a patch that is generated by an automated program repair tool.
we also provide additional resources including full source code files that are repaired by the patch link to the g ithub repository of the project outputs when executing failing test cases and source code of the failing test cases.
based on this information participants are asked to evaluate the correctness of the patch by answering the question is the generated patch semantically equivalent to the correct patch?
to answer this question participants can choose one of the following options yes no or i don t know .
finally if they wish to they can provide some reasons that explain their decision.
our web interface will record participants answers and the amount of time they need to complete each task.
participants and task assignment.
to recruit participants we sent emails to our industrial contacts about this user study.
our contacts then advertised the study and provided us emails of developers who are willing to participate.
thirty three of the professional developers participating in this study work for two large software development companies named company c1 and c2 while another two work as engineers for an educational institution.
company c1 currently has more than employees and company c2 has more than employees.
both companies have a large number of active projects that expose developers to various business knowledge and software engineering techniques.
all the developers work for projects that use java as the main programming language.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
the average number of years of work experience that these participants have is .
.
the two developers from the educational institution are senior and have worked for .
and years respectively.
the most experienced developer from industry has worked for seven years while some has only worked for one year.
participants are classified into two groups junior and senior according to their years of experience following the company s internal classification.
companies that our participants work for consider developers with less than years of experience as juniors and those with more than years of experience as seniors.
there are junior developers and senior developers.
we divided the participants into seven groups.
the ratio ofjunior and senior developers for each group was kept approximately the same.
each patch generated by program repair tools is labeled by five participants.
participants in the same group receive the same set of patches to label.
correct patch abstract...render.java3generated patch abstract...render.java4 source org ... abstract...render.java source org ... abstract...render.java if dataset null if dataset !
null return result if dataset null return result project failing test case output other infor failing test source jfreechart5root cause in triggering tests org.jfree.chart.renderer...tests test2947660 junit.framework.assertionfailederror expected but was ....... abstract...test.java is generated patch semantically equivalent to the correct patch ?
yes no i don t know if possible provide reason here ... next8 fig.
.
a sample task on our web interface.
and show developer and machine generated patches and show links to patched source files shows github repository and show output of failed test cases and their source files is the question we asked a participant.
iv .
a ssessing independent annota tors l abels the user study presented in section iii was conducted to build a set of gold standard labels for machine generated patches which can reliably be used to assess reliability of author and automated annotations.
before using the labels produced by our user study we need to first ascertain their quality.
agreement among annotators is often used as a measure of quality .
thus in this section we investigate the degree to which the annotators agree with one another.
this answers rq1 can independent annotators agree on patch correctness?
methodology.
to answer rq1 we first compute some simple statistics highlighting the number of agreements and disagreements among annotators.
we then calculate several wellaccepted measures of inter rater reliability.
finally we perform some sanity checks to substantiate whether or not annotators are arbitrary in making their decisions.table ii results of participant annota tions all agree all agree unk majority agree incorrect correct total results.
to recap our annotators are professional developers who are tasked to annotate machine generated patches.
each patch is annotated by five professional developers each provides either one of the following labels incorrect correct or unknown.
table ii summarizes the number of agreements and disagreements among annotators.
in the first column all agree the number of patches in which all developers agree on each patch s label is .
of all patches of which patches are labeled as incorrect and patches are labeled as correct.
in the second column all agree unk ignoring unknown labels the number of patches for which the remaining annotators fully agree on their labels is .
of all patches .
out of these the numbers of patches that are labeled as incorrect and correct are and respectively.
in the last column majority agree for out of patches .
of all patches there is a majority decision i.e.
most annotators agree on one label .
out of these and patches are identified as incorrect and correct respectively.
we also compute several inter rater reliability scores mean pairwise cohen s kappa and krippendorff s alpha .
using the earlier test we consider three different ratings i.e.
correct incorrect and unknown while the latter test which allows different number of ratings for each data point enables us to ignore unknown ratings.
inter rater reliability scores measure how much homogeneity or consensus there is between raters labelers.
the importance of rater reliability hinges on the fact that it represents the extent to which the data collected in the study are correct representations of the variables being measured.
a low inter rater reliability suggests that either the rating scale used in the study is defective or raters need to be retrained for the rating task or the task is highly subjective.
the higher the inter rater reliability the more reliable the data is.
reliability score values by landis and koch suggest that moderate substantial and almost perfect agreements are associated with values in ranges and respectively.
scores below .
indicate fair slight or poor agreements.
it is worth noting that there is another interpretation of kappa value by manning et al.
which indicates that a kappa value falling between .
and .
demonstrates a fair agreement between raters the second highest level of agreement by their interpretation.
it has been shown that this fair level of inter rater agreement normally happens in popular datasets such as those used for evaluations on text retrieval conference trec which is championed by us national institute of standards and technology nist since and provides benchmark datasets for various text retrieval tasks see and medical information retrieval collections .
based on this interpretation authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
300completion time seconds confirmed unknown fig.
.
time taken by annotators to decide whether a patch s label is either known confirmed as correct or incorrect or unknown.
we have the following findings on the gold set annotated by independent developers the computed mean pairwise cohen s kappa and krippendorff s alpha for our independent annotators labels are .
and .
respectively.
these scores indicate a substantial agreement among participants which satisfies the standard normally met by quality benchmark datasets.
we further perform two sanity checks to substantiate whether or not annotators are arbitrary in their decisions.
first we expect conscientious annotators to spend more time inspecting patches that are eventually labeled as unknown than other patches.
annotators who label patches as unknown without thinking much would be likely making arbitrary decisions.
figure depicts a box plot showing the time participants took on patches that are labeled as known correct or incorrect or unknown.
it can be seen that participants took more time on the later set of patches.
wilcoxon signed rank test returns a p value that is less than .
indicating a statistically significant difference.
moreover the cliff s delta which is a non parametric effect size measure is .
medium .
second we expect conscientious annotators to spend more time inspecting difficult patches than easy ones.
we consider disagreement among annotators as a proxy for patch difficulty.
we compare the time taken by participants in identifying patches for which there is complete agreement to those for which disagreement exists.
figure shows a box plot which shows that participants spend more time on disagreement cases.
wilcoxon signed rank test returns a p value that is less than .
indicating statistically significant difference.
moreover the cliff s delta is .
small .
the above results substantiate the quality of our dataset.
in the subsequent sections which answer rq2 and rq3 we use two versions of our dataset all agree see all agree column in table ii and majority agree see majority agree column in table ii to assess the reliability of author and automated annotations.
v. a ssessing author annota tion a number of studies proposing automated repair approaches evaluate them through manual annotation performed by au0 200completion time seconds agreement with disagreement fig.
.
time taken by annotators to decide a patch s label for full agreement and disagreement cases.
table iii independent indep a nnota tor vs .a uthor labels indep annotators authors all agree majority agree sameincorrect incorrect correct correct differentincorrect correct correct incorrect incorrect unknown correct unknown total thors e.g .
author subjectivity may cause bias which can be a threat to the internal validity of the study.
author bias has been actively discussed especially in the medical domain e.g.
.
unfortunately so far there has been no study that investigates presence or absence of bias in author annotation and its impact to the validity of the labels in automated repair.
this section describes our effort to fill this need by answering rq2 how reliable is author annotation?
methodology.
recall that our user study makes use of patches released by three research groups including xiong et al.
martinez et al.
and le et al.
who created program repair tools namely acs nopol and s3 respectively.
authors of each tool manually labeled the patches generated by their tool and competing approaches by themselves.
to answer rq2 we compare labels produced by the three research groups with those produced by our independent annotators whose quality we have validated in section iv.
we consider the all agree and majority agree datasets mentioned in section iv.
results.
table iii shows the detailed results on the comparisons between independent annotators and authors labels.
we found that for all agree dataset authors labels match with independent annotators labels same for out of patches .
.
there are patches for which authors labels mismatch those by independent annotators different .
among these patches are identified by independent annotators as incorrect but identified by authors as correct incorrectcorrect .
for the other patches authors labels are unknown while independent annotators labels are incorrect incorrectunknown .
for the majority agree dataset .
of the labels match.
there are mismatches belong to incorrect correct cases to correct incorrect cases and to incorrect unknown cases.
figure shows an example authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
public class stopwatch 2public void stop if this .runningstate !
state running this .
runningstate !
state suspended throw new illegalstateexception ... if this .runningstate state running developer patch if stoptime generated patch stoptime system.currenttimemillis this .runningstate state stopped fig.
.
an example of a patch that has mismatched labels.
xiong et al.
identified the patch shown at line as correct while independent annotators identified this patch as incorrect.
the ground truth developer patch is shown at line .
patch generated by nopol that has mismatched labels.
it is labeled as correct by martinez et al.
and incorrect by independent annotators.
we also compute inter rater reliability of authors labels and labels in all agree and majority agree datasets.
the cohen s kappa values are .
and .
considering the all agree and majority agree datasets respectively.
the krippendorf s alpha values are .
and .
.
comparing these scores with landis and koch s interpretation described in section iv there is substantial agreement.
a majority .
.
of patch correctness labels produced by author annotation match those produced by independent annotators.
inter rater reliability scores indicate a substantial agreement between author and independent annotator labels.
to characterize cases where author and independent annotator labels match same and those where they do not match different we investigate the time that participants of our user study took to label the two sets of patches.
since the number of mismatches is smaller in the all agree dataset we focus on comparing labels in majority agree dataset.
figure depicts a box plot showing the distribution of completion time corresponding to the two sets of patches.
the figure shows that patches with matching labels took participants a shorter period of time to label comparing to those whose labels mismatched.
wilcoxon signed rank test returns a pvalue that is less than .
indicating statistically significant difference.
the cliff s delta is equal to .
small .
since task completion time can be used as a proxy for measuring task difficulty or lack thereof we consider participants completion time as a proxy of difficulty in assessing patch correctness.
the result suggests that disagreements between authors and independent annotators happen for difficult cases.
vi.
a ssessing automa ted annota tion we also investigate the reliability of the use of automatically generated independent test suite its in annotating patch labels.
its has been used as an objective proxy to measure patch correctness a patch is deemed as incorrect if it does not pass the its and as correct or generalizable otherwise .
it is unequivocal that incorrect patches determined by its 200completion time seconds same different fig.
.
participant completion time for patches for which author and independent annotator labels match same and mismatch different are indeed incorrect.
however it is unclear if its can detect a large proportion of incorrect patches.
moreover the extent to whether correct generalizable patches determined by its are indeed correct remains questionable.
thus to assess the usefulness of its we investigate the answer to rq3 how reliable is automatically generated its in determining patch correctness?
methodology we employ the recently proposed test case generation tool d ifftg en by xin et al.
and r andoop to generate its.
to generate its using d ifftg en and r andoop the human patched program is used as ground truth.
for d ifftg en we run using its best configuration reported in allowing it to invoke e vosuite in trials with the search time of each trial limited to seconds.
a machine generated patch is identified as incorrect if there is a test in the d ifftg en generated its that witnesses the output differences between the machine and human patches.
for r andoop we run it on the ground truth program with different seeds with each run limited to minutes.
a machinegenerated patch is identified as incorrect if there is at least one test case in the r andoop generated its that exhibits different test results in machine patched and human patched ground truth programs e.g.
it fails on the machine patched program but passes on the ground truth program or vice versa.
by this way we allow both tools to generate multiple test suites.
it is however worth noting that d ifftg enand r andoop are incomplete in the sense that they do not guarantee to always generate the test cases that witness incorrect patches.
we use test cases generated by the tools to automatically annotate the patches and compare the generated labels to those in all agree and majority agree datasets which are created by our user study.
results out of the patches in our study d ifftg en generates test cases that witness incorrect overfitting patches.
details of these patches are shown in table v. the all agree ground truth identifies of these patches as incorrect the other patches lie outside of the all agree dataset while the majority agree dataset identifies all of them as incorrect.
unfortunately most of the patches labelled as incorrect in all agree patches and majority agree patches datasets failed to be detected as such by its generated by d ifftg en.r andoop performs similarly as compared to d ifftg en.
it identifies authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iv kappa and alpha v alues when using difftg en r andoop and their combina tion to label pa tches all agree majority agree difftr and comb difftr and comb cohen s kappa .
.
.
.
.
.
kripp s alpha .
.
.
.
.
.
patches as incorrect all of which are also identified as incorrect in the majority agree dataset.
note that difftg enand r andoop when combined can identify totally unique patches as incorrect.
for each of the total patches d ifftg en and r andoop generated from to unit test cases per method.
there are a few patches that the tools cannot generate test cases for.
in their studies smith et al.
and le et al.
assume a patch is incorrect if it does not pass an its and correct or generalizable otherwise.
using the same assumption to generate correctness labels we can compute inter rater reliability between labels automatically annotated by running its generated by d ifftg enand r andoop and labels in all agree and majority agree datasets.
as readers may have expected the cohen s kappa values are very low as shown in table iv e.g.
kappa values when using d ifftg en generated its for all agree and majority agree are .
and .
respectively.
the corresponding krippendorff s alpha values are .
and .
.
we now compare author labels discussed in section v with its labels.
table v shows the author labels of the and patches identified as incorrect by d ifftg enand r andoop respectively.
for these patches the majority of the labels by authors and d ifftg en match.
however interestingly there are four special patches in which labels generated by automated and author annotations are mismatched.
these cases are highlighted in gray in table v. particularly three patches are identified as incorrect by d ifftg en including math generated by kali chart generated by genprog and math generated by nopol while author labels are unknown .
one patch identified as incorrect by r andoop math generated by genprog is labelled as correct by authors.
based on results above we conclude independent test suites generated by d ifftg en and r andoop can only label fewer than a fifth of incorrect patches as such in all agree and majority agree datasets.
however generated test suites can be used as a complement for author annotation to increase accuracy.
finally we want to investigate the difficulty of judging correctness of patches that are labelled as incorrect by itss generated by d ifftg enand r andoop .
to do so we compare participant completion time for the set of unique patches and another set containing the other patches.
we find that they are more or less the same.
wilcoxon signed rank test confirms that the difference is not statistically significant.
thus patches that its successfully labels as incorrect aretable v labels by independent annota tors a nnot column and authors a uthors column of pa tches identified by independent test suite its genera ted by difftgen orrandoop as incorrect .
difftg en randoop annot authors kalitime incorrect incorrect incorrect incorrect math incorrect incorrect incorrect math incorrect incorrect incorrect math incorrect incorrectunknownmath incorrect incorrect incorrect incorrect math incorrect incorrect incorrect chart incorrect incorrect incorrect chart incorrect incorrect incorrect chart incorrect incorrect incorrect incorrect chart incorrect incorrect incorrect incorrect genprogmath incorrect incorrect incorrect math incorrect incorrect incorrect math incorrect incorrect incorrect math incorrect incorrect incorrect math incorrect incorrect incorrect incorrect math incorrect incorrect incorrect math incorrect incorrectcorrectchart incorrect incorrect incorrect chart incorrect incorrectunknownchart incorrect incorrect incorrect incorrect chart incorrect incorrect incorrect incorrect nopolmath incorrect incorrect incorrect math incorrect incorrect incorrect math incorrect incorrect incorrect math incorrect incorrectunknownmath incorrect incorrect incorrect math incorrect incorrect incorrect time incorrect incorrect incorrect time incorrect incorrect incorrect chart incorrect incorrect incorrect chart incorrect incorrect incorrect chart incorrect incorrect incorrect chart incorrect incorrect incorrect closure incorrect incorrect incorrect closure incorrect incorrect incorrect closure incorrect incorrect incorrect closure incorrect incorrect incorrect closure incorrect incorrect incorrect closure incorrect incorrect incorrect closure incorrect incorrect incorrect closure incorrect incorrect incorrect closure incorrect incorrect incorrect closure incorrect incorrect incorrect closure incorrect incorrect incorrect closure incorrect incorrect incorrect closure incorrect incorrect incorrect closure incorrect incorrect incorrect mockito incorrect incorrect incorrect angelix lang incorrect incorrect incorrect cvc4 lang incorrect incorrect incorrect enum lang incorrect incorrect incorrect not necessarily the ones that participants require more time to manually label.
vii.
d iscussion in this section we first provide implications of our findings.
we then discuss our post study survey in which we asked a number of independent annotators for rationales behind their patch correctness judgements.
future work and possible challenges inspired by our study are described next.
at the end of this section we discuss some threats to validity.
a. implications to recap we have gained insights into the reliability of patch correctness assessment by authors and by automatically generated independent test suite its each of them has their own advantages and disadvantages.
based on these insights we provide several implications as follows authors evaluation of patch correctness should be made publicly available to the community.
xiong et al.
martinez et al.
and le et al.
released their patch correctness labels publicly which we are authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
grateful for.
we believe that considerable effort has been made by authors to ensure the quality of the labels.
still we noticed that for slightly more than of the patches authors labels are different from the ones produced by multiple independent annotators.
thus we encourage future asr paper authors to release their datasets for public inspection.
the public including independent annotators can then provide inputs on the labels and possibly update labels that may have been incorrectly assigned.
our findings here e.g.
author annotations are fairly reliable may not generalize to patches labelled by authors which have not been released publicly.
it is possible that the quality of correctness labels for those patches which are not made publicly available to be lower.
also as criticized by monperrus et al.
the conclusiveness of the evaluation of techniques that keep patches and their correctness labels private is questionable.
collaborative effort is needed to distribute the expensive cost of asr evaluation.
in this study we have evaluated correctness of automatically generated patches by involving independent annotators.
we have shown that the quality of the resultant labels measured using inter rater reliability are on par with high quality text retrieval benchmarks .
unfortunately evaluation using independent annotators is expensive.
to evaluate patches we needed to get professional developers each agreed to spend up to an hour of their time.
this process may not be scalable especially considering the large number of new asr techniques that are released in the literature year by year.
thus there is a need for more collaborative effort to distribute the cost of asr evaluation.
one possibility is to organize a competition involving impartial industrial data owners e.g.
software development houses willing to share some of their closed bugs who are willing to judge correctness of generated patches.
similar competitions with industrial data owners have been held to advance various fields such as forecasting2and fraud detection3.
independent test suite its alone should not be used to evaluate the effectiveness of asr.
independent test suites itss generated by d ifftg en and r andoop have been shown to be ineffective in annotating correctness labels for patches see section vi .
only fewer than a fifth of the incorrect patches are identified as such by itss generated by d ifftg en and r andoop .
based on effectiveness of state of the art test generation tool for automatic repair that we assessed in this study we believe that its alone should not be used for fully automated patch labeling.
the subject of its generation for program repair is new though and we encourage future studies to improve the quality of automatic test generation tools so that more incorrect analyticup task1.html r correlationmatrix.getentry i j else out tdistribution.cumulativeprobability t out tdistribution.cumulativeprobability t public class pearsonscorrelation double t math.abs r math.sqrt nobs r r double corr correlation matrix.getco lumn i matrix.getcolumn j 3for intj j i j if nvars outmatrix.setentry j i corr outmatrix.setentry i j corr a human patch b generated patch fig.
.
a machine generated patch labeled by its as incorrect but labeled by author annotation as unknown.
patches can be detected.
that being said automated patch annotation may not be a silver bullet the general problem of patch correctness assessment judging the equivalence of developer patch and automatically generated patch is a variant of program equivalence problem which has been proven to be undecidable with no algorithmic solution .
independent test suite despite being less effective can be used to augment author annotation.
it has been shown in section vi that its generated by difftg en and r andoop identified four patches as incorrect whereas the labels generated by author annotation were unknown or correct.
an example of such a patch is shown in figure .
from the figure we can notice that it is hard to manually determine whether the patch is correct or not.
from this finding we believe that its despite being less effective than author annotation in identifying correct patches can be used to augment author annotation by helping to resolve at least some of the ambiguous cases.
authors can possibly run difftg enand r andoop to identify clear cases of incorrect patches the remaining cases can then be manually judged.
the use of both author and automated annotation via its generation can more closely approximate multiple independent annotators labels while requiring less cost.
b. post study survey we conducted a post study survey to investigate why a developer chooses a different answer from the majority.
among the patches there are several patches where the majority but not all participants agree on patch correctness.
among participants annotating these patches we selected who answered differently from the majority and emailed them to get deeper insights into their judgments.
in our email we provided a link to the same web interface used in our user study to allow participants to revisit their decision for the patch in question.
notice that we did not inform the participants that their answers were different from the majority.
we received replies from out of the participants .
response rate .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
we found that out of developers changed their correctness labels after they looked into the patch again their revised labels thus became consistent with the labels that the majority agree.
the remaining three kept their correctness labels two judged two different patches as incorrect while the majority labels are correct while another judged a patch as correct while the majority label is incorrect .
these participants kept their decision for different reasons one was unsure of a complex expression involved in the patch another highlighted a minor difference that may be considered ignorable by others and the other participant viewed the generated and ground truth patch to have similar intentions.
c. future extensions beyond program repair .
the contribution of this work is an empirical investigation on the reliability of popular evaluation methods followed in past studies on program repair.
we believe that this kind of meta study that assesses reliability of evaluation methods should also be performed beyond program repair in areas such as software mining fault localization defect prediction static analysis and others that require a validation of results.
often past studies involve performance assessment made by authors done by e.g.
manually or semi automatically labelling the results or based on historical data that are dirty .
effort should be made for a more rigorous assessment which may be more costly to see if biases exists with the cheaper and existing evaluation alternatives and if biases exist the extent to which they exist.
we believe that our work can provide valuable insights in the design of these future studies.
there have been already efforts done in this area studies that investigate bias in software engineering .
our work is unique compared to these existing studies in terms of the target task investigated i.e.
asr and the methodology employed e.g.
the use of multiple independent professionals as annotators .
these studies are a good start but much more work is needed to ensure that current assessment methods employed to evaluate performance of many existing research solutions correctly reflect the quality of underlying tools being assessed.
usage of specifications.
in this work we used labels by independent annotators as ground truth to assess reliability of author and automated annotations.
independent annotators are however still humans and can admittedly make mistakes even with a substantial amount of time devoted to the annotation task.
to avoid this threat complete and correct specifications can be used in conjunction with a sound static verifier to serve as a reliable patch validation method e.g.
a patch passing the verification is definitely a correct one .
this could be achieved by creating a benchmark of programs equipped with complete and correct specifications and a set of test cases.
test cases can then be used by program repair techniques to generate patches and those machine generated patches can then be validated against specifications using a sound verifier.
we plan to investigate this direction by using the openjml verifier on programs accompanied by jmlannotations .
although complete and correct specifications are hard to obtain in practice a study with such specifications would be worth exploring since by so the extent to which a program repair technique overfits to test suite used for repair can be unequivocally determined.
to make this possible we plan to tradeoff the scale of studied systems for a higher degree of soundness in patch assessment.
d. threats to v alidity threats to internal validity.
these threats relate to potential errors and biases in our study.
we discuss them below to reduce the threat of potential errors in our code we conducted a pilot study with a few graduate students and thoroughly checked our code.
we do not use all patches in the original dataset by xiong et al.
martinez et al.
and le et al.
due to constrained resources we only have professional developers agreeing to devote an hour of their time the number is similar to those of past studies .
the results may differ if the whole dataset is used.
to mitigate this threat we randomly selected patches included in this study while keeping the ratios of patches generated by asr tools approximately the same.
the professional developers that we employed are not the original developers of the buggy code and ground truth patches.
unfortunately since the original developer patches included in xiong et al.
s study were committed many years ago the earliest being it is hard to contact those developers.
even if we can involve them they may have forgotten the detail of the patches.
however since the patches are small professional developers participated in our study should be able to assess patch correctness.
indeed in our study respondents were able to provide definite labels to a majority of patches i.e.
only .
are unknown while the rest are either incorrect or correct .
additionally we asked not only one professional developer but five of them to label each patch.
section iv highlights that there is a substantial agreement among participants which is on par with high quality benchmark datasets.
moreover participants are provided with multiple resources e.g.
source code files failed test cases g ithub link of the project etc for the annotation task.
a large number of past software engineering studies e.g.
has also involved thirdparty labelers who are not content creators to assign labels for data.
and the same annotation setup was also followed in other related areas e.g.
information retrieval .
last but not least we also make the patches and participants responses publicly available for public inspection .
threats to external validity.
these threats relate to the generalizability of our results.
we discuss them below we included patches generated by asr tools to fix buggy code from software projects.
we believe this is a substantial number of patches generated by a substantial number of state of the art asr tools.
past empirical studies on asr e.g.
include five tools and patches from bugs.
still we acknowledge that results may differ if more patches projects and asr tools are considered.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
we have included professional developers in our user study.
this number is larger or similar to those considered in many prior work e.g.
.
the results may differ for other groups of developers.
to reduce this threat we have selected a mix of junior and senior developers from two large it companies and a large educational institution.
threats to construct validity.
these threats relate to the suitability of our evaluation metrics.
in this study we use average pairwise cohen s kappa and krippendorff s alpha to evaluate the reliability of the patch labels from independent annotators.
we also use the two to measure agreement between independent annotators labels and those produced by author and automated annotations.
these metrics are widely used in many research areas e.g.
information retrieval software engineering etc.
thus we believe there is little threat to construct validity.
viii.
r ela ted work program repair.
there are several asr techniques beyond those investigated in our study rsrepair and ae are random search techniques.
par uses templates to repair.
prophet and hdrepair use historical bug fix data to guide the repair process.
semfix directfix and spr use symbolic execution and angelix debugging.
qlose use program traces to rank repairs in the order of likelihood of being correct.
elixir uses machine learning to generate repairs.
jaid builds rich abstraction state for repair.
we refer interested readers to gazzola et al.
s survey paper for a more comprehensive review.
patch correctness assessment.
qi et al.
empirically studied patches generated by genprog rsrepair and ae .
they manually investigated the patches wrote additional test cases and reported the results on running the patches against additional test cases.
authors of par performed a user study on the acceptability of patches generated by their tool.
they employed students and developers to confirm that patches generated by par are more acceptable than genprog.
monperrus et al.
discuss the main evaluation criteria of automatic software repair including understandability correctness and completeness.
they suggest that repair techniques having their generated patches along with correctness labels kept private such as par are questionable.
to avoid potential bias of manual human investigation smith et al.
use automatic test case generation tool klee to generate independent test suites its that maximize coverage of ground truth program to assess machine generated patches .
using its they evaluate the effectiveness of genprog rsrepair aka.
trpautorepair and ae on the introclass dataset .
recently xin et al.
and xiong et al.
proposed an automated approach to identify incorrect machine generated patches via execution traces.
they leverage automatic test generation to generate additional test cases and use execution traces when executing test cases to determine whether a machine generated patch is correct or incorrect.unlike previous works which compare and evaluate effectiveness of asr solutions the main goal of our study is to assess whether methodologies that are often used for effectiveness evaluation of asr are fair or reliable.
we do this by assessing reliability of author annotation and automated annotation by using a gold set of labels collectively built by professional developers following standard best practice.
.
empirical studies on biases and reliability.
bird et al.
highlighted that only a fraction of bug fixes are labelled in version control systems and this causes a systematic bias in the evaluation of defect prediction tools .
herzig et al.
manually examined reports from issue tracking systems of open source projects and reported that .
of all bug reports to be misclassified .
they showed that the misclassification introduces bias to defect prediction studies since a substantial number of files is wrongly marked as defective.
the goal of our study is similar to the goals above we want to highlight and reduce bias in the evaluation of automated software engineering tools.
ix.
c onclusion and future work we assessed the reliability of existing patch correctness assessment methods via a user study.
the study involved professional developers and resulted in a high quality gold set of correctness labels for patches generated by different asr techniques.
using the gold set we assess reliability of author annotation i.e.
xiong et al.
martinez et al.
and le et al.
and automated annotation i.e.
difftg en and r andoop .
we find that a majority .
.
of labels produced by authors match those produced by independent annotators only fewer than a fifth of incorrect patches can be labelled by d ifftg enand randoop as such.
d ifftg enand r andoop can however uncover multiple incorrect patches labeled as unknown or correct by authors.
based on our findings we recommend that asr authors publicly release their labels and that more collaborative effort to distribute the expensive cost of asr evaluation.
we also stressed that although its alone should not be used to fully judge patch correctness labels it can be used in conjunction with author annotation to increase accuracy.
we plan to explore the extensions described in section vii c and expand our gold set by recruiting more professional developers and collecting more asr generated patches.
organizing competitions with industrial data owners e.g.
with our two industrial partners whose developers have participated in this study is also interesting to explore.