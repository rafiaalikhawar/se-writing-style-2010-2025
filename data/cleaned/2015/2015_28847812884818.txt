release planning of mobile apps based on user reviews lorenzo villarroel1 gabriele bavota1 barbara russo1 rocco oliveto2 massimiliano di penta3 1free university of bozen bolzano bolzano italy 2university of molise pesche is italy 3university of sannio benevento italy lorenzo.villarroel stud inf.unibz.it gabriele.bavota barbara.russo unibz.it rocco.oliveto unimol.it dipenta unisannio.it abstract developers have to to constantly improve their apps by xing critical bugs and implementing the most desired features in order to gain shares in the continuously increasing and competitive market of mobile apps.
a precious source of information to plan such activities is represented by reviews left by users on the app store.
however in order to exploit such information developers need to manually analyze such reviews.
this is something not doable if as frequently happens the app receives hundreds of reviews per day.
in this paper we introduce clap crowd listener for rele ase planning a thorough solution to i categorize user reviews based on the information they carry out e.g.
bug reporting ii cluster together related reviews e.g.
all reviews reporting the same bug and iii automatically prioritize the clusters of reviews to be implemented when planning the subsequent app release.
we evaluated all the steps behind clap showing its high accuracy in categorizing and clustering reviews and the meaningfulness of the recommended prioritizations.
also given the availability of clap as a working tool we assessed its practical applicability in industrial environments.
ccs concepts software and its engineering !software maintenance tools keywords release planning mobile apps mining software repositories .
introduction the market of mobile apps is exhibiting a tangible growth and it is expected to reach billion in annual revenue by .
the typical app delivery mechanism is a store in which on the one hand new releases of the app are available for download and on the other hand users rate the app and post reviews.
user reviews have the purpose of explaining why the users like or do not like the app report bugs or permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may austin tx usa c acm.
isbn .
.
.
.
new features.
in such a scenario there is an enormous competition among stakeholders producing similar apps.
if an app does not satisfy the users and if relevant suggestions to improve the app are simply ignored it is very likely that the app would loose its market share.
user reviews and ratings are therefore very important assets in the development and evolution of mobile apps.
indeed satisfactorily addressing requests made through user reviews is likely to increase the app rating .
however manually read each user review and verify if it contains useful information e.g.
bug reporting or request for a new feature is not doable for popular apps receiving hundreds of reviews per day.
for such reasons researchers have developed approaches to analyze the content of user reviews with the aim of crowd source requirements .
among others ar miner is able to discern informative reviews group and rank them in order of importance.
while approaches to identify and classify relevant and informative reviews have been proposed it would be desirable to have a fully automated or semi automated solution that given the user reviews for an app recommends which ones being them requests for new features or for bug xes should be addressed in the next release.
in this paper we propose clap an approach to i automatically categorize user reviews into suggestion for new feature bug report and other including non informative reviews ii cluster together related reviews in a single request and iii recommend which review cluster developers should satisfy in the next release.
unlike ar miner clap classi es reviews into bug report and feature suggestion providing additional insights to the developer about the nature of the review.
also while ar miner simply provides a ranking of the user reviews based on their importance as assessed by a pre de ned formula clap learns from past history of the same app or of other apps to determine the factors that contribute to determining whether a review should be addressed or not.
we thoroughly evaluated each step of clap as well as of the whole tool.
first we performed a study to assess the accuracy of clap in classifying reviews.
the second validation stage aimed at comparing the review clusters generated byclap with respect to manually produced clusters.
the third validation assessed the ability of clap to recommend features and bug xes for the next app releases.
last but not least in the fourth validation stage we provided our tool to managers of three italian software companies to get quantitative and qualitative feedback about the applicability ofclap in their everyday decision making process.
ieee acm 38th ieee international conference on software engineering ieee acm 38th ieee international conference on software engineering .
clap in a nutshell clap provides support to developers for the release planning of mobile apps by automatically importing and mining user reviews through a three step process detailed in the following subsections.
.
categorizing user reviews the rst step aims at classifying user reviews into three categories bug report suggestion for new feature and other .
the rationale is that as it will be clear in section .
developers can use di erent motivations to decide upon implementing bug xes or requests for new features.
other tools such as ar miner classify reviews into informative and non informative.
in our case we make a more speci c classication of informative reviews whereas the non informative ones fall into the other category.
also we are aware that besides bug reports and suggestions for new features other relevant information could be contained in user reviews e.g.
general comments on the user experience in using the mobile app .
however in this version of clap we choose to focus the classi cation on bugs to x and features to be implemented since we believe that they concern the macro activities in the maintenance and evolution of apps as also con rmed by the three project managers that participated to the evaluation of clap .
clap uses the weka implementation of the random forest machine learning algorithm to classify user reviews.
the random forest algorithm builds a collection of decision trees with the aim of solving classi cation type problems where the goal is to predict values of a categorical variable from one or more continuous and or categorical predictor variables.
in our work the categorical dependent variable is represented by the type of information reported in the review bug report suggestion for new feature or other and we use the rating of the user reviews and the terms sentences they contain as predictor variables.
we have chosen random forest after experimenting with di erent machine learner algorithms details of the comparison are in the replication package .
we adopt a customized text preprocessing to characterize each review on the basis of its textual content.
the steps of such a process are detailed in the following and the bene ts brought by each step will be shown in our empirical evaluation section .
handling negations.
mining text in code reviews present challenges related to negation of terms.
for instance reviews containing the words lag and glitches generally indicate bug reporting.
however there is a strong exception to this general trend that is due to the negation of terms.
consider the following review i love it it runs smooth no lags or glitches .
it is clear that in this case the context in which the words lag and glitches are used does not indicate any bug.
however the presence of these words in the review could lead to misclassi cations from the prediction model.
thus we adopt the stanford parser to identify negated terms in the reviews and remove them.
for example we convert i love it it runs smooth no lags or glitches into i love it it runs smooth a set of words better representing the message brought by the review.
stop words and stemming.
terms belonging to the english stop words list are removed to reduce noise from the user reviews.
also we apply the porter stemmer to reduce all words to their root.
unifying synonyms.
one possibility to unify synonyms rating .
true false bug true false bug reporting needs update true false bug reporting wish true false feature suggestion should add true false other... feature suggestionfigure example of regression tree generated by clap when categorizing user reviews.
would be to use existing thesaurus such as wordnet .
however in the context of user reviews we found that general purpose thesaurus are not adequate.
thus we rely on a customized dictionary of synonyms that we de ned by manually looking at reviews not used in the empirical evaluation we collected for a previous work .
examples of synsets we obtained are ffreeze crash bug error fail glitch problemg terms related to a bug crash or fadd miss lack wishg terms related to the need for adding a new feature .
noticeably words such as freeze crash bug and glitch would not be considered synonyms by a standard thesaurus while they are very likely to indicate the same concept in mobile apps reviews.
n grams extraction.
besides analyzing the single words contained in each review we extract the set of n grams composing it considering n2 .
for instance the extraction of n grams from a review stating the app resets itself needs to be xed will result in the extraction of n grams likeresets itself needs to be xed etc.note that the three preprocessing steps described above are not performed on the n grams i.e.
they only a ect the single words extracted from the review .
this is done to avoid loosing important information embedded in n grams.
for example managing negations is not needed when working with n grams since n grams extracted from a review like i love it it runs smooth no lags or glitches will include no lags no lags or glitches etc.
synonyms merging also is not applied to n grams to avoid changing their meaning e.g.
the app freezes should not be converted in the app bug being freeze and bug synonyms according to our thesaurus .
after text preprocessing we classify a user review using as predictor variables i its rating ii the list of n grams derived from it and iii the bag of words left as output of the previously described steps.
training data with pre assigned values for the dependent variables are used to build the random forest model.
an example of generated classi cation tree is shown in figure due to limited space we just show the left hand subtree of the root node .
.
clustering related reviews in order to identify groups of related reviews e.g.
those reporting the same bug we cluster reviews belonging to the same category e.g.
those in bug report .
clustering reviews is needed for two reasons i developers having hundreds of reviews in a speci c category could experience information overloading wasting almost all advantages provided by the review classi cation and ii knowing the number of users who are experiencing a speci c problem bug or that desire a speci c feature already represents an important information about the urgency of xing a bug implementing a feature.
note that we only cluster reviews classi ed as bug report orsuggestion for new feature since those are the ones the developer should be interested in for planning the next release of her app.
reviews .
true false rating .
low priority true false devices .
low priority true false low priority ...figure example of regression tree generated by clap when prioritizing clusters.
review clustering is performed by applying dbscan a clustering algorithm identifying clusters as areas of high element density assigning the elements in low density regions to singleton clusters i.e.
clusters only composed by a single element .
in clap the elements to clusters are the reviews in a speci c category and the distance between two reviews ri andrjis computed as dist r i rj vsm ri rj where vsm is the vector space model cosine similarity between riandrjadopting tf idf as term weighting schema.
before applying vsm the text in the reviews is processed as described in the categorization step section .
with the only exception of the synonyms merging.
indeed merging synonyms before clustering could be counterproductive since for example a review containing freezes and a review containing crash could indicate two di erent bugs.
dbscan does not require the de nition a priori of the number of clusters to extract.
however it requires the setting of two parameters i minpts the minimum number of points required to form a dense region and ii the maximum distance that can exist between two points to consider them as part of the same dense region cluster .
we set minpts since we consider two related reviews su cient to create a cluster while in section we describe how to set .
.
prioritizing review clusters the clusters of reviews belonging to the bug report and suggestion for new feature categories are prioritized with the aim of supporting release planning activities.
also in this stepclap makes use of the random forest machine learner with the aim of labelling each cluster as high orlowpriority where high priority indicates clusters clap recommends to be implemented in the next app release.
thus the dependent variable is represented by the cluster implementation priority high orlow while we use as predictor features the number of reviews in the cluster jreviewsj .
the rationale is that a bug reported feature suggested by several users should have a higher priority to be xed implemented than a bug feature experienced wanted by a single user.
the average rating of the cluster rating .
we hypothesize that a review cluster having a low average rating has a higher chance to indicate a higher priority bug or a feature to be implemented urgently than a cluster containing highly rated reviews and thus should have a higher priority.
for example people frustrated by the presence of critical bugs are more likely to lowly rating the app.
the di erence between the average rating of the cluster and the average rating of the app rating app .
this feature aims at assessing the impact of a speci c cluster on the app total rating.
we expect a lower di erence especially negative ones to indicate higher priority for the cluster.
the average di erence of the ratings assigned by users in the cluster who reviewed older releases of the app rating u .
a google play user can review multiple releases of an app over time.
clearly her rating can change over time as a consequence of her satisfaction in using the di erent 4figure user interface of clap .
releases.
given a cluster ccontaining a set of reviews r referring to the release ri we compute the average di erence of the ratings assigned by authors of rwith respect to last rating if any they assigned to the releases rx withx i .
if the authors of rdid not review the app before ri she is not considered in the computation of rating u. if none of the authors ofrevaluated the app in the past rating u .
the number of di erent hardware devices in the cluster jdevicesj .
one of the information available to app developers when exporting their reviews from google play is the reviewer hardware model reporting the name of the device used by the reviewer.
we conjecture that the higherjdevicesj the higher the priority of a cluster.
for example if a cluster of bug report reviews contains reviews written by users exploiting several di erent devices the bug object of the cluster is likely to a ect a wider set of users with respect to a bug only reported by users working with a speci c device.
similarly this holds in the case of desired features since the same app can expose di erent features on di erent devices e.g.
on the basis of the screen size .
also in this case historical data with known and labeled value of the dependent variable is used to build the random forest decision tree.
note that given the di erent nature of reviews reporting bugs and those suggesting new features the prioritization is performed separately for clusters containing the two types of reviews bugs and features .
a portion of a tree generated in this step can be found in figure .
.
clap prototype figure reports an excerpt of the user interface of clap we removed side and top menu due to lack of space .
in the example shown in figure the user imported in clap a set of user reviews from the facebook app.
as a result the reviews have been categorized into suggestion for new feature bug report and other see e.g.
element in figure .
in the example the bug report category is expanded to unveal its review clusters.
each cluster see e.g.
element has a label composed of i a simple identi er e.g.
c1 and ii the ve most frequent terms in the reviews belonging to it.
red clusters e.g.
c0 in figure are those marked byclap as high priority clusters while grey clusters e.g.
c1 represent the low priority ones.
the tool also 16provides a feedback mechanism to allow the developer to indicate whether or not she is going to implement the reviews contained in a cluster element in figure .
such a manual feedback can be used by the developer to expand revise the automatic prioritization training set according to the features bugs actually implemented.
finally by expanding a cluster one can see the reviews it contains.
as it can be seen in the example the three reviews of cluster c0 report a similar problem despite the users installed the facebook app when clicking on the app icon they receive the app not installed message.
also in this case there is a feedback mechanism element in figure to change the review category or to assign it to a di erent cluster.
.
empirical study design the goalof this study is to evaluate clap in terms of its i accuracy in categorizing user reviews in the three categories of interest i.e.
bug report suggestion for new feature and others ii ability in clustering related user reviews belonging to the same category e.g.
all reviews reporting the same bug iii ability in proposing meaningful recommendations on how to prioritize the bugs to be xed and new features to be implemented while planning the next release of the app and iv its suitability in an industrial context.
the context of the study consists of reviews of android mobile apps and three italian software companies.
the material used in this evaluation along with its working data set is publicly available in our replication package .
.
research questions in the context of our study we formulated the following four research questions rq rq how accurate is clap in classifying user reviews in the considered categories?
this rq assesses the accuracy of clap in classifying user reviews in thebug report suggestion for new feature and others categories.
it aims at evaluating the step categorizing user reviews described in section .
.
rq how meaningful are the clusters of reviews generated by clap ?this rq focuses on the meaningfulness of clusters of reviews extracted by clap in a speci c category of reviews e.g.
those reporting bugs .
we are interested in assessing the di erences between clusters of reviews automatically produced by clap with respect to those manually produced by developers.
rq evaluates the step clustering related reviews described in section .
.
rq how accurate is the new features bug xing prioritization recommended by clap ?our third rq aims at evaluating the relevance of the priority assigned by clap to the bugs to x and new features to be implemented in sight of the next release of the app.
we assess the ability of clap in predicting which bugs will be xed features will be implemented by developers among those reported requested in user reviews of releaseriwhen working on release ri .
this rq evaluates the prioritization step described in section .
.
rq would actual developers of mobile applications consider exploiting clap for their release planningtable objects used in our research questions.
r q apps reviews origin r q1 randomly selected from rq reviews from popular apps referring to the same app release rq selected on the basis of speci c criteria from rq reviews from two very popular apps facebook and twitter activities?
for a tool like clap a successful technological transfer is the main target objective.
in rq 4we investigate the industrial applicability of clap with the help of three software companies developing android apps.
thus rq 4evaluates the clap prototype tool as a whole as described in section .
.
.
context selection and data analysis table summarizes the objects i.e.
apps and user reviews used in each of our research questions.
to address rq 1we manually classi ed a set of users reviews randomly selected from di erent android apps extracted from the dataset by chen et al.
.
in particular two of the authors independently analyzed the reviews by assigning each of them to a category among bugs report suggestion for new feature and others .
then they performed an open discussion to resolve any con ict and reach a consensus on the assigned category.
this was needed for out of the reviews.
in total of the considered reviews we labeled as bug report as suggestion for new feature and as others .
then we used this dataset to perform a fold cross validation computing the overall average accuracy of the model and reporting the obtained confusion matrix.
forrq 2we manually collected a second set of user reviews among ve android apps i.e.
facebook twitter yahoo mobile client viber and whatsapp .
for this research question we have selected very popular apps since we needed to collect from each app a good number of reviews i related to the same app s release and ii belonging to the bug report or to the suggestion for new feature category.
in particular we randomly selected from each of these apps reviews bug reports and suggestions for new features referring to the same app s release1.
then we asked three industrial developers having over ve years of experience each to manually clustering together the set of reviews belonging to the same category e.g.
bugs report in each app.
we clearly explained to the developers that the goal was to obtain clusters of reviews referring to the same bugs to be xed or feature to be implemented.
the three developers independently analyzed each of the reviews to cluster them.
after that they reviewed together their individual clustering results and provided us a single oracle re ecting their overall point of view of the existing clusters of reviews.
once obtained the oracle we used clap and in particular the process detailed in section .
to cluster together the same sets of reviews.
as previously explained to apply the dbscan clustering algorithm we need to tune its parameter.
we performed such a tuning by running the dbscan algorithm on the yahoo app varying between .
and .
at steps of .
i.e.
nine di erent con gurations .
note that it does not make sense to run dbscan with or since the output would trivially be a set of singleton clusters in the former case and a single cluster with all reviews in the second case.
1as previously said in clap we are not interested in clustering reviews belonging to the other category.
17mojofm .
.
.
.
.
.
.
.
.
.
figure tuning of the dbscan parameter.
to de ne the best con guration among the nine tested ones we measured the similarity between the two partitions of reviews i.e.
the oracle and the one produced by clap by using the mojo effectiveness measure mojofm a normalized variant of the mojo distance based on the minimum number of move orjoin operations one needs to perform in order to transform a partition ainto a partition b.mojofm returns if partition ais the farthest partition away fromb it returns if ais exactly equal to b. the results of this tuning are shown in figure .
in general values between .
and .
allows to achieve good performances with the highest mojofm reached at .
.
this is the default value in clap and thus the one we will use in the evaluation.
in order to evaluate the clap s clustering step we measured the mojofm distance on reviews of the remaining apps i.e.
excluding yahoo .
we report the mojofm achieved in two di erent scenarios intermediate feedback available.
as explained in section .
clap allows the user to x review misclassi cations e.g.
a review reporting a bug classi ed as a suggestion for new features .
thus we rstly simulate a scenario in which the categorization of the reviews has been manually checked and xed when needed by the developer.
this is done by separately running the clustering algorithm on the bug report reviews and on the suggestions for new feature reviews available for each app.
this scenario allows us to assess the performances of the clap s clustering step in isolation without the risk of having the achieved mojofm values a ected by misclassi cation performed in the categorization step.
fully automated approach.
to simulate a fully automated clap s usage scenario in which the developer does not act on the review categorization we use clap to automatically categorize the reviews of each app identifying those reporting bugs and suggesting new features.
then we cluster them by using the dbscan algorithm and compare via mojofm the produced clusters of reviews with the oracle de ned by the developers.
note that in this case it is possible that some of the reviews are categorized byclap in the other category.
in a real usage scenario clap would not cluster these reviews thus we remove them before the clustering.
however since it is not possible to compute the mojofm between two di erent sets of clustered entities reviews we also removed these reviews from the oracle.
for this reason in this evaluation scenario we report i the mojofm achieved by our approach when automatically categorizing the reviews and ii the numberof instances wrongly discarded by our approach due to a misclassi cation.
to answer rq 3we exploited the android user reviews dataset made available by chen et al.
.
this dataset reports user reviews for multiple releases of 21k apps showing for each review i the date in which it has been posted ii the app s release it refers to iii the user who posted it iv the hardware device exploited by the user v the rating and vi the textual content of the review itself.
in addition each app in the dataset is associated to a metadata le containing its basic information including the updated optional eld that app s developers can use to describe the changes they made to the di erent app s releases i.e.
a sort of release note shown in the google play store .
we exploited such a dataset to build for a given app an oracle reporting which of the reviews left by its users for the releaserihave been implemented by the developers in the releaseri i.e.
high priority reviews and which instead have been ignored postponed i.e.
low priority reviews .
to reach such an objective rstly we identi ed the apps in the dataset having all the information characteristics required to build the oracle .
a non empty updated eld containing at least one english word .
as said before this is an optional eld where the app developers can report the changes they applied in a speci c app s review.
this rst ltering was automated by looking for the updated elds matching at least one term excluding articles in the mac os x english dictionary.
this left us with 11k apps.
.
explicitly reporting the app s version to which the updated eld refers .
often developers simply put in the updated eld the changes applied to the last release of the app without specifying the release number e.g.
release .
.
this is an information needed to build our oracle.
indeed starting from the release note i.e.
the content of the updated eld of a speci c releaseri we have to look at the reviews left by users of the release rito verify which of them have been actually implemented by the developers.
we adopt regular expressions e.g.
version release v rfollowed by at least two numbers separated by a dot to automatically identify apps reporting the release number in the updated eld.
this left us with .4k apps.
.
having a non ambiguous release note update eld .
release notes only containing sentences like xed several bugs or this release brings several improvements and new features are not detailed enough to understand which of the user reviews have been implemented by developers.
for this reason one of the authors manually looked into each of these .4k apps for those containing a non ambiguous release note.
this selection led to only apps remaining.
.
having available at least reviews for the release ri preceding the ri 1described in the release note.
the dataset by chen et al.
does not report reviews for all releases of an app.
thus it is possible that the reviews for riare not available or are too few for observing something interesting.
this further selection process led to the ve apps considered in our study barebones .
.
hmbtned .
.
timeriffic .
ebay .
.
and viber .
.
.
the ve selected app releases received a total of user reviews from the rireleases to be labeled as implemented or not implemented in ri .
since manually labeling all of them would not be feasible in a reasonable time we randomly 18selected from each app a statistically signi cant sample of reviews with con dence level and con dence interval.
this resulted in reviews that were manually analyzed by two of the authors independently and labeled as implemented not implemented on the basis of the information contained in the related release note.
also in this case con icts raised for reviews were solved with an open discussion.
once built the oracle for the ve apps we performed a fold validation to assess the ability of clap and in particular of the prioritization step described in section .
to correctly identify the clusters of reviews that should be prioritized in sight of the next app s release i.e.
those that have been actually implemented by developers in theri 1reviews .
as already done for rq we report the performance of clap considering two di erent scenarios .
a scenario in which the categorization of the reviews into bug report suggestion for new feature and other as well as the result of the clustering step has been manually checked and xed when needed by the developer.
to support such a scenario the two authors who labeled the reviews as implemented not implemented also i categorized them in one of the three supported categories and ii manually clustered them.
this manual process led to the identi cation of clusters of bug report reviews and clusters of suggestion for new features of which of those reporting bugs and of those suggesting features contain implemented reviews i.e.
arehigh priority clusters .
in this rst scenario we assess the accuracy of the clap s prioritization step in isolation by performing a ten fold validation on the set of clusters manually de ned.
given the unbalanced distribution of high priority and low priority clusters e.g.
5vs55 for those related to bug reporting we balanced at each iteration of the ten fold validation the training set via an under sampling procedure randomly selecting from the training set an equal number i.e.
the number of the underrepresented cluster category of high and low priority clusters.
to avoid any bias no changes were applied to the test set.
.
a fully automated scenario in which we used clap to categorize cluster and prioritize the obtained clusters.
note that di erently from the manually de ned clusters it is possible that automatically generated clusters contain both implemented and not implemented reviews as manually de ned in the oracle due to errors in the clustering step.
in this case we consider a cluster as correctly classi ed as high low priority if its centroid has been marked as implemented not implemented in the manually de ned oracle.
also in this case a ten fold validation has been performed and under sampling applied in case of unbalanced training sets.
finally to answer rq 4we conducted semi structured interviews with the project managers of three software companies developing android apps2.
before the interviews one of the authors showed a demo of clap and let the participant interact with the tool.
to avoid biases and evaluate the tool with a consistent set of reviews all project managers worked with a version of clap having the reviews for twitter and facebook imported.
note that using the reviews of the apps developed by the three companies was not an option since most of the reviews they receive are not in english and the current implementation of clap only supports such a language.
the interviews lasted for two hours with 2rq s participants were not the same involved in rq .table rq classi cation accuracy of reviews.
ca tegoryno text n gram handling stop words unifying preprocessing analysis negations stemming synonyms r p r p r p r p r p b ug report sugg.
new feature other o verall accuracy table rq confusion matrix.
b ug report sugg.
new feature other recall precision b ug report sugg.
new feature other each company.
each interview was based on the think aloud strategy.
speci cally we showed all the tool features to the managers to get qualitative feedback on both the tool and the underlying approach.
in addition we explicitly asked the following questions usefulness of reviews .
do you analyze user reviews when planning a new release of your apps?
factors considered for the prioritization phase .
are the factors considered by clap reasonable and su cient for the prioritization of bugs and new features?
review categories .
is the categorization of reviews into bug report andsuggestion for new feature su cient for release planning or there are other categories that should be taken into account?
tool usefulness .
would you use the tool for planning new releases of your apps?
participants answered each question using a score on a four point likert scale absolutely no no yes absolutely yes.
the interviews were conducted by one of the authors who annotated the provided answers as well as additional insights about the clap s strengths and weaknesses that emerged during the interviews.
.
study results this section reports the analysis of the results for the four research questions formulated in section .
.
rq how accurate is clap in classifying user reviews in the considered categories?
table reports the recall r precision p and overall accuracy achieved byclap when classifying user reviews.
in particular we show the accuracy of our classi er when considering not considering the di erent text preprocessing steps.
the second column on the left no text preprocessing reports the classi cation accuracy obtained without performing any text preprocessing i.e.
by providing to the machine learner all terms present in the user reviews .
by moving toward the right part of table we can observe the impact on the accuracy of clap when i including the extracted n grams ii handling negations iii performing stop word removal and stemming and iv unifying synonyms .
as it can be seen the text preprocessing steps adopted in clap ensure a of accuracy over the baseline.
table reports the confusion matrix of clap along with recall and precision values for each category detailing the overall accuracy correct classi cations out of reviews achieved by clap .
the most frequent case of failure for clap is represented by the misclassi cation of reviews belonging to bug report and suggestion for new feature categories as other accounting for a total of errors 19table rq mojofm achieved by clap .
f acebook twitter viber whatsapp average sc enario i manual categorization b ug report sugg.
new feature sc enario ii automatic categorization a ll of the overall errors .
a manual inspection revealed that this is mainly due to reviews related to bugs or new features not containing any of the keywords that according to the learned decision tree lead towards a bugornew feature classi cation.
rq how meaningful are the clusters of reviews generated by clap ?table shows the mojofm between the clusters of reviews manually de ned by developers and those resulting from the clustering step of clap in the two evaluation scenarios described in section .
.
in the rst evaluation scenario in which we assume that the developer has xed possible categorization errors made by clap e.g.
abug report review classi ed as a suggestion for new feature the average mojofm is for bugs and for new features suggesting a high similarity between manually and automatically created clusters.
in one case i.e.
the clustering of reviews suggesting new features in whatsapp the partition is exactly the same indicating the meaningfulness of the clusters generated by clap .
in order to give a better idea of the meaning of such mojofm values figure shows the two partitions of reviews manually created by the developers involved in the study left side and automatically generated by clap right side for the twitter reviews suggesting new features .
the points in light grey represent the eleven reviews considered both by developers and by clap as singleton clusters i.e.
each of these reviews recommended the implementation of a di erent feature .
the points in black represent instead the reviews clustered by developers into four non singleton clusters depicted with di erent colors in figure .
the rst two clusters the grey and the green ones are exactly the same in the oracle and in the clusters generated by clap .
the yellow cluster is similar between the two partitions.
however in the automatically generated partition it does not include the review r5 isolated as a singleton cluster byclap .
finally the blue cluster composed of r8and r9is the only one totally missed by clap that does not recognize the two reviews as semantically related.
overall these di erences resulted in a mojofm of .
mojofm twitter features oracle recommended r2r1 r4r3 r5r7r6 r9r8r2r1 r4r3r5r7r6 r9 r8 figure rq clap vsoracle when clustering suggestions for new features on twitter.table rq prioritization accuracy.
c orrectly classi ed false positive false negative sc enario i manual categorization and clustering b ug report sugg.
new feature sc enario ii automatic categorization and clustering b ug report sugg.
new feature the example reported in figure is very representative of the errors made by clap in clustering related reviews.
we observed as it tends to be more conservative in clustering the reviews with respect to the manually produced oracle i.e.
it generates more singleton clusters .
while this could suggest a wrong calibration of the parameter we also replicated this study with and since higher values of should promote the merging of related reviews.
however these settings resulted in lower values of the mojofm across all experimented systems due to a too aggressive merging of reviews.
the bottom part of table reports the mojofm achieved in the second evaluation scenario where the automated categorization of clap has been applied.
note that in this case we report the overall mojofm without distinguishing between bug reports and suggestions for new features since we run the clustering on the whole dataset of reviews for each app.
this is needed since the automatic categorization of reviews could lead to the introduction of misclassi cation and it is not possible to compare via mojofm partitions composed of di erent elements.
for example if a bug report review is misclassi ed by clap as a suggestion for new features the set of reviews clustered in the oracle and those clustered by clap would be di erent if looking into the speci c categories.
for a similar reason as explained in section .
we excluded from the comparison reviews misclassi ed in the other category.
the number of such reviews are indicated in parenthesis in table and always account for less than of the classi ed reviews i.e.
no more than three out of the reviews categorized in each app is wrongly put in the other category .
as for the mojofm it uctuates between viber and whatsapp showing again the ability of clap to generate clusters of reviews close to those manually de ned by developers.
rq how accurate is the new features bug xing prioritization recommended by clap ?table reports the accuracy achieved by clap in classifying clusters of bug report andsuggestion for new feature reviews as high priority i.e.
the cluster of reviews has been actually implemented by the apps developers in the subsequent release and low priority .
false positives are clusters wrongly classi ed by clap ashigh priority while false negatives are clusters wrongly classi ed as low priority .
results are reported for both the scenarios described in section .
.
in the rst scenario top part of table simulating a situation in which the clap user has manually xed possible categorization and clustering errors clap correctly prioritizes of clusters containing bug report reviews out of producing ve false positive and two false negatives.
the accuracy is slightly lower when prioritizing new features to be implemented with of correctly classi ed clusters out of six false positives and zero false negatives.
for example a bug report cluster correctly highly prioritized by clap is the one from the ebay app in which di erent users using a total of nine di erent hardware 20devices were pointing out a bug present in the release .
.
that prevented the app user to visualize the seller s feedbacks.
this cluster also had a very low average rating rating .
much lower that the average app rating rating app .
moreover the reviewers in this cluster who already evaluated past ebay releases assigned a much lower score to this speci c release rating u .
.
the ebay developers xed this bug in the release .
.
reporting in the release note fixed bug where seller feedback would not load .
a false negative generated by clap when prioritizing clusters reporting suggestions for new features is a singleton cluster from the barebones app a lightweight mobile browser.
one of the users reviewing the release .
assigned ve stars to the app and asked for the implementation of search suggestions i wish it can have search suggestions in the search bar .
despite the single user requiring such a feature and the high rating she assigned to the app the barebones developers implemented search suggestions in the release .
added google search suggestions .
the characteristics of this cluster led clap to a misclassi cation since the decision trees generated in the prioritization step tend to assign a high priority to clusters having high values forjreviewsjandjdevicesj and low values for rating rating app rating u see the example in figure .
note that these classi cation trees are the results of the training performed on the ve considered apps.
in a real scenario theclap user can explicitly indicate which clusters she is going to implement allowing the machine learner to adapt the classi cation rules on the basis of the user feedback.
when considering the clusters as produced automatically bottom part of table the prioritization accuracy of clap exhibits an expected decrease.
of clusters related to bug reporting and of those grouping suggestions for new features are correctly prioritized with the majority of prioritization errors and for the two categories of clusters due to false positives i.e.
low priority clusters classi ed as high priority ones .
such a decrease of performance is due to misclassi cations in the review categorization step as shown in the rq 1results clap misclassi es of the reviews and to errors introduced in the clustering step rq .
however we still believe that this level of accuracy represents a good starting point for helping app developers during release planning activities.
indeed as highlighted by all the feedback mechanisms we implemented in clap we did not envision our tool to be used as a black box taking user reviews as input and producing a list of prioritized clusters.
we rather look at it as a support for app developers interacting with them in order to gather as much information as possible from the user reviews.
comparison with ar miner.
to further assess the prioritization performances of clap we compared them with the prioritization performed by the state of the art technique ar miner .
note that we did not compare ar miner with clap in the previous steps i.e.
the categorization and clustering of reviews since the categories exploited by the two techniques are di erent with ar miner limiting its categorization to informative vs non informative reviews andclap looking for bugs reports and suggestions for new features .
instead both techniques aim at prioritizing groups of reviews based on their importance for developers i.e.
their relevance when working on a subsequent release .
the prioritization applied by ar miner focuses on the reviews classi ed as informative and it is based on a weighted sum ofthree factors i the number of reviews in the group cluster ii the average rating of the reviews in the group and iii the temporal distribution of reviews more recent reviews are considered more important .
since ar miner is not available3 we reimplemented its prioritization feature and tuned the weighting parameters as reported in .
then we applied ar miner on the same set of clusters prioritized byclap .
in particular we considered as informative the reviews that were manually tagged as bug reporting or as suggestion for new feature and as cluster to prioritize those manually de ned for these two categories of reviews i.e.
exactly the same clusters prioritized in this evaluation by clap .
then we compare the area under the curve auc for both techniques.
we use auc as ar miner produces a ranked list whereas clap produces a classi cation hence it is not possible to directly compare precision and recall values.
as expected clap obtained a higher auc when prioritizing bugs .
while the auc is lower .
when prioritizing features.
however in both cases the auc achieved by clap is much higher than the one achieved by ar miner when prioritizing the same set of informative reviews .
.
note that the prioritization step of the two techniques has been compared exactly on the same set of manually created clusters of informative reviews.
the only di erence is that clap separately prioritizes bug reports from suggestions for new features while ar miner prioritizes all the informative reviews as a whole this is why we only have one value of auc for it .
rq would actual developers of mobile applications consider exploiting clap for their release planning activities?
in order to answer our last research question we qualitatively discuss the outcomes of the semistructured interviews we conducted with project managers of three italian companies aimed at analyzing the practical applicability of clap in a real development context.
nicola noviello project manager next .
nicola answered our rst question i.e.
usefulness of user reviews with absolutely yes specifying that before planning a new release of an app the developers of his company manually analyze the app reviews to identify critical bugs or feature recommendations.
nicola also con rmed that such a task is time consuming when planning the release .
of the appunlikely quotes a developer spent two days in analyzing more than reviews.
while the need to x some bugs and to implement some features was easily spotted due to the fact that they were reported required by several users there were also interesting features and critical bugs hidden in a large amount of non informative reviews.
i strongly believe that clap would have sensibly reduced the e ort we spent to identify such reviews.
nicola also positively answered to our questions related to the completeness of the categories of the reviews considered by clap and the factors it uses for prioritization yes and absolutely yes respectively .
concerning the review categories considered by clap nicola suggested an additional category that could be considered reviews referring to the app sales plan .
nicola considers these reviews very important and he explained that the version .
of the app unlikely quotes was released both in a free and non free versions with the latter introducing some features for which the users explicitly claimed in their reviews that they will to pay for having such functionalities.
3we contacted the authors on july and they conrmed that the tool is not publicly available.
21thus user reviews could not only be useful to plan what to implement in the next release of an app but also to de ne the sale strategies.
finally nicola was really enthusiastic about clap and he will be happy to use it in his company.
indeed he considers the tool highly usable and ready to the market.
he also pointed out two possible improvements for clap .
first it would be useful to make the tool able to store and analyze user reviews coming from di erent stores e.g.
google play and apple app store putting together reviews posted by users running the app on di erent platforms could be important to discriminate between bugs strongly related to the app from those lying in the server side.
for example if the bug is reported by both android and ios users it is very likely that the bug is in the services exploited by the app rather than in the app itself.
also nicola suggested to integrate in clap a mechanism that allows to read and analyze the reviews of competitive apps in order to identify features for my app that are not explicitly required by my users but that have been suggested by users of competitive apps.
in other words i do not want to listen only to my users but also the users of competitive apps!
clearly this would require the implementation of techniques to automatically identify similar apps.
we consider this point as part of our future work agenda.
giuseppe socci project manager genialapps .
as well as nicola giuseppe answered absolutely yes to our rst question related to the usefulness of user reviews very often reviews are informative and useful to understand which are the directions for new releases.
i usually analyze the reviews manually and such a task is really time consuming.
in the rst year of life of our app credit for i analyzed more than reviews dedicating six or seven hours per week to this speci c task.
however keep up with the reviews helps a lot in making the app more attractive.
giuseppe also answered absolutely yes to our second question related to the completeness of the review categories when i manually analyze the user reviews i classify them in exactly the same categories.
instead giuseppe answered yes to the question related to the completeness of the factors used to prioritize the bugs and the new features while the exploited factors are reasonable in my experience i also implemented several features and xed some bugs that require few hours of work even if they were reported by just one person who is also already happy about the app.
for instance a user of the app happy birthday show rated the app with ve stars and requested to change the color of some buttons.
such a request required just a couple of hours of work.
thus i decided to implement it.
considering the change impact of a new request or a bug x might make the prioritization even more useful .
in addition giuseppe highlighted that the prioritization of the new features should take into account the kind of revision to perform i.e.
minor or major revision if a major revision is planned i tend to include as many feature requests as possible.
instead if i am working on a minor revision i really look for the most important feature requests to include those having the highest payo .
in this case the factors considered by clap in the prioritization are certainly valid.
finally giuseppe answered positively absolutely yes to our last question and he is willing to useclap as a support for the release planning of his future apps.
the only showstopper for the application of clap in genialapps is that most of the user reviews are written in languages di erent from english e.g.
spanish italian french .
we are currently adapting the tool aiming at making it multi languages by exploiting automatic translation tools.
luciano cutone project manager ideasoftware .
while luciano considers the user reviews useful for planning new releases in his company in general user reviews are not analyzed.
the reason is simple.
ideasoftware usually develops app on commission.
thus instead of considering user reviews the developers of ideasoftware implement the features and x the bugs required by their customers.
despite this luciano claimed that some of the features and bug xes required by the customers of our apps were derived from the in formal analysis of user reviews.
.
luciano answered yes to the questions related to the completeness of the review category and the factors used to prioritize the bugs and the new features.
however he also noticed that the tool could be more usable if a criticality index is provided for each feature and bug.
speci cally instead of having features bugs classi ed as high and low priority i would like to see a list of features bugs to be implemented ranked according to a criticality index ranging between low priority and high priority .
this would provide a better support for release planning especially when the number of features bugs classi ed as high priority is quite high and i do not have enough resources to implement all of them.
finally luciano claimed that the tool seems to be useful especially when a high number of reviews needs to be analyzed and he is willing to use the tool in his company for analyzing the user reviews of the apps they plan to develop for the mass market as opposed to those they currently implemented on commission for speci c customers .
luciano also suggested to capture more information on how and when feature requests and bug xes clusters have been implemented for each cluster i would like to store the version of the app in which i implemented it.
in this way i can maintain in clap the revision history of my apps and i could automatically generate release notes for each version .
.
threats to validity threats to construct validity are mainly related to imprecisions made when building the oracles used in the rst three research questions.
as explained in section the manual classi cations performed for rq 1andrq as well as the golden set clusters for rq 2have been performed by multiple evaluators independently and their results discussed to converge when discrepancies occurred.
threats to internal validity concern factors internal to our study that could have in uenced our ndings.
one threat is related to the choice of the machine learning algorithm random forest .
as explained in section we have experimented various approaches and chosen the one exhibiting the best performance but we cannot exclude that machine learners we did not consider or di erent settings of the algorithm could produce better accuracy.
similar considerations apply for the clustering algorithm.
the parameter of the dbscan algorithm has been chosen using the tuning explained in section .
.
for the comparison with ar miner review prioritization we use default weights reported in the paper but it could be the case that they are not the most suited ones for our dataset.
finally we are aware that planning the next release is a very complex process which involve di erent factors.
therefore the prioritization simply based on the factors we considered in clap is only a recom22 22mendation that need to be complemented by factors related to the expertise and experience of software engineers.
threats to external validity concern the generalization of our ndings.
in the context of rq 1we chose to select the reviews from a high number of apps instead that from just one or two apps to obtain a more general model.
indeed training the machine learner on reviews of a speci c app iwould likely result in a model e ectively working on appi s reviews but exhibiting low performances when applied on other apps.
still while we tried to assess our approach on a relatively large and diversi ed set of apps it is possible that results would not generalize to other apps e.g.
those developed for other platforms such as ios or windows phone or the approach adaptation to reviews written in languages di erent from english might not exhibit the same performances we obtained.
.
related work several works have focused the attention on the mining of app reviews with the goal of analyzing their topics and content the correlation between rating price and downloads and the correlation between reviews and ratings .
also crowdsourcing mechanisms have been used outside the context of mobile development for requirements engineering for example to suggest product features for a speci c domain by mining product descriptions to identify problematic apis by mining forum discussions zhang and hou and to summarize positive and negative aspects described in user reviews .
due to lack of space we focus our discussion on approaches aimed at automatically mining requirements from app reviews.
galvis and winbladh extract the main topics in app store reviews and the sentences representative of those topics.
while such topics could certainly help app developers in capturing the mood and feelings of their users the support provide by clap is wider thanks to the automatic classi cation clustering and prioritization of reviews.
iacob and harrison provided empirical evidence of the extent users of mobile apps rely on reviews to describe feature requests and the topics that represent the requests.
among reviews manually analyzed expressed feature requests.
then linguistic rules were exploited to de ned an approach coined as mara to automatically identify feature requests.
linguistic rules have also been recently exploited by panichella et al.
to classify sentences in app reviews into four categories feature request problem discovery information seeking and information giving.
clap di erently from mara and the approach by panichella et al.
also provides clustering and prioritization functionalities to help the developers in planning the new release of their app.
in our classi cation we only consider categories relevant to the subsequent clustering and prioritisation.
chen et al.
pioneered the prioritization of user reviews withar miner the closest existing approach to clap .arminer automatically lters and ranks informative reviews.
informative reviews are identi ed by using a semi supervised learning based approach exploiting textual features.
once discriminated informative from non informative reviews arminer groups them into topics and ranks the groups of reviews by priority.
the main di erences between ar miner andclap are .
bug new feature reviews vs. informative noninformative reviews .clap explicitly indicates to developers the category to which each review belongs e.g.
bug report vs suggestion for new feature while ar miner only discriminates between informative and non informative reviews.
clearly this di erent treatment also a ects the grouping step.
indeed while in ar miner a speci c topic e.g.
a topic referred to a speci c app s feature could indicate both suggestions on how to improve the feature as well as bugs reports in clap the review clustering is performed separately between the di erent review categories.
also while both techniques exploit textual features to categorize reviews clap introduces a set of pre processing steps e.g.
n grams extraction negations management customized synonyms list that as shown in table help in substantially increase the classi cation accuracy.
.
recommending next release features xes vs. ranking reviews .clap exploits a machine learner to prioritize the clusters to be implemented in the next app release.
this allows our approach to learn from the actual decisions made by developers over the change history of their app see also the next point .
on the opposite ar miner ranks the importance of reviews based on a prioritization score i.e.
a weighted sum of prioritization factors .
as shown in our evaluation clap outperforms ar miner in predicting the items that will be implemented by developers in the next release of their app.
above all since clap recommends reviews to be addressed in the next release based on the past history it would be able to weigh di erent features of the prediction model di erently for di erent apps and in general for di erent contexts.
finally the bug feature classi cation permits the use of di erent prioritization models for di erent kinds of change requests.
.
conclusion and future work this paper described clap a tool supporting the release planning activity of mobile apps by mining information from user reviews.
the evaluation of clap highlighted its i high accuracy in categorizing user reviews on the basis of the contained information i.e.
bug report suggestion for new feature and other ii ability to create meaningful clusters of related reviews e.g.
those reporting the same bug of mojofm iii accuracy in recommending the features to implement and the bugs to x in sight of the next app release and iv suitability in industrial contexts where we gathered very positive qualitative feedbacks about clap .
such qualitative feedbacks will drive our future work agenda aimed at improving clap with novel features and in particular i the identi cation of similar apps in the store with the goal of mining user reviews from competitive apps ii the multi store support and iii the automatic translation of reviews in english to overcome the current language limitation of clap .