effective api recommendation without historical software repositories xiaoyu liu department of computer science and engineering southern methodist university dallas texas usa xiaoyul smu.eduliguo huang department of computer science and engineering southern methodist university dallas texas usa lghuang smu.eduvincent ng human language technology research institute university of texas at dallas richardson texas usa vince hlt.utdallas.edu abstract it is time consuming and labor intensive to learn and locate the correctapiforprogrammingtasks.thus itisbeneficialtoperform api recommendation automatically.
the graph based statistical model has been shown to recommend top api candidates effectively.
it falls short however in accurately recommending an actualtop 1api.toaddressthisweakness weproposerecrank anapproachandtoolthatappliesanovelranking baseddiscriminative approach leveraging api usage path features to improve top api recommendation.
empirical evaluation on a large corpus of open source projects shows that recrank significantly improvestop 1apirecommendationaccuracyandmeanrecipro calrankwhencomparedtostate of the artapirecommendation approaches.
ccs concepts software and its engineering api languages software maintenance tools keywords api recommendation machine learning acm reference format xiaoyu liu liguo huang and vincent ng.
.
effective api recommendation without historical software repositories.
in proceedings of the 33rd acm ieee international conference on automated software engineering ase september montpellier france.
acm new york ny usa 11pages.
introduction during daily software development application programming interfaces apis are provided as functional building blocks to programsoftwaresystems.apisareclasses methods andfieldsprovided by the library s designers to enable developers to access the functionality of a code library.
however developers need to permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september montpellier france association for computing machinery.
acm isbn ... .
provided by a large number of apis in the library and pick the correct api for development tasks.
for instance developersneed to manually browse a long list of apis to identify bufferedwriter.write the api that enables them to efficiently write to a file by buffering the characters in java memory.
as another example developers have to choose from a list of candidate member methodsof stringinthejavadevelopmentkit jdk toidentifythe appropriate api for converting all of the corresponding characters to upper case i.e.
string.touppercase .
to address this challenge many automatedapi recommendation approachesand tools have beenproposedtorelievetheburdenofdevelopersinunderstandingandlocatingapis eitherbytakingadvantageofapiusagepatterns orbyusingstatisticallearningtorecommendthenext token .forinstance gralanusesastatisticallanguage model for api recommendation that relies on features extracted from the preceding context i.e.
the code that has been written so far .
the model was trained by collecting statistics on how oftenacandidateapico occurswiththeapisinitsprecedingcontext .
being a generative model however gralanis sensitive to the presenceof overlapping featuresand irrelevant features.specifically if two features encode overlapping information e.g.
two features arecomputedbasedonthesameapiintheprecedingcontext it willundesirablyamplifytheimportanceofthisapiintheprediction process thus possibly harming model performance.
irrelevant features i.e.
featuresthatarelargelynotpredictiveofthetarget api too couldbeharmful whilethestatisticscollectedduringthe trainingprocesscouldtosomeextentindicate whetherafeature is relevant the multiplicativeeffect resulting from a large number ofirrelevantfeaturesinagenerativemodelcouldoverwhelmthe positiveeffectoftherelevantfeatures againharmingmodelperformance.hence featureengineeringisimportantwhenemploying generative models.
unfortunately as we will see in the next section a number of features that gralanemploys are by design both overlapping and irrelevant.
more recently apirec a state of the art api recommendation approach was proposed by gralan s authors.
apirecmakes a key assumption changes that serve the same higher level intent ofthedeveloperswillco occurmorefrequentlythannon related changes .hence byleveragingtheregularityandrepetitiveness of software changes of a software system apireccan identify and focus on changes features that are relevant to api recommendation therebyreducingtheimpactofthefeatureirrelevanceproblem mentionedearlier.nevertheless theapplicabilityof apirecisseverely limited by the large number of historical software change authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france xiaoyu liu liguo huang and vincent ng repositories it requires.
specifically not only does it need to be trainedon changed source code files and change commits but it can only be appliedto files with a similarly long change history.
ourgoalinthispaperistoadvancethestate of the artinapi recommendation specificallybyimprovingthetop 1apirecommendationaccuracy.inviewoftheaforementionedlimitationsof apirec wedesireanapproachthatdoes notrelyoncodechange history.
the design of our system recrank is motivated by akey observation while gralanis unable to achieve a high top recommendation accuracy it achieves a reasonably high top recommendationaccuracy .
.
.giventhisobservation we take the top api candidates identified by gralanas our starting point and re rankthese candidates so that the correct api surfaces tothetopofthelist.thequestion then is howshouldwere rank?
recall that a key weakness of gralanconcerns the use of a generativemodel whichissensitivetothepresenceofoverlappingand irrelevant features.
recrank is specifically designed to address this weakness.first recrankemploysa discriminative re rankerthat is trained to re rank gralan s top candidate apis.
the key advantage of a discriminative approach over a generative approach is that the former can automatically discriminate relevant from irrelevant features by assigning high weights to the relevant ones andlowweightstotheirrelevantones .second weproposeanovel kind of features for use in conjunction with our discriminative re ranker apiusagepath basedfeatures.thesefeaturespartially address the feature irrelevance problem and can arguably better capturethelinguistictopicoftheprogramexpressingtheintention of the developer.
in sum our contribution in this paper lies in the proposal of recrank a novel discriminative ranking approach that employs a novelkindoffeaturesbasedonusagepathstoautomaticallyrecommend top apis based on the top api candidates suggested by gralan.
in an evaluation on eight large scale open source projects recrank outperforms apirecwith respect to two evaluation metrics top recommendation accuracy and mean reciprocal rank mrr a commonly used metric for evaluating ranking tasks in information retrieval achieving state of the art results.
preliminaries and motivating examples .
graph based generative api recommendation gralan sincerecrankisbuiltuponthetop 10apicandidatessuggestedby gralan we will provide an overview of gralanin this subsection.
as mentioned before given a recommendation point gralanrecommends an api using its preceding context i.e.
the code that has beenwrittensofar .1gralanencodestheprecedingcontextasa set ofapi usage graphs.
in an api usage graph each node is an api used in a method call operator overloading field access or branching e.g.
if while for etc.
.allapinodesareconnectedby directededges.eachedgerepresentsadataflowdependency i.e.
1the reason that only the preceding context is used is to mimic the realistic situation thatwhenanapiistoberecommendedtoadeveloper onlythecodethathasbeen written so far is available.
a child graph b parent graph figure parent child graph example overloading operator method calls and field accesses or a control flow dependency i.e.
condition and repetition between two apis.
anexampleofanapiusagegraphisshowninfigure1 a where nodenistherecommendationpoint.thecorrespondingcontext graph i.e.
thegraphthatencodesthecontextinwhichnoccurs is shown in figure b .
as can be seen this context graph is created by removing node n as well as all of its incoming and outgoing edges.throughoutthepaper iftwographs e.g.
theonesshownin figure have a parent child relationship we will refer to the one withouttherecommendationpointastheparentgraphandtheone with the recommendation point as the corresponding child graph.
asmentionedbefore gralanusestheparentgraphforpredicting the api at the recommendation point.
one way to make use of the parent graph is to estimate the probability that a candidate apico occurs with the parent graph in the training data.
the higherthe co occurrence probability is the more likely that the candidateapiisthecorrectapi.however aparentgraph suchasthe one shown in figure b could be fairly complex.
complex parentgraphscouldyieldadatasparsityproblem themorecomplex a parent graph is the less likely it will be seen in the trainingdata.
to alleviate data sparsity gralanalso makes use of all the non empty subgraphsoftheparentgraphintheapiprediction process.
for instance from the parent graph in figure b we canextractsubgraphs withoneapi e.g.
control.while subgraphs with two apis e.g.
subgraphs with three apis e.g.
filewriter.
init bufferedreader.
init control.while and subgraphs with four apis e.g.
filewriter.
init bufferedreader.
init bufferedreader.readline control.while .
specifically given a parent graph and subgraphs 1 ... nof gralancomputes the probability of a child graph c which is authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
effective api recommendation without historical software repositories ase september montpellier france created by filling the recommendation point with a candidate api using bayesian statistical inference as follows log pr c 1 2 ... n log pr 1 c ...pr n c pr c n summationdisplay.
j 1log methods j c log methods c n log methods c methods log methods wheretheexpressioninthesecondlineisobtainedusingbayesrule andthethirdlineshowshowtheprobabilitiesinthesecondlinecan beestimated.specifically methods c isthenumberoftimes appears as the parent of c in the training data methods is thenumberoftimes appearsinthetrainingdata and methodsis thetotalnumberofmethodsinthetrainingdata.2toavoidfloating underflow logarithm log is applied to all the probabilities in the equation.toassignnon zeroprobabilitiestoeventsnotseeninthe training data a smoothing factor i.e.
is used.
note that each of the graphs being conditioned on in equation i.e.
1 ... n can be viewed as a feature used by gralanin the recommendation process.
because the i s are subgraphs of these features are by design overlapping which could harm the performance of a generative model like gralan as noted in the introduction.
thereisanothercaveat.recallthatfigure1 a onlyshows one of the many api usage graphs that gralangenerates for the recommendation point n. the exact number of api usage graphs that gralangenerates for a recommendation point depends on a parameter d which specifies the maximum distance between the recommendationpointandanyofthenodesinanapiusagegraph.
for instance if d gralanwill generate allapi usage graphs that can possibly be generated by including any subset of nodes whose distance is no larger than from the recommendation point.
for eachoftheseapiusagegraphs gralangeneratesthecorresponding parent graph.
given each parent graph and its subgraphs 1 ... n gralancomputes the probability of each child graph c using equation .
the candidate api that corresponds to the most probable child graph over all the parent graphs will be the api recommended by gralanfor a given recommendation point.
.
motivating example wemotivatethedevelopmentofrecrankthroughthefollowing example.
a developer is developing a software function to readtext from a .txt file input.txt and write the processed text toanother .txt file output.txt .
the code snippet is shown in fig ure in which the input text file input.txt is read using java developmentkit jdk api bufferedreader line6 andwrittento output.txt using jdk api bufferedwriter line .
a while loop is used to iteratively read each line of the input text file line .
now this developer needs to decide what api should be used in line to write to the output.txt file.
modern integrated developmentenvironment ide tools suchaseclipse providealistof 2our training data is composed of the set of api usage graphs generated from all the methods in the source code collected from open source projects see section .
for details .
figure a code snippet figure an api recommendation example from eclipse apis for developers to choose.
this list of methods and fields is usuallyrankedinalphabeticalordersinceitsimplyshowsallmembermethods fieldsofthecallingapi.figure3showsthateclipse recommends16apisforline12infigure2.notethatthesemember methods and fields of the calling api bufferedwriter arenotprioritizedbasedonrelevance theyaresimplylistedinalphabetical order.
sincethedeveloperstillcannotdecidewhichapitochoosefrom thelist recommendedby idetools shewould liketoask forhelp fromgralan.asaforementioned gralanrankscandidateapisby theprobabilitiesofthecorrespondingchildgraphsgivenaparent graph and its subgraphs.
specifically it starts by building a setofapiusagegraphs suchastheoneshowninfigure1 a ofthe code snippet in figure .
for each of the api usage graphs gralan extractsthecorrespondingparentgraphand its subgraphs.these context graphs are then used in calculating the probability of each candidate api using equation .
however not all context graphs arerelevanttotherecommendationpoint.inotherwords notall context graphs implement the same linguistic topics as that of the recommendation point.
for example the recommendation pointn in figure a implements the linguistic topic write to outputtext file with its context graph in the green rectangle while its context graph in the blue rectangle implements the linguistic topic read from input text file .
however based on equation this contextgraph isconsideredasimportant asothercontext graphs likeothergenerativemodels theoneemployedby gralanmerely multipliestheprobabilitiesassociatedwiththeparentgraphand all of its subgraphs.
table1showsafewexamplesoftheparentgraphs i.e.
and their corresponding child graphs i.e.
c for the code snippet in figure2aswellastheprobability i.e.
score ofeachchildgraph.
the scores of the child graphs over all of the parent graphs are authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france xiaoyu liu liguo huang and vincent ng table probability scores of candidate apis c candidate api score filereader.
init bufferedreader.
init bufferedreader.readline control.whilefilereader.
init ... bufferedreader.closebufferedreader.
close0.
filereader.
init ... control.while bufferedwriter.writebufferedwriter.
write .
... ... ... bufferedwriter.
init control.whilebufferedwriter.
init control.while buffered writer.writebufferedwriter.
write .
bufferedwriter.
init control.while buffere dreader.closebufferedreader.
close .
... ... ... control.whilecontrol.while buffere dreader.closebufferedreader.
close0.
control.while bufferedwriter.writebufferedwriter.
write .
... ... ... ... ... ... ... calculatedandsorted.aswecanseeintable1 eventhoughapi bufferedwriter.write is the correct api for the recommendation point n gralanrecommended bufferedreader.close since it has the highest score i.e.
.
.
the main reason behind this miss isthatbufferedreader.close co occurredmorefrequentlywiththe irrelevant context graph in the blue rectangle in figure whichimplements the linguistic topic read from input text file rather than the topic that corresponds to the developer s intent write to output text file .
note that this is just oneexample of an irrelevant feature employed by gralan because of the way parent graphs aregeneratedfor arecommendationpoint many ofthem aswell as the subgraphs generated from them are irrelevant.
together with the overlapping features these irrelevant features could harm gralan s performance.
then the developer decides to try a state of the art approach apirec .thekeyideabehind apirecistoleveragetheregularity and repetitiveness of api usage patterns learned from software change history.
it assumes that the changes that serve the same higher level intent will co occur more frequently than unrelated changes .
in other words those apis in the context graphs that have a higher frequency of source code change co occurrence andhenceareassumedtohaveahigherpredictivepowerinapirecommendation will be given more importance in the api recom mendation process.
for each candidate api apirecfirst computes a score based on the change history and then adds the resulting score to the one computed by gralanto form the final score.
notallchangesareapplicable however sincesomeofthemcould be specific to a historical project and could therefore incur noise in the change patterns.in the example in figure a after analyzing a large number of historical fine grained changes apireclearned thatbufferedwriter.write changed with bufferedreader.
init with a probability of .
and that it changed with filereader.
init withaprobabilityof0.
.meanwhile italsolearnedthat bufferedreader.close changedwith bufferedreader.
init withaprobability of0.7andthatitchangedwith filereader.
init withaprobability of0.
.hence usingonlythecodechangehistory apirecwillselect the wrong api bufferedreader.close for the given recommendation pointsinceitsprobabilityofchangeco occurrence .
.
.
is larger than that of bufferedwriter.write .
.
.
.
in otherwords usingthecodechangehistory apireccannotoverride gralan serroneousrecommendationforthisrecommendationpoint.
in addition apirecrequires a long source code change history ofeachsubjectproject whichlimitsitsapplicabilitytoscenarios where long code change history is unavailable or inaccessible.
toaddressthechallengeofaccurateapirecommendation we propose recrank which recommends apis based on the api us age paths generated from api usage graphs.
an api usage path henceforth usage path is generated to represent a data control flowsequenceofapisthatcanarguablybetterencodetheintentionofthedeveloper.usingdiscriminativelearningincombinationwith usage paths as features higher weights can be learned for usage paths that are more relevant and coherent to the given recommendation point thereby reducing the noise possibly introduced by irrelevantorincoherentusagepaths.forexample infigure1 a we extract one usage path filewriter.
init bufferedwriter.
init recommendation point from the code snippet in lines in figure .
this usage path implements the linguistic topic write to output text file which is weighted higher than other usage paths extracted in figure a .
since recrank seeks to improve the accuracy of recommending the top api it could save the developer s timeandeffortinmanuallyselectingthecorrectapifrommultiple candidates.notethatrecrankseekstoachievethisgoal without mining and using long fine grained code change histories.
discriminative re ranking for api recommendation recrank .
overview inthissection wepresentanovelapproachtoapirecommendation recrank which operates by re ranking the top candidate apisrecommendedby gralanforeachrecommendationpointusing a learned discriminative re ranker in combination with our usage path based features.
before describing recrank we present two re ranking systems that could help the reader better understand the power of discriminative re ranking.
the first re ranking system is trained using the na ve bayes nb generative model onourusagepath basedfeatures.thesecondre rankingsystem isadiscriminativeclassifiertrainedusingthesupportvectormachinelearner henceforthsvc onourusagepath basedfeatures.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
effective api recommendation without historical software repositories ase september montpellier france theperformancedifferencebetweenthenbsystemandthesvc systemcanshedlightsontherelativeeffectivenessofgenerative models which are sensitive to the presence of overlapping and irrelevant features and discriminative models which are robust to such features.
note that the svc system is one step closer to recrankthanthenbsysteminthesensethatbothsvcandrecrank are discriminative in nature the primary difference between them lies in the fact that svc recasts the api recommendation task as a classification task whereas recrank recasts the task as a ranking task.
the performance difference between them can therefore shed lightsontherelativeeffectivenessofclassificationandranking.we willdiscussthedifferencesbetweenclassificationandrankinglater in this section.
.
nb innb weemploythena vebayeslearningalgorithmimplemented in scikit learn python library to train a binary classifier to classify whether a given recommended api is the correct api at the recommendationpoint i.e.
a hit ornot i.e.
a miss .recallthat nb employs the following generative model p c candidate api p c n summationdisplay.
i 1p fi c wherecis the class which in our case is either hit or miss and each ficorresponds to a usage path based feature extracted for the candidate api under consideration.
as can be seen thenb generative model assumes that the values of the usage pathbasedfeaturesareconditionallyindependentofeachothergiven theclass.eachoftheprobabilitiesinthegenerativemodelcanbe estimated using maximum likelihood estimation from the training data.
specifically p c is the fraction of instances in the training set that are labeled as c.p fi c is the fraction of training instances labeled as cthat contain feature fi.
weemploythetrainednbmodeltore rankthetop 10candidate apis suggested by gralanas follows.
since the model computes for each candidate api the probability that it is a hit we rank the candidateapisusingtheirassociatedprobabilities wherehigher probabilities correspond to higher ranks.
next we discuss how the training instances are created and how the usage path based features are extracted for each training instance.
creating training instances.
for each api recommendation point inthetrainingset wecreateonetraininginstanceforeachofthe api candidates recommended by gralan labeling an instance as hit or miss depending on whether the corresponding candidate api is the correct api for the recommendation point under consideration.eachinstanceisrepresentedusingasetofusagepath based features eachofwhichcorrespondstoanusagepath.thissetof usage paths is the union of the usage paths extracted from each of the api usage graphs created for the given recommendation point seesection2onhowtheseapiusagegraphsarecreated .below we define usage paths.
each usage path is extracted from an api usage graph and is definedbythreeconstraints.first ausagepathisformedbyasequenceofapisconnectedbydirecteddataand orcontrolflowedges.second theapisinausagepatharesequentiallyconnected listedinapi usage order with one entry api and one exit api.
finally each usage path contains a candidate api one of the candidate apis recommended by gralan that appears either at the end where the directed flow ends or at the beginning where the directed flow starts of the path.
usage paths of various lengths could be generated from an api usage graph.
the length of a usage pathis between and the threshold parameter d which determines the maximum distance between any node and the recommenda tion point in the graph defined in section .
.
for example 13usage paths can be generated from the api usage graph in fig ure a such as filereader.
init bufferedreader.
init bufferedreader.readline control.while candidate api .
to model different data control flow in usage paths we have designed different types of usage path features as described below.
aforwardusagepath feature is created from a usage path in which the apis in the path are connected by edges in the pointforwarddirectionwiththecandidateapiappearingattheendof thepath.aforwarddata controlflowtowardstherecommendation point usually implies that the api at the recommendation point consumes the data passed by data control flow.
as an example consider the api usage graph in figure .
from this graph we can createaforwardusagepathfeature fromthepath filereader.
init bufferedreader.
init bufferedreader.readline control.
while candidate api .
we create forward usage path features frompathsofdifferentlengths wherethelengthofapathisdefined as the number of apis involved in the path.
for instance the path is oflength2.
weconsider all paths of up to length d. abackward usage path feature iscreatedfromausagepath thatstartswiththecandidateapi andinwhichtheapisareconnected by edges with a point backward direction.
a backward data control flow from the recommendation point usually impliesthattheapiattherecommendationpoint produces or returns thedatatobedeliveredtotheapisinthebacktrack.infigure1 a wecancreatea backwardusagepathfeature fromthepath .
similar to forward usage path features backward usage path features are generated from paths of different lengths.
in addition we derive fuzzy usage path features from the forwardusagepathfeatures andthebackwardusagepathfeatures .
to motivate fuzzy usage path features we note the correspondence betweentheseusagepathsandtheword n gramsusedinnatural language processing nlp .
specifically the sequence of apis in aforward backwardusagepathisreminiscentofthesequenceof w or dsinaw or dn gram.nlpresearchershavenotedaweakness of using word n grams as features in natural language learning ifnis large the resulting n grams will suffer from data sparsity and ifnis small the n grams will fail to capture longer distance dependencies.toaddressthisweakness theyhaveproposedthe use ofskipgrams in which they allow all but the first word and the last word in an n gram to match any words.
for instance given the word n gram iamaboy onecangenerateaskipgram i boy where each wildcard can match any word.
this provides generalization of the original n gram and therefore addresses data sparsity but at the same time captures the relationship between non adjacent words in this case i and boy .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france xiaoyu liu liguo huang and vincent ng fuzzyusagepathfeatures aremotivatedbyskipgrams.specifically afuzzyusagepathfeature iscreatedfromaforward backward usagepathfeaturebyreplacingallbuttheentryapiandtheexitapi inthecorrespondingpathwithwildcards.returningtotheexampleinfigure1 a filereader.
init control.while candidateapi isafuzzyusagepathfeaturewithtwo fuzzy apis in the path represented as .
as in skipgrams wildcards i.e.
fuzzyapis canonlyappearinthemiddleofafuzzyusagepath.as with skipgrams the goal of these fuzzy path features is to provide generalizations of the forward backward usage path features.
incomparisontotheparentgraphsandsubgraphsthat gralan uses as features our usage paths are arguably more relevant to api recommendation.
first since each usage path has to begin orendwithacandidateapi itensuresthatthepathcontainsan api that is immediately adjacent to the candidate api thereby increasing its relevance for api prediction.
in contrast a subgraph employedby gralanmaynotcontainanynodesthatareadjacentto therecommendationpoint thuspossiblymakingitlessrelevantfor api prediction.
second from the example in figure each context graphcanpotentiallycontainmorethanonelinguistictopic e.g.
both read and write to a file .
on the other hand a usage path cantypicallyallowustofocusonjustonelinguistictopic.thisis especially important when it comes to discriminative learning adiscriminative learner can assign high weights to those features that encode the intended linguistic topic and low weights to those features that do not.
if context graphs encoding multiple linguistic topics were used as features the learner could find it difficult todecide whether high or low weights should be assigned to suchfeatures.
note that the computation of these usage path features can be doneoffline i.e.
during training with theresulting values stored in a database.
during testing their values can simply be retrieved from the database.
afinalissuethatwehaveeludedsofarconcernshowweobtain gralan stop 10candidateapisonthe trainingset.recallthatwe createonetraininginstancefromeachof gralan stop 10candidate apis.
this means that before we can create training instances we need toproduce gralan s top candidate apis on the training set.
we do so using fold cross validation on the training set we partition the training set randomly into five folds of roughly equal sizes.
in each fold experiment we train gralanon four folds andapplyingthetrained gralantogeneratethetop 10candidate apis on the remaining fold.
we repeat this five times each time generating top candidates on a different fold.
applying the nb classifier.
test instances are created in the same way as the training instances.
specifically we create one test instance from each of gralan s top candidate apis.
this means thatbeforewecreatetestinstances weneedtoproduce gralan s top 10candidateapisforeachrecommendationpointinthetest set.todoso wetrain gralanontheentiretrainingsetandapply the trained gralanto generate top candidate apis on the test set.
asmentionedbefore theresultingnbclassifiercanbeusedto compute the probability that each candidate api is a hit for a recommendationpoint.theseprobabilitiesarethenusedtore rank the candidate apis.
.
svc oursecondre rankingsystem svc isadiscriminativeclassifier trainedusingthe svmlearningalgorithmwithalinear kernel as implemented in the libsvm software package .
as in nb we first use cross validation on the training set to produce gralan s top candidate apis on the training set and then create one traininginstancefromeachofthe10candidateapis.eachtraining instance in svc is represented using the same set of usage pathbasedfeaturesasinnb.theonlydifferenceliesinthevalueofeach feature.
as nb is generative each feature is conditioned on the class.incontrast svcisdiscriminative sowedesirethatthevalueofafeatureprovidessomeindicationofhowusefulitis.specifically wedesirethat higherfeaturevaluesimplymorerelevant features.
to this end we compute the value of a feature as follows.
first we countthenumberoftimesthecorrespondingusagepathappearsin the training set call this number a .
second we count the number of times the path appears in the training set afterremoving from it thecandidateapi callthisnumber b .finally wesetthefeature value tob a. in other words the more often the candidate api cooccurswiththerestofthepathinthetrainingset thelargerthe featurevalueis.asinnb thevaluesoftheseusagepathfeatures canbecomputedandstoredinadatabaseduringtraining andthey can simply be retrieved from the database during testing.
trainingthesvcclassifier.
giventhetraininginstances thesvm learnerlearnsamaximummarginhyperplanethatminimizesthe training error i.e.
the error of the hyperplane in classifying the traininginstances .ahyperplaneisdefinedbyasetofweights eachof which is associated with exactly one feature.
in other words thesvmlearnerlearnsasetoffeatureweightsthatminimizestraining error specifically by associating larger absolute weights with relevantfeaturesandlowerabsoluteweightswithirrelevantfeatures.
thisdistinguishesadiscriminativelearnerfromagenerativemodel such as nb.
applyingthesvcclassifier.
aftertraining theresultinghyperplanecanbeused toclassifythetestinstances which arecreated inthesamewayasthetraininginstances.asinnb gralan stop10 candidates on the test set are obtained by training gralanon the entire training set and applying the trained gralanon the test set.
we re rank the top candidate apis based on their distances from the hyperplane.
specifically the candidate api on the hit sideofthehyperplanethatisfarthestawayfromthehyperplane receives the highest rank whereas the one on the miss side of the hyperplane that is farthest away from the hyperplane receives the lowest rank.
.
recrank next we describe recrank which differs from svc in one respect svcclassifies candidate apis whereas recrank ranks candidate apis.tounderstandthedifferencebetweenclassificationandranking we first note that api recommendation is inherently a ranking task itsgoalistocompare rankcandidateapisandpickthebest i.e.
highest ranked candidate api for a given recommendation point.
when applying svc we essentially recast api recommendation as a classification task where each candidate api is classified as hit or miss independently of other candidate apis.
in other words svc does notcompare candidate apis against each other authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
effective api recommendation without historical software repositories ase september montpellier france table dataset statistics training test total projects total classes total methods total distinctive jdk api elements total recommendation points average features per api candidate andwithoutsuchcomparisons itfailstodeterminewhichcandidate api is the best.
in contrast the goal of ranking is precisely to compare candidate apis by imposing a ranking on them.
trainingrecrank.recrank trainsansvmrankerusingthelinearkernel ranker learning algorithm implemented in svmrank .
thetraininginstances andthefeaturesthatrepresenteachtraining instance are created in the same way as in svc.
the resulting traininginstancesarethengroupedintodifferentrankingproblems.
specifically each ranking problem corresponds to exactly one recommendationpointandiscomposedofthe10traininginstances correspondingtothetop 10candidateapisforthisrecommendation point.
the goal of the ranker training procedure is to learn a hyperplane by adjusting the feature weights to minimize the numberofviolationsofpairwiserankinginthetrainingset.specifically a violation occurs if a training instance labeled as hit is ranked below a training instance labeled as miss by the ranker.
applying recrank.
after training the ranker can be used to directly rank the top candidate apis for each recommendation point in the test set.
specifically the ranker assigns each candidate api a value based on which a ranking can be imposed on the candidate apis.
recrank then recommends the candidate api that has the highest rank.
empirical evaluation .
experiment setup datasets.
wecollectedalargedatasetof1385javaprojectsfrom github for training api recommendation systems and another eight for evaluation.
statistics of this dataset are shown in table .
in order to obtain high quality api usage graphs we follow previous work we filter out the projects that are not parsable experimentalortoyprograms.also weuseonlythelatestsnapshot of each project.
for generalization purposes we focus solely on java development kit jdk apis.
to facilitate comparison with previouswork theeightprojectsinourevaluationsetarethesameasthoseusedtoevaluate apirec astate of the artapirecommendationsystem .trainingandtestrecommendationpointsare created from these projects in thesame way as in previous work except for the first two apis in each method we create one recommendation point for each api.
evaluation measures.
we employ two evaluation measures top 1accuracyandmeanreciprocalrank mrr .top 1accuracyisameasureusedinpreviousworkonapirecommendation .foreachapirecommendationpointinthetestset ifthetop apicandidatereturnedbyasystemisthecorrectapiattherecommendationpoint wecountitasa hit .thetop 1accuracyisthetable re implemented and original gralanresults projecttop accuracy top accuracy dupli catedgralanorigingralan errordupli catedgralanorigingralan error antlr .
.
.
.
.
.
galaxy .
.
.
.
.
.
froyoemail25.
.
.
.
.
.
gridsphere31.
.
.
.
.
.
itext .
.
.
.
.
.
jgit .
.
.
.
.
.
log4j .
.
.
.
.
.
spring .
.
.
.
.
.
ratio of the total number of hits to the total number of recommendation points.
mrr is an evaluation measure commonly used in informationretrievaltoevaluatesearchresults.liketop 1accuracy ascoreof1isgiventoarecommendationpointforwhichthetop 1candidateisthecorrectapi.unliketop 1accuracy whereasystem isnotrewardedatallifthecorrectapiisnotthetop 1candidate api mrr partially rewards a system as follows a score of1 ris giventoarecommendationpointifthecorrectapiappearsinrank r.inotherwords the partial rewardisinverselyproportionalto the rank of the correct api.
mrr then averages the scores over the recommendationpointsinthetestset.thus mrrcanbeviewed as a relaxed version of top accuracy that partially rewards a system where the correct api is not the top candidate.
since we are re ranking gralan stop 10candidateapis recommendationpoints where the correct api is not in gralan s top will receive a score of .
baselinesystems.
weemploytwobaselinesystems neitherof whichispubliclyavailable.asourfirstbaseline weemploy apirec.
theapirecresults reported in this paper are taken verbatim from the original apirecpaper .
asoursecondbaseline weemploy gralan.sincenb svc and recrank are all built upon gralan s top candidate apis we re implement gralan following the steps mentioned in section .
.
specifically we first build the api usage graphs from the collected1385opensourceprojectsinthetrainingset.then following nguyenetal.
foreachapiusagegraphwesimulatetheapi recommendationprocess by predictingeachapi givenitspreceding context.
we set the parameter dto meaning that only the context graphs involving the one two or three apis preceding arecommendation point are considered.
the reason for setting d to3isthataccordingtonguyenetal.
whend thetop accuracyachievedby gralan .
isclosetothebestaccuracy .
.
table3comparestheoriginal gralanresults withourduplicated re implemented gralanresultsonthesameeightsubjectprojects.
3thereasonwedidnotre implement apirecisthatthesignificantlargehistorical changerepositorydataset i.e.
103changecommitsand471 730changedsource code files according to nguyen et al.
is hard to acquire.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france xiaoyu liu liguo huang and vincent ng as can be seen duplicated gralanachieves better or comparable top 1and top 10accuraciesthan original gralanresultsacross all projectsexcept froyo email top .
notethata strictcomparison isnotpossibleowingtothefactthatoriginal gralanandduplicated gralanwere trained on different projects.
evaluation settings.
for nb svc and recrank we use the projects in the training set for model training.
we conduct an foldcrossvalidationonthe8projectsinthetestsetasfollows.in eachfoldexperiment weholdoutexactlyoneprojectfortesting andusetheremaining7projectsfordevelopment i.e.
parameter tuning .
we repeat this experiment times each time choosing a different project as our held out test set.
for parameter tuning we tune nb s laplace smoothing parameter as well as libsvm andsvmrank s regularization parameter c to maximize the top accuracyonthedevelopmentset.welimitthelengthanusagepath based feature to no more than .
.
experimental results this section empirically answers our research questions.
rq1.how accurate do recrank nb and svc recommend apis in comparison to the two baselines?
results of nb svc recrank and the two baselines apirec andduplicated gralan a.k.a.
d gralan expressed in terms of perproject and overall top accuracy and mrr are shown in table .
as we can see our proposed approaches rows outperform d gralan row2 inbothtop 1accuracyandmrracrossallsubject projects.
in particular recrank is the best performer in terms of both measures achieving better top accuracy than apirecin all eight projects.
we further make several interesting observations.
first the proposed learning based approaches nb svc and recrank achieve better top accuracy and mrr than d gralan top accuracy improvesby0.
andmrrimprovesby0.
.
.comparedto apirec stop 1accuracy .
svcandrecrankachievecomparable or better results i.e.
.
and .
respectively .
encouragingly recrank improves the state of the art top accuracies across all eight subject projects by .
.
.
todeterminewhethertheimprovementsinoveralltop 1accuracy and overall mrr between recrank and other approaches are statistically significant or not we conduct the wilcoxon rank sum test.followingmiller theresultofasignificancetestcanbe interpreted as follows.
the performance difference between the twosystemsundercomparisonis highlysignificant ifthenull hypothesis i.e.
thereisnoperformancedifferencebetweenthetwo systems can be rejected at the .
level represented as in the table significant if it can be rejected at the .
level represented as and moderately significant if it can be rejected at the0.1level representedas .otherwise thedifferenceis statistically indistinguishable.
as can be seen in table recrank is either highlyormoderatelysignificantlybetterthanothersystems.6to evaluatetheamountofperformancedifferencebetweenrecrank andeachoftheotherapproaches wecomputecliff sdelta a non parametric effect size measure.
results show that in each case 4the list of projects used to train original gralanis not revealed by the authors.
5mrr results are missing for apirecbecause they are not reported in the original paper.
6significancetestscannotbeconductedon apirecbecausewedonothaveitsoutput.the delta value is greater than .
which according to romano et al.
implies a large effect size.
rq2.how effectiveare usagepath featuresfor apirecommendation compared with context graphs?
to compare the effectivenessof these twotypes of features we employthemtotrainfourapproaches recrank nb svcand dgralan.thisresultsintheeightcombinationsshownintable5.for instance recrank e is the variant of recrank trained using the usage path features whereas recrank c is the variant of recrank trained using context graphs.
note that the two variants within each of the four approaches differ only with respect to the feature set.
in particular the value of a feature is computed in the same wayinthetwo variantsofanapproach.forinstance thevalue of a feature in recrank c is computed in the same way as that in recrank e which was described in section .
.
as can be seen in table for nb svc and recrank the e variant is highly significantly better than the c variant in terms of both top accuracy and mrr with a large effect size.
theseresults provide suggestive evidence that the usage path features areconsiderablymoreeffectivethanthecontextgraph basedfeaturesforbothdiscriminativemodels svcandrecrank andthenb generative model.
the only exception is gralan where its c variantishighlysignificantlybetterthanitsevariant.wespeculate that context graphs were specifically designed by their original authors so that they could work well when used in conjunction with gralan s generative model but additional experiments are needed to determine the reason.
rq3.how effective are different classes of usage path features for api recommendation?
to answer this question we divide our usage path features into 12groupsbasedon whetherthepathisforwardorbackward whether the path contains fuzzy apis or not and the length of the path which could be or recall that we limit the length to no more than in section .
.
to determine the contribution of each ofthese 12groups offeatures torecrank s performance we conduct ablation experiments where in each ablation experiment were trainrecrankbyleavingoutoneormoreofthe12feature typesand measurethe performanceof there trainedrecrank on thetestprojects.intuitively thelargerthedropinperformanceis in an ablation experiment the more important the missing feature group s are as far as performance is concerned.
ablationresultsareshownintable6.foreaseofcomparison we show in row the results of recrank when all usage path features are used.
the remaining rows show the results when one or more ofthefeaturegroupsareremoved.incomparisontotherecrank thatusesalloftheusagepathfeatures performancedropshighly significantly with respect to both top accuracy and mrr in three cases whenthelength2forwardfeaturesareremoved whenallforwardfeaturesareremoved and whenalllength2features are removed.
interestingly removal of other feature groups does notresultinsignificantdropsinperformance.inparticular removal of any of the length and features causes little and sometimesno change in performance.
however it is important to note that this by no means implies that features of lengths and are not useful these experiments only suggest that the feature group that is being removed is not useful in the presence of the remaining features.
in other words if two feature groups encode redundant authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
effective api recommendation without historical software repositories ase september montpellier france table evaluation results of api recommendation systems system top accuracy overall antlrgalaxy froyo email grid sphere itextjgitlog4jspring 1apirec .
.
.
.
.
.
.
.
.
2d gralan .
.
.
.
.
.
.
.
.
3nb34.
.
.
.
.
.
.
.
.
4svc .
.
.
.
.
.
.
.
.
5recrank .
.
.
.
.
.
.
.
.
system mrr 1apirec 2d gralan .
.
.
.
.
.
.
.
.
3nb0.
.
.
.
.
.
.
.
.
4svc .
.
.
.
.
.
.
.
.
5recrank .
.
.
.
.
.
.
.
.
table evaluation results fordifferent model feature combinations combination overall top accuracy overall mrr 1recrank e .
.
2recrank c .
.
3svc e .
.
4svc c .
.
5nb e .
.
6nb c .
.
7d gralan e .
.
8d gralan c .
.
table feature ablation results of recrank system overall overall top acc mrr all features .
.
no length2 forward .
.
no length3 forward fuzzy .
.
no length3 forward no fuzzy .
.
no length4 forward fuzzy .
.
no length4 forward no fuzzy .
.
no length2 backward .
.
no length3 backward fuzzy .
.
no length3 backward no fuzzy .
.
no length4 backward fuzzy .
.
no length4 backward no fuzzy .
.
no backward .
.
no forward .
.
no length 3or4 no fuzzy .
.
no fuzzy .
.
no length2 .
.
information thenremovalofoneofthemwillnotcauselargedrops in performance.
in fact the usefulness of features of length and canbeseenwhencomparingthe nolength2forward resultsand figure learning curves of api recommendation approaches on the entire test set then no forward results the performance differences between thesetwoablatedsystemscanbeattributedtothelength3and4 features.specifically top 1accuracydropsbymorethan15 points and mrr drops by points when the length and features areremoved.similarly theusefulnessofthebackwardfeaturescan be seen by comparing the no length forward results and the nolength2 results theperformancedifferencesbetweenthese two ablated systems can be attributed to the backward features.
specifically top accuracy drops by points.
rq4.what is the learning curve of each system?
toanswerthisquestion figure4presentsthelearningcurvefor each of these four systems when measured in terms of top accuracy.eachcurveisplottedusingfivedatapointsthatcorrespond to using and of the available trainingprojects collected in section .
.
as we can see in none of the systemsdoestop 1accuracyplateauevenwhenweuseallofthe availabletrainingdataset.thisimpliesthattheperformanceofeach api recommendation system will likely to improve further as additionaltrainingprojectsaremadeavailable whichisencouragingas additional projects canbe easily obtained.
inaddition we observe thatsvcachievesconsistentlybetteroveralltop 1accuracythan d gralan regardlessoftheamountofavailabletrainingdata.nb achievesbetteroveralltop 1accuracythan d gralan whenmore authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france xiaoyu liu liguo huang and vincent ng than40 oftrainingprojectsareavailablefortraining.themost effective learner however is recrank.
threats to validity our main threats to internal validity can occur to our training and testsets.toaddressthisconcern wetrainallapirecommendation systems on the same training set.
then each system is evaluated on each subject project with parameters tuned on the rest of the seven subject projects.
in addition threats to external validity can occur during data collection.
for generalization purposes similar to previous works ourexperimentsareperformedonjdkapisonly.meanwhile for comparison purposes we run experiments on the same subject projects as the baseline api recommendation systems i.e.
gralan andapirec .
related work .
code suggestion based on mined software repositories inthissubsectionwesummarizesourcecodesuggestionapproaches based on mined software repositories.
bruch et al.
adapt the k nearest neighboralgorithmtofindmethodcallstorecommend for particular objects.
robbes et al.
use change history such as code insertion and modification to improve code completion.
hou et al.
present a way of grouping api proposals from historical data for better code completion.
hill et al.
build a tool to automatically complete a method by cloned code.
asaduzzaman et al.
and zhang et al.
both use parameter filtering to do code recommendation.
omar et al.
uses an interactive wayto generatecode.reiss etal.
andstolee etal.
usesemantic searchtomapretrievedcodeintowhatisaskedforbyusers.thung et al.
present an approach that learns from records of other changes made to software systems and compares the textual descriptionoftherequestedfeaturewiththetextualdescriptionsof various api methods.
wang et al.
propose two quality metrics succinctnessandcoverage forminedusagepatterns.xie etal.
presentmapotogeneratepatternsbyminingandrankingfrequent sequencesineachclusteraccordingtothesimilarityheuristicsof source code such as method names.
most of these approach rely on alarge number ofsoftware historical repositories.this kind of approach is not applicable when such repository is not available.
differentfromtheaboveapproaches recrankdoesnotrelyonany software historical repository.
.
code suggestion using statistical models thissubsectionsummarizessourcecodesuggestionapproachesusing statistical models.
gu et al.
adapt a neural language model named rnn encoder decoder which encodes a word sequence user query into a fixed length context vector and generates an apisequencebasedonthecontextvector.white etal.
apply the rnn lm model on lexically analyzed source code to code suggestion.allamanis etal.
presentnaturalize whichlearns coding conventionsto suggest naturalidentifier names andformattingconventions.theyalsoapplythebinomialmodeltoretrieve sourcecodesnippetsgivenanaturallanguagequeryandretrievenaturallanguagedescriptionsgivenasourcecodequery .maddisonetal.
useprobabilisticcontextfreegrammar pcfg based modeltorepresentsourcecode.mcmillan etal.
proposeacombinationofassociationbetweenqueriesandfunctionsmodeland navigation behavior of programmers model to retrieve and visualize relevant functions and their usages.
chan et al.
perform api recommendation based on the textual similarity between code and queryphrases.codehow expandsthequerywiththeapisand performs code retrieval by applying the extended boolean model which considers the impact of both text similarity and potential apisoncodesearch.mulapi recommendsfeaturerelatedapi from feature request documents.
in this paper we propose a novel ranking baseddiscriminativemodeltoimprovethestate of the art top api recommendation accuracy.
.
code suggestion based on code structure this subsection overviews source code structure based approaches.
asaduzzaman et al.
recommend api by ranking the similarities between code contexts and thecontext of the targetapi method call.
raychev et al.
extract indexed sequences of method calls and use a statistical language model to find the highest rankedsentences to synthesize a code completion.
mou et al.
p r o pose a tree based convolutional neural network tbcnn on ast treestructuretodetectcodesnippetsofcertainpatterns.holmes et al.
present an approach for locating relevant code that is based on heuristically matching the structure of the code under development to the example code.
saul et al.
use a random walk approach on asubset of a callgraph inorder to recommend source code.
ekoko et al.
propose an approach that leverages thestructuralrelationshipsbetween apistodiscoverinaccessible apimethodsortypes.mcmillan etal.
recommendsourcecode examples by querying against api calls and documentations about code structural information.
moritz et al.
present an approach to recommend api usage by representing software as a relational topicmodel.fowkes etal.
proposeaprobabilisticalgorithm to find the most informative and parameter free api call patterns.
in our approach recrank recommends api based on api usage graphs which includes data flow dependencies and control flowdependencies among apis.
and compared with the state of theartgraph basedapirecommendationapproach gralan recrank significantly improves the top accuracy of api recommendation.
conclusions and future work weproposedanoveldiscriminativere ranking basedapirecommendation system recrank which uses usage path based featurestorankthetop 10apicandidatesgeneratedby gralan.inanevaluationoneightlargescaleopensourceprojects recranksignificantly improved top accuracy by .
.
and mrr by .
.49in comparison to gralan.
when compared to apirec recrank improved top accuracy by as much as .
yielding an overall improvementof5.
absolute.perhapsevenmoreencouragingly we saw performance improvements in each of the eight projects.
importantly recrank does not require access to a large number of historicalcodechangesfortrainingandapplication.infuturework we will extend our approach on a wider spectrum of api types and experiment on additional projects.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
effective api recommendation without historical software repositories ase september montpellier france