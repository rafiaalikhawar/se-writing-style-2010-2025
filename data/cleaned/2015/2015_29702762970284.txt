processpair a tool for automated performance analysis and improvement recommendation in software development mushtaq raza inesc tec university of porto faculty of engineering rua dr. roberto frias s n porto portugal mushtaq.raza fe.up.pt jo o pascoal faria inesc tec university of porto faculty of engineering rua dr. roberto frias s n porto portugal jpf fe.up.pt abstract high maturity s oftware development processes can generate significant amounts of data that can be periodically analyzed to identify performance problems determine their root causes and devise improvement actions.
however conducting that analysis manually is challenging because of the pote ntially large amount of data to analyze and the effort and expertise required.
in this paper we present processpair a novel tool designed to help developers analyze their performance data with less effort by automatically identifying and ranking performance problems and potential root causes so that subsequent manual analysis for the identification of deeper causes and improvement actions can be properly focused .
the analysis is based on performance models defined manually by process experts and calibrated automatically from the performance data of many developers.
we also show how processpair was successfully applied for the personal software process psp .
a video ab out processpair is available in .
ccs concep ts software and its engineering software development process management keywords software process performance analysis improvement recommendation .
introduction software development processes making intensive use of metrics and quantitative methods such as t he team software process tsp and personal software process psp can generate large amounts of data that can be periodically analyzed by developers to identify their performance problems determine root causes and devise improvement actions .
although tools exist to automate data collection and produce performance charts and reports for manual analysis of tsp psp data practically no tool support exists to automate developer performance analysis.
the manual analysis of performance da ta for determining root causes of performance problems and devising improvement actions is challenging because of the amount of data to analyze and the effort and expertise required.
processpair is a novel tool designed to help developers analyze their performance data with less effort by automatically identifying and ranking performance problems and potential root causes so that subsequent manual analysis for the identification of deeper cause s and improvement actions can be properly focused .
the analysis is based on a performance model pm defined by experts in the process under consideration and calibrated automatically from the data of many process users.
in previous work we developed the overall technique pms specific for the psp and a prototype tool .
in this tool demonstration paper we present a significantly improved version of processpair available f reely in together with several tutorials and videos.
a video tutorial is available in .
the paper is organized as follows.
section presents the overall approach tool architecture and underlying metamodels .
section s and explain the model cal ibration and performance analysis process es and user interface s. section presents some experimental results.
some related work is presented in section .
section concludes the paper.
.
approach architecture and metamodels .
overall a pproach and architecture our approach involves three main steps see figure .
define process experts define the structure of a pm suited for the development process under consideration.
in our approach a pm comprises a set of performance indicators pis organized hierarchically by cause effect relationships .
.
calibrate the pm is automatically calibrated by processpair based on the performance data of many process users.
the statistical distribution of each pi and statistical relations between pis are computed from the d ata set .
.
analyze once a pm is defined and calibrated the performance data of individual developers can be automatically analyzed with processpair to identify and rank performance problems and root causes.
processpair currently comprises a core framework representing of the code base independent of the process under analysis and an extension for the psp representing of the c ode base sample permission to make digital or hard copies of all or part of t his work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
conference month city state country.
copyright acm ... .
.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
as depicted in figure .
other extensions may be easily implemented in the future for other processes.
the core framework comprises three layers a graphic al user interface layer at the top gui package a n intermediate logic layer responsible for the representation and manipulation of pms performancemodel and subject data under analysis subjectdata a layer with common utilities at the bottom statisti cs .
the psp extension pspextension contains the definition of pms for the psp and subject data loaders from the most relevant project management tools used by psp developers the sei s psp student workbook and process dashboard .
figure .
uml activity diagram depicting the main activities and artifacts in the processpair approach .
figure .
uml package diagram depicting the logical architecture of the processpair tool.
.
performance model m etamodel a pm for a development process under consideration is defined by means of the following information see figure set of relevant base measures generated by the development process with name description scale and units set of relevant pis with the same attributes as the base measures plus the formula for its computation from base measures and the optimal value usually implied by the definition of the pi subset of top level pis dependencies between pis representing cause effect relationships determined by a formula or statistical evidence sensitivity coefficients xi y y xi xi y i ... n for each pi y that depends on pis x1 ... x n according to a formula y f x ... x n .
for example our pm for the psp comprises three top level indicators time estimation accuracy process quality index and productivity.
the time estimation accuracy is computed from base measures as a ratio actualtim e estim atedtime being the optimal value.
since in the psp s probe estimation method a time estimate is obtained based on a size estimate of the de liverable in added or modified size units and a productivity estimate in size per time units we consider that the time estimation accuracy timeea is affected by or depends on the size estimation accuracy sizeea and the productivity estimation ac curacy prodea .
from their definitions we conclude that these pis are related by the formula timeea sizeea prodea so the sensitivity coefficients are sizeea timeea and prodea timeea .
sensitivity coefficients are used for ranking the causes of performance problems.
figure .
uml class diagram depicting the main concepts in the performancemodel package .
the pm is automatically calibrated by processpair from training data sets generating the following data also visible in figure approximate statistical distribution of each pi represented by a cumulative distribution function act process v iew processpair process expert1.
define performance model structure .
calibrate performance model .
analyze dev eloper performance dataperformance model structure performance indicators pis relationships between pis calibrated performance model statistical distribution of pis statistical relationships between pis performance analysis and recommendation report performance problems ranked root causesperformance data of a single dev eloperperformance data from many dev elopers pkg logical view extensions core gui statisticsperformancemodelsubjectdatapspextension class performancemodel performanceindicator formula numericfunction project optimalvalue double recommendedranges map interval semaphor approximatestatisticaldistribution statisticaldistributionperformancemodel name string dependency sensitivitycoefficient numericfunction project semaphor green yellow redmeasure name string description string scale interval units string basemeasure toplevelindicators childindicator parentindicator allindicators recommended performance ranges for each pi sensitivity coefficients between pis not related by an exact formula.
the approximate cumulative distribution function of each pi is computed by linear interpolation between a few percentiles computed from the training data.
performance ranges are needed for classifying values of each pi of a subject under analysis into thr ee categories semaphores green no performance problem yellow a possible performance problem red a clear performance problem.
such ranges are determined automatically from the statistical distribution of the training data so that there is an even distribution of data points by the colors.
sensitivity coefficients between pis not related by an exact formula are computed by first determining a linear regression equation from the training data and subsequently computing the corresp onding sensitivity coefficient.
.
subject data m etamodel the base performance data of a subject under analysis developer team or company that need to be uploaded by processpair consists in the values of the base measures defined in the pm for a sequence of projects see subject project and projectbasemeasure in figure .
based on that information processpair computes the following data for each pi and project see projectindicator and indicatorinstance in figure value computed from the base measures and pi s formula percentile computed from the previous value and the statistical distribution of the pi in the pm normalized so that corresponds to the opti mal value and corresponds to extreme values to the left or to the right of the optimal value semaphore computed from the previous percentile as follows green for the .
range yellow for the .
.
range and red for the .
range percentile coefficient computed from the percentile and the statistical distribution of the pi as explained in it is used as an indicator of the cost or difficulty of improving the value of the pi based on the idea that the closer a value is to the optimal the more difficult it is to improve.
summary information for each pi is computed at the subject level see subject indicator and indicatorinstance in figure minimum maximum average simple statistics calculated from the values computed at the project level percentile weighted average of the percentiles co mputed at the project level using an exponentially decaying weight for older projects with a configurable time constant semaphore and percentile coefficient computed from the previous percentile.
for each dependency defined in the pm and project it is computed the following information see dependencyinstance in figure sensitivity coefficient computed from the project data and the sensitivity formula defined in the pm ranking coefficient computed as the product of the previous sensitivity coefficient and the percentile coefficient of the child pi it is used to rank child pis based on a cost benefit estimate of improvement actions with the cost factor giv en by the percentile coefficient and the benefit factor given by the sensitivity coefficient ranking label a discretization of the ranking coefficient by order s of magnitude for user presentation purposes.
similar information is also computed at the subject level by summarizat ion and between leaf and top level indicators by using the laws of partial differentiation of composite functions for computing leaf to top sensitivity coefficients .
figure .
uml class diagram depicting the main concepts in the subjectdata package .
.
model calibration the user interface for performing the automatic calibration is shown in figure .
the user has to select the pm to be calibrated from the list of pms previously defined as tool extensions the file with the data set to be used for calibration in a format supported by the data loaders defined together with the pm and the xml file for saving the calibration results.
figure .
model calibration window .
in this example t o calibrate the psp pm we used a large psp data set from the software engin eering institute sei referring to projects concluded by engineers during classes of the classic psp for engineers i ii training courses running between and .
in this training course targeting professional developers each engin eer develops small projects.
processpair performs several data quality checks during the calibration process according to rules defined together with the class subjectdata project seqnumber int name string projectindicator value doublesubject projectbasemeasure value double subjectindicator minimum double maximum double average double enumeration rankinglabel very large large medium small very small dependencyinstance sensitivitycoefficient double rankingcoefficient double rankinglabel rankinglabel indicatorinstance percentile double sempahor semaphor percentilecoefficient doubleperformancemodel basemeasure dependencyperformanceindicator parentortopind11 .. childorleafind1 800pm and presents a summary of problems encountered at the end of the calibration process as illu strated in figure .
figure .
summary of calibration results .
instead of using the full dataset for calibration it is also possible to filter the data points to be used for calibration.
one possibility is to restrict the data points projects to the ones most similar to a given user profile as illustrated in figure .
the parameters that can be provided depend on the pm and data loader.
similarity is computed with the gower similarity coefficient .
in this example see figure only the most similar data points were selected minimum number required by the tool for statistical significance with a similarity co efficient greater than .
.
figure .
dialog for providing a user profile .
figure .
calibration results with filtering .
.
performance analysis having defined and calibrated the pm the performance data of individual developers can be automatically analyzed by processpair to identify and rank performance problems and potential causes.
as exemplified in figure the user has t o select the pm from the list of pms previously defined as tool extensions the calibration file generated as previously explained the type of input file with performance data to analyze according to the data loaders defined together with the pm and the file with the actual data.
by pressing the analyze file button the analysis is performed and the results are presented in multiple views.
figure .
entry window .
.
table v iew the table v iew figure shows the values of the pis defined in the model for the projects described in the input file as well as summarized performance information.
each cell is colored green yellow or red in case its value suggests no performance problem a potential performance problem or a clear performance problem respectively see calculations in section .
this w ay the table view h elps in quickly identifying the performance problems.
the exact ranges considered can be consulted in the indicator view .
the percentile all column shows an overall percentile for each pi computed from the per project values with higher importance for the last projects and col ored according to the percentile.
the pis are organized hier archically starting from the top level indicators time estimation accuracy process quality index and productivity in this case and descending to lower level indicators child indicators th at affect the higher level ones according to a formula or statistical evidence .
this way by drilling down from the top level indicators to the lower level one s focusing on the red or yellow colored cells one can easily identify potential root causes of performance problems.
figure .
table view example partially expanded .
.
report v iew the goal of the report v iew figure is to indicate in a simple way overall summary or project by project the most relevant top level performance problems colo red red or yellow in the table view and potential root causes l eaf causes in the cause effect view properly pr ioritized according to the ranking coefficients explained in section .
intermediate causes can be consulting by uncheck ing the show only leaf causes checkbox.
combob oxes allow selecting information for specific projects and or pis.
the links skip to the indicator view for d etailed information about each pi .
figure .
report view example .
.
indicator v iew the goal of the indicator view figure is to show the behavior of each pi along t he projects under analysis and provide associated model definition and calibration information description units optimal value recommended performance ranges and statistical distribution .
in the bottom left it is presented the statistical distribution of the pi in the data set used for calibrating the model.
the colors correspond to the performance ranges.
the actual values in the file under analysis are also shown marked with the symbol for benchmarking purposes.
the user may also select multiple pis for comparative visualization in a single chart.
figure .
indicator view example .
.
cause effect view the cause effect view figure is an advanced view that provides essentially the same information as t he report v iew with additional details but in a diagrammatic way.
the goal of the cause effect view it to help identifying and prioritizing project by project or overall the root causes of performance problems so that subsequent improvement actions can be properly directed.
the child indicators are sorted according to the value of the ranking coef ficient.
as explained in section t he ranking coefficient represents a costbenefit estimate that relates the cost of improving the value of the child pi with the benefit on the value of the parent pi.
intermediate causes may be consulted by un select ing the show only leaf causes checkbox.
by default the ranking coefficients are shown by means of t shirt sizes ranking labels .
the numerical values of the ranking coefficients can be consulted by selecting numerical ranking labels in a combo box.
figure .
cause effect view example .
.
experimental results two experiments have been conducted to evaluate processpair.
.
postmortem e xperiment the goal of the first experiment was to assess the accuracy of automatic performance problem and root cause identification with processpair .
to that end we used as input the psp performance data and final reports of master students from tec de monterrey in mexico that attended the software quality and testing course in .
in that course each student developed projects using the psp and collected base measures with process dashboard .
in the end of the seq uence of projects the students analyzed their personal performance in those projects and documented their findings and improvement proposals in a psp final report .
we compared the performance problems and root causes identified and documented by the stu dents in their final reports with the performance problems and root causes identified automatically by processpair from the students performance data.
regarding problem identification from the cases in which students explicitly characterized their performance regarding a specific pi and a specific project or all projects we compared the student assessment with the tool based assessment and got the following results in of the cases the results of manual and automatic analysis matched i.e.
both the student and the tool indicated good performance or bad performance in of the cases the tool indicated a clear or potential problem and the manual analysis indicated good performance false positives in of the cases the tool indicate d no performance problem but the developer explicitly indicated a performance problem false negatives .
for each performance problem identified both in manual and automatic analysis and with root causes explicitly pointed out by the students cases we compared the causes identified in manual and automatic analysis and got the following results in of the cases the tool and the developer pointed out the same causes tool benefit eliminate manual effort in of the cases the tool accurately pointed out intermediate causes and the developer pointed out deeper causes tool benefit reduce manual effort in of the cases the causes identified were inconsistent because of faults in manual analysis tool benefit prevent user errors .
these results show that processpair has indeed the potential to accurately identify performance problems and causes and consequently reduc e the user effort and errors in performance analysis.
.
controlled e xperiment the second experiment is a n ongoing controlled experiment involving master students from tec de monterrey in mexico that are attending the software quality and testing course edition in .
the main goal is to quantify the benefits of using processpair in performance analysis in te rms of time spent and quality of the results.
in their final assignment students were asked to analyze their personal performance along the psp projects and document their findings and improvement proposals in a psp final report .
to 802perform the assignme nt students were randomly split into two groups a control group and an experimental group.
the students in the control group did the final assignment in a traditional way by inspecting their performance data stored in the process dashboard tool through the standard psp forms charts and reports.
the students in the experimental group used processpair for analyzing their performance data.
upon completion of the assignment students in both groups responded a questionnaire containing some free text qu estions plus questions in a five point scale related with installability usability efficiency usefulness and level of support provided by the tool they used for conducting the performance analysis.
the average scores given by the students were as fol lows average score given by the students that used processpair .
in a scale of to average score given by the students that used process dashboard .
in a scale of to .
this shows a very favorable evaluation of processpair.
the tim e spent by the students in performing their final assignment and the grades given by their instructor still being collected will allow us to assess the benefits of processpair as compared to the traditional approach in terms of effort needed and quality of results produced.
.
related work our approach draws inspiration from existing work on process performance models ppm benchmark based approaches for software product evaluation and defect causal analysis dca techniques .
in the context of the cmmi process improvement framework a ppm is a description of the relationship among attributes of a process or sub process and its outcomes developed from historical performance data and calibrated using collected process and product measures .
the main difference is that our pm conveys additional elements needed to identify performance problems in the outcomes and rank po tential root causes factors recommended ranges for each pi approximate statistical distribution of each pi sensitivity coefficients derived from exact or regression equations .
in our approach in order to enable the automated identification of perfo rmance problems after deciding on the relevant pis one has to decide on the relevant ranges.
our approach for defining such ranges draws inspiration from the benchmark based approach developed by researchers of the software improvement group to rate the maintainability of software pro ducts with adaptations for process evaluation instead of product evaluation.
the dca approach is essentially complementary to our approach.
t he main advantage of our approach is that it has the potential to identify relevant performance problems and causes in a fully automatic way so that subsequent manual activities can be conducted in a more focused and efficient way to further determine root causes and devise improvement actions.
.
conclusions and future work we presented a novel tool processpair for automat ing the identification and prioritization of performance problems and root causes in software development and showed its successful application for the psp.
as future work we intend to add to processpair the capability of recommending detailed improvement actions for the identified causes of performance problems.
we also intend to apply processpair for other software development processes .
.