how v erified is my code?
falsification driven v erification alex groce iftekhar ahmed carlos jensen school of electrical engineering and computer science oregon state university corvallis oregon email agroce gmail.com ahmedi onid.oregonstate.edu cjensen eecs.orst.edupaul e. mckenney ibm linux technology center email paulmck linux.vnet.ibm.com abstract formal verification has advanced to the point that developers can verify the correctness of small critical modules.
unfortunately despite considerable efforts determining if a verification verifies what the author intends is still difficult.
previous approaches are difficult to understand and often limited in applicability.
developers need verification coverage in terms of the software they are verifying not model checking diagnostics.
we propose a methodology to allow developers to determine and correct what it is that they have verified and tools to support that methodology.
our basic approach is based on a novel variation of mutation analysis and the idea of verification driven by falsification.
we use the cbmc model checker to show that this approach is applicable not only to simple data structures and sorting routines and verification of a routine in mozilla s javascript engine but to understanding an ongoing effort to verify the linux kernel read copy update rcu mechanism.
i. i ntroduction software model checking has recently thanks to improvements in model checking tools and advanced sa t smt solvers become a potentially valuable tool for developers of critical software modules who want to either perform a very aggressive search for bugs or ideally prove correctness of their code.
tools such as cbmc the c bounded model checker allow a software engineer to model check code by writing what is essentially a generalized test harness in the language of the software under test sut .
figure shows an example cbmc harness for sorting routines.
only a few aspects differ from normal testing.
first nondet int in cbmc can return any value.
it is not equivalent to a random choice but true nondeterminism cbmc will explore all values of the type.
the cprover assume statement has the usual assume semantics cbmc ignores all executions that violate assumptions.
cbmc compiles a harness and the sut here a quicksort implementation into a goto program instruments this program with property checks for assertions array bounds violations etc.
and then unrolls loops based on a user provided unwinding bound to produce a sa t problem or smt constraint such that satisfying assignments are representations of a trace demonstrating a property violation known as a counterexample .
for cbmc this means that if any possible execution allowed by the harness violates any properties checked a counterexample will be produced.
this includes user specified 1by a harness we mean a program that defines an environment and the form of valid tests and provides correctness properties.
include sort.h int a int ref int nondet int int main int i v prev int s nondet int cprover assume s s size f o r i i s i v nondet int printf log ref d n i v ref v a v sort a s prev a f o r i i s i printf log a d n i a assert a prev prev a fig.
.
cbmc harness to check a sorting routine.
assertions and automatically generated properties such as array bounds and pointer validity checks.
one generated property is that no loop in the program executes more than the unwinding bound times.
for example if we run cbmc on the harness shown and set the unwinding bound to and add dsize we will check the correctness of the sut over all possible arrays of size or less including checking that sorting never requires passing through any loop more than times.
when a model checker produces a counterexample a developer s task is straightforward if sometimes difficult either the sut has a fault or the harness itself is flawed.
in both cases the output of the verification effort is the counterexample trace which is full of evidence as to the reason for the failure to verify the sut.
moreover any solution fix to sut or harness is easily checked if it is correct the model checker stops reporting the previous counterexample.
unfortunately model checkers do not invariably report counterexamples eventually the sut is likely to satisfy the properties encoded in the harness!
it is in this case that problems arise what precisely has been verified?
is the sut actually correct?
formal verification is not only subject to the problems that make no faults detected results dubious in testing but also to more subtle problems.
for example an incorrect assume statement may constrain a system so that not only are there no counterexamples there are no interesting executions of the system at all.
moreover formal verification tools are themselves extremely complex software artifacts and like production compilers may 30th ieee acm international conference on automated software engineering .
ieee authorized licensed use limited to access paid by the uc irvine libraries.
downloaded on january at utc from ieee xplore.
restrictions apply.
themselves have serious bugs that produce wrong results .
in the course of this research we have ourselves encountered several tool induced incorrect verifications.
the problem of checking verification results has concerned the model checking community for some time and resulted in efforts to define coverage metrics for model checking .
while such metrics are interesting and useful they have typically been aimed at hardware verification and most useful to experts in formal verification.
in this paper we adapt traditional mutation testing to the problem of software verification.
a mutant of a program is a version of the program with a small syntactic change.
the idea behind mutation testing is that a good test suite will be able to detect when as is usually the case such a change introduces a bug in the sut.
in the case of bounded model checking since we aim at bounded verification rather than merely good testing surviving mutants are likely to indicate a real problem.
in software engineering research mutation is often used only as a way to compare competing test suites by comparing kill rates .
this is not enough for verification.
the typically small scope of the code to be verified and the presumed importance of verified code suggests an approach in which individual mutants are examined by the developer.
without additional assistance such an approach cannot scale.
this paper aims to describe how to make this seemingly toodemanding approach practical for real verification tasks.
the contribution of this paper is a falsification driven verification methodology that uses mutants to aid developers in understanding successful verification results determining when a harness is flawed and correcting the harness.
we show how to use mutation testing to choose a problem size in bounded model checking how to mutate a harness to determine if any similar harnesses have an equal or better mutation kill rate and most importantly how to modify cbmc a harness and mutants to automatically produce successful high coverage executions covering mutated code in order to understand mutant and thus harness behavior.
this approach unlike a simpler method of searching for cases where the mutated and original code behave differently for identical inputs applies to verification of reactive and concurrent systems where there is no simple notion of identical inputs.
at a more general level we discuss the fundamental nature of verification in a real world context where specifications are never known to be complete.
we propose that falsification as in popper s philosophy of science is a useful conceptual framework for verification efforts rather than focusing on what can be proven about a program we focus on how a verification distinguishes the real program from similar alternative programs that do not match the theory of program behavior.
this approach still aims at verification but continually evaluates and refines a verification effort by its ability to falsify rather than to verify.
ii.
a s imple example verifica tion as an example of the proposed verification methodology consider again the harness shown in figure .
if we take an early google result for quicksort in c shown in figure we can model check it using the harness defining size 2in fact that actual code is incorrect with an access a that does not properly use short circuiting logical operators to protect array bounds cbmc detected this and we fixed it for this paper.
include sort.h void quicksort int a int l int r printf log called with l d r d n l r int j 9i f l r divide and conquer j partition a l r quicksort a l j quicksort a j r int partition int a int l int r int pivot i j t pivot a i l j r 26while do i while i r a pivot do j while a pivot if i j break t a a a a t t a a a a t return j void sort int a unsigned int size quicksort a size fig.
.
quicksort code.
and setting the unwinding bound to we need one more unwinding than the maximum number of items in the array .
cbmc reports verification successful in less than a second.
have we verified what we want to verify?
a. finding a good problem size the first question we face is whether is a good maximum array size to examine.
the problem of determining a completeness threshold an execution length bound sufficient to prove correctness in all cases for a given property for bounded model checking is fundamentally difficult and is for real world c programs more an art than a science at present3.
are there bugs for which is too small an array size?
in order to find out we generate a set of mutants for quicksort.c .
using the mutation tool for c code developed by jamie andrews we can produce mutants of this code in less than a second.
we then run the harness with unwinding bound and size on each of the mutants.
the process takes less than a minute and a half on a macbook pro with 16gb ram and dual core .1ghz intel core i7 using only one core .
cbmc reports that mutants do not compile these remove variable declarations for the most part are detected by the harness a counterexample is produced we say the mutant is killed and mutants pass without detection the verification is successful in which case we say the mutant survives .
clearly length arrays are not sufficient to detect even the most glaring bugs in a sort algorithm no surprise all size arrays are sorted .
what about our choice of size ?
re checking the mutants with this bound dropping those already killed by the smaller bound takes slightly over minutes due to one mutant requiring over minutes to model check and reduces the number of surviving mutants to .
we could inspect these mutants by hand but it seems 3in our own practice the most common way of setting it is to guess a bound and see if the resulting problem is too large for the available resources.
authorized licensed use limited to access paid by the uc irvine libraries.
downloaded on january at utc from ieee xplore.
restrictions apply.
rep op i f l r rep const while rep const while rep op do i while i r a pivot rep op do i while i!
r a pivot rep op do i while i r a pivot rep op i f i j break del stmt t a a a a t fig.
.
surviving mutants at size .
highly unlikely that a complete verification over all possible arrays with a good specification for sorting would produce such a poor mutation kill rate.
if we increase the size limit to and unwinding now the analysis takes just over minutes only mutants survive.
note that each problem due to the harness assignment of sto any size smaller than the current size includes all smaller problem sizes.
this makes the behavior of the verification problem size setting match that of cbmc where an unwinding bound is a maximum rather than a fixed size.
we assume inclusiveness in this paper4.
at this point we can increase size to which will kill one additional mutant but the time required to check the remaining mutants is growing rapidly.
in fact completing the check for size even though only the original program and mutants have to be checked requires nearly hours.
when the model checking difficulty grows more slowly with problem size we propose the simple if highly imprecise heuristic of increasing size until the number of mutants killed does not increase with a step up in size we call such a size mutant stable .
however in many cases such as this one the time required to check mutants starts growing unacceptably.
we propose a more efficient algorithm for finding a mutantstable size below figure and mutations can be checked in parallel but the fundamental problem for size and above is that some individual mutants require hours to model check.
what is a developer to do?
b. examining surviving mutants the developer should turn to the surviving mutants.
the surviving mutants for size are shown in figure .
the comment indicates the type of mutant and the line number in the quicksort file is also given for reference.
the relevant lines are marked in figure .
some of these mutants are easily seen to be equivalent to the original code.
for example the tworep const mutations simply change a while into an equivalent infinite loop with a different constant non zero value.
these two mutants could in fact have been automatically removed from the set like uncompilable mutants by checking their compiled code for equivalence with the original program .
we suggest always pruning mutants via trivial compiler equivalence tce .
the remaining mutants produce different binaries when compiled with an optimizing compiler so require manual analysis.
the rep op mutations all alter comparison operators by changing their value on one corner case and we may suspect that quicksort is robust to for example changing i r toi!
r since iis initially set to l which we know to be less than r. thedel stmt mutant however is clearly problematic.
how can quicksort be correct if the inner loop s swapping of 4there is one noted exception in section iv d.log ref log ref log ref log called with l r log called with l r log called with l r log called with l r log called with l r log a log a log a fig.
.
witness to the harness inability to kill the del stmt mutant.
... int i v count qcount prev ... sort a s pick a value to check v nondet int count qcount ... f o r i i s i ... if ref v count if a v qcount ... assert count qcount fig.
.
modifying the harness to ensure ais a permutation of ref .
a anda is changed to instead copy a toa ?
the consequences of this mutant are clearly drastic but why are they not detected by our harness?
we find out by asking cbmc to produce an execution such that the mutated code is covered other coverage is maximized to avoid degenerate executions e.g.
over size arrays and the execution is not a counterexample.
we have modified cbmc and written instrumentation tools that produce a modified mutant and harness allowing us to pose such queries see section iii .
running cbmc in this mode with the target of maximum branch coverage and statement coverage of the del stmt mutant actually the statement after it since it no longer exists we produce the witness in figure in less than a minute5.
our harness checks that the array ais sorted after the call to sort but it does not check that it is a permutation of the input!
we might have discovered this problem by a different method if we remove the call to sort in the harness and replace it by a loop assigning nondet int to every element in array a a kind of most general any order type correct mutant of sort we can run the modified cbmc and see examples of executions our harness allows which should include any sorted array.
the problem with this method is that while it sometimes works cbmc is also free to set all elements in all arrays to and to generally provide an uninformative example of a successful execution.
the requirement to cover a mutant and as much other code as possible helps guide cbmc to a successful execution that is likely to be incorrect because a non equivalent mutant changes the original program s behavior.
moreover while the problem with the harness in this case is simple understanding arbitrary passing but wrong executions can be very difficult without the ability to think about a specific bug the model checker 5we show the output of the print statements not the full cbmc trace this is what a developer will examine first.
authorized licensed use limited to access paid by the uc irvine libraries.
downloaded on january at utc from ieee xplore.
restrictions apply.
is missing.
moreover basing the production of witnesses on mutants allows us to compare harnesses even over killed mutants if one harness reduces the set of passing executions for a mutant it is arguably a better verification of correctness than one allowing more executions of the mutant even if both produce a counterexample killing the mutant.
unlike traditional mutation analysis we can take the question how killed is this mutant?
seriously because we aim at exhaustive testing.
a harness is most effective with respect to a mutant if it allows no executions covering the mutant to pass.
the witness tells us that the sorting harness is too weak.
we say that a harness is weak if it fails to detect incorrect executions.
one harness is stronger than another if it detects more failures we can indirectly estimate strength by determining how many mutants a harness can kill at a given problem size and how executions covering killed mutants can still satisfy the harness.
figure shows how to modify the sorting harness to check for permutations6.
because cbmc is exhaustive instead of performing a complete check for permutation we can detect violation of the property by letting vbe any value and ensuring that both aandref contain the same number of elements equal to v.i fais not a permutation of ref there exists a vsuch that this is not true and we can rely on cbmc to report it as part of a counterexample.
while a cbmc harness resembles a program to test the sut it can make use of unusual specifications relying on exhaustiveness.
if we modify the harness as shown we can re check our mutants including those tce would remove .
with the revised harness checking mutants at size takes slightly longer more seconds and kills the same number of mutants since the problem is the size not the harness.
at size mutant kill results are again unchanged but analysis now completes in about minutes.
finally at size we kill the del stmt mutant that previously survived after only minutes not much longer than at size with the weaker harness.
the size verification is stable.
checking stability by running size now only requires slightly more than an hour nearly an order of magnitude faster than before.
as briefly mentioned in the introduction it is also possible to understand a mutant by modifying the harness to call both the mutated code and the original code on the same inputs and search for witnesses where the execution is passing but the return value s for the mutant differ s from the original.
however this increases the complexity of the model checking problem checking equivalence of two functions is often harder than specifying valid executions and does not easily apply to any verifications other than simple function calls.
for example forcing the same interleavings in threaded programs or detecting all differences in state modification for reactive code is often infeasible or requires significant human intervention.
while we do apply differential checks in some cases below we do not propose this as a core technique suitable for general purpose falsification driven verification.
c. mutating the harness previous efforts to understand model checking results have also considered mutants to the property usually given as a 6in fact if we choose a val to check before we assign to ref we could completely dispense with storing ref at all.temporal logic formula .
once a developer is satisfied with a harness has a mutant stable bound for verification and is convinced all surviving mutants are semantically equivalent to the original program or if not equivalent also satisfy the same correct specification we propose the developer mutate the test harness itself.
the idea is to check that most mutants of the harness reject the sut and the remaining mutants have a mutant kill rate no greater than that of the harness.
for the fixed sort harness there are mutants of which do not compile.
of these produce an incorrect counterexample for the original correct quicksort.
an additional have mutant kill rates worse than the original harness from as low as of mutants killed to only a few percent worse than the fixed harness .
the remaining harness mutants have the same ability to kill mutants as the original harness.
most of these involve modifying a relational operator in a loop or an assumption in a way that preserves semantics.
the only interesting surviving harness mutant is one that removes the assignment of a fresh non deterministic value to v after the call to sort .
this means the check for permutation difference will always be performed on the last element of ref .
on reflection it seems plausible that this is sufficient to produce a counterexample for all the quicksort mutants but it is clearly not an improvement to the harness in terms of either verification strength or clarity.
in addition to showing the current harness is at least a local minima with respect to mutants mutation analysis of the harness also provides some evidence of our technique s ability to detect subtle specification and environment flaws.
in particular it shows the value of inspecting all surviving mutants.
one mutant modifies the assumption on sto be s size rather than s size which is the same as lowering thesize by one this is a fairly easy mistake to make in a harness or any code .
this reduces the effectiveness of the verification by mutants so is likely not to escape notice and would also in our framework simply result in a higher size being chosen as mutant stable.
deleting the assignment prev a however only kills fewer mutants than the original harness.
traditional coverage and some model checking coverages cannot detect this problem because of the assignment to prev outside the loop the variable isused in the specification and in fact used to detect many faults it eliminates any mutants that can cause a to not be the least element .
the harness covers all behavior of quicksort in general since the permutation requirement remains in place.
however it cannot detect versions of quicksort that preserve permutation and make the first element correct but don t always sort the array correctly.
in particular the call to quicksort withj can be removed or modified to j .
examining the deleted removed recursive calls shows the developer the problem in this case.
our modified cbmc easily produces a witness showing a permuted array with correct a but with out of order later elements.
iii.
a lgorithms and techniques falsification driven verification is a semi automated approach that relies heavily on algorithmic and tool support.
while the typically smaller scope of code targeted for verification vs. testing makes the work easier it is not likely to be feasible without automation of many subtasks.
existing tools make producing a set of mutants and checking them using a authorized licensed use limited to access paid by the uc irvine libraries.
downloaded on january at utc from ieee xplore.
restrictions apply.
harness relatively easy but other tasks require new algorithms and tools.
figure shows the basic flow which is directed not by a fixed algorithm but by the intelligence guided by experience of the developer.
novel tools or techniques are on the right side of the diagram mutation analysis itself is not novel but our tool for integrating this with the model checker and recording results for use by other parts of the toolchain is non standard other than the model checker itself.
figures show core algorithms implemented as prototype tools in our framework .
in these algorithms o s is a function mapping an abstract size into particular options e.g.
dsize .
the uses of these algorithms are described at a high level in the introductory example and in the case studies below.
one additional requirement is a version of cbmc capable of converting built in assertions checks e.g bounds checks pointer dereference division by zero to assumptions.
for harness assumptions this is done by automatic source to source transformation figure procedure cover harness but cbmc s internal constraints have to be handled inside the model checker.
we implemented a new cbmc command line option find success that provides this functionality.
in all algorithms check means running cbmc as usual with any needed automatic property checks while scheck indicates running cbmc with find success enabled.
in figure we assume the use of a modified version of cbmc.
thefind size algorithm figure finds a suitable problem size and returns the set of surviving mutants for a harness and program performing as few model checker calls as possible once we know a bound is not stable we move on to the next bound .
this algorithm can be easily parallelized by running mutants in the for loop at the same time with any goto top killing all cbmc runs not terminated.
the maxcover algorithm figure returns for a given mutant and harness a witness program trace that covers the mutant and covers as much other code as possible in terms of branch coverage using the cover harness andcover mutant procedures to instrument harness and mutant it proceeds by starting with a minimal constraint on coverage the trace must cover the mutated code and increases this bound by incrementing it to one more than the actual coverage of the last witness found until the model checker can prove the coverage is impossible.
other strategies for maximal coverage are possible trying maximal coverage first and decreasing the required coverage as attempts fail but this approach minimizes the number of model checker runs that will fail to produce a witness which is critical for performance reasons see section iv d .
the check harness algorithm figure analyzes harness mutants producing a report of harness mutants that are killed either they do not verify the sut or they have worse kill rates than the original harness that are equal to the original harness in strength and that are stronger than the original harness.
it also returns information on all mutants killed by any harness mutant except those that reject the sut that are not killed by the original harness.
the algorithm killed not shown simply returns the set of mutants killed by a given harness.
in our implementations these tools perform additional record keeping.
for example harness analysis records killing counterexamples and execution times for each run.
we also make use of convenience scripts such as a tool to automatically call maxcover on all mutants which provides a measure of harness strength g5 g23 g13 g14 g20 g1 g12 g17 g14 g12 g19 g14 g25 g1 g4 g10 g25 g22 g14 g26 g26 g1 g3 g14 g31 g14 g20 g23 g24 g14 g25 g36 g31 g14 g25 g18 g15 g12 g10 g28 g23 g22 g1 g14 g22 g16 g18 g22 g14 g14 g25 g1 g6 g8 g7 g1 g5 g30 g27 g10 g28 g23 g22 g1 g27 g23 g23 g20 g1 g2 g23 g30 g22 g27 g14 g25 g14 g33 g10 g21 g24 g20 g14 g26 g1 g10 g22 g13 g1 g31 g14 g25 g18 g15 g12 g10 g28 g23 g22 g26 g1 g5 g20 g18 g7 g15 g10 g16 g1 g21 g13 g29 g10 g20 g1 g32 g11 g17 g9 g31 g21 g13 g29 g10 g33 g1 g3 g6 g20 g17 g10 g21 g21 g1 g6 g17 g6 g15 g28 g21 g13 g21 g1 g32 g8 g12 g10 g8 g14 g31 g12 g6 g20 g17 g10 g21 g21 g33 g1 g4 g25 g22 g6 g17 g22 g1 g10 g27 g19 g15 g6 g17 g6 g23 g18 g17 g1 g32 g16 g6 g27 g8 g18 g26 g10 g20 g33 g1 g2 g6 g21 g13 g8 g1 g16 g25 g22 g6 g23 g18 g17 g1 g6 g17 g6 g15 g28 g21 g13 g21 g1 g3 g6 g20 g17 g10 g21 g21 g1 g6 g17 g6 g15 g28 g21 g13 g21 g1 g32 g8 g12 g10 g8 g14 g31 g12 g6 g20 g17 g10 g21 g21 g33 g4 g25 g22 g6 g17 g22 g10 g27 g19 g15 g6 g17 g6 g23 g18 g17 g32 g16 g6 g27 g8 g18 g26 g10 g20 g33 g1 g6 g8 g7 g1 g5 g30 g27 g10 g28 g23 g22 g27 g23 g23 g20 g4 g10 g25 g22 g14 g26 g26 g22 g6 g30 g25 g31 g18 g31 g18 g22 g16 g1 g17 g10 g25 g22 g14 g26 g26 g14 g26 g1 g39 g1 g19 g18 g20 g20 g1 g25 g10 g27 g14 g26 g1 g9 g18 g27 g22 g14 g26 g26 g1 g1 g27 g25 g10 g12 g14 g1 g37 g17 g23 g32 g1 g21 g30 g27 g10 g22 g27 g1 g12 g10 g22 g1 g24 g10 g26 g26 g1 g17 g10 g25 g22 g14 g26 g26 g38 g1 g6 g27 g10 g11 g20 g14 g1 g26 g18 g34 g14 g1 g39 g1 g26 g30 g25 g31 g18 g31 g18 g22 g16 g1 g21 g30 g27 g10 g22 g27 g26 g1 g6 g30 g25 g31 g18 g31 g18 g22 g16 g1 g21 g30 g27 g10 g22 g27 g26 g1 fig.
.
basic flow of falsification driven verification.
int survivors find size h m s0 int o int options u int int s s0 r mutant result top s s r r r for m m ifm negationslash domain r r check h m u s o s if r verification failed once killed assume always killed m m m if r verification successful r check h m u s o s if r verification failed m m m goto top no result changed so s is mutant stable return s m fig.
.
algorithm finding size unwinding bound and surviving mutants.
that is more fine grained than a simple kill rate harnesses can be compared by the maximum coverage of all mutants even if they have the same kill rate.
if one harness produces executions with lower coverage or no executions at all for some killed mutants it is stronger.
for some mutants any passing executions show a harness flaw.
while not polished enough for release these tools implemented as python scripts have proven robust in our experiments and are available along with our experimental data and cbmc patch at https github.com agroce cbmcmutate .
iv .
c ase studies and experiment al resul ts a. algorithm implementations our initial experiments involved relatively small verification problems based on implementations taken from the web or student code for popular algorithms and data structures.
here we highlight the most interesting of these we also successfully applied the method to bubble sort a duplicateremoving array merge function an a vl tree and a student s harness for verifying a version of dijkstra s shortest path authorized licensed use limited to access paid by the uc irvine libraries.
downloaded on january at utc from ieee xplore.
restrictions apply.
harness cover harness h ta r g et h prime h for stmt h prime if stmt assert p stmt assume p cover assume total coverage ta r g et assert !mutant covered insert cover at end of h prime.main return h prime mutant cover mutant m n m prime m for if stmt c in m prime if c has no else add t oc for basic block b in m prime b covered total covered b n n for stmt s in m prime if mutant s s mutant covered s m prime int total covered int mutant covered int covered m prime return m prime trace maxcover m h s o u m prime cover mutant m t trace failed false while not failed h prime cover harness h t r scheck h m prime u s o s if r verification successful failed true else if r verification failed trace r.trace t trace.read total covered return trace fig.
.
algorithm find a maximally covering execution trace that covers a mutant.
report check harness sut m h m h s o u kh killed m h s hkills hequal hbetter n for h iinm h original check hi sut u s o s if original verification failed hkills h i else check if this kills fewer mutants khi killed m h s for k khi if k negationslash kh n hi k if k hi k h hbetter h i k hi if k hi k h hequal h i k hi else hkills h i k hi return hkills hequal hbetter n fig.
.
algorithm analyze a harness.algorithm that enables path reconstruction .
in the case of the a vl tree the harness we were working with was unable to reach a mutant stable unwinding without exhausting memory on the verification of the main program for a vl trees of up to size .
we are investigating a more efficient harness encoding based on the inability to reach mutant stability.
for the dijkstra harness the low mutant kill rate of only showed that while the harness checks incorrect returned paths it cannot detect when return values indicate there is no path but one exists.
improving the harness is a substantial exercise but can be guided by the survival witnesses.
binary search the ideas in this paper grew out of a side project of the first author to write a follow up to jon bentley s article on verifying binary search in the context of modern software verification tools and joshua bloch s discovery of a bug in the assumptions behind bentley s proof .
the modeling required is moderately complex to scale well an abstract sorted array that represents all sorted arrays but only introduces variables equal to the number of probes made by the search is essential .
in this case we did not produce an initial weaker version of the harness but checked the existing harness using mutants and determined that all surviving mutants are equivalent to a true binary search.
checking harness mutants which took minutes including computing the kills for the original harness produced results confirming the belief this is a good harness.
of the compiling harness mutants failed to verify the correct binary search and had worse kill rates than the original harness.
the remaining harnesses all with equal kill rates of .
all modify an assumption to allow the harness to also check size arrays.
this doesn t kill any additional mutants but is harmless as expected.
of the harness mutants with worse kill rates three are mutants of the assumptions on the nondeterministic value used to make sure that if binary search returns no index in the array actually contains the searched for item.
two of these mutants are off by one errors that exclude item from the check an easy to make mistake.
both of these fail to kill exactly one mutant killed by the correct harness the mutant that sets the lower bound initially to instead of .
traces of passing runs for these mutants show the problem clearly the sought item at index .
doubly linked list insertion sort another example making use of recursive data structures and pointer validity checks is code for inserting an item in sorted order into a doubly linked list .
our initial harness omitted a check for correctness of prev pointers.
this problem didn t directly prevent mutants from being detected but pushed the stable size larger as with the quicksort example above.
looking at a trace of a size run that fails to kill a clearly problematic mutant easily reveals the problem the results are correct up toprev pointers .
this example also showed another use of mutants in that some seemingly problematic surviving mutants actually just showed a pointless redundancy in the implementation enabling the removal of an entire conditional branch.
a harness check requiring minutes including computing the mutant kills for the original harness showed that of the compiling harness mutants fail to verify the original code.
another have a worse kill rate than the original which kills of mutants a low rate due to the code redundancy and survive.
the large number of survivors authorized licensed use limited to access paid by the uc irvine libraries.
downloaded on january at utc from ieee xplore.
restrictions apply.
jsint js boyermoorehorspool const jschar text jsint textlen const jschar pat jsint patlen jsint start jsint i j k m uint8 skip jschar c js assert patlen patlen bmh patlen max for i i bmh charset size i skip uint8 patlen m patlen f o r i i m i c pat if c bmh charset size return bmh bad pattern skip uint8 m i for k start m k textlen k c text bmh charset size ?
patlen skip f o r i k j m i j if j retur ni if text !
pat break return fig.
.
spidermonkey .
boyer moore horspool code.
is due to a redundancy of the final harness which checks sortedness and the permutation property for both a forward next traversal of the list and a prev traversal.
omitting any one of these e.g.
prev sortedness or next permutation the harness can still detect all mutants.
removing two however fails to kill mutants.
the two harness mutants with worse kill rates have extremely poor kill rates and .
b. spidermonkey boyer moore horspool implementation figures and show respectively the source code and an initial harness for verification of the boyer moorehorspool substring finding algorithm in version .
of mozilla s spidermonkey javascript engine.
v erifying this code presents one immediate issue that is not unusual in verification how to handle an assert in the code being verified.
an assert at the end of a function or in the main body is typically an additional part of the specification and is often best left unchanged.
an assert at the beginning of a function s body however is typically a precondition for the code .
it is natural to consider changing such an assertion into an assume and ignoring any problems produced by calling the code with non conforming inputs.
while this can be a useful technique for instance when it is hard to write a harness that only produces valid inputs but easy to filter out the invalid inputs and only verify behavior for those it is also a dangerous technique.
mutation analysis of the harness shows that is a mutant stable size where the same size is used for text length pattern length and character set size with a kill rate of .
.
on initial examination the surviving mutants do not seem problematic.
a large number involve the js assert converted to a cprover assume showing the harness cannot tell if the assumption is incorrect which is not surprising the harness only generates good inputs and some of the mutants simply discard too many inputs .
at this point we were satisfied with our harness and ran include bmh.h int main int i unsigned int v char itext char ipat unsigned int itext s nondet uint cprover assume itext s tsize unsigned int ipat s nondet uint cprover assume ipat s psize printf log size text u pat u n itext s ipat s for i i itext s i v nondet unit cprover assume long v long bmh charset size itext v cprover assume itext bmh charset size printf log text u n i itext for i i ipat s i v nondet uint cprover assume long v long bmh charset size ipat v cprover assume ipat bmh charset size printf log pat u n i ipat jsint r js boyermoorehorspool itext itext s ipat ipat s printf log return d n r int pos ppos found v nondet uint printf log looking at u n v cprover assume v if r cprover assume v itext s pos v ppos found while ppos ipat s printf log itext u ipat u n pos itext ppos ipat if pos itext s itext !
ipat found break pos ppos assert !found else p o s r p p o s while ppos ipat s assert itext ipat pos ppos v nondet uint printf log looking at u n v cprover assume v r pos v ppos found while ppos ipat s printf log itext u ipat u n pos itext ppos ipat if pos itext s itext !
ipat found break pos ppos assert !found fig.
.
boyer moore horspool harness.
a check on mutants of the harness itself.
to our surprise three mutants of the harness had a better kill rate than the correct harness killing .
of mutants.
investigating these better harnesses showed mutants that broke processing of some return values in such a way that while these harnesses failed to detect certain major bugs in the code they were able to detect some js assert assumption mutants.
guided by this we produced a revised harness that raised the kill rate to .
.
however on examining the surviving mutants we realized that our verification was still unsatisfactory as a good regression for the boyer moore horspool code in particular if the assertion authorized licensed use limited to access paid by the uc irvine libraries.
downloaded on january at utc from ieee xplore.
restrictions apply.
static int rcu read nesting global static void rcu read lock void void sync fetch and add rcu read nesting global static void rcu read unlock void void sync fetch and add rcu read nesting global static void synchronize rcu void for if sync fetch and xor rcu read nesting global return set noassert return fig.
.
approximate model of rcu were ever modified to allow bad inputs to pass through or otherwise incorrectly changed we would those bugs.
we then changed the js assert into code that returned a special value to signal assertion failure and modified the harness once more allowing some incorrect values to pass through and checking that assertion failure happened if and only if the inputs were invalid.
this harness killed .
of mutants and the six surviving mutants were easily understood to be equivalent to the bmh code under all valid inputs in one case we weren t certain about we had cbmc verify that for all non assertion violating inputs this was true up to size .
the new harness informed by the harness mutations in fact had a better mutant kill rate for size .
than our first harness had at the mutant stable point.
this examples serves as our best evidence of the value of harness mutation.
c. linux kernel rcu v erification challenges read copy update rcu is a synchronization mechanism sometimes used as a replacement for reader writer locking for linked structures allowing extremely lightweight readers .
in the limiting case achieved in server class builds of the linux kernel overhead for entering and exiting an rcu read side critical section using rcu read lock and rcu read unlock respectively is exactly zero making rcu an excellent choice for read mostly workloads .
however lightweight readers mean updaters cannot exclude readers so must take care to avoid disrupting readers.
updaters typically maintain multiple versions of the portion of the data structure being updated removing old versions only when readers are no longer accessing them.
to this end rcu provides synchronize rcu which waits for agrace period when all pre existing rcu readers complete.
rcu updaters typically remove a data element rendering it inaccessible to new readers invoke synchronize rcu and then reclaim a removed element.
because both rcu and the linux kernel are moving targets any validation and verification must be both automated and repeatable for inclusion in a regression test suite.
at present the rcutorture stress test provides some assurance in the form of automated testing but ideally would be complemented by some formal verification of the implementation s in the kernel.
an important question is whether available formalverification tools can provide effective additional regression checking for rcu.
we use a pair of rcu related benchmarks to provide the beginnings of an answer to this question.
the first benchmark applies formal verification to the simplest of the linux kernel s rcu implementations tiny rcu which targets single cpu systems.
this model includes tiny rcu s handling of idle cpus as well as its trivial grace period detection scheme.
the second benchmark creates the trivial model approximating an rcu implementation for multiprocessor systems shown in figure .
in this model the number of rcu read side critical sections currently in flight is tracked by the global rcu read nesting global which is atomically incremented by rcu read lock and atomically decremented by rcu read unlock .
this allows synchronize rcu to atomically xor rcu read nesting global s bottom bit to detect whether the current execution has waited for all pre existing readers over approximated by checking the absence of all readers with set noassert being invoked to suppress all future assertions.
although this model has a number of shortcomings perhaps most prominently excessive read side ordering it is capable of detecting common rcu usage bugs including failure to wait for an rcu grace period and failure to enclose read side