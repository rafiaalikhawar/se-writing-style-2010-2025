taco a tool to generate tensor algebra kernels fredrik kjolstad mit csail usa fred csail.mit.edustephen chou mit csail usa s3chou csail.mit.edudavid lugato cea france david.lugato cea.frshoaib kamil adobe usa kamil adobe.comsaman amarasinghe mit csail usa saman csail.mit.edu abstract tensor algebra is an important computational abstraction that is increasingly used in data analytics machine learning engineering and the physical sciences.
however the number of tensor expressions is unbounded which makes it hard to develop and optimize libraries.
furthermore the tensors are often sparse most components are zero which means the code has to traverse compressed formats.
to support programmers we have developed taco a code generation tool that generates dense sparse and mixed kernels from tensor algebra expressions.
this paper describes the taco web and command line tools and discusses the benefits of a code generator over a traditional library.
see also the demo video at tensor compiler.org ase2017.
index terms tensor algebra linear algebra sparse compiler i. i ntroduction tensors generalize matrices to any number of dimensions and are used in diverse domains from quantum physics to machine learning and data analytics.
tensor algebra generalizes linear algebra to tensors and is a powerful abstraction that can be used to express many sophisticated computations.
the traditional approach to linear algebra is to create software libraries with optimized functions or methods kernels for all the binary expressions e.g.
matrix addition and matrixvector multiplication .
to compute a compound non binary expression the programmer calls a sequence of binary kernels with vector and matrix temporaries.
this approach to software development worked well in the past but is now unsuitable due to new features that cause an explosion in the number of variants that must be developed.
first the vectors matrices and tensors of interest are often sparse which means that most of the components are zero.
for example a tensor of amazon reviews used to predict how a user will respond to a new product contains gigabytes of non zeros but exabytes of zeros .
to take advantage of sparsity several compressed formats have been devised that store only non zeros.
however this requires library developers to develop a variant of each kernel for each combination of supported formats.
second the number of target platforms such as multi core cpus gpus tpus and distributed systems is increasing.
to take advantage of these architectures the library developers must rewrite each kernel for each platform.
third it is expensive to compute compound expressions through a sequence of kernels because the vector matrix and tensor temporaries that are passed between them can be large resulting in poor temporal locality.
to address this issue library developers write kernels that compute compound expressions.
however the number of compound expressions isunbounded so developers can support only a subset of them.
finally when we generalize linear algebra to tensor algebra the number of binary expressions also becomes unbounded since tensors can have any number of dimensions.
compositional complexity is often managed with composable software components.
at some point the interactions between these components become so complex that the interfaces start to look like a language and a meta programming approach becomes necessary .
the mathematical tensor algebra notation that we are concerned with in this paper calls for a meta programming approach because it is a small language.
performance is however essential for tensor and linear algebra.
for example google has expressed concern that the cost of deep neural networks will become prohibitive unless the performance of tensor computations is improved .
to resolve the tension between the need for generality and performance we turn to code generation.
in this demo we discuss taco the first tool that can generate efficient parallel code for compound tensor expressions where the tensors are stored in dense and sparse formats.
the tool implements the tensor algebra compiler theory described in previous work and is available as a web tool a command line tool and as a library.
these tools can be used to generate and benchmark kernels to search for optimal formats and to interactively optimize code.
thetaco command line tool and library are available under the permissive mit license at tensor compiler.org and the web tool at tensor compiler.org codegen.
a video demo is available at tensor compiler.org ase2017.
ii.
t ensor algebra storage and kernels a tensor generalizes a matrix with two dimensions to any number of dimensions called the tensor s order.
a vector is thus a 1st order tensor and a matrix is a 2nd order tensor.
tensor algebra also called multilinear algebra is a generalization of linear algebra to work on tensors of any order linear algebra is a subset of tensor algebra .
tensor algebra expressions are best expressed using tensor index notation developed by ricci curbastro levi civita and einstein .
the following example uses index notation to compute a tensor vector multiplication contraction resulting in a matrix aij x kbijkck with tensor index notation tensor algebra expressions are written as scalar expressions with subscripted index variables .
c ieeease urbana champaign il usa tool demonstrations943 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
for int i i m i for int j j n j int pb2 i n j int pa2 i n j for int k k p k int pb3 pb2 p k a b c fig.
.
aij p kbijkck for int pb1 b1 pos pb1 b1 pos pb1 int i b1 idx for int pb2 b2 pos pb2 b2 pos pb2 int j b2 idx int pa2 i n j for int pb3 b3.pos pb3 b3.pos pb3 int k b3 idx a b c fig.
.
aij p kbijkck sparse b for int pb1 b1 pos pb1 b1 pos pb1 int i b1 idx for int pb2 b2 pos pb2 b2 pos pb2 int j b2 idx int pa2 i n j int pb3 b3 pos int pc1 c1 pos while pb3 b3 pos pc1 c1 pos int kb b3 idx int kc c1 idx int k min kb kc if kb k kc k a b c if kb k pb3 if kc k pc1 fig.
.
aij p kbijkck sparse b c that connect each component of the result to components of the operands.
the tensor vector contraction has two free variables iandjand one summation variable k. free variables always index the result tensor while summation variables never index the result tensor.
the simplest storage format for a tensor is a multidimensional array which we call a dense format.
dense formats are appropriate for tensors that have few zeros and have useful properties such as fast random access and simple iteration spaces.
for example dense tensors with the same dimensions have the same iteration space which can be used to generate efficient code.
many tensors are sparse which means that most components are zero.
for these tensors storing only the non zero values saves memory and may increase performance.
many sparse storage formats have been devised for matrices and for higher order tensors .
the key idea is to store the non zero values together with an index data structure that identifies the tensor coordinates of each non zero.
compute kernels for tensor algebra expressions must iterate over operands to produce the non zero values of the result.
for dense expressions the loop nest simply iterates over the polyhedral space defined by the range of each tensor index variable.
fig.
shows a c kernel for tensor vector multiplication.
note the simple loop bounds and the statements on lines 7and11that compute locations in the tensor multidimensional arrays.
compute kernels for tensor expressions with sparse operands require more care.
the loops iterate over a sparse subset of the dense polyhedral iteration space a polyhedron with holes.
fig.
shows tensor vector multiplication code when tensor bis stored as a sparse tensor in every dimension with corresponding index structures.
each loop iterates over the entries in a single dimension the last loop iterates overnon zeros.
unlike dense storage indirect loads are needed to traverse the index structure which consist of two arrays for each dimension a pos and an idx array.
the idx array stores the coordinates of non zero entries in that dimension and the pos array stores the ranges of idx values belonging to each tensor slice in the preceding dimension.
the code becomes more complicated when more than one tensor operand is sparse.
when only one operand is sparse the code can iterate over its index structure and access the other operand s components by computing their location.
however index structures do not permit such fast random access.
if more than one operand indexed by an index variable is sparse we must iterate over their merged iteration spaces similar to a database merge join or the merge in mergesort .
fig.
shows the tensor vector multiplication kernel when both bandcare sparse.
since the operator is a multiplication the loops must iterate over the intersection between each row of band the vector c. the intersection merge code is shown on lines .
we iterate over the intersection because if a component of either borcat a location is zero then the result is zero and we do not need to compute it.
in contrast for addition we must iterate over the union of iteration spaces.
with sparse iteration spaces and merges the kernels become more difficult to write by hand motivating the automated code generation approach taken by the taco tools.
iii.
t he taco tools thetaco tool suite consists of a web tool a commandline tool and a c library.
the command line tool is built on top of the library and the web tool is built on top of the command line tool.
all three can be used to generate kernels.
in addition the command line tool can be used to benchmark kernels and to interactively optimize code.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
the taco web tool with the mttkrp tensor factorization kernel tensor compiler.org codegen?demo mttkrp .
the generated code iterates through the sparse index of b the other operands are dense and support random access.
a. web tool thetaco web tool is a hosted code generation tool available at tensor compiler.org codegen.
it consists of a javascript client and a remote code generation server written in python.
the web client implements a gui where users can enter tensor index notation expressions in textual form summations are implied when a variable does not index the result .
fig.
shows a screenshot with the matricized tensor times khatrirao product mttkrp expression.1as a user enters an 1mttkrp is a key kernel in algorithms that compute the canonical polyadic decomposition tensor factorization which is one generalization of svd to higher order tensors.expression in the text box the client parses it and dynamically populates a table with one format description row per tensor.
the format descriptions specify the format of the tensor in each dimension.
taco currently supports dense and sparse dimensions and we plan to support more format types in the future.
the dropdown menus can also be re ordered through drag and drop to specify formats that store tensors in different directions e.g.
row major versus column major .
the user can instruct the web tool to generate code for the expression and tensor formats by pressing the button labeled generate kernel .
the client then sends a request to a code generation server that calls the taco command line tool to authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
generate code.
the code is then sent back to the client and displayed at the bottom of the webpage.
there are three tabs one that shows only the loops to compute values one that shows only the loops to assemble sparse result tensors and one that shows the complete code.
the complete code is a c header file that the user can download or copy paste into an application if it only needs that kernel.
this is a lightweight alternative to downloading and linking against the full taco c library that supports every kernel and that provides convenient functionality such as file loaders.
b. command line tool thetaco command line tool is written in c and is built on top of the taco c library .
both are publicly available under the permissive mit license at code.tensor compiler.org.
the command line tool provides all the code generation functionality of the web tool but also supports measuring the size of tensors in different formats as well as benchmarking and code optimization workflows.
tensor size measurements it is useful to be able to measure the data size of a tensor in different formats.
if the tensor is stored on disk the user can measure its size in a given format by combining the ioption that loads a tensor from a file with the foption that sets tensor formats.
the ioption supports several file formats including the frostt sparse tensor format .tns and the tensor market exchange format .ttx for general tensors and the matrix market exchange format .mtx and the harwell boing format .rb for matrices.
the following command creates a tensor bwhose format is sparse in all dimensions and fills it with data from a file containing the facebook activities data set taco f b sss i b facebook.tns b size x x bytes choosing the first dimension to be dense slightly decreases the memory consumption which means most matrix slices have at least one value.
dense dimensions also often lead to faster kernels so this format is likely better for this tensor taco f b dss i b facebook.tns b size x x bytes benchmarking since performance is essential for tensor algebra taco supports benchmarking kernel performance with the time option.
to aid benchmarking the tool also provides the goption to generate synthetic data.
with these options we can use the following command to benchmark the mttkrp kernel from fig.
on the facebook tensor taco a i j b i k l c k j d l j f b sss f c dd f d dd i b facebook.tns d j g c d g d d time b file read .
ms b pack .
ms b size x x bytes c size x bytes d size x bytes compile .
ms assemble .
ms compute .
msthe first four lines is the command.
the first line contains the mttkrp tensor index notation expression.
the second line specifies formats for the operands bis all sparse while canddare all dense.
the third line loads bfrom a file.
thei kandlindex variables are used to index into bso their ranges are inferred from the input file.
since the jindex variable is not used to index bwe set its size manually with the doption.
finally on the fourth line we use the goption to generate dense data for the canddmatrices and include time to tell taco to run benchmarks.
note that this option takes an optional number e.g.
time which denotes the number of times the compute kernel should be run.
if this option is given then taco emits the mean standard deviation and median across the runs.
the output of this command is given on the following lines.
first it prints the time spent reading the file and packing it into the sparse format of b. next it prints the size of each tensor and finally it prints the time spent compiling the mttkrp kernel and assembling and computing the values of a. assembly is cheap since ais a dense matrix without indices.
interactive optimization workflow finally taco supports an interactive optimization workflow where programmers can use code generated by taco as a starting point for further manual optimization.
the motivation for this workflow is to give developers an easy way to instrument kernels and to provide them with an escape hatch when taco does not yet support an optimization they need.
the taco command line tool writes source code to a file when passed the write source option.
this lets a developer modify the code and then verify and or benchmark it against the taco generated kernel with the read source option.
for example suppose we want to try parallelizing the mttkrp kernel with the cilk parallel programming model .
taco emits code that uses openmp but we can write out the kernel modify it to use cilk and then use the following command line option to load verify and benchmark it taco a i j b i k l c k j d l j f b sss f c dd f d dd i b facebook.tns d j g c d g d d time verify read source mttkrp cilk.h b file read .
ms b pack .
ms b size x x bytes c size x bytes d size x bytes compile .
ms assemble .
ms compute time ms mean .
stdev .
median .
mttkrp cilk.h assemble .
ms compute time ms mean .
stdev .
median .
verifying... done this workflow lets developers quickly try out verify and benchmark new ideas for tensor algebra kernel optimization.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i benchmark data collected with the t a c o command line tool t i m e option .
the benchmarks show the time in milliseconds to compute a matrix vector multiplication with four matrices stored in four different formats .
the matrices exemplify common sparsity structures .
the table diagonal shows the importance of choosing formats to match the matrices .
dense dense dense sparse sparse dense sparse sparse dense matrix .
.
.
.
thermal matrix .
.
.
.
slicing matrix .
.
.
.
hypersparse matrix .
.
.
.
table ii time in milliseconds to compute a tensor vector multiplication using synthetic tensors with increasing numbers of randomly located zeros .
the data shows that an all sparse tensor outperforms an all dense tensor on this operation when of the values are zero .
dense .
.
.
.
.
.
.
sparse .
.
.
.
.
.
.
c. summary of evaluation we have evaluated the correctness of the taco tool suite with more than unit tests.
we have also run it on large data sets and compared the results to those produced by other popular libraries such as the matlab tensor toolbox and eigen .
to further test the code generator we plan to develop a fuzz tester that generates tensor algebra expressions and runs them with operands in every combination of formats comparing the results to each other.
we also collect anonymized information from the web tool that we will use to learn about tensor algebra usage.
we have also evaluated the performance of the code produced by taco and found that it is competitive with hand optimized kernels from libraries such as eigen and splatt .
furthermore it is typically faster than the matlab tensor toolbox which is the most general prior system we found.
for additional performance results including a comparison of formats and plots showing when sparse formats are better than dense formats see our paper on the tensor algebra compiler theory and library .
tables i and ii shows performance results collected using the command line tool.
table i shows matrix vector multiplication performance for four matrices in four formats.
the results demonstrate the importance of an approach that can generate code for many different formats each matrix performs best when using a different storage format.
table ii shows tensor vector multiplication performance with synthetic matrices of increasing sparsity for sparse and dense formats in all dimensions .
this result demonstrates the importance of sparsity and that sparse formats make sense at about sparsity for this operation.
iv.
d iscussion to the best of our knowledge taco is the first tool to generate code for such a large set of dense and sparse tensorindex notation expressions.
library and application developers can use taco to generate linear and tensor algebra kernels freeing them from error prone kernel development.
we believe the existence of this tool opens up many exciting opportunities.
with the ability to automatically generate code programmers can quickly explore the space of tensor kernels and different formats.
furthermore this process can now be automated using heuristics or an autotuner.
such automatic systems become much more powerful when they are no longer constrained to a limited set of kernels and formats but can instead optimize across the set of all kernels and many formats.
thetaco tools is publicly available under the mit license and we are seeing increasing traffic.
there are efforts underway to integrate taco with julia and tensorflow and in the future we plan to expand the code generator to support more formats.
we also plan to support distributed code generation and accelerators so that programmers are also relieved from porting kernels.
we believe this requires support for new formats that lay out data to fit each accelerator architecture.
finally another exciting direction for future work is to provide a scheduling language that lets users interactively control code optimization.
v. r elated work there are many available libraries and frameworks for linear and tensor algebra.
most are written in the traditional way with hand optimized kernel implementations.
on one extreme is the ubiquitous blas dense linear algebra library which consists of functions that are mostly variants of a smaller set of kernels .
oski is a library that supports the blocked compressed sparse row format bcsr a sparse format with dense inner blocks and supports autotuning the block sizes for a few kernels .
eigen is a modern and convenient c linear algebra header library that uses metaprogramming through c templates to optimize dense linear algebra operations .
many dense and sparse tensor libraries have been developed in the last decade.
because tensor algebra results in an unbounded number of kernels for binary expressions and because compound kernels are often essential for performance these libraries tend to use meta programming techniques.
the tensor contraction engine is an early library for compound dense tensor expressions that relied on compiler techniques to produce a fused loops .
the matlab tensor toolbox is an early library for sparse tensor algebra that supports many sparse expressions and is built on top of the sparse linear authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algebra in matlab .
finally tensorflow is a recent framework that provides many hand written kernels for dense tensor algebra and some for sparse tensor algebra .
the tensorflow developers have recently adopted meta programming with their xla accelerated linear algebra compiler for dense linear algebra.
vi.
c onclusion the unbounded number of tensor algebra kernels and the need for performance makes a code generation approach necessary.
the tool we have demonstrated taco automatically generates tensor algebra kernels for many different formats.
the user specifies a tensor algebra expression and formats andtaco generates a c function to compute the expression.
this relieves programmers from kernel development and lets them quickly explore different kernels and formats.
acknowledgment we thank rami manna and apurva shrivastava for their help with the tensor compiler.org webpage and documentation.
the development of taco was supported by the national science foundation under grant no.
ccf by the u.s. department of energy office of science office of advanced scientific computing research under award numbers de sc008923 and de sc014204 by the direction g en erale de l armement projet ere and by the toyota research institute.