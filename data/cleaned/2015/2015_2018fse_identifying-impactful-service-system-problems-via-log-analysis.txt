identifying impactful service system problems via log analysis shilin he the chinese university of hong kong hong kong china slhe cse.cuhk.edu.hkqingwei lin microsoft research beijing china qlin microsoft.comjian guang lou microsoft research beijing china jlou microsoft.com hongyu zhang the university of newcastle nsw australia hongyu.zhang newcastle.edu.aumichael r. lyu the chinese university of hong kong hong kong china lyu cse.cuhk.edu.hkdongmei zhang microsoft research beijing china dongmeiz microsoft.com abstract logs are often used for troubleshooting in large scale software systems.
for a cloud based online system that provides service a huge number of logs could be generated every day.
however these logs are highly imbalanced in general because most logs indicate normal system operations and only a small percentage of logs reveal impactful problems.
problems that lead to the decline of system kpis key performance indicators are impactful and should be fixed by engineers with a high priority.
furthermore there are various types of system problems which are hard to be distinguished manually.
in this paper we propose log3c a novel clustering based approach to promptly and precisely identify impactful system problems by utilizing both log sequences a sequence of log events and system kpis.
more specifically we design a novel cascading clustering algorithm which can greatly save the clustering time while keeping high accuracy by iteratively sampling clustering and matching log sequences.
we then identify the impactful problems by correlating the clusters of log sequences with system kpis.
log3c is evaluated on real world log data collected from an online service system at microsoft and the results confirm its effectiveness and efficiency.
furthermore our approach has been successfully applied in industrial practice.
ccs concepts software and its engineering software testing and debugging maintaining software keywords log analysis problem identification clustering service systems also with shenzhen research institute the chinese university of hong kong.
work done mainly during internship at microsoft research asia.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa association for computing machinery.
acm isbn .
.
.
.
reference format shilin he qingwei lin jian guang lou hongyu zhang michael r. lyu and dongmei zhang.
.
identifying impactful service system problems via log analysis.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
introduction for large scale software systems especially cloud based online service systems such as microsoft azure amazon aws google cloud high service quality is vital.
since these systems provide services to hundreds of millions of users around the world a small service problem could lead to great revenue loss and user dissatisfaction.
large scale software systems usually generate logs to record system runtime information e.g.
states and events .
these logs are frequently utilized in the maintenance and diagnosis of systems.
when a failure occurs inspecting recorded logs has become a common practice.
particularly logs play a crucial role in the diagnosis of modern cloud based online service systems where conventional debugging tools are hard to be applied.
clearly manual problem diagnosis is very time consuming and error prone due to the increasing scale and complexity of large scale systems.
over the years a stream of methods based on machine learning have been proposed for log based problem identification and troubleshooting.
some use supervised methods such as classification algorithms to categorize system problems.
however they require a large number of labels and substantial manual labeling effort.
others use unsupervised methods such as pca and invariants mining to detect system anomalies.
however these approaches can only recognize whether there is a problem or not but cannot distinguish among different types of problem.
to identify different problem types clustering is the most pervasive method .
however it is hard to develop an effective and efficient log based problem identification approach through clustering due to the following three challenges first large scale online service systems such as those of microsoft and amazon often run on a 24basis and support hundreds of millions of users which yields an incredibly large quantity of logs.
for instance a service system of microsoft that we studied can produce dozens of terabytes of logs per day.
notoriously conducting conventional clustering on data of such order of magnitude 60esec fse november lake buena vista fl usa s. he q. lin j. lou h. zhang m. r. lyu d. zhang consumes a great deal of time which is unacceptable in practice .
second there are many types of problems associated with the logs and clustering alone cannot determine whether a cluster reflects a problem or not.
in previous work on log clustering developers are required to verify the problems manually during the clustering process which is tedious and time consuming.
third log data is highly imbalanced.
in a production environment a well deployed online service system operates normally most of the time.
that is most of the logs record normal operations and only a small percentage of logs are problematic and indicate impactful problems.
the imbalanced data distribution can severely impede the accuracy of the conventional clustering algorithm .
furthermore it is intrinsic that some problems may arise less frequently than others therefore these rare problems emerge with fewer log messages.
as a result it is challenging to identify all problem types from the highly imbalanced log data.
to tackle the above challenges we propose a novel problem identification framework log3c using both log data and system kpi data.
system kpis key performance indicators such as service availability average request latency failure rate etc.
are widely adopted in industry.
they measure the health status of a system over a time period and are collected periodically.
to be specific we propose a novel clustering algorithm cascading clustering which clusters a massive amount of log data by iteratively sampling clustering and matching log sequences sequences of log events .
cascading clustering can significantly reduce the clustering time while keeping high accuracy.
further we analyze the correlation between log clusters and system kpis.
by integrating the cascading clustering andcorrelation analysis log3c can promptly and precisely identify impactful service problems.
we evaluate our approach on real world log data collected from a deployed online service system at microsoft.
the results show that our method can accurately find impactful service problems from large log datasets with high time performance.
log3c can precisely find out problems with an average precision of .
and an average recall of .
.
we have also successfully applied log3c to the maintenance of many actual online service systems at microsoft.
to summarize our main contributions are threefold we propose cascading clustering a novel clustering algorithm that can greatly save the clustering time while keeping high accuracy.
the implementation is available on github1.
we propose log3c which is a novel framework that integrates cascading clustering and correlation analysis.
log3c can automatically identify impactful problems from a large amount of log and kpi data efficiently and accurately.
we evaluate our method using the real world data from microsoft.
besides we have also applied log3c to the actual maintenance of online service systems at microsoft.
the results confirm the usefulness of log3c in practice.
the rest of this paper is organized as follows in section we introduce the background and motivation.
section presents the proposed framework and each procedure in detail.
the evaluation of our approach is described in section .
section discusses the experiment results and section shares some success stories and leaving monitored scope ensurelistitemsdata execution time .
http request url bbbb sitedata .html05 http request url rst uvx adeg lists files docxx .doc03 http request url emails mrx mrx mail .com 1c 48f0 b29.eml01 name request get bbbb sitedata .html leaving monitored scope request post bbbb sitedata .html execution time .
http request method get overridden http request method get e1 name request e3 http request method e5 overridden http request method e4 http request url log parsinge2 leaving monitored scope execution time t 41bx0 t 51xi4 t 23hl3 t 41bx0 t 01mu1 t 41bx0 t 41bx0 t 41bx0 task id figure an example of log messages and log events experiences obtained from industrial practice.
the related work and conclusion are presented in section and section respectively.
background and motivation cloud based online service systems such as microsoft azure google cloud and amazon aws have been widely adopted in the industry.
these systems provide a variety of services and support a myriad of users across the world every day.
therefore one system problem could cause catastrophic consequences.
thus far service providers have made tremendous efforts to maintain high service quality.
for example amazon aws and microsoft azure claim to have five nines which indicates the service availability of .
.
although a lot of efforts have been devoted to quality assurance in practice online service systems still encounter many problems.
to diagnose the problem engineers often rely on system logs which record system runtime information e.g.
states and events .
the top frame of figure shows eight real world log messages from microsoft some fields are omitted for simplicity of presentation .
each log message comprises two parts a constant part and a variable part.
the constant part consists of fixed text strings which describe the semantic meaning of a program event.
the variable part contains parameters e.g.
url that record important system attributes.
a log event is the abstraction of a group of similar log messages.
as depicted in figure the log event for log message is e4 http request url where the constant part is the common part of these log messages http request url and the asterisk represents the parameter part.
log parsing is the procedure that extracts log events from log messages and we defer details to section .
.
a log sequence is a sequence of log events that record a system operation in the same task.
in figure log message are sequentially generated to record a typical http request.
these log messages share the same task id t 41bx0 and thereby the corresponding log sequence is .
for a well deployed online service system it operates normally in most cases and exhibits problems occasionally.
however it does not imply that problems are easy to identify.
on the contrary problems are hidden among a vast number of logs while most logs record the system s normal operations.
in addition there are various types of service problems which may manifest different patterns occur at different frequencies and affect the service system in different 61identifying impactful service system problems via log analysis esec fse november lake buena vista fl usa log sequence types100101102103104105 occurrence figure long tail distribution of log sequences manners.
as a result it is challenging to precisely and promptly identify the service problems from the logs.
as an example figure shows the long tail distribution of types of log sequences in logarithmic scale for easy plotting which are labeled by engineers from product teams.
the first two types of log sequences occupy more than .
of the total occurrences head and are generated by normal system operations.
the remaining ones indicate different problems but they all together only take up less than .
of all occurrences long tail .
besides the occurrences of distinct problem types varies significantly.
for example the first type of problem the 3rd bar in figure is a sql connection problem which shows that the server cannot connect a sql database.
the most frequent problem occurs over times more often than the least frequent one.
the distribution is highly imbalanced and exhibits strong long tail property which poses challenges for log based problem identification.
among all the problems some are impactful because they can lead to the degradation of system kpis.
as aforementioned system kpis delineate the system s health status.
a lower kpi value indicates that some system problems may have occurred and the service quality deteriorates.
in our work we leverage both log and kpi data to guide the identification of impactful problems.
in practice systems continuously generate logs but the kpi values are periodically collected.
we use time interval to denote the kpi collection frequency.
the value of time interval is typically hour or more which is set by the production team.
in our setting we use failure rate as the kpi which is the ratio of failed requests to all requests within a time interval.
in each time interval there could be many logs but only one kpi value e.g.
one failure rate .
log3c the proposed approach in this paper we aim at solving the following problems given system logs and kpis how to detect impactful service system problems automatically?
how to identify different kinds of impactful service system problems precisely and promptly?
to this end we propose log3c whose overall framework is depicted in figure .
log3c consists of four steps log parsing sequence vectorization cascading clustering and correlation analysis.
in short at each time interval logs are parsed into log events andvectorized into sequence vectors which are then grouped into multiple clusters through cascading clustering.
however we still cannot extrapolate whether a cluster is an impactful problem which necessitates the use of kpis.
consequently in step four we correlate clusters and kpis over different time intervals to find impactful problems.
more details are presented in the following sections.
.
log parsing as aforementioned log parsing extracts the log event for each raw log message since raw log messages contain some superfluous information e.g.
file name ip address that can hinder the automatic log analysis.
the most straightforward way of log parsing is to write a regular expression for every logging statement in the source code as adopted in .
however it is tedious and time consuming because the source code updates very frequently and is not always available in practice e.g.
third party libraries .
thus automatic log parsing without source code is imperative.
in this paper we use an automatic log parsing method proposed in to extract log events.
following this method firstly some common parameter fields e.g.
ip address are removed using regular expressions.
then log messages are clustered into coarsegrained groups based on weighted edit distance.
these groups are further split into fine grained groups of log messages.
finally a log event is obtained by finding the longest common substrings for each group of raw log messages.
to form a log sequence log messages that share the same task id are linked together and parsed into log events.
moreover we remove the duplicate events in the log sequence.
generally repetition often indicates retrying operations or loops such as continuously trying to connect to a remote server.
without removing duplicates similar log sequences with different occurrences of the same event are identified as distinct sequences although they essentially indicate the same system behavior operation.
following the common practice in log analysis we remove the duplicate log events.
.
sequence vectorization after obtaining log sequences from logs in all time intervals we compute the vector representation for each log sequence.
we believe that different log events have different discriminative power in problem identification.
as delineated in step of figure to measure the importance of each event we calculate the event weight by combining the following two techniques idf weighting idf inverse document frequency is widely utilized in text mining to measure the importance of words in some documents which lowers the weight of frequent words while increasing rare words weight .
in our scenario events that frequently appear in numerous log sequences cannot distinguish problems well because problems are relatively rare.
hence the event and log sequence are analogous to word and document respectively.
we aggregate log sequences in all time intervals together to calculate the idf weight which is defined in equation where n is the total number of all log sequences and neis the number of log sequences that contain the event e. with idf weighting frequent events have low weights while rare events are weighted high.
widf e log n ne 62esec fse november lake buena vista fl usa s. he q. lin j. lou h. zhang m. r. lyu d. zhang .
log parsing t1 td .
sequence vectorization norm w idf w cor .
.
.78kpis ... .
cascading clustering .
correlation analysis .
.
.78kpis ... ... ...c1 c2 c3 c4 ... ... kpis cluster sizee1 e2 e3 e4 e5 e6 ... sum ... ...............c1 c2 c3 c4 clusters ...... t2 t1 td t2 t1 td t2 td t2 t1 figure overall framework of log3c w e norm widf e wcor e importance weighting in problem identification it is intuitive that events strongly correlate with kpi degradation are more critical and should be weighted more.
therefore we build a regression model between log events and kpi values to find the importance weight.
to achieve so as shown figure in each time interval we sum the occurrence of each event in all log sequences three in the example as a summary sequence vector.
after that we getdsummary sequence vectors and dkpi values are also available as aforementioned.
then a multivariate linear regression model is applied to evaluate the correlation between log events and kpis.
the weights wcor e obtained from the regression model serve as the importance weights for log events e. note that the regression model only aims to find the importance weight for the log event.
as denoted in equation the final event weight is the weighted sum of idf weight and importance weight.
besides we use sigmoid function to normalize the idf weight into the range of .
since the importance weight is directly associated with kpis and is thereby more effective in problem identification we value the importance weight more i.e.
.
.
in our experiments we empirically set to .
.
given the final event weights the weighted sequence vectors can be easily obtained.
for simplicity hereafter we use sequence vectors to refer to weighted sequence vectors .
note that each log sequence has a corresponding sequence vector.
.
cascading clustering once all log sequences are vectorized we group sequence vectors into clusters separately for each time interval.
however the conventional clustering methods are incredibly time consuming when the data size is large because distances between any pair of samples are required.
as mentioned in section log sequences follow the long tail distribution and are highly imbalanced.
based on the observation we propose a novel clustering algorithm cascading clustering to group sequence vectors into clusters different log sequence types promptly and precisely where each cluster represents one kind of log sequence system behavior .
figure depicts the procedure of cascading clustering which leverages iterative processing including sampling clustering matching and cascading.
the input of cascading clustering is all the sequence vectors in a time interval and the output is a number of clusters.
to be more specific we first sample a portion of sequence vectors on which a conventional clustering method e.g.
hierarchical clustering is applied to generate multiple clusters.
then a pattern can be extracted from each cluster.
in the matching step we match all the original unsampled sequence vectors with the patterns to determine their cluster.
those unmatched sequence vectors are collected and fed into the next iteration.
by iterating these processes all sequence vectors can be clustered rapidly and accurately.
the reason behind is that large clusters are separated from the remaining data at the first several iterations.
.
.
sampling.
given numerous sequence vectors in each time interval we first sample a portion of them through simple random sampling srs .
each sequence vector has an equal probability p e.g.
.
to be selected.
suppose there are nsequence vectors in the input data then the sampled data size is m p n .
after sampling log sequence types clusters that dominate in the original input data are still dominant in the sampled data.
.
.
clustering.
after sampling msequence vectors from the input data we group these sequence vectors into multiple clusters and extract a representative vector pattern from every cluster.
to do so we calculate the distance between every two sequence vectors and apply an ordinary clustering algorithm.
distance metric during clustering we use euclidean distance as the distance metric which is defined in equation uandv are two sequence vectors and nis the vector length which is the number of log events.
uiandviare the i th value in vector uand v respectively.
d u v p u v vtn i ui vi d a b max d a b a a b b min d x pj j ... k clustering technique we utilize hierarchical agglomerative clustering hac to conduct clustering.
at first each sequence vector itself forms a cluster and the closest two clusters are merged into a new one.
to find the closest clusters we use the complete linkage to measure the cluster distance.
as shown in equation dis the cluster distance between two clusters aandb which is defined as the longest distance between any two elements one in each cluster in the clusters.
the merging process continues until reaching a distance threshold of .
that is the clustering stops when all the distances between clusters are larger than .
in section .
we also study the effect of different thresholds.
after clustering similar sequence vectors are grouped into the same cluster while dissimilar sequence vectors are separated into different clusters.
pattern extraction after clustering a representative vector is extracted for each cluster which serves as the pattern of a group of similar log sequences.
to achieve so we compute the mean 63identifying impactful service system problems via log analysis esec fse november lake buena vista fl usa vector of all sequence vectors in a cluster.
assume that there arekclusters then kmean vectors patterns can be extracted to represent those clusters respectively.
.
.
matching.
as illustrated in figure we match each sequence vector in the original unsampled input data of size n to one of the kpatterns which are obtained by clustering msampled sequence vectors.
to this end for each sequence vector x we calculate the distance between it and every pattern.
furthermore we compute the minimum distance as denoted in equation where pis a set of all patterns.
if the minimum distance is smaller than the threshold defined in the clustering step the sequence vector xis matched with a pattern successfully and thereby can be assigned to the corresponding cluster.
otherwise this sequence vector is classified as mismatched.
note that those mismatched sequence vectors would proceed to the next iteration.
algorithm cascading clustering input sequence vector data d sample rate p clustering threshold output sequence clusters lobalclusters pattern set loablpatlist 1mismatchdata d 2 lobalpatlist lobalclusters 3while mismatchdata !
do sampledata sampling sequence vectors foreach seqvec ddo ifrandom pthen sampledata .append seqvec end end hierarchical clustering localclusters hac sampleddata localpatlist patternextraction clusters matching and finding mismatched data newmismatchdata foreach seqvec mismatchdata do foreach pat localpatlist do distlist .append dist seqvec pat end ifmin distlist then newmismatchdata .append seqvec end end mismatchdata newmismatchdata 25 lobalpatlist .extend localpatlist 26 lobalclusters .extend localclusters 27end 28return lobalclusters lobalpatlist .
.
cascading.
after going through the above three processes i.e.
sampling clustering and matching the majority of the data in a time interval can be grouped into clusters while some sequence vectors may remain unmatched.
hence we further process the sampling clustering pattern extraction matching log sequences matched data mismatched data figure overview of cascading clustering algorithm unmatched data by repeating the abovementioned procedures.
that is during each iteration new clusters are grouped based on current mismatched data new patterns are extracted and new mismatched data are produced.
in our experiments we cascade these repetitions until all the sequence vectors are successfully clustered.
.
.
algorithm and time complexity analysis.
the pseudo code ofcascading clustering is detailed in algorithm .
the algorithm takes sequence vectors in a time interval as input data with sample rate and clustering distance threshold as hyper parameters.
after cascading clustering the algorithm outputs all sequence vector clusters and a set of patterns.
to initialize we assign all sequence vectors dto the mismatched data.
besides we define two global variables lines to store the clusters and patterns.
then the sampled data is obtained with a sampling rate of p lines .
in lines and we perform the hierarchical agglomerative clustering hac on sampled data with threshold and extract the cluster patterns.
in fact other clustering methods e.g.
k means are also applicable here.
during the matching process line we use distlist line to store the distances between a sequence vector and every cluster pattern.
the sequence vector is allocated to the closest cluster if the distance is smaller than the threshold .
the remaining mismatched data is updated lines and processed in the next cascading round.
we now analyze the time complexity of the proposed algorithm.
note that only the core parts of the cascading clustering algorithm are considered i.e.
distance calculation and matching because they consume most of the time.
we set the data size to n which is a large number e.g.
larger than .
the sample rate pis usually a userdefined small number e.g.
less than .
for standard hierarchical agglomerative clustering the distance calculation takes o n2 time complexity and no matching is involved.
for cascading clustering suppose that pndata instances are selected and clustered into k1 groups firstly and further n1instances are mismatched.
therefore the time complexity of the first round is t1 p2n2 k1n.
after t iterations the total number of clusters is k t i 1ki.
therefore the overall time complexity t cc is calculated as t cc p2n2 k1n p2n2 k2n1 ... p2n2 t ktnt p2n2 t i 1p2n2 i k1n t i 1ki 1ni p2n2 tp2n2 kn pn p tn1 kn since nis a large number and kis the total number of clusters we have k nand kn n. because the data follows the long tail distribution and the head occupies most of the data e.g.
more than .
after several iterations most data can be successfully clustered and matched.
recall that p we then have p tn1 n 64esec fse november lake buena vista fl usa s. he q. lin j. lou h. zhang m. r. lyu d. zhang andpn n. therefore the inequality pn p tn1 kn n holds and the left hand side is much smaller.
given that f x x2 is a monotonic increasing function x where f x decreases with the decreasing of x. we then have pn p tn1 kn n2 satisfied which indicates that cascading clustering consumes much less time than standard clustering in terms of distance calculation and matching.
in our experiments we empirically evaluate the time performance of cascading clustering and the results support our theoretical analysis.
.
correlation analysis as described in figure log sequence vectors are grouped into multiple clusters separately in each time interval.
these clusters only represent different types of log sequences system behaviors but may not necessarily be problems.
from the clusters we identify the impactful problems that lead to the degradation of kpi.
intuitively kpi degrades more if impact problems occur more frequently.
hence we aim to identify those clusters that highly correlate with kpi s changes.
to do so we model the correlation between cluster sizes and kpi values over multiple time intervals.
unlike the importance weighting in section .
that discriminates the importance of different log events this step attempts to identify impactful problems from clusters of sequence vectors.
more specifically we utilize a multivariate linear regression mlr model equation which correlates independent variables cluster sizes with the dependent variable kpi .
among all independent variables those have statistical significance make notable contributions to the dependent variable.
moreover the corresponding clusters indicate impactful problems whose occurrences contribute to the change of kpi.
statistical significance is widely utilized in the identification of important variables .
c11 c12 c13 .
.
.
c1n c21 c22 c23 .
.
.
c2n ............... cd1cd2cd3.
.
.
cdn 1 2 ... n 1 2 ... d kpi kpi ... kpid in equation suppose there are nclusters generated during d time intervals cijrepresents the cluster size the number of sequence vectors of the j th cluster at time interval i.kpiiis the system kpi value at time interval i. iand iare coefficients and error terms that would be learned from data.
to find out which clusters are highly correlated with the kpi values we adopt the t statistic which is a widely used statistical hypothesis test.
in our mlr model important clusters indicating impactful problems make major contributions to the change of kpis and their coefficients should not be zero.
therefore we make a null hypothesis for each independent variable that its coefficient is zero.
then a two tailed t test is applied to measure the significant difference of each coefficient i.e.
the probability pthat the null hypothesis is true.
a lower p value is preferred since it represents a higher probability of the null hypothesis being rejected.
if p value is less than or equal to a given threshold significance level the corresponding cluster implies an impactful problem that affects the kpi.
in this paper we set to .
which is a common setting in hypothesis test.
experiments in this section we evaluate our approach using real world data from industry.
we aim at answering the following research questions rq1 how effective is the proposed log3c approach in detecting impactful problems?
rq2 how effective is the proposed log3c approach in identifying different types of problems?
rq3 how does cascading clustering perform under different configurations?
.
setup datasets we collect the real world data from an online service system x of microsoft.
service x is a large scale online service system which serves hundreds of millions of users globally on a basis.
service x has been running over the years and has achieved high service availability.
the system is operating in multiple data centers with a large number of machines each of which produces a vast quantity of logs every hour.
service x utilizes the load balancing strategies and end user requests are accepted and dispatched to different back ends.
there are many components at the application level and each component has its specific functionalities.
most user requests involve multiple components on various servers.
each component generates logs and all the logs are uploaded to a distributed hdfs like data storage automatically.
each machine or component has a probability to fail leading to various problem types.
we use failure rate as the kpi which shows the percentage of failed requests in a time interval.
table summary of service x log data data snapshot starts log seq size events types data sept 5th 722mb data oct 5th 996mb data nov 5th 407mb service x produces a large quantity of log data consisting of billions of log messages.
however it is unrealistic to evaluate log3c on all the data due to the lack of labels.
the labeling difficulties origin from two aspects first the log sequences are of huge size.
second various problem types can exist and human labeling is very time consuming and error prone.
therefore we extract logs that were generated during a specified period2on three different days.
in this way three real world datasets i.e.
data are obtained as shown in table .
besides the log data we also collect the corresponding kpi values.
during labeling product team engineers utilize their domain knowledge to identify the normal log sequences.
then they manually inspect the rest log sequences from two aspects does the log sequence indicate a problem?
what is the problem type?
table shows the number of problem types identified in the evaluation datasets.
note that the manual labels are only used for evaluating the effectiveness of log3c in our experiments.
log3c is an unsupervised method which only requires log and kpi data to identify problems.
implementation and environments we use python to implement our approach for easy comparison and run the experiments on a windows server intel r xeon r cpu e5 4657l v2 .40ghz .
with .00tb memory .
2the actual period is anonymous due to data sensitivity.
65identifying impactful service system problems via log analysis esec fse november lake buena vista fl usa table accuracy of problem detection on service x data data data data data metrics precision recall f1 measure precision recall f1 measure precision recall f1 measure pca .
.
.
.
.
.
.
.
.
invariants mining .
.
.
.
.
.
.
.
log3c .
.
.
.
.
.
.
.
.
evaluation metrics to measure the effectiveness of log3c in problem detection we use the precision recall and f1 measure.
given the labels from engineers we calculate these metrics as follows precision the percentage of log sequences that are correctly identified as problems over all the log sequences that are identified as problems precision t p t p f p. recall the percentage of log sequences that are correctly identified as problems over all problematic log sequences recall t p t p f n. f1 measure the harmonic mean of precision andrecall .
tpis the number of problematic log sequences that correctly detected by log3c fpis the number of non problematic log sequences that are wrongly identified as problems by log3c.
fnis the number of problematic log sequences that are not detected by log3c tnis the number of log sequences that are identified as non problematic by both engineers and log3c.
to measure the effectiveness of clustering we use the normalized mutual information nmi which is a widely used metric for evaluating clustering quality .
the value of nmi ranges from to .
the closer to the better the clustering results.
to measure the efficiency of cascading clustering we record the total time in seconds spent on clustering.
.
rq1 effectiveness of log3c in detecting impactful problems to answer rq1 we apply log3c to the three datasets collected from service x and evaluate the precision recall and f1 measure.
the results are shown in table .
log3c achieves satisfactory accuracy with recall ranging from .
to .
and precision ranging from .
to .
.
the f1 measures on the three datasets are .
.
and .
respectively.
furthermore we compare our method with two typical methods pca and invariants mining .
all these three methods are unsupervised log based problem identification methods.
pca projects the log sequence vectors into a subspace.
if the projected vector is far from the majority it is considered as a problem.
invariants mining extracts the linear relations invariants between log event occurrences which hypothesizes that log events are often pairwise generated.
for example when processing files file a is opened and file a is closed should be printed as a pair.
log sequences that violate the invariants are regarded as problematic.
log3c achieves good recalls similar to those achieved by two comparative methods and surpasses the comparative methods concerning precision and f1 measure.
the absolute improvement in f1 measure ranges from .
to .
on the three datasets.
the two comparative methods all achieve low precision less than .
while the precisions achieved by log3c are greater than .
.
we also explore the reasons for the low precision of the competingmethods.
in principle pca and invariants mining aim at finding the abnormal log sequences from the entire data.
however some rare user system behaviors can be wrongly identified as problems.
thus many false positives are generated which result in high recall and low precision.
more details are described in section .
.
.
regarding the time usage of problem detection on average it takes log3c .
seconds to produce the results for each dataset while pca takes around .
seconds and invariants mining consumes .
seconds.
the time performance of log3c is satisfactory considering the large amount of log sequence data.
.
rq2 effectiveness of log3c in identifying different types of problems in log3c we propose a novel cascading clustering algorithm to group the log sequences into clusters that represent different types of problems.
for the ease of evaluation clusters that represent normal system behaviors are considered as special non problem types.
in this section we use nmi to evaluate the effectiveness of log3c in identifying different types of problems.
we also compare the performance of cascading clustering denoted as cc with the standard clustering method hierarchical agglomerative clustering denoted as sc .
to compare fairly we implement a variant of log3c that replaces cc with sc denoted as log3c sc .
all the other settings e.g.
distance threshold event weight remain the same.
table nmi of clustering on service x data data 1size 10k 50k 100k 200k log3c sc .
.
.
.
log3c .
.
.
.
data 2size 10k 50k 100k 200k log3c sc .
.
.
.
log3c .
.
.
.
data 3size 10k 50k 100k 180k log3c sc .
.
.
.
log3c .
.
.
.
table presents the nmi results in which data size refers to the number of log sequences.
we sample four subsets of each dataset with size ranging from 10k to 200k for data 180k is used instead of 200k as its total size is around 180k .
from the table we can conclude that log3c with cascading clustering is effective in grouping numerous log sequences into different clusters and outperforms log3c sc on all three datasets.
besides with the increase of data size clustering accuracy increases.
this is because more accurate event weights can be obtained with more data during sequence vectorization.
for instance when 200k data is used the nmi values achieved by log3c range from .
to .
180k for data .
we also evaluate the time performance of cascading clustering.
table shows that our cascading clustering cc dramatically save 66esec fse november lake buena vista fl usa s. he q. lin j. lou h. zhang m. r. lyu d. zhang table time performance in seconds of clustering data 1size 10k 50k 100k 200k sc .
.
.
.
cc .
.
.
.
data 2size 10k 50k 100k 200k sc .
.
.
.
cc .
.
.
.
data 3size 10k 50k 100k 180k sc .
.
.
.
cc .
.
.
.
the time in contrast to the standard hac clustering sc and the comparison is more noticeable when the data size grows.
for example our approach is around 1800x faster than standard clustering on dataset with a size of 200k.
.
rq3 cascading clustering under different configurations in section .
we introduced two important hyper parameters that are used in cascading clustering the sample rate pand the distance threshold .
in this section we evaluate clustering accuracy and time performance under different configurations of parameters.
we conduct the experiments on data but the results are also applicable to other datasets.
.
.
.
sample rate102103running time in seconds time performance .
.
.
.
.
.
sample rate0.
.
.
.
.
.
.
.
.00nmi accuracy figure cascading clustering on service x data under different configurations distance threshold we first fix the sample rate and vary the distance threshold for cascading clustering.
the clustering accuracy nmi is given in table .
when is .
the highest nmi value .
is achieved.
however we also observe that nmi changes slightly when the threshold changes within a reasonable range.
the results show that our proposed cascading clustering algorithm is insensitive to the distance threshold to some degree.
sample rate p it is straightforward that sample rate can affect the time performance of cascading clustering because it takes more time to do clustering on a larger dataset.
to verify it we change the sample rate while fixing the distance threshold.
figure depicts the results.
we conduct the experiments with different sample rates under three distance thresholds .
.
and .
.
the left subfigure shows that a higher sample rate generally causes more time usage.
however when the sample rate is very small e.g.
.
a little more time is required.
this is because in cascading clustering a small sample rate leads to more iterations of clustering which hampers the efficiency of cascading clustering.
besides we alsotable nmis of cascading clustering under different distance thresholds .
.
.
.
.
.
.
.
nmi .
.
.
.
.
.
.
.
evaluate the clustering accuracy under different sample rates.
as shown in the right sub figure of figure clustering accuracy nmi is relatively stable when the sample rate changes.
it shows that the nmi value floats slightly with a small standard deviation of .
.
in summary we can conclude that generally a small sample rate does not affect clustering accuracy and cost much less time.
this finding can guide the setting of the sample rate in practice.
discussions .
discussions of results .
.
performance of cascading clustering with different numbers of clusters.
in section we explored the efficiency and effectiveness of cascading clustering on real world datasets.
in this section we evaluate our cascading clustering algorithm on some synthetic datasets.
more specifically we generate some synthetic datasets with different number of clusters.
to simulate a scenario that is similar to problem identification we synthesize several synthetic datasets consisting of multiple clusters where the cluster sizes follow the long tail distribution.
in more detail we firstly synthesize a dataset of multiple clusters and the data sample dimension is fixed at .
the data samples in each cluster follow the multivariate normal distribution .
then we use the pow law function i.e.
f x x k to determine the size of each cluster with some gaussian noises added as noises always exist in real data.
in this way we can generate multiple datasets with different data sizes from 20k to 600k and various numbers of clusters from to .
20k 50k 100k 200k 300k 600k data size100101102103104running time in seconds clustersstandard clustering cascading clustering 20k 50k 100k 200k 300k 600k data size100101102103104 clusters figure time performance of clustering methods on synthetic data with cluster left and clusters right figure shows the time performance in logarithmic scale for easy plotting of standard clustering and cascading clustering where the number of clusters is and .
we also vary the synthetic data size from 20k to 600k.
it is clear that cascading clustering requires much less time than standard clustering hierarchical agglomerative clustering with different cluster numbers.
for example standard clustering takes .
seconds around .
hours on 200k data 67identifying impactful service system problems via log analysis esec fse november lake buena vista fl usa table nmis of standard clustering sc and cascading clustering cc on synthetic data size 20k 50k 100k 200k 300k 600k clusters sc cc sc cc sc cc sc cc sc cc sc cc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
with clusters while cascading clustering sample rate is only takes .
seconds on the same dataset.
our cascading clustering is more than faster than standard clustering.
in table we measure the clustering accuracy in terms of nmi under different data sizes and cluster numbers.
we can conclude from the table that overall cascading clustering leads to equal or slightly better accuracy when compared with the standard clustering.
the main reason is that our cascading clustering algorithm is specially designed for long tailed data.
the small clusters can be precisely clustered.
moreover the evaluation results of standard clustering on 600k data are not available due to the out of memory more than 1tb computation.
from table we can also observe that clustering accuracy increases with the increase of cluster number and decreases when the data size increases.
.
.
impact of log data quality.
the quality of log data is crucial to log based problem identification.
for a large scale service system logs are usually generated on local machines which are then collected and uploaded to a data center separately.
during the process some logs may be missed or duplicated.
for duplicated logs they do not affect the accuracy of log3c as duplicates are removed from log sequences as described in section .
.
to evaluate the impact of missing log data we randomly remove a certain percentage missing rate of logs from data and then evaluate the accuracy of log3c.
we use three different missing rates .
.
and .
the resulting f1 measures are .
.
.
respectively.
it can be concluded that a higher missing rate could lead to a lower problem identification accuracy.
therefore we suggest ensuring the log data quality before applying log3c in practice.
.
threats to validity we have identified the following threats to validities subject systems in our experiment we only collect log data from one online service system service x .
this system is a typical large scale online system from which sufficient data can be collected.
furthermore we have applied our approach to the maintenance of actual online service systems of microsoft.
in the future we will evaluate log3c on more subject systems and report the evaluation results.
selection of kpi in our experiments we use failure rate as the kpi for problem identification.
failure rate is an important kpi for evaluating system service availability.
there are also other kpis such as mean time between failures average request latency throughput etc.
in our future work we will experiment with problem identification concerning different kpi metrics.
noises in labeling our experiments are based on three datasets that are collected as a period of logs on three different days.
the engineers manually inspected and labeled the log sequences.
noises false positives negatives may be introduced during the manual labeling process.
however as the engineers are experienced professionals of the product team who maintain the service system we believe the amount of noise is small if it exists .
success story and lessons learned .
success story log3c is successfully applied to microsoft s service x system for log analysis.
service x provides online services to hundreds of millions of global end users on 24basis.
for online service systems like service x inspecting logs is the only feasible way for fault diagnosis.
in service x more than one terabyte of logs around billions are generated in a few hours and it is a great challenge to process the great volume of logs.
a distributed version of log3c is developed and employed in service x. billions of logs can be handled within hours using our method which helps the service team in identifying different log sequence types and detecting system problems.
for example in april a severe problem occurred to one component of service x on some servers.
the problem was caused by an incompatibility issue between a patch and a previous product version during a system upgrade.
the service team received lots of user complains regarding this problem.
our log3c successfully detected the problem and reported it to the service team.
the service team also utilized log3c to investigate the logs and precisely identified the type of the problem.
with the help of log3c the team quickly resolved this critical issue and redeployed the system.
log3c is also integrated into microsoft s product b an integrated environment for analyzing the root causes of service issues.
tens of billions of logs are collected and processed by product b every day in which log3c is the log analysis engine.
using log3c product b divides the log sequences into different clusters and identifies many service problems automatically.
log3c greatly reduces engineers efforts on manually inspecting the logs and pinpointing root causes of failures.
furthermore fault patterns are also extracted and maintained for analyzing similar problems in the future.
.
lessons learned .
.
problems !
outliers.
recent research proposed many approaches to detect system anomalies using data mining and machine learning techniques.
these approaches work well for relatively small systems.
their ideas are mainly based on the following hypothesis systems are regular most of the time and problems are outliers .
many current approaches try to detect the outliers from a huge volume of log data.
for example pca attempts to map all data to a normal subspace and those cannot be projected to the normal space are considered as anomalies.
68esec fse november lake buena vista fl usa s. he q. lin j. lou h. zhang m. r. lyu d. zhang however the outliers are not always real problems.
some outliers are caused by certain infrequent user behaviors e.g.
rarely used system features.
our experiences with the production system reveal that there are indeed many rare user behaviors which are not real problems.
a lot of effort could be wasted by examining these false positives.
in our work we utilize system kpi to guide the identification of real system problems.
.
.
the trend of problem reports is important.
in production engineers not only care about the occurrence of a problem but also about the number of problem reports i.e.
the instances of problems over time which reflects the number of users that are affected by the problem over time .
through our communication with a principal architect of a widely used service in microsoft we conclude three types of important trends increasing.
when the size of one certain problem continuously increases for a period the production team should be notified.
this is because the number of problem reports may accumulate and cause even serious consequences.
the appearance of new problems when a previously unknown problem appears it is often a sign of new bugs which may be introduced by software updates or a newly launched product feature.
the disappearance of problems the disappearing trend is very interesting.
in production after fixing a problem the scale of the problem is expected to decrease.
however sometimes the disappearing trend may stop at a certain point the service team continues to receive reports for the same problem which often indicates an incomplete bug fix or a partial solution.
more debugging and diagnosis work are needed to identify the root cause of the problem and propose a complete bug fixing solution.
related work .
log based problem identification logs have been widely used for the maintenance and diagnosis of various software systems with the abundant information they provide.
log based problem identification has become a very popular area in recent years.
typically based on information extracted from logs these work employ machine learning and data mining techniques to analyze logs for anomaly detection and problem diagnosis .
the target of anomaly detection is to find system s abnormal behaviors and give feedback to engineers for further diagnosis.
lou et al.
mined the invariants linear relationships among log events to detect anomalies.
liang et al.
trained a svm classifier to detect failures.
however anomaly detection can only determine whether a log sequence is abnormal or not it cannot determine if there is a real problem.
our log3c can not only detect system problems but also cluster them into various types.
problem identification aims at categorizing different types of problems by grouping similar log sequences together.
lin et al.
proposed a clustering based approach for problem identification.
based on testing environment logs a knowledge base is built firstly and updated in production environment.
however it requires manual examination when new problems appear.
yuan et al.
employed a classification method to categorize system traces by calculating the similarity with traces of existing and known problems.
beschastnikh et al.
inferred system behaviors by utilizinglogs which can support anomaly detection and bug finding.
ding et al.
correlated logs with system problems and mitigation solutions when similar logs appear.
shang et al.
identified problems by grouping the same log sequences after removing repetition and permutations.
however they ignored different importance of log events and the similarity between two log sequences.
in our work we use cascading clustering to quickly and precisely cluster log sequences into different groups and correlate clusters with the kpis to identify problems.
our approach does not require manual examination nor a knowledge base of known problems.
.
log parsing and logging practice logs cannot be utilized towards automatic analysis before being parsed into log events.
many log parser e.g.
logsig slct iplom have been proposed in recent years and the parsing accuracy can significantly affect the downstream log analysis tasks .
xu et al.
extracted log events from console logs using the source code.
fu et al.
parsed logs by clustering with weighted edit distance.
makanju et al.
proposed a lightweight log parsing method by iteratively partitioning logs into subgroups and extracting log event.
recently there are also research on logging practice .
for example fu et al.
yuan et al.
and shang et al.
provide suggestions to developers by exploring the logging practice on some open source systems and industry products.
zhu et al.
proposed a method to guide developers on whether to write logging statements.
kabinna1 et al.
examined the stability of logging statements and suggested to developers the unstable ones.
the above research improves the quality of logs which could in turn help with log based problem identification.
conclusion large scale online service systems generate a huge number of logs which can be used for troubleshooting purpose.
in this paper we propose log3c a novel framework for automated problem identification via log analysis.
at the heart of log3c is cascading clustering a novel clustering algorithm for clustering a large number of highly imbalanced log sequences.
the clustered log sequences are correlated with system kpi through a regression model from which the clusters that represent impactful problems are identified.
we evaluate log3c using the real world log data.
besides we also apply our approach to the maintenance of actual online service systems.
the results confirm the effectiveness and efficiency of log3c in practice.
in the future we will apply log3c to a variety of software systems to further evaluate its effectiveness and efficiency.
also the proposed cascading clustering algorithm is a general algorithm which can be applied to a wide range of problems as well.
acknowledgement the work described in this paper was supported by the national natural science foundation of china project no.
and the research grants council of the hong kong special administrative region china no.
cuhk of the general research fund the national basic research program of 69identifying impactful service system problems via log analysis esec fse november lake buena vista fl usa china project no.
2014cb347701 and the microsoft research asia microsoft research asia collaborative research award .
we thank the principal architect anthony bloesch and the intern student yanxia xu from product teams for their valuable work.
we also thank partners at microsoft product teams for their collaboration and suggestions on the application of log3c in practice.