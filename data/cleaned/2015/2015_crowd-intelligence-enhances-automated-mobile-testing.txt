crowd intelligence enhances automated mobile testing ke mao mark harman and y ue jia facebook london facebook brock street london nw1 3fg uk crest university college london malet place london wc1e 6bt uk kemao markharman yuej fb.com abstract we show that information extracted from crowdbased testing can enhance automated mobile testing.
we introduce p olariz which generates replicable test scripts from crowd based testing extracting cross app motif events automatically inferred reusable higher level event sequences composed of lower level observed event actions.
our empirical study used crowd workers from mechanical turk to perform testing tasks on popular google play apps each with at least1 million user installs.
the findings reveal that the crowd was able to achieve .
unique activity coverage and proved to be complementary to automated search based testing in out ofthe subjects studied.
our leave one out evaluation demonstrates that coverage attainment can be improved out of cases with no disimprovement on the remaining by combining crowd based and search based testing.
index t erms crowdsourced software engineering mobile app testing test generation i. i ntroduction there has been much recent progress in automated testing with recent advances in automated testing of mobile apps .
however automated test data generation techniqueslack domain knowledge and may generate either unrealistictest cases or fail to find test cases that explore aspects of func tionality that matter to users .
a recent study of open sourceandroid apps with relatively simple user flows reportedthat current state of art automated mobile testing techniquesachieve only approximately statement coverage.
fortunately linares v asquez et al.
recently showed how app execution usage data can be mined for valuable insights while moran et al.
subsequently introduced crashscope which supports the discovery of app crashes and their replication.
there has also been considerable recent interest inthe possibilities of crowdsourcing as a means of collectingsuch usage data in a cost effective manner.
this recent worksuggests that data mining and extraction perhaps from crowd sourced usage might discover useful cross app patterns thatimprove automated app testing performance.
we explore the complementarity between automated machine generated tests and human crowd generated tests.
we specifically focus on the ability of crowd based tests to this research forms part of the phd work of ke mao the lead author supervised by y ue jia and mark harman while all three were at university college london.
dr. mao dr. jia and prof harman moved to facebook fulltime in february and dr. jia and prof. harman also retain part time positions at ucl.assist the state of the art search based testing tool sapienz.
an open source research prototype of sapienz was releasedin and the technology that underpins it has been underdevelopment at facebook since february .
in this paperwe use the open source version of sapienz to facilitate repli cation.
the sapienz approach to search based testing is well suited to augmentation with crowd based tests due to sapienz concept of a motif gene a sequence of low level events thathas a context sensitive meaning to the app s users therebydenoting an atomic event to users although appearing tobe a non atomic event sequence to the device and any testingapproach that lacks the necessary context awareness .
we show that these strands of work on mining crowdsourcing and automated test generation can be combined in amutually complementary hybrid.
our hybrid uses automatedtest generation to explore the search space of test cases informed by data mined from crowdsourced tests to identifymotif genes.
to do this we introduce a crowd based approach called p olariz which collects and analyses test inputs from a crowd call to non technical users with no specific softwaretesting expertise or experience.
that is the call is open to anycrowd workers to participate whether or not they have testing expertise.
however since it is an open call in the spirit ofcrowdsourcing we cannot guarantee that we do not recruit any crowd workers with testing expertise.
p olariz uses a platform with a mobile device infrastructure remote device control and screen streaming automated subject distribution permission control and crowd trace col lection.
with this approach a non professional crowd fromthe general public such as those from amazon mechanicalturk can contribute to mobile testing from anywhere withany clients with a web browser e.g.
desktop pc android ios or windows phone mobile devices .
following linares v asquez et al.
we introduce a novel data mining algorithm to extract motif event sequences sequences composed of lower level events that our approachinfers may denote higher level atomic units thereby providingone source of guidance for automated testing.
we define a motif event sequence or motif pattern as a common user interaction pattern that is learned from some apps and can be subsequently generalised to other apps such that the motifsequence can play the role of a higher level atomic event thatcan be re used to assist automated mobile testing.
.
c circlecopyrt2017 ieeease urbana champaign il usa t echnical research16 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a motif pattern may occur multiple times there may be many instances of a motif pattern each occurrence of which we refer to as motif events .
our approach thus bridgesthe gap between automated mobile test input generationtechniques and human domain context awareness using non professional crowd testers.
the primary contribution of our work is the scientific evidence that motifs extracted from crowdsourced tests can complement and extend state of the art automated test gener ation.
more specifically the contributions of our work are asfollows we introduce and implement the p olariz approach for harnessing crowd intelligence to support mobile testing.
using our implementation we report the results of thefirst empirical study of crowdsourcing for mobile test automation.
we posted tasks on amazon mechanical turk to test popular google play apps each with at least1 million user installs.
the crowd was able to attain .
overall unique activity coverage.
we compare the results of app activities covered by the crowd with those by the automated search based android testing tool s apienz revealing complementarity between the two.
we chose s apienz because it has recently been shown to outperform other state of the art and stateof practice tools.
unsurprisingly the crowd imbued withits superior domain knowledge achieved higher activitycoverage on all but one subject google translate forwhich s apienz produced slightly higher coverage .
more importantly for the subjects the two techniques ex hibited complementary coverage motivating our goal ofcombining them.
we introduce a motif extraction algorithm and demonstrate its effectiveness by showing that it can enhance s apienz coverage.
for of the subjects the coverage is improvedby motif extraction with the best case improvement in creasing coverage obtained from to of possible unique activities.
ii.
t hepolariz approach the p olariz approach is designed to tackle the two main challenges involved in harnessing the non professional crowdto perform remote mobile testing and further to learn fromcrowd intelligence embodied in the crowdsourced manuallyconstructed tests.
the first challenge requires an intermediary platform able to harness a general public crowd to work on remote mobile testing tasks.
the second challenge involvesthe representation and extraction of useful crowd intelligence.
figure depicts the high level p olariz workflow.
three actors are involved in the workflow app developer researcher who seeks mobile test automation enhancement crowd workers testers the intermediary platform i.e.
p olariz platform on which the crowd works.
fig.
.
overall workflow of polariz polariz uses its own device infrastructure users do not execute apps on their own devices.
this insulates the user from security issues while insulating p olariz against android device fragmentation.
it also gives p olariz full control over real time data collection and monitoring.
however it means that p olariz can only collect touch screen events not devicespecific events such as gps and accelerometer events.
it also involves a latency since testing activity occurs over thenetwork which we checked and report on.
fortunately theseresults indicate that the latency is acceptable.
the outputs consist of three parts first the bug reports automatically generated during the crowd testing process second the replicable test scripts generated based on crowd sourced test manual traces which can be replayed via an an droid test replayer such as reran third the automati cally summarised motif events learned from crowd interactiontraces which can be further used to enhance existing search based mobile test generators such as s apienz .
both the android test replayer and test generator can remotely connect to p olariz mobile device infrastructure for test execution.
polariz top level consists of its crowd testing platform for manual trace collection and its crowd motif extractionalgorithm for learning from crowd intelligence .
our platformuses crashing as an implicit oracle automatically capturingcrash triggering stack traces event sequences and witnessvideos using the existing s apienz infrastructure .
a. the polariz platform the platform is illustrated in figure .
given a set of mobile apps under test p olariz subject dispatcher component automatically instruments assigns and installs each app on a devicein its mobile device infrastructure.
the screen streamer anddevice controller provide web services for controlling these devices.
crowd users simply access the remote devices which install apps via web browsers from any user platform.
exposing our hosted mobile devices to the general public might raise security concerns so p olariz has a permission control component that monitors crowd interactions onlypermitting testing activities on the specified subjects.
duringthe the crowd testing process p olariz logging components such as a crash detector and trace collector automaticallycollect the information from which p olariz generates its reports.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
detailed components of polariz platform b. the crowd motif extraction algorithm our use of crowd motif patterns stems from dna sequence motif.
according to d haeseleer a dna sequence motif is a short over represented pattern with anassumed biological function.
the crowd motif extraction prob lem is to find a set of recurring substrings within a set ofstrings which can be described as follows given a set of ncrowd generated test event sequences s s ... s n each formed by events from an event set y swipe rotate f lip p inch click roi cl i c k roi ... pre s s key1 pr e s s key2 ... the motif extraction problem is to find a set of instances m m1 ... m n n n where each miis a wsized subsequence of sequences in sthat maximises m s information content ic m according to the equation icm w summationdisplay i summationdisplay y ypy ilogpy i by where py i is the probability of event yat position iin m and byis the probability of event yin the background distribution.
thus icmcomputes the relative entropy of the event sequences in m favouring those sub sequences that are prevalent in the crowd s behaviour yet are relatively lessprevalent in the overall distribution.
our approach is inspiredby dna sequence motif discovery .
we use a genetic algorithm to extract crowd motif events for mobile testing.
the adapted crowd motif extraction algorithmis listed in algorithm .
the algorithm extracts multiple motifpatterns from a set of collected log trace pairs.
the log pro vides subject execution state information such as transitionsfrom one app activity to another and the trace saves manualinteractions that were used to trigger the app s state changes.the log and trace items are linked via their timestamps.
in order to learn from the wisdom of the crowd lines extract the minimum trace that enables a transition fromone activity to another.
that is there may exist many ways sequences of events through which the user interactionstrigger the same app activity.
we favour the minimum trace that requires the fewest operations.
the generated minimumtransitional trace collection s is represented as a list of strings in which each string is a minimum trace that triggers a specific a to b activity transition.algorithm crowd motif extraction algorithm 1description findmmotif patterns from nsubject log trace pairs.
input a list of log trace pairs d where liis the app execution state log and tiis the app event traces for the ith app number of motif patterns to find m. output a list of recurring motif patterns r .
2r s initialisation get the minimum operations to switch from one activity to another 3for each l t inddo 4s s getminimumactivitytransitiontrace l t findmmotif patterns by evolving candidate motif substring locations 5foriinrange m do generation g for each individual generate random candidate motif locations in s 7p initialisepopulation s evaluatepby calculating icmfor each individual in q see equation whileg max generations do p prime tournamentselection p q variation p prime crossover and mutate motif locations and length evaluateqby calculating icmfor each individual in q q elitismselection q p g g p q 16r getbestindividual p may contain zero or one motif location for s s 17r r r excluding found motif substrings for next motif pattern 18s removemotifsubstrings s r 19returnr lines use a genetic algorithm to find multiple motif patterns.
at each iteration the algorithm finds a single motif pattern and excludes the matched motif substrings from s line .
each individual genetic algorithm population member represents a candidate motif pattern a list of candidate motif locations in s. the individual s fitness is evaluated based on the information content score as described by equation .
the variation operator line applies both crossover and mutation to manipulate the location and length of eachmotif substring.
the best individuals with highest information content score i.e.
their motif substrings are most conservative are selected for the next generation.
in this way our genetic algorithm uses elitism in its selection and retention.
evolutionstops after a given maximum number of generations savingthe best individual found.
the overall process repeats until all mmotif patterns have been discovered.
our implementation consists of the two top level components as described in section ii to produce the platform shown in figure .
p olariz implementation s mobile device infrastructure consists of nexus tablets connected to a hostpc via a usb hub.
we adapt the android getevent tool fortrace recording and use the reran tool for trace replay the getevent tool captures a list of low level android eventson the fly saving to a script which is subsequently interpretedby the reran tool.
for remote device control we use theopen sourced openstf project and we deployed the platform to a server with a proxy service to speed up global visits.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
we use the s apienz implementation obtained from the open source prototype made available by the s apienz research project .
when improving s apienz we integrate the learned motifs into s apienz motifcore component which combines the motif patterns which s apienz calls motif genes with atomic genes .
in order to learn the crowd motifs from the collected event traces we implement the p olariz motif extraction component in python according to algorithm1.
s apienz can record and replay event sequences and uses the low level android monkey representation of events to gothis.
this low level representation is not immediately human readable but since our event sequences and motifs extracted are intended purely for machines this is not a problem.
iii.
e mpirical ev alua tion of polariz we want to investigate the usefulness of p olariz both as a source of test data garnered from an untrained and technicallynon specific crowd and also as a mechanism for augmentingexisting automated techniques for test data generation.
in thissection we outline and motivate the four research questionsthat we choose to pose and answer in this paper rq1 demographics and behviour before investigating the nature of test cases generated and analysed from our crowdworkers we first report on the demographic diversity andbehavioural characteristics of the crowd.
we do this to supportcomparison and replication and subsequent study which willexhibit inherent variability due to the nature of the crowdrecruited for any such subsequent study.
rq1.
the demographic diversity of the crowd rq1.
reports on the diversity of the crowd workers recruited in orderto perform the testing activities in our study.
in order for thecrowd to denote an affective source of test data which exhibitsdiversity the crowd itself will need to be diverse.
this diversity is important in order to ensure that the test cases explore app behaviour exercised by all of the types of users who may usethe application under test.
it would also be important for themotif extraction approach because this needs to generalisefrom a set of instances observed from crowd behaviour.
if welack diversity in the crowd then there is the possibility that the motif extraction algorithm will overfit.
we report on the diversity of the crowd in terms of demographic distribution gender educational background and prior experience both with mobile applications in general andsoftware testing in particular.
we also report on the level ofreturning workers those who come back to complete furthertesting tasks that we set having already tackled our previoustesting tasks.
rq1.
the crowd s interest level in order to be sufficiently motivated to tackle the testing tasks we set the crowd needsto feel interested in these tasks.
the tasks we set are notspecifically related to testing but simply involve using theapplications under test.
we survey the crowd for their selfassessed level of interest on a standard lickert scale in orderto provide some initial evidence relating to the level of crowdinterest.rq1.
the crowd s response rates we also investigate the behaviour of the crowd with respect to response rates reporting on the distribution of the number of tasks submittedper crowd worker and their performance in terms of speedof acceptance and completion of tasks.
it is impossible toaccurately measure the time a crowd member specificallydevotes to a task because we cannot know whether they aresolely focused on the task.
nevertheless we can report on thetime between creation and acceptance of the task betweenacceptance and submission of the task and also the total loggedtime that a crowd member spends working on a task.
rq2 the crowd s coverage attainment we use the crowd as a source of test data in its own right as well as the abilityof the crowd to provide observations from which we canextract motif patterns for automated test techniques.
in orderto investigate the crowd s value as a source of test data in its own right we report on the coverage obtained by the crowd asthe number of tasks completed increases.
we report both the overall number of unique and non unique activities covered and also the level of activity coverage per subject for each ofthe nine subject apps under test.
having investigated the demographics and behaviour of the crowd of their ability to generate test data we move on toconsider the relationship between crowd based testing andautomated testing.
in particular we compare crowd testing forandroid with a recently proposed state of the art technique s apienz for automated test data generation for android using search based software testing.
we first compare thecrowd s and s apienz coverage attainment in terms of unique activities covered and then investigate the degree to whichmotif patterns extracted from the crowd using our motifextraction algorithm can improve the coverage performance of s apienz .
rq3 the comparative activity coverage achieved by thecrowd and by s apienz in order to answer this question we report on the number of unique activities covered bythe crowd and by s apienz and their intersection.
this allows us to explore the degree to which the two techniquesare complementary to one another and also the degree ofoverlap between automated testing and crowd based testing.should it turn out that the automated technique subsumes thehuman based technique then there would be little point ininvestigating motif pattern extraction but if crowd testing canachieve better or different complementary coverage then thissuggests that there may be scope to improve automated testdata generation with motif pattern extraction from observedcrowd based tests.
the current version of p olariz records coverage but not faults found future work will extend it torecord detailed fault context.
rq4 the improvement in s apienz performance when using motif patterns extracted from crowd based tests finally we investigate the degree to which s apienz coverage is improved for each of the nine apps under test when s apienz is imbued with information extracted from the crowd basedtesting in the form of motif patterns.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i nine popular google pla y subject apps a for number of activities m for number of methods i nstalls is in millions subject ver.
category description a m rating installs hp all in one printer remote4.
.
productivity help users scan and print documents with hp printers.
.
50m tunein radio .
music audio let users listen to radio stations for free.
.
500m trainline .
.
maps navigation a railway information provider.
.
5m power security .
.
tools scan and kill viruses malware and spyware.
.
10m google translate .
.
tools translate between languages.
.
500m brightest flashlight .
productivity a multi functional flashlight app.
.
10m duolingo .
.
education learn multiple languages fast fun and free.
.
100m clean my android .
.
productivity a light phone cleaner and app manager.
.
5m citymapper .
maps navigation a journey planner and route finder.
.
5m a. subject applications we perform the empirical evaluation on randomly selected real world google play apps from top most popular free in app purchase apps as listed in the google play appstore on december .
we chose subjects from alist of all apps filtered based on their availability for ourhardware resources nexus tablets in the p olariz device infrastructure and constraints imposed by a desire to use thesubjects in experiments on the crowd based testing.
that is when we perform the random selection we first exclude gaming apps that are not based on standard androidnative ui components.
also to protect the crowd testers privacy we also exclude apps that request user account in formation after launching.
the crowd was also notified thatthey should not disclose any personal information during thetesting process.
the apps that were selected randomly after this filtering process are closed sourced and cover multiple app categories.each app has at least million user installs according togoogle play .
detailed subject information including versionnumbers sizes ratings and the number of installs are presented in table i. b. experimental settings for each subject we assign the same app running environment i.e.
the same software and hardware configurations.
these configurations mimic general real world end usertesting scenarios e.g.
with real devices that have googleservice framework and wifi network connection but withoutproviding app specific contexts.
for example the hp all in one printer remote app may require an hp printer fortesting some of its functionalities.
in our experiments we donot provide such app specific equipment for the generalisationpurpose.
we also need to recruit crowd workers and manage payments by using a third party intermediary.
we report on ourapproach to tackling these issues in the remainder of the section in order to support replication and to give the contextto the results we present for crowd based testing.crowd recruitment we use amazon mechanical turk amt for recruiting non professional crowd workers from the general public.
amt is currently one of the most popularcrowdsourcing platforms for micro tasks with general crowd workers.
we recruit amt workers to perform remote mobile testing tasks on our p olariz platform by posting human intelligence tasks hits on amt.
anyone from any country who is eligible to work on amt is allowed to work onour hit assignments.
we only disclose the task informationand our p olariz web service url via the amt hit for controlling the worker sources i.e.
only amt workers are expected because this may interfere the recruitment speed and polariz visitor statistics.
to motivate the crowd we provide .
usd payment for each approved submission as the extrinsic incentive to the crowd workers.
we expect each worker will spend minutes or less on one hit assignment.
the payment rate is higher than current uk national minimum wage .
gbp hour andalso the us standard .
usd hour .
intrinsic incentivesinclude the opportunity to experience manual mobile testing and maybe also to test the remote apps for fun we investigatethe task interest level in the results section .task design and quality control a clear task descriptionis considered to be one of the most important factors forsuccessful software crowdsourcing tasks .
this motivatesour careful design of our hit.
the general workflow of our designed hit task is as follows the crowd worker views the task assignment description on amt and can choose to accept or decline the task.
the worker follows the task instruction and works on our p olariz platform via any devices with a browser and performs manual testing on one arbitrarily selected app.
upon finishing the testing task the worker copies the polariz generated app execution log as the proof of task completion and goes back to the amt hit.
the worker submits the automatically generated log and answers a questionnaire which contains brief questions.
the worker waits for requester s review and gets paid via amt assuming their submission meets our sanity check for appropriate engagement.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
in the task description for comprehensive testing we instruct the workers that the goal is to explore and trigger as manyfunctionalities of the subject app as possible.
a few detailedsteps for accessing our p olariz platform are illustrated using snapshots.
in the questionnaire we ask short questions to collect their feedback on the interest level of the task and background information regarding the workers includingtheir daily mobile usage duration software testing experience country gender and education level.
no personal informationthat may reveal the worker s personal identity is collected.
for quality control we give three criteria to the workers which form our sanity check for task approval and con sequent payment first the worker has tried to explore and trigger multiple app functions preferably as many as possible .second the worker has tested the app for at least minutes.third the submitted app execution log contains at least lines.
normally these criteria can be easily satisfied by testingthe app for a few minutes.
we review each submission bychecking above three criteria in a semi automated manner.
we measure these three criteria based on the submitted logs for criteria we use at least two activities as the lower bound .
as a further sanity check we also manually inspectthe submissions periodically.
if a submission is rejected wedo not repost that assignment.
we posted assignments from december to january in a continuous manner in order to leave timeto perform daily reviews.
these assignments were splitinto hits each containing assignments.
all hits andtheir assignments are the same.
each hit may contain oneor more task assignments.
each worker can work on multiple hits but can only work on one assignment in one hit.
our quality control filter removed .
of these hits to leave1 for subsequent testing and motif extraction.
p olariz deployment we deploy p olariz as a publicly accessible web service at a server located in the uk plusa linode cloud server as a proxy to speed up global visits.
the mobile device infrastructure is hosted at the author s lab and connected to the front end server.
accessing the remotedevice does not require authentication but the mobile devicesare monitored and manipulated under p olariz permission control component where changes to environment settings areprohibited.
the subjects are pre installed on nexus tablets one per device.
user interactions are logged with timestampinformation which can be mapped to the submitted logs.
allsubjects are reset to their initial states every half an hour.
thisis to avoid the case that one worker drives the app into a state from which the subsequent workers cannot recover.
of course we could have chosen reset app state per worker but we foundthat multiple workers can collectively work on one aut bysetting the reset duration to minutes.performance metrics we measure coverage attained sincethis is a fundamental metric for testing .
in terms of granularity of coverage we measure app activity coverage anapproach to coverage measurement that has been adopted inprevious studies on automated mobile testing .fig.
.
crowd worker demographic information based on submitted assignments this metric thus gives us a baseline against which to assess and compare the ability of tests to explore the aut.
motif extraction we learn generalised event patterns because high level events learned from only a single subjecthas already been proved useful in the literature .
in ourexperiment we perform a leave one out evaluation on theextracted crowd motifs.
that is when evaluating a subject we will use only the motifs extracted from the remaining 8subjects event traces.
for each subject we apply the p olariz motif extraction algorithm to learn three motif patterns.
improving s apienz to examine whether the learned generalised motifs are helpful in improving app activity coverage we run s apienz without any motif information and compare results to these obtained from running s apienz with the learned crowd motifs.
on each subject we run s apienz for minutes wall clock time.
we set the delay betweeneach two events to ms so that given the same amountof wall clock time roughly the same number of events willbe used.
this setting aims for a fair comparison betweenthe two s apienz versions with and without motif patterns.
for s apienz parameters we use the default settings as reported by the authors of the s apienz paper .
in all experiments the parameters were not tuned to avoid anyimplicit experimental biases that might otherwise arise.
we run all experiments on the same macbook pro with .3ghz intel core i7 cpu and 16g ram.
the mobile sidefor app execution is a nexus real device.
iv .
r esults the results show that p olariz successfully assisted the crowd workers to complete all amt tasks from de cember to january .
we also find evidence tosupport the claim that there is complementarity between thecrowd based tests and search based tests found by s apienz .
we further report evidence to support the claim that motif pat terns extracted using or algorithm can improve the attainmentof activity coverage by s apienz .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a. rq1 demographics and behaviour rq1 is decomposed into three sub questions concerning demographics interest level and crowd behaviour each of which we report on below.
rq1.
the demographic diversity of the crowd according to visitor tracking data from google analytics from december22nd.
to january 2nd.
there were sessions of visits to our remote crowd testing service.
of these sessions .
come from new visitors and .
from returningvisitors.
the records show the traffic comes from at least 9countries with most coming from the usa .
andindia .
.
note that there are other countries with largepopulations such as china whose workers are ineligible towork on amt so there are no visits from these countries.the average response times from the global sites accessedby p olariz during the course of our empirical study range from .
ms london to .
ms sydney .
this resultindicates a reasonably good connectivity of our distributedinfrastructure for performing remote testing over a wide range of geographical locations.
during the days of experimentation time our posted hit assignments were all finished by the crowdworkers.
of all submitted solutions .
wereapproved according to the criteria for quality control discussed in section iii b. we received submissions from dis tinct workers.
results from our questionnaire show that theseworkers come from countries while submissions arefrom the top most frequently submitting countries as listed in figure .
note that the number of countries is inconsistent with the traffic we observed according to google analytics the questionnaire reveals a far wider country participation thanthat would be suggested by the google analytics data.
ourinterpretation is that a small number of amt workers may use proxies to visit the amt to overcome the fact that the service is disabled in their countries .
figure presents worker demographic information based on the responses submitted by the crowd.
this self assessment is broadly consistent with the analytics data re ported by google most workers come from usa .
and india .
.
however using the self assessment ques tionnaire we were able to obtain further demographic infor mation more male workers .
submitted than femaleworkers .
.
regarding the educational level .
workers at least attended some college education includingundergraduate students .
this generally high educational levelis consistent with previous studies although ourresults show that there are more workers with some collegeeducation than those holding a bachelor s degree.
since our remote testing tasks require basic skills for interacting with mobile apps we expected the crowd to have reg ular daily mobile usage.
our questionnaire results on dailymobile usage suggest that only .
of the respondentsspend less than hour per day on mobile usage indicatingthat our expectation is reasonable.fig.
.
worker feedback on self assessed interest level of the task v ery interesting v e r yboring we also recruit the crowd from the general public rather than software testing experts.
as the distribution presented in figure indicates .
respondents have less than one yeartesting experience and the remaining .
have at least oneyear s experience in software testing.
given that we recruitfrom the general public a proportion of over a quarter havingtesting experience was a surprise to us since testers do notoccupy of the world s population .
our understanding is that their experience may come from working on testing tasks posted on amt or other crowd testingplatforms such as utest or they may have professional careerexperience in software testing.
furthermore those with testingexperience may favour our hit while those without suchexperience may have self selected out.
it is interesting to notethat an open call with no pre requisites for test experience stillends up recruiting a crowd with higher than average testingexpertise.
finally we observe that .
of the crowd workers recruited are new .
that is they only completed one task while the remaining workers completed at least two tasks.this high rate of returning workers may be correlated withthe interest level of our task a topic to which we now turn.
rq1.
the crowd s interest level figure shows the feedback from the crowd on the interest levels of our task.
theboxplot suggests a mean rating of .
between interesting and normal lower values to note higher interest levels and a median rating of .
this relatively high rating of interestlevel may explain the high rate of returning workers revealedin the results of figure .
a detailed distribution of the numberof submitted tasks by each worker is given in figure .the distribution shows that although there is a high rate ofreturning workers the total submissions are not dominated bya small number of super workers thereby giving cause foroptimism regarding the crowd s diversity.
rq1.
the crowd s response rates we investigated crowd performance along two dimensions the speed of task perfor mance and the thoroughness of crowdsourced manual testingin terms of activity coverage.
the speed data is extractedfrom amt task records and also the app execution logs submitted by the crowd.
the app activity coverage data iscalculated based on the submitted logs which are produced by android l ogca t .
the log information contains detailed activity launch warning and error information.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
task distribution per worker many testing scenarios may be sensitive to test speed.
figure presents three boxplots on the crowdsourced mobile testing enabled by p olariz .
the create accept time is the elapsed time from posting a task on amt to a workerconsenting accepting to work on the task.
in the first boxplot the time for the 75th percentile is .
minutes and .
of the posted tasks were accepted within one hour.
the accept submit time reports the time from task acceptanceto submission of a solution by the crowd worker.
the secondboxplot reveals that all posted tasks finished within onehour with a median value of .
minutes.
note that this time cost may not reflect the actual working time because the worker may simply accept the task and work on something else first.
thus we regard the data presented inthe second boxplot as an upper bound on the working time.to further examine the lower bound we check the crowd sworking time based on the logs submitted.
the logged timemay not reflect the time required to become familiar with our p olariz platform thus we regard it as a lower bound.
as shown in the third boxplot in figure the interquartile range 25th to 75th percentiles area shows a range of .
to .2minutes which falls into our expectation on the working time i.e.
within minutes.
rq2 the crowd s level of coverage attainment the crowd s performance in terms of test coverage attained isshown in figures to .
first we examine the overall coverageand then consider the detailed coverage results for each of thesubjects.
figure shows boxplots that depict the number of covered unique and non unique activities per task.
for non uniqueactivities the number of triggered activities is to forthe interquartile range while the number for unique activities is to .
considering the real world complexity of thesubjects and the fact that our testing tasks are designed to be lightweight micro tasks this coverage performance isreasonable and is within our expectation.
figure shows the crowd s cumulative coverage over all subjects.
the horizontal axis represents tasks in chronologicalsubmission order while the vertical axis reports activity coverage.
in total non unique activities were manipulatedby the crowd which covered out of total uniquefig.
.
task acceptance and completion times fig.
.
number of covered activities per task activities over all subjects .
unique activity coverage.
figure reports the coverage achieved on each of the subjects.
each subject is randomly assigned so the x axis for the number of tasks may vary slightly between subjects but each subject corresponds to at least tasks.
in all 9cases the cumulative coverage grows rapidly for the first 10tasks and subsequently plateaus out .
in a few cases e.g.
thetrainline the coverage was still able to grow after morethan tasks have been considered.
the highest coverage is achieved on the cleanmyandroid subject .
.
while the lowest coverage is on the all inone printer remote subject .
which is the only subjectwith a coverage below .
this low coverage is caused bythe app specific contexts which require external hardware to bepresent such as connecting to a hp printer.
in our experiments such external hardware was unavailable.
rq3 the comparative activity coverage achieved by the crowd and by s apienz we map the s apienz coverage for each subject to the coverage achieved by the crowd as the v enn diagrams illustrate in figure .
from the 9v enn diagrams we can see that the crowd covered more appactivities than the fully automated s apienz approach in of the cases.
as expected the superior to main knowledgeof the crowd and the high level understanding of the purpose of the apps under test gives them an advantage in coveringactivities compare to cheaper fully automated techniques.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
overall cumulative crowd test coverage fig.
.
crowd test coverage by subject with total number of activities in parentheses .
there is only one case google translate for which sapienz triggered more activities than the crowd.
however figure also reveals that the two approaches complement one another in out of the cases including the googletranslate case.
rq4 the improvement in s apienz performance when using motif patterns extracted from crowd based tests w e analyse the effectiveness of the crowd motifs learned by our polariz motif extraction algorithm.
on all subjects we confirmed all the learned motif events were indeed reused by sapienz.
in figure we draw the coverage achieved by both sapienz with and without the learned motif events where the blue darker grayscale when viewed in black and white lines indicate the performance of s apienz with motifs and the red lighter grayscale lines denote results for s apienz without a motif.
as suggested by the line charts of cumulative coverage on each of the subjects the learned motifs were ableto enhance s apienz in achieving higher test coverage in out of cases.fig.
.
activities by sapienz and the crowd fig.
.
improvement in coverage.
lower red lighter gray lines denote sapienz without motif patterns.
in the remaining cases the integrated motif patterns led to neither improvement nor disimprovement in terms of app activity coverage.
however in the best case brightest flashlight activity coverage improved by rising from to unique activities covered out of total possible uniqueactivities.
the parameters used in the experiments have not been tuned so our results represent fair lower bounds on the improvementthat could be expected to accrue parameter tuning and more targeted learning might improve the results we report here.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
b. threats to v alidity the primary threat to validity of our empirical studies is the threat to external validity.
our subject dataset excluded two types of mobile apps i.e.
games with non standard androidui components and those having an initial login activity forprotecting the crowd s privacy .
our results may therefore failto generalise to these kinds of apps.
to partly mitigate the generalisation issue we randomly chose apps and we note that they did fall into multiple app cat egories from widely installed real world apps each of whichhas at least million installs.
we also cannot be sure that theimproved coverage observed for s apienz would necessarily be observed for other automated testing approaches.
we also found that our crowd contains a surprisingly high level oftesting expertise for the general public a characteristic thatmay also fail to generalise to other scenarios to minimise internal threats to validity we tested both the components of p olariz and the scripts for data collection and analysis.
one threat to internal validity that we cannot avoid is related to the permission control component of p olariz platform to guarantee that the app testing contexts e.g.
wificonnection will not be changed by the crowd workers thepermission component disallows any call to external activitiesthat are not part of the app under test.
it is possible that for certain subjects such calls to external activities are a precondition to trigger some of their own activ ities.
although the same restriction applies to both techniquesstudied we cannot discount the possibility that such security sensitive blocking might have disproportionately affected oneor other of our two treatments.
v. r ela ted work our work is most closely related to previous work on extraction of useful patterns for app testing crowdsourcingand automated test generation the three areas it combines.
pattern extraction linares v asquez et al.
introduced monkeylab .
like monkeylab p olariz extracts patterns from app usage data.
however unlike monkeylab p olariz exploits a crowdsourced model which is context free whereas mokeylab is concernewd with context in its model building .furthermore while monekylab focuses on extraction of valuefor a single app under test and is agnostic about its downstreamuse p olariz introduces a novel crowdsourcing platform and extraction algorithm that targets common patterns extractedfrom and for multiple apps for subsequent exploitation by the specific downstream application of automated test data generation.
crowdsourced testing crowdsourcing is increasingly popular in software engineering research .
previous workon crowdsourced software testing has formulated the testdesign problem as one to be outsourced to the crowd.
forexample dolstra et al.
and vliegendhart et al.
demonstrated the usefulness of using amazon mechanicalturk workforce to perform continuous gui testing.schneider and cheung proposed to employ on demand crowd users for usability testing.
chen and kim proposeda puzzle based automatic testing pa t technique that trans forms object mutation problems into puzzles for the crowdto solve.
pastore et al.
used crowdsourcing to tacklethe oracle problem .
in this previous work crowdsourcingis used as an independent source of test data whereas ourapproach uses the crowd to help guide automated testing.
automated test generation there exist several mature semi automated testing frameworks such as appium androbotium that are widely used in industry but theseframeworks automate capture and replay but not test case design.
by contrast fully automated mobile test generationresearch prototypes have rarely proved able to outperform random testing .
for example dynodroid usesa biased random strategy while swifthand and or bit and puma used model based approaches.other approaches such as acteve and trimdroid are based on program analysis.
nevertheless the coverageachieved by the state of practice tool android monkey hastended to achieve higher coverage than all of these researchprototypes according to recent empirical results .
evodroid was the first search based software testing system for android reported in the literature.
in this work we chose to use s apienz partly because it is publicly available unlike evodroid but primarily because it has been recentlydemonstrated to significantly outperform both the state of the art automated testing dynodroid and the state of practice android monkey .
we thus used s apienz in order to ensure that our approach can further improve on the current best obtainable results for automated android testing.this allows us to be sure that our approach advances the current state of the art in automated testing by hybridisingwith mining from crowdsourced usage patterns.
compared to this previous work p olariz is the first to combine automated search based testing and crowdsourcing and also the first to leverage cross app usage patterns for improved mobile testing.
our results demonstrate that thiscombination complements and extends the state of the art insearch based testing.
vi.
s ummary we introduced the p olariz approach to crowd based testing which leverages a non professional crowd to provide testcases from which we extract motif patterns to help guide the s apienz automated testing technique.
our evaluation on popular google play apps showed that p olariz was able to harness crowd workers from countries to perform1 testing assignments.
the automatically learned motifpatterns improved s apienz activity coverage of out of subjects leaving it no worse on the remaining .
we also foundthat s apienz and crowd based approaches complemented one another in out of subject apps further motivatingapproaches such as ours that seek to combine them.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.