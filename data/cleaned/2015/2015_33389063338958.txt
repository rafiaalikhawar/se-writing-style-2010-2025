reinam reinforcement learning for input grammar inference zhengkai wu zw3 illinois.edu university of illinois at urbana champaign illinois usaevan johnson enjhnsn2 illinois.edu university of illinois at urbana champaign illinois usawei yang wei.yang utdallas.edu university of texas at dallas texas usa osbert bastani obastani seas.upenn.edu university of pennsylvania pennsylvania usadawn song dawnsong cs.berkeley.edu university of california berkeley california usajian peng jianpeng illinois.edu university of illinois at urbana champaign illinois usa tao xie taoxie illinois.edu university of illinois at urbana champaign illinois usa abstract program input grammars i.e.
grammars encoding the language of valid program inputs facilitate a wide range of applications in software engineering such as symbolic execution and delta debugging.
grammars synthesized by existing approaches can cover only a small part of the valid input space mainly due to unanalyzable code e.g.
native code in programs and lacking high quality and high variety seed inputs.
to address these challenges we present reinam a reinforcement learning approach for synthesizing probabilistic context free program input grammars without any seed inputs.
reinam uses an industrial symbolic execution engine to generate an initial set of inputs for the given target program and then uses an iterative process of grammar generalization to proactively generate additional inputs to infer grammars generalized from these initial seed inputs.
to efficiently search for target generalizations in a huge search space of candidate generalization operators reinam includes a novel formulation of the search problem as a reinforcement learning problem.
our evaluation on real world benchmarks shows that reinam outperforms an existing state of the art approach on precision and recall of synthesized grammars and fuzz testing based on reinam substantially increases the coverage of the space of valid inputs.
reinam is able to synthesize a grammar covering the entire valid input space for some benchmarks without decreasing the accuracy of the grammar.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august tallinn estonia copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
concepts theory of computation program analysis software and its engineering software testing and debugging .
keywords reinforcement learning grammar synthesis dynamic symbolic execution fuzzing acm reference format zhengkai wu evan johnson wei yang osbert bastani dawn song jian peng and tao xie.
.
reinam reinforcement learning for inputgrammar inference.
in proceedings of the 27th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august tallinn estonia.
acm new york ny usa pages.
introduction many programs take strings of symbols as inputs.
the set of such strings that a program accepts is called a language which is represented by a program input grammar.
program input grammars facilitate understanding of the input structure and are essential for a wide range of applications such as symbolic execution generally for test input generation reverse engineering protocol specification delta debugging prevention of exploits and improvement of system resilience .
despite the importance of program input grammars acquiring the grammars often requires much manual effort and these grammars are often either not specified or specified in a machine unfriendly form e.g.
text documents .
for example the full specification of the pdf format is available only in the form of a text document with over pages .
for a program whose input grammar is not specified in a machinefriendly form existing approaches have been proposed for attempting to infer the input grammar using program analysis language induction and machine learning .
however these existing approaches of grammar inference are not able to produce grammars of sufficient quality esec fse august tallinn estonia z. wu e. johnson w. yang o. bastani d. song j. peng and t. xie in terms of completeness and accuracy for real world software systems due to the following three main challenges.
unanalyzable code.
existing approaches based on program analysis infer input grammars based on static analysis information of the target program s code or from runtime program information collected via instrumentation of the program.
however these approaches cannot handle programs that cannot be instrumented such as web services or parts of programs that are too difficult for static program analysis to handle such as native code or dynamic language features .
low variety and quality of seed inputs.
existing approaches based on language induction leverage language induction algorithms to synthesize input grammars given a set of seed inputs.
however the effectiveness of language induction algorithms heavily depends on the variety and quality of the seed inputs.
for example to infer an input grammar for a program that parses ip addresses if the seed inputs contain only ipv4 addresses then the grammar inferred by the language induction algorithms cannot capture the ipv6 format.
lack of seed inputs.
given a large number of seed inputs existing approaches based on machine learning can train machine learning models representing input grammars that can be used to generate inputs for fuzz testing.
however there are often not sufficiently many valid examples to learn from.
we aim to tackle the preceding challenges by addressing a key limitation of existing state of the art approaches for grammar inference e.g.
glade .
existing approaches usually leverage active learning i.e.
they use an iterative process of generalization steps each of which generates new candidate grammars from the given seed inputs see section for details .
such approaches discard a candidate grammar if anyof its generated strings are rejected by the target program .
this design choice is common in active learning approaches resulting in a rigid strategy of no overgeneralization allowed to ensure that candidate grammars in each generalization step are precise i.e.
all strings generated from a synthesized grammar are covered by the ideal input grammar .
however this design choice forgoes the opportunity to potentially expand the coverage achieved by the final synthesized grammar i.e.
the overlapping scope of all the strings generated from the final synthesized grammar and all the ones generated by the ideal input grammar .
for instance a generalization operator may increase coverage at the expense of a tiny amount of overgeneralization yet existing approaches would still reject such a generalization.
furthermore as we illustrate in section even if a generalization operator often results in an inaccurate grammar the composition of multiple such inaccurate generalizations may complement each other in a way that forms an accurate composite generalization.
we propose reinam a novel framework that uses reinforcement learning to synthesize program input grammars.
in particular reinam improves over existing approaches by retaining the ability to accept inaccurate generalization steps while still synthesizing a final grammar that achieves high accuracy.
reinam achieves this goal using three key design choices.
first rather than representing the grammar as a deterministic context free grammar cfg reinam represents the accuracy of each production rule as a probability in a probabilistic context free grammar pcfg .
this representation allows reinam to quantify the quality of acandidate production rule beyond simply whether the rule overgeneralizes.
for example it enables reinam to retain an inaccurate production rule during a single generalization step and later decrease eliminate the inaccuracy via a composite generalization.
second reinam enhances the completeness of the final synthesized grammar by incrementally improving imperfect candidate grammars instead of discarding these grammars as done by the rigid no overgeneralization allowed strategy used in prior approaches .
in particular reinam incrementally adjusts the probability of candidate production rules in the pcfg model using machine learning to make the rules more accurate.
a challenge is that a large dataset of seed inputs is usually needed to train a probabilistic generative model such as pcfg.
thus reinam uses reinforcement learning to tune the pcfg.
in particular reinforcement learning uses the generative pcfg model itself to generate new training data.
then reinam runs the target program as a black box oracle to check whether the generated inputs are valid and uses this feedback to improve the pcfg model.
finally reinam iteratively generates more data to further tune the pcfg.
figure shows how reinam formulates the grammar synthesis task as a reinforcement learning problem.
in our formulation an agent the pcfg is interacting with an environment the target program .
the agent chooses an action the choice of productions to use to generate a program input that causes a state transition the portion of the program input constructed so far .
upon taking an action the agent observes the next state.
eventually once the program input is completely constructed the agent receives a reward from the environment based on whether the input is accepted or rejected .
then reinam uses a reinforcement learning algorithm to update the agent parameters the pcfg probabilities .
a key challenge is that reinam needs to adjust not only the probabilities of the pcfg but also the structure of the pcfg e.g.
by adding new candidate productions to synthesize a more general grammar.
however traditional reinforcement learning algorithms e.g.
deep q learning tune only the parameters of the agent e.g.
a deep neural network .
thus in addition to using reinforcement learning to adjust the weights of the pcfg reinam additionally adjusts the pcfg using generalization operators that modify the structure of the pcfg.
in particular reinam first applies generalization operators to construct candidate grammars then optimizes the probabilities of the corresponding pcfg and finally uses the pcfg probabilities to determine whether to accept productions in the candidate grammar.
finally reinam uses automatic test generation algorithms to generate additional seed inputs .
by so reinam improves generalization and alleviates the shortcomings of existing state of the art approaches such as glade caused by their focus on avoiding overgeneralization.
we evaluate reinam on real world benchmarks with manually written grammars used in real scenarios.
we measure the precision and recall of the synthesized grammars as well as the benefits of the synthesized grammars in grammar based fuzz testing.
our evaluation results show that reinam outperforms glade in terms of precision recall and fuzz testing coverage for most of the benchmarks.
in one of our benchmarks namely the input grammar encoding regular expressions that are accepted by the gnu grep reinam improves recall from .
to .
indicating 489reinam reinforcement learning for input grammar inference esec fse august tallinn estonia s ta .
.
.
.
.4agent the pcfg environment the target programaction a the choice of production rule to apply in each stepreward rwhether the program accepts or rejects the constructed input figure formulation of grammar synthesis as a reinforcement learning problem.
that the grammar inferred by reinam actually covers the entire program input space.
in summary this paper makes the following main contributions a novel formulation of grammar synthesis as a reinforcement learning problem with a pcfg as the agent sampled inputs as the actions and input acceptance as the rewards.
a corresponding learning algorithm called reinam which iteratively chooses a generalization operator to apply to the current pcfg adjusts the probabilities of the pcfg using reinforcement learning and then retains only general and accurate production rules.
an evaluation of on real world benchmarks for showing that reinam effectively synthesizes program input grammars and furthermore improves the effectiveness of fuzz testing.
background grammar synthesis.
we first describe glade an existing state of the art approach for synthesizing a program input grammar from a given set of seed inputs specified by the users for a program that includes input validation.
glade requires only blackbox access to the program and uses the program as an oracle in order to determine whether a given input is valid.
in particular glade iteratively generates new candidate grammars by applying generalization operators from a predefined set to the given seed inputs.
glade then checks the correctness of these candidate grammars by generating new inputs from the candidate grammars and seeing whether the new inputs are accepted by the target program.
glade consists of two steps which they call phases .
the first step learns a regular grammar by applying generalization operators such asrepetition andalternation on the given seed inputs.
the second step transforms the learned regular grammar into a context free grammar by applying merging operators.
between the first and second steps the character generalization operator is applied to generalize characters.
the following includes more details 1for conciseness we describe the key features of our approach in this paper and relegate implementation details to an appendix available on our project website https sites.google.com site reinamlearning .
alternation decompose a substring in the grammar inside a repetition and form an alternation.
for example ab can be generalized to a b .
repetition repeat a given substring in the grammar.
for example a can be generalized to a a .
merging equate two non terminal symbols in the contextfree grammar translated from the regular grammar resulted from step .
for example suppose we have a cfg s a t a t b c .2we can merge sandt by substituting twith s so the grammar becomes s a s a s b c .
character generalization allow certain terminal symbols e.g.
a to be substituted for other ones e.g.
b c ... .
glade performs a set of checks to avoid overgeneralization.
in particular glade constructs a set chk of strings such that each chk uses the candidate production rule added by the generalization operator in its derivation.
then glade executes the program on each chk and determines whether it is accepted or rejected.
ifany is rejected then glade rejects this generalization.
this mechanism is designed to enforce the no overgeneralization allowed strategy.
however generalization can still occur because it could be the case that all chk are accepted but there exists other inputs generated by the grammar that would be rejected by the program.
indeed in our evaluation we find that glade occasionally overgeneralizes.
probabilistic context free grammar pcfg .
a pcfg is a cfg augmented with a probabilistic distribution.
in particular a pcfg gis a tuple g m t r s p where mis the set of nonterminal symbols tis the set of terminal symbols ris the set of production rules sis the start symbol and pis a set of the probability distributions over production rules.
more precisely for each non terminal symbol a if there are kdifferent production rules r1 ... rkwith aas the left hand side then pa i is the probability of choosing production rule ri.
these probabilities should satisfy k i 1pa i 1andpa i .
motivating example a key shortcoming of glade is its reliance on the no overgeneralization allowed condition.
in this section we give an example showing why this design choice can be problematic.
in particular we find that as a consequence of this design choice the grep program used in the evaluation of glade can achieve only very low coverage of the valid input space .
figure shows that the grammar of the grep program consists of many special characters nbchar andnpchar .
these special characters not only form the basic building blocks for the grammar char but also serve as the special control characters in the grammar see the production rule of sin le .
therefore in step of glade i.e.
generalization to a regular language the generalization process can fail.
for example suppose the seed input is then the generalization operators in step are unable to cover the grammar here can also be .
or since they appear in the same production rule .
the best that we can do is to apply the repetition operator in which case the grammar is generalized to .
next at the intermediate step of character generalization step we consider the generalization but find that 2note that this cfg is not in normal form.
490esec fse august tallinn estonia z. wu e. johnson w. yang o. bastani d. song j. peng and t. xie enchar a a nbchar enchar s t !
.
... sin le sin le sin le sin le sin le sin le re ex sin le re ex sin le re ex re ex re ex figure the ground truth grammar of grep some rules are omitted .
this generalization is invalid.
thus glade is unable to insert the both before and after a to generalize the grammar to cover .
this example demonstrates two shorcomings of glade.
first it shows that glade is very sensitive to the given seed inputs.
for example glade will not cover unless the user provides a seed input that includes an expression of the form .
in the evaluation of glade s grammar generation the authors use seed inputs that are randomly generated from the ideal grammar.
however in a real world scenario the developers using glade most likely do not know the ideal grammar otherwise they would not need to use glade.
therefore we can expect that the quality of seed inputs will be far worse than those sampled from the ideal grammar.
additionally the grammar of grep queries is very coarse in the sense that special characters can be used as both the content of queries as well as control characters in the queries.
this property makes grep challenging for glade since the seed inputs must cover the behaviors of all the special characters.
thus randomly generated seed inputs are not enough for glade to synthesize the desired grammar.
since the quality of seed inputs greatly affects the performance of glade we propose to use test generation tools such as pex toautomatically generate seed inputs.
pex is a white box automated testing tool based on dynamic symbolic execution.
it explores possible program execution paths to generate test inputs that cover as many parts as possible of the program.
our results show that we substantially increase both the precision and the recall on the grep benchmark with the help of pex.
the second shortcoming is that glade works very hard to avoid overgeneralizing even when overgeneralization occurs it is due to a shortcoming in the checks used to detect overgeneralization rather than a deliberate choice.
in our example to generalize to a grammar that covers glade would have to perform character generalization in two places simultaneously i.e.
at each of the two characters .
if we instead allow for overgeneralization then we can keep the intermediate grammar after applying character generalization to the second .
note that this grammar can generate the input being rejected by grep so glade does not retain this generalization.
however if we subsequently apply another character generalization to the first then grammar can be transformed into which increases coverage since it now covers .
thus glade s strategy of no overgeneralizations allowed makes performing this pair of generalization steps impossible.
in contrast reinam represents the grammar as a pcfg and uses reinforcement learning to adjust the probabilities of this pcfg to improve performance.3thus reinam can retain some amount of overgeneralization at each step enabling it to achieve larger coverage while sacrificing only a small amount of accuracy.
this ability is even more powerful if the ideal grammar is not contextfree.
in this case reinam would learn an overgeneralized grammar that can achieve high coverage whereas glade would frequently fail to generalize due to non context free constraints on valid inputs that it is unable to capture.
finally the third shortcoming of glade is that the generalization operators are divided rigidly into two steps i.e.
repetition and alternation in step and merging in step .
glade performs only the repetition and alternation in step and transforms the resulting regular expression into a cfg in step .
however the repetition and alternation are still viable and often needed in step .
for example consider figure .
suppose that the left most grammar is an intermediate state during execution of step of glade.
the middle and right grammar are constructed by first applying an alternation operator that changes the production of tform p qto p q left to middle and then applying a merging operator on the symbols tands.
however in glade such a generalization is not possible since repetition and alternation are not performed in step .
in contrast reinam combines the two steps of glade reinam can perform any of the different kinds of generalization steps on the current cfg.
reinam at a high level reinam takes as input the target program for which we want to synthesize an input grammar and then proceeds in two phases.
in phase reinam generates high variety high quality seed inputs using automatic test generation e.g.
the symbolic execution engine pex and then uses an existing grammar synthesizer e.g.
glade to synthesize an initial cfg.
in phase reinam converts the cfg from phase to a pcfg and then uses reinforcement learning to refine this pcfg.
an overview of reinam is shown in figure .
reinforcement learning consists of iteratively performing five steps i apply generalization operators ii sample strings from the pcfg iii calculating the reward for each production rule iv adjusting the probability distribution based on the calculated reward and v removing low probability rules.
note that the generalization operators applied to modify the current pcfg can be customized for different grammars including operators beyond the ones described in this paper thereby bringing flexibility to our approach.
.
phase generating seed inputs using pex phase first runs a symbolic execution engine we use pex on the assembly code of the target program.
next it runs a grammar synthesizer we use glade using the output of the engine as the 3note that glade uses a pcfg to generate new inputs only for fuzzing not for synthesis.
491reinam reinforcement learning for input grammar inference esec fse august tallinn estonia s a t a t p q t p q p ... q ... s a t a t p t q t p q p ... q ... s a s a s p q p s q s p ... q ... figure an example of an initial grammar top left and two generalization steps top right bottom .
phase 1phase 2source code symbolic execution engine seed inputs language inference algorithm initial cfg initial pcfg generalization mutated pcfg input sampling reward calculation probability adjustmentrefined pcfg result cfg figure workflow of reinam.
seed inputs.
pex performs path bounded dynamic symbolic execution by repeatedly executing the program to generate path based constraints and using an smt satisfiability modulo theory solver to solve these constraints to obtain the program inputs that would lead to different execution paths.
as discussed in section the quality of a set of seed inputs is primarily determined by the various categories of inputs that the set can cover corresponding to inputs that achieve high code coverage.
test generation tools such as pex are designed to generate inputs that achieve high code coverage.
.
phase generalizing pcfgs via reinforcement learning initializing the pcfg.
phase first converts the cfg from phase to a pcfg.
for each nonterminal s we count the number kof production rules that expand s i.e.
sis on the left hand side of the rule then we assign each of these rules probability1 k. reinforcement learning.
reinforcement learning rl is an area of machine learning inspired by behavioral psychology.
the basic idea is that software agents take actions in the environment to maximize a given metric called the reward .
in our context of the grammar synthesis problem these concepts are agent the pcfg.
environment the target program.
state a partial derivation of the pcfg i.e.
a sequence consisting of both terminal and non terminal symbols .
action the choice of the production rule to apply to one non terminal symbol in in current state.
reward whether the constructed input is accepted by the target program i.e.
1if it is accepted and 0if it is rejected.
in other words the agent is taking actions that choose producitons used to construct a program input.
for the pcfg agent the productions are chosen randomly according to the pcfg probabilities we describe this process in detail below.
the reward is whether the input constructed by the agent is accepted by the program.
the goal of rl is to optimize parameters of the agent so it takes actions that maximize the reward.
the purpose in our context is to increase the quality i.e.
precision and recall of the grammar.
however existing rl algorithms are designed to maximize realvalued parameters .
in contrast we may also need to modify the structure of the pcfg itself i.e.
which productions are available .
to do so we interleave a traditional rl algorithm with the application of generalization operators to the pcfg.
in particular we iteratively perform the following i apply a generalization operator ii run a traditional rl algorithm in particular policy gradients to adjust the pcfg probabilities and iii remove any production rule with probability lower than a fixed threshold from the pcfg.
generalization operators.
we use four kinds of generalization operators.
first we use the character generalization operator described in section adapted to work for pcfgs.
we assign the probability of1 current charactersto the newly added character and reduce other probabilities proportionally.
second the repetition operator changes part of the grammar from p to p p .
in particular the operator picks a production rule and tries to repeat the symbols on the right hand side i.e.
for the rule s p q we would try two generalizations i add the rules p p q and ii add the rule s p q q .
third the alternation operator changes part of the grammar from pq to p q .
in particular the operator picks a production rule randomly decomposes the right hand side of the rule into two parts and replaces the right hand side with an alternation of these two parts e.g.
for the rule s p q a it may add new rules s p1 a andp1 p q. the probability of the original rule s p q a is split equally between s p q a ands p1 a and the probability of p1 p qis1.
fourth the merging operator merges two nonterminal symbols by substituting all usage of one symbol for another.
in particular to merge pandq we add rules p qandq p. we assign probability1 production rules of ptop qand reduce the other probabilities proportionally and similarly for q p. these generalization operators have two advantages i they are already used by glade and other grammar synthesis algorithms use similar operators and ii they are simple to implement.
constructing inputs using a pcfg.
we perform the following steps to inputs from our pcfg initialize s where sis the start symbol of the pcfg .
while contains non terminal symbols uniformly randomly choose a random non terminal ain and then randomly apply a production rule to abased on the pcfg probabilities.
return 1... k now each iis a terminal symbol .
after sampling an input we execute the program on this input and record whether the program accepts or rejects the input.
492esec fse august tallinn estonia z. wu e. johnson w. yang o. bastani d. song j. peng and t. xie probability adjustment.
recall that in our pcfg each probability quantifies the correctness of the corresponding production rule i.e.
whether this rule exists in the ideal grammar.
unlike glade we allow occasional overgeneralization.
in glade if any input generated using a new production rule is rejected by the program then that rule is rejected.
in contrast our algorithm does not necessarily reject a rule if the rule fails a check as long as it produces at least one input that is accepted.
in particular we use reinforcement learning to automatically adjust the probability of each production rule.
to do so we track which new production rules are used to construct each input.
then we define the following aggregate reward for each new production rule ri reward ri accepted inputs that use ri inputs that use ri we use the policy gradient algorithm to tune the pcfg probabilities .
we consider different non terminal symbols separately since the probability of a production rule is related only to those of other production rules for the same non terminal symbol.
consider a non terminal symbol aand the probability distribution for its productions is i.e.
ahaskproduction rules ra ... ra k and the probability for rule ra iis ra i .
the policy gradient update gives us the following adjusted probabilities log st at v where st at is the probability that the agent chooses action at in state st is a fixed learning rate and vis the reward.
in our setting we have ra i ra i log ra i reward ra i ra i reward ra i ra i finally we normalize the new probability distribution ra i so it sums to .4after each application of a generalization operator we iteratively sample new inputs and run the policy gradient update until the pcfg probabilities converge.
we use the convergence threshold1 production rules of a i.e.
we terminate if no probability changes by more than this threshold.
rule removal.
finally after adjusting the probabilities our algorithm removes production rules with probability lower than the convergence threshold or equivalently we set their probabilities to zero .
this step plays the role of un failed generalization operators and also avoids wasting computation time on adjusting the probabilities of incorrect production rules.
.
discussion negative probability adjustments.
note that in the probability adjustment formula the gradientreward ra i ra i is non negative.
thus a potential concern is that the gradient can only ever increase ra i .
the reason why this is not an issue is that because after taking a gradient step we normalize the updated probabilities so that i ra i .
in particular if a production rule ra iachieves higher reward reward ra i then the gradient computed using formula is larger so the probability increases i.e.
ra i ra i .
4this normalization can formally be derived using lagrange multipliers.conversely if ra iachieves lower reward then the probability becomes decreases i.e.
ra i ra i .
time complexity.
we can compute the time complexity of our approach by estimating the number of sampling and probability adjustment steps needed for the convergence.
suppose the threshold used by us to determine stability of the probability is i.e.
if no probability changes by more than then we terminate this round of rl and start the next generalization step and assume that the set of sampled inputs produces the same reward each time.
then the gradient difference each step is reward ra i ra i where reward ra i remains constant by assumption.
thus ra i converges proportionally to the ratio of accepted strings among all strings using that rule.
the number of sampling and adjustment steps for convergence is approximately o which is constant.
therefore the expected time complexity of our approach is o n m rl n where nis the number of symbols in the grammar rl n is the time needed for one iteration of rl and mis the total number of possible production rules.
in particular o n m is an upper bound on the number of generalizations that may be tried while generalization operators can introduce new production rules but we apply only generalization to the initial production rules set .
also rl n equals the time used for sampling strings and executing the program on these input strings.
the time used for probability adjustment is negligible.
by our previous discussion each generalization takes o rounds of sampling and adjustment so rl n o total strings sampled average execution time .
we discuss the choice of the number of strings sampled in our appendix available on our project website.
pcfg probabilities.
we note that the pcfg probability of a production rule represents how likely a resulting string would be accepted by the program if we apply that rule not the actual frequency with which the production rule used to construct realworld inputs.
the reason is that we do not have any information about the distribution of real world program inputs.
as mentioned above we initially set the probability of each production rule ra ifor a fixed non terminal symbol ato be constant.
afterwards we are adjusting the probabilities such that they converge proportionally to the ratio of accepted strings among all strings using a given rule.
we then eliminate production rules with small probability corresponding to unreasonable overgeneralizations .
empirical evaluation to evaluate the effectiveness of reinam and the contribution of each phase to the effectiveness we conduct an empirical evaluation on benchmarks.
we seek to answer the following research questions rq1 how effective is the final grammar synthesized by reinam in terms of precision and recall?
rq2 how effective is the final grammar synthesized by reinam in terms of improving fuzz testing?
rq3 how do the two phases of reinam contribute to the grammar s precision recall and performance in fuzz testing?
5this assumption may not be true in practice due to random noise we make the assumption to simplify our analysis of convergence.
493reinam reinforcement learning for input grammar inference esec fse august tallinn estonia rq4 what is the execution time of reinam?
in rq1 we compare reinam with glade in terms of the precision and recall of the final synthesized grammar.
we compute the two metrics using the manually written ideal grammar for the benchmarks as ground truth.
in rq2 we evaluate the capability of reinam to learn a grammar for a fuzz testing task.
we feed the grammar synthesized by reinam to a grammar based fuzzer to perform fuzz testing on programs.
we compare our results with glade and an industrial fuzzer.
in rq3 we compare the grammar after phase of our algorithm to the final grammar after phase of our algorithm.
we compare the difference both in precision recall and in fuzz testing coverage.
in rq4 we measure the time used by each phase of reinam.
.
benchmarks we include the benchmarks of manually written grammars from the glade evaluation a grammar used to match urls .
we separate this grammar into four benchmarks based on the protocols i.e.
http https mailto and nntp .
we test whether reinam can infer a complete grammar starting from seed inputs including only one of the four protocols.
a grammar for the regular expressions accepted as input by gnu grep .
this grammar is shown in figure .
a grammar for a simple lisp parser including support for quoted strings and comments.
a grammar for an xml parser including all xml constructs except that only a fixed number of tags are included to ensure that the grammar is context free .
we use the .net xml library and it has an alphabet of tags to choose from.
neither glade nor reinam are provided with the alphabet.
a grammar for a cascading style sheets css parser css is a language used to describe the presentation of a document written in a markup language such as html.
in addition to the glade benchmarks we include a grammar used to match ipv4 and ipv6 addresses and two benchmarks generated from antlr a widely used parser generator.
we use these two benchmarks to evaluate the performance of reinam on synthesizing a grammar for a program generated by an automatic parser generator.
we select the following grammars from the official website of antlr that have less than non terminal symbols a grammar used to match csv files .
a grammar used to describe simple first order logic fol formulas .
the data and benchmarks of our evaluation are available on our project website.
.
evaluation setup precision and recall.
precision measures the probability that a randomly generated string from our synthesized lanugage lis accepted by the target program i.e.
included in the ideal language l and recall measures the probability that a randomly generated string from the ideal language l belongs to our synthesized language l. in other words the precision of reinam indicates whether our synthesized language loverapproximates the ideallanguage l and the recall indicates whether lunderapproximates l .
we calculate precision as eprec l eprec where eprecis a set of strings randomly sampled from l and we calculate recall as erec l erec where erecis a set of strings randomly sampled from a reference grammar used to specify l .
generation of seed inputs.
we use pex s dynamic symbolic execution capabilities to generate input strings that are accepted by the parser and then use these strings as seed inputs to glade.
sampling from pcfg.
we sample a string matching a nonterminal ain the cfg gas follows randomly select a production a a1...anfora.
for each i ... n ifaiis a nonterminal then recursively sample a string matching ai and if aiis a terminal then return ai.
for simplicity we use a uniform distribution over productions when sampling strings to measure precision and recall.
in contrast our rl algorithm samples strings using the probabilities in the pcfg g. here we discard the probabilities since they do not necessarily capture the true distribution of inputs.
programs used for evaluation.
because pex requires source code to perform dynamic symbolic execution and generate program inputs we write c programs to parse the grammars described in section .
.
we use parsers in the .net system library to parse the grammars of urls ip addresses regular expressions and xml documents.
we also test against an open source lisp parser and an open source css parser.
the programs for the two antlr grammars are generated by using antlr in c mode.
fuzz testing.
fuzz testing or fuzzing is an automated software testing technique.
an effective fuzzer generates sufficiently valid inputs and then monitors the execution of the program on these inputs.
one of the purposes of using fuzz testing is to observe the behavior of the target program under various inputs so we want the fuzzer to achieve high code coverage.
grammar based fuzzer.
both glade and reinam synthesize a cfg lthat approximates the ideal language l .
we use a grammarbased fuzzer to leverage the synthesized grammar to improve fuzzing.
the fuzzer is similar to the step of input sampling described earlier.
we randomly sample strings from the grammar using a uniform distribution over productions.
as before for reinam we use a uniform distribution instead of using the pcfg probabilities since these probabilities have no relation to the distribution of real inputs.
then we execute the program on the sampled inputs and use visual studio to measure the code coverage as the number of blocks covered.
we use the code coverage achieved using the manually written ideal grammar as an upper bound.
we compare our fuzzer against a state of the art fuzzer that does not employ grammar based fuzzing radamsa .
radamsa combines random bit flipping with domain independent heuristics designed to test edge cases to create high quality mutation.
note that the learn fuzz tool is not available to us so we cannot compare against their approach.
.
rq1 precision and recall as shown in table reinam performs well across all benchmarks.
the row li shows the result of the grammar synthesized by solely 494esec fse august tallinn estonia z. wu e. johnson w. yang o. bastani d. song j. peng and t. xie table precision p and recall r .
benchmarkhttp https nntp mailto ip css xml grep lisp csv fol p r p r p r p r p r p r p r p r p r p r p r li .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
li se .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
li se rl .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
figure code coverage achieved by different fuzzers as percentages table coverage numbers are basic blocks covered in code .
benchmark http https nntp mailto ip lisp css xml grep csv fol radamsa li li se li se rl total li language inference algorithm results of glade only li se symbolic execution engine results of reinam phase li se rl reinforcement learning results of reinam phase radamsa code coverage achieved by radamsa running glade while the row li se rl shows the result of the final grammar synthesized by reinam.
on average the precision stays almost the same with a less than change .
on average while the recall improves drastically by .
.
this rate is calculated by subtracting the number on the third row by the first row and taking the average across benchmarks.
on all of our benchmarks reinam outperforms glade in recall without losing precision.
in the three benchmarks css xml and lisp glade already achieves .
recall indicating that the grammar synthesized by glade already covers the entire input space of the three programs.
for these benchmarks reinam cannot further improve recall but manages to keep the perfect score as expected.
as mentioned in section .
we intentionally split the url grammar into four benchmarks http https nntp and mailto .
for these four benchmarks we see a substantial increase in recall.
glade performs poorly with recall lower than .
on all four benchmarks due to the limitation of glade that the quality of the synthesized grammar highly depends on the quality of the seed inputs.
since glade starts with input strings starting with http https nntp or mailto in other three benchmarks as the seed inputs glade is not able to explore other possible url protocols.
meanwhile reinam leverages the help of pex in phase in order to generate a set of seed inputs for achieving higher coverage.
therefore the final synthesized grammar of reinam for these four benchmarks has recall above .
without substantial loss of precision.
the ip benchmark is similar.
glade starts with the seed inputs in ipv4 is unable to generalize the grammar to cover the ipv6 case.
the performance on benchmarks grep and csv is similar.
we have already analyzed why glade achieves only .
recall on grep in section .
we can see that reinam gets .
recall i.e.
all possible strings that can be accepted by the program are covered by the final synthesized grammar of reinam.
given that the precision is similar .
vs. .
the improvement is substantial.
the csv benchmark is similar to the grep benchmark in the sense that it includes many characters with special behaviors.
in particular since the grammar describes all possible csv files there are many special characters that can be in a data field and several special characters serve as both separating symbols and content symbols.
as discussed in section this property makes the benchmark challenging for glade since the seed inputs must cover all of these behaviors.
therefore reinam outperforms glade by improving recall from .
to .
.
the case for the fol benchmark is unique.
although reinam improves recall from .
to .
it decreases precision from .
to .
.
this benchmark is the only one for which reinam cannot achieve a recall above .
.
intuitively first order logic fol formulas are more complex to write.
for example to write a valid http address it only needs to start with and have a .
between the site and domain segments.
however fol formulas are much more strictly formatted.
intuitively the ideal language of fol formulas is quite sparse indicating that if we consider the entire input space of a fol parser it would be much smaller than the input space of a url parser.
therefore the generalization operators in reinam may have difficulty generalizing to the complicated structure of the fol grammar.
.
rq2 application in fuzzers in the fuzz testing experiment we can see from table and figure that reinam greatly improves code coverage.
if we compare the rows radamsa li and li se rl we find that on average reinam improves coverage by .
compared to the radamsa fuzzer and by .
compared to glade s grammar based fuzzer.
in the benchmarks xml and grep all fuzzers achieve perfect code coverage.
interestingly the grammar synthesized by glade li achieves only .
recall in table but the fuzzer using this grammar still achieves perfect code coverage.
the reason is that different strings generated from the same grammar can share same or similar execution paths that would cover similar basic blocks in code.
therefore coverage is not always correlated with recall.
ignoring the two benchmarks with perfect coverage for all approaches reinam outperforms the na ve fuzzer on all benchmarks.
495reinam reinforcement learning for input grammar inference esec fse august tallinn estonia table execution time breakdown of reinam number in seconds and percentage of total time .
benchmark http https nntp mailto ip lisp css xml grep csv fol se .
.
.
.
.
.
.
.
.
.
.
li .
.
.
.
.
.
.
.
.
.
.
rl .
.
.
.
.
.
.
.
.
.
.
total when compared to glade s grammar based fuzzer reinam performs slightly worse in the css benchmark but outperforms glade on all other benchmarks.
this advantage is due to the benefit that we get from allowing overgeneralization since reinam produces more rejected inputs than glade.
these rejected inputs cover parts of source code that the accepted inputs cannot cover e.g.
the code used to handle invalid inputs.
another observation is that the radamsa fuzzer performs poorly on the benchmarks of parenthesis matching pattern lisp and css and also on the benchmark fol .
especially for the css benchmark radamsa cannot even reach code coverage.
the reason is that as we discussed the ideal grammars for lisp css and fol are more structured than those for other benchmarks.
the characterlevel modifications used by a non grammar based fuzzer cannot synthesize these structured strings whereas the generalization operators used by reinam can infer these kinds of structures.
.
rq3 comparison of phases from table table and figure we see that phase and phase contribute differently to the improvements over glade in terms of precision recall and fuzzing.
in rq1 if we compare row li se which shows the result of the grammar synthesized by running glade on the pex generated seed inputs reinam phase against row li which shows the result of synthesized grammar of glade itself we find that phase on average improves precision by .
and recall by .
.
in the three parenthesis matching pattern benchmarks css xml and lisp glade already achieves .
recall which cannot be improved upon.
however phase does improve precision in both the xml and lisp benchmarks and maintains the same precision in the css benchmark.
the reason is that pex generates a set of seed inputs with higher coverage.
as described in section the first step of glade directly synthesizes a regular grammar that only captures the seed inputs.
therefore a set of seed inputs with higher coverage would result in an initial regular grammar with higher coverage so fewer further generalizations are needed to infer the ideal grammar.
since imprecision happens during generalization fewer generalizations result in higher precision.
we observe similar behaviors for other benchmarks especially when the grammar synthesized by glade has low precision.
however for the three benchmarks fol ip and mailto we find that the grammar synthesized by glade already achieves perfect or near perfect precision.
in these cases recall improves substantially after phase however precision after phase is reduced.
this reduction is a sacrifice for achieving better recall.
the reason is that a set of seed inputs with higher coverage brings more opportunities for generalization but also causes the check mechanism in glade to fail more frequently resulting in more uncaught overgeneralization which thereby reduces precision.next in rq1 if we compare row li se which shows the result of the grammar synthesized by running glade on the pex generated seed inputs reinam phase and row li se rl which shows the result of the final grammar synthesized by reinam we find that on average the results after phase improve recall by .
but worsen precision by .
.
in particular we observe that precision decreases for all benchmarks compared to the results of phase .
this finding is expected since the reinforcement learning algorithm in phase allows overgeneralization to further generalize the grammar.
compared to the no overgeneralization allowed strategy used in glade our allowance of overgeneralization leads to reduced precision.
furthermore we observe that in the benchmarks from the url grammar i.e.
http https nntp and mailto and the grep and csv benchmarks after phase the recall is still low.
the highest is https with a recall of .
all others are less than .
.
the reinforcement learning in phase further generalizes the grammar to achieve higher coverage.
table shows that among these six benchmarks recall improves by an average of .
over phase alone.
in addition grep and csv actually achieve .
recall indicating that the final grammar synthesized by reinam can perfectly cover the entire program input space.
this result suggests that our reinforcement learning approach is effective especially in cases when the input space is large but existing approaches can synthesize a grammar that covers only part of the input space.
in the fuzz testing task the average improvement in coverage achieved by the grammar based fuzzer using reinam s grammar row li se rl to the coverage achieved by the grammar based fuzzer using the phase grammar row li se is .
.
similarly the average improvement of row li se to row li is .
.
from the data we can see that phase contributes more to the improvement in coverage.
this comparison shows the benefits of keeping some overgeneralized rules in our rl algorithm.
we see that for benchmarks such as css phase does not increase recall which remains at .
.
however the final synthesized grammar improves coverage by .
vs. compared to phase .
thus while phase does not improve the coverage of the ideal language it improves the actual code coverage acheived using fuzz testing.
this result further demonstrates that our reinforcement learning approach is effective not only for exploring the input space of all valid program inputs but also for generating invalid program inputs that cover execution paths that cannot be covered by valid inputs.
.
rq4 execution time we can see from table that the rl phase phase takes an average of .
of the total execution time.
pex takes an average of .
of the time in phase .
the execution time of pex can be tuned by setting the timeout parameters.
the timeout in the evaluation is set to seconds pex times out in five benchmarks.
496esec fse august tallinn estonia z. wu e. johnson w. yang o. bastani d. song j. peng and t. xie based on the discussion in section .
the execution time of the rl phase is primarily dependent on the size of the grammar and the execution time of the program.
the time overhead of the rl phase is unsatisfying however the bottleneck in phase is waiting for the results of executing the program on sampled strings.
this step can be parallelized by executing the program on ksampled strings simultaneously.
currently we execute the program on sampled strings in parallel on threads.
the results can be improved by parallelizing the program execution across more threads.
threats to validity threats to external validity.
our evaluation uses manually written ideal grammars to measure precision and recall.
additionally the grammars in evaluation benchmarks may not be complicated enough to reflect real world scenarios.
we attempt to combat this threat by selecting complex grammars such as the css grammar.
threats to internal validity.
we randomly sample strings from the ideal grammars for glade to match with the increased number of seed inputs generated by pex included in reinam.
the randomly generated seed inputs for glade may not be sufficiently representative and may affect glade s effectiveness.
the technique by which we sample from the pcfg is simpler and is potentially more biased compared to more sophisticated techniques.
also a different learning rate may affect the evaluation results.
nevertheless we synthesize grammars using different and find that the results are quite close i.e.
less than .
difference in precision and recall and no difference in coverage.
related work our approach draws on both existing work in grammar inference as well as techniques used in machine learning and automated test generation.
inferring input grammars.
h schele et al.
propose the autogram approach based on dynamic taint tracing to extract syntactic entities of a given seed input.
by tracing the data flow of particular characters of the seed input in the parsing method of the target program autogram decomposes formats into meaningful fields.
however since autogram traces only paths taken by a given seed input it requires that the given set of seed inputs capture all meaningful features of the input grammar in order to infer a complete grammar.
coverage guided fuzzing.
in recent years coverage guided fuzzing has achieved substantial success with tools such as american fuzzy lop afl 6and fairfuzz .
however afl does not support fuzzing .net executables and does not have an appropriate replacement that we can compare to.
many fuzzers fail to explore paths that involve a difficult check such as string equality comparisons.
recent advances in grey box fuzzing have utilized lightweight program analysis to mitigate this problem.
steelix tracks progress in string comparison checks to incrementally discover inputs that can bypass these checks.
angora solves path constraints via a searching algorithm based on gradient descent.
other tools such as driller use more heavyweight program analysis to bypass these checks by symbolically executing an intermediate representation and solving constraints to these checks.
reinam is able to discover the constants in grammar to bypass these checks in the grammar inference process.
as we can see in the inference of the url protocol the discovery of protocol names such as mailto and https is unlikely to be discovered using grey box approaches.
machine learning for fuzzing.
godefroid et al.
use a recurrent neural network to learn an input model and generate inputs for fuzzing with the learn fuzz algorithm.
they have recently formalized fuzzing as a reinforcement learning problem .
their work does not produce an explicit grammar but instead uses a generative deep neural network to serve as the grammar.
pcfg inference from examples.
the problem of inferring a probabilistic context free grammar pcfg from a set of examples has been studied extensively for the purpose of natural language processing.
belz extends standard split merge grammar inference techniques to optimize grammars from examples but requires a large corpus of annotated examples which are not viable for inferring program input grammars.
scicluna and higuera propose an unsupervised approach to grammar inference without annotated examples.
while they show that this approach works for small samples with respect to nlp standards the polynomial number of examples with respect to the number of productions in ideal grammars this approach is still not viable for inferring program input grammars.
reinam addresses these issues by expanding seed inputs using pex before synthesizing the grammar.
conclusion we have presented reinam a reinforcement learning approach for synthesizing a pcfg that encodes the language of valid program inputs.
to address the challenge of lacking high variety and high quality seed inputs faced by the existing approaches reinam includes an industrial symbolic execution engine pex to generate initial seed inputs for the given target program and includes a grammar generalization loop to proactively generate additional inputs during grammar inference.
in the grammar generalization loop instead of eliminating production rules in a candidate grammar that may not be accurate initially as done by glade an existing state of the art approach reinam keeps and evolves inaccurate grammars enabling it to infer ground truth grammars whose inference requires composite generalizations from the initial seed inputs.
to efficiently search for such composite generalizations in a huge search space of candidate generalization operators reinam includes a novel formulation of the search problem as a reinforcement learning problem.
our evaluation results show that reinam outperforms glade on both precision and recall of the synthesized grammars and fuzz testing based on reinam substantially increases the coverage of the space of valid inputs.
reinam is often able to synthesize a grammar covering the whole valid input space without decreasing the precision of the grammar.