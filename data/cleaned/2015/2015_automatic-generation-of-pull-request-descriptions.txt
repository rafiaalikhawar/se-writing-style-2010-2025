automatic generation of pull request descriptions zhongxin liu bardbl xin xia check christoph treude david lo shanping li college of computer science and technology zhejiang university hangzhou china ningbo research institute zhejiang university ningbo china bardblpengcheng laboratory shenzhen china faculty of information technology monash university melbourne australia school of computer science university of adelaide adelaide australia school of information systems singapore management university singapore singapore liuzx zju.edu.cn xin.xia monash.edu christoph.treude adelaide.edu.au davidlo smu.edu.sg shan zju.edu.cn abstract enabled by the pull based development model developers can easily contribute to a project through pull requests prs .
when creating a pr developers can add a free form description to describe what changes are made in this pr and or why .
such a description is helpful for reviewers and other developers to gain a quick understanding of the pr withouttouching the details and may reduce the possibility of thepr being ignored or rejected.
however developers sometimesneglect to write descriptions for prs.
for example in our collected dataset with over 333k prs more than of the pr descriptions are empty.
to alleviate this problem we proposean approach to automatically generate pr descriptions based onthe commit messages and the added source code comments in theprs.
we regard this problem as a text summarization problemand solve it using a novel sequence to sequence model.
to copewith out of vocabulary words in software artifacts and bridge the gap between the training loss function of the sequence tosequence model and the evaluation metric rouge which hasbeen shown to correspond to human evaluation we integratethe pointer generator and directly optimize for rouge usingreinforcement learning and a special loss function.
we build adataset with over 41k prs and evaluate our approach on this dataset through rouge and a human evaluation.
our evaluation results show that our approach outperforms two baselines bysignificant margins.
i. i ntroduction the pull based development model is popular on modern collaborative coding platforms e.g.
github .
it easesdevelopers contributions to a project.
in this model a developer does not need to have access to the central repository to contribute to a project.
she only needs to fork the centralrepository i.e.
create a personal clone make changes e.g.
fix a bug or implement a feature independently in the personalclone and submit the changes to the central repository througha pull request from hereon pr .
usually the pr will be tested by continuous integration services and reviewed by core team members or reviewers before being merged into the centralrepository .
a pr consists of one or more interrelated commits.
to create a pr on github a developer needs to provide a titleand can add a free form text i.e.
a pr description to describefurther what changes are made and or why they are needed.
pr descriptions can help reviewers gain a quick and adequate checkcorresponding author.understanding of prs without digging into details and mayreduce the possibility of prs being ignored or rejected .in addition pr descriptions can help software maintenance and program comprehension tasks .
however pr descriptions are sometimes neglected by developers.
for example in our dataset which contains 001prs collected from 1k engineered java projects on github over of the pr descriptions are empty.
to alleviate thisproblem we propose an approach to automatically generate pr descriptions from the commits submitted with the correspond ing prs.
our approach can be used to generate pr descriptionsto replace existing empty ones and can also assist developers in writing pr descriptions when creating prs.
some tools have been proposed to automatically generate descriptions for software changes e.g.
generate commit messages and release notes .
commits prsand releases can be regarded as software changes occurringat different granularity.
distinct from commit messages whichonly describe one commit pr descriptions often need tosummarize multiple related commits.
a release is a collection of plenty of commits and or prs.
release notes are prepared for both developers and end users people who use the librariesor apps while pr descriptions readers are usually solelydevelopers.
hence they have different focuses and informationstructure.
besides the existing technique for release note generation does not explicitly summarize multipleinterrelated commits.
it recovers links between commits and bug reports and uses bug report titles as the summaries ofcorresponding commits.
in this work we focus on explicitlysummarizing the commits in a pr to generate its description and we treat traceability as a separate problem.
moreover when developers document a change of some granularity e.g.
a pr the documents of the smaller changes it contains e.g.
the commits in the pr are usually available.
therefore thetechniques of automatically documenting changes at differentgranularity are complementary rather than competing.
to the best of our knowledge there is no prior work focusing on generating descriptions for prs.
it is challenging to automatically generate a description for a single commit not to mention a pr with multipleinterrelated commits.
fortunately when a developer writes apr description the commit messages of the commits in this ui .
oufsobujpobm pogfsfodf po vupnbufe 4pguxb sf ohjoffsjoh authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
pr are usually available.
these valuable messages together with the patch of each commit shed light into the generationof pr descriptions.
as the first step of this task this workaims to generate pr descriptions from the commit messagesand the added source code comments in the prs.
given a pr we regard the combination of its commit messages and the source code comments added in it as an article and its description as the summary of this ar ticle .
the generation of pr descriptions is then regardedas a text summarization problem by us.
we have appliedtwo commonly used extractive text summarization methods to solve this problem but find their effectiveness is limited described in section v d .
in this work we propose a moreeffective approach for pr description generation.
specifically.our approach builds upon the attentional encoder decodermodel which is an effective sequence to sequence modelfor text summarization.
it first learns how to write pr descriptions from existing prs and then can generate descriptions for new prs.
there are two challenges which make a basic attentional encoder decoder model not effective enough for pr descrip tion generation out of vocabulary oov words.
due to the developernamed identifiers e.g.
variable names oov words arewidespread in software artifacts.
however the attentionalencoder decoder model can only produce words in a fixedvocabulary hence cannot deal with oov words.
the gap between the training loss function and the evaluation metric.
since different sentences may convey similar meanings researchers usually leverage a flexible discretemetric named rouge to evaluate text summarizationsystems.
rouge allows for different word orders between agenerated text and the ground truth and correlates highly with human evaluation .
however the training objective of the attentional encoder decoder model is minimizing a maximum likelihood loss which is strict and will penalize all literaldifferences between a generated text and the ground truth.
dueto this gap the model minimizing the maximum likelihoodloss may not be the one with the best generation performance.
we observe that the oov words in a pr description can often be found in the corresponding article .
therefore weintegrate the pointer generator in our approach to over come the first challenge.
with this component our approachcan generate a word from either the fixed vocabulary or the input.
to deal with the second challenge we need to find away to optimize for rouge directly when training.
however we cannot simply use rouge as the training objectivesince rouge scores are non differentiable i.e.
the parametergradients of our model cannot be calculated from them.
wesolve this problem by using a reinforcement learning rl technique named self critical sequence training scst .
based on scst we adopt a special loss function named rlloss in our approach.
this loss is both related to rougescores and differentiable with which we can train a model andguide it to produce results that are more likely to be good forhuman evaluation.as this is the first work on this topic we use two extractive methods i.e.
leadcm and lexrank as baselines.
givena pr leadcm extracts its first few commit messages as thegenerated description and lexrank selects and outputs salientsentences in the pr s article .
to evaluate our proposedapproach we collected over 333k prs from 1k engineeredjava projects with the most merged prs on github buildinga dataset with over 41k prs after preprocessing.
we evaluateour approach on the dataset using rouge.
the evaluationresults show that our approach outperforms the baselines interms of rouge rouge and rouge l by .
.
and .
.
we also conduct a human evaluation toassess the quality of the generated pr descriptions whichshows that our approach performs significantly better than thebaselines and can generate more high quality pr descriptions.
in summary our contributions are three fold we propose a novel approach to generate descriptions for prs from their commit messages and the code comments that are added.
our approach can cope with oov words with the pointer generator and directly optimize forrouge which has been shown to correspond to humanevaluation with the rl loss.
we build a dataset with over 41k pull requests fromgithub for the pr description generation task.
we evaluate our approach on the dataset using therouge metric and a human evaluation.
the evaluation results show that our approach outperforms two baselines by significant margins.
the remainder of this paper is organized as follows section ii describes the motivation the usage scenarios and some background knowledge of our approach.
section iii elaborates our approach including the pointer generator and the rlloss.
we describe our dataset in section iv and presentthe procedures and results of our evaluation in section v.section vi discusses situations where our approach performs badly and threats to validity.
after a brief review of related work in section vii we conclude this paper and point outpotential future directions in section viii.
ii.
m otiv ation and preliminary in this section we present the motivation and usage scenarios of our approach formulate the problem of pr descrip tion generation and introduce the attentional encoder decoder model.
a. motivating example table i shows a motivating example of our approach which is a pr in the pitest project .
we can see that the description describes the changes made i.e.
added an option and activated from maven plugin and the motivation i.e.
ignore failing tests from coverage of this pr.
this prcontains two commits.
one source code comment is addedin commit and no code comment is added in commit .we can know the changes from the commit messages and authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i apull request in the pitest project description added an option to ignore failing tests from coverage activated from maven plugin commit commit message added skipfailingtests option from maven plugin added code comments when set will ignore failing tests when computing coverage.
otherwise the run will fail.
if parsesurefireconfig is true will be overridden from surefire configuration property testfailureignorecommit commit message simplified surefire testfailureignore value retrieval added code comments n a the motivation from the added code comments in commit .
this example indicates that we may be able to generate thedescription of a pr by summarizing its commit messages andthe source code comments added in it.
b. usage scenario our approach aims to automatically generate descriptions for prs based on their commit messages and the code com ments that are added.
its usage scenarios are as follows first of all our approach can be used to generate pr descriptions to replace existing empty ones.
the generateddescriptions may help reviewers and developers quickly cap ture prs key ideas without reading the detailed commits.such key ideas can be very helpful when reviewers anddevelopers are making quick decisions e.g.
assigning a tagor estimating whether two prs are related.
the generateddescriptions may also be useful for software maintenance and program comprehension tasks.
for example tools for pr reviewer recommendation can use the generated description asone of the features.
our approach can also assist developers in writing pr descriptions.
if it takes several days to finish a pr thedeveloper may forget some important information in this prwhen writing the description.
she may either ignore suchinformation which may affect the acceptance of the pr or spend some time to check the detailed commits which may decrease her productivity.
the description generated byour approach can remind the developer of the importantinformation in the pr and assist her in writing a high qualitypr description.
c. problem f ormulation inspired by the motivating example we regard the generation of a pr description as a text summarization task withthe combination of the commit messages and the added code comments in the pr as the article and the pr description as the summary .
specifically in this work we treat thetext summarization task as a sequence to sequence learningproblem where the source sequence is the article and thetarget sequence is the summary .
therefore the problemis formulated as follows given a source sequence w w w2 ... w w and a target sequence y y1 y2 ... y y find a function f so that f w y. denotes the length of a sequence.
g5 g13 g7 g18 g10 g9 g1 g15 g8 g10 g9 g9 g14 g16 g12 g2 g7 g21 g10 g18 g4 g3 g3 g4 g3 g3 g4 g3 g3 g4 g3 g3 g12 g15 g12 g16 g12 g17 g12 g18 g2 g14 g16 g10 g7 g18 g22 g6 g7 g16 g13 g22 g5 g17 g11 g19 g15 g7 g20 g9 g15 g9 g16 g9 g17 g9 g18 g8 g15 g8 g16 g8 g17 g8 g18 g1 g5 g4 g3 g3 g4 g3 g3 g2 g14 g16 g10 g7 g18 g22 g5 g17 g11 g19 g15 g7 g20 g10 g16 g10 g14 g3 g16 g13 g30 g16 g11 g15 g11 g16 g11 g17 g11 g18 g13 g30 g14 g13 g30 g15 g7 g23 g22 g24 g16 g6 g4 g29 g7 g23 g22 g24 g16 g6 g13 g30 g15 g2 g27 g25 g21 g19 g20 g2 g21 g25 g26 g28 g12 g30 g16 g1 g6 g6 g2 g4 g6 g3 g5 g4 fig.
.
attentional encoder decoder model with pointer generator attn pg d. attentional encoder decoder model our approach builds upon the attentional encoder decoder model from hereon attn which is an effective model for sequence to sequence learning problems.
attn s frameworkis depicted in black in figure .
it uses two distinct recurrentneural networks rnns as encoder and decoder respectively.the input of the encoder and the decoder is first mappedto word embeddings by a shared embedding layer.
given a source sequence w at time step i the encoder calculates a hidden state h ibased on the word embedding xiofwiand the previous hidden state hi 1using its rnn.
the last hidden state h w is regarded as the intermediate representation of the source sequence and input to the decoder as the initial hidden state.
at decoding step j the decoder computes a hidden state sj from xj which is the embedding of the previous reference token when training or the previously generated token whentesting.
attn leverages the attention mechanism so thedecoder also calculates a context vector c j as follows ej i vetanh whhi wssj be aj softmax ej cj x summationdisplay iaj ihi where wh wsandbeare learnable parameters and ej iis the score ofxiat decoding step j.ajis the attention distribution which informs the decoder of the importance of each encoding step.
cjis computed as the weighted sum of all encoder hidden states which can be regarded as the representation of thesource sequence at decoding step j. then c jis concatenated with decoder hidden state sjto produce the vocabulary distribution pvocab pvocab softmax v prime v b b prime authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
where v prime v bandb primeare learnable parameters.
pvocab is used to decide which token in the vocabulary should be outputat the current decoding step.
it also provides the conditionalprobability of generating the j threference token yj which is p yj y0 ... yj w pvocab yj where yis the input of the decoder.
at each training iteration the optimization objective of attn is to minimize the negative log likelihood of the referencesequence as follows loss ml y y summationdisplay j 1logp yj y0 ... yj w iii.
a pproach this section elaborates our approach including the pointer generator and the rl loss.
a. pointer generator as described in section ii d attn produces tokens by selecting from a fixed vocabulary.
however out of vocabulary oov words are ubiquitous in software artifacts due todeveloper named identifiers such as variable names and filenames.
attn alone cannot cope with such oov words andhence its performance is limited.
we observe that the oov words in pr descriptions usually appear in the corresponding source sequences.
so we integrate the pointer generator inour approach to solve this problem.
with this component ourapproach can either select a token from the fixed vocabularyor copy one from the source sequence at each decoding step.
the structure of attn with the pointer generator attn pg is presented in figure where the pointer generator is highlighted in green.
the switch between selection and copy is softly controlled by the generation probability which is calculated from the word embedding x jof the decoder input yj the decoder state sjand the context vector cjat time stepj pj gen wt ccj wt ssj wt x xj bgen where wc ws wxandbgenare learnable parameters and is the sigmoid function.
pj genmeasures the probability that thejthoutput of the decoder is generated from the fixed vocabulary and the probability of copy is hence pj gen. the conditional probability of producing the jthreference token equation is then modified as p yj y0 ... yj w pj genpvocab yj pj gen pcopy yj wherepcopy yj is the probability of copying yjfrom the source sequence w and is computed from the attention distribution aj equation as follows pcopy yj summationdisplay i wi yjaj i we can see that when yjis an oov word pvocab yj is zero but if yjappears in the source sequence our approachcan still generate it through pcopy yj .
in this way our approach alleviates the problem of oov words and still holdsthe capability of producing novel words from the vocabulary.in addition the training loss is still calculated using equa tion but the conditional probability p y j y0 ... yj w is computed by equation now.
b. rl loss as described in section ii d attn uses the negative log likelihood loss to guide the training process.
this loss function isstrict and will penalize any literal difference between generatedsequences and the ground truth.
for example if the groundtruth is the cat sat on the mat but the decoder produces on the mat sat the cat the loss will be high since the two sentences only literally match at the .
therefore researchers do not use this loss function to measure the performance oftext summarization systems.
instead they usually use a flexiblediscrete evaluation metric named rouge see section v a which can tolerate generated sentences with different word orders from the ground truth and has been shown to correlate highly with human evaluation .
the gap between thetraining loss function and rouge may result in the modelwith the least loss not being the one producing the best prdescriptions.
we can bridge this gap by directly optimizing for rouge when training.
however rouge scores are nondifferentiable which means the parameter gradients of ourmodel cannot be calculated only from rouge scores.
hencewe cannot directly use rouge as the loss function.
recently it has been shown that reinforcement learning rl techniques can be incorporated to enable direct optimization of discrete evaluation metrics .
our approach also lever ages an rl technique named self critical sequence training scst and adopts a special loss function named rlloss to solve this problem.
we can cast the generation of pr descriptions using rl terminology.
the decoder is the agent which interacts withthe environment the encoder s output and the decoder sinput .
at each decoding step the action of the agent isto predict an output token according to a policy with parameters .
actually the neural network of the decoder defines and is its parameters.
once finished generating a sequence y the agent will observe a reward defined as follows r y g y y where yis the ground truth of yandgis a function related to rouge.
in this work we defined gas the rouge l f1 score.
the training objective of the rl problem is minimizing the negative expected reward l eys according to the scst algorithm the expected gradient ofl can be computed as follows l eys authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
g8 g6 g6 g2 g12 g10 g18 g11 g12 g20 g1 g22 g22 g12 g17 g22 g14 g18 g17 g7 g18 g14 g17 g22 g12 g20 g8 g6 g6 g3 g17 g10 g18 g11 g12 g20 g21 g9 g16 g19 g15 g12 g9 g20 g13 g16 g9 g25 g6 g10 g6 g7 g8 g12 g24 g9 g20 g11 g4 g23 g17 g10 g22 g14 g18 g17 g6 g5 g18 g21 g21 g4 g23 g17 g10 g22 g14 g18 g17 g4 g11 g6 g10 g12 g4 g11 g6 g7 g12 g1 g2 g5 g5 g9 g8 g3 g11 g6 g10 g12 fig.
.
the rl loss where ybis a baseline sequence also generated from .
we obtain ybthrough a greedy search.
specifically we choose the token with the highest output probability i.e.
p y b j yb ... yb j w at each decoding step to form yb.i n practice the expectation can be approximated by a single monte carlo sample ysfrom which means l r ys r yb log ys r ys r yb ys summationdisplay j 1logp ys j ys ... ys j w we define the rl loss of our model following paulus et al.
as follows lossrl r ys r yb ys summationdisplay j 1logp ys j ys ... ys j w r ys andr yb are non differential and are regarded as constant values when calculating gradients.
from equation 5and equation we can see that minimizing loss rlis equal to minimizing l .lossrlcan also be viewed as the lossml equation weighted by a normalized reward.
if the normalized reward i.e.
r ys r yb is positive i.e.
the sequence sampled from our model is better than the baseline sequence minimizing loss rlis equivalent to maximizing the likelihood of the sampled sequence and vice versa.
the calculation process of the rl loss is shown in figure .
in practice only using lossrlcan be detrimental to the readability of the generated texts .
we hence combine lossmlandlossrlto form a hybrid loss for training as follows loss lossrl lossml iv .
d ataset a. data collection to collect pr data from github we first used the reporeapers framework to select engineered software projects.we obtained all java repositories that had been clas sified as containing engineered software projects by repore apers s random forest classification and retrieved the number of merged prs for each repository.
of these repositories contained at least one merged pr.
we then sorted these 700repositories in descending order of their number of mergedprs downloading the data of merged prs from the top 000projects through github s apis.
for each project we collectedat most the first merged prs returned by github s searchapi.
since our approach takes commit messages and source code comments that are added as input and pr descriptionsas output given a pr we retrieved its description and commitmessages parsed the patches of its commits and extracted theadded comments in each patch.
in total we collected 001merged prs from engineered java projects.
b. data preprocessing we preprocessed the collected prs according to the following processes preprocess text to filter out trivial and templated information in prs we leveraged the same procedure to preprocessthe texts of pr descriptions commit messages and sourcecode comments.
given a text we first removed the htmlcomments and the paragraphs starting with a headline named checklist from it through regular expressions because the text in html comments and checklist paragraphs usually only describe the general procedure of finishing a pr suchas functionality works and passes all tests .
then we split the text into sentences using nltk identifying and deleting the sentences with url internalreference e.g.
signature e.g.
signed off by emails name and markdown headlines e.g.
why through regular expressions.
we filtered sentences with1 and since this work focuses on summarizing the changesin a pr and we regard recovering links between prs and othersoftware artifacts as a separate problem.
besides sentenceswith and usually do not describe the changes made in a pr and may bring in many oov words.
next we tokenized the text using nltk.
previous work has shown that nltk outperforms other common nlp libraries interms of tokenizing software documentation .
the tokensthat only consist of or more hexadecimal characters wereconsidered as sha1 hash digests and replaced with sha .
similarly version strings e.g.
.
.
and numbers were converted into version and respectively.
finally tokens with non ascii characters were removed and referred to as non ascii tokens and texts with more than non ascii tokens were marked as non ascii .
construct target sequence the target sequence of a pr only consists of its description.
to construct it we simplypreprocessed the pr description using the general text preprocessing procedure mentioned above.
the pr descriptions which were marked as non ascii when preprocessing contain less than tokens or only consist of punctuation markswere removed and referred to as trivial desc .
construct source sequence a pr s source sequence is constructed from the combination of its commit messagesand the added code comments in it.
specifically we firstlisted the commits in this pr in ascending order of their creation time.
then for each commit we extracted its commit message and the code comments that are added.
the commitmessage was directly preprocessed using the general textpreprocessing procedure.
as for the added comments copy right comments license comments function signatures in javadocs e.g.
param param1 and the comments with only authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
g8 g9 g25 g2 g21 g20 g25 g26 g24 g27 g13 g26 g11 g12 g24 g16 g15 g26 g10 g15 g23 g4 g19 g22 g26 g29 g30 g3 g15 g25 g13 g8 g9 g5 g17 g18 g26 g15 g24 g1 g14 g15 g23 g27 g12 g26 g15 g8 g9 g25 g11 g24 g17 g28 g17 g12 g18 g30 g3 g15 g25 g13 g8 g9 g5 g17 g18 g26 g15 g24 g6 g21 g20 g16 g30 g3 g15 g25 g13 g8 g9 g5 g17 g18 g26 g15 g24 g2 g21 g20 g25 g26 g24 g27 g13 g26 g10 g21 g27 g24 g13 g15 g10 g15 g23 g2 g21 g19 g19 g17 g26 g7 g27 g19 g5 g17 g18 g26 g15 g24 g6 g21 g20 g16 g30 g10 g21 g27 g24 g13 g15 g8 g9 g5 g17 g18 g26 g15 g24 fig.
.
procedure of filtering pull requests table ii statistics of our collected pull requests typeempty desc prtrivial desc prlong desc prpr with only valid commitpr with valid commitslong source pradequate pr total number long desc pr and long source pr refer to the prs for which the target sequence and the source sequence do not meet the length constraints respectively.
punctuation marks were filtered.
the remaining comments were concatenated as a comment paragraph which was thenpreprocessed using the general text preprocessing procedure.if the preprocessed commit message or comment paragraphor both are not empty we regard this commit as a valid commit .
finally we concatenated all the preprocessed commit messages as the first paragraph of the source sequence and listed the comment paragraph of each commit as the followingparagraphs.
the commit messages were sorted according tothe order of commits i.e.
ascending order of their creationtime and were separated by a special token .
the comment paragraphs were also listed in the order of commits and all paragraphs were separated by .
filter prs with constructed source sequences and target sequences prs were filtered according to the procedureshown in figure .
the prs with empty descriptions were firstremoved and referred to as empty desc prs .
if a pr description was a trivial desc after preprocessing the corresponding pr was also filtered and referred to as a trivial desc pr .w e deleted the prs with less than or more than valid commits because we can directly use the only commit message asthe description if a pr only contains one commit and apr with too many commits often aims to synchronize with another repository instead of being a contribution from a contributor.
to reduce the training time of our approach wealso constrained the maximal length of the source sequence tobe and that of the target sequence to be .
the prs notsatisfying these length constraints were hence filtered.
afterpreprocessing we collected prs.
the statistics of the removed and the adequate prs are presented in table ii.
v. e v aluation in this section we first describe the evaluation metrics and the baselines.
then we present our research questions rqs and corresponding experiment results.
finally we show the procedure and results of our human evaluation.
a. evaluation metrics we evaluate our approach with the rouge metric which has been shown to correlate highly with human assessments of summarized text quality .
specifically we userouge n n and rouge l which are widely used to evaluate text summarization systems .
the recall precision and f1 score for rouge n are calculated as follows rrouge n summationtext gen ref s summationtext gram n refcntgen gramn summationtext gen ref s summationtext gram n refcntref gramn prouge n summationtext gen ref s summationtext gram n refcntgen gramn summationtext gen ref s summationtext gram n gencntgen gramn f1rouge n 2rrouge nprouge n rrouge n prouge n wheregen ref andsrefer to a generated description its reference description and the test set gramnis an n gram phrase and cntgen gramn andcntref gramn refer to the occurrence number of gramningen andref respectively.
in summary rrouge nmeasures the percentage of the ngrams in reference descriptions that an approach can generate andp rouge npresents the percentage of correct n grams i.e.
n grams appearing in reference descriptions in generateddescriptions.
f1 rouge nis a summary measure that combines both precision and recall.
the precision recall and f1 score forrouge l are similar with those for rouge n but insteadof n grams they are calculated using the longest commonsubsequences between generated descriptions and reference descriptions .
when comparing two approaches we care more about f1 scores since they balance precision and recall.
rouge is usually reported as a percentage value between and .
we obtained rouge scores using the pyrougepackage with porter stemmer enabled.
b. baselines as this is the first work on pr description generation we use two extractive baselines leadcm and lexrank.
leadcm leadcm is proposed by us for this task.
given the source sequence of a pr leadcm outputs the first 25tokens of the commit message paragraph as its generateddescription.
is the median length of the pr descriptionsin our dataset.
according to the construction of the sourcesequence described in section iv b the generated description is actually the first few commit messages in this pr.
the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
hypothesis behind leadcm is that developers may commit key changes e.g.
implementing a feature first and make other lessimportant changes such as fixing typos later so the first fewcommit messages may summarize the core of a pr.
lexrank lexrank summarizes an article by calculating the relative sentence importance and selecting the mostimportant sentences in the article as the generated summary.
it hypothesizes that a sentence is more salient to an article if it is similar to many sentences in this article.
the importance or centrality of each sentence in an article is computedby applying the pagerank algorithm on the sentencesimilarity matrix of this article.
given the source sequenceof a pr we first use the continuous lexrank method to rank its sentences according to their importance.
then we concatenate these ranked sentences and keep the first tokensas the output just like leadcm.
c. experiment settings similar to jiang et al.
and hu et al.
we randomly select of the 41k prs in our dataset for testing for validation and the remaining for training.
our approachuses dimensional word embeddings.
the encoder is asingle layer bidirectional lstm the decoder is the same butunidirectional and both of them use dimensional hiddenstates.
since our encoder and decoder share the embeddinglayer we collect words from both the source sequences andthe target sequences of the training set to build the vocabulary.the vocabulary size is set to 50k following see et al.
.
at training time we first train our model for iterations only using the maximum likelihood ml loss loss ml evaluating the model every iterations with the validation set.
the best performing ml model is obtained after 000iterations.
then we continue training this best ml model withthe hybrid loss loss defined in equation for another iterations and also perform evaluation every iterations.we get the final best model after iterations i.e.
000iterations in total.
we leverage adam with a batch sizeof to train our models.
as suggested by paulus et al.
we set the in equation to .
and the learning rate of adam is set to .
for the training with loss mland .
for the training with loss.
when testing we leverage beam search of width to generate sequences.
we notice that there exist repeating phrases in some generated sequences and adopt a heuristic rule which ignores a candidate beam if its current generated tokencreates a duplicate trigram at each decoding step to reducesuch repetition.
our replication package which contains our dataset source code trained model and test results is available online .
d. rq1 the effectiveness of our approach to investigate our approach s effectiveness we evaluate our approach on our dataset in terms of rouge rouge and rouge l and compare it with the two baselines i.e.
leadcm and lexrank.the evaluation results are shown in table iii and our approach is referred to as attn pg rl.
for text generationtasks the f1 scores for rouge are typically between .2to .
.
we can see from table iii that firstour approach tends to generate shorter descriptions .21tokens on average than lexrank and leadcm.
second itoutperforms lexrank in terms of all metrics by large margins from .
to .
.
also it obtains higher precisionand f1 score than leadcm for each rouge metric.
theimprovements in terms of the three f1 scores are .
.
and .
points respectively.
these results indicate thatcompared to the two baselines our approach can capture thekey points of a pr more precisely.
we also manually inspect our test results.
table iv presents an example in the test set.
according to our inspection weargue that the better performance of our approach mainlycomes from its two advantages our approach can accurately identify important phrases or sentences in source sequences.
it learns knowledge aboutwhich phrases are important for summarizing a pr from thetraining data hence it is more intelligent.
for example thepr in table iv contains commit messages and multipleadded code comments.
lexrank extracts the wrong sentences as output performing worst.
leadcm outputs all commit messages without filtering or sorting while our approachprecisely identifies the salient phrase i.e.
tomcat support inthe s ramp in the source sequence.
therefore our approachoutperforms the baselines.
our approach is an abstractive method with the ability of dynamic generation.
lexrank and leadcm generate descrip tions by extracting important sentences and cannot rephraseextracted sentences generate novel words or dynamicallydecide how many sentences tokens to generate.
however our approach can generate descriptions with different lengths based on the input.
moreover our approach can rephrase important sentences as shown in table iv.
this advantagereduces the number of unimportant phrases produced by ourapproach and results in high precision.
we also notice that the recall of our approach for rouge and rouge l are slightly lower than that of leadcm.
this is because leadcm always generates as many tokens as possibleuntil it gets while our approach tends to procedure short butaccurate descriptions.
some phrases output by leadcm maybe trivial but they may contain some tokens in the referenceand hence improve the recall.
for example in table iv the third sentence output by leadcm is not a salient sentence.
however it contains a for which also occurs in the referenceand makes the recall of leadcm higher than ours in thisexample.
in addition it is a little surprising that the relatively complicated lexrank performs worse than the naive leadcm.
we argue that the reason is the hypothesis of lexrank which isthat a sentence similar to many sentences is more important does not always hold in the source sequences.
for instance in table iv there are two identical comments in the sourcesequence which are computed as the most important sentences authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii comparisons of our approach attn pg rl with each baseline in terms of rouge scores approach avg.
lengthrouge rouge rouge l recall precision f1 score recall precision f1 score recall precision f1 score lexrank .
.
.
.
.
.
.
.
.
.
leadcm .
.
.
.
.
.
.
.
.
.
attn pg rl .
.
.
.
.
.
.
.
.
.
attn pg rl vs lexrank .
.
.
.
.
.
.
.
.
.
attn pg rl vs leadcm .
.
.
.
.
.
.
.
.
.
avg.
length refers to the average length of the generated descriptions by each approach.
table iv test example source sequence initial tomcat support .
tomcat support in the s ramp installer.
fixes for tomcat support .
eat the error and try the next optioneat the error and try the next option this filter can be used to supply a source of credentials that can be used when logging in to the jcr repository modeshap e .i t uses the inbound request as the source of authentication .
constructor .
login with credentials set by some external force .
this may be a servletfilterwhen running in a servlet container or it may be null when running in a jaas compliant application server e.g .
jboss .
note when passing null it forces modeshape to authenticate witheither .
reference added support for tomcat in s ramp .
lexrank eat the error and try the next option eat the error and try the next option this filter can be used to supply leadcm initial tomcat support .
tomcat support in the s ramp installer .
fixesfor tomcat support .
attn pg rl initial tomcat support in the s ramp installer .
by lexrank.
but they have no token in common with the reference.
in summary our approach outperforms the two baselines in terms of rouge rouge and rouge l and can generate more accurate descriptions than thebaselines.
e. rq2 the effects of main components our approach generates pr descriptions based on the attentional encoder decoder model attn .
it integrates the pointergenerator pg to deal with oov words and adopts the rl lossto directly optimize for rouge when training.
we compare our approach i.e.
attn pg rl with attn and attn pg in terms of rouge to understand the influence of the pointergenerator and the rl loss.
besides since attn is an effectiveand popular model for text generation tasks we also regard itas an abstractive baseline for pr description generation andcomparing our approach with it can further investigate theeffectiveness of our approach.the evaluation results are shown in table v. we can see that attn pg outperforms attn in terms of all metrics by .
to .
which means the pointer generator can effectivelycope with oov words and the generation of pr descriptionsbenefits a lot from it.
table vi presents one of our test results.we can see that fulltextonline and webpagearchived are two oov words.
attn cannot handle them hence produces instead while both attn pg and attn pg rl cangenerate the two words correctly.
compared to attn pg attn pg rl performs better in terms of all recall and f1 score metrics by more than .
butslightly worse in terms of all precision metrics.
to figure outthe reason we inspect the descriptions generated by attn pgand attn pg rl and find that attn pg rl tends to gen erate longer descriptions than attn pg in order to increasethe rl reward i.e.
the f1 score for rouge l. for example attn pg rl generates more relevant tokens than attn pg for the pr in table vi.
on average attn pg rl produces .19more tokens than attn pg as shown in table v. therefore the reduced precision of attn pg rl can be regarded as theexpense of the improved recall and the gain in recall is higherthan the loss in precision which translates to high f1 scores.
in summary our approach outperforms attn and attn pg.
the pointer generator and the rl loss areeffective and helpful for boosting the effectiveness of ourapproach.
f .
human evaluation we also conduct a human evaluation to investigate our approach s effectiveness.
we invite human evaluators toassess the quality of the pr descriptions generated by our approach and the two baselines.
all of them are ph.d. students with years of experience in java programming.
procedure we randomly select prs from the test set and evenly divide them into two groups.
each group is evaluated by different evaluators.
for each pr we showits source sequence and reference description followed by thethree pr descriptions generated by our approach and the twobaselines to the evaluators.
the three generated descriptionsare randomly ordered.
human evaluators also have no ideaabout how these approaches work so they cannot figure out which description is generated by which approach.
the evalua authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table v effectiveness of each component in our approach approach avg.
lengthrouge rouge rouge l recall precision f1 score recall precision f1 score recall precision f1 score attn .
.
.
.
.
.
.
.
.
.
attn pg .
.
.
.
.
.
.
.
.
.
attn pg rl .
.
.
.
.
.
.
.
.
.
pg .
.
.
.
.
.
.
.
.
.
rl .
.
.
.
.
.
.
.
.
.
attn pgand rlrefer to the attentional encoder decoder model the pointer generator and the rl loss respectively.
table vi test example source sequence add some more links to fultextonline.fulltextonline and webpagearchived in parallel rather than choosing be tween one of them but this should be ok. add when 6551ex starts with onl add when medium is rdvocab add resource ht018400499 to the test set adjust test sets .
add rule to avoid redundant entries in fulltextonline and webpagearchivedintroduced with the last commit we will now have fulltextonline and webpagearchived not in parallel butrather choose between them with preferring webpagearchived .
adjust test sets .
reference we will now have fulltextonline and webpagearchived not in parallel butrather choose between them with preferring webpagearchived .
add when 6551ex starts with onl add when medium is rdvocab adjust test sets attn adapt and in parallels .
attn pg fulltextonline and webpagearchived in parallel rather than choosing be tween one of them but this should be ok. attn pg rl fulltextonline and webpagearchived in parallel rather than choosing be tween one of them but this should be ok. add when 6551ex starts with onl adjust test sets .
tors are asked to assign a score from to to each generated description to measure the semantic similarity between thegenerated description and the reference.
the higher the scorethe closer is the corresponding pr description to the reference.
evaluators are allowed to search related information such as unfamiliar concepts through the internet.
results each pr description obtains three scores from three evaluators.
we calculate the average score as its final score .
the distribution of the final scores is presented in figure .
we can see that compared to the baselines ourapproach produces more pr descriptions with high scores greaterorequalslant4 and less with low scores .
besides our approach generates far more pr descriptions with the average scorebetween and .
but we also notice that our generateddescriptions with the average score between and are alittle more than those generated by the baselines.
the reason g19 g20 g21 g22 g23 g24 g25 g26 g36 g89 g72 g85 g68 g74 g72 g3 g54 g70 g82 g85 g72 g19 g20 g19 g21 g19 g22 g19 g49 g88 g80 g47 g72 g91 g53 g68 g81 g78 g47 g72 g68 g71 g38 g48 g36 g87 g87 g81 g14 g51 g42 g14 g53 g47 fig.
.
the distribution of the final scores obtained from our human evaluation.
each bar presents the number of the average scores obtained byan approach that fall in a specific score interval.
for example the leftmostblue bar shows that descriptions generated by lexrank obtain an averagescore between to .
may be that our approach generates pr descriptions from scratch instead of directly extracting source sentences andhence sometimes may fail to generate important words.
we calculate the average scores of lexrank leadcm and attn pg rl across all sampled prs which are .
.73and .
respectively.
although the average score of ourapproach is still not perfect our approach is the first step onthis topic and can inspire follow up work.
we also conductwilcoxon signed rank tests at the confidence level of considering the final scores for each approach.
the pvalues of our approach compared with lexrank and leadcm are all less than .
which means the improvements achievedby our approach are significant.
in summary our human evaluation shows that our approach outperforms the baselines significantly and cangenerate more high quality pr descriptions.
vi.
d iscussion in this section we discuss situations where our approach performs badly and threats to validity.
a. where does our approach perform badly we carefully inspect the prs where our approach does not obtain good rouge l f1 scores.
we find that our approachusually performs badly if the reference description mainlypresents the information that cannot be found in the sourcesequence.
we find three types of such information context information including the motivation and the test results of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table vii test example source sequence improve performance of preprocess .
improve performance ofpreprocess cleanup code .
improve performance of preprocess cleanup code newcommand newcommand.replaceall empty newcommand newcommand.replaceal l .
empty build up the decimal formatter .build up the regular expression .return command.replaceall empt y .
only build the decimal formatter if the truncation length has changed .build up the decimal formatter .build up the regular expression .
reference when opening large files the preprocessing takes a very long time .
by compiling the regular expressions once rather than using the string.replaceall methods we can save significant time .on my lapto p i opened a gcode file with sha lines laser image raster with my changes 16155msmaster 56178ms attn pg rl improve performance of preprocess cleanup code .
a pr implementation details subjective sentences i.e.
sentences describing personal feelings plans etc.
table vii presents a typical example.
we can see that the reference description of this pr contains three sentences which respectively describe the motivation the implementa tion details and the test results of the pr and such informationdoes not appear in the source sequence.
the description generated by our approach has little in common with the reference description hence it gets low rouge scores.
as for subjective sentences we find a test pr for which the reference description is we did make awesome shall we landit .
this description is a subjective sentence without describingthe changes made in the pr while our approach tries to summarize the pr by generating refactored common pieces out into httpobject.
.
upon our inspection the descriptionproduced by our method correctly captures the meaning ofthe pr while the reference description is not helpful forunderstanding the changes well.
sometimes our approach also fails to capture key phrases in the source sequence and consequently performs badly.
but thissituation is less common than the one mentioned above.
ourevaluation results in section v d also show that our approachcan better capture key phrases than the two baselines.
b. threats to v alidity one threat to validity is that our dataset was built only from java projects which may not be representative of all programming languages.
however java is a popular program ming language.
besides our approach takes commit messagesand source code comments as input hence can also be appliedto projects of other programming languages.
another threat to validity is that the non summary information such as signatures and subjective sentences in pr descriptions may affect the effectiveness of our approach.pr descriptions are free form text and we cannot guarantee their quality and content.
we mitigate this threat by using aset of heuristic rules to filter out non summary informationwhen preprocessing.
but it is hard to distill the patterns ofall non summary information.
since this work focuses onlearning to generate pr descriptions from existing prs furtherimprovements on data preprocessing are more suitable forfuture work.
there is also a threat related to our human evaluation.
we cannot guarantee that each score assigned to every prdescription is fair.
to mitigate this threat each sampled pris evaluated by human evaluators and we use the average score of the evaluators as the final score.
in addition our baseline approaches produce summaries of length tokens unless there are fewer than tokens available in the source sequence.
this may result in incompletesentences in the output of these approaches which mayhave negatively affected the corresponding ratings by human evaluators.
however this threat does not affect the rougescores which also confirm the superiority of our approach.
vii.
r elated work this section discusses the related studies on documenting software changes understanding pull requests and summarizing and documenting other software artifacts.
a. documenting software changes commits prs and releases are software changes of different granularity.
some tools have been proposed to document commits based on diverse inputs automatically .for example buse and weimer proposed d elta doc a technique that can summarize a commit by first using symbolic execution and path predicate analysis to generate the behavioral difference and then applying some heuristictransformations to generate a natural language description.similarly cortes et al.
built changescribe a tool which first identifies the stereotype of a commit from the abstractsyntax trees before and after the commit and then generates a descriptive commit message using pre defined filters andtemplates.
rastkar and murphy proposed an approach to generate the motivation of a commit by extracting motivational sentences from its relevant documents.
jiang et al.
adoptedan attentional encoder decoder model to generate commit mes sages from diffs .
liu et al.
proposed an informationretrieval based method to generate commit messages for new diffs by reusing proper existing commit messages.
researchers have also explored the automatic generation of release notes .
for instance abebe et al.
identified six types of information contained in release notesand leveraged machine learning techniques to decide whetheran issue should be mentioned in release notes.
moreno et al.
proposed a tool named arena which first extracts and summarizes each commit in a release and thenuses manually defined templates to organize these summarieswith their related information in the issue tracker to generate release notes.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
commit messages pr descriptions and release notes document software changes occurring at different granularity .
as described in section i pr descriptions oftenneed to summarize several related commits when compared tocommit messages and are different from release notes in termsof target audiences and information structure.
moreover sincethe documents of small granularity changes e.g.
the commitsin a pr are usually available when developers document alarge granularity change e.g.
the pr the techniques for gen erating commit messages and release notes are complementaryrather than competing with our approach.
b. understanding pull requests many empirical studies were focusing on understanding pull requests prs and the pull based development.
someof them focused on analyzing which factors affect the prevaluation .
for example rahman and roy investigated how the discussion texts project specific information e.g.
project maturity and developer specific information e.g.
experience of prs affect their acceptance.
tsay etal.
found that both technical and social factors influencethe acceptance of prs.
some other studies aimed to understand how the pull based development works .
for instance gousios etal.
analyzed the popularity of the pull based development model characterized the lifecycle of prs and also explored which factors influence the merge decision and delay of a pr.in their following work gousios et al.
conductedlarge scale surveys to study how integrators people who areresponsible for integrating prs and contributors collaboratein the pull based development model.
they highlighted the challenges faced by integrators such as maintaining projects quality and prioritizing external contributions and the chal lenges faced by contributors like unawareness of project statusand poor responsiveness from integrators.
prior work also proposed many techniques to deal with the challenges developers face when using the pull baseddevelopment .
for example veen et al.
proposed prioritizer a tool which prioritizes prs based on machine learning techniques and multiple extracted features such as the number of discussion comments.
to reduce theresponse time of prs yu et al.
proposed an approachto automatically recommend reviewers for a pr based onits title description and the social relations of developers.
these studies motivate our work to generate pr descriptionsto facilitate downstream tasks.
c. summarizing and documenting other software artifacts besides software changes researchers have studied the automatic summarization of other software artifacts such as source code bug reports app reviews developer discussions and developmentactivity .
concerning source code some techniques havebeen proposed to summarize source code based on programanalysis and manually defined templates infor mation retrieval and learning based methods .
some of them also use encoder decoder models.
for example hu et al.
proposed a framework which leveragedan attentional encoder decoder model to generate commentsfor java methods.
wan et al.
proposed a novel encoder decoder model with a hybrid encoder and a reinforcement learning based decoder to generate code comments.
they usedthe actor critic algorithm with an extra neural network as the critic.
different from wan et al.
s work our approach uses the scst algorithm which is based on the reinforcealgorithm and does not require extra networks.
besides we do not directly leverage rl to decode but only to computea special loss for better training.
as for bug reports previous work focused on identifying and extracting important sentences from bug reports as their summaries.
for example rastkar et al.
trained aconversion based summarizer using a bug report corpus toidentify important sentences automatically.
mani et al.
and lotufo et al.
proposed unsupervised approaches basedon noise reducer or heuristic rules to perform bug report summarization.
different from their work this work aims to generate pr descriptions from commit messages andsource code comments that are added using an abstractivemethod.
viii.
c onclusion and future work in this paper we aim to automatically generate descriptions for pull requests from their commit messages and the sourcecode comments that are added.
we formulate this problem asa sequence to sequence learning problem and point out twochallenges of this problem i.e.
out of vocabulary words andthe gap between the training loss function of sequence to sequence models and the discrete evaluation metric rouge which has been shown to correspond to human evaluation .
we propose a novel encoder decoder model to solve thisproblem.
to handle out of vocabulary words our approachadopts the pointer generator to learn to copy words from thesource sequences.
as for the second challenge our approachincorporates a reinforcement learning technique and adoptsa special loss function to optimize for rouge directly.comprehensive experiments on a dataset with over 41k pullrequests and a human evaluation show that our approachoutperforms two competitive baselines.
in the future we plan to further investigate the usefulness of our approach by using it to generate summaries for the pull requests without descriptions.
we also plan to improve our approach by involving additional related software artifacts asinput.
for example by taking diff files and relevant bug reportsas input our approach may be able to infer the implementationdetails and the motivation of a pr.
a cknowledgment this research was partially supported by the national key research and development program of china 2018yfb1003904 nsfc program no.
and theaustralian research council s discovery early career researcher award decra funding scheme de180100153 .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.