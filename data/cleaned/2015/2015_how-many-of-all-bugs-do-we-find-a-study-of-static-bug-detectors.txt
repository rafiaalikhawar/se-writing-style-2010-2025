how many of all bugs do we find?
a study of static bug detectors andrew habib andrew.a.habib gmail.com department of computer science tu darmstadt germanymichael pradel michael binaervarianz.de department of computer science tu darmstadt germany abstract static bug detectors are becoming increasingly popular and are widely used by professional software developers.
while most work on bug detectors focuses on whether they find bugs at all and onhowmanyfalsepositivestheyreportinadditiontolegitimate warnings the inverse question is often neglected how many of all real world bugs do static bug detectors find?
this paper addresses this question by studying the results of applying three widely used static bug detectors to an extended version of the defects4j dataset that consists of java projects with known bugs.
to decide whichof thesebugs thetools detect we usea novelmethodology thatcombinesanautomaticanalysisofwarningsandbugswitha manual validation of each candidate of a detected bug.
the results ofthestudyshowthat i staticbugdetectorsfindanon negligible amountofallbugs ii differenttoolsaremostlycomplementaryto eachother and iii currentbugdetectorsmissthelargemajority ofthestudiedbugs.adetailedanalysisofbugsmissedbythestaticdetectors shows that some bugs could have been found by variantsoftheexistingdetectors whileothersaredomain specificproblems that do not match any existing bug pattern.
these findings help potentialusersofsuchtoolstoassesstheirutility motivateandoutline directions for future work on static bug detection and provide a basis for future comparisons of static bug detection with other bug finding techniques such as manual and automated testing.
ccs concepts softwareanditsengineering automatedstaticanalysis software testing and debugging general and reference empirical studies keywords static bug checkers bug finding static analysis defects4j acm reference format andrewhabibandmichaelpradel.
.howmanyofallbugsdowefind?
astudyofstaticbugdetectors.in proceedingsofthe201833rdacm ieee permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthe firstpage.copyrights forcomponentsof thisworkowned byothersthan the author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
ase september montpellier france copyright held by the owner author s .
publication rights licensed to acm.
acm isbn ... .
conference on automated software engineering ase september3 montpellier france.
acm newyork ny usa 12pages.
introduction findingsoftwarebugsisanimportantbutdifficulttask.foraverage industry code the number of bugs per lines of code has been estimated to range between .
and .
even after years of deployment softwarestillcontainsunnoticedbugs.forexample studiesofthelinuxkernelshowthattheaveragebugremainsin thekernelforasurprisinglylongperiodof1.5to1.8years .
unfortunately asinglebugcancauseseriousharm evenifithas beensubsistingforalongtimewithout examplesofsoftwarebugsthathavecausedhugeeconomicloses and even killed people .
giventheimportance offindingsoftwarebugs developers rely on several approaches to reveal programming mistakes.
one approach is to identify bugs during the development process e.g.
through pair programming or code review.
another direction is testing ranging from purely manual testing over semi automated testing e.g.
via manually written but automatically executed unit tests to fully automated testing e.g.
with ui level testing tools.
oncethesoftwareisdeployed runtimemonitoringcanrevealso farmissedbugs e.g.
collectinformationabout abnormalruntime behavior crashes andviolationsofsafetyproperties e.g.
expressedthroughassertions.finally developersusestaticbugdetectiontools which check the source code or parts of it for potential bugs.
inthispaper wefocusonstaticbugdetectorsbecausetheyhave becomeincreasinglypopularinrecentyearsandarenowwidely used by major software companies.
popular tools include google s error prone facebook s infer or spotbugs the successor to the widely used findbugs tool .
these tools are typically designed as an analysis framework based on some form of static analysisthatscalestocomplexprograms e.g.
ast basedpattern matching or data flow analysis.
based on the framework the tools contain an extensible set of checkers that each addresses a specific bug pattern i.e.
a class of bugs that occurs across different code bases.typically abugdetectorshipswithdozensorevenhundreds ofpatterns.themainbenefitofstaticbugdetectorscomparedto other bug finding approaches is that they find bugs early in the development process possibly right after a developer introduces a bug.furthermore applyingstaticbugdetectorsdoesnotimpose any special requirements such as the availability of tests and can be fully automated.
the popularity of static bug detectors and the growing set of bug patterns covered by them raise a question how many of all authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france andrew habib and michael pradel real worldbugsdothesebugdetectorsfind?
orinotherwords what is the recall of static bug detectors?
answering this question is important for several reasons.
first it is an important part of assessing the current state of the art in automatic bug finding.
most reportedevaluationsofbugfindingtechniquesfocusonshowing that a technique detects bugs and how precise it is i.e.
how many ofallreportedwarningscorrespondtoactualbugsratherthanfalse positives.
we do not consider these questions here.
in contrast practicallynoevaluationconsiderstheaboverecallquestion.the reasonforthisomissionisthatthesetof allbugs isunknown oth erwise thebugdetectionproblemwouldhavebeensolved makingitpracticallyimpossibletocompletelyanswerthequestion.second understanding the strengths and weaknesses of existing static bug detectors will guide future work toward relevant challenges.
for example better understanding of which bugs are currently missed mayenable futuretechniques tocover previouslyignored classes of bugs.
third studying the above question for multiple bug detectorsallowsustocomparetheeffectivenessofexistingtoolswith eachother areexistingtoolscomplementarytoeachotherordoes one tool subsume another one?
fourth and finally studying the abovequestionwillprovideanestimateofhowclosethecurrent state of the artistotheultimate butadmittedlyunrealistic goal of finding all bugs.
toaddressthequestionofhowmanyofallbugsdostaticbug detectors find we perform an empirical study with real world bugsfrom15softwareprojects whichweanalyzewiththreewidelyusedstaticbugdetectors.thebasicideaistoruneachbugdetectoronaversionofaprogramthatcontainsaspecificbug andtocheck whether the bug detector finds this bug.
while being conceptually simple realizingthisideaisnon trivialforreal worldbugsandbugdetectors.
the main challenge is to decide whether the set of warnings reported by a bug detector captures the bug in question.
to address this challenge we present a novel methodology thatcombines automatic line level matching based on the lines involved in thebugfix andamanualanalysisofthematchedlines.themanual analysis is crucial because a bug detector may coincidentally flag a line as buggy due to a reason different from the actual bug such asanunrelatedproblemorafalsepositive.sinceourstudyfocuseson a finite set of bugs we cannot really answer how many of allbugs are found.
instead we approximate the answer by considering a large and diverse set of bugs from various real world projects.
ourstudyrelatestobutsignificantlydiffersfromapreviousstudy by thung et al.
.
their work also addresses the question of how many of all real world bugs are found by static checkers.
our work differs in the methodology used to answer this question we manually validate whether the warnings reported by a tool correspondtoaspecificbuginthecode insteadofcheckingwhether the lines flagged by a tool include the faulty lines.
this manual validation leads to significantly different results than the previous study because many warnings coincidentally match a faulty line but are actually unrelated to the specific bug.
another difference is thatourstudyfocusesonamorerecentandimprovedgeneration ofstaticbugdetectors.thungetal.
sstudyconsiderswhatmightbecalledthefirstgenerationofstaticbugdetectorsforjava e.g.
pmd and checkstyle.while thesetools contributed significantlyto the state of the artwhentheywereinitiallypresented ithasalsobeen shownthat theysuffer fromsevere limitations in particular largenumbersoffalsepositives.hugeadvancesinstaticbugdetection havebeenmadesincethen.ourstudyfocusesonanovelgeneration ofstaticbugdetectors includingtoolsthathavebeenadoptedby major industry players and that are in wide use.
the main findings of our study are the following thethreebugdetectorstogetherreveal27ofthe594studied bugs .
.
this non negligible number is encouraging and shows that static bug detectors can be beneficial.
the percentage of detected among allbugs ranges between .
and depending on the bug detector.
this result pointsoutasignificantpotentialforimprovement e.g.
by consideringadditionalbugpatterns.italsoshowsthatcheckers are mostly complementary to each other.
themajorityofmissedbugsaredomain specificproblems notcoveredbyanyexistingbugpattern.atthesametime severalbugscouldhavebeenfoundbyminorvariantsofthe existing bug detectors.
methodology this section presents our methodology for studying which bugs are detected by static bugdetectors.
at first we describe the bugs .
and bug detection tools .
that we study.
then we present ourexperimental procedure foridentifying and validating matchesbetween thewarnings reportedbythe bugdetectors and the real world bugs .
.
finally we discuss threats to validity in .
.
.
real world bugs ourstudybuilds onanextendedversionof thedefects4jdataset a collection of bugs from popular java projects.
in total the data setconsistsof597bugsthataregatheredfromdifferentversions of projects.
we use defects4j for this study for three reasons.first it provides a representative set of real world bugs that has beengatheredindependentlyofourwork.thebugscoverawide spectrumof applicationdomains andhavebeen sampledin away thatdoesnotbiasthedatasetinanyrelevantway.second thedata set is widely used for other bug related studies e.g.
on test generation mutationtesting faultlocalization andbug repair showingthatishasbeenacceptedasarepresentative set of bugs.
third defects4j provides not only bugs but also the corresponding bug fixes as applied by the actual developers.
each bug is associated with two versions of the project that containsthe bug a buggy version just before applying the bug fix and a fixed version just after applying the bug fix.
the bug fixes have been isolated by removing any irrelevant code changes such asnew features or refactorings.
as a result each bug is associated with one or more java classes i.e.
source code files that have been modified to fix the bug.
the availability of fixes is important not onlytovalidatethatthedevelopersconsideredabugasrelevant but also to understand its root cause.
the current official version of defects4j version .
.
consists of395bugscollectedfrom6javaprojects.arecentadditiontothese bugs extends the official release with additional bugs from additionalprojects.1inourwork weusetheextendedversionof the dataset andrefer toit as defects4j .
table 1liststhe projects authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
how many of all bugs do we find?
ase september montpellier france table projects and bugs of defects4j.
id project bugs original defects4j chart jfreechart closure google closure lang apache commons lang math apache commons math 106mockito mockito framwork 38time joda time total of projects extended defects4j codec apache commons codec cli apache commons cli 24csv apache commons csv 12jxpath apache commons jxpath 14guava guava library 9jcore jackson core module jdatabind jackson data binding jxml jackson xml utilities 5jsoup jsoup html parser total of projects total of projects and bugs in the data set.2we exclude three of the bugs for technicalreasons lang 48becauseerrorpronedoesnotsupport java .
and codec and jsoup because they introduce a new classinthebugfix whichdoesnotmatchourmethodologythat relies on analyzing changes to existing files.
.
static bug detectors we study three static bug detectors for java i error prone atooldevelopedbygoogleandisintegratedintotheirtricorder static analysis ecosystem ii infer a tool developed and usedinternallybyfacebook and iii spotbugs thesuccessorofthepioneeringfindbugs tool.thesetoolsareusedbyprofessional softwaredevelopers.forexample errorproneandinferareauto maticallyappliedtocodechangestosupportmanualcodereview atgoogleandfacebook respectively.allthreetoolsareavailable as open source.
we use the tools with their default configuration.
.
experimental procedure givenasetofbugsandasetofstaticbugdetectors theoverallgoal of the methodology is to identify those bugs among the set bof provided bugs that are detected by the given tools.
we representadetected bug as a tuple b w where b bis a bug and wis a warning that points to the buggy code.
a single bug bmay be detectedbymultiplewarnings e.g.
b w1 and b w2 andasingle warning may point to multiple bugs e.g.
b1 w and b2 w .
a naive approach to assess whether a tool finds a particular bug would be to apply the tool to the buggy version of the code and to 2we refer to bugs using the notation projectid n where n is a unique number.
figure overview of our methodology.
manually inspect each reported warning.
unfortunately static bug detectors may produce many warnings and manually inspecting eachwarning foreach buggyversion ofa programdoes notscale tothenumberofbugsweconsider.anotherpossibleapproachistofullyautomaticallymatchwarningsandbugs e.g.
byassumingthateverywarningatalineinvolvedinabugfixpointstotherespective bug.
while this approach solves the scalability problem it risks to overapproximatethenumberofdetectedbugs.thereasonisthat some warnings may coincidentally match a code location involved in a bug but nevertheless do not point to the actual bug.
ourapproachtoidentifydetectedbugsisacombinationofautomatic and manual analysis which reduces the manual effortcompared to inspecting all warnings while avoiding the overap proximation problem of a fully automatic matching.
to identify thedetectedbugs weproceedintwomainsteps assummarized infigure .
the first step automatically identifies candidates for detected bugs i.e.
pairs of bugs and warnings that are likely to matcheachother.weapplythreevariantsofthemethodologythat differ in how to identify such candidates anapproachbasedondifferencesbetweenthecodebefore and after fixing the bug anapproachbasedonwarningsreportedforthecodebefore and after fixing the bug and the combination of the two previous approaches.
the second step is to manually inspect all candidates to decide which bugs are indeed found by the bug detectors.
this step is importanttoavoidcountingcoincidentalmatchesasdetectedbugs.
.
.
identifying candidates for detected bugs.
commondefinitions.
weexplainsometermsandassumptions used throughout this section.
given a bug b we are interested in the set lbofchanged lines i.e.
lines that were changed when fixing the bug.
we assume that these lines are the locations where developers expect a static bug detector to report a warning.
in principle thisassumptionmaynotholdbecausethebuglocation and the fix location may differ.
we further discuss this potential threat to validityin .
.
we computethe changed lines based on the differences or short the diff between the code just before and justafterapplyingthebugfix.thediffmayinvolvemultiplesource code files.
we compute the changed lines as lines that are modified authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france andrew habib and michael pradel or deleted as these are supposed to directly correspond to the bug.
inaddition weconsideraconfigurablewindowoflinesaroundthe location of newly added lines.
as a default value we use a window size of .
applyingabugdetectortoaprogramyieldsasetofwarnings.
we refer to the sets of warnings for the program just before and just after fixing a bug baswbefore b andwafter b or simply wbeforeandwafterifthebugisclearfromthecontext.thebug detectorsweusecananalyzeentirejavaprojects.sincethepurpose ofourstudyistodeterminewhetherspecificbugsarefound we apply the analyzers only to the files involved in the bug fix i.e.
files that contain at least one changed line l lb.
we also provide eachbugdetectorthefullcompilepathalongwithallthird party dependencies of each buggy or fixed program so that inter project and third party dependencies are resolved.
the warnings reported when applying a bug detector to a file are typically associated with specific line numbers.
we refer to the lines that are flagged by a warning waslines w .
diff basedmethodology.
oneapproachtocomputeasetofcandidatesfordetectedbugsistorelyonthediffbetweenthebuggyand thefixedversionsoftheprogram.theintuitionisthatarelevant warning should pinpoint one of the lines changed to fix the bug.
inthisapproach weperformthefollowingforeachbugandbug detector computethelinesthatareflaggedwithatleastonewarning in the code just before the bug fix lwarnin s uniondisplay.
w wbef orelines w computethecandidatesofdetectedbugsasallpairsofabug andawarningwherethechangedlinesofthebugoverlap with the lines that have a warning bdiff cand b w lb lwarnin s nequal forexample thebugin figure2a isacandidateforabugdetected byspotbugsbecausethetoolflaggedline55 whichisalsointhe set of changed lines.
fixedwarnings basedmethodology.
as analternative approach foridentifyingasetofcandidatesfordetectedbugs wecompare thewarningsreportedforthecodejustbeforeandjustafterfixingabug.theintuitionisthatawarningcausedbyaspecificbugshould disappearwhenfixingthebug.inthisapproach weperformthe following for each bug and bug detector compute the set of fixed warnings i.e.
warnings that disappear after applying the bug fix wfixed wbefore wafter compute the candidates for detected bugs as all pairs of a bugandawarningwherethewarningbelongstothefixed warnings set bfixed cand b w w wfixed inthis step wedo notmatch warningmessages based online numbers because line numbers may not match across the buggy and fixed files due to added and deleted code.
instead we comparethemessagesbasedonthewarningtype category severity rank and code entity e.g.
class method and field.
for example figure 2c shows a bug that the fixed warningsbased approach finds as a candidate for a detected bug by error pronebecausethewarningmessagereportedatline175disappears in the fixed version.
in contrast the approach misses the candidate buginfigure2a becausethedeveloperre introducedthesamekind of bug inline andhence the same warning isreported in the fixed code.
combinedmethodology.
thediff basedapproachandthefixed warnings based approach may yield different candidates for detected bugs.
for instance both approaches identify the bugs in figure 2candfigure 2d as candidates whereas only the diff based approach identifies the bugs in figure 2a andfigure 2b.
therefore weconsiderasathirdvariantofourmethodologythecombination of the fixed warnings and the diff based approach bcombine cand bdiff cand bfixed cand unless otherwise mentioned the combined methodology is the default in the remainder of the paper.
.
.
manual inspection and classification of candidates.
the automatically identified candidates for detected bugs may contain coincidentalmatchesof abugandwarning.
forexample suppose that a bug detector warns about a potential null dreference at a specificlineandthatthislinegetsmodifiedaspartofabugfix.if the fixed bug is completely unrelated to dereferncing a null object thenthewarning wouldnothavehelpedadeveloperinspotting the bug.
toremovesuchcoincidentalmatches wemanuallyinspectall candidatesfordetectedbugsandcomparethewarningmessages against the buggy and fixed versions of the code.
we classify each candidateintooneofthreecategories i ifthewarningmatches thefixedbugandthefixmodifieslinesthataffecttheflaggedbug only then this is a full match.
ii if the fix targets the warning but also changes other lines of code not relevant tothe warning then it is apartial match.
iii if the fix does not relate to the warning message at all then it is a mismatch.
forexample thebugin figure2d isclassifiedasafullmatchsince thebugfixexactlymatchesthewarningmessage topreventa nullpointerexception on the value returned by ownerdocument a checkfornullnesshasbeenaddedinthehelpermethod getoutputsettings whichcreatesanempty document objectwhen ownerdocument returns null.
as an example of a partial match consider the bug in figure 2a.
as we discussed earlier in .
.
the developer attempted a fix by applyingpropercheckandcastinlines58 63ofthefixedversion.
weconsiderthiscandidatebugapartialmatchbecausethefixed version also modifies line in the buggy file by changing the returnvalueofthemethod hashcode .thischangeisnotrelated to the warning reported by spotbugs.
it is worth noting that the factthatthedeveloperunfortunatelyre introducedthesamebug in line of the fixed version does not contribute to the partial matching decision.
finally thebugin figure2b isanexampleofamismatchbecause the warning reported by error prone is not related to the bug fix.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
how many of all bugs do we find?
ase september montpellier france buggy code override 54public boolean equals object o 55returnmethod.equals o override 59public int hashcode 60return1 bug fix fixed code override 54public boolean equals object o 55if this o 56return true 58if oinstanceof delegatingmethod 59delegatingmethod that delegatingmethod o 60returnmethod.equals that.method else 62returnmethod.equals o override 67public int hashcode 68returnmethod.hashcode a bug mockito .
warning by spotbugs at line equality check for operand not compatible with this.lb .
found by diff based methodology.
classification partial match.
buggy code 1602publicdfp multiply final int x 1603returnmultiplyfast x bug fix fixed code 1602publicdfp multiply final int x 1603if x x radix returnmultiplyfast x else returnmultiply newinstance x b bug math .
warning by error prone at line missing override.
lb .
found by diff based methodology.
classification mismatch.
buggy code 173publicweek date time timezone zone defer argument checking... 175this time regulartimeperiod.default time zone locale.getdefault bug fix fixed code 173publicweek date time timezone zone defer argument checking... 175this time zone locale.getdefault c bugchart .warningbyerrorproneatline175 chainingconstructorignoresparameter.
lb .foundby diff basedmethodology and fixed warnings based methodology.
classification full match.
buggy code 214publicdocument ownerdocument 215if this instanceof document 216return document this 217else if parentnode null 218return null 219else 220returnparentnode.ownerdocument ... 362protected void outerhtml stringbuilder accum newnodetraversor new outerhtmlvisitor accum ownerdocument .outputsettings .traverse this bug fix fixed code 362protected void outerhtml stringbuilder accum 363newnodetraversor new outerhtmlvisitor accum getoutputsettings .traverse this if this node has no document or parent retrieve the default output settings 367private document.outputsettings getoutputsettings 368returnownerdocument !
null?
ownerdocument .outputsettings new document .outputsettings d bugjsoup .warningbyinferatline363 nulldereference.
lb .foundby diff basedmethodology and fixed warnings based methodology.
classification full match.
figure candidates for detected bugs and their manual classification.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france andrew habib and michael pradel .
.
error rate.
beyond the question of how many of all bugs are detected we also consider the error rate of a bug detector.
intuitively it indicates how many warnings the bug detector reports.
we compute the error rate by normalizing the number of reported warnings to the number of analyzed lines of code er summationtext.
b b wbefore b summationtext.
b b summationtext.
f files b loc f wherefiles b arethefilesinvolvedinfixingbug bandloc f yields the number of lines of code of a file.
.
threats to validity asforallempiricalstudies therearesomethreatstothevalidity of the conclusions drawn from our results.
one limitation is the selection of bugs and bug detectors both of which may or may not be representative for a larger population.
to mitigate this threat we use a large set of real world bugs from a diverse set of popular open source projects.
moreover the bugs have been gathered independently of our work and have been used in previous bug related studies .forthebugdetectors westudytoolsthatare widely used in industry and which we believe to be representative for the current state of the art.
despite these efforts we cannot claim that our results generalize beyond the studied artifacts.
anotherthreattovalidityisthatourmethodologyforidentifyingdetectedbugscould inprinciple bothmisssomedetectedbugsandmisclassifycoincidentalmatchesasdetectedbugs.areasonfor potentially missing detected bugs is our assumption that the lines involved in a bug fix correspond to the lines where a developer expects a warning to be placed.
in principle a warning reported at someotherlinemighthelpadevelopertofindthebug e.g.
because the warning eventually leads the developer to the buggy code location.sincewecouldonlyspeculateaboutsuchcausaleffects we instead use the described methodology.
the final decision whether a warning corresponds to a bug is taken by a human and therefore subjective.
to address this threat both authors discussed every candidate for a detected bug where the decision is not obvious.
a final threat to validity results from the fact that static bug detectors may have been used during the development processof the studied projects.
if some of the developers of the studied projectsusestaticbugdetectorsbeforecheckingintheircode they mayhavefoundsomebugsthatwemissinthisstudy.asaresult our results should be understood as an assessment of how many of those real world bugs that are committed to the version control systems can be detected by static bug detectors.
experimental results thissectionpresentstheresultsofapplyingourmethodologyto bugs and three bug detectors.
we start by describing some properties of the studied bugs .
and the warnings reported by thebug detectors .
.next we reporton thecandidates for detected bugs .
and how many of them could be manually validatedandtheirkinds .
followedbyacomparisonofthe studied bug detectors .
.
to better understand the weaknesses of current bug detectors .6discusses why the detectors miss 1number of bugs number of buggy files501 a number of buggy files.
diff size between buggy and fixed versions loc b total size of diffs between buggy and fixed files.
figure properties of the studied bugs.
table warnings generated by each tool.
the minimum maximum and average numbers of warnings are per bug and consider all files involved in the bug fix.
warnings per bug tool min max avg total error rate error prone .
.01225infer .
.00055spotbugs .
.
total manybugs.finally .7empiricallycomparesthethreevariants of our methodology.
.
properties of the studied bugs to better understand the setup of our study we measure several propertiesofthe594studiedbugs.
figure3a showshowmanyfiles are involved in fixing a bug.
for around of the bugs the fixinvolves changing a single source code file.
figure 3b shows the numberoflinesofcodeinthediffbetweenthebuggyandthefixed versions.
this measure gives an idea of how complex the bugs and their fixes are.
the results show that most bugs involve a small numberoflines for424bugs thediffsizeisbetweenoneandnine lines of code.
two bugs have been fixed by modifying deleting or inserting more than lines.
.
warnings reported by the bug detectors thefirststepinourmethodologyisrunningeachtoolonallfiles involvedineachofthebugs.
table2showstheminimum maximum and average number of warnings per bug i.e.
in the files involved infixingthebug thetotalnumberofwarningsreportedbyeach tool and the error rate as defined in .
.
.
we find that error pronereportsthehighestnumberofwarnings withamaximumof warnings and an average of .
warnings per bug.
this is also reflected by an error rate of .
.
the studied bug detectors label each warning with a description of the potential bug.
table 3shows the top kinds of warnings reported by each tool.
the most frequent kind of warning by error prone is about missing the override annotation when a method authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
how many of all bugs do we find?
ase september montpellier france table top warnings reported by each static checker.
warning count error prone missing override comparison using reference equality boxed primitive constructor 234operator precedence type parameter unused in formals infer nulldereference thread safety violation unsafe guardedby access resource leak method with mutable return type returns immutable collection1 spotbugs switchwithoutdefault inefficient numberconstructor read of unwritten field method naming convention reference to mutable object overrides amethodwith thesame signaturein itsparent class.infer s most reported kind of warning complains about a potentialnull dereference.
finally the most frequent kind of warning by spotbugs is related to missing the default case in aswitchstatement.thequestionhowmanyofthesewarningspointtoavalid problem i.e.
true positives is outside of the scope of this paper.
.
candidates for detected bugs given the number of reported warnings which totals to itwouldbeverytime consumingtomanuallyinspecteach warning.theautomatedfilteringofcandidatesfordetectedbugs yields a total of warnings and candidates which significantlyreducesthenumberofwarningsandbugstoinspect.
compared to all reported warnings the selection of candidates reduces the number of warnings by .
thenumberofwarningsisgreaterthanthenumberofcandidates because we count warnings and candidates obtained from all tools together and each tool could produce multiple warnings per line s .
.
validated detected bugs to validate the candidates for detected bugs we inspect each of themmanually.basedontheinspection weclassifyeachcandidateasafullmatch apartialmatch oramismatch asdescribedin .
.
.
overall the three tools found bugs as detailed in the tableinfigure .
after removing duplicates i.e.
bugs found by more than one tool there are unique validated detected bugs.
we draw two conclusions from these results.
first the fact that unique bugs are detected by the three studied bug detectors showsthatthesetoolswouldhavehadanon negligibleimpact if they would have been used during the development of the studiedtool bugs error prone 8infer 5spotbugs total total of 27unique bugs figure total number of bugs found by all three staticcheckers and their overlap.
programs.
this result is encouraging for future work on static bug detectors and explains why several static bug detection tools have beenadoptedinindustry.second evenwhencountingbothpartial and full matches the overall bug detection rate of all three bug detectors together is only .
.
while reaching a detection anywherecloseto100 iscertainlyunrealistic e.g.
becausesomebugsrequireadeepunderstandingofthespecificapplicationdomain webelieve that the current state of the art leaves room for improvement.
togetanideaofthekindsofbugsthecheckersfind wedescribe the most common patterns that contribute to finding bugs.
outof the eight bugs found by error prone three are due to missingan override annotation and two bugs because the execution may fall through a switchstatement.
for the five bugs found by infer fourbugsarepotential nulldeferences.outofthe18bugs detected by spotbugs three are discovered by pointing to dead local stores i.e.
unnecessarily computed values and two bugs are potential nulldeferences.finally thetwobugsfoundbybothinfer and spotbugs are nulldeferences whereas the two bugs found by both error prone and spotbugs are a string format error and an execution that may fall through a switchstatement.
.
comparison of bug detectors the right hand side of figure shows to what extent the bug detectors complement each other.
spotbugs finds most of the bugs 18ofall27 ofwhich14arefoundonlybyspotbugs.errorprone finds bugs that are not found by any other tool and infer finds bugs missed by the other tools.
we conclude that the studied tools complementeachothertoalargeextent suggestingthatdevelopers may want to combine multiple tools and that researchers couldaddress the problem of how to reconcile warnings reported by different tools.
.
reasons for missed bugs tobetterunderstandwhythevastmajorityofbugsarenotdetected bythe studiedbugdetectors wemanuallyinspect andcategorize some of the missed bugs.
we inspect a random sample of ofall bugs that are not detected by any bug detector.
for each sam pled bug we try to understand the root cause of the problem by inspectingthediffandbysearchingforanyissuereportsassociated with the bug.
next we carefully search the list of bug patterns supported by the bug detectors to determine whether any of thedetectors could have matched the bug.
if there is a bug detectorthat relates to the bug e.g.
by addressing a similar bug pattern then we experiment with variants of the buggy code to understand authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france andrew habib and michael pradel why the detector has not triggered an alarm.
based on this process we have the two interesting findings.
.
.
domain specific bugs.
first the majority of the missed bugs out of are domain specific problems not related to any of the patterns supported by the bug checkers.
the root causes of these bugs are mistakes in the implementation of applicationspecificalgorithms typicallybecausethedeveloperforgottohandle a specific case.
moreover these bugs manifest in ways that are difficulttoidentifyasunintendedwithoutdomainknowledge e.g.
by causing an incorrect string to be printed or an incorrect number to be computed.
for example math is a bug in the implementation of a mathematical optimization algorithm that returns the last computed candidate value instead of the best value found so far.
anotherexampleisclosure abuginajavascriptcompilerthat failstoproperlyhandlesomekindsoffunctiondeclarations.finally time is due to code that handles dates but forgot to consider leap years and the consequences of february .
.
.
near misses.
second some of the bugs out of could be detected with a more powerful variant of an existing bug detector.
we distinguish two subcategories of these bugs.
on the one hand therootcausesofsomebugsareproblemstargetedbyatleast one existing bug detector but the current implementation of the detectormissesthebug.thesebugsmanifestthroughabehavior that istypicallyconsidered unintended such asinfinite recursion orout of bounds arrayaccesses.for example commons csv 7is causedbyaccessinganout of boundsindexofanarray whichis oneofthebugpatternssearchedforbyspotbugs.unfortunately thespotbugscheckerisintra procedural whiletheactualbugcomputes the array index in one method and then accesses the array inanothermethod.anotherexampleislang whichcausesan infinite loop because multiple methods call each other recursively and the conditions for stopping the recursion miss a specific input.
both error prone and spotbugs have checkers for infinite loops caused by missing conditions that would stop recursion.
however these checkers target cases that are easier to identify than lang which would require inter procedural reasoning about integer values.athirdexampleinthissubcategoryischart whichcausesan indexoutofboundsexception when calling arraylist.add .
the existingcheckerforout of boundsaccessestoarraysmighthave caught this bug but it does not consider arraylists.
ontheotherhand therootcausesofsomebugsareproblems that are similar to but not the same as problems targeted by an existing checker.
for example commons codec is about forgetting to override some methods of the jdk class filterinputstream .
while spotbugs and error prone have checkers related to streams including some that warn about missing overrides none of the existing checkers targets the methods relevant in this bug.
.
.
implications for future work.
we draw several conclusions from our inspection of missed bugs.
the first and perhaps most important is that there is a huge need for bug detection techniques that can detect domain specific problems.
most of the ex isting checkers focus on generic bug patterns that occur across projectsandoftenevenacrossdomains.however asmostofthe missed bugsare domain specific future workshould complement theexistingdetectorswithtechniquesbeyondcheckinggenericbugtable4 candidatewarnings w andbugs b obtainedfrom the automatic matching.
approach diff based fixed warnings combined tool wb wb wb error prone infer spotbugs total patterns.
one promising direction could be to consider informal specifications such as natural language information embedded in code or available in addition to code.
wealsoconcludethatfurtherworkonsophisticatedyetpractical staticanalysisisrequired.giventhatseveralcurrentlymissedbugscouldhavebeenfoundbyinter proceduralvariantsofexistingintra proceduralanalysessuggestsroomforimprovement.thechallenge here is to balance precision and recall because switching to interproceduralanalysisneedstoapproximate e.g.
callrelationships thisstepriskstocauseadditionalfalsepositives.anotherpromising direction suggested by our results is to generalize bug detectors that havebeen developed fora specific kind ofproblem to related problems e.g.
arraylists versus arrays.
finally our findings suggest that some bugs are probably easier todetectwith techniquesotherthanstaticcheckers.
forexample themissedbugsthatmanifestthroughclearsignsofmisbehavior suchasaninfiniteloop aregoodcandidatesforfuzz testingwith automated test generators.
.
assessment of methodologies we compare the three variants of our methodology and validate thatthemanualinspectionofcandidatesofdetectedbugsiscrucial.
.
.
candidates of detected bugs.
ourmethodology foridentifying candidates for detected bugs has three variants .
.
.
table 4compares them by showing for each variant how many warnings and bugs it identifies as candidates.
the number of warningsislargerthanthenumberofbugsbecausethelinesinvolved in a single bug may match multiple warnings.
overall identify ingcandidatesbasedondiffsyieldsmanymorewarnings 132in total than by considering which warnings are fixed by a bug fix which yields warnings.
combining the two methodologies by consideringtheunionofcandidatesgivesatotalof153warnings correspondingto89bugs.sincemorethanonestaticcheckercould pointtothesamebug thetotalnumberofuniquecandidatesfor detected bugs by all tools together boils down to bugs.
figure5visualizeshowthevariantsofthemethodologycomplement each other.
for example for error prone the fixed warningsbasedapproachfinds14candidates 2ofwhichareonlyfoundby this approach.
the diff based technique finds candidates not found by the fixed warnings approach.
overall the diff based and the fixed warnings based approaches are at least partially complementary making it worthwhile to study and compare both.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
how many of all bugs do we find?
ase september montpellier france figure5 candidatedetectedbugsusingthetwodifferentautomatic matching techniques.
.
.
validated detected bugs.
figure6showshowmanyofthe candidates obtained with the diff based and the fixed warningsbased approach we could validate during the manual inspection.
theleftchartof figure6a showstheresultsofmanuallyinspecting each warning matched by the diff based approach.
for instance out of the matched warnings reported by error prone are full matchesand2arepartialmatches whereastheremaining43donot correspond to any of the studied bugs.
the right chart in figure 6a shows how many of the candidate bugs are actually detected by the reported warnings.
for example out of bugs that are possiblydetectedbyinfer wehavevalidated3.
figure6b andfigure6c show the same charts for the fixed warnings based approach and the combined approach.
thecomparisonshowsthatthediff basedapproachyieldsmany more mismatches than the fixed warnings based approach.
given this result one may wonder whether searching for candidates only based on fixed warnings would yield all detected bugs.
in figure we see for each bug detector how many unique bugs are found by the two automaticmatching approaches.
for both error proneandinfer althoughthediff basedapproachyieldsalargenumber of candidates the fixed warnings based methodology is sufficient toidentifyalldetectedbugs.forspotbugs though onedetected bug would be missed when inspecting only the warnings that have been fixed when fixing the bug.
the reason is bug mockito infigure 2a.
the fixed warnings based methodology misses this bugbecausethebugfixaccidentallyre introducesanotherwarning of the same kind at line of the fixed code.
in summary we find that the fixed warnings based approach requireslessmanualeffortwhilerevealingalmostalldetectedbugs.
this result suggests that future work could focus on the fixed warnings based methodology allowing such work to manually inspect even more warnings than we did.
.
.
manual inspection.
table4showsthatthecombinedapproach yields candidate warnings corresponding to unique bugs.
however the manual validation reveals that only ofthosewarningsandacorrespondingnumberof31 27unique bugs correspond to actual bugs whereas the remaining matches are coincidental.
out of the validated candidates are fullmatches and are partial matches figure 6c .
in other words ofthecandidatewarningsand66 ofthecandidatebugsare spuriousmatches i.e.
thewarningisaboutsomethingunrelatedto the specific bug and only happens to be on the same line.
these results confirm that the manual step in our methodology is important to remove coincidental matches.
omitting the manual inspectionwouldskewtheresultsandmisleadthereadertobelievethatmorebugsaredetected.thisskewingofresultswouldbeeven stronger for bug detectors that report more warnings per line of code as evidenced in an earlier study .
to ease reproducibility and to enable others to build on our results full details of all results are available online.
related work studies of static bug detectors.
most existing studies of static bug detectors focus on precision i.e.
how many of all warnings reported by a tool point to actual bugs .
in contrast our study asks the opposite question what is the recall of static bug detectors i.e.
howmanyofall known bugsarefound?another differencetoexistingstudiesisourchoiceofstaticbugdetectors to the best of our knowledge this is the first paper to study the effectiveness of error prone infer and spotbugs.
theperhapsmostrelatedexistingworkisastudy bythunget al.
thatalsofocusesontherecallofstaticbugdetectors.our work differs in the methodology because we manually validate eachdetectedbug andintheselectionofbugsandbugdetectors becausewefocusonmorerecent industriallyusedtools.asaresultofourimprovedmethodology ourresultsdiffersignificantly while theyconcludethatbetween64 and99 ofallbugsarepartiallyor fully detected we find that only .
of all studied bugs are found.themainreasonforthisdifferenceisthatsome ofthebug detectorsusedbythung etal.reportalargenumber ofwarnings.
forexample asingletoolalonereportsover39 000warningsfor the lucene benchmark loc causing many lines to beflagged with at least one warning with error rate .
.
.
.
since their methodologyfully automaticallymatches sourcecode linesandlineswithwarnings mostbugsappeartobefound.instead we manually check whether a warning indeed corresponds to a particular bug to remove false matches.
to facilitate evaluating bug detection techniques several benchmarksofbugshavebeenproposed.bugbench consistsof17 bugsincprograms.cifuentesetal.significantlyextendthisbenchmark resulting in181 bugsthat aresampledfrom fourcategories e.g.
bufferoverflows.theyusethebenchmarktocompare4bug detectorsusinganautomatic line basedmatchingtomeasurerecall.
future work could apply our semi manual methodology to theirbug collection to study whether our results generalize to c programs.rahmanetal.comparethebenefitsofstaticbugdetectors andstatisticalbugprediction .toevaluatewhetheranapproach wouldhavedetectedaparticularbug theirstudycomparesthelines flagged with warnings and the lines changed to fix a bug which roughly corresponds to the first step of our methodology and lacks amanualvalidationwhetherawarningindeedpointstothebug.
johnson et al.
conducted interviews with developers to understand whystaticbugdetectorsare not used .thestudysuggeststhat betterwaysofpresentingwarningstodevelopersandintegrating bugdetectorsintothedevelopmentworkflowwouldincreasethe usage of these tools.
studies of other bug detection techniques.
other studies consider bug detection techniques beyond static bug detectors testgeneration .
one of these studies also considers bugs authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france andrew habib and michael pradel error prone infer s potbugsnumber of warnings63 error prone infer s potbugsnumber of bugs a diff based approach.error prone infer s potbugsnumber of warnings error prone infer s potbugsnumber of bugs b fixed warnings based approach.error prone infer s potbugsnumber of warnings error prone infer s potbugsnumber of bugsfull match partial match mismatch c combined approach.
figure manual inspection of candidate warnings and bugs from the two automatic matching approaches.
figure actual bugs found using the two different automatic matching techniques.
in defects4j and finds that most test generators detect less than of these bugs.
another study focuses on manual bug detection and compares the effectiveness of code reading functional testing andstructuraltesting .finally legunsenetal.studytowhat extent checking api specifications via runtime monitoring reveals bugs.
all these studies are complementary to ours as we focus on static bug detectors.
future work could study how well different bug finding techniques complement each other.
studiesofbugsandbugfixes.
animportantsteptowardimproving bug detection is to understand real world bugs.
to this end studieshaveconsideredseveralkindsofbugs includingbugsinthe linuxkernel concurrencybugs andcorrectness and performance bugs in javascript.
pan et al.
study bug fixes and identify recurring syntactical patterns .
work by ray et al.
reports that statistical language models trained on source code show buggycodetohavehigherentropythanfixedcode whichcan help static bug detectors to prioritize their warnings.
static bug detectors and real world deployments.
the lint tool originally presented in is one of the pioneers on staticbugdetection.sincethen staticbugdetectionhasreceived significant attention by researchers including work on finding apimisuses name basedbugdetection security bugs finding violations of inferred programmer beliefs and otherkindsofanomalydetection bugdetectionbasedonstatisticallanguagemodels andondetectingperformancebugs .
severalstaticbugdetectionapproacheshavebeenadoptedbyindustry.
bessey et al.
report their experiences from commercializing staticbugdetectors .ayewahetal.sharelessonslearnedfrom applyingfindbugs thepredecessorofthespotbugstoolconsidered in our study at google .
a recent paper describes the success of deploying a name based static checker .
since many bugdetectorssufferfromalargenumberofwarnings someofwhich are false positives an important question is which warnings to inspect first.
work on prioritizing analysis warnings addresses this question based on the frequency of true and false positives the version history of a program and statistical models based on features of warnings and code .
conclusion this paper investigates how many of all bugs can be found by currentstaticbugdetectors.toaddressthisquestion westudyaset of real world java bugs and three widely used bug detection tools.thecoreofourstudyisanovelmethodologytoassessthe recall of bug detectors which identifies detected bugs through a combination of automatic line based matching and manual validationofcandidatesfordetectedbugs.themainfindingsofthestudy include the following i static bug detectors find a non negligible number ofreal worldbugs showingthatstaticbugdetectors arecertainlyworthwhileanddevelopersshouldusethemduring development.
ii differentbugdetectorscomplementeachother inthesensethattheydetectdifferentsubsetsofthestudiedbugs.
usersofstaticbugdetectorsmaywanttorunmorethanonetool.
iii the large majority .
of the studied bugs are not detected by the studied tools showing that there is ample of room for improving thecurrent state of the art.
iv many ofthe missedbugs are due to domain specific problems instead of generic bug patterns while some currently missed bugs could be found with more powerful variants of existing checks.
overall we conclude that future work should focus not only on reducing false positives as highlighted by previous studies butalsoon detecting alargerfractionof all real worldbugs e.g.
by considering a larger variety of bug patterns and by searching domain specific bugs.
beyond these findings which are relevant to usersandcreatorsofstaticbugdetectors ourresultscanserveas abasisforafuturestudyoncomparingstaticanalysiswithother bug detection techniques such as manual and automated testing.