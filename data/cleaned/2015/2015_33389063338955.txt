a comprehensive study on deep learning bug characteristics md johirul islam mislam iastate.edu iowa state university ames ia usagiang nguyen gnguyen iastate.edu iowa state university ames ia usa rangeet pan rangeet iastate.edu iowa state university ames ia usahridesh rajan hridesh iastate.edu iowa state university ames ia usa abstract deep learning has gained substantial popularity in recent years.
developers mainly rely on libraries and tools to add deep learning capabilities to their software.
what kinds of bugs are frequently found in such software?
what are the root causes of such bugs?
what impacts do such bugs have?
which stages of deep learning pipeline are more bug prone?
are there any antipatterns?
understanding such characteristics of bugs in deep learning software has the potential to foster the development of better deep learning platforms debugging mechanisms development practices and encourage the development of analysis and verification frameworks.
therefore we study high quality posts from stack overflow and bug fix commits from github about five popular deep learning libraries caffe keras tensorflow theano and torch to understand the types of bugs root causes of bugs impacts of bugs bug prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy software.
the key findings of our study include data bug and logic bug are the most severe bug types in deep learning software appearing more than of the times major root causes of these bugs are incorrect model parameter ips and structural inefficiency si showing up more than of the times.
we have also found that the bugs in the usage of deep learning libraries have some common antipatterns.
ccs concepts software and its engineering software defect analysis computing methodologies machine learning .
keywords deep learning software q a forums bugs deep learning bugs empirical study of bugs this work was supported in part by us nsf under grants cns and ccf .
all opinions are of the authors and do not reflect the view of sponsors.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august tallinn estonia association for computing machinery.
acm isbn .
.
.
.
reference format md johirul islam giang nguyen rangeet pan and hridesh rajan.
.
a comprehensive study on deep learning bug characteristics.
in proceedings of the 27th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august tallinn estonia.
acm new york ny usa pages.
introduction a class of machine learning algorithms known as deep learning has received much attention in both academia and industry.
these algorithms use multiple layers of transformation functions to convert input to output each layer learning successively higher level of abstractions in the data.
the availability of large datasets has made it feasible to train adjust the weights of these multiple layers.
while the jury is still out on the impact of deep learning on overall understanding of software s behavior a significant uptick in its usage and applications in wide ranging areas combine to warrant research on software engineering practices in the presence of deep learning.
this work focuses on the characteristics of bugs in software that makes use of deep learning libraries.
previous work on this topic generally fall under two categories those that have studied bugs in the implementation of machine learning libraries themselves and those that have studied bugs in the usage of a specific deep learning library.
a key work in the first category is thung et al.
who studied bugs in the implementation of three machine learning systems mahout lucene and opennlp.
in the second category zhang et al.
have studied bugs in software that make use of the tensorflow library.
while both categories of approaches have advanced our knowledge of ml systems we do not yet have a comprehensive understanding of bugs encountered by the class of deep learning libraries.
this work presents a comprehensive study of bugs in the usage of deep learning libraries.
we have selected top five popular deep learning libraries caffe keras tensorflow theano andtorch based on the user counts from developers q a forum stack overflow .
while each of these libraries are for deep learning they have different design goals.
for example tensorflow focuses on providing low level highly configurable facilities whereas keras aims to provide high level abstractions hiding the low level details.
theano andtorch are focused on easing the use of gpu computing to make deep learning more scalable.
thus studying them simultaneously allows us to compare and contrast their design goals vis vis bugs in their usage.
esec fse august tallinn estonia md johirul islam giang nguyen rangeet pan and hridesh rajan table summary of the dataset used in the study librarystack overflow github posts bugs commits bugs caffe keras tensorflow theano torch total we have used two sources of data in our study posts about these libraries on stack overflow and also github bug fix commits.
the first dataset gives us insights into bugs that developers encounter when building software with deep learning libraries.
a number of these bugs would hopefully be fixed based on the discussion in q a forum.
the second dataset gives us insights into bugs that were found and fixed in open source software.
our study focuses on following research questions and compares our findings across the five subject libraries.
rq1 bug type what type of bugs are more frequent?
rq2 root cause what are the root causes of bugs?
rq3 bug impact what are the frequent impacts of bugs?
rq4 bug prone stages which deep learning pipeline stages are more vulnerable to bugs?
rq5 commonality do the bugs follow a common pattern?
rq6 bug evolution how did the bug pattern change over time?
findings at a glance.
our study show that most of the deep learning bugs are data bugs andlogic bugs the primary root causes that cause the bugs are structural inefficiency si and incorrect model parameter ips most of the bugs happen in the data preparation stage of the deep learning pipeline.
our study also confirms some of the findings of tensorflow conducted by zhang et al.
.
we have also studied some antipatterns in the bugs to find whether there is any commonality in the code patterns that results in bugs.
our findings show that there is strong correlation among the distribution of bugs as well as in the antipatterns.
finally we conclude with a discussion on our findings suggesting immediate actions and future research directions based on these findings.
methodology .
data collection we have used two different data sources for studying the bugs in deep learning software stack overflow posts and github bug fix commits.
a summary of these datasets is shown in table .
.
.
stack overflow data collection.
to study bugs in deep learning software we have collected data from stack overflow a wellknown q a site for developers to discuss software development problems.
the data collection process consists of two steps.
in the first step we select candidate posts discussing deep learning libraries.
we focus on five deep learning libraries caffe keras tensorflow theano and torch .
these are the five most discussed deep learning libraries on stack overflow .
we did that by searching for posts tagged with caffe keras tensorflow theano and torch .
when posts are about specific libraries they are more likely to talk about bugs in using deep learning libraries.
using these criteria we selected all posts about these five libraries.
we further filteredthe posts that did not contain any source code because posts about bugs usually contain code snippets.
moreover we reduced the number of posts by selecting the posts whose scores computed as the difference between the number of its upvotes and the number of its downvotes were greater than to focus on the high quality posts and keep the manual effort manageable.
after this step in total we retrieved and posts for caffe keras tensorflow theano and torch respectively for further study.
in the second step we manually read these candidates to identify the ones about bugs.
after that the second and the third authors manually reviewed the candidates.
for each post we read the question and all answers focusing on the best accepted one.
if the best accepted answer was to fix the usages of the deep learning api s in the question we considered that post as talking about deep learning bugs.
after this step we found and bugs for caffe keras tensorflow theano and torch respectively.
.
.
github data collection.
we mine the github commits to study the change in the commits and to check and confirm the bug patterns that we studied from stack overflow .
the data collection process consists of two steps.
first we collect all the repositories of caffe keras tensorflow theano and torch .
for collecting the repositories that use these libraries we first find the repositories that contain the keywords related to the libraries.
after that we mine all the commits whose title contains the word fix .
then we check the import statements in the program to identify if those repositories truely use deep learning libraries.
next we randomly select commits for each library from mined commits and classify them.
secondly we use the same process that we used for stack overflow.
specifically the second and the third authors manually studied the commits and separately label them.
after that these two authors compare their results to fix the conflict in the labeling process.
we study each line of change in the commits.
note that some commits may have more than one bugs and some commit may not have bug.
overall we got and bugs for the commits of caffe keras tensorflow theano and torch respectively.
.
classification in our classification we focus on three criteria which are bug types root causes and effects of bug.
the classification scheme used for labeling of the bugs in each of these three criteria discussed in .
.
and .
.
we have also classified the bugs into different deep learning stages .
to label the bug types we followed the classification from an already existing well vetted taxonomy and appended on top of that.
the added types were based on the data that we studied following an open coding scheme.
the bugs may have different root causes and effects.
a supervised pilot study and open coding schemes were used to identify the effects that are possible through these bugs.
we have adapted the classification scheme of root causes and bug effects from and added on top of that as found from the study of the posts.
one of the authors with expertise in these libraries studied the posts initially to come up with the classification scheme for bug types root causes and effects.
we followed the open coding scheme and a pilot study was conducted to get agreement on the classification.
511a comprehensive study on deep learning bug characteristics esec fse august tallinn estonia we also classified the bugs into different stages of the pipeline to understand which stages are more vulnerable to bugs.
deep learning process can be divided into seven stage pipeline .
the stages are data collection data preparation choice of model training evaluation hyper parameter tuning and prediction.
among the seven stages the first one is not related to software development.
the other stages are related to software development and are supported by the deep learning libraries through their apis.
we use these stages to label the bugs into different stages.
.
labeling the bugs once we have all the classification criteria we used those criteria to label the posts.
the second and the third authors independently studied the posts.
we measured the inter rater aggrement among the labellers using cohen s kappa coefficient when and of the posts were labeled.
after labeling the cohen s kappa coefficient was close to .
then we conducted a training session among the raters to clarify the labeling and what they mean.
after the training session we conducted another pilot study at including the first .
this time the cohen s kappa coefficient was .
we again discussed the results and find out the reasons for major disagreements.
we then discussed those cases further through examples and continued labeling.
the cohen s kappa coefficient was more than in subsequent pilot studies.
the labeling effort was continuously being monitored with the help of kappa coefficient to understand the agreement.
we conducted reconciling efforts ideally at every interval of the labeling.
the posts where there was disagreement between the raters were further discussed in the presence of a supervisor.
after discussion and arguments a common label was given.
finally all the bugs were given a common label.
.
types of bugs in deep learning software developers often encounter different types of bugs while trying to write deep learning software.
to understand those bugs and their root causes we have classified them into different categories.
the classification is inspired from and adapted based on all the stack overflow posts that we have analyzed.
.
.
api bug.
this group of bugs is caused by deep learning apis.
generally when a developer uses a deep learning api different bugs associated with that api are inherited automatically without the knowledge of the user.
the prime causes for triggering of deep learning api bugs can be because of the change of api definition with different versions lack of inter api compatibility and sometimes wrong or confusing documentation.
.
.
coding bug.
these kind of bugs originate due to programming mistakes.
this in turn introduces other types of bugs in the software which lead to either runtime error or incorrect results.
a big percentage of the deep learning bugs that we have checked arises from syntactic mistakes that cannot be fixed by changing only some lines of code.
this type of bugs are not identified by the programming language compiler resulting in wrong output.
.
.
data bug.
this bug may arise if an input to the deep learning software is not properly formatted or cleaned well before supplyingit to the deep learning model.
this type of bug occurs before data is fed to the deep learning model.
it is not because of the wrong deep learning model rather it is purely based on the type and structure of training or test data.
similar to coding bugs data bugs are usually flagged by the compiler but in some scenarios it can pass unchecked through the compilation process and generate erroneous results.
.
.
structural bug sb .
a vast majority of the deep learning bugs are occurring due to incorrect definitions of the deep learning model s structure.
these include mismatch of dimensions between different layers of deep learning models the presence of anomaly between the training and test datasets use of incorrect data structures in implementing a particular function etc.
these type of bugs can be further classified into four subcategories.
control and sequence bug.
this subclass of the bug is caused by the wrong structure of control flow.
in many scenarios due to wrong if else or loop guarding condition the model does not perform as expected.
this type of bug either leads to a crash when a part of deep learning model does not work or leads to incorrect functionality due to mishandling of data through the layers.
data flow bug.
the main difference between the data flow bug and the data bug is the place of origin.
if a bug occurs due to the type or shape mismatch of input data after it has been fed to the deep learning model we label it as data flow bug.
it includes those scenarios where model layers are not consistent because of different data shape used in consecutive layers.
to fix these bugs developers need to modify the model or reshape the data.
initialization bug.
in deep learning initialization bug means the parameters or the functions are not initialized properly before they are used.
this type of bugs would not necessarily produce runtime error but it will simply make the model perform worse.
here the definition of functions includes both user defined and api defined.
we also categorize a bug into this category when the api has not been initialized properly.
logic bug.
in deep learning the logical understanding of each stage of the pipeline is an integral part of the coding process.
with an incorrect logical structure of the deep learning model the output of a program may result in either a runtime error or a faulty outcome.
these bugs are often generated in the absence of proper guarding conditions in the code.
processing bug.
one of the most important decisions in the deep learning model structure is to choose the correct algorithm for the learning process.
in fact different deep learning algorithms can lead to different performance and output .
also to make different layers be compatible with each other the data types of each layer need to follow a contract between them.
processing bugs happen due to the violation of these contracts.
.
.
non model structural bug nmsb .
unlike sb nmsb occur outside the modeling stage.
in other words this bug can happen in any deep learning stage except the modeling stage such as the training stage or the prediction stage.
nmsb has similar subcategories as sb.
the subcategories of nmsb are control and sequence bug logic bug processing bug and initialization bug.
we do not define non model structural data flow bug like structural data 512esec fse august tallinn estonia md johirul islam giang nguyen rangeet pan and hridesh rajan flow bug because data bug already covers the meaning of non model structural data flow bug.
control and sequence bug.
this subclass is similar to control and sequence bug in sb.
the bug is caused by an incorrect structure of control flow like wrong if else condition however this kind of bug happens outside modeling stage.
initialization bug.
this subclass is similar to initialization bug in sb.
the bug is caused by incorrect initialization of a parameter or a function prior to its use.
logic bug.
this subclass is similar to logic bug in sb.
the bug is caused by misunderstanding the behavior of case statements and logical operators.
processing bug.
this subclass is similar to processing bug in sb.
the bug is caused by an incorrect choice of algorithm.
.
classification of root causes of bugs .
.
absence of inter api compatibility.
the main reason for these bugs is the inconsistency of the combination of two different kinds of libraries.
for example a user cannot directly use numpy function inkeras because neither tensorflow backend nor theano backend ofkeras has the implementation of numpy functions.
.
.
absence of type checking.
this kind of bugs involves a type mismatch problem when calling api methods.
these bugs are usually mistakes related to the use of wrong type of parameters in an api.
.
.
api change.
the reason for these bugs is the release of the new versions of deep learning libraries with incompatible apis.
in other words the bug happens when the new api version is not backward compatible with its previous version.
for example a user updates the new version of a deep learning library which has new api syntax however the user does not modify his her code to fit with the new version which leads to the api change bug.
.
.
api misuse.
this kind of bugs often arises when users use a deep learning api without fully understanding.
missing conditions can be one kind of api misuse and this bug occurs when a usage does not follow the api usage constraints to ensure certain required conditions.
crash is the main effect of these bugs.
.
.
confusion with computation model.
these bugs happen when a user gets confused about the function of deep learning api which leads to the misuse of the computation model assumed by the deep learning library.
for instance a user gets confused between the graph construction and the evaluation phase.
.
.
incorrect model parameter or structure ips .
ips causes problems with constructing the deep learning model e.g.
incorrect model structures or using inappropriate parameters.
ips is a common bug in the deep learning software because of both the lack of deep learning knowledge among the users and the incomprehensibilty of deep learning models.
this kind of bugs causes the functional incorrectness thus the effect of this bug is a crash.
.
.
others.
these bugs are not related to deep learninng software.
in other words these bugs are mostly related to mistakes in the development process like incorrect syntax.
.
.
structure inefficiency si .
si causes problems related to modeling stage in deep learning software like ips however si leads to bad performance of the deep learning software while ips leads to a crash.
.
.
unaligned tensor ut .
these bugs often occur in the computation graph construction phase.
when a user builds the computation graph in deep learning process they have to provide correct input data that satisfies input specifications of the deep learning api however many users do not know the api specifications or they misunderstand api signature leading to ut bugs.
.
.
wrong documentation.
incorrect information in library documentation leads to these bugs.
deep learning library users may face this kind of bugs when they read an incorrect definition or an incorrect usage of a deep learning api from documentation.
.
classification of effects of bugs .
.
bad performance.
bad performance or poor performance is one of common kind of effect in deep learning software.
furthermore the major root causes of this effect are si or ccm that are related to the model construction.
even though developers can use deep learning libraries correctly they still face model construction problems because apis in these libraries are abstract.
.
.
crash.
crash is the most frequent effect in deep learning.
in fact any kind of bugs can lead to crash.
a symptom of crash is that the software stops running and prints out an error message.
.
.
data corruption.
this bug happens when the data is corrupted as it flows through the network.
this effect is a consequence of misunderstanding the deep learning algorithms or apis.
when data corruption occurs a user will receive unexpected outputs.
.
.
hang.
hang effect is caused when a deep learning software ceases to respond to inputs.
either slow hardware or inappropriate deep learning algorithm can lead to hang.
a symptom of hang is that the software runs for a long period of time without providing the desired output.
.
.
incorrect functionality.
this effect occurs when the software behaves in an unexpeced way without any runtime or compile time error warning.
this includes the incorrect output format model layers not working desirably etc.
.
.
memory out of bound.
deep learning software often halts due to unavailability of the memory resources.
this can be caused by either the wrong model structure or not having enough computing resources to train a particular model.
frequent bug types in this section we explore the answer to rq1 through a statistical analysis of the labeled data.
the normalized distribution of bug types in stack overflow data is shown in figure .
the distribution of bugs shown in figure and the stack overflow andgithub data in table shows the presence of different kinds of bugs in both stack overflow andgithub for the deep learning libraries we have studied.
we present some of the key findings related to bug types in the following subsections.
513a comprehensive study on deep learning bug characteristics esec fse august tallinn estonia caffe keras tf theano torch api bug data bug nmsb.initialization bug nmsb.logic bugs nmsb.processing bug nmsb.control and sequence bug sb.control and sequence bug sb.data flow bug sb.initialization bug sb.logic bugs sb.processing bug figure distribution of bug types in stack overflow .
data bugs finding data bugs appear more than of the times from figure we see that among the bug types the data bugs frequently appear in all the libraries.
in the studied stack overflow data we have seen of the posts in tensorflow posts in keras posts in torch posts in theano and posts in caffe have data bugs.
data bugs mostly appear due to the absence of data pre processing facilities like feature engineering data validation data shuffling etc.
for example a developer is trying to read some image files using the following method1.
1d e f r e a d b y t e s t r e a m d t numpy .
dtype numpy .
u i n t .
newbyteorder r e t u r n numpy .
f r o m b u f f e r b y t e s t r e a m .
read dtype d t the developer eventually got stuck with the following error while trying to train the model using the data returned by the previous library call.
1typeerror only i n t e g e r s c a l a r a r r a y s can be c o n v e r t e d t o a s c a l a r index an expert suggested an answer to change the last return statement with the following which solved the problem and was accepted.
1r e t u r n numpy .
f r o m b u f f e r b y t e s t r e a m .
read dtype d t the bug is hard to fix by just looking at the error message.
it is difficult to identify the exact reason of bug which led the developer to post a question on stack overflow and the question was upvoted by other fellow developers as a qualified post.
the large percentage of data bugs indicate data pre processing related difficulties are quite common in deep learning software.
these bugs could be addressed by development and refinement of data verification tools.
support for modern abstract data types like dataframe and the properties of the model in data verification tool would help the deep learning community.
.
structural logic bugs finding caffe has structural logic bugs the second major bug type is structural logic bug in stack overflow that was expected from our initial hypothesis based on a pilot study.
caffe has more structural logic bugs in stack overflow compared statistics of bug types in stack overflow andgithub caffe keras tf theano torchp valueso github so github so github so github so githubapi bug .
data bug .
nmsb.control and sequence bug0 .
nmsb.initialization bug .
nmsb.logic bugs .
nmsb.processing bug .
sb.control and sequence bug .
sb.data flow bug .
sb.initialization bug .
sb.logic bugs .
sb.processing bug .
to other libraries.
other libraries also have significant portion of structural logic bugs ranging from .
.
api bugs finding torch keras tensorflow have and api bugs respectively in deep learning libraries api changes sometimes break the entire production code.
the implicit dependencies between libraries cause problems when one library has some major changes.
for example when numpy is updated tensorflow keras software may fail.
keras often uses tensorflow ortheano as backend and hence update of tensorflow ortheano can cause the software developed using keras to crash.
api bugs arise more often in keras andtensorflow as shown in figure .
more than of the api bugs are from keras andtensorflow .
an example of such bug is shown in the code snippet below.
the bug in the code below arises because the keyword names in the api signature of keras has changed.
1model .
f i t tx ty epochs b a t c h s i z e v e r b o s e the developer will get the error because epochs keyword does not exist in version of keras .
1model .
f i t tx ty b a t c h s i z e v e r b o s e epochs f i l e k e r a s models .
py l i n e i n f i t s t r kwargs 3e x c e p t i o n r e c e i v e d unknown keyword arguments epochs to fix this error the developer needs to change the keyword parameter from epochs tonb epoch .
1model .
f i t tx ty nb epoch b a t c h s i z e v e r b o s e .
bugs in github projects we have also analyzed the distributions of bugs in some github bug fix commits.
the distribution of bugs across different libraries in github data is shown in table .
we computed the p value using ttest where one distribution is bug type in github for all the libraries and the other distribution is bug type for all the libraries in stack overflow .
finding all the bug types have a similar pattern in github and stack overflow for all the libraries we analyze the stack overflow andgithub result using the t test to find whether the distributions differ significantly.
we use significant level to find the difference beween stack overflow and github results for each of the bug type in our analysis the null hypothesis is h0 the distributions are same .
if we fail to reject 514esec fse august tallinn estonia md johirul islam giang nguyen rangeet pan and hridesh rajan caffe keras tf theano torch absence of type checking api change api misuse confusion with computation model incorrect model parameter or structure structure ineffciency unaligned tensor absense of inter api compatibility others figure stack overflow root cause classification this null hypothesis using the t test then we can say the distributions follow the same pattern in both stack overflow andgithub data.
we see that for all the bug types except non model structural logic bug the p value is greater than indicating they have a similar pattern as we fail to reject the null hypothesis.
root cause in this section we present the analyses and findings to answer rq2 identifying major root causes of bugs in deep learning software.
the normalized distribution of root causes in stack overflow code snippets is shown in figure .
the data in table shows the presence of different categories of root causes in both stack overflow and github for the deep learning libraries and presents p value showing the similarity of distributions using t test.
we discuss the significant root causes in the following subsections.
.
incorrect model parameter ips finding ips is the most common root cause resulting in average of the bugs across the libraries ips results in bugs that causes the program to crash at runtime and the execution does not succeed.
in tensorflow andtheano ips leads other root causes in causing bugs having and of the total share of root causes respectively.
.
structural inefficiency si finding keras caffe have and bugs that arise from si si bugs do not cause the program to crash.
these bugs often yield suboptimal performance of the deep learning model.
these bugs have more relation to qos or non functional requirements.
for example a programmer is trying to train a model to recognize handwritten digits but the accuracy does not improve and stays constant from epochs .
1epoch s l o s s .
acc .
v a l l o s s .
v a l a c c .
3epoch s l o s s .
acc .
v a l l o s s .
v a l a c c .
.
.
.
.
.
.
.
.
.
statistics of the root causes of bugs caffe keras tf theano torchp valueso github so github so github so github so githubabsense of inter api compatibility0 .
absence of type checking .
api change .
api misuse .
confusion with computation model14 .
incorrect model parameter or structure26 .
others .
structure ineffciency .
unaligned tensor .
wrong documentation .
6epoch s l o s s .
acc .
v a l l o s s .
v a l a c c .
s the problem that was pointed out by an expert which solved the performance degradation bug is following in summary r e p l a c e t h i s l i n e 2model .
compile l o s s c a t e g o r i c a l c r o s s e n t r o p y o p t i m i z e r adam with t h i s 4from k e r a s .
o p t i m i z e r s import sgd 5opt sgd l r .
6model .
compile l o s s c a t e g o r i c a l c r o s s e n t r o p y o p t i m i z e r opt the answer suggested to change optimizer for enhancing the performance.
.
unaligned tensor ut finding torch has of the bugs due to ut in deep learning tensor dimensions are important for successful construction of the model.
tensorflow keras torch theano caffe have and of bugs due to ut respectively.
in torch ut is the leading root cause of bugs.
.
absence of type checking finding theano has of the bugs due to the absence of type checking most of the deep learning libraries are written in python.
due to the dynamic nature of python the problem of the absence of type checking is felt strongly in these libraries.
the absence of type checking leads to of the bugs in theano of the bugs in keras and of the bugs in tensorflow .
.
api change finding tensorflow andkeras have and bugs due to api change in deep learning libraries api change tends to have a drastic effect.
these libraries are interdependent.
so api change in one library breaks other libraries.
515a comprehensive study on deep learning bug characteristics esec fse august tallinn estonia absence of type checking absense of inter api compatibility api change apic api misuse apim confusion with computation model ccm incorrect model parameter or structure ips others structure ineffciency si unaligned tensor ut figure relation between root causes and types of bugs .
root causes in github data finding except api misuse all other root causes have similar patterns in both github andstack overflow root causes of bugs we computed the p value at significant level for both the stack overflow andgithub data for all the root causes in the five libraries.
we see that p value for api misuse root cause is much less than indicating api misuse in stack overflow andgithub has different distribution compared to other root causes as we reject the null hypothesis.
the other root causes are similar for both stack overflow andgithub data as their p value is greater than .
.
relation of root cause with bug type finding si contributes and ips contributes of the bugs related to model we have seen from figure that most of the non model related bugs are caused by api misuse .
non model structural initialization bugs and non model structural processing bugs are caused by api misuse in of the time in our studied data.
interestingly in api bug api change plays the vital role compared to api misuse however the model related bugs are more vulnerable to ips and si root causes.
we see from figure that structural control and sequence bug structaral data flow bug structural initialization bug structural logic bug structural processing bug which are related to model are caused by si and of the times respectively and caused by ips of the times respectively.
impacts from bugs in this section we explore the answer to rq3 to understand the major effects of bugs in deep learning software.
the normalized distribution of effects of stack overflow is shown in fig.
.
the data in table shows the presence of different kinds of effects in both stack overflow andgithub for the deep learning libraries.
we discuss some of the major effects of bugs in deep learning software in the rest of this section.
caffe keras tf theano torch bad performance crash data corruption hang incorrect functionality memory out of bound unknownfigure distribution of bug effects in stack overflow .
crash finding more than of the bugs cause crash.
our analysis reveals that the most severe effect of bugs is crash.
in deep learning the bugs mostly cause total failure of the program.
in all the libraries crash is the top impact ranging from as shown in figure .
.
bad performance finding in caffe keras tensorflow theano torch and bugs lead to bad performance respectively bad performance is often a concern for deep learning software developers.
even though the model trains successfully during the evaluation or prediction phase the model may give very poor accuracy in classifying the target classes.
for example in the following code snippet the user had low accuracy after traning because of the use of incorrect value of parameter nb words that is the value of the maximum size of the vocabulary of the dataset.
the developer should use nb words 1instead of nb words as answered by an expert3.
if the developer uses nb words instead of nb words the model will not train on the last word which can lead to the bad performance effect.
1embedded embedding nb words output dim hidden i n p u t l e n g t h maxlen sequence .
incorrect functionality finding of the bugs cause incorrect functionality incorrect functionality happens when the runtime behavior of the software leads to some unexplainable outcome that is not expected from the logical organization of the model or from previous experience of the developer.
for example in the following code snippet the user wants to convert the image to a 28numpy array however the output is a black image.
1with t f .
s e s s i o n as s e s s f i r s t i m a g e mnist .
t r a i n .
images f i r s t i m a g e np .
a r r a y f i r s t i m a g e dtype u i n t p i x e l s f i r s t i m a g e .
r e s h a p e p l t .
imshow p i x e l s cmap gray 516esec fse august tallinn estonia md johirul islam giang nguyen rangeet pan and hridesh rajan table effects of bugs in stack overflow andgithub caffe keras tf theano torchp valueso github so github so github so github so githubbad performance .
crash .
data corruption .
hang .
incorrect functionality .
memory out of bound .
unknown .
data preparation choice of modeltrainingevaluation hyperparameter tuningprediction stages...in...the...pipeline05101520253035bugs figure bugs across stages of the deep learning pipeline the user got incorrect output because of casting a float array to uint8 which will convert all the pixels to if they are less than .
to fix the problem the user can multiply the array with as suggested by an answer.
theano has a higher percentage of posts about incorrect functionality problems compared to bad performance.
.
effects of bugs in github finding for all the libraries the p value for stack overflow and github bug effects reject the null hypothesis to confirm that the bugs have similar effects from stack overflow as well as github bugs the p value is shown in table shows that bad performance in stack overflow andgithub have of p value which indicates that they are very similar.
crash has p value of in stack overflow and github indicating they also can not reject the null hypothesis with strong confidence.
none of the impacts reject the null hypothesis at significance level.
difficult deep learning stages in this section we answer rq4 by studying the bugs arising at the different stage of the deep learning pipeline.
we use the categorization of the posts about deep learning stages to analyze rq4 .
.
data preparation finding of the bugs are in the data preparation stage keras tensorflow caffe torch theanotheano torch caffe tensorflow keras0.
.
.
.
.
.
.
.
.99correlation of bug type .
.
.
.
.
figure correlation of bug types among the libraries from figure we see most of the bugs in deep learning programming happen at the data preparation stage.
.
training stage finding of the bugs are seen during the training stage the next bug prone stage is the training stage which is as expected.
most bugs related to ips and si arise in the training stage.
.
choice of model finding choice of model stage shows of the bugs choice of model is the third bug prone stage.
in choice of model stage we construct the model and chose the right algorithm.
major root causes of bugs in this stage are ips si and ut.
commonality of bug in this section we explore the answer to rq5 to identify whether there is any relationship among the bugs in different deep learning libraries.
our primary hypothesis was that the libraries will be strongly correlated based on the distribution of bugs as they are performing similar tasks.
our analysis confirms that hypothesis as shown in figure .
we see that the libraries have a strong correlation coefficient close to .
surprisingly caffe has shown very weak correlation with other libraries in terms of bug type.
we then randomly studied stack overflow posts for each of the libraries to see whether we notice any common antipatterns that can lead to this strong correlation of bug type.
finding tensorflow andkeras have a similar distribution of antipatterns while torch has different distributions of antipatterns we have identified the antipatterns through deeper analysis of the stack overflow buggy codes for further investigating the strong correlation of tensorflow andkeras as well as the weak correlation oftorch andcaffe .
the antipatterns found are continuous obsolescence cut and paste programming dead code golden hammer input kludge mushroom management spaghetti code .
this classification is taken from .
the distribution of different antipatterns across the libraries is shown in figure .
we see that in tensorflow andkeras of the antipatterns are input 517a comprehensive study on deep learning bug characteristics esec fse august tallinn estonia continuous obsolescencecut and paste programmingdead code golden hammer input kludge mushroom managementsphagetti code of antipatterncaffe keras tensorflow theano torch figure distribution of different antipatterns kludge.
on the other hand in torch of the bugs arise due to the cut and paste programming antipattern.
tensorflow andkeras have almost same distribution in continuous obsolescence and dead code as well.
this shows that the strong correlation between the distribution of bugs in tensorflow andkeras can be explained from the similarity of common antipatterns for these two libraries.
the weak correlation between the distribution of torch andcaffe bugs can be the result of a dissimilar distribution of antipatterns between these two libraries.
for example we see stack overflow code snippets of input kludge antipatterns from tensorflow and keras in the example shown in figure .
both of these programs can be easily broken by user input and the program does not perform sanity check on the inputs.
evolution of bugs in this section we explore the answer to rq6 to understand how the bug patterns have changed over time.
.
structural logic bugs are increasing finding in keras caffe tensorflow structural logic bugs are showing increasing trend from structural logic bugs in caffe are respectively indicating structural logic bugs are being discussed more by the developers since .
it is expected as deep learning started gaining increasing attention since and more developers started to use deep learning libraries to write software.
.
data bugs are decreasing finding data bugs slowly decreased since except torch intorch data bugs stayed almost consistent maintaining close to of the bugs in discussed in .
in keras data bugs slowly decreased from since .
in tensorflow data bugs slowly decreased from since .
in the other two libraries also the data bugs slowly decreased reaching close to .
the possible reason for this trend is the development of popular specialized data libraries like pandas that enable exploratory data analysis to understand the properties of data better.
besides theuse of tensor data type having type and shape information helps to get rid of some of the data bugs.
still more verification support in these libraries will help to get rid of these bugs.
threats to validity internal threat.
one internal threat to the validity of our results could be our classification of the bugs.
we used the classification scheme from a vetted taxonomy to classify the bugs.
we also followed open coding scheme to add more types if needed.
one phd student was initially dedicated to go over all the posts to come up with additional classification scheme if necessary.
this whole process was monitored using pilot study.
another possible source of the threat is that the labeling of the data can be biased.
to mitigate this threat two trained ph.d. students independently studied the misuse posts to label them.
the inter rater agreements was measured using cohen s kappa coefficient and the disagreements were reconciled under the monitoring of an expert.
we conducted pilot study to continuously monitor the labeling process and conducted further training at and of the labeling where the kappa coefficient was close to and .
external threat.
an external threat can be the trustworthiness of the dataset we collected.
to avoid low quality posts we only collected the posts that have score of at least .
a score of can be a good metric to trust the post as a good discussion topic among the programmer community that cannot merely be solved using some google search.
the reputation of the users asking question about deep learning can be another reason to question the quality of the posts.
to alleviate this threat we have only studied top scored posts which are from users with different range of reputations 150k .
this indicates that the posts are from users ranging from newbie to experts.
the dataset is unbalanced in terms of frequency of bugs studied for each library however to confirm the distribution of the bugs we have performed anova test on the bug types root causes and impacts for each library.
we have found that f .
f critical .
.
this implies that the means of the five libraries population are not significantly different.
this suggests that even though the dataset seems unbalanced in term of frequency the bug distribution is not.
discussion we have seen in the analysis of rq1 that most of the bugs in deep learning programming are data bugs.
these type of bugs can have drastic effect causing the program to crash as well as leading to bad performance.
in general we see the programmers have very limited or no access to data verification tools.
it is often confusing whether the data is in right format needed by the model whether the variables are properly encoded or not whether there are missing data that can cause the model to fail whether the train test split is good enough whether the data is shuffled properly to avoid training bias etc.
this finding suggests that development of data verification tools can help programmers solve a large number of data bugs.
as deep learning models are strongly coupled with data model analysis tool to explore whether a particular model is the right fit for the data in hand can help to resolve these strong coupling of data and model related problems.
518esec fse august tallinn estonia md johirul islam giang nguyen rangeet pan and hridesh rajan a tensorflow example of input kludge b keras example of input kludge figure example of similar antipattern in tensorflow andkeras figure timeline of evolution of bugs we have also seen while exploring rq1 that structural logic bugs are the second major type of bugs.
this happens due to wrong logical organization of the model hidden layers using wrong codes etc.
these kind of problems can be solved by some automated model and parameter recommendation tools.
how to develop these kind of tools need further research.
a methodology could be to mine large scale open source code repositories using python dataset and identify the common code patterns and suggest examples from common code patterns.
related works the closest related work is by zhang et al.
who have investigated bugs from deep learning applications built on top of tensorflow.
they collected stack overflow posts and filtered them to study posts and selected github projects to include commits with bugs using keywords e.g.
bug fix wrong etc.
in contrast we studied a cross section of five deep learning libraries with different design constraints using bugs from github and 415stack overflow posts that allowed us to draw interlibrary observations.
zhang et al.
have studied the bugs and have categorized them into types of bug root causes and types of impacts symptoms.
our work expanded the study to include bug types from literature and categorizes the bugs into bug types root causes and impacts.
in term of results our study both confirms what was known as a small scale and produces new knowledge e.g.
correlating antipatterns with bugs .thung et al.
studied three machine learning systems apache mahout lucene and opennlp and manually categorize the bugs into different categories.
they focused on bug frequencies bug types severity of the bug bug fixing duration bug fixing effort and bug impact.
different from them we focus on bug types bug root causes and bug impact of five deep learning libraries which are tensorflow keras torch caffe and theano.
there are some empirical studies focused on specific types of bugs.
lu et.
al.
studied real world concurrency bug characteristics.
gao et.
al.
conducted an empirical study on recovery bugs in large scale distributed systems.
api changes problems was studied by .
our work focuses on the bugs in the usage of deep learning libraries.
other prior work that have studied stack overflow posts e.g.
have not focused on deep learning software.
conclusion and future work although deep learning has gained much popularity and strong developer community in recent years developing software using existing deep learning libraries can be error prone.
in this paper we have presented an empirical study to explore the bugs in software using deep learning libraries.
in our study we have studied qualified stack overflow posts and 500github bug fix commits to identify the bug types root causes of bugs effects of bugs in usage of deep learning.
we have also performed an inter stage analysis to identify the stages of deep learning pipeline that are more vulnerable to bugs.
we have also studied the buggy codes in stack overflow to find antipatterns leading to bugs to understand the strong correlation of the bug types in deep learning libraries.
our study found that data bug and logic bug are the most severe bug types in deep learning software appearing more than of the times.
major root causes of these bugs are incorrect model parameter ips and structural inefficiency si .
last but not least bugs in the usage of deep learning libraries are strongly correlated.
this work opens multiple avenues for exploration.
for instance while we have studied bugs we haven t yet examined the fix strategies that programmers use.
this study is also on a relatively modest dataset and could be repeated on a much larger dataset.
finally repair strategies could be developed for deep learning programs.
519a comprehensive study on deep learning bug characteristics esec fse august tallinn estonia