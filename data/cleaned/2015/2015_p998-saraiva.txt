produ cts developers and milestones how should i build my n gram language model juliana saraiva federal university of pernambuco recife brazil jags2 cinf.ufpe.br christian bird microsoft research redmond wa usa cbird microsoft.com thomas zimmermann microsoft research redmond wa usa tzimmer microsoft.com abstract recent work has shown that although programming languages enable source code to be rich and complex most code tends to be repetitive and predictable.
the use of natural language processing nlp techniques applied to source code such as n gram language models show great promise in areas such as code completion aiding impaired developers and code search.
in this paper we address three questions related to different methods of constructing language models in an industrial context .
specifically we ask do application specific but smaller language models perform better than language models across application s?
are developer specific language models effective and do they differ depending on what parts of the codebase a developer is working in?
finally do language models change over time i.e.
does a language model from early development model change later on in development?
the answers to these questions enable techniques that make use of programming language models in development to cho ose the model training corpus more effectively.
we evaluate these questions by building language models across developers time periods and application s within microsoft office and present the results in this paper.
we find that developer and applicat ion specific language models perform better than models from the entire codebase but that temporality has little to no effect on language model performance.
categories and subject descriptors d. .
coding tools and techniques gene ral terms measurement experimentation keywords n gram models natural language processing .
introduction in their work on the naturalness of software hindle et al.
showed that n gram based language models perform quite well when used in the software engi neering domain on source code .
a language model assigns a probability to a sequence of words n grams the probability is typically learned from a training corpus.
in recent years these language models trained on source code corpora have been leveraged to aid in a wealth of tasks in software engineering including code completion detecting and enfor cing team coding conventions generating comments suggesting accurate names of program entities improving error messages and migr ating code between languages .
a language model assign s probabilities to sequences of token also called n grams based on frequencies of the sequences in the training corpus.
these probabilities can then be used to help developers in common programming tasks.
a simple example is code completion e.g.
after e ncountering the sequence for int i i n a tool would automatically suggest the suffix i because it is the most frequent suffix for such code .
when training language models on source code one faces two competing forces specificity.
language models can only provide help in source code that is similar to source code that it has seen before.
thus the data sparsity problem i.e.
the need to see instances of many code contexts drives the use of larger and larger corpora to train the model.
generality.
on the other hand the more disparate code bases are used the less specific the model is and the less nuanced the help that it can provide.
put concretely training a model on apache lucene will lead to a model that has lucene specific knowle dge but the model may not have suggestions or help for code contexts outside of the text search domain.
in contrast training a model on all of the code on github will lead to models that contain general knowledge of programming for virtually every api code construct or pattern but will not be specific to any particular application.
given these tradeoffs practitioners hoping to use language models in their own work are faced with the question how should i train my language model?
in this paper we shed light on this question by sharing our experience on building language models on several different slices of the same codebase and comparing the results.
specifically we examine the code of microsoft office hereafter referred to as office .
office is a prime subject for such a study because the code is large i.e.
tens of millions of lines of code and can be partitioned along a number of dimensions.
office is a suite of office productivity software applications including a word processor word a spreadsheet application excel and a presentation creator powerpoint .
thus we can naturally divide the code by application and train application specific language models .
office is developed by thousands of full time developers which allows us to partition the changes by the individual s and train developer specific models .
finally office is developed in development milestones allowing us to train language models on the changes that are made in certain periods i.e.
time specific models .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
copyright is held by the owner author s .
publication rights licensed to acm.
esec fse august september bergamo italy acm.
... .
998we use office to answer the following three research questions.
the answers will help developers make informed decisions about how to train language models for their software engineering tools rq1 does a smaller application specific language model perform better than a language model built from multiple applications?
rq2 does a programmer generate the same patterns n grams regardless of where he she is working?
rq3 does a model built from changes over the last milestone perform as well as one traine d over the whole history of changes i.e.
is there a temporal relationship to the language models ?
.
metho d we examined the c code and changes in office since that was the last full release and development cycle for the office codebase.
we used the roslyn api to extract the lexical tokens from the c code we did not include comments in the analysis.
we used grams also called trigrams because the majority of the works related to nlp used grams and or grams and when n grams models are appl ied to source code cross entropy saturates around and grams .
we implemented the language model generation and evaluation ourselves in c and r. application specific models.
we built four language models based on the n grams of tokens taken from the source code of excel word powerpoint and the entire office source code.
developer specific models.
we selected the five most active developers in office d1...d5 .
each developer s language model was built considering all their respective source code changes .
for each change made by the developer we generated two multi sets of ngrams with their frequencie s i the n gram s in the version of the file prior to the change and ii the n grams in the version after the change.
the n grams used to generate the developer specific language model are the multi set difference between the after n grams and the before n grams.
if an n gram occurred in the original file twice and in the modified file five times then we would use add three occurrences of the n gram to the language model for that developer if an n gram occurs the same amount or lower in the changed file the count for that n gram is .
the n grams that were added as a result of a developer s change s allow us to build a language model based on implementation pattern s by developers .
time specific models.
we use a similar technique for extracting n grams when building language models for different time periods e.g.
the last milestone of the office development.
for each change in the milestone period we extract the n grams from the file both before and after the change and use the added n grams to build language model for the last milestone.
in summary we built a total of distinct language models .
general model for a ll code in office application specific models all code in i word ii excel and iii powerpoint developer specific models i.e.
models f or each of the most active developers d1...d5 changes by the developer in i office ii word iii excel and iv powerpoint time specific models changes in the last milestone of i office ii word iii excel and i v powerpoint .
language model quality evaluation to evaluate language models we split each corpus into two halves a training corpus and a test corpus.
it is important to highlight that for our test data we chose files and in cases of changes changes to those files distinct from those used to train the language models.
to evaluate the quality of language models we use cross entropy the standard measure of language model quality which measures how surprising a te st corpus is to a language model built from a training corpus.
lower values indicate a better model .
the formula to compute cross entropy h is shown below.
an n gram in the testing corpus is represented by the tokens a1...an.
m represents the language model and pm is the probability of encountering the token ai after encountering tokens a1...ai .
hm s n logpm ai a1 ai n the cross entropy calculation depends on the probability of the occurrence of a certain token given a previous sequence of tokens.
however there are some cases where the probability of the occurrence of a particular token following a given sequence is for a trained language model.
this occurs when an n gram that occurs in the testing corpus doe s not occur in the training corpus which is not uncommon given that one source file may contain identifiers such as names of local variables or private methods that do not occur in any other file .
as the cross entropy measurement is based on a log funct ion and the log of tends to negative infinity we use smoothing techniques which attempt to estimate the likelihood of encountering a particular n gram even if it has not been seen before to avoid these situ ations.
we used the additive smoothing technique because prior studies have found that it works well and it is used frequently in practice.
for each research question we computed different groups of cross entropies and compared their values with others to determine if certain models perform better than others.
in this section we present the research hypotheses that we evaluate to answer each of our research questions.
rq1 the goal is to determine if a general language model generated from all of the c code in office performs well for each of the individual application s excel word powerpoint or if application specific language models are better in terms of cross entropy .
the common wisdom is tha t general model s which are based on larger data sets perform better observed by hindle et al .
.
however application specific models may be more effective in capturing application specific programming idioms or api.
we therefore trained four models a general model for all of office and application specific models for word excel and powerpoint.
we then computed the cross entropy of these models with the test sets for word excel and powerpoint again note that there is no overlap in the trainin g and tests sets .
rq2 the goal is to determine if developers write code differently in different parts application s of the code base.
this answers the question whether a single language model for a developer is sufficient e.g.
for code completion and whether context specific models should be built for developers e.g.
one language model for each application that a developer is working on.
from the office codebase we identified developers who work ed on all three applications in the same development cycle and selected the five most active developers based on the number of changes that they made in each application for our analysis .
rq3 this research question asks if we can represent all of the changes across an enti re development cycle with a language model created from the changes from just one milestone.
put more simply is the language model for a n application time independent?
to answer this question we built models using only changes from the last milestone in the development cycle and compare with models built from the changes during the entire development cycle .
to compare 999the accuracy the models we use a test corpora composed of changes from the entire development cycle of entire office excel word and powerpoint.
.
results the main goal of this research is to understand what factors have an effect on the quality of a language model.
by understanding the answers to these questions we can generate high quality language models to aid and improve development activities through te chniques such as code completion anomaly detection and assis tance of disabled developers .
we now present the results of our analysis in an attempt to provide evidence to answer our research questions rq1 does a n application specific language model perform better than a language model across all of microsoft office?
the excel word and powerpoint application s were analyzed to answer this rq.
in each case we compared the quality of a n application specific language model to the general office wide language model.
in all cases the application specific model performed better than the general model .
the same testing corpora were used for each pair of cross entropies calculation.
figure shows the cross entropy values.
the dark bars represent the values obtained when using the application specific models and the light bars indicate values when using the general model .
in all three cases the cross entropies were lower when the application specific language model w as used on the test corpus.
powerpoint shows the least difference with a delta of .
in figure .
we conclude that in the case of the application s examined within office our answer to rq1 is yes a n application specific language model performs better than a model across the entire codebase .
rq2 does a programmer generate the same patterns and idioms n grams regardless of where he she is working?
five developers who all made changes to word excel and powerpoint between january 1st an d july 31st were analyzed in this study.
table shows the cross entropies results.
the first column indicates which language model lm was used on the testing data.
the second third and fourth columns represent the cross entropies results found f or excel word and powerpoint respectively.
observing table we can confirm once again that the language model generated specifically from a n application performed better than a general language model that is a general language model for a specific developer i.e.
the cross entropy values are lower.
the better language model of the two is indicated in bold for each pair.
we note that for each developer the application specific model for that developer performs better than the general office wide model for that developer when evaluated on each of the application s. after testing that the data was dis tributed normally using shapiro wilk normality tests we used a paired t test.
the t test showed that there was a statistically significant improv ement for the application specific models for individual developers p .
.
we therefore conclude that for the five most active developers in office our answer to rq2 is no individuals developers use slightly different patterns and write less pred ictable code across application s .
the implication of this result is that when building developer specific models it is better to use less data per model and build multiple context specific models per developer than to build one large model per developer.
rq3 is there a temporal relationship to the language models?
does a model built from changes over the last milestone perform as well as one trained over the whole history of changes?
the intention of this research question was to determine if a language model drawn from a shorter period of time performs well across all of the changes in an entire development cycle.
this has implications on how frequently language models need to be updated as well as the resources needed to build these language models da ta from a full development cycle requires more resources to generate a language model than data from one milestone .
figure shows the cross entropies for excel word powerpoint and the general office language models for the last milestone and the enti re figure .
cross entropy results for models generated from the last mile stone and for the entire dev cycle r3.
figure .
cross entropy results by application rq1.
.
.
.
.
.
.
excel word powerpointcross entropies for rq1 project specific lm office wide lmtable .
cross entropy results for developers by application and a cross all applications rq2 excel word power using application s lm for d1 .
.
.
using general lm for d1 .
.
.
using application s lm for d2 .
.
.
using general lm for d2 .
.
.
using application s lm for d3 .
.
.
using general lm for d3 .
.
.
using application s lm for d4 .
.
.
using general lm for d4 .
.
.
using application s lm for d5 .
.
.
using general lm for d5 .
.
.
1000development cycle evaluated against a test corpus for each application drawn from the entire development cycle.
the dark bars represent the cross entropies resulting of the whole application s language model and the light gray bars indicate the cross entropies of the last milestone s language model.
analyzing figure in three of the four cases there is only a small difference in the cross entropies with the last milestone model actually performing better.
however for word the language model built from the entire development cycle performed much better than the last milestone language model.
we thus conclude that for rq3 for most but not all cases a language model generated from the last milestone performs as well as a language model generated from the entire development cycle .
the answer to this question indicates that it may be a fruitful direction for further research.
we are unaware of any other research on the effects of t emporality factors on language model quality.
.
threats to valitity our work is subject to many of the usual threats to validity in a n industrial software engineering empirical study.
the primary threat in such studies is of external validity wh ile office is a large product suite it is still just one software project thus it is unclear how generalizable our results are .
nonetheless we believe th at this study provides some insight for industrial software projects considering using language models to aid i n various development tasks.
we encourage others to investigate similar ecosystems of projects such as gnome kde the apache family of projects or in proprietary codebases .
.
related work we are unaware of work that evaluates how to select the training corpus for language models.
hindle et al.
built language models from java and c codebases and evaluated cross entropy for each corpus and between corpora but did not investigate different sets of documents along dim ensions such as time or developer.
the applications of language models in software engineering are varied.
the most common is code completion as demonstrated by raychev et al.
and franks et al .
however allamanis at al.
have shown that language models can be used to detecting and enforcing team coding conventions as well as suggesting accura te names of program entities such as variables methods and classes .
hellendoorn showed that language models can be used to determine how well a code contribution to a project matches the project s coding style and thus can indicate if a contribution will be rejected and need further work .
language models can also be used to improve natural text that is tightly coupled to code.
for example movshovitz attias and cohen demonstrated how to use lms to generat e code comments and campbell et al.
showed they could improv e error reporting .
our work is orthogonal to the appli cations of language models as this study is focused on how to select the corpus of source code to train a language model before using it in various tasks.
.
conclusion in this work we have investigated how different aspects of language model generation affe ct their quality.
we used the standard metric of cross entropy for evaluating various language models.
we found that the more specific a language model is the better its performance even when models are tailored to specific developers and less data to t rain a model is available .
in contrast we found that in many cases the temporality of the models has little impact .
we recommend that language models to improve development tasks such as code completion should consider the context such as the applicat ion or the developer .
this is an initial step into the realm of exploring the various methods of generating language models.
similar to defect prediction bug triage and other research problems we hope that others will build on these ideas and investiga te the effects of considering various attributes when building language models in an effort to build knowledge and guidelines for those applying n gram based language models to development tasks.
.