neuro symbolic program corrector for introductory programming assignments sahil bhatia netaji subhas institute of technology delhi india sahilbhatia.nsit gmail.compushmeet kohli google deepmind london uk pushmeet google.comrishabh singh microsoft research redmond usa risin microsoft.com abstract automaticcorrectionofprogramsisachallengingproblemwithnumerous real world applications in security verification and education.oneapplicationthatisbecomingincreasinglyimportantisthe correctionofstudentsubmissionsinonlinecoursesforproviding feedback.mostexistingprogramrepairtechniquesanalyzeabstract syntax trees asts of programs which are unfortunately unavailableforprogramswithsyntaxerrors.inthispaper weproposea novel neuro symbolic approach that combines neural networks with constraint based reasoning.
specifically our method first uses arecurrentneuralnetwork rnn toperformsyntaxrepairsfor thebuggyprograms subsequently theresultingsyntactically fixed programs are repaired using constraint based techniques to ensure functionalcorrectness.thernnsaretrainedusingacorpusofsyntacticallycorrectsubmissionsforagivenprogrammingassignment andarethenqueriedtofixsyntaxerrorsinanincorrectprogrammingsubmissionby replacingorinsertingthepredicted tokensat the error location.
we evaluate our technique on a dataset comprising of over student submissions with syntax errors.
our method is able to repair syntax errors in of submissions and finds functionally correct repairs for .
submissions.
ccs concepts computing methodologies neural networks software andits engineering functionality search based software engineering keywords neuralprogramcorrection automatedfeedbackgeneration neural guided search acm reference format sahil bhatia pushmeet kohli and rishabh singh.
.
neuro symbolic program corrector for introductory programming assignments.
in icse icse 40th international conference on software engineering may june gothenburg sweden.
acm newyork ny usa 11pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may june gothenburg sweden association for computing machinery.
acm isbn ... .
introduction theincreasingimportanceofcomputinghasresultedinadramatic growthinthenumberofstudentsinterestedinlearningprogramming.
initiatives such as edx and coursera attempt to meet this demand by providing massive open online courses moocs that areeasilyaccessibletostudentsworldwide.whilemoocshavenumerousbenefits theireffectivenessoverthetraditionalclassroom setting is limited by the challenge of providing quality feedback to studentsontheirsubmissionstoopen responseprogrammingassignments.
we present a learning based technique to automatically generate corrections for student submissions that in turn can be used to generate feedback.
most existing systems for automated feedback generation are based on the functional correctness and style characteristics of student programs.
for instance the autoprof system uses constraint basedsynthesistofindtheminimumnumberofchanges toanincorrectstudentsubmissionthatwouldtransformittobecome functionally equivalent to a reference teacher implementation.
incontrast thecodewebs system adoptsa searchbased approach where feedback generated by teachers on a handful ofsubmissions is propagated to provide feedback on thousands of submissionsbyqueryingthedatasetusingcodephrases.codewebs allows querying the dataset of student submissions using codephrases which are subgraphs of ast in the form of subtrees subforests and contexts.
these systems assume the availability ofprogramasts whichunfortunatelydonotexistforprograms with syntax errors.
as an example almost of submissions in a datasetweobtainedfromanintroductorypythoncourseonedx hadsyntaxerrors.inthispaper weproposeanovelneuro symbolic programcorrectionapproachthatovercomesthisproblembyusing a hybrid approach that combines deep neural networks with constraint based reasoning.
while the neural networks are able to correctsyntacticproblemswithstudentsubmissions theconstraintbased synthesis techniques allow for finding semantic corrections.
aparticularlyinterestingaspectenabledbyourhybridapproachis thatofgeneratingthecorrectusageofinfrequentvariablenames in a student program.
therearetwokeystepsinourapproach.foragivenprogramming problem we first use the set of student submissions without syntaxerrorstolearnamodeloftokensequences whichisthenused to hypothesize possible fixes to syntax errors in a student submission.oursystemenumeratesoverthesetofpossiblemodifications to the incorrect program in an ordered fashion to compute the syntax error fixes.
we use a recurrent neural network rnn to learn the token sequence model that can learn large contextual dependencies between tokens.
in the second step we use constraint based program synthesis the sketch solver acm ieee 40th international conference on software engineering authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden sahil bhatia pushmeet kohli and rishabh singh to find minimal changes to the student program such that it becomes functionally equivalent to a reference implementation.
note that the small size of student programs in this domain of introductoryprogramming around10 20loc allowsforperforming sophisticated search based correction techniques.
our approach is inspired from the recent pioneering work on learningprobabilisticmodelsofsourcecodefromalargerepository ofcodeformanydifferentapplications .hindleet al.
learnann gramlanguagemodeltocapturetherepetitivenesspresentinacodecorporaandshowthatn grammodelsare effectiveatcapturingthelocalregularities.theyusedthismodel for suggesting next tokens that was already quite effective as comparedtothetype basedstate of the artidesuggestions.nguyen et al.
enhanced this model for code auto completion to also include semantic knowledgeabout the tokens such as types and the scope and dependencies amongst the tokens to consider global relationshipsamongstthem.thenaturalizeframework learns an n gram language model for learning coding conventions and suggestingchangestoincreasethestylisticconsistencyofthecode.
more recently some other probabilistic models such as conditional randomfieldsandlogbilinearcontextmodelshavebeendeveloped for suggesting names for variables methods and classes .
we also learn a language model to encode the set of valid token sequences butourapproachdiffersfrompreviousapproachesin four key ways i our application of using a language model learnt from syntactically correct programs to fix syntax errors is novel ii since we cannot obtain the ast of the programs with syntaxerrors many of these techniques that use the ast information forlearningthe languagemodelarenotapplicable iii welearna recurrent neural network rnn that can capture more complex dependenciesbetweentokensthanthen gramorlogbilinearneural networks andfinallyiv insteadoflearningonelanguagemodel for the complete code corpus we learn individual rnn models for different programming assignments so that we can generate more precise repairs for different problems.
we evaluate our system on student solutions obtained from programming problems taken from the intro to programming class .00x offered on edx.
we consider student submissions in total out of which .
submissions have syntax errors.
our technique can find repairs for fixing syntax errors for .
ofthesesubmissionsandfindssemanticrepairs for23.
ofthesubmissions.tosummarize thepapermakes the following key contributions weformalizetheproblemofsyntaxcorrectionsinprograms as a token sequence prediction problem using the recurrent neural networks rnn .
we present the synfixalgorithm that uses the predicted token sequences for finding syntax repairs using a searchprocedure.
the algorithm then uses constraint based synthesis to find minimal repairs for semantic correctness.
we evaluate the effectiveness of our system on more than student submissions.
motivating examples inthissection wepresentasampleofdifferenttypesofcorrections oursystemisabletogenerateasshowninfigure2.theexampleprograms showninfigure2 comefromthestudentsubmissions fordifferentproblemstakenfromtheintroductiontopythonprogramming mooc .00x on edx.
our syntax correction algorithm considers two types of parsing errors in python programs i syntax errors and ii indentation errors.itusestheoffsetinformationprovidedbythepythoncompilertolocatethepotentialerrorlocations andthenusestheprogramto kensfromthebeginningofthefunctiontotheerrorlocationasthe prefixtokensequencetoquerythernnmodeltopredictthecorrecting token sequences.
the algorithm enumerates subsequences ofpredictedsequenceinarankedordertogenerateproposalsfor insertion or replacements at the error location.
however there are manycases suchas theones shownin figure2 c where thecompiler is not able to accurately locate the exact offset location for the syntax error.
in such cases our algorithm ignores the tokens present in the error line and considers the prefix ending at the previous line.
using the prefix token sequence the algorithm uses aneuralnetworktoperformthepredictionofnext ktokensthat are most likely to follow the prefix sequence which are then either inserted at the error location or are used to replace the original token sequence.
a sample of repairs generated by our algorithm emphasized in red basedoninsertingthepredictedtokensattheoffsetlocation isshowninfigure2 a .thekindsoferrorsinthisclasstypically include inserting unbalanced parenthesis completing partial expressions suchas exp toexp addingsyntactictokenssuchas afterifandelse etc.
some example syntax errors that require replacing the original tokens in the incorrect program with the predictedtokensareshowninfigure2 b .theseerrorstypically includereplacinganincorrectoperatorwithanotheroperator suchasreplacing with incomparisonswith deletingadditional mismatched parenthesis etc.
since the compiler might not always accurately identify the error location oursystem predicts the complete replacementline forsuchcasesasshowninfigure2 c .theseerrorstypicallyinclude wrongspellingofkeywords e.g.
retruninsteadof return wrong expression for the return statement etc.
for fixing such syntax errors ouralgorithmgeneratestheprefixtokensequencethatendsatthelasttokenofthelineprevioustotheerrorline.itthenqueries the model to predict a token sequence that ends at a new line and replacestheerrorlinecompletelywiththepredictedline.asample of indentation errors is shown in figure d and submissions that requiremultiplefixesareshowninfigure2 e .afterfixingasyntax error the algorithm iteratively calls itself to fix other errors.
thefixestosyntaxerrorsinthecodemayormaynotcorrespond to the correct semantic fix i.e.
the fixed program may not compute the desired result.
an end to end example of semantic repair is shown in figure .
since the python compiler does not predict the correct location for syntax error for this case the rnn model uses the previous line method to insert a new line while a t !
or b t !
0as the suggested fix.
this results in fixing the syntax error but introduces an unknown variable t. in the second phase our system usesanerrormodelconsistingofasetofrewriterulestomodify different program expressions and then uses the sketch solver to efficientlysearchoverthislargespacetocomputeminimalmodifications toprogram such thatit becomes functionallyequivalent withrespecttoateacher simplementation.forthisexample we authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
neuro symbolic program corrector for introductory programming assignments icse may june gothenburg sweden defgcditer a b m min a b whilea m!
orb m!
m m returnmdefgcditer a b m min a b whilea t!
orb t!
m m returnm def gcditer a b m min a m b b m a while a m b t a m b !
or b m a t a b m !
m m a b return mdef gcditer a b m min a b while a m!
or b m!
m m return m sketch figure an end to end correction example the rnn model first fixes the syntax error by replacing line but introduces an unknown variable t. the second phase applies an error model to introduce various replacement choices and uses sketchto find minimal repair for the input program to be functionally correct.
an expression of the form a b c denotes a choice expression where the sketch compiler can choose any expression amongst them.
the first option in the choice expression denotes the default expression with cost while all other options result in a cost of .
defgcdrecur a b ifb return a x y max a b min a b return gcdrecur x y y defrecpower b e ife return return b recpower b e a syntaxerror insert token defiswordguessed sword lguess result true forsinsword if!not sinlguess result false return resultdefrp b e t b if e return t else t b return t rp b e b syntaxerror replace token defrecpower b e fe i fe return b return b recpower b e defrecpower b e ife return b ife return e returnb r ec p o w e r b e c syntaxerror previous line insert defrecpower b e ife return return return b recpower b e defrecpower b e x b while e x b e return b d indentation error insert token defrecpower b e ife return e return b recpower b e defgcditer a b mi min a b while a mi!
orb mi!
mi return mi e multiple errors combination of insert replace previous line figure2 asampleofexamplesyntaxfixesgeneratedbythernnmodelforthestudentsubmissionswithsyntaxerrors.the fix suggestions are emphasized in red font whereas the expressions removed are emphasized in blue with a frame box.
useasimpleerrormodelforbrevitythatcanreplaceavariablein assignments and conditionals with any other variable and replace an operator with or .
sketch is then able to identify the correct semantic fix that replaces twithmiand changes to in line .
neuro symbolic repair algorithm anoverviewoftheworkflowofoursystemisshowninfigure3.
foragivenprogrammingproblem wefirstusethesetofallsyntactically correct submissions to learn a generative token sequence authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden sahil bhatia pushmeet kohli and rishabh singh synfix syntactically correct student submissions student submission with syntax errorscorrected programlearnt model sketcherror model figure an overview of the system workflow.
modelusinganrnn .thesequencemodelsareproblem specific and we learn different models for different problems.
we then use thesynfixalgorithmtofindsmallsyntacticcorrectionstoastudent submission by modifying the submission with the token sequences predicted by the learnt model.
finally we use the sketch synthesis system to find minimal modifications defined by rewrite rules to the syntactically repaired program such that the program becomes functionally correct with respect to a reference implementation.
we now briefly describe the three key phases in our workflow i thetrainingphase andii the synfixalgorithm and iii sketch based synthesis.
.
rnn based language model therecurrentneuralnetwork rnn isageneralizationoffeedforwardnetworksthatcanencodesequencesofvariablelengthand rnns have been shown to be effective in many domains such as machinetranslation andspeec hrecognition .wefirstuse the syntactically correct student submissions to obtain the set of all valid token sequences and then use a threshold frequency value to decide whether to relabel a token to a general identtoken for handlingrarelyusedtokens suchasinfrequentvariable method names .
a token is encoded into a fixed length one hot vector wherethesizeofthevectorisequaltothesizeofthetrainingvocabulary.
in the training phase we provide the token sequences to theinputlayerofthernnandtheinputtokensequenceshiftedleft by1asthetargettokensequencetotheoutputlayerasshownin figure4 a .afterlearningthenetworkfromthesetofsyntactically correct token sequences we use the model to predict next token sequencesgivenaprefixofthetokensequencetotheinputlayer asshowninfigure4 b .thefirstoutputtokenispredictedatthe output layer using the input prefix sequence.
for predicting thenext output token the predicted token is used as the next input token in the input layer as shown in the figure.
we first present a brief overview of the computational model of a simple rnn with a single hidden layer.
consider an input sequence oflength land anrnn with inumber of inputs a single hidden layer with hnumber of hidden units and koutput units.
letxt ridenote the input at time step t encoded as a vector ht rhdenotethehiddenstateattimestept w rh idenote the weight matrix corresponding to the weights on connections frominputlayertohiddenlayer v rh hbetheweightmatrix from hidden to hidden layer recursive and u ri hbe the weightmatrixfromhiddentotheoutputlayer.thecomputationmodel of the rnn can be defined as following ht f w xt v ht ot softmax u st the hidden state htat time step tis computed by applying a non linearfunction f e.g.tanhorsigmoid toaweightedsumof the input vector xtand the previous hidden state vector ht .
the output vector otis computed by applying the softmaxfunction to theweightedstatevectorvalueobtained stobtainedbyanaffine transformation u ht.
the hidden units take the weighted sum as input and map it to a value in the set using the sigmoid functiontomodelnon linearactivationrelationships.thernns can be trained using backpropagation through time bptt t o calculatethegradientandadjusttheweightsgivenasetofinput and output sequences.
.
the synfix algorithm wefirstpresentthe synfixone algorithmthatfixesthefirstsyntax errorinthestudentsubmission ifpossible andthenpresentthe synfix algorithm to fix multiple syntax errors.
fixing one syntax error thesynfixone algorithm shown in algorithm.
takes as input a program p with syntax errors and a token sequence model m and returns a program p primewith its first syntax error on the error location fixed if possible or denoting that the syntax error can not be fixed.
the algorithm first uses a parser to obtain the type of error errand the token location locwhere the first syntax error occurs and computes a prefix of the token sequence tildewidetprefcorresponding to the token sequence startingfromthe beginning oftheprogramuntiltheerrortokenlocationloc.
we use the notation a to denote a subsequence ofasequence astartingatindex i inclusive andendingatindex j exclusive .thealgorithmthenqueriesthemodel mtopredict the token sequence tildewidetkof a constant length kthat is most likely to follow the prefix token sequence.
afterobtainingthetokensequence tildewidetk thealgorithmsearches over token sequences tildewidetk of increasing lengths i k until either inserting or replacing the token sequence tildewidetk at theerrorlocationresultsinaprogram p primewithnosyntaxerrorat the error location.
if the algorithm cannot find a token sequencethat can fix the syntax error at loc the algorithm then creates another prefix tildewidetprev prefof the original token sequence such that it ignoresallprevioustokensinthesamelineasthatoftheerrortoken location.itthenpredictsanothertokensequence tildewidetprev kusingthe modelforthenewtokensequenceprefix andselectsasubsequence tildewidetprev k that ends at a new line token.
finally the algorithm checksifreplacingthelinecontainingtheerrorlocationwiththe predictedtokensequenceresultsinfixingthesyntaxerror.ifyes it returns the fixed program p prime.
example considerthepythonprogramshowninfigure5 a .
the python parser returns a syntax error in line with the error offset corresponding to the location of the token.
the synfix algorithm first constructs a prefix of the token sequence consisting of tokens from the start of the program to the error location such that tildewidetpref def recpower b e r n t if e .
it then queries the learnt model to predict the most likelytokensequencethatfollowstheinputprefixsequence.letus assumekis and the model returns the predicted token sequence authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
neuro symbolic program corrector for introductory programming assignments icse may june gothenburg sweden if exp exp r n r n t input layerhidden layeroutput layer if exp r n t input tokenspredicted tokens a training phase b prediction phase figure4 themodelingofoursyntaxrepairproblemusinganrnnwith1hiddenlayer.
a weprovideinputandoutputtoken sequences in the training phase to learn the weight matrices.
b in the prediction phase we provide a token sequence to the input layer of the rnn and generate the output token sequences using the learnt model.
algorithm synfixone input buggy program p token sequence model m err loc parse p tildewidet tokenize p tildewidetpref tildewidet tildewidetk predict m tildewidetpref fori range k do p prime ins insert p loc tildewidetk iffixed p prime ins loc returnp prime ins p prime repl replace p loc tildewidetk iffixed p prime repl loc returnp prime repl end for tildewidetprev pref tildewidet tildewidetprev k predict m tildewidetprevpref p primeprev replaceline p line loc tildewidetprev k iffixed p primeprev loc returnp primeprev else return algorithm synfix input buggy program p sequence model m max fixesf fori range f do p prime synfixone p m ifp prime return ifparse p prime returnp prime elsep p prime end for return tildewidetk .thealgorithmfirsttriestousethesmallerprefixes of the predicted token sequence in this case t os e ei f the syntax error can be fixed.
it first tries to insert the predicted token sequence in the original program but that results in the expression ife whichstillresultsinanerror.itthentries to replace the original token sequence with the predicted token sequence which results in the expression i fe that passes the parser check and returns the fixed program.
fixing multiplesyntax errors thesynfixalgorithm shown in algorithm.
takes as input the buggy student submission p the tokensequencemodel m andaparameter fdenotingthemaximum number of syntax corrections considered by the algorithm and returns either the fixed program p primeobtained using fewer than fnumber ofcorrectionsor otherwise.the algorithmiteratively callsthesynfixone amaximumof ftimes tofixonesyntaxerror inpat a time.
some example programs requiring multiple fixes are shown in figure e .
.
constraint based semantic repair afterobtainingasyntacticallycorrectprogramusingthe synfix algorithm our system uses a technique similar to that of autoprof to findminimal transformations to thestudent program such that it becomes functionally equivalent with respect to a refer ence implementation.
the class of transformations are definedusing a generic error model comprising of five different typesof transformations r rewrite rules comparison operators cop !
i.e.
a comparison operator can be modified to any of the operators in the right hand set arithmeticoperators aop variablereplacement v ?von rhs of assignments and expressions where ?
vdenotes the set of all variables live at the program location off by one errors a a a and5 constantmodification n ?
?
where?
?denotes any integer constant value.
for example the first rewrite rule states that any comparison operator in the student program can be potentially rewritten to any of the operator in the set !
.
since this results in a huge search spaceofprogrammodification weuseaconstraint basedsynthesis authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden sahil bhatia pushmeet kohli and rishabh singh solversketchtosolveforminimalnumberofmodificationssuch that the student program becomes functionally correct as follows.
p prime in p prime rewrite p r pt in p prime in min cost p p prime the above constraint requires sketch to find a program p primein the space of programs obtained from the rewrite rules rsuch thatp primeis functionally equivalent with respect to the reference implementation ptandp primerequiresminimalnumberofapplications of the rewrite rules.
more details about how sketch solves the above constraint using the cegis algorithm can be found in .
consider the program shown in figure b which requires corrections.theprogramusesanundefinedvariable tandusesthe wrongcomparisonexpressions a t !
1insteadof a mi !
.given thisprogram sketchusesthegenericerrormodeltoidentifythe correct loop comparison expression a mi !
or b mi !
which results in a functionally correct program.
evaluation we evaluate our system on python submissions obtained fromtheintrotoprogramminginpythoncourse .00x ontheedx mooc platform.
our benchmark set consists of student submissionsfor9programmingproblems whichasksstudentstowriteiterative or recursive functions over integer string and list data types.thenumberofsubmissionsforeachprobleminourevaluationsetisshownintable1andasignificantfractionofstudent submissions have syntax errors .
.
using the evaluation we aimtoanswerthefollowingresearchquestions howwellour algorithm is able to repair syntax and semantic errors?
what algorithmicchoicesarechoseninthe synfixalgorithmforfixing the errors?
how do problem specific models compare with general models?
and finally how does the rnn language model compares with an n gram baseline model.
.
benchmarks our benchmark set consists of student submissions for programmingproblems recurpower iterpower oddtuples evalpoly compderiv gcdrecur hangman1 hangman2 and gcditertakenfrom theedxcourse.theseproblemsaskstudentstowriteiterativeor recursive functions over integer string and list data types and requires students to use different language constructs such as conditionals nested loops functions list comprehensionsetcto solve the problems.
the recurpower problem asks students to write a recursivefunctionthattakesasinputanumber baseandaninteger exp andcomputesthevalue baseexp.theiterpower problemhas the same functional specification as the recurpower problem but asks studentsto writean itervative solutioninstead ofa recursive solution.
the oddtuples problem asks students to write a function that takes as input a tuple land returns another tuple that consists of every other element of l. theevalpolyproblem asks students to writeafunctiontocomputethevalueofapolynomialonagiven point where the coefficients of the polynomial are represented using a list of doubles.
finally the compderiv problem asks students to write a function to return a polynomial represented as a list thatcorrespondstothederivativeofagivenpolynomial.thedistri butionofthelinesofcode loc fordifferentbenchmarkproblemsis shown infigure .the overallmean is7.
locwith astandard deviation of .
.
the number of student submissions for each problem in our evaluationsetisshownintable1.intotalourevaluationsetconsists of student submissions.
the number of submissions for the evalpoly andcompderiv problems are relatively fewer than the numberofsubmissionsfortheotherproblems.thisisbecausethese problems were still in the submission stage at the time the data snapshotwasobtainedfromtheedxplatform.butthisalsogives us an opportunity to evaluate how well our technique performs when we have thousands of correct attempts in the training phase ascomparedtoonlyafewhundredattempts.anotherinteresting facttoobservefromthetableisthatasignificantfractionofstudent submissions have syntax errors .
.
trainingphase thetokensequencemodelforagivenproblem is learnt over all the problem submissions without syntax errors.
thesubmissionsarefirsttokenizedintoasetofsequenceoftokens withtokensoccurringbelowathresholdrenamedas ident which are then fed into the rnn.
we used a learning rate of .
a sequence length of and a batch size of .
we use the batchgradient descent method with rmsprop decay rate .
to learn theweights wherethegradientswereclippedatathresholdof1.
weusernnwithlstmcellsconsistingof2hiddenlayerseachwith hidden units and train the model for epochs.
we also varied thenumberofhiddenlayers thehiddenunits and the type of rnn cells rnn lstm but did not observe significant changes in correction accuracy.
.
number of corrected submissions theoverallresultsofoursystemisshownintable1.the synfix algorithmisabletogeneraterepairstofixthesyntaxerrorsin8689 .
programs.
amongst these repaired programs .
of the programs are also semantically correct i.e.
the repaired programsexhibitthedesiredfunctionalbehavior.usingthesketch systemwithagenericerrormodel oursystemisabletogenerate semantically correct repairs for .
of the submissions.
the average time taken by the synfixto repair syntax errors is .15sand by sketch to generate semantic repair is .7s.
we observe that even with relatively fewer number of total attempts for the evalpoly andcompderiv problems the system isabletorepairasignificantnumberofsyntaxerrors .
and .
respectively .
the average number of tokens per problem is whereas the average vocabulary size for training rnns is only .
this implies that there are a large number of common identifiernamessharedacrosslargenumberofstudentsubmissions.
fortheresultsshowninthetable weuseavocabularythreshold value oft .
additionally a large fraction of syntax errors can befixedusingonly1repair buttherearestillasignificant number of submissions that are repaired by considering additional changes.
anotherinterestingobservationisthatthedifferencebetween vocabulary size number of unique tokens and the training vocabulary obtained after removing tokens below the threshold is not verylarge implyingthattherearemanypopularidentifiernames thataresharedacrosssubmissionsthatthelanguagemodelcanuse for prediction.
finally the number of semantic corrections is the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
neuro symbolic program corrector for introductory programming assignments icse may june gothenburg sweden defrecpower b e ife return else return b recpower b e defgcditer a b mi min a b while a t!
orb t!
mi return mi a b figure a a submission with syntax error in line instead of b a syntactically correct but semantically incorrect program generated by synfix algorithm.
problem total syntax errors total training syntax semantic rnn sketch attempts percentage tokens vocab fixed fixed semantic fix recurpower .
.
.
.
iterpower .
.
.
.
oddtuples .
.
.
.
evalpoly .
.
.
.
compderiv .
.
.
.
gcdrecur .
.
.
.
hangman1 .
.
.
.
hangman2 .
.
.
.
gcditer .
.
.
.
total .
.
.
.
table the overall results of the synfix algorithm on benchmark problems.
problemincorrectsyntactically fixed num.
of fixes offset offset 1prevline f 1f 2attempts insert replace insert replace recurpower iterpower oddtuples evalpoly compderiv gcditer hangman1 hangman2 gcdrecur total table the detailed breakdown of the algorithmic choices taken by synfix.
figure6 thedistributionoflinesofcode loc fordifferent benchmark problems.highest for recursive problems as compared to other problems that ask for imperative solutions.
adetailedbreakdownofthechoicestakenbythe synfixalgorithm is shown in table .
the table reports the number of cases forwhichthesyntaxerrorswerefixedbythepredictedtokensequences using different algorithmic choices i offset the prefix token sequence is constructed from the start of the program to the error token reported by the parser ii offset the prefix token sequence is constructed upto one token before the error token iii prevline the prefix token sequence is constructed upto the previouslineandtheerrorlineisreplacedbythepredictedtoken sequence iv insert thepredictedtokensequenceisinsertedat the offset location and v replace the original tokens starting at authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden sahil bhatia pushmeet kohli and rishabh singh recurpower iterpower oddtuples evalpoly com pderiv gcdrecur hangman1 hangman2 gcditerpercentage of submissions fixedproblem specific vs general models specific general figure the performance of problem specific models vs general models trained on all the problems.
the offset location are replaced by the predicted token sequence.
the table also reports the number of repairs that require and changes.
we can observe that there is no one single choice that worksbetterthaneveryotherchoice.thismotivatestheneedofthe synfixalgorithm that tries all of the different algorithmic choices in the order of first finding an insertion or a replacement fix us ing the predicted token sequences of increasing length and thenusing the previous line method.
we use this ranking order over thechoicestopreferrepairsthatresultinsmallerchangestothe original program.
first wecanobservethatgeneratingtheprefixtokensequences for querying the language model that end at one token earlier than the error token offset performs significantly better than using prefixsequencesthatendattheerrortoken offset .second the prevline choiceperformsthebestcomparedtootherchoices.the reason for this is that the algorithm has more freedom to make largerchangestotheoriginalprogrambycompletelyreplacingthe error line but it may sometimesalso lead toundesirable semantic changes.
the previous line changes are explored only after trying out the insertion replace choices in the synfixalgorithm.
the replacement of original tokens with the predicted token sequences performsalittlebetterthantheinsertionchoice.finally weobserve that there are many student submissions that are fixed uniquely by each one of the choices and the algorithm therefore needs to consider all of the choices.
we also report the number of submissions that need and changes.
a large fraction of submissions can be fixed using only repair buttherearestillasignificantnumberofsubmissions that are repaired by considering additional changes.
we observeasmallinsignificantincreaseinthenumberofadditional repaired programs when considering f number of changes.
.
general vs problem specific models wenowpresentanexperimenttoevaluatewhetherweneedtotrain separate token sequence models per problem or one global general modelcanperformequallywell.forevaluatingthisquestion we perform two sets of experiments.
for the first experiment we train a general model using the set of all correct submissions for all the benchmark problems and compare its performance with that of problem specificmodelsthataretrainedindividuallyonlyonthe correctsolutionsforagivenproblem.forthesecondexperiment wetrainamodelonlyonthecorrectsubmissionsforaparticularbench markproblem recurpower tocorrecttheremainingproblems andcompare its performance with the problem specific models.
thesecond experiment is performed to evaluate how well the tokenmodels learnt from one problem with a large number of diverse solutions generalizetootherproblems.thecomparisonsareperformedbased onthe numberof submissionsthat aresyntactically fixed with one f fix.
theresultsforthefirstexperimentareshowninfigure7.overall the general model performs comparable to the problem specificmodels.
in total the general model fixes syntax errors for studentsubmissionscomparedto6966fixedbytheproblem specific models.
moreover for two problems gcdrecur andgcditer the general model performs a little better than the problem specific models.thisresultshowsthatitmightnotbenecessarytomaintain and train separate models per problem and a global general model trained on all the problems together can work almost as well in most of the cases.
for investigating how well the token sequence models learnt fromoneproblem generalizetoanotherproblem we performthe secondexperiment.wetrainthernnmodelonthesyntactically correctsubmissions forthe recurpower problem anduseit toperform corrections on submissions to other problems.
the results for this experiment are shown in figure a .
we can observe that the problem specificmodelconsistentlyfixesmorenumberofsyntax errors as compared to the recurpower language model.
in total the recurpower model fixes submissions whereas the problemspecific models repair about more number of submissions .inaddition wealsoempiricallyobservethatthefixesgenerated by problem specific models resulted in significantly higher semantically correct fixes in comparison to the fixes generated by therecurpower model.thisresultshowsthatforthernnmodelto perform well on a larger number of submissions to a new problem itneedstobetrainedonatleastsomecorrectprogramsfromthe new problem.
.
comparison with n gram baseline wecomparetheeffectivenessofusinganrnnmodeltolearntoken sequences over using an n gram language model with n .
the gram model first queries the model with a prefix of tokens and returns the next token if one exists.
otherwise it falls back to 3size token sequence and so on until it finds one in the frequency dictionary.weperformthestandardadd 1smoothingon5 gram models.
the results are shown in figure b .
the rnn modelsignificantly outperforms the n gram model consistently across allproblems.then grammodelintotalcanfixsyntaxerrorsfor problems .
whereas the rnn language model can fix errorsfor8689problems .
.thernnlanguagemodelisable to capture and generalize long term dependencies between tokens as compared to a small context learnt by the n gram model.
related work in this section we describe several related work on learning lan guagemodelsforbigcode automatedcodegradingapproaches and machine learning based grading techniques for other domains.
neural networks for syntax correction deepfix uses anattention basedseq2seqmodelforlearningatokenmodelfroma synthetic set of buggy programs.
the learnt model in deepfix first authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
neuro symbolic program corrector for introductory programming assignments icse may june gothenburg sweden iterpower oddtuples evalpoly compderiv gcdrecur hangman1 hangman2 gcditerpercentage of submissions fixedrecurpower model vs problem specific models problem specific recurpower recurpower iterpower oddtuples evalpoly com pderiv gcdrecur hangman1 hangman2 gcditerpercentage of submissions fixedrnn vs n gram baseline rnn n gram a b figure8 a theperformanceofproblem specificvsmodeltrainedonrecurpower.
b comparisonoftheperformanceofrnn token sequence model vs a gram baseline.
predictsthebuggylineintheprogramandreplacesthecomplete line with the statement predicted by the model.
sk p uses a skip gramneuralnetworkmodeltopredictaprogramstatement usingthelinesbeforeandaftertheerroneousline.itenumerates over all lines in the program and their potential replacements until findingoneprogramthatiscorrect.unlikethesetechniquesthat always replace the complete statement line in a student program our system uses an iterative algorithm that is able to generate fine grainedtoken levelfixestogeneratesmallrepairs whichare morelikelytocorrespondtostudent sintent.moreover oursystem uses constraint based synthesis to perform semantic repairs and complementthesyntacticrepairsfoundbythernntokenmodel.
finally the model in deepfix is trained on a synthetic dataset obtainedbymutatingcorrectstudentsubmissionsusinganerror model whereas our token sequence model is trained directly on the correct submissions.
the synthetic dataset generation not only requiresknowingsomeerrormodelupfront butalsotheconsidered error model might not correspond to the distribution of real world student errors.
language models for big code our work is inspired from the work on capturing naturalness in terms of syntactic patterns ofcodeinlargerepositories .unlikethesetechniques that analyze large code repositories our system analyzes student submissions that are typically much smaller and therefore also more amenable to more expensive analyses such as enumerative and constraint based synthesis for program correction.
hindle et al.
use an n gram model to capture the regularity of local project specific contexts in software and use it for next token suggestions.
nguyen et al.
extended this model to also include semantic token annotations that describe the token type and theirsemantic role.
allamanis et al.
showed that the n gram models can significantly increase their predictive capabilities when learnt on alarge corpus containing350 millionlines of code.naturalize is a language independent framework that uses the n gram languagemodeltolearnthecodingconventionfromaprojectcode baseandsuggestsrevisionssuchasidentifiernamesandformatting conventions to improve stylistic consistency.
allamaniset al.
recently presented a technique for suggesting method and class namesfromitsbodyandmethodsrespectivelyusinganeuralprobabilistic language model.
jsnice uses structured predictionwithcrfsforpredictingidentifiernamesandtypeannotationof variables in javascript programs.
ourtechniqueisinspiredfromthesepreviousworks butuses problem specific rnn language models to compute fixes to syntaxerrors in student submissions.
campbell et al.
presented a technique to use unnaturalness of code with syntax errors to locate the actual source of errors.
ray et al.
recently showed that even syntacticallycorrectbuggycodearetypicallyunnatural whichcan then be used to assist bug repair methods.
our system currently usesthepythoncompilertolocateerrorlocations butthesetechniquescanbeusedtocomplementandenhanceourtechniqueto increase the robustness of locating errors in submissions.
code grading andfeedback the problem of providing feedbackonprogrammingassignmentshasseenalotofrecentinterest.
these approaches can be categorized into two broad categories peer grading and automated grading techniques .
a recently proposed approach uses neural networks to simultaneously embed both the precondition and postcondition spacesofasetofprogramsintoafeaturespace whereprograms are considered as linear maps on this space.
the elements of the embedding matrix of a program are then used as code features for automatically propagating instructor feedback at scale .
however thesetechniquesrelyontheabilitytogeneratetheprogram ast and cannot repair programs with syntax errors.
machine learning for grading in other domains there have been similar automated grading systems developed for do mains other than programming such as mathematics and shortanswerquestions .themathematicallanguageprocessing mlp framework leverages solutions from large number of learners to evaluate correctness of student solutions to open response mathematical questions.
it first converts an open response toasetofnumericalfeatures whicharethenusedforclustering to uncover structures of correct partially correct and incorrect solutions.ateacherassignsagrade feedbacktoonesolutionina cluster whichisthenpropagatedtoallsolutionsinthecluster.basu etal.
presentanapproachtotrainasimilaritymetricbetween shortanswer responsesto unitedstatescitizenship exam which is then used to group the responses into clusters and subclusters for powergrading.
the main difference between our technique and these techniques is that we use rnns to learn a language model authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden sahil bhatia pushmeet kohli and rishabh singh fortokensequencesunlikemachinelearningbasedclusteringapproachesusedbythesetechniques.moreover wefocusongiving feedback on syntax errors whereas these techniques focus on semantic correctness of the student solutions.
automated program repair the research in automated program repair focuses on automatically modifying incorrect programssuchthatthemodifiedprogrammeetssomedesiredspecification .genprog usesanextendedformofgeneticprogramming to search for a source level patch by evolving a pro gram variant to fix some defects in the original buggy program whilemaintainingfunctionalitywithrespecttoasetoftestcases.
semfix usesacombinationofsymbolicexecution constraint solving andprogramsynthesistoautomaticallyderiverepairexpressions for a buggy program given a set of testcases.
it first uses statisticalfaultlocalizationtechniquestolocatetheerrorlocation and then uses layered component based synthesis techniques to generaterepairexpressionsofvaryingcomplexity.qlose also usesprogramsynthesistechniquestoperformautomatedrepairby optimizing a multi objective constraint of minimizing both syntacticandsemanticdistancesbetweentheoriginalbuggyprogramand the fixed program.
these systems focus on fixing semantic bugsin programs by encoding and analyzing program asts whereas oursystemfocusesonrepairingprogramswithsyntaxerrorsfor which program asts can not be obtained.
neural program synthesis there are some recent neural architectures developed for program synthesis that learn to search over a discrete space of programs .
unlike learning to search over programs from scratch we instead search over minimal modifications to student programs and use the prefix context in student submissions to guide the search more efficiently.
we believe our techniqueoflearningsequencemodelsovercorpusofrealprograms can complement program synthesis techniques as well.
limitations and future work our systemcurrently only usesthe prefixtoken sequence forpredicting the potential token sequences for syntax repair.
for the programshown infigure the synfixalgorithmsuggests thefix corresponding to the expression exp .
if the algorithm also took into account the token sequences following the error location such asreturn base then it could have predicted a better fix corresponding to the token sequence exp and could have produced a semanticallycorrectprogram.oursystemcurrentlyusessketchto perform such semantic edits but in future we would like to extend the rnns to also encode the suffix sequences to potentially generate more semantic repairs.
another limitation of our system is that wecurrentlydependonthepythoninterpretertoprovidetheerror locationforsyntaxerrors whichissufficientformostbutnotall cases.infuture wewouldliketotrainaseparateneuralmodelthatalso learnsto predict sucherror locationsin an end to endmanner.
there is another interesting research question on how to best translate the generated repairs into good pedagogical feedback.
somesyntaxerrorsaresimplytypossuchasmismatchedparenthesis operators forwhichthefeedbackgeneratedbyourtechnique shouldbesufficient.buttherearesyntaxerrorsthatpointtodeeperdefrecurpower base exp ifexp return ifexp return base return base recurpower base exp figure the synfix algorithm suggests the fix e x p 0using the prefix sequence.
misconceptionsinthestudent smind.someexamplesofsucherrorsincludeassigningtoreturnkeyworde.g.
return exp performinganassignmentinsideaparametervalueofafunctioncalle.g.
recurpower base exp etc.weplantobuildasystemontopof our technique that can first distinguish small errors from deeper misconceptionerrors andthentranslatethesuggestedrepairfix accordinglysothatstudentscanlearnthehigh levelconceptsfor correctly understanding the language syntax.
threats to validity a threat to internal validity is that the syntax error repairs generatedbythe synfixalgorithmmightnotbenaturalordesirable since the algorithm selects any repair that passes the compiler check.tomitigatethisissue weperformtwosteps.first weuse the sketch solver to compute semantic repairs that reduces thepotential chances to computing an undesirable syntax repair as itwilllikelyprecludethecorrespondingsemanticrepair.second we also randomly sampled syntactically fixed programs for assignments and manually checked that of the fixed pro grams corresponded to desirable syntax repairs.
another threatto internal validity is that we did not comprehensively evaluate alldifferentneuralnetworkconfigurationsandparametervalues due to compute resource constraint.
however we sampled someconfigurations from the space using a limited parameter sweep.
a threat to external validity of our results is that we have only evaluatedourframeworkonsmallpythonprogramsobtainedfrom anintroductorycourse whichmightnotberepresentativeofother programming courses taught in different languages.
conclusion we presented a technique to combine rnns with constraint based synthesis to repair programs with syntax errors.
for a programmingassignment ourtechniquefirstusesthesetofallsyntactically correct student submissions to train an rnn for learning the token sequence model and then usesthe trained model to predict token sequencesforfindingsyntaxrepairsforstudentsubmissions.itthen uses constraint based synthesis techniques to find minimal semanticrepairsbasedonasetofrewriterules.weshowtheeffectiveness of our systemon a large set ofstudent submissions obtained from edx.
we believe that this combination of rnns with constraintbasedsynthesiscanprovideabasisforprovidingeffectivefeedback on student programs with syntax errors.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
neuro symbolic program corrector for introductory programming assignments icse may june gothenburg sweden