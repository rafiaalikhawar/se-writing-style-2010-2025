evaluating non adequate test case reduction mohammad amin alipour1 august shi2 rahul gopinath1 darko marinov2 and alex groce1 1school of electrical engineering and computer science oregon state university usa 2department of computer science university of illinois at urbana champaign usa alipourm gopinathr agroce oregonstate.edu awshi2 marinov illinois.edu abstract given two test cases one larger and one smaller the smaller test case is preferred for many purposes.
a smaller test case usually runs faster is easier to understand and is more convenient for debugging.
however smaller test cases also tend to cover less code and detect fewer faults than larger test cases.
whereas traditional research focused on reducing test suites while preserving code coverage recent work has introduced the idea of reducing individual test cases rather than test suites while still preserving code coverage.
other recent work has proposed non adequately reducing test suites by not even preserving all the code coverage.
this paper empirically evaluates a new combination of these two ideas non adequate reduction of test cases which allows for a wide range of trade o s between test case size and fault detection.
our study introduces and evaluates c coverage reduction where a test case is reduced to retain at least c of its original coverage and n mutant reduction where a test case is reduced to kill at least nof the mutants it originally killed .
we evaluate the reduction trade o s with varying values ofc andnfor four real world c projects mozilla s spidermonkey javascript engine the yaffs2 ash le system grep and gzip.
the results show that it is possible to greatly reduce the size of many test cases while still preserving much of their fault detection capability.
ccs concepts software and its engineering !software testing and debugging keywords test reduction test adequacy coverage mutation testing .
introduction smaller test cases are in many ways preferable to larger test cases.
for example smaller test cases tend to run faster which can improve the e ciency of running test suites i.e.
sets of of individual test cases.
smaller simpler test cases are also easier to understand and enable more e ective debugging.
this was the initial motivation for deltadebugging a technique for reducing the size of failing test cases.
because of the advantages of smaller test cases random test generation is often combined with deltadebugging making research on e ective reduction techniques itself an important topic .
test suites with small test cases that focus on separate functional properties also make it possible for test case selection and prioritization to operate more e ectively than when applied to test suites mostly consisting of large complex test cases.
while smaller test cases have advantages it is also true that smaller test cases all else being equal detect fewer faults than larger test cases .
the trade o between size and e ectiveness for individual test cases is similar to the trade o between smaller and larger test suites .
researchers have extensively studied test suite reduction which removes entire test cases from test suites.
the problem of test suite reduction is to reduce a given test suite while preserving most of its fault detection capability.
various techniques have been proposed and many are summarized in a survey by yoo and harman .
testsuite reduction trades o reduced fault detection capability most often measured by the number of killed mutants for reduced test suite size typically measured by the number of test cases .
traditional techniques completely preserve some property of a test suite e.g.
its code coverage while removing test cases that are redundant and do not contribute to that property.
recently we evaluated non adequate testsuite reduction that only partially preserves the property of interest e.g.
preserves of code coverage.
the problem of test case reduction is to reduce an individual test case while preserving most of its fault detection capability.
reducing a test case essentially requires slicing and dicing the atomic parts that make the test case.
for example if a test case is a unit test composed of a sequence of function calls reduction usually involves removing function calls.
if a test case is de ned by an input le reduction can involve removing characters from the le.
note that measuring the sizeof a test case is inherently projectspeci c depending entirely on the semantics of test cases whereas the size of test suites can be de ned in a projectagnostic way as the number of test cases in the test suite though not perfectly correlated with the time to execute the tests .
while test suite reduction has been studied in depth at least since test case reduction research is much more recent.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
zeller and hildebrandt proposed delta debugging the best known test case reduction technique usually applied to reduce a failing test case to a minimal test case that still fails.
recently we proposed cause reduction as a generalization of delta debugging and used it to reduce a passing or failing test case while preserving its original coverage.
cause reduction completely preserves coverage the reduced test case has to cover all the code elements that the original test case covered and can potentially cover even more .
we call such reductions adequate because they preserve of some property.
note that adequate in our context refers to the relationship between the reduced and original test cases although the original test case itself may provide far from adequate code coverage.
the utility of non adequate reduction for test suites naturally suggests that non adequate reduction may be useful for test cases as well.
non adequate reduction either for test suites or test cases greatly enlarges the number of points to explore in trading o size and fault detection capability.
for test cases requiring adequacy limits how much size can be reduced some test cases cannot be reduced substantially without sacri cing at least some coverage or killed mutants and increases the time required to reduce test cases because searching for an adequate reduction is often harder than nding a good enough reduction .
combining the recent ideas of non adequate reduction for test suites and adequate reduction for test cases this paper empirically evaluates a new combination nonadequate reduction for test cases .
to the best of our knowledge ours is the rst such evaluation.
speci cally we evaluate c coverage reduction where a test case is reduced to retain at least c of its original coverage andn mutant reduction where a test case is reduced to kill at least a given set of nmutants it originally killed .
both reductions reduce a larger test case to a smaller test case while only partially preserving some property.
hence we call these reductions non adequate because they do not necessarily preserve completely either the code coverage or all mutants killed.
however the reduced test case could in theory cover code elements or kill mutants that the original test case does not even if the reduced test case does not cover all code elements or kill all mutants that the original test case did in fact the reduced test case can even cover more code or kill more mutants.
non adequate test case reduction further generalizes previously proposed test case reductions.
by parameterizing the level to which the reduced test case needs to preserve a property we allow more freedom to explore trade o s between size reductions and preservation of fault detection capability .
for example cause reduction becomes just a special case of our c coverage with c .
preserving to kill only one mutant that encodes some fault n mutant with n can mimic delta debugging.
at the other extreme setting nto equal the total number of all mutants originally killed results in a very strict test case reduction that preserves allmutants killed however such reduction may be prohibitively expensive to perform and would likely provide very little reduction unless test cases have excessive redundancy so our evaluation concerns only small values for n. we evaluate non adequate test case reduction on four realworld c projects mozilla s spidermonkey javascript engine the yaffs2 ash le system grep and gzip.
we usedmanual grep test cases and automatically generated test cases for the other projects.
we evaluate c coverage for various levels of c from to .
we evaluate nmutant with randomly selected mutants for various values ofnfrom to and mutants that are hard to kill based on the minimal mutant set .
we measure size reduction code coverage and mutants killed with the latter two1used as proxies for fault detection capability.
our results show that in many cases non adequate test case reduction can substantially reduce the size of the given test cases while still preserving considerable fault detection capability.
perhaps most interestingly when performing c coverage reduction the largest gain in size reduction for all cases comes when c changes from to the gain is typically twice as large as for any other c change.
this gain does notresult in a similarly large loss in mutation detection.
in brief simply giving up on perfection enables a larger reduction in size than the associated reduction in effectiveness.
additionally preserving even a small number n of mutants killed usually indirectly preserves a large fraction of all other mutants killed often more than .
this paper makes the following contributions novel test case reduction approach we de ne two types of non adequate test case reduction c coverage and n mutant reduction.
evaluation of reduction trade o s using four real world c projects we evaluate the relationship between the size reductions obtained with varying parameters for these reductions and the code coverage and killed mutants for reduced test cases relative to the original unreduced test cases.
.
non adequate test reduction we next describe our test case reductions in more detail.
we usetoto denote the original test case and trto denote the reduced test case.
we use tto denote an arbitrary test case cov t to denote the set of statements2covered by t mut t to denote the set of mutants killed by t jsjto denote the cardinality of the set s andsize t to denote the size of t. measuring the size of a test case is speci c to the project or the format of test cases section .
.
precisely de nes size for the projects used in our evaluation.
conceptually we de ne size as the number of atomic parts that a test case has.
the parameterized nature of parts is taken from the original delta debugging work .
in some projects parts are function calls in other projects they are lines or characters in a le string and in rare cases they may be much more complex e.g.
de ned by a grammar.
for example reduction of test cases that are computer programs e.g.
an input to a compiler often relies on a semantically involved notion of part.
the high level goal of test case reduction is to produce a reduced test case trwith size smaller than the size of to i.e.
size tr size to and ideally size tr size to such thattrstill retains either completely or partially some desirable property of to.
that is for some notion of quality 1although they are not ideal proxies code coverage is often used by developers to evaluate quality of test cases and both are commonly used to evaluate test cases in research.
2while we present and evaluate c coverage only for statement coverage it can generalize e.g.
to branch coverage.
17trhas similar quality to to.toitself may have good or bad quality but trshould not have much worse quality than to.
while in principle the reduction process can stop at various steps and in the limit even the original test case can be considered a reduced version of itself we are interested in so called minimal test cases where no single part of trcan be removed without losing some desired property.
.
reduction algorithm the test case reduction algorithm we use is derived from the original delta debugging algorithm and we modify it to support non adequate test case reduction.
deltadebugging takes as input a failing test case and reduces it by removing parts that are not relevant for the failure.
a generalized algorithm for cause reduction extends delta debugging to reduce a test case with respect to any property not just failure that can be detected when running the test case.
the most direct application of cause reduction is to completely preserve code coverage.
at a high level the delta debugging algorithm described in detail by zeller and hildebrandt and extended in the work on cause reduction iteratively splits a test case into multiple candidate test cases.
at each of these steps the algorithm checks if any candidate satis es the desired property which in traditional delta debugging is whether the test case fails .
if there is a satisfactory candidate it becomes the new base test case to be reduced further in the future steps.
if no candidate is satisfactory the granularity for splitting is increased until the algorithm determines that the test case is minimal removing any single part produces a test case that does not satisfy the property.
in this paper we further generalize delta debugging and cause reduction by allowing the candidate test case to only partially preserve some property.
.2c coverage reduction we relax the requirement from cause reduction that the reduced test case trpreserve all code coverage obtained by the original test case to with the requirement thattrpreserve at least c of coverage obtained by to.
reducing large test cases to preserve all statement coverage can be prohibitively expensive.
for example we previously reported that cause reduction of a single test case for the gcc compiler could take days .
moreover preserving of the coverage may not be necessary because a test case that preserves less may still have acceptable quality.
hence we propose c coverage reduction definition .c coverage test case reduction produces a reduced test case trthat covers at least c of the statements covered by the original test case to jcov tr cov to j jcov to j c note that the percentage is determined by the coverage of the original test case and notby coverage over allstatements in the code under test.
the property is notjcov tr j jcov to j c because trcould then end up covering statements unrelated to those covered by to.
coverage based cause reduction can be re de ned as c coverage with c jcov tr cov to j jcov to j or equivalently cov tr cov to .c coverage does not impose any requirements over statements notcovered by the original testcase the reduced test case may or may not cover those statements.
also c coverage does not directly require any relationship between jcov tr jandjcov to j so it can even happen that jcov tr j jcov to jiftrcovers some statements that todoes not cover.
.3n mutant reduction we de nen mutant reduction in a similar fashion but with three important di erences n mutant uses killed mutants instead of covered statements n mutant preserves the ability of a test case to kill an absolute number nof mutants rather than a relative ratio of mutants and n mutant considers the same set of selected mutants for all steps of the reduction algorithm definition .n mutant test case reduction produces a reduced test case trthat kills a speci c set of nmutants selected from the set of mut to where typically n jmut to j. the di erence from c coverage is largely motivated by the cost of determining the complete set of mutants killed for every candidate test case at each step of the reduction algorithm.
we did initially experiment with allowing the set of mutants to change while requiring only that the number of mutants be preserved through reduction steps be at least n. however by allowing the algorithm to only preserve at least any nmutants it can be necessary to run a large number of mutants at each step of the reduction algorithm until at least nmutants are killed or allmutants are run andnare not killed .
as a result the time to perform nonadequate test case reduction was often prohibitively long.
again we only require the selected nmutants to be a subset ofmut to .
mutants other than those in the selected set may or may not be killed by tr.
.
metrics we describe three metrics for evaluating the e ectiveness of test case reduction size reduction rate srr coverage preservation rate cpr and mutant killing preservation rate mpr .
we de ne all metrics such that higher values are better and values are normalized to the range .
.
size reduction rate srr the goal of test case reduction is to reduce the size of a test case.
as such it is important to measure how much smaller the reduced test case is compared to the original test case.
recall that size t denotes the size of a test case t i.e.
the number of the atomic parts that the test case has.
definition .for an original test case toand its reduced test casetr size reduction rate srr is srr to tr size to size tr size to a higher srr is desirable as it indicates that more parts have been removed from the test case resulting in a smaller reduced test case.
.
coverage preservation rate cpr our reduction is non adequate test case reduction so we need some metrics to measure how much fault detection capability the reduced test case loses compared to the original 18test case.
structural code coverage although not an ideal proxy for fault detection capability is commonly used to evaluate the quality of test cases the more code a test case covers the higher the chance it can detect a fault.
we therefore use statement coverage as one way to evaluate quality.
recall that cov t denotes the set of statements covered by a test case t. definition .for an original test case toand its reduced test casetr coverage preservation rate cpr measures the ratio between the number of statements covered by both trandtoand the number of statements covered by to cpr to tr jcov tr cov to j jcov to j a higher cpr is desirable as it indicates the reduced test case covers a larger subset of statements covered by the original test case.
note that while a reduced test case can potentially cover more statements than the original test case cpr is limited to as it considers only the statements covered by to.
.
mutant preservation rate mpr mpr is essentially the same as cpr except measured with respect to mutants killed not statements covered definition .for an original test case toand its reduced test casetr mutant preservation rate mpr measures the preservation of mutants killed by trrelative to the mutants killed byto mpr to tr jmut tr mut to j jmut to j a higher mpr is desirable as it indicates the reduced test case is better at killing mutants among those that the original test case kills.
like cpr mpr is relative to the original test and cannot exceed .
.
reduction requirements vs. metrics although both of the reduction algorithms and the metrics are based on coverage and mutants note that the requirements for reduction are notthe same as the metrics used to evaluate the reduced test cases.
therefore we cannot a priori tell how high or low the metrics will be for all reductions.
for c coverage reduction we know that cpr will be at least c but it could be much higher up to and mpr could in theory range from literally to .
forn mutant reduction we know that mpr will be at leastn jmut to j but it could be much higher in our experiments even when n jmut to j mpr can be quite high and cpr could range from almost to .
.
evaluation methodology we describe the projects test cases and mutants used in our evaluaton section .
and the experimental setup section .
.
our experiments ran on a high performance cluster of commodity computing nodes each node had .6ghz intel xeon cores.
.
projects table lists the projects used in our evaluation.
we tabulate the project name the number of non comment lines ofcode the number of test cases used in our evaluation what an atomic part is the total number of mutants used and the minimum and maximum number of mutants killed by each test case.
the last two columns are the number of tests in a randomly generated test pool for each project and the number of minimal mutants determined for each project these last two columns are metrics relevant for our analysis involving minimal mutants section .
.
.
we use four small to medium size c projects spidermonkey is mozilla s javascript engine yaffs2 is a popular ash le system used in the early android versions grep is the standard unix utility for searching les and gzip is the standard unix utility for compressing decompressing les.
.
.
test cases we use automatically generated test cases for spidermonkey yaffs2 and gzip and we use manually written test cases for grep.
the spidermonkey test cases are javascript programs randomly generated using the highly successful jsfunfuzz fuzzer.
the yaffs2 test cases are sequences of api calls to the le system randomly generated using a publicly available test generator for yaffs2 that has been used by several research projects on test generation .
the gzip test cases are les that have to random bytes.
for grep we use the manually written test cases obtained from sir each test case consists of command line arguments to grep.
how best to measure the sizeof a test case is an open question in software testing research.
researchers use a variety of metrics such as the number of api calls execution time or number of assertions.
as in our previous work we de ne size as the number of atomic parts of atest case.
the concrete part di ers from one project to another as summarized in table .
a part is a javascript code fragment in the generated program for spidermonkey one api call in the generated sequence of api calls for yaffs2 a character in the command line arguments for grep and simply one byte in the input le for gzip.
we limit the size of generated test cases to enable experiments to nish in a reasonable amount of time.
time complexity of the basic delta debugging algorithm is quadratic in the number of parts in a test case.
for spidermonkey yaffs2 and gzip we control the number of parts based on the speci c limits from our initial experiments trying to nish most test case reductions within minutes.
in particular we limit each spidermonkey test case to be exactly lines of javascript code each yaffs2 test case to be a sequence consisting of exactly api calls and each gzip test case to be a le consisting of at most bytes.
for grep we use allthe test cases manually written by others and we do not limit their sizes the largest test case forgrep has characters in the command line arguments.
.
.
mutants we use a mutation testing tool for c code developed by andrews et al.
and used in many previous studies.
quoting the tool provides the following four classes of mutation operators replace an integer constant iby i or i replace an arithmetic relational logical bit wise logical increment decrement or arithmetic assignment operator by another operator from the same class negate the decision in an if or while statement and delete a statement.
19table four projects used in our evaluation and some statistics of their test cases and mutants project ncloc test cases de nition of an atomic part mutants min killed max killed test pool minimal mutants spidermonkey a statement of javascript program yaffs2 one api call grep a character in command line arguments gzip a byte in the input le each mutant was compiled with gcc using the highest optimization o3and compared with other binaries to avoid trivially equivalent mutants .
about of the generated mutants were found to be trivially equivalent.
table shows the number of mutants for each project and the minimum and maximum number of mutants killed by each test case.
a mutant is considered killed if its output including stdout stderr and produced les di ers from the output of the original code.
.
.
minimal mutants to evaluate n mutant reduction we use two methods to select mutants.
the rst method is simple random sampling we select nmutants from the set of mutants killed by the test case.
in the second method we wanted to alleviate the impact of redundant and trivial mutants on the results.
redundant mutants are those mutants that are semantically equivalent to one another albeit syntactically different.
trivial mutants are those mutants that are killed by a majority of test cases.
the impact of these two kinds of mutants can be alleviated by using minimal mutant sets introduced by ammann et al.
.
a minimal mutant set is computed based on an original set of killed mutants and a test suite the rst step is to construct a minimal test suite from the original test suite i.e.
a subset of the original test suite that kills all the mutants killed by the original test suite.
removing any test case from the minimal test suite means failing to kill some mutant.
given a minimal test suite a minimal mutant set is the smallest subset of mutants from the original mutant set such that killing all the mutants from the minimal mutant set using the minimal test suite also kills all the mutants from the original set of killed mutants.
we generated minimal mutant sets for projects as follows rst we generated a large test pool of random test cases for each project.
we used these larger pools because minimal mutants require almost adequate test suites to ensure that useful mutants are not removed.
then we obtained the complete set of mutants killed by each test pool.
we minimized each test pool with respect to its corresponding project s set of mutants to obtain the minimal set of test cases from the test pool that kill all those mutants using a greedy test suite reduction algorithm .
using this minimal set of tests we minimized the mutant set to obtain the minimal mutant set.
table shows the number of test cases in these pools and the sizes of the minimal mutant sets.
we later compare randomly selected mutants with minimal mutants by reducing the test cases taken from the large test pool for each project.
.
experimental setup forc coverage we perform experiments with the nonadequacy value cchosen from the set f70 100g.
for each original test case we create a reduced test case that preserves at least c of the statements covered bythe original test case.
we use gcov to obtain the set of statements covered by each test case.
forn mutant we perform experiments with the nonadequacy value nchosen from the set f1 32g.
for each original test case we rst determine what mutants the test case kills and then randomly select nof those mutants for a small number of test cases that kill fewer than n mutants we use all mutants to create a reduced test case that preserves these nselected mutants.
to compare randomly sampled mutants with the harder to kill minimal mutants we take each test case from the minimal test suite constructed from the large test pool is described in section .
.
and reduce the test case while preserving one randomly selected mutant and then reduce it to perserve the one mutant n from the minimal mutant set that the test case uniquely contributes to the minimal mutant set.
if a test case kills no mutants in the minimal mutant set we do not reduce the test case at all.
performing test case reduction can take a long time for some test cases.
we limit reduction to minutes per test case.
we observed that n mutant test case reduction starts having many timeouts when ngets greater than about so we restrict our choices of nto values less than .
the experiments ignore test cases whose reduction times out.
for each reduced test case we further generate three randomly reduced test cases that have exactly the same size as the reduced test case.
we create such a randomly reduced test case by starting from the original test case and iteratively choosing to remove uniformly randomly selected one part at a time until the resulting test case has the same number of parts as the reduced test case.
we perform random test case reduction merely as some kind of baseline to show the bene ts of preserving bene ts of a test case we do not actually recommend actually using random test case reduction in practice.
.
research questions our evaluation addresses the following questions about the e ects of non adequate test case reduction rq1 how much are test cases reduced srr ?
rq2 how much are code coverage and mutants killed preserved cpr and mpr ?
rq3 how do srr cpr and mpr trade o ?
rq4 how do cpr and mpr for our approaches compare to cpr and mpr for random test case reduction?
.
rq1 srr figures and summarize the results for srr on the test cases reduced using c coverage and n mutant respectively.
for each project and level of candn the boxplots show the distribution of srr.
from the gures we see that both approaches can greatly reduce the size of spidermonkey yaffs2 grep gzip projectsize reduction ratec 100figure srr for c coverage spidermonkey yaffs2 grep gzip projectsize reduction raten figure srr for n mutant test cases.
in most con gurations the median srr for all test cases reduced using either c coverage or n mutant is greater than the size of a reduced test case is usually less than half the size of the original test case.
for both reductions grep behaves somewhat di erently with median srr.
the likely cause is the small size of test cases in grep most have characters.
srr decreases when cornincreases as expected.
we emphasize that srr for c is particularly low compared with srr for other values allowing coverage to miss even a small set of statements increases srr substantially.
for example for grep the median srr for c and c di er by over 30pp3.
.
rq2 cpr and mpr figures and summarize the results for cpr on the test cases reduced using c coverage and n mutant respectively.
cpr is of course always at least as high as thec value given to the reduction.
from figure for spidermonkey andyaffs2 cpr is almost exactly the given c but for the other two projects cpr is sometimes much higher.
overall the median cpr across di erent values of cacross all projects ranges from to .
figure illustrates the relation between di erent values ofnand cpr.
the range of median cpr here goes from to which is quite high showing that preserving even just one mutant leads to cpr close to .
figures and summarize the results for mpr on the test cases reduced using c coverage and n mutant respectively.
for c coverage reduction the median mpr ranges from to across all projects and all val3the pp metric from percentage points represents differences in values that are already expressed as percentages.
spidermonkey yaffs2 grep gzip projectcoverage preservation ratec 100figure cpr for c coverage spidermonkey yaffs2 grep gzip projectcoverage preservation raten figure cpr for n mutant spidermonkey yaffs2 grep gzip projectmutant preservation ratec figure mpr for c coverage spidermonkey yaffs2 grep gzip projectmutant preservation raten figure mpr for n mutant size reduction ratemutant preservation ratemutant minimal randomspidermonkey size reduction ratemutant preservation ratemutant minimal randomyaffs2 size reduction ratemutant preservation ratemutant minimal randomgrep size reduction ratemutant preservation ratemutant minimal randomgzipfigure srr vs. mpr contrasting minimal mutants against randomly chosen mutants for n mutant test case reduction ues ofc.
concerning general trends we see that mpr is positively correlated to the value of c more coverage preserved yields more mutants killed.
with cof or higher the reduced test cases have the median mpr of at least for all projects.
kendall values for spidermonkey yaffs2 grep and gzip were and respectively all withp showing a strong positive correlation between the value of cand mpr.
comparing across the projects we see that yaffs2 has the lowest median mpr when reduced using n mutant reduction .
yaffs2 test cases are sequences of function calls to the le system api such as mount open orclose .
there is little dependency across those functions e.g.
only a few functions call one another so it is the yaffs2 test cases that e ectively control the interaction among the functions by the ordering of the api calls.
thus individual mutants can be isolated reasonably well from the other mutants due to better decoupling between functions.
on the other hand modules in spidermonkey like in any other interpreter or compiler are deeply intertwined.
thus each test case exercises multiple functions.
as a result killing a mutant in the parsing module may also correlate with killing many other mutants in passes before or after parsing such as lexing or interpretation.
therefore it is expected that reduced test cases based on even a single mutant in spidermonkey could still kill a large portion of the mutants killed by the original test cases with median mpr of .
asngrows mpr of the reduced test cases increases unsurprisingly more interdependent mutants can be killed.
this observation is validated by the kendall values and for spidermonkey yaffs2 grep and gzip respectively all with p suggesting that there is a strong positive correlation between nand mpr.
however a trade o is that as nincreases the time to perform the reduction increases as well because intermediate testcases need to be checked against more mutants and the chance of timeout increases.
in addition to performing n mutant test case reduction usingnrandom mutants we also used minimal mutants.
figure shows for each project the relationship between srr and mpr for test cases from the large randomly generated test pool reduced using n mutant with a randomly selected mutant or a minimal mutant.
these plots only show the values for n because each test case can kill at most one minimal mutant.
surprisingly there is no statistically signi cant di erence p except for yaffs2 .
for yaffs2 test cases reduced based on minimal mutant often result in a better trade o between srr and mpr for the same srr test cases tend to have a higher mpr.
.
rq3 trade offs figure shows the trade o between srr and cpr for yaffs2 test cases reduced using c coverage and n mutant.
we show plots only for yaffs2 due to space reasons the plots for the other projects are similar.
for c coverage the cpr values cluster very closely with cvalues but the srr values vary with higher srr usually corresponding to lower cpr.
for n mutant many test cases have high srr but cpr values vary widely.
figure shows the trade o between srr and mpr for spidermonkey test cases.
again we show plots only for spidermonkey the other projects are similar.
for c coverage we obtain good srr and mpr without preserving all coverage many points for c orc cluster in the upper right of the plot.
for n mutant more reduced test cases have high srr and larger nvalues have higher mpr.
finally figure visualizes the trade o between cpr and mpr for all projects.
for both plots we see a linear correlation between cpr and mpr especially for test cases reduced using c coverage.
this trend suggests that the size reduction ratecoverage preservation ratec 100c coverage size reduction ratecoverage preservation raten 32n mutantfigure srr vs. cpr for yaffs2 size reduction ratecoverage preservation ratec 100c coverage size reduction ratecoverage preservation raten 32n mutant figure srr vs. mpr for spidermonkey coverage preservation ratemutant preservation rateproject spidermonkey yaffs2 grep gzipc coverage coverage preservation ratemutant preservation rateproject spidermonkey yaffs2 grep gzipn mutantfigure cpr vs. mpr for all four projects more statements a test case covers the more mutants it kills.
forn mutant especially for yaffs2 there is more clustering towards the right side of the plot indicating that even with a high cpr mpr can still vary widely for the test cases reduced using n mutant.
.
rq4 comparison with random we also compared our approaches to simple random testcase reduction that simply forces a certain size reduction on test case.
for each test case reduced using non adequate test case reduction we generate three reduced test cases of exactly the same size by randomly removing parts from the original test case.
srr is exactly the same for a randomly reduced test case as for its corresponding test case.
therefore we measure only cpr and mpr for these randomly reduced test cases.
figure shows boxplots that compare cpr for test cases reduced using c coverage and n mutant with test cases reduced randomly.
we see from these gures that the median cpr computed for test cases reduced by non adequate test case reduction is greater than the median cpr computed for the test cases reduced randomly.
figure shows the same comparison for mpr.
once again we see from these plots that the median mpr computed for test cases reduced by non adequate test case reduction is greater than the median mpr computed for the test cases reduced randomly.
the median cpr mpr for test cases reduced using non adequate test case reduction is greater than the median cpr mpr for the test cases reduced randomly.
a values of randomly reduced test cases are signi cantly di erent p from the test cases reduced using non adequate test case reduction.
this is hardly surprising but con rms that our approaches add value.
for yaffs2 there is also a speci c cause for the extreme di erences due to the validity of the reduced test cases if the original test case is valid our non adequate spidermonkey yaffs2 grep gzip projectcoverage preservation ratereduction c coverage randomc coverage spidermonkey yaffs2 grep gzip projectcoverage preservation ratereduction n mutant randomn mutantfigure comparing cpr of non adequate testcase reduction with random test case reduction spidermonkey yaffs2 grep gzip projectmutant preservation ratereduction c coverage randomc coverage spidermonkey yaffs2 grep gzip projectmutant preservation ratereduction n mutant randomn mutant figure comparing mpr of non adequate testcase reduction with random test case reduction .
.
.
.
mutants sampledmutant preservation rateyaffs2 mutants sampledmutant preservation rategrepfigure percentage of mutants used for n mutant test case reduction vs. mpr test case reduction is unlikely to produce an invalid reduced test case.
each valid test case in yaffs2 starts by calling a startup function that prepares for mounting the le system.
if a test case does not start with this function the other function calls in the test case fail.
the random test case reduction is unaware of this so if it has to reduce a sequence of function calls to each function call including the startup function has only4 chance to be in the reduced test case i.e.
there is a high chance the reduced test case does not include the startup call and is invalid.
.
discussion inter dependencies among mutants.
from the mpr values forn mutant reduction we see that focusing the reduced test case to preserve only a small number of mutants killed by the original test case still kills a large fraction of all those mutants.
for example by reducing test cases based on only one mutant i.e.
n the median mpr values are and for spidermonkey yaffs2 grep and gzip respectively.
these high mpr values for such a small nsuggest that many mutants killed by a test case have strong dependencies.
figure illustrates this.
the x axis shows the ratio of nto the total number of mutants killed by the original test case i.e.
n mut to and the y axis shows the corresponding mpr.
for space reasons we show plots only for yaffs2 andgrep the other two projects are similar to yaffs2 but grep is di erent from all others.
we see that a test case reduced based on less than of the mutants can still kill more than of originally killed mutants.
note that when a test case reduced to kill some mutant m1also kills another mutant m2 it does not imply that m1subsumesm2in the sense that alltests killingm1also killm2 .
time for non adequate test case reduction.
the time for reducing a test case depends on the number of 24table time in seconds to perform test reduction project c coverage n mutant min med max min med max spidermonkey yaffs2 grep gzip parts in the test case the time to execute the test case and the cost of computing coverage or mutants killed.
table summarizes the time required for test case reduction in our experiments.
for of the test cases in spidermonkey yaffs2 and grep bothc coverage and n mutant non adequate test case reduction nish relatively fast under two minutes for c coverage and under one minute for n mutant .
gzip has signi cantly more parts up to than the other projects which increases the time needed for reduction.
in our experiment all coverage adequate reduction i.e.c of gzip test cases failed due to timeout but allc coverage non adequate test case reduction nished within the time limit with of them being reduced in under ten minutes.
.
threats to validity based on our results it appears that non adequate testcase reduction can substantially reduce the size of test cases while still preserving much of test case quality.
as usual experimental result may not generalize to other projects beyond the four we evaluated or even to other test cases and mutants than the ones we used for these projects.
a particular threat is how we measure quality.
we do not consider some interesting metrics at all e.g.
the execution time of reduced test cases and the ones used are imperfect.
mpr considers all the mutants killed after performing nmutant test case reduction even though the reduction already uses some killed mutants as guidance to reduce the test case one may argue that by construction the reduced test cases will be good by this metric or dually that this metric is bad.
however we perform non adequate test case reduction that does not aim to preserve allmutants killed by the original test case while mpr does consider all mutants killed.
therefore we do not produce test cases that necessarily have a high mpr.
moreover we also measure the cpr of these reduced test cases and we do not use coverage to guiden mutant test case reduction.
mutants of c code can introduce unde ned behavior.
for example a mutant that removes initialization of a local variable can introduce such behavior.
we did not explicitly remove such mutants but we expect them to be relatively few.
therefore we generate a large number of mutants for each project reducing the chance that mutants that introduce unde ned behavior signi cantly bias our results.
another problem was that of the non deterministic load on the shared high performance cluster.
due to nodes having di erent con gurations and with di erent loads a xed timeout of minutes may not correspond to the same amount of reduction.
hence some mutants that happened to be evaluated on a slow machine may have been considered killed due to timeout while similarly slow running mutants evaluated on a fast machine may have managed to complete successfully thereby not considered killed.
.
related work test case reduction aims to reduce the size or complexity of test cases while preserving some desirable properties of these test cases.
reduction is essentially a search in the space of possible modi cations to the original test case.
in many uses the only modi cation allowed is removing a part of the test case .
one goal of test case reduction is to speed up testing and this goal is shared with many techniques for regression testing including regression test selection test prioritization and test suite reduction .
the most similar to test case reduction is test suite reduction.
whereas test case reduction aims to reduce a single test case test suite reduction aims to reduce the size of an entire test suite while preserving some desirable properties for the reduced test suite.
many studies investigated test suite reduction techniques e.g.
including our recent work on non adequate test suite reduction .
however this paper presents the rst study of non adequate test case reduction.
test case reduction and test suite reduction can be easily combined either in succession or in tandem.
delta debugging is the best known technique for reducing the size of a failing test case it reduces the test case so that it still fails but no single part can be removed without passing.
cause reduction generalizes deltadebugging by reducing a test case so that it still has the same coverage or another property but no single part can be removed without losing coverage or another property .
our non adequate test case reduction further generalizes cause reduction by not requiring a test case to preserve the complete property the original test case satis es.
.
conclusion having smaller test cases is desirable for developers such test cases run faster and make debugging easier.
test case reduction reduces the size of test cases.
previous research has studied how to conduct test case reduction while completely preserving some property of the original test case e.g.
failure or coverage.
we evaluate a more general approach to test case reduction called non adequate test case reduction that allows only partially preserving a property.
speci cally we propose and evaluate c coverage and nmutant.
our results show that non adequate test case reduction can substantially reduce the size of test cases while still preserving a large percentage of all coverage or mutants killed by the original test cases.
for c coverage in particular simply giving up on a very small percentage of coverage can greatly reduce reduction time and produce a higher gain in size reduction than the associated loss in coverage.
the idea of non adequate test case reduction greatly expands the options available in exploring trade o s between test suite size measured by adding sizes of individual test cases for fault detection capability.
.