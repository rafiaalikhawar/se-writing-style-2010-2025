guiding deep learning system testing sing surprise adequacy jinhan kim robert feldt shin yoo school of computing kaist daejeon republic of korea jinhankim shin.yoo kaist.ac.kr dept.
of computer science and engineering chalmers university gothenburg sweden robert.feldt chalmers.se dept.
of software engineering blekinge inst.
of technology karlskrona sweden robert.feldt bth.se abstract deep learning dl systems are rapidly being adopted in safety and security critical domains urgently calling for ways to test their correctness and robustness.
testing of dl systems has traditionally relied on manual collection and labelling of data.
recently a number of coverage criteria based on neuron activation values have been proposed.
these criteria essentially count the number of neurons whose activation during the execution of a dl system satisfied certain properties such as being above predefined thresholds.
however existing coverage criteria are not sufficiently fine grained to capture subtle behaviours exhibited by dl systems.
moreover evaluations have focused on showing correlation between adversarial examples and proposed criteria rather than evaluating and guiding their use for actual testing of dl systems.
we propose a novel test adequacy criterion for testing of dl systems called surprise adequacy for deep learning systems sadl which is based on the behaviour of dl systems with respect to their training data.
we measure the surprise of an input as the difference in dl system s behaviour between the input and the training data i.e.
what was learnt during training and subsequently develop this as an adequacy criterion a good test input should be sufficiently but not overtly surprising compared to training data.
empirical evaluation using a range of dl systems from simple image classifiers to autonomous driving car platforms shows that systematic sampling of inputs based on their surprise can improve classification accuracy of dl systems against adversarial examples by up to .
via retraining.
index t erms test adequacy deep learning systems i. i ntroduction deep learning dl systems have achieved significant progress in many domains including image recognition speech recognition and machine translation .
based on their capability to match or even surpass human performance dl systems are increasingly being adopted as part of larger systems in both safety and security critical domains such as autonomous driving and malware detection .
such adoption of dl systems calls for new challenges as it is critically important that these larger systems are both correct and predictable.
despite their impressive experimental performances dl systems are known to exhibit unexpected behaviours under certain circumstances.
for example in a reported incident an autonomous driving vehicle expectedanother vehicle to yield in one of the rarer circumstances and crashed into the other vehicle when the expectation proved incorrect .
there is an urgent need to verify and validate behaviours of dl systems.
however a significant part of existing software testing technique is not directly applicable to dl systems.
most notably traditional white box testing techniques that aim to increase structural coverage is not very useful for dl systems as their behaviour is not explicitly encoded in their control flow structures.
a number of novel approaches towards testing and verification of dl systems have been recently proposed to fill in the gap .
most of these techniques share two assumptions.
the first assumption is essentially a generalisation of the essence of metamorphic testing if two inputs to a dl system are similar with respect to some human sense the outputs should also be similar.
for example deeptest checks whether an autonomous driving system behaves in the same way when the input image is transformed as if the same scene is under a different weather condition.
the second assumption also based in more traditional software testing results is that the more diverse a set of input is the more effective testing of a dl system one can perform.
for example deepxplore presented the neuron coverage the ratio of neurons whose activation values were above a predefined threshold as the measure of diversity of neuron behaviour and subsequently showed that inputs violating the first assumption will also increase the neuron coverage.
while the recently introduced techniques have made significant advances over manual ad hoc testing of dl systems there is a major limitation.
the coverage criteria proposed so far are not sufficiently fine grained in a sense that all of them simply count neurons whose activation values satisfy certain conditions.
while this aggregation by counting does allow the tester to quantify the test effectiveness of a given input set it conveys little information about individual inputs.
for example it is not immediately clear when an input with higher nc should be considered better than another with lower nc and why certain inputs may naturally activate more neurons above the threshold than others and vice versa.
another example is the k multisection neuron coverage whichu ieee acm 41st international conference on software engineering icse .
ieee ieee acm 41st international conference on software engineering icse .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
partitions the ranges of activation values of neurons observed during training into kbuckets and count the number of total buckets covered by a set of inputs.
when measured for a single input the coverage will be either1 kif the input activates each neuron with a value from one of the kbuckets or smaller than that if some neurons activate outside the range observed during training.
again the information about how far such activations go beyond observed range is lost during aggregation making it hard to evaluate the relative value of each input.
for a test adequacy criterion to be practically useful it should be able to guide the selection of individual inputs eventually resulting in improvements of the accuracy of the dl system under investigation.
to overcome these limitations we propose a new test adequacy for dl systems called surprise adequacy for dl systems sadl .
intuitively a good test input set for a dl system should be systematically diversified to include inputs ranging from those similar to training data to those significantly different and adversarial.1at individual input granularity sadl measures how surprising the input is to a dl system with respect to the data the system was trained with the actual measure of surprise can be either based on the likelihood of the system having seen a similar input during training here with respect to probability density distributions extrapolated from the training process using kernel density estimation or the distance between vectors representing the neuron activation traces of the given input and the training data here simply using euclidean distance .
subsequently the surprise adequacy sa of a set of test inputs is measured by the range of individual surprise values the set covers.
we show that sadl is sufficiently fine grained by training adversarial example classifiers based on sadl values that can produce higher accuracy compared to the state of the art.
we also show that sampling inputs according to sadl for retraining dl systems can result in higher accuracy thus showing that sadl is an independent variable that can positively affect the effectiveness of dl system testing.
the technical contributions of this paper are as follows we propose sadl a fine grained test adequacy metric that measures the surprise of an input i.e.
the difference in the behaviour of a dl system between a given input and the training data.
two concrete instances of sadl are proposed based on different ways to quantify surprise.
both are shown to be correlated with existing coverage criteria for dl systems.
we show that sadl is sufficiently fine grained in capturing the behaviour of dl systems by training a highly accurate adversarial example classifier.
our adversarial example classifier shows as much as and .
roc auc score when applied to mnist and cifar dataset respectively.
we show that sadl metrics can be used to sample effective test input sets.
when retraining dl systems us1experiments show benefits of diversity for general testing and benefits of a scale of distances of test inputs for robustness testing introduced in .ing additional adversarial examples sampling additional inputs with broader sa values can improve the accuracy after retraining by up to .
.
we undertake all our experiments using publicly available dl systems ranging from small benchmarks mnist and cifar to a large system for autonomous driving vehicles dave and chauffeur .
all implementations are available online.
the remaining of this paper is organised as follows.
section ii introduces surprise adequacy for dl systems sadl two variants of sadl are presented along with algorithms that measure them.
section iii sets out the research questions and section iv describes the experimental set up of the empirical evaluations.
section v presents the results from empirical evaluations.
section vi addresses threats to validity.
section vii presents related work and section viii concludes.
ii.
s urprise adequacy for deeplearning systems all existing test adequacy criteria for dl systems aim to measure the diversity of an input set.
neuron coverage nc posits that the higher the number of neurons that are activated above a predefined threshold the more diverse input the dl system has been executed with.
deepgauge proposed a range of finer grained adequacy criteria including k multisection neuron coverage which measures the ratio of activation value buckets that have been covered across all neurons and neuron boundary coverage which measures the ratio of neurons that are activated beyond the ranges observed during training.
we argue that diversity in testing of dl systems is more meaningful when it is measured with respect to the training data as dl systems are likely to be more error prone for inputs that are unfamiliar i.e.
diverse.
furthermore while neuron activation above thresholds or beyond observed ranges may be closely related to diversity of the given input they do not measure to what degree the activations of the network for one input differs from the activations for another input.
they are fundamentally discretisations and do not utilize the fact that neuron activations are continuous quantities.
in contrast our aim is to define an adequacy criterion that quantitatively measures behavioural differences observed in a given set of inputs relative to the training data.
a. activation trace and surprise adequacy letn n1 n2 ... be a set of neurons that constitutes a dl system d and letx x1 x2 ... be a set of inputs.
we denote the activation value of a single neuron nwith respect to an input xas n x .
for an ordered sub set of neurons let n n n x denote a vector of activation values each element corresponding to an individual neuron in n the cardinality of n x is equal to n .
we call n x the activation trace at of xover neurons in n. similarly letan x be a set of activation traces observed over neurons inn for a set of inputs x an x n x x x .w e 2please refer to authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
note that the activation trace is trivially available after each execution of the network for a given input.
since behaviours of dl systems are driven along the dataflow and not control flow we assume that activation traces observed over all nwith respect to x an x fully captures the behaviours of the dl system under investigation when executed using x. surprise adequacy sa aims to measure the relative novelty i.e.
surprise of a given new input with respect to the inputs used for training.
given a training set t we first computean t by recording activation values of all neurons using every input in the training data set.
subsequently given a new input x we measure how surprising xis when compared totby comparing the activation trace of xtoan t .
this quantitative similarity measure is called surprise adequacy sa .
we introduce two variants of sa each with different way of measuring the similarity between xandan t .
note that certain types of dl tasks allow us to focus on parts of the training set tto get more precise and meaningful measurement of sa.
for example suppose we are testing a classifier with a new input x which is classified by the dl system under investigation as the class c. in this case the surprise of xis more meaningfully measured against an tc in which tcis the subset of twhere members are classified asc.
basically the input might be surprising as an example of classceven if not surprising in relation to the full set of training examples.
b. likelihood based surprise adequacy kernel density estimation kde is a way of estimating the probability density function of a given random variable.
the resulting density function allows the estimation of relative likelihood of a specific value of the random variable.
likelihood based sa lsa uses kde to estimate the probability density of each activation value in an t and obtains the surprise of a new input with respect to the estimated density.
this is an extension of existing work that uses kde to detect adversarial examples .
to reduce dimensionality and computational cost we only consider the neurons in a selected layer nl n which yields a set of activation traces anl x .
to further reduce the computational cost we filter out neurons whose activation values show variance lower than a pre defined threshold t as these neurons will not contribute much information to kde.
the cardinality of each trace will be nl .
given a bandwidth matrix hand gaussian kernel function k the activation trace of the new input x and xi t kde produces density function fas follows f x anl t summationdisplay xi tkh nl x nl xi 3for the sake of simplicity we assume that it is possible to get the complete activation traces from all the neurons in a dl system.
for network architectures with loops such as recurrent neural nets rnns it is possible to unroll the loops up to a predefined bound .
4however the main idea is general and other specific variants would result if using other similarity functions.since we want to measure the surprise of the input x w e need a metric that increases when probability density decreases i.e.
the input is rarer compared to the training data and vice versa i.e.
the input is similar to the training data .
adopting common approach of converting probability density to a measure of rareness we define lsa to be the negative of the log of density lsa x log f x note that extra information about input types can be used to make lsa more precise.
for example given a dl classifier d we expect inputs that share the same class label will have similar ats.
we can exploit this by computing lsa per class replacing twith x t d x c for class c.w eu s e per class lsa for dl classifiers in our empirical evaluation.
c1c2 boundary learnt by dl x1 x2a1b1 a2b2 x1a x2ax1bx2b fig.
an example of distance based sa.
black dots represent ats of training data inputs whereas grey dots represent ats of new inputs x1andx2.
compared to distances from x1aand x2ato classc2 a to fx1is farther out from class c1than that ofx2 i.e.
a1 b1 a2 b2 see equations and .
consequently we decide that x1is more surprising than x2w.r.t.
class c1.
c. distance based surprise adequacy an alternative to lsa is simply to use the distance between ats as the measure of surprise.
here we define distancebased surprise adequacy dsa using the euclidean distance between the at of a new input xand ats observed during training.
being a distance metric dsa is ideally suited to exploit the boundaries between inputs as can be seen in the classification example in figure .
by comparing the distances a1anda2 i.e.
distance between the at of a new input and the reference point which is the nearest at of training data in c1 to distances b1andb2 i.e.
distance to c2measured from the reference point we get a sense of how close to the class boundary the new inputs are.
we posit that for classification problems inputs that are closer to class boundaries are more surprising and valuable in terms of test input diversity.
on the other hand for tasks without any boundaries between inputs such as prediction of appropriate steering angle for autonomous driving car dsa may not be easily applicable.
with no class boundaries an at of a new input being far from that of another training input does not guarantee that the new input is surprising as the at may still be located in crowded parts of the at space.
consequently we only apply dsa for classification tasks for which it can be more effective than lsa see section v a and v b for more details .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
let us assume that a dl system d which consists of a set of neurons n is trained for a classification task with a set of classes c using a training dataset t. given the set of activation traces an t a new input x and a predicted class of the new input cx c we define the reference point xato be the closest neighbour of xthat shares the same class.
the distance between xandxafollows from the definition xa a r g m i n d xi cx bardbl n x n xi bardbl dist a bardbl n x n xa bardbl subsequently from xa we find the closest neighbour of xa in a class other than cx xb and the distance dist b as follows xb a r g m i n d xi c cx bardbl n xa n xi bardbl dist b bardbl n xa n xb bardbl intuitively dsa aims to compare the distance from the at of a new input xto known ats belonging to its own class cx to the known distance between ats in class cxand ats in other classes in c cx .
if the former is relatively larger than the latter xwould be a surprising input for class cxto the classifying dl system d. while there are multiple ways to formalise this we select a simple one and calculate dsa as the ratio between dist aanddist b. investigation of more complicated formulations is left as future work.
dsa x dist a dist b d. surprise coverage given a setof inputs we can also measure the range of sa values the set covers called surprise coverage sc .
since both lsa and dsa are defined in continuous spaces we use bucketing to discretise the space of surprise and define both likelihood based surprise coverage lsc and distancebased surprise coverage dsc .
given an upper bound of u and buckets b b1 b2 ... b n that divide u intonsa segments sc for a set of inputs xis defined as follows sc x bi x x sa x u i n u i n n a set of inputs with high sc is a diverse set of inputs ranging from similar to those seen during training i.e.
low sa to very different from what was seen during training i.e.
high sa .
we argue that an input set for a dl system should not only be diversified but systematically diversified considering sa.
recent results also validate this notion by showing that more distant test inputs were more likely to lead to exceptions but might not be as relevant for testing .
while we use the term cover and coverage the implications of sa based coverage is different from the traditional structural coverage.
first unlike most of the structural coverage criteria there is no finite set of targets to cover as in statement or branch coverage an input can at least in theory be arbitrarilysurprising.
however an input with arbitrarily high sa value may simply be irrelevant or at least less interesting to the problem domain e.g.
an image of a traffic sign will be irrelevant to the testing of animal photo classifiers .
as such sc can only be measured with respect to pre defined upper bound in the same way the theoretically infinite path coverage is bounded by a parameter .
second sc does not render itself to a combinatorial set cover problem which the test suite minimisation is often formulated into .
this is because a single input yields only a single sa value and cannot belong to multiple sa buckets.
the sense of redundancy with respect to sc as a coverage criteria is weaker than that of structural coverage for which a single input can cover multiple targets.
while we aim to show that sa can guide the better selection of inputs rigorous study of optimisation of test suites for dl systems remains a future work.
however as we show with our empirical studies sc can still guide test input selection.
iii.
r esearch questions our empirical evaluation is designed to answer the following research questions.
rq1.
surprise is sadl capable of capturing the relative surprise of an input of a dl system?
we provide answers to rq1 from different angles.
first we compute the sa of each test input included in the original dataset and see if a dl classifier finds inputs with higher surprise more difficult to correctly classify.
we expect more surprising input to be harder to correctly classify.
second we evaluate whether it is possible to detect adversarial examples based on sa values as we expect adversarial examples to be more surprising as well as to cause different behaviours of dl systems.
using different techniques multiple sets of adversarial examples are generated and compared by their sa values.
finally we train adversarial example classifiers using logistic regression on sa values.
for each adversarial attack strategy we generate adversarial examples using original test images provided by mnist and cifar10.
using original test images and adversarial examples all chosen randomly we train the logistic regression classifiers.
finally we evaluate the trained classifiers using the remaining original test images and adversarial examples.
if sa values correctly capture the behaviour of dl systems we expect the sa based classifiers to successfully detect adversarial examples.
we use area under curve of receiver operator characteristics roc auc for evaluation as it captures both true and false positive rates .
rq2.
layer sensitivity does the selection of layers of neurons used for sa computation have any impact on how accurately sa reflects the behaviour of dl systems?
bengio et al.
suggest that deeper layers represent higher level features of the input subsequent work that introduced kde based adversarial example detection technique assumes the deepest i.e.
the last hidden layer to contain the most information helpful for detection.
we evaluate this authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i list of datasets and models used in the study.
dataset description dnn model of neuron synthetic inputs performance mnist handwritten digit images composed of images for training and images for test.a five layer convnet with max pooling and dropout layers.
fgsm bim a bimb jsma c w. .
accuracy cifar object recognition dataset in ten different classes composed of images for training and images for test.a layer convnet with max pooling and dropout layers.
fgsm bim a bimb jsma c w. .
accuracy udacity self driving car challengeself driving car dataset that contains camera images from the vehicle composed of images for training and images for test.
the goal of the challenge is to predict steering wheel angle.dave architecture from nvidia.
deepxplore s test input generation via joint optimization.
.
mse chauffeur architecture with cnn and lstm.
deeptest s combined transformation.
.
mse assumption in the context of sa by calculating lsa and dsa of all individual layers and subsequently by comparing adversarial example classifiers trained on sa from each layer.
rq3.
correlation is sc correlated to existing coverage criteria for dl systems?
in addition to capturing input surprise we want sc to be consistent with existing coverage criteria based on counting aggregation.
if not there is a risk that sc is in fact measuring something other than input diversity.
for this we check whether sc is correlated with other criteria.
we control the input diversity by cumulatively adding inputs generated by different method i.e.
different adversarial example generation techniques or input synthesis techniques execute the studied dl systems with these input and compare the observed changes of various coverage criteria including sc and four existing ones deepxplore s neuron coverage nc and three neuron level coverages nlcs introduced by deepgauge k multisection neuron coverage kmnc neuron boundary coverage nbc and strong neuron activation coverage snac .
for mnist and cifar we start from the original test data provided by the dataset images and add adversarial examples generated by fgsm bim a bim b jsma and c w at each step.
for dave we start from the original test data images and add synthetic images generated by deepxplore at each step.
for chauffeur each step adds synthetic images set1 to set3 each produced by applying random number of deeptest transformations.
rq4.
guidance can sa guide retraining of dl systems to improve their accuracy against adversarial examples and synthetic test inputs generated by deepxplore?
to evaluate whether sadl can guide additional training of existing dl systems with the aim of improved accuracy against adversarial examples we ask whether sa can guide the selection of input for additional training.
from the adversarial examples and synthesised inputs for these models5 we choose four sets of images from four different sa ranges.
given 5we could not resume training of chauffeur model for additional five epochs which is why it is absent from rq4.uas the upper bound used in rq3 to compute the sc we divide the range of sa into four overlapping subsets the first subset including the low sa values u the second including the lower half 2u the third including the lower 3u and finally the entire range .
these four subsets are expected to represent increasingly more diverse sets of inputs.
we set the range rto one of these four randomly sample images from each r and train existing models for five additional epochs.
finally we measure each model s performance accuracy for mnist and cifar mse for dave against the entire adversarial and synthetic inputs respectively.
we expect retraining with more diverse subset will result in higher performance.
iv .
e xperimental setup we evaluate sadl on four different dl systems using a the original test sets b adversarial examples generated by five attack strategies and c synthetic inputs generated by deepxplore and deeptest .
this section describes the studied dl systems and the input generation methods.
a. datasets and dl systems table i lists the subject datasets and models of dl systems.
mnist and cifar are widely used datasets for machine learning research each of which is a collection of images in ten different classes.
for mnist we adopt the widely studied five layer convolutional neural network convnet with max pooling and dropout layers and train it to achieve .
accuracy on the provided test set.
similarly the adopted model for cifar is a layer convnet with max pooling and dropout layers trained to achieve .
accuracy on the provided test set.
for evaluation of sadl for dl systems in safety critical domains we use the udacity self driving car challenge dataset which contains a collection of camera images from the driving car.
as its aim is to predict steering wheel angle the model accuracy is measured using mean squared error mse between actual and predicted steering angles.
we use a pre trained dave model which is a public artefact provided by deepxplore6 and a pre trained chauffeur 6deepxplore is available from authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
model made publicly available by the udacity self driving car challenge.
dave consists of nine layers including five convolutional layers and achieves .
in mse.
chauffeur consists of both a convnet and an lstm sub model and achieves .
in mse.
b. adversarial examples and synthetic inputs sadl is evaluated using both adversarial examples and synthetic test inputs.
adversarial examples are crafted by applying to the original input small perturbations imperceptible to humans until the dl system under investigation behaves incorrectly .
we rely on adversarial attacks to generate input images for mnist and cifar these generated images are more likely to reveal robustness issues in the dl systems than the test inputs provided by the original datasets.
we use five widely studied attack strategies to evaluate sadl fast gradient sign method fgsm basic iterative method bim a bim b jacobian based saliency map attack jsma and carlini wagner c w .
our implementation of these strategies is based on cleverhans and a framework of ma et al.
.
for dave and chauffeur we use the state of the art synthetic input generation algorithms deepxplore and deeptest .
both algorithms are designed to synthesise new test input from existing ones with the aim of detecting erroneous behaviours in autonomous driving vehicle.
for dave we use deepxplore s input generation via joint optimization algorithm whose aim is to generate inputs that lead multiple dl systems trained independently but using the same training data to disagree with each other.
using dave and its two variants dave dropout and dave norminit we collect synthetic inputs generated by lighting effect light occlusion by a single black rectangle singleocc and occlusion by multiple black rectangles multiocc .
for chauffeur we synthesise new inputs by iteratively applying random transformations provided by deeptest to original input images translation scale shear rotation contrast brightness and blur.
table ii configurations for rq3.
dnn nc nlcs lsc dsc model th k layer nu b nu b mnist .
activation .
cifar .
activation .
dave .
block1 conv2 n a chauffeur .
convolution2d n a c. configurations for all research questions the default activation variance threshold for lsa is set to and the bandwidth for kde is set using scott s rule .
the remaining of this section details rq specific configurations.
for rq1 we use the activation layer for mnist and activation for cifar10 when computing lsa values.
computation of lsa based 7at the time of our experiments the publicly available version of deeptest did not internally support realistic image transformations such as fog and rain effects.on all neurons is computationally infeasible due to precision loss.
for rq2 we set the activation variance threshold for layers activation and activation of cifar to which reduces the number of neurons used for the computation of lsa and consequently the computational cost.
for computation of other coverage criteria in rq3 we use the configurations in table ii.
the threshold of nc is set to .
.
for nlcs we all set the number of sections k to .
for lsc and dsc we manually choose the layer the number of buckets n and the upper bound ub .
for rq4 the layers chosen for mnist and cifar are activation and activation respectively.
we perform runs of retraining for each subject and report the statistics.
all experiments were performed on machines equipped with intel i7 cpu 32gb ram running ubuntu .
.
lts.
mnist and cifar are implemented using keras v. .
.
.
v. r esult due to the space limit we cannot include all plots and tables and make them available online a. input surprise rq1 figure shows how the classification accuracy changes when we classify sets of images of growing sizes from the test inputs included in the mnist and cifar dataset.
the sets of images corresponding to the red dots ascending sa start with images with the lowest sa and increasingly include images with higher sa in the ascending order of sa the sets of images corresponding to the blue dots grow in the opposite direction i.e.
from images with the highest sa to lower sa .
as a reference the green dots show the mean accuracy of randomly growing sets across repetitions.
it is clear that including images with higher lsa values i.e.
more surprising images leads to lower accuracy.
for visual confirmation on another dataset we also chose sets of inputs synthesised for chauffeur by deeptest from three distinct levels of lsa values figure shows that the higher the lsa values are the harder it is to recognise images visually.
both quantitatively and visually the observed trend supports our claim that sadl captures input surprise even for unseen inputs sa can measure how surprising the given input is which is directly related to the performance of the dl system.
figure shows plots of sorted dsa values of adversarial examples generated by each of the five techniques as well as the original test inputs.
figure contains similar plots based on lsa values of randomly selected adversarial examples and the original test set from different layers of mnist and cifar .
for both mnist and cifar the test inputs provided with the datasets represented in blue colour tend to be the least surprising whereas the majority of adversarial examples are clearly separated from the test inputs by their higher sa values.
this supports our claim that sadl can capture the differences in dl system s behaviours for adversarial examples.
finally table iii shows the roc auc results of dsabased classification using all neurons in mnist and cifar10441044 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a selected test inputs based on lsa in mnist b selected test inputs based on dsa in mnist c selected test inputs based on lsa in cifar d selected test inputs based on dsa in cifar fig.
accuracy of test inputs in mnist and cifar dataset selected from the input with the lowest sa increasingly including inputs with higher sa and vice versa i.e.
from the input with the highest sa to inputs with lower sa .
.8the results show that the gap in dsa values observed in figure can be used to classify adversarial examples with high accuracy.
for the relatively simpler mnist model the dsabased classifier can detect adversarial examples with rocauc ranging from .
to .
.
the dsa based classification for the more complicated cifar model shows lower roc auc values but answers to rq2 suggest that dsa from specific layers can produce significantly higher accuracy see section v b .
based on three different analyses the answer to rq1 is that 8lsa based classification is only possible for subsets of neurons due to the computational cost of kde hence we introduce the results of lsa based classification when answering the impact of layer selection for rq2.
a low lsa b medium lsa c high lsa fig.
synthetic images for chauffeur model generated by deeptest.
images with higher lsa values tend to be harder to recognise and interpret visually.
fig.
sorted dsa values of adversarial examples for mnist and cifar .
sadl can capture the relative surprise of inputs .
inputs with higher sa are harder to correctly classify adversarial examples show higher sa values and can be classified based on sa accordingly.
b. impact of layer selection rq2 table iv shows the roc auc of classification of adversarial examples resulting in each row corresponding to a classifier trained on lsa and dsa from a specific layer in mnist respectively.
rows are ordered by their depth i.e.
activation is the deepest and the last hidden layer in mnist.
the highest roc auc values for each attack strategy are typeset in bold.
for mnist there is no clear evidence that the deepest layer is the most effective.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
sorted lsa of randomly selected adversarial examples for mnist and cifar from different layers table iii roc auc of dsa based classification of adversarial examples for mnist and cifar dataset fgsm bim a bim b jsma c w mnist .
.
.
.
.
cifar .
.
.
.
.
table iv roc auc results of sa per layers on mnist.
sa layer fgsm bim a bim b jsma c w lsaactivation .
.
.
.
.
activation .
.
.
.
.
pool .
.
.
.
.
activation .
.
.
.
.
dsaactivation .
.
.
.
.
activation .
.
.
.
.
pool .
.
.
.
.
activation .
.
.
.
.
the cases for which roc auc is can be explained by figure lsa values from activation of mnist for example show a clear separation between the original test inputs and fgsm bim a or bim b by choosing an appropriate threshold it is possible to completely separate test inputs from adversarial examples.
similarly the plot of lsa from activation of mnist shows that c w lsa line crossing with that of the original test data i.e.
c w adversarial examples are less surprising than the original test data this results in the low roc auc value of .
.
table v contains the roc auc values of lsa and dsabased classifiers trained on each layer of the cifar model for each attack strategy the highest roc auc values are typeset in bold.
interestingly lsa and dsa show different trends with cifar .
with lsa there is no strong evidence that the deepest layer produces the most accurate classifiers.
however with dsa the deepest layer produces the most accurate classifiers for three out of five attack strategies bimb jsma and c w while the second deepest layer producesthe most accurate classifier for bim a. more importantly perlayer dsa values produce much more accurate classification results than all neuron dsa values as can be seen in the comparison between table iii and table iv v. identical models have been used to produce results in tables above.
table v roc auc results of sa per layers on cifar .
sa layer fgsm bim a bim b jsma c w lsaactivation .
.
.
.
.
activation .
.
.
.
.
pool .
.
.
.
.
activation .
.
.
.
.
activation .
.
.
.
.
pool .
.
.
.
.
activation .
.
.
.
.
activation .
.
.
.
.
pool .
.
.
.
.
activation .
.
.
.
.
activation .
.
.
.
.
dsaactivation .
.
.
.
.
activation .
.
.
.
.
pool .
.
.
.
.
activation .
.
.
.
.
activation .
.
.
.
.
pool .
.
.
.
.
activation .
.
.
.
.
activation .
.
.
.
.
pool .
.
.
.
.
activation .
.
.
.
.
activation .
.
.
.
.
based on these results we answer rq2 that dsa is sensitive to the selection of layers it is computed from and benefits from choosing the deeper layer .
however for lsa there is no clear evidence supporting the deeper layer assumption.
the layer sensitivity varies across different adversarial example generation strategies.
c. correlation between sc and other criteria rq3 table vi shows how different coverage criteria respond to increasing diversity levels9.
columns represent steps at each 9see for plots.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
of which more inputs are added to the original test set.
if the increase in coverage at a step is less than .
percentage point when compared to the previous step the value is underlined.
the threshold of .
percentage point is based on the finest step change possible for lsc dsc as well as kmnc as all three use bucketing with k .
we acknowledge that the threshold is arbitrary and provide it only as a supporting aid.
note that dsc cannot be computed for these two dl systems as they are not classifiers see section ii c .
overall most of the studied criteria increase as additional inputs are added at each step.
the notable exception is nc which plateaus against many steps.
this is in line with results in existing work .
there exists an interplay between the type of added inputs and how different criteria respond snac kmnc and nbc show significant increases with the addition of bim b examples to cifar but change little when c w inputs are added.
however only snac and nbc exhibit a similar increase with the addition of input set for chauffeur while kmnc increases more steadily.
overall with the exception of nc we answer rq3 that sc is correlated with other coverage criteria introduced so far .
dnn criteria test step step step step step fgsm bim a bim b jsma c w mnistlsc .
.
.
.
.
.
dsc .
.
.
.
.
.
nc .
.
.
.
.
.
kmnc .
.
.
.
.
.
nbc .
.
.
.
.
.
snac .
.
.
.
.
.
cifar 10lsc .
.
.
.
.
.
dsc .
.
.
.
.
.
nc .
.
.
.
.
.
kmnc .
.
.
.
.
.
nbc .
.
.
.
.
.
snac .
.
.
.
.
.
dnn criteria test singleocc multiocc light dave 2lsc .
.
.
.
nc .
.
.
.
kmnc .
.
.
.
nbc .
.
.
.
snac .
.
.
.
dnn criteria test set set set chauffeurlsc .
.
.
.
nc .
.
.
.
kmnc .
.
.
.
nbc .
.
.
.
snac .
.
.
.
table vi changes in various coverage criteria against increasing input diversity.
we put additional inputs into the original test inputs and observe changes in coverage values.
d. retraining guidance rq4 table vii shows the impact of sa based guidance for retraining of mnist cifar and dave models.
the columnrfrom1 4to4 4represents the increasingly wider ranges of sa from which the inputs for additional training are sampled rows with r show performance of the dl system before retraining.
overall there are retraining configurations sa types dl systems adversarial attack strategies and sa type dl system three input synthesis methods each of which is evaluated against four sa ranges with repetitions.
columns and contain the mean and standard deviation of observed performance metric i.e.
the highest accuracy for mnist and cifar the lowest mse for dave .
the best performance is typeset in bold.
the full range produces the best retraining performance for configurations followed by2 configurations configurations and1 configurations .
note that for the configuration of cifar and bim b both ranges2 4and 4produces the same and the best retraining performance.
the largest improvement is observed when retraining mnist against fgsm using dsa the accuracy of the4 4range shows .
increase from that of1 i.e.
from .
to .
.
while retraining mnist against bim b using dsa shows even greater improvement from .
to .
we suspect this is an outlier as the accuracy for ranges1 4and2 4are significantly smaller when compared to other configurations.
while our observations are limited to the dl systems and input generation techniques studied here we answer rq4 that sa can provide guidance for more effective retraining against adversarial examples based on our interpretation of the observed trend .
dnn sar fgsm bim a bim b jsma c w model mnist .
.
.
.
.
lsa1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dsa1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.19cifar .
.
.
.
.
lsa1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dsa1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a mnist and cifar dnn sar singleocc multiocc light model dave .
.
.
lsa1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
b dave table vii retraining guided by sa we sample inputs from four increasingly wider ranges of sa u 2u 3u and and retrain for five additional epochs using the samples as the training data and measure the accuracy and mse against the entire adversarial and synthetic inputs.
sampling from wider ranges improves the retraining accuracy.
vi.
t hreats to validity the primary threat to internal validity of this study is the correctness of implementation of the studied dl systems as well as the computation of sa values.
we have used publicly available architectures and pre trained models as our subjects to avoid incorrect implementation.
sa computation depends on a widely used computation library scipy which has authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
stood the public scrutiny.
threats to external validity mostly concerns the number of the models and input generation techniques we study here.
it is possible that sadl is less effective against other dl systems.
while we believe the core principle of measuring input surprise is universally applicable only further experimentations can reduce this particular risk.
finally threats to construct validity asks whether we are measuring the correct factors to draw our conclusion.
for all studied dl systems activation traces are immediate artefacts of their executions and the meaning of output accuracy is well established minimising the risk of this threat.
vii.
r elated work adversarial examples pose significant threats to the performance of dl systems .
there are existing work in the machine learning community on detection of such inputs.
feinman et al.
first introduced the kde as a means of similarity measurement with the aim of detecting adversarial examples.
sadl improves upon the existing work by a number of different ways.
first we generalise the concept of surprise adequacy sa and introduce distance based sa.
second our evaluation is in the context of dl system testing.
third our evaluation of sadl includes more complicated and practical dl systems as well as testing techniques such as deepxplore and deeptest.
finally we show that the choice of neurons has limited impact on lsa.
a range of techniques has been recently proposed to test and verify dl systems.
the existing techniques are largely based on two assumptions.
the first assumption is a variation of metamorphic testing .
suppose a dl system n produces an output owhen given ias the input i.e.
n i o. then we expect n i prime similarequalowheni prime similarequali.
huang et al.
proposed a verification technique that can automatically generate counter examples that violate this assumption.
pei et al.
introduced deepxplore a white box technique that generates test inputs that cause disagreement among a set of dl systems i.e.
nm i negationslash nn i for independently trained dl systems nmandnn.
tian et al.
presented deeptest whose metamorphic relations include both simple geometric perturbations as well as realistic weather effects .
the second assumption is that the more diverse a set of input is the more effective it will be for testing and validating dl systems.
pei et al.
proposed neuron coverage nc which measures the ratio of neurons whose activation values are above a predefined threshold .
it has been shown that adding test inputs that violate the first assumption increases the diversity measured through nc.
similarly deepgauge introduced a set of multi granularity coverage criteria that are thought to reflect behaviours of dl systems in finer granularity .
while these criteria capture input diversity all of them are essentially count of neurons unlike sa and therefore cannot be directly linked to behaviours of dl systems.
we show that sa is closely related to the behaviours by training accurate adversarial example classifiers based on sa.
apart from coverage criteria other concepts in traditional software testing have been reformulated and applied to testingof dl systems.
ma et al.
proposed deepct which views ranges of neuron activation values as parameter choices and applies combinatorial interaction testing cit to measure interaction coverage .
sc is different from deepct as sadl aims to quantify the amount of surprise rather than simply to detect surprise via increase in coverage.
deepmutation applies the principle of mutation testing to dl systems by mutating training data test data as well as the dl system itself based on source and model level mutation operators .
viii.
c onclusion we propose sadl a surprise adequacy framework for dl systems that can quantitatively measure relative surprise of each input with respect to the training data which we call surprise adequacy sa .
using sa we also develop surprise coverage sc which measures the coverage of discretised input surprise ranges rather than the count of neurons with specific activation traits.
our empirical evaluation shows that sa and sc can capture the surprise of inputs accurately and are good indicators of how dl systems will react to unknown inputs.
sa is correlated with how difficult a dl system finds an input and can be used to accurately classify adversarial examples.
sc can be used to guide selection of inputs for more effective retraining of dl systems for adversarial examples as well as inputs synthesised by deepxplore.
acknowledgement this work was supported by the engineering research center program through the national research foundation of korea funded by the korean government msit nrf2018r1a5a1059921 institute for information communications technology promotion grant funded by the korean government msit no.
and the nextgeneration information computing development program through the national research foundation of korea funded by the korean government msit 2017m3c4a7068179 .
robert feldt acknowledges the projects tocsyc swedish knowledge foundation kks num.
and baseit swedish science council vr num.
for funding parts of the work of this paper.