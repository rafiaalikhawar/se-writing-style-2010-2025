autoconfig automatic configuration tuning for distributed message systems liangbao school of computer science and technology xidianuniversity xi an shaanxi china baoliang mail.xidian.edu.cnxin liu departmentof computer science universityof california davis davis california usa xinliu ucdavis.edu zihengxu school of computer science and technology xidian university xi an shaanxi china .combaoyinfang school of computer science and technology xidianuniversity xi an shaanxi china fbyxdu .com abstract distributed message systems dmss serve as the communication backbone for many real time streaming data processing applications.
to support the vast diversity of such applications dmssprovide a large number of parameters to configure.
however itoverwhelmsformostuserstoconfiguretheseparameterswellforbetter performance.
although many automatic configuration ap proaches have been proposed to address this issue critical chal lengesstillremain totrainabetterandrobustperformancepre dictionmodelusingalimitednumberofsamples and2 tosearchfor a high dimensional parameter space efficiently within a time constraint.
in this paper we propose autoconfig an automatic configuration system that can optimize producer side throughputondmss.autoconfigconstructsanovelcomparison basedmodel cbm that is more robust that the prediction based model pbm usedbypreviouslearning basedapproaches.furthermore au toconfig uses a weighted latin hypercube sampling wlhs ap proach to select a set of samples that can provide a better cov erage over the high dimensional parameter space.
wlhs allows autoconfigtosearchformorepromisingconfigurationsusingthe trainedcbm.wehaveimplementedautoconfigonthekafkaplatform and evaluated it using eight different testing scenarios deployedonapubliccloud.experimentalresultsshowthatourcbmcanobtainbetterresultsthanthatofpbmunderthesamerandomforestsbasedmodel.furthermore autoconfigoutperformsdefaultconfigurationsby215.
onaverage andfivestate of the artcon figurationalgorithms by .
.
.
permission to make digital or hard copies of all or part of this work for personal or classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributedfor profit or commercial advantage and that copies bear this notice and the full cita tiononthefirstpage.copyrightsforcomponentsofthisworkownedbyothersthan acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand or a fee.request permissions frompermissions acm.org.
ase september montpellier france 2018association for computing machinery.
acmisbn ... .
software and its engineering search based software engineering softwaremaintenancetools computingmethodologies classification and regressiontrees keywords distributed message system automatic configuration tuning comparison based model weighted latin hypercube sampling acmreferenceformat liang bao xin liu ziheng xu and baoyin fang.
.
autoconfig automatic configuration tuning for distributed message systems.
in proceedingsofthe201833rdacm ieeeinternationalconferenceonautomatedsoftware engineering ase september montpellier france.
acm newyork ny usa pages.
introduction distributedmessagesystems dmss suchaskafka rabbitmq activemq and rocketmq have been widely adopted as the underlying communication backbone to support manydifferent real time stream data processing applications.
these ap plications have vastly diverse characteristics.
to support such diversities dmssprovidealargenumberofparameters forusersto configure.
for example both kafka and rocketmq have pa rametersthat an application can configure .
the configuration of these parameters significantly affects the applicationperformanceondmss .forinstance changing thebatch.size parameter of kafka for three different test cases and7 seetable2 from1to60canresultin5 10timesthroughputgaingivenanapplicationworkload asshowninfigure1.such variationisevenmoresignificantiftheworkloadisrecurringona daily base which is likely especially for realtime reactive applicationssupportedbydmss.ontheotherhand whenfixingthevalueofbatch.size changing the num.network.threads parameter from to20onlyleadsto8 throughputvariation whichmeansthatnum.network.threads has much less influence on throughput than thatofbatch.size .
pursuing good performance of different applications on dmss is non trivial as dmss are complex systems with a large numberof configurable parameters that control nearly all aspects of their authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france l. bao et al.
7kurxjksxw v 7hvw fdvh qxp qhwzrun wkuhdgv edwfk vl h 7kurxjksxw v 7hvw fdvh qxp qhwzrun wkuhdgv edwfk vl h 7kurxjksxw v 7hvw fdvh qxp qhwzrun wkuhdgv edwfk vl h figure1 performance surfaces of three differenttest cases on kafka runtime behaviors.
previous investigations have found that optimizingadmstomeettheperformancerequirementofanapplica tionhasfarsurpassedtheabilitiesoftypicalsystemusers .asaresult usersoften haveto acceptthedefaultsettings .alter natively many organizations choose to hire expensive experts toconfigure the dmss for their applications.
unfortunately manualconfiguring is labor intensive time consuming and often subop timal.
therefore there is a dire need for automatical application configurationson dmss.
becauseconfigurationparametershavehighdimensionality naive exhaustive search is infeasible.
recently learning based configuration has received much attention.
in general such an approach constructsaperformance predictionmodelusingtrainingsamplesofdifferentconfigurations andthenexploresbetterconfigurationsusingsome searchingalgorithms.
although previousstudies onlearning based configuration have shown promising results onecriticalchallengeistoconstructbetterandmorerobustprediction modelswithalimitednumberofsamples becausegeneratingsamplesneedstorundmsandistimeconsuming.anotherchallengeistosearchformorepromisingconfigurationsinahigh dimensionalparameterspace.
toovercomethesechallenges weproposeautoconfig anautomaticconfigurationsystemthataimstoconfiguresystemparam eters for a specific dms within a given time constraint.
autocon fig consists of three important steps initial sampling and modeltraining explorationandexploitation e e process andbestconfigurationselection.first weconstructacomparison basedmodel cbm fromexistingsamples.themotivationofconstructingcmb instead of a conventional prediction based model pbm is that the ultimate goal of configuring a dms is to find the best config urations rather than to predict the performance of a given systemunderdifferentconfigurations.inanotherword wecareabouttherelative performance instead of the absolute one.
as discussed insection4 cbmismorerobustthanpbm becausecbmusescom binationsoftheoriginaltrainingsamples whichresultsinasignificant larger number of available samples than pbm.
furthermore autoconfig searches for better configurations by integrating the cbm and e e process under the time constraint.
the key of the autoconfig is to train a more robust cbm using combinations of original samples and to search for promising area of parameter space using skewed random sampling strategy i.e.
wlhs .
it alsoneeds to balance the effort on the initial sampling and the e eprocess.
insummary our workmakes the followingcontributions we propose a novel comparison based performance model cbm whichisdifferentfromtheconventionalpredictionbased model pbm used by previouslearning basedapproaches.
cbm allows us to obtain more combinatorial training samples from original samples and thus can produce a more accurate and robust prediction model given a number of original samples.
wedeveloptheautoconfigalgorithm.itusesweightedlatinhypercube sampling wlhs to generate effective samplesin the high dimensional parameter space and multiplebound and searchtoselectpromisingconfigurationsintheboundedspacesuggestedbytheexistingbestconfigurations.
weevaluatetheperformanceofautoconfigthroughexten siveexperimentsusingeightdifferenttestingscenariosina publiccloud.weshowthatautoconfigoutperformsdefault configurationsby215.
onaverage andfivestate of the art tuning algorithms by .
.
.
relatedwork dms configuration has received much attention from both indus try and academia.
previous studies on this problem can be classi fiedintofourcategories model based measurement based search based and learning based configuration approaches as discussednext.
the best practices on dms configuration are also reviewedatthe end of this section.
.
model based configuration in model based configuration an analytical model is constructedon the early cycle of the development of a software system .balsamoetal.
reviewedthemodel basedsoftwareperformanceprediction methods before .
koziolek et al.
surveyed themodel basedperformanceevaluationmethodsforcomponent basedsoftware systems.
commonly used models include queueing network petrinet palladiocomponentmodel pcm and others .
model based configuration has two main limitations.
first it relies on analytical or design models derived from mathematicaltheories or software architecture abstraction which are typically authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
autoconfig automatic configuration tuning for distributed message systems ase september montpellier france coarse grained and could be imprecise .
second model based configurationcannotadaptasframeworksorhardwareevolve.oncethearchitectureofaspecificsoftwaresystemchanges theexistinganalytical models may not work and need to be redesigned.
the sameissuearisesasclustersevolvewithnewprocessors memory andstorage technologies .
.
measurement based configuration measurement based configuration aims to evaluate software application focusing on the performance quality features such as responsetimeandthroughput.theseapproachesmainlyrelyonsta tistical inferencing techniques to derive performance predictionsbasedonbenchmarkedmeasurementdata suchas .
in this process many strategies suchassampling profiling symbolicevaluation and feature interactions are proposed to accelerate per formance inference.
the prevalent dms benchmarks are specjm s2007 jms2009 ps ddsbench and two benchmark suitesintroduced in .
measurement basedapproacheshavetwomaindrawbacks.first they collect program profiles to identify performance bottlenecks which often fail to capture the overall program performance .
second most of these approaches lack generality as they are applicableonlytospecificapplicationscenariosorinfrastructures .
.
search based configuration search basedapproachesregardconfigurationproblemasablack box optimization problem and uses search algorithms to solve it such as in .
the objective of the search isto evaluate candidate solutions of different parameters to mini mizeanobjectivefunction.thesearchstrategiesincluderecursiverandom search rrs heuristics evolutionary algorithms hill climbing algorithms and recursive bound search .
search based configuration is simpler and more general comparedtootherapproaches becauseittakesthetargetsoftwaresys temasablack boxfunctionanddoesnotneeddetailedinformationabouttheinternals.however theseapproacheshavetoruntheap plication at each iteration of the search which is time consuming andimpractical whenoptimizinglarge scaleproductionsystems.
.
learning based configuration most relevant to our work is learning based configuration.
theseapproaches try to construct performance prediction models firstby observing a large collection of running results under differentparameterconfigurations andthenapplysomesearchalgorithms tofindtheoptimalconfigurationbasedonthesemodels.forexample sarkaretal.
adaptedprogressiveandprojectivesamplingstrategiestoperformancepredictionofconfigurablesystems.guoet al.
used cart to predict the performance of configurablesystems.zhangetal.
proposedanalgorithmbasedonfourierlearning fl for the performance prediction of configurable sys tems.sayyadetal.
employedacombinationofstaticandevo lutionary learning of model structure to find sound and optimumconfigurations.soltanietal.
employedanartificialintelligenceplanning technique to automatically select suitable features thatsatisfytheusers businessconcernsandresourcelimitations.tang provided an approach to find better configurations using ran domforestsmodelandgeneticalgorithmbasedsearchingstrategy.
tantithamthavorn et al.
applied caret method to investigate the performance of defect prediction models.
nair et al.
pro posed a spectral learning based method to explore the configura tion space efficiently to find the measurement that reveal key per formance characteristics.
jamshidi et al.
proposed a gaussianprocesses gps based method to iteratively capture posterior dis tributions of the configuration spaces and sequentially drive theexperimentation.
bei et al.
integrated the random forest andgenetic algorithm to automatically configure the hadoop configuration parameters for optimized performance for a given application and cluster.
jamshidi et al.
conducted an empiricalstudyonfourpopularsoftwaresystemstoidentifythekeyknowl edge pieces that can be exploited for transfer learning.
chen et al.
proposed a transfer learning based approach to facilitate theconfigurationof large scale computing systems.
learning basedapproachesrequireconsiderablenumbersofsamplestoconstructausefulperformance predictionmodelforasoftware system .
this requirement is challenging here because only a limited set of samples can be acquired under a given timeconstraint sinceitistimeconsumingtosampleonadeployedpro duction system.
to address this issue we need to derive a robustprediction model by sampling the high dimension configurationspace wisely which motivated us to propose a novel comparison based model and a weighted lhs sampling method.
.
best practices on dms configuration there have been many best practices devoted to configure dmssfromdifferentperspectives.forexample johnetal.
focusedontwopopulardmss kafkaandamqp andexploredthedivergence intheirfeaturesaswellastheirperformanceundervariedtesting workloads.
rubio conde et al.
compared the performance of twoopensourcemiddleware namelyzerociceandamqp inthe contextofmedicalsystems.dobbelaereetal.
conductedaqual itativeandquantitativecomparisonofthecommonfeaturesofthetwomessagesystems namelyrabbitmqandkafka.prazeresetal.
proposedamessage serviceorientedmiddlewarenamedfot msomforthefogofthings fot paradigm.xuetal.
providedquantitativeevidencefortheover deliveredflexibilityrepresented by configuration parameters through the study of the large scale configurationsettingsofrealusers.leetal.
madeathoroughevaluation of different configurations and performance metrics ofkafka in order to allow users to avoid bottlenecks and leveragesomegood practice for efficient streamprocessing.
problem statement in this paper we study the configuration problem for distributedmessage systems short for cdmsproblem .
as illustrated in figure a distributed message system e.g.
kafka rocketmq rab bitmq etc.
is often deployed on a cloud environment comprised ofacollectionofinterconnectedvirtualmachines vms .itserves as a real time streaming data pipelines that transfer messages formany systems.
once a dms has been deployed the user needs to authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france l. bao et al.
application producer producer producerconsumerconsumer distributed message system dms configuration parameterstransaction system cloud environment figure2 overviewof cdmsproblem specify its configurations according to the specific application scenario and such configurations have a significant impact on theperformance of the dms .
the goal of cdmsproblem is to find an optimal configuration thatmaximizesthe producer side throughputofadmswithinapredefined time period given a specific dms an application and the underlying runtime environment.
the reason we choose the throughput metricisbecauseitcantellhowmuchdataadmscan storeinaunitoftime whichcharacterizesthemostimportantca pabilityofadms.othermeasurement suchasend to endlatency i.e.
how long does it take for a message to go though the dms can be also introduced as a target metric because our approach is with generality and is independent to the metric being optimized.
specially cdmsproblemhas the followingcomponents.
application an application represents a real time streaming dataproducingsystem producers thatwantstotransfermessagestotransaction systems consumers .wemodel it as a tuplea angbracketleftl p s r q angbracketright where lrepresents the length of a message pis the number of producers sindicates the message sending mode synchronous or asynchronous rdenotes the message receiving mode with or without acknowledgement and qdesignates the number of available servers queues .
runtime environment runtime environment is the executionenvironmentprovided to a dms.
wemodel it as a tuple e angbracketlefto f m d w angbracketright where odenotes the number of cpu cores f is the cpu frequency mrepresents the physical memory size d indicates the available disk space and wreflects the networking setup.
configuration and throughput let c c c2 cn be the configuration of a dms.
for example one can configure as many as parameters in a kafka platform such as the num ber of network processing threads the number of i o processingthreads maximum number of requests in a queue memory buffersize etc.
as shown in table .
given a configuration cfor an application aand its environment e the throughput is denoted as tp a e c .
timeconstraint inpractice thetimeforconfigurationtuning isoftenrestricted.wedefinethisrestrictedtuningtimeas timeconstraint denoted as tc.
any solution to the cdmsproblem must terminatewhen tcis met.
best configurationautoconfig application runtimedmstime constraint initial sampling and modeltraining exploration and exploitation best configuration selectioniterationstransaction systems figure3 overviewof autoconfigsystem insummary the cdmsproblemcan be stated as follows max c cbtp a e c s.t.tuning time tc where statesthatthegoalof cdmsproblemistofindaconfigurationcthatmaximizesthroughputofadmsforagivenapplicationaanditsenvironment e.inc thevalueofeachcomponent parameter cimust be within cb the configuration bound predefined by the dms.
the constraint says that any tuning processofa solution must terminate after a tcamountof time.
autoconfigapproachfor cdms problem the key idea of our approach is to generate a set of samples thatis used to train a comparison based model and search for morepromising configurations using the trained model.
figure illus tratesthreeimportantstepsonourproposedautoconfigapproach.
initial sampling and model training .
this step generates a set of different configurations using classic latin hypercube sampling lhs method first given the configuration bound cb for each parameter.
it then runs the target application on the dms withtheseconfigurations collectthroughputvalues andgenerate the initial training set.
finally we choose the best of bconfigurationsto generate the initial good configuration set.
exploration and exploitation.
the purpose of this step is to searchforbetterconfigurationsbyintegratingthecomparison basedperformance model and the exploration and exploitation e e process under the time constraint.
more specifically we first estimate the weight value for each configuration parameter using lassomethodandthetrainingset andinitiateourweighted lhs wlh s method.intheexplorationphase werandomlygenerateasetofconfigurationsusingourwlhswithincbtoformtheexplorationconfigurationset andtrainacomparison basedmodel cbm fromthetrainingset.intheexploitationphase weapplyawlhs basedbound and search algorithm bs to find potential better con figurationsnearknowngoodconfigurationsusingthetrainedcbmand generate the exploitationconfiguration set.
bestconfigurationselection .ateachiteration weupdatethe goodconfigurationsetbyselectingthebest bconfigurationsusing authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
autoconfig automatic configuration tuning for distributed message systems ase september montpellier france the trained cbm from the current good configuration set the explorationconfigurationset andtheexploitationconfigurationset.ifourtimeconstraint tc permitsmoretests werepeatourexplo rationandexploitationprocess.otherwise weruntheapplication on the dms with every configuration in the good configuration set and returnthe best one.
.
comparison based model thekeystepofalearning basedconfigurationapproachistocon structaperformance predictionmodelforthedms.typically sucha model is evaluated based on its accuracy or error.
the error per centagecan be computed as mmre predicted actual actual in this paper we propose a novel approach that constructs a comparison basedmodel cbm whichisdifferentfromthepredictionbased model pbm used by most of the previous learning basedapproaches as described in section .
.
specially given the fixedaande and two different configurations c 1andc2 our model denoted as f c c can compare the throughput values of the dms with c1andc2 f c1 c2 braceleftbigg t p a e c tp a e c t p a e c tp a e c thereareanumberofadvantagesofusingacomparison based approach performance comparison is extremely robust since it is only mildly affected by outliers .
in the context of parame ter configuration a practitioner is often more interested inknowingtherank bycomparison ratherthanthepredictedperformance scores.
performance comparison is the ultimate goal of our cdmsproblem.ausermayjustwanttoidentifythebestconfigura tions rather than to predict the performance of a given system under different configurations.
for example an administratortryingtooptimizeakafkaplatformistosearchforasetofconfigurationsthatcanobtainmaximumthroughput andis not interested in the whole configuration space.
performance comparison increases the number of availabletraining samples significantly .
suppose we have a set containingnsamplesbyrunning ndifferentconfigurations.we can generate parenleftbig n parenrightbig n n 2samples by simply grouping any two configurations and their throughput values.
we willdemonstrateintheexperimentsectionthatthecomparisonbasedmodelhasbetterperformancethanconventionalprediction basedapproaches sincethenumberoftrainingsamplesavail abletoconstructacomparison basedapproachisincreasedconsiderably.
besides insteadofusingresidualmeasuresoferrors asdescribed inequation whichdependonresiduals r actual predicte we use a rank based measure .
once the comparison based model is trained the accuracy of the model is measured by sort ingthe throughputvalues of ndifferentconfigurations that is tp a e c tp a e c tp a e c n parameter xparameter y figure4 twosets of lhs samples for a 2d space obviously equation can be generated by simply using somesortalgorithm e.g.quick sort and continuously applying ourcomparison basedmodeltocomparethethroughputvaluesbetweentwo configurations.
after obtaining the predicted rank order it iscomparedtotheactualrankorder.therankaccuracy ra isthuscalculated using the mean rank difference ra k k summationdisplay i rank yi rank tp a e c i n wherenisthenumberofallconfigurations kisnumberofoptimal configurationswewanttofindfromthese ncandidates.thismeasure simply counts how many of the pairs in the first kelements of the test data were ordered incorrectly by the prediction model andmeasuresthe averageof magnitude of the ranking difference.
.
weighted latin hypercube sampling oneimportantcomponentinourautoconfigalgorithmisthesampling strategy.
because the configuration parameter space is highdimensional naive sample methods such as random or grid sam pling can become very expensive and is hard to provide a goodcoverage in it especially with a small number of samples .
to address this issue we note that the latin hypercube sampling lhs performs better compared to random or grid sampling because it allows each of the key parameters to be represented in afullystratifiedmanner nomatterwhichparametersareimportant .
specifically lhs divides the range of each parameter into k intervals and take only one sample from each interval with equal probabilities .figure4illustratestwosetsoflhssampleswithfiveintervalsina2ddimension onedenotedbydotsandtheotherbytriangles.
tomakelhsmoreefficient weobservethattheperformanceof a dms is often dominated by a few key configuration parameters e.g.seefigure1 andtheimpactofaninfluentialparameter svaluesontheperformancecanbedemonstratedthroughcomparisonsofperformances regardlessotherparameters values .based onthisobservation weproposedweightedlhs wlhs whichex tendsthestandardlhsalgorithmtotakeintoaccountknowledgeabout the correlations between configuration parameters and sys temperformance.suchcorrelationscorrespondtoalinearapproxi mationoftheobjectivefunction whichcanbeestimatedusingthesampled points.
the correlation information i.e.
weights can be authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france l. bao et al.
combinedwiththelhstogenerate skewedrandomsearchsamples thatarelikely to lead to moreefficient searches.
among many models we adopt linear regression a simple yet effectivestatisticalmethod tomeasurethestrengthoftherelationshipbetweenoneormoredependentvariables y andeachofthe independent variables x .
these relationships are modeled usingalinearpredictorfunctionwhoseweights i.e.
coefficients arees timated from the data.
more specifically wlhs employs a regu larized version of least squares known as lasso to estimatetheweightforeachconfigurationparameter.lassocanreducetheeffectofirrelevantvariablesinlinearregressionmodelsbypenaliz ingmodelswithlargeweights.themajoradvantageoflassooverother methods is that it is interpretable stable and computationally efficient .
there are also many practical and theoretical studies that have proven its effectiveness as a promising featureselection algorithm .
lasso performs l1 regularization which adds a penalty equal to the sum of absolute weights times a constant to the regression error.
this type of regularization can result in sparse models withfewnonzerocoefficients.therefore lassoregressionisableto perform variable selection in the liner model.
variables with non zeroregressioncoefficientsvariablesaremoststronglyassociated withtheresponsevariable.
istheturningfactorthatcontrolsthe amount of regularization.
as the value of increases more coefficients will be set to zero provided fewer variables are selected andsoamongthenonzerocoefficients moreshrinkagewillbeem ployed.wlhsusesthelassopathalgorithmtodeterminetheorderofimportanceoftheconfigurationparametersofadms.thealgo rithmstartswithahighpenaltysettingwhereallweightsarezeroand thus no configurations are selected in the regression model.it then decreases the penalty in small increments recomputes the regression and tracks what configurations are added back to the model at each step.
wlhs uses the order in which the configura tions first appear in the regression to determine how much of animpact they have on the target metric e.g.
the first configurationselected is the most important .
the key ideas in wlhs is that samples should follow the correlation pattern exhibited by the lasso based weight calculation.
ifthe past measurements show that smaller values of configurationc i ctendtomaketheperformancebetter i.e.
astrongpositive correlation then smaller values are more important than largerones.
hence we should sample more on the smaller values for pa rameterc i. to realize the above weighted sampling idea we use a truncatedexponentialdensityfunctionproposedbyxi forgeneratingthe samples.
for each component parameter c iof a configuration i n weassumeatruncatedexponentialdensityfunctionof the form f c d x de icx on sampling range x where iis determined by lasso method and represents the correlation between performance andc ithroughallpastobservations.parameter cisusedtoreflecthow aggressivetheuserwantstheweightedsamplingtobe.parameterdis the normalizing factor so that f c d x isa density function.
while sampling we need to divide the interval intok intervals with equal probability k. letz jbe thej th dividing0 parameter values00.
.
.
.
.
.06sampling possibility figure5 applying wlhs to a parameter ciwith i .
point with j kandz0 aandzk b. then j k integraldisplayzj z0f c d x dx wenowneedtodrawonepoint jfromgiveninterval which follows the conditional probability f c d x h where h isthe scaling constant integraldisplayzj zjf c d x hdx todraw j wefirstdrawarandomnumber u fromtheuniform distributionin andneed to satisfy the followingequation u integraldisplay j zjf c d x hdx finally wehave j log e iczj u e iczj e iczj ic thesolutionsforintermediatevariables d zj hcanbefoundin .
it is worth noting that the weighted lhs takes advantage of knowledge about the correlations between configuration parameters and system performance from past observations.
the weight can be based on other metrics rather than correlation also onecan choose other density functions rather than exponential to re alize a similar concept of assigning different weights on differentsamplingregions .
to explain our wlhs method more intuitively suppose there is a particular parameter c iwith a positive correlation i .
tothethroughput.wlhsthatusesthetruncatedexponentialden sity function equation would divide the sampling space say into3equal probabilityintervals asshowninfigure5.we can observe from figure that clearly smaller values are stressedmore under weighted sampling.
the constant c 5in equation can be determined through some preliminary studies.
larger c wouldresultin moreaggressiveweighted sampling .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
autoconfig automatic configuration tuning for distributed message systems ase september montpellier france .
autoconfigalgorithm basedonthecomparison basedmodelandtheweightedlhsmethod wecanimplementourautoconfigalgorithm.thedetailedprocess isspecified in algorithm .
algorithm1 autoconfig dms cb tc require dms the target dms cb configuration bounds tc time constraint.
generate hdifferentconfigs using lhs within cb run these hconfigs on dms collect throughput results and generate the initial training set t b thebestbconfigs in t whiletcpermits moretests do calculate weight set for config parameters using lasso method and t generate hdifferentconfigsusingwlhsand withincb and then choose bout ofhconfigs randomly run these bconfigs on dms collect throughput results and generate the explorationset ep t t uniontextep traina comparison based prediction model musingt foreachconfig ci bdo set exploitationset ei selecthconfigs using wlhs in the bounded space of ci choose the best config c ifrom these hconfigs using m rundmswithc i collect the throughput tp i ei ei uniontext c i tp i end for b thebestbconfigs from b uniontextep uniontextei t t uniontextei end while returnthebest config in b autoconfig initially samples hconfigurations using standard lhs line1 andpicks bconfigurationswiththebestperformance line .
with these hinitial samples we use lasso method to calculate weight set for configuration parameters line .
to explore the configuration space we apply wlhs with andhintervals and choose bconfigurations randomly line .
after that werun these bconfigurations on the dmsand use the resultsto generatethe explorationset ep line .
based on already generated samples we train a comparisonbased prediction model based on random forests line which indicatesbetterperformanceamongmanydifferentmachinelearn ingalgorithms.
toexploitthepreviously foundbestconfigurations weapplya bound and search bs algorithm to find potential better configurations near already known good configurations line .
this strategy works well in practice because there is a high possi bilitythatonecanfindotherconfigurationswithsimilarorbetterperformancesaroundtheconfigurationwiththebestperformanceinthesampleset .morespecifically foreachconfiguration c i inb bs generates another set of samples in the bounded space aroundci foreachparameter cjinci bsfindsthelargestvaluec1c3 c2 c4 c5 parameter xparameter ybounded space figure6 theboundandsearch bs algorithmina2dspace cl j lower bound that is represented in band is smaller than that ofci.
it also finds the smallest value cuj upper bound that is represented in ciand that is larger than that of ci.
the same bounding mechanism are carried out for every component parameter in ci.
figure illustrates this bounding mechanism of bs with 2d space fivedots c1 c5 .afterdeterminingtheboundfor ci weuse wlhs again to divide each bound into hintervals and generate h samplescloseto ci line12 .giventhese hconfigurations weuse the trained prediction model mto choose the best configuration c i line and then collect the corresponding throughput tp i by running the dmswithc i line .
after that the sample c i tp i is added to the exploitation set ei line .
we repeat theseboundandsearchstepsuntileveryconfigurationin bistested andupdate bwiththebest bconfigurationsfrom b uniontextep uniontextei line .
we refine mand by adding new samples from explorationandexploitationphasestothetrainingset t line8and18 .
oncethetimebudgetonexplorationandexploitationismet we stopsearchingand returnthe best configuration in b line .
to satisfy the overall time constraint we divide the autoconfigalgorithmintotwophases i.e.initialsampling andexplorationand exploitation.
suppose the time constraint is tc the proportion of time spent in these two phases is denoted as and respectively where 1and0 .
let the average timeofrunninganapplicationwithoneconfigurationonthe dms equals to t we have h tc t and andb tc iter t where iterrepresent the approximate iterations in the exploration and exploitation phase and h bare hyperparameters in autoconfig algorithm.
experiments we have implemented our approach and conducted extensive experimentsunderdifferenttestingscenarios.thesourcecodecanbefoundinourprojectwebsite this section we first describe our experiment setup and thenpresent the experimental results to prove the efficiency and effec tivenessof the proposed approach.
.
experimental settings runtime environment.
we conduct our experiments on a publiccloudinfrastructurenamedaliyun .weuse4aliyunecsinstances authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france l. bao et al.
table1 performance relevantparameters in kafka parameters default value num.network.threads num.io.threads queued.max.requests num.replica.fetchers socket.receive.buffer.bytes socket.send.buffer.bytes socket.request.max.bytes buffer.memory compression.type batch.size linger.ms none consisting of two types memory type instance r5 for producers and3generaltypeinstance g5 fordmsnodes.theproducernodeisequippedwithan8 coreintelskylakexeonplatinum8163 .5ghz processor 16gb ram and 50g disk.
each of the dms nodes is equipped with a core intel skylake xeon platinum .5ghz processor 8gb ram and 50g disk.
both instances havecentos6.
64bit installed.allofthevminstancesareconnected via a high speed1.5gbps lan.
system and configuration setup .
we choose kafka as our experimental system.
kafka is a distributed streaming platform for pub lishing and subscribing to streams of records which is similar toa message queue or enterprise messaging system.
we choose kafkabecauseitisawidelyadoptedopen sourcedistributedmessage system.
basedonthekafkamanual andpreviousstudies we identify out of parameters that are considered critical to the performance of kakfa platform as listed in table .
notethat even with only parameters the search space is still enor mous and exhaustive search is infeasible.
the reason we choosea small subset of parameters that have great impact on perfor mance instead of all configurable parameters is because reducing the number of tuning parameters can reduce the search space exponentially and most existing automatic configuration approach es also adopt this feature selection strategy.
it s worthnoting that because of the linear computing complexity of wlhsonthenumberofparameters ourapproachisgeneralenoughand worksfor high dimensional parameter spaces.
benchmark.
based on the previous kafka testing experiences we design eight testing scenarios with the combination s of different numbers of producers message sizes and message acknowledgement modes.
table lists these testing setups.
.
baseline algorithms to evaluate the performance of autoconfig we compare it with five state of the art algorithms namely random search best config rfhoc hyperopt andsmac .weprovide abriefdescriptionforeachalgorithmandreportitshyperparame ters if necessary as follows randomsearch random isasearch basedapproachthatexplores each dimension of parameters uniformly at random.
it ismore efficient than grid search in high dimensional configurationspaces and is a high performance baseline as suggested in .table2 testingscenarios of our experiment no.
of producers messagesize ackmodes1 .1kb .0kb .1kb .0kb .1kb .0kb .1kb .0kb 1in kafka ack 1means that the leader will write the recordto its local logbut will respond without awaiting full acknowledgement fromall followers.ack 1means that the leader will wait for the full set ofin syncreplicasto acknowledge the record.for moredetails see in .
bestconfig2is a search based approach that uses divide anddivergesamplingandrecursivebound and searchalgorithmtofind abest configuration.
wefollowthe suggestions in .
rfhoc is a learning based approach that constructs a prediction model using random forests and a genetic algorithm to auto maticallyexploretheconfigurationspace.weusethehyperparameterssuggested in .
hyperopt 3isalearning basedapproachbasedonbayesianoptimization.
it is widely used for hyperparameter optimization.
we usethe suggested settings in .
smac4is a learning based method using random forests and anaggressiveracing strategy.
inautoconfigalgorithm wesetthenumberofiterationsto forrandomforestmodel anddonotlimitthedepthofthedecisiontree and the number of available features for the tree the propor tionsoftimespentininitialsamplingande ephasesare0.4and .
i.e.
.
.
thevaluesofothertwohyperparameters i.e.
h andbforeach testing scenario areset to and respectively.
for each run in our experiments every algorithm is executed under the same time constraint and stops once the constraint is met.
.
evaluation metrics we consider two performance metrics in our experiments for performance evaluation namely rank accuracy ra and throughput tp .raisametricthatevaluatesthepredictionmodel andtpis theultimate performance metric for autoconfig.
rank accuracy ra .
as defined in section ra is used here toevaluatethequalityofapredictedranking theprediction fora set of configurations by comparing it with the corresponding real ranking the truth .
note again that the ranking ability of a pre diction model is more important than its prediction accuracy in autoconfig because we need to use a prediction model to choose the best configurations in each exploitation phase of algorithm line in algorithm .
throughput tp .
tp is the rate of successful message deliverytoadistributedmessagesystem i.e.producer sidethroughput .
2code is available from 3code is available from 4code is available from authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
autoconfig automatic configuration tuning for distributed message systems ase september montpellier france table the ra values of random forests based cbm and pbm no.
of samples in trainingsetra cbm ra pbm best?
cbm best?
pbm .
.
no no .
.
yes no .
.
yes no .
.
yes yes .
.
yes no .
.
yes yes .
.
yes yes .
.
yes yes .
.
yes yes .
.
yes yes 1whetheror not the model can find the best configuration in the test thetpimprovementofanalgorithmsoverthebaselinealgorithm incomparison is defined as imp baseline tp tpbaseline tpbaseline wheretpbaselineisthethroughputofthebaseline and tpisthat ofthe algorithm being evaluated.
to ensure consistency we run each application five times and calculate the average of these five runs.
the standard deviation ofthe execution time is .
or smaller which indicates the stabilityofthe performance.
.
experiment results ra.thefirstexperimentistoevaluatethequalityofrankingwithdifferent prediction models.
we first construct a sample pool by running kafka with randomly generated different configura tions.
after that samples are selected randomly and then re movedfromthepooltogeneratethe testingset.last werandomly choose10 100samplesfromthepooltoconstruct10differenttrainingsets.tworandomforestsbasedpredictionmodels one is a comparison based model cbm and another is a prediction basedmodel pbm aretrainedusingthesetrainingsets.wethenuse these trained models to find the best configurations fromtesting set respectively.
table lists the ra values on cbm and pbm.
we find that all these ra values of cbm outperform those of pbm and the average improvement is .
.
the results also in dicatethatthecbmcanfind9bestconfigurationsoutof10exper iments wherepbm only finds out of .
throughput.
given a fixed time constraint tc for each testing scenario we run six different tuning algorithms plus defaultconfiguration independently.
table lists the throughput mb s .
asexpected thedefaultconfigurationdoesnotperformwell.our algorithm achieves an average of .
improvement over the default configurations.
furthermore autoconfig outperforms allotherfivealgorithms .
.
improvementoverrandom .
.
improvementoverbestconfig .
.
improvement over rfhoc .
.
improvement over hyperopt and9.
.
improvementoversmac.
finally we plot the overall throughput improvement percentage of bestconfig rfhoc hyperopt smac and autoconfig infigure using the random algorithm as the baseline.
in the fig ure7 x axisliststheeighttestingscenariosand y axisrepresents the improvement percentage over the random algorithm.
we ob serve that compared with the random algorithm our approach achieves11.
.
improvementamongalltestingscenarios.
autoconfigachievesanaverageof20.
improvementoverrandom .
improvement over bestconfig .
improvement over rfhoc .
improvement over hyperopt and .
im provement over smac.
we can conclude from figure that au toconfig achieves stable and significant improvements compared with the other five algorithms.
another interesting observation fromfigure7isthattherandomsearchachievessurprisinglygoodresults in our experiments.
this is consistent with the findings of bergstraand bengio in .
.
threatsto validity internal validity to increase internal validity we performed a controlledbenchmarkexperimentbyexecutingeachtestcasefive times and calculate the average of these five runs.
such methodcan avoid misleading effects of specifically selected test cases andensuresthestabilityofthethroughputresultforeachscenario.inaddition we use the same sample set and the same random forestmodel with identical hyperparameter values to compare the rankaccuracy for the cbm and the pbm in each test case.
such results are reliable and can be treated as the ground truth.
in our experiments the value of the time proportion and they control the time ratio between model training and e e phases and in turndecide the hyperparameters handbof autoconfig are set to .
and .
respectively and the value of constant cin equation is setto5 assuggestedin .however thevaluesoftheseparam eters are domain specific and can be set by a domain expert.
n evertheless incaseswheretheprecisevaluesareunknown usingthemiddlevalue for and andthevaluesuggestedbyprevious study for c seems to be reasonable choices.
in addition we tried multiple values of these parameters in our experiments andobservedthatthegoodvaluesoftheseparametersthatcanleadtobetter throughputresultsaredifferentfromtest case to test case.
externalvalidity weaimedatincreasingexternalvalidityby choosingeighttestingscenarioswithvariousapplication levelpa rameters on a mainstream open source software kafka.
further more we are aware that because the two key components of au toconfig i.e.comparison basedmodelandweightedlhs aregeneralenoughandindependenttothesystemundertune sut theresultsof our evaluationsaretransferable to other dmss.
conclusion in this paper we propose autoconfig an automatic configura tion system to optimize throughput for dmss.
autoconfig con structs a novel comparison based model cbm different from theprediction basedmodel pbm usedbypreviouslearning basedap proaches.
intensive experiments show that our cbm can obtainbetter results than that of pbm under the same random forestsbasedmethodandagiventimeconstraint.furthermore theautoconfigalgorithmusesweightedlhsmethodtoselectasetofsamples that can provide a better coverage over the high dimensionalparameter space and searches for more promising configurations authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
ase september montpellier france l. bao et al.
table4 throughputresults mb s fromdifferentalgorithms with fixed time constraints no.tc h default imp default random imp random bestconfig imp bestconfig rfhoc imp rfhoc hyperopt imp hyperopt smac imp smac autoconfig .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
12345678testing scenarios 10010203040imp random bestconfig rfhoc hyperopt smac autoconfig figure7 performance comparison among differentalgorithms usingthetrainedcbm.indepthexperimentsonautoconfigdemonstrateitssuperiorperformancetoexistingfivestate of the artcon figurationalgorithms on eight differenttesting scenarios.
our further work includes refining our autoconfig approach by supporting the automatic selection of the appropriate hyper parameters i.e.
andc given a specific test case and a time constraint.wewillalsoinvestigatethepossibilityofapplyingourapproachtootherdmsssuchasrabbitmq activemq androcket mq.last wehope to abstract the proposed algorithm and releaseit as an automatic parameter configuring service for other config urablesoftwaresystems.