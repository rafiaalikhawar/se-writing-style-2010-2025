the importance of accounting for real world labelling when predicting software vulnerabilities matthieu jimenez university of luxembourg luxembourg matthieu.jimenez uni.lurenaud rwemalika university of luxembourg luxembourg renaud.rwemalika uni.lumike papadakis university of luxembourg luxembourg michail.papadakis uni.lu federica sarro university college london uk f.sarro ucl.ac.ukyves le traon university of luxembourg luxembourg yves.letraon uni.lumark harman university college london and facebook uk mark.harman ucl.ac.uk abstract previous work on vulnerability prediction assume that predictive models are trained with respect to perfect labelling information includes labels from future as yet undiscovered vulnerabilities .
in this paper we present results from a comprehensive empirical study of real world vulnerabilities reported in releases of three security critical open source systems linux kernel openssl and wiresark .
our study investigates the effectiveness of three previously proposed vulnerability prediction approaches in two settings with and without the unrealistic labelling assumption.
the results reveal that the unrealistic labelling assumption can profoundly mislead the scientific conclusions drawn suggesting highly effective and deployable prediction results vanish when we fully account for realistically available labelling in the experimental methodology.
more precisely mcc mean values of predictive effectiveness drop from .
.
and .
to .
.
.
for linux kernel openssl andwiresark respectively.
similar results are also obtained for precision recall and other assessments of predictive efficacy.
the community therefore needs to upgrade experimental and empirical methodology for vulnerability prediction evaluation and development to ensure robust and actionable scientific findings.
ccs concepts software and its engineering software defect analysis .
keywords software vulnerabilities machine learning prediction modelling acm reference format matthieu jimenez renaud rwemalika mike papadakis federica sarro yves le traon and mark harman.
.
the importance of accounting for real world labelling when predicting software vulnerabilities.
in proceedings of the 27th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august tallinn estonia.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august tallinn estonia association for computing machinery.
acm isbn .
introduction manually assessing large scale software systems for potential vulnerabilities is increasingly impractical given that such systems may consist of many millions of lines of code any of which might potentially contain vulnerability inducing faults.
automated vulnerability prediction addresses this scalability challenge by adapting and augmenting widely studied defect prediction techniques .
vulnerability prediction systems use both software metrics e.g.
imports function calls and developer metrics e.g.
developers per component which have also been used for defect prediction.
the vulnerability prediction approaches previously proposed in the software engineering literature are each modified from traditional defect prediction to the more specific problem of vulnerability prediction by training on datasets that contain known vulnerabilities and using a variety of machine learning techniques to classify code as either vulnerable or not vulnerable.
hitherto empirical studies of the effectiveness of these vulnerability prediction approaches have implicitly assumed that labelling information is available regardless of temporal constraints table summarises the evaluation method used to assess vulnerability prediction methods in previous work .
that is the methodology does not account for the gradual revelation of vulnerabilities over time the vulnerability labels used for training the prediction models need to be more realistically available at training time and not include those subsequently uncovered.
new techniques for converting apparently non vulnerable software faults into vulnerabilities are also discovered.
vulnerability detection is an adversarial process in which those who seek to exploit faults continue to innovate.
the perfect labelling assumption that all vulnerabilities known from time tonwards are available at all times even before t is clearly unrealistic a software engineer could only ever hope to train a predictive model on a partial ground truth that will include some degree of misclassification.
with the present state of the research literature on vulnerability prediction we do not know the impact of this unrealistic perfect labelling assumption because previous studies omit to discuss how they account for realistically available labelling at model training time.
to address this issue we reformulate the methodology used to empirically evaluate vulnerability prediction.
our reformulated methodology takes full account of the vulnerability labelling information that could reasonably and realistically be assumed to be available to train any predictive model.
esec fse august tallinn estonia matthieu jimenez renaud rwemalika mike papadakis federica sarro yves le traon and mark harman to realistically take account of the vulnerability information we train the predictive models at time t based on all but only those vulnerabilities known already discovered at time t and then evaluate the so trained model on its ability to predict vulnerabilities in subsequent releases.
we conducted a comprehensive empirical study of the three main previously proposed approaches applied each one to guide the learning phase of a set of five widely used machine learners evaluated on a single large corpus of real vulnerabilities consisting of vulnerable components and releases from three open source systems.
we study the effectiveness of these vulnerability prediction techniques in two scenarios firstly we train on the realistic scenario by assuming that mislabelling noise is inevitable at any given time t due to vulnerabilities unknown at time t. secondly we re evaluate the vulnerability predictors with the same experimental settings except that we make a more ideal perfect labelling assumption we assume that the training phase at time thas available to it all labelling of vulnerabilities available at all times even those that are discovered after time t .
our findings provide strong evidence that the perfect labelling assumption is not only unrealistic but that it can also mislead the scientific conclusions of any studies that make such an assumption.
therefore future work on vulnerability prediction needs to use the reformulated methodology in order to ensure the scientific reliability of the conclusions drawn.
we have taken steps to ensure that this claim is based on firm foundations through the selection of a broad set of approaches data sources techniques considered and assumptions made.
more specifically our conclusions are based on a study of vulnerabilities an empirical evidence base that is approximately four times larger than any of the one previously used by the originally proposed methods .
the vulnerabilities used in our study are drawn from real world systems and concern previously reported real world vulnerabilities thereby avoiding any potential effects due to artificial or otherwise simulated systems or vulnerabilities.
our study also includes all the model features introduced in each and all of the previous studies.
finally in order to be as comprehensive as possible in our evaluation with respect to the knowledge realistically available at model training time we also investigate a previously proposed but asyet unevaluated approach to improve predictive power.
that is we investigate the previous suggestion to include allfault data available to the model training phase including faults previously known to exist yet not currently known to induce any vulnerability.
the intuition for so is to exploit the potential link between bugs and vulnerabilities i.e.
bugs often provide indicators for vulnerabilities even when these are currently not yet known because no exploit has yet been found.
in summary the contributions of our paper are we present the largest comprehensive empirical study on vulnerability prediction to date.
we provide evidence that vulnerability prediction can be effective mcc mean values of .
.
and .
over the releases of linux openssl andwiresark respectively when making the assumption that perfect labelling is available to train the model.
more importantly we also show dramatically lower predictive effectiveness mcc mean values of .
.
and .
are achieved for linux openssl andwiresark respectively when we remove this unrealistic labelling assumption instead training the models only on vulnerability labellings that could realistically be available to the learner at model training time.
we investigate whether imbuing the training data with additional fault data from previously known faults that have not been determined to be vulnerabilities might enhance vulnerability prediction efficacy.
these results show little improvement mcc values are still below .
and indicate that more work remains to be done to develop deployable vulnerability prediction for real world systems.
background .
security vulnerabilities a security vulnerability is defined as a mistake in software that can be directly used by a hacker to gain access to a system or network by the common vulnerability exposures terminology .
such mistakes are usually unexpected behaviours backdoors insufficient security measurements or code omissions lack of defensive programming .
vulnerabilities are considered as of critical importance and their resolution is usually prioritized over other bugs.
to this end vendors usually make new releases in order to fix vulnerabilities faster and reduce their impact.
to support secure software products and vulnerability fixing vulnerabilities are usually reported in publicly available databases.
one such database is the national vulnerability database nvd which has been established by the national institute of standards and technology nist and u.s. government in order to encourage secure software development public disclosure and management of vulnerabilities.
nvd is built upon the cve list which is a list of entries containing an identification number a description and at least one public reference of the vulnerability.
thus every publicly disclosed vulnerability is referenced with a unique identifier called common vulnerability exposures cve number or id.
nvd enriches each cve entry with information such as the severity named as cvss and the type named as cwe of a vulnerability.
this data is continuously updated by the nvd staff .
.
predictive modelling for software security vulnerabilities predictive modelling is a process of forecasting future outcomes a.k.a.
target or dependent variable by using historical data.
each prediction model is composed by a number of predictors a.k.a.
independent variables that are deemed likely to influence predict the future outcomes.
once historical data has been collected for relevant predictors a prediction model can be generated using various techniques such as statistical analysis machine learning or search based algorithms.
previous studies have shown that predictive modelling can be used to aid software engineers in their activities ranging from project managment to software testing .
696the importance of accounting for real world labelling when predicting software vulnerabilities esec fse august tallinn estonia in the context of software security vulnerabilities predictive models have been used to classify part of the software as either predicted vulnerable or predicted non vulnerable with the ultimate goal to support engineering in testing and code review activities.
for example if one could identify with a high accuracy those part of the software that might be vulnerable engineers could prioritise their testing over testing other parts which are less likely to be vulnerable.
depending on the target analysis prediction models may focus on different granularity levels i.e.
one can predict vulnerabilities at line method component or package level.
in a sense the granularity level is the entity on the code based on which prioritization will be performed.
evidently different granularity levels offer different advantages .
for instance the line level granularity can be direct but can produce many false errors and can be too fine grained for the developers to identify issues.
in our study we adopt the file component granularity level following the findings of morrison et al.
who found that the file component level was sufficient for microsoft developers to work with.
this decision is also in accordance to what most of the previously published work does .
once established the granularity level the dependent variable should indicate whether a target component contains one or more vulnerabilities while the independent variables i.e.
predictors can be many and related to different aspects of the software and its production.
in the literature three main methods have been proposed to extract vulnerability predictors from historical data named as imports and function calls code and process metrics and bag of words .
these predictors can be used with traditional machine learning classifiers in order to build prediction models.
in our study we realise and investigate all the three methods to extract different predictors set as to the best of our knowledge there has been no study comparing these approaches on a level playing field so far and assess their effectiveness in combination with five different machine learners i.e.
adaboost j48 k nearest neighbourhood logistic regression and random forest .
.
methods to extract vulnerability prediction features here we describe the three main methods proposed in literature to extract from historical data the vulnerabilities predictors that can be used as input to automated vulnerability prediction systems.
imports and function calls neuhaus et al.
observed that vulnerable files tend to import and call a particular small set of functions.
based on this observation they suggested the first approach that implements vulnerability prediction.
this is a simple prediction model over the components imports and function calls.
in other words the imports and function calls are the training features.
to apply this prediction modelling technique one needs to extract the imports and function calls of the components under analysis.
in our experiment we retrieve this information by traversing the abstract syntax trees asts of the files.
following the recommendation of neuhaus et al.
we use imports and function calls as a separate set of features and therefore trained two models one per each set .code and process metrics shin et al.
used code metrics related to code complexity code churn and developer activity to build vulnerability prediction.
according to these studies the combined use of these metrics gives the best results.
in summary the features used by this approach are the following complexity and coupling linesofcode lines of code preprocessorlines preprocessing lines of code commentdensity ratio lines of comments to lines of code countdeclfunction number of functions defined countdeclvariable number of variables defined cc sum avg max sum average and max cyclomatic complexity scc sum avg max strict cyclomatic complexity cce sum avg max essential cyclomatic complexity maxnesting sum avg max maximum nesting level of control constructs fanin sum avg max number of inputs i.e.
input parameters and global variables to functions fanout sum avg max number of outputs i.e.
assignments to global variables and parameters of function calls.
code churn added lines modified lines and deleted lines in the history of a component.
developer activity metrics number of commits impacting a component number of developers modified a component current number of developers working on a component .
we computed the above metrics by analysing the program ast and the git history.
bag of words this approach treats code as a set of words.
it tokenizes the code and puts every token into a reference bag along with its appearance frequency.
it is known as text mining and has been suggested for vulnerability prediction .
the features are the appearance frequency of the tokens i.e.
unigrams in the code of the components.
as the features dimensionality explodes quickly reducing it is mandatory .
to this end previous studies discretized the frequency of tokens to make them binary using the method of kononenko .
related work vulnerability prediction has been attempted in previous studies differing from each other mainly from the predictors used the subject and the validation carried out.
in this paper we investigate all previously proposed predictors sets creating a different prediction model using each for each set as described in section .
and validate them as done in previous work and also in a more realistic scenario.
table summarises and compares the key aspects of related work with respect to our work including their validation procedure and whether they consider mislabelling noise.
we can observe that all studies perform a cross validation which does not consider the temporal aspect and therefore it is unrealistic.
697esec fse august tallinn estonia matthieu jimenez renaud rwemalika mike papadakis federica sarro yves le traon and mark harman table comparison with previous work.
all studies perform cross validation to assess the effectiveness of the prediction models and only three of them also consider a release based validation.
cross validation does not consider temporal aspects and therefore it is unrealistic.
among the three release based studies one uses synthetic data one does not consider mislabelling noise and the other does not specify.
study systems no.
of vulnerabilitiesgranularity method evaluation method results mislabelling noise neuhaus et al.
mozilla component imports and function calls cross validation precision recall does not consider zimmermann et al.
windows vista binary code metrics dependenciescross validation precision .
recall 40not specified shin et al.
mozilla firefox red hat enterprise linux kernel389 file code metrics cross validation release basedprecision recall 85not specified shin et al.
mozilla firefox file code metrics cross validation precision recall does not consider scandariato et al.
android apps n a file bag of words cross validation release basedprecision recall does not consider and uses artificial data walden et al.
drupal moodle phpmyadmin223 file bag of words cross validation precision recall 81does not consider zhang et al.
drupal moodle phpmyadmin223 file code metrics text featurescross validation precision recall 69does not consider jimenez et al.
linux kernel file imports and function calls code metrics bag of wordscross validation release basedprecision recall 48does not consider this paper ideal world linux kernel openssl wireshark1593 fileimports and function calls code metrics bag of wordsrelease basedprecision .
.
recall .
.5evaluates the impact of mislabelling noise real world precision .
.
recall .
.
number of vulnerable files.
estimated from the graphs and reported data of the paper.
on the other end three of these studies perform also a releasebased analysis but one uses synthetic data one does not consider mislabelling noise and the other does not specify.
therefore we can conclude that all previous studies overlooked temporal labelling assumptions and our paper is the first to analyse the impact of this assumption on the quality of the prediction.
indeed our ideal world results are close to those reported by previous work ignoring time when labelling however when the same models are used in a realistic scenario their predictive performance dramatically drop revealing that previously reported results were optimistic.
in the following we describe each of the previous work in detail.
neuhaus et al.
were the first to find a correlation between import function calls and vulnerabilities and to use the import and function calls as features to train a classifier able to predict vulnerable components.
they empirically evaluated this proposal for the mozilla firefox project achieving a recall of and a precision of .
shin et al.
experimented with complexity metrics along with code churn and developer metrics.
the authors validated their approach for mozilla firefox and red hat linux and obtained a recall of up to for mozilla firefox and up to for the linux.
however they reported a low precision.
subsequently the same authors analysed whether a traditional defect prediction models trained on complexity code churn and past fault history is capable of predicting software vulnerabilities .
they found that distinguish between bugs and vulnerabilities is a hard task as they obtained similar results for both cases.
chowdhury and zulkernine proposed a similar approach but using a slightly different set of metrics complexity coupling and cohesion.
the evaluation performed for mozilla firefox showed an average recall of .
.
zimmerman et al.
carried out an empirical study to evaluate the efficacy of code churn code complexity dependencies and organizational measures to build a vulnerability prediction model forwindows vista.
their proposal obtained a good precision but low recall.
nguyen et al.
used an approach based on dependency graph rather than traditional source code metrics to train the vulnerabilities prediction model and evaluated it on mozilla javascript engine obtaining an average precision and recall of .
recently scandariato et al.
investigated the use of text mining.
the combination of natural language processing and prediction models was introduced for defect prediction by hata et al.
and has been successfully used for other software engineering prediction tasks .
scandariato et al.
decompose the source code into a bag of words which is then used to train a classifier.
this approach was validated for android applications yielding a precision and recall of about .
however the dataset used in this study was built using a static analysis tool and since these tools are quite imprecise they might produce a lot of type i and type ii errors.
such concerns were addressed by walden et al.
who evaluated the same approach on different settings.
they used a dataset composed by three web applications written in php for a total of about vulnerabilities per application and applied cross validation as there was not enough data to create two independent sets.
this undermine the validity of the results since the evaluation settings used have been shown to lead to generalization and overfitting problems .
using the same dataset zhang et al.
propose to combine code metrics and text mining techniques.
overall the authors manage to improve the results for precision while the recall is only improved in one case.
jimenez et al.
carried out an empirical study comparing the vulnerability prediction approaches using a dataset of vulnerabilities from the linux kernel which was split into independent training and evaluation data sets and found that function calls and text mining were the best performing approaches.
although related jimenez et al.
used a commit based analysis for only one system while herein we use a release based one for three systems and does not investigate the impact of data leakage.
698the importance of accounting for real world labelling when predicting software vulnerabilities esec fse august tallinn estonia the approaches discussed so far are somehow generic and can work with most of the existing software.
however they do not specialise on specific types of vulnerabilities.
two examples of approaches requiring additional data and or the help of a tool are those of smith et al.
and theisen et al.
.
smith et al.
approach for sql hotspot revealed a correlation between vulnerabilities and the number of sql statements.
theisen et al.
suggested to use crash dumps to identify part of system that might be vulnerable.
in particular they define the notion of attack surface approximation that can be used to help vulnerability prediction.
the authors empirically compared a model based on this approach against one based on code metrics using a windows vulnerability dataset and found slightly better results based on attack surface approximation.
since this approach is unsupervised it does not require labelling the training data which is an advantage however recall cannot be achieved as there is no information available for all those source code files that do not have a crash history.
research questions we start our empirical study by assessing the effectiveness of previously proposed vulnerability prediction techniques in the realistic setting in which only reasonably available vulnerability labelling are assumed to be available at model training time.
rq1 how well do prediction models identify vulnerable components between software releases in the real world using the reformulated and more robust experimental methodology?
to establish realistic settings in answering rq1 we train the prediction models using the information available reported vulnerabilities at release time and evaluate against the ground truth data vulnerabilities reported over the whole period of time that we consider .
after checking the performance of prediction models in this realistic setting we repeat the entire process for the ideal world setting in which all vulnerability labels known at any time are also assumed to be all available at model training times.
rq2 how well do prediction models identify vulnerable components between software releases in the ideal world?
finally we wish to be comprehensive so we also evaluate the suggestion raised in previous work that previously discovered faults not known to be vulnerabilities should be included in the training data available since this might improve vulnerability prediction.
hence we ask rq3 can we improve the accuracy of vulnerability prediction in the real world setting by providing prediction models with more general defect based information?
to answer rq3 we repeat the analysis carried out for rq1 but we use modified training sets and compare their performance against the models built using the original training sets.
in particular we include in the training sets defect related information.
we thus augment the training sets with components which were defective but considering them as vulnerable by assigning them a lower weight equal to one with respect to the weight equal to five of the actual vulnerable components.
this practise is known as training set augmentation and attempts to tackle the insufficient learning signal and the class imbalance problem .table vulnerability data in our corpus.
software system vulnerabilities vulnerable components linux kernel wireshark openssl total corpus in our study we consider three large security intensive open source software systems the linux kernel the openssl library and the wireshark tool.
these systems are widely used mature and have a long history of releases and vulnerability reports which is needed to perform realistic experiments with machine learning.
additionally these systems are publicly available on git which allows for their releases analysis by simply linking them with the nvd moreover other researchers can access the same data for reproducibility and extensions.
in the following we describe these systems the procedure we followed to collect the data and the characteristics of the vulnerabilities we collected.
additional details about the data collection and analysis can be found in the dissertation of matthieu jimenez .
.
software systems thelinux kernel is an operating system.
to date it is integrated in billions of systems and devices such as android.
linux is one of the largest open source code bases including approximately .
million loc and has a long history since recorded in its repository.
it is relevant for our study as it has many security aspects and is among the projects with the higher number of reported vulnerabilities in nvd.
openssl is a library implementing the ssl and tls protocols commonly used in communications.
in the project was used by more than of the web servers worldwide .
openssl has approximately kloc.
it is relevant for our study because of its critical importance as highlighted by the heartbleed vulnerability which made half of a million web servers vulnerable to attacks .
wireshark is a network packet analyser mainly used for troubleshooting and debugging.
it supports developers and network managers by capturing traffic analysis protocol and interface controller behaviour.
the project is open source and involves more .
million loc and is relevant for our study because it is integrated on most operating systems.
.
data collection we collected all vulnerabilities reported in nvd for the three systems under study using the vuldata7 framework .
vuldata7 automatically retrieves all declared bug reports and patches by crawling nvd.
using this information the framework retrieves all the related commits from git i.e.
for each vulnerability that has a link to a patch.
to make sure that vuldata7 retrieves all reported vulnerabilities it also searches the projects version history to identify commit messages with