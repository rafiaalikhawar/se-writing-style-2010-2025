prioritizing browser environments for web application test execution jung hyun kwon school of computing kaist south korea junghyun.kwon kaist.ac.krin young ko school of computing kaist south korea iko kaist.ac.krgregg rothermel university of nebraska lincoln usa grother cse.unl.edu abstract whentestingclient sidewebapplications itisimportanttoconsiderdifferentweb browserenvironments.differentpropertiesof these environments such as web browser types and underlying platforms may cause a web application to exhibit different typesof failures.
as web applications evolve they must be regression testedacrossthesedifferentenvironments.becausetherearemanyenvironmentstoconsiderthisprocesscanbeexpensive resultingindelayedfeedbackaboutfailuresinapplications.inthiswork wepropose six techniques for providing a developer with faster feedback on failureswhen regressiontesting webapplications acrossdifferent web browser environments.
ourtechniques drawon methods usedintestcaseprioritization however inourcaseweprioritize web browser environments based on information on recent and frequentfailures.weevaluatedourapproachusingfournon trivial andpopularopen sourcewebapplications.ourresultsshowthat ourtechniquesoutperformtwobaselinemethods namely noorderingandrandomordering intermsofthecost effectiveness.the improvement rates ranged from .
to .
for no ordering and from .
to .
for random ordering.
ccs concepts software and its engineering software testing and debugging empiricalsoftwarevalidation maintainingsoftware keywords web application testing regression testing browser environments acm reference format jung hyun kwon in young ko and gregg rothermel.
.
prioritizing browser environments for web application test execution.
in icse icse 40th international conference on software engineering may 27june gothenburg sweden.
acm new york ny usa pages.
introduction modern client side web applications are becoming increasingly more complex and fault prone .
faults in these applications permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may june gothenburg sweden association for computing machinery.
acm isbn ... .
manifest themselves differently in various combinations of web browsers their versions and underlying operating systems.
onereasonthisoccursisthatsomebrowsercombinationsmaynot support certain features that a client side web application requires.
to prevent such configuration specific faults from affecting the usageofawebapplication appropriatetestingisneeded.thisisparticularlytrueasawebapplicationevolves becauseneworchangedcodecancauseunexpectedfaultsinexisting previously testedfunctionalities.forthisreason techniquesforregressiontestingweb applicationshavebeenactivelystudied.suchtechniquesinclude approaches for automatically generating test cases or oracles from previous versions of web applications repairing test cases that are invalid after web application updates selecting subsets of or prioritizing regression test suites augmentingtestsuites amongothers.noneofthiswork however has considered the additional problems that arise when regressiontestingmustbeperformedacrossdifferentweb browser environments henceforth referred to as browser environments such as environments in which different web browsers and operating systems are utilized.
whenadevelopermodifiesawebapplicationthatismeantto function in different browser environments regression testing that web application can require a large amount of time and computing resources.
to date more than a hundred different web browsers exist .eachwebbrowsercanhavemanydifferentversionsin thefieldsimultaneously andeachcanberunonvarioustypesof operating systems on a wide range of different computing devices.
thiscreatesproblemsofscaleforregressiontestingofwebapplications .
extended regression testing times delay the feedback that can be provided to developers on failing combinations of web browsers and therefore delay the debugging of failures and the release of new versions.
in this work to improve the process of regression testing clientside web applications and in particular to reduce delays in providing feedback about failures we propose six different techniquesforprioritizing browser environments.
1our techniques make use of information about the failure history associated with browser environments in prior applications of regression testing.
two of ourtechniquesutilizerecentfailureinformation includingtypes of web browsers browser versions and of operating systems that is stored in a cache test execution environments are scheduled according to the cached information.
browser environments with recentfailureinformationaregivenhigherprioritythanothers and willbetestedearly.anothertwotechniquesarebasedoncommonly 1our techniques should not be confused with existing approaches for prioritizing test cases wearefocusinginsteadonbrowserenvironmentsundertheassumptionthat existing test cases will be run on each environment on which they are applicable.
acm ieee 40th international conference on software engineering authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden jung hyun kwon in young ko and gregg rothermel failing browser environments and we propose a failure frequencybasedapproachthatgivesgreaterprioritytobrowserenvironments underwhichregressiontestingfailedmanytimesinpriorbuilds andmachine learning basedapproachesthatautomaticallylearn thepatternoffailuresandpredictsbrowserenvironmentsonwhich a web application test is likely to fail.
the last two techniques consider information on both recent failing and commonly failing environments.thetechniquesfirstschedulebrowserenvironments basedoncachedinformation andforbrowserenvironmentswith equalpriority applyafailure frequency basedormachine learningbased approach to order them.
our prioritization techniques have several advantages.
first our techniquesareapplicabletomodernwebapplications.modernweb applicationsarewritteninamixtureoflanguagessuchashtml javascript and css therefore obtaining code coverage for test cases whichisusuallyrequiredbytraditionaltestcaseprioritization methods ischallengingandexpensive.ourtechniques however do not require code coverage information.
second our techniques are especially effective for supporting continuous integration ci practices.incienvironments thereisashorttimeintervalbetween runs of regression tests.
developers frequently check their code in to the mainline codebase and regression tests relevant to thatcode need to be performed in applicable browser environments.
techniques for calculating code coverage cannot keep up with the pace of change that occurs in such processes.
we empirically studied our techniques on four non trivial popular open source web applications.
our results show that our techniques can be cost effective.
our approaches generally detect more failures faster than two baseline approaches in which browser environments are not prioritized or are randomly ordered.
we also compared our six prioritization techniques and analyzed whichtechniques are more cost effective than the other techniques foreach experiment object.
this analysis can suggest which prior itization techniques a developer should apply first in their web application testing.
in addition we addressed some practical issues about using our techniques in industry such as prioritization time overhead and parallel test execution.
the main contributions of this work are as follows thisisthefirstworktoinvestigatetheprocessofprioritizing browser environments for web applications.
weproposefournoveltechniquesforprioritizingwebbrowser environments for regression testing that rely on test results from prior regression testing sessions.
we report the results of an empirical evaluation of our techniques using non trivial real world web applications our results demonstrate the cost effectiveness of our techniques.
wemakeourimplementationsanddatapubliclyavailable .
motivation tofurthermotivatethiswork wepresenttheresultsofaninitial investigation into whether web browser environment scheduling affects the cost effectiveness of regression testing for web applications.
forthis analysis we consideredmootools a popular open sourcewebapplicationthatmanipulatesdocumentobject model dom elementsandhandlesevents.toobtainweb browserfigure the effect of prioritizing web browser environ ments for web applications environments andtomeasurethetimerequiredtotesttheapplicationineachenvironment weretrievedpreviousbuildhistories of mootools from travis ci where test results for mootools are stored.
the build histories we used began on march 2014and ended on february .
we excluded build histories that were not directly relevant to this investigation.
for instance some buildhistoriesdonotcontaintestresultsduetocompilationfailures and some build histories do not provide information on failed web browserenvironments.afterexcludingsuchbuildhistories ten remained.
for each build we counted the number of web browser environmentstested.themedian minimum andmaximumnumbers acrossthebuildswere23.
.
and24.
respectively.eachwebbrowser environment consists of multiple properties these include browser type and version operating system and version and build type default and nocompat .2therearefivebrowsertypes but by varying other properties of the web browser environments the number of environments is increased to .
onecommonstrategyforreducingregressiontestingtimeacross execution environments is to run test suites under each environ ment concurrently.
popular cloud based testing services such as saucelabs andbrowserstack supportsuchapproaches and mootools uses sauce labs.
although test suites can be run concurrently in such cases the total testing time required may vary with the number of environments to be tested.
if the number of testing environments is greater than the number of resources available for parallelization a substantial amount of time can still berequiredtodetectfaultsthatcanbeuncoveredonlyinspecific environments.
increasing the number of resources available can address this problem but at additional costs.
if there are many web applications to manage it becomes more important to use computing resources in an efficient manner by preventing each web application from consuming excessive resources.
for a similar reason existing studies on test case prioritization emphasize the importanceofschedulingtestcasesinaconcurrentenvironment to improve the cost effectiveness of regression testing .
wecomparedtwodifferentschedulesofweb browserenvironmentsonmootools optimalandoriginal andmeasuredwhether the optimal schedule improved the cost effectiveness of regressiontesting.
the original ordering involves using the test schedule that 2theterms default and nocompat indicatewhetherthe applicationiscompatible or not with older versions.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
prioritizing browser environments for web application test execution icse may june gothenburg sweden isrecordedinthebuildhistoryofmootools whereastheoptimal schedule places all web browser environments that were observed to fail ahead of those that were not observed to fail.
we then extractedtheexecutiontimeforeachofthebrowserenvironments jobtimes insaucelabs fromthebuildhistory.next wecalculatedthetimesatwhichweb browserenvironmentsthatcontained at least one failing test cases were reported.
we then compared the calculated times feedback times reported for the optimal and original schedules.
across all ten builds the optimal schedule provided feedback .
seconds faster on average than the original schedule.
figure provides a line plot showing the feedback times obtained for each schedule when browser environments used in build id werescheduled.theoptimalscheduleispresentedasasolidline and the unordered schedule as a dashed line.
across browser environments fivefailed.thex axisdenoteseachfailedbrowser environment andthey axisdenotesthefeedbacktimeassociated witheachfailedbrowserenvironment.theoptimalscheduleprovided feedback to seconds faster than the original schedule to faster .
this shows that techniques for scheduling web browser environments do have the potential to improve feedback.
techniques for prioritizing web browser environments in this section we present techniques for prioritizing web browser environments for use when regression testing web applications.
wemodifytheformaldefinitionofthetestcaseprioritizationproblem toformallydefinetheproblemofprioritizingweb browser environments as follows definition problem of prioritizing test execution environments given e a list of web browser environments pe the set of permutations of e and f a function from peto the real numbers.
problem find e prime pes.t.
e prime prime e prime prime pe e prime prime nequale prime indefinition1 pereferstoallpossibleorderingsof e and frepresents an objective function used to quantify a goal of prioritization.inthiswork ourgoalistoincreasetherateoffaultdetectionofthe testsuitesforawebapplicationunderacertainorderingofwebbrowser environments.
different objective functions are defined and used for different prioritization techniques.
in this work we consider eight different techniques for ordering web browser environments overall.
the first two techniques represent cases in which no heuristic is applied these serve as baseline orderings to compare results against as follows m1 no prioritization.
the web browser environments are not prioritized they retain the ordering utilized by developers initially.
m2 random prioritization.
the web browser environments are ordered randomly.
thesixothertechniqueswepresentcanbecategorizedintothree classes techniques that consider recently failing web browser environments techniquesthatconsider frequentlyfailing environments and techniques that consider both.
we describe these next.
m3 exact matching based prioritization.
this technique considers recently failed web browser environments first.
the technique is based on the assumption that recently failed web browser environments may fail again on future builds of a web application.build type browser name browser version os production safari os x .
id build type browser namebrowser versionos test result cache hit development internet explorer windows pass development safari os x .
fail production internet explorer windows pass production safari os x .
fail 1cache testing environments prioritization result figure example of matching browser environments assuch thetechniqueisbasedonpriorworkthatconsidersrecent failure information .
to utilize recent failure information in prioritizing web browser environments we use a cache.
a cache lists the environments for which previous builds failed.
if web browser environments in the current build are contained in the cache a higher priority is assigned to those environments.
we formally define a cache hit as follows te c1 c2 ... cn build i te cache it te cm x1 x2 ... xk cache hiti build i cache it here teis a set of web browser environments that a developer is considering for regression testing.
build irepresents a subset of te at build i.cache itrefers to the subset of tethat exhibited failures whenmovingfrombuild i ttobuild i .tisathresholdthat specifies thebuild range and abrowserenvironment inthe range canbecached.if i tislessthan1 thevalueissetto1.each browser environment cm i nteis a vector containing properties ofbrowserenvironments.forexample figure2illustrateshowthe cache hitoccurs.aweb browserenvironmentinthefigureconsists of four properties build type browser name browser version and os.
each property in the vector takes a value from a set of possible values.
cache hitirefers to the intersection of build iandcache it.
infigure2 asinglebrowserenvironmenthitsthecache.thecachehit browser environment receives a higher priority than the other environments.
thebrowser environmentsare scheduledbased on priorityscoreinadescendingorder.iftheprioritiesoftwobrowser environmentsarethesame theycanbescheduledintheoriginal order or randomly.
in figure the priority of the last browser environment is1 andthe prioritiesof theother environmentsare therefore the scheduling result becomes .
an advantage of the cache based approach is that it is computationallylight weight andthereforeissuitableforfastdevelopment environmentssuchascienvironments.adisadvantageofusing the cache based approach however is that the performance of the approach could be poor if a web application does not satisfy our assumption that a recently failed browser environment is likely to failagaininnearfuture.asecondpotentialdisadvantageisthattheapproachmaynotbecost effectiveinsituationsinwhichtestingof the previous build fails on most browser environments but testing in the next build fails on few of the environments.
because there is no additional priority considered among cache hit browser environments the cost effectiveness of this approach may suffer if the authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden jung hyun kwon in young ko and gregg rothermel failedenvironmentsarenotdetectedbeforemostofthecache hit browser environments have been tested.
a third potential disadvantage is that it is possible that only some properties in a browser environment are relevantto predicting failed environments in the nextbuild andthecache basedapproachdoesnotconsidersuch cases.instead itrequiresallpropertiesofabrowserenvironment to be matched.
m4 similaritymatching basedprioritization.
oursecondtechniqueattemptstoaddressthesecondandthirddisadvantagesofthe exact matching technique.
instead of assigning the same priorities to cache hit web browser environments this technique calculates the similarity between each environment in the build under test and each environment in the cache.
the priority of a browser environment inthebuild undertest isthe averagesimilarity whichis calculated by summing up the similarity values between that environmentandeachenvironmentinthecache dividedbythenumber of cached browser environments.
then the browser environments arescheduledbasedontheirsimilarityscoresinadescendingorder.
to measure similarity in this context we use the jaccard similarity coefficient .
this assigns a floating number between and to a browser environment.
in comparison to the exact matching technique thisreducesthenumberofcasesinwhichbrowserenvironmentshavethesamepriority.inaddition thisassignsapositive prioritytobrowserenvironmentsthatdonotexactlymatchthose inthecache.forexample evenifabrowserversioninabrowser environmentdoesnotexistinthecache theenvironmentcanreceive a positive priority if other features such as the browser name and operating system are in the cache.
we use equation to measure the similarity between a browser environmentinabuildundertestandanenvironmentinthecache.
the notations are the same as those used in equation .
simij represents the similarity score for the jth browser environment in build i.inaddition foreachvectorin cache it thejaccardsimilarity coefficient is calculated by finding the norm of the intersectionbetweenavectorfrom cache it andavectorfrom build idivided bythenormoftheir union.thejaccardsimilaritycoefficientsare then averaged.
simij cache it summationdisplay ck cache it build ij ck build ij ck for the browser environments shown in figure using our approach the similarity scores calculated for the environments are .
.
and4 respectively.notethat whenusingtheexactmatchingmethod browserenvironments2 and have priority .
when using the similarity matching method however theyhavepriorityvalues0.6and0.
respectively and therefore the prioritization result becomes .
if this added level of distinction is useful it may render the similarity matching technique more cost effective than the exact matching technique.
m5 failure frequency basedprioritization.
onewayinwhich toconsiderfrequentlyfailedweb browserenvironmentsistocount the number of previous failures for each browser environment.
thismethodisbasedontheassumptionthatfrequentlyfailedwebbrowser environments are likely to fail again in future builds.
if testing against a web application fails more often in a certain webbrowser environment than in other environments this method can be effective for predicting environments that will fail in futurebuilds.
thus this method gives higher priority to browser environments that have greater numbers of previous failures.
to schedule browserenvironmentsusingthismethod weneedtohaveakeyvalue data structure in which the key is a browser environment vector ci and the value is the number of previous builds in which the environment failed.
one issue regarding this technique is that using all the properties of a browser environment to count failure frequency may decrease the cost effectiveness of the browser environment scheduling.
for example when a browser environment consists of the browser name browser version and os name using a subset of thosepropertiessuchasthebrowsernameandosnameasthekey and the number of the failed browser environments containing the keyasthevaluecanresultinhighercosteffectiveness.itisdifficult however for a developer to manually determine which subset ofthe properties might be effective to render scheduling more costeffective.
as the number of properties in a browser environment increases this issue may become more important.
m6 machine learning basedprioritization.
toaddresstheproblems of the failure frequency method a machine learning ml method may be useful.
an ml method can automatically learn apatternoffailuresfromthepreviousbuildhistory andprovidea failure probability for browser environments in subsequent builds.
here we use bernoulli naive bayes classifier which calculates failure probabilities based on bayes theorem .
the properties of thebrowserenvironmentsinthisstudyarediscretedata andthe bernoulli naive bayes classifier is suitable for such data.
a classifier is built by learning test history from previous builds in which some but not all browser environments failed.
the classifierreturnstheprobabilityoftestfailuresunderagivenbrowser environment.theclassifieradaptivelyupdatesitslearningmodel once the testing of a build is finished and has failed on some but not all browser environments.
the priority score for each browser environmentisequaltotheoutputofthemlmodel theprobabilityoftestfailuresundertheenvironments.thebrowserenvironments are then sorted based on priority scores in descending order.
equation3showshowtocalculatetheprobabilityoftestfailures under a given browser environment.
drefers to a set of browser environmentsand kreferstothenumberofpossiblebrowserenvironments.
tis a set of test results which are either pass or fail .
p fail di is the probability of test failures under a given browser environment di.
using bayes theorem the posterior p fail di can be calculated by a likelihood p di fail times the prior p fail dividedbytheevidence p di .p fail orp true can be obtained from either a uniform distribution or a domain expert p di fail orp di true canbecalculatedfromthetestresultsfromthepreviousbuilds and p di canbecalculatedfromthe priorresultsandlikelihood.notethatthistechniqueordersbrowser environments based on the posterior probability so a threshold of the posterior probability is not needed.
d d1 d2 ... dk t pass fail p di summationtext j tp di tj p tj p fail di p di fail p fail p di authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
prioritizing browser environments for web application test execution icse may june gothenburg sweden one potential advantage of the ml based method is that it providesmoresophisticatedschedulingthanthecache basedorfailurefrequency basedmethods.thus eachpropertyofa browserenvironmentcanbeusedinanadaptivemannertopredictfailure prone environments.
a second advantage is that the approach is suitable forfastdevelopmentenvironments becauseadeveloperdoesnot need to create a new learning model whenever a new build is conducted.instead theyneedonlyupdatetheexistingmodelbasedon thenewtestingresultsthataregeneratedfromthecurrentbuild.apotentialdisadvantageoftheml basedapproachisthatitassumesthere exist frequently failed browser environments or properties if thisassumptionisnotsatisfied theperformanceoftheapproach may be compromised.
m7andm8 hybrid prioritization.
techniques may perform differentlyacrossapplications soitcouldbedifficultforadevelopertodecide which technique to use.
it is possible to combine techniquestoreducethesedifferences.thehybridprioritizationtechniqueuses two prioritization criteria so browser environments are ordered based on the first criterion and if more than one environment has thesamepriority thehybridprioritizationusesthesecondcriterion to order those environments.
wechosetheexactmatching basedprioritizationtechniqueas the first criterion because the priority score of the exact matchingbased technique is either zero or one so it is more likely for the technique to have more browser environments with the same priority than the other techniques.
the second criterion can be either the failure frequency based or ml based techniques.
we define m7as a prioritization technique that combines the exact matchingbasedtechniquewiththefailure frequency basedtechnique and we define m8as a prioritization technique that combines the exactmatching basedtechniquewiththeml basedtechnique.one advantageofthehybridprioritizationtechniquesisthattheycan considerbothrecentlyfailedbrowserenvironmentsandfrequently failed browser environments.
table shows the code mnemonic and description of the prioritization techniques that we introduced in this section.
table prioritization techniques code mnemonic description m1untreated no ordering m2 random random ordering m3exact matching exact match based m4similar matching similar match based m5 fail freq failure frequency based m6 ml machine learning based m7 m3 m5exact matching plus fail frequency m8 m3 m6exact matching plus ml evaluation weconductedanempiricalstudytoinvestigatetheeffectivenessof the foregoing techniques.
.
objects of study forthisstudy weselectedfourwebapplications.theseapplications wereallobtainedfromthe popularpackage sectionofbower.io table objects of study and build information lines of codebuild periodbuildsfailed buildsmedian bes backbone bootstrap lodash underscore which is a repository of client side javascript packages.
table provides details on the study objects as of october including lines of code time period of collected build history number ofbuilds numberoffailedbuilds andthemediannumberofweb browser environments obtained via processes described below .
backbone is a javascript framework that supports the use of model view controllerpatternsinwebapplicationdevelopment.
bootstrap is an html css and javascript framework for developing responsive and mobile web applications.
lodash i sa javascriptutilitylibrarythatprovidesvariousfunctionsforimproving modularity and performance among others.
underscore is a javascript utility library that provides help functions for functional programming.
we excluded mootools which was used for ourinitialinvestigationdescribedinsection2 becauseithastoo few builds available to support statistical analysis.
wemeasuredlinesofcodeusingcloc whichcountsthe lines of code excluding blank and comment lines.
we collectedall previous build information for each object from travis ci a cloud basedcontinuousintegrationservice .foreachbuild unit testingisconductedinatestingenvironment.qunit istheunit testing framework that is used for the objects.
a cloud based crossbrowser testing service sauce labs is used to conduct regression testing of the objects in each testing environment.
sauce labs considerstestinginatestingenvironmenttobeajob sothenumber ofjobsisequaltothenumberofweb browserenvironments.for all objects except lodash the job ids are recorded in build logs obtainedfromtravisci.givenjobids wewereabletoobtaintest resultsforeachtestingenvironmentusingthesaucelabsrest api.in lodash passedjobidsarenotrecordedinbuildlogs but property names of the web browser environments and unit testing resultsarerecordedinthelogs.intable2 the buildperiod and builds columns showthe period and number of buildson which regression testing is performed for each object.
prioritizationtechniquesareappliedtobrowserenvironments following the occurrence of a failed build.
in this study we exclude builds that have passed in regression testing for all browser environments and builds that have failed in regression testing for allbrowserenvironments.the failedbuilds columnintable2 indicates the number of builds that we actually utilized.
thenumberofbrowserenvironmentsconsideredinagivenbuild canchangeacrossbuilds becausesomebrowserenvironmentsmay be removed and some new ones e.g.
using new web browsersor browser versions may be added.
the median be column in table2indicatesthemediannumberofbrowserenvironmentsthat authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden jung hyun kwon in young ko and gregg rothermel aretestedacrossallbuildsforourobjects.
lodashhasthelargest numberofbrowserenvironmentsbecausethenumberofproperties in its browser environments five is larger than that for the other objects all of which have three .
specifically lodashadds module nameswithinformationonrelevantmodules andabuildtypesuch as development or production .
.
variables and measures .
.
independent variable.
our independent variable involves prioritization techniques.
we evaluated eight techniques these correspond to the two baseline techniques m1andm2 and the six heuristics m3 m8 described in section .
the ordering in which browser environments were tested in practice for each web applicationwasobtainedfromthetraviscilogdata weregard this ordering as the original schedule.
.
.
dependent variable.
tomeasuretheeffectivenessofprioritization techniques at improving the rate of failure detection weused theaveragepercentagefaults detected apfd c metric this metric is typically used to measure the cost effectiveness ofprioritizing test cases when those test cases have different costs execution times .
we used this metric because we can re gard a browser environment as a test case therefore schedulingbrowser environments will produce effects similar to those seen when prioritizing test cases.
the equation for apfd cis as follows ap f d c summationtextm i fi summationtextn j citj 2tci summationtextn i 1ti summationtextm i 1fi here nisthenumberofbrowserenvironmentsand misthenumber of failures.
cirefers to the position in the list of browser environments considered of the browser environment that detects the ith failure.
tjdenotes the cost of performing regression testing on a cj and firefers to the severity of ith failure.
the more effective a prioritization technique is the closer the apfd cvalue is to .
fromthelogdataforeachapplicationwewereabletodetermine whenregressiontestingoneachbrowserenvironmentbeganand ended thisyieldsameasureof jobtime.inthisstudy forallapps otherthan lodash for lodash jobtimesforbuildsarenotavailable so we set tjto which implies treating all builds as being equally costly.wewereunabletoassesstheseverityoffailures however and therefore we set fito for each failure.
.
study procedure we simulate regression testing on different browser environments and with different orders of environments using the log data for travis ci and sauce labs.
the procedure was as follows buildthecache datastructureforfailurefrequency andml model based on the test results of the first failed build.
prioritize the browser environments in the current build based on the priority produced by each technique.
a ifmorethanonebrowserenvironmenthasthesamepriority order them randomly.
measure the apfd cvalue of the result of the regression testing session on the current build.
repeat and times average the apfd cvalues.
updatethecache datastructureforfailurefrequency and ml model based on the test result from the current build when regression testing on that build fails on some but not all browser environments.
repeat steps for the next failed build.
we performed the foregoing process for each of the eight prioritizationtechniquesconsidered withtwoexceptions.forrandom ordering step isperformedviasimplerandomizationusinga differentorderingoneachofthe30runs andfornoordering steps a and are skipped.
we performed runs for techniques other than no ordering step because we used a random order wheneverthereisatie instep a whichrenderstechniques non deterministic.
we could have used no ordering instead of randomordering however thereisnoevidencethattheoriginalorder is meaningful.
we adaptively update the ml model online so training data consists of the properties of the failed browser environments in previousbuildsandtheirtestresults andtestdataistheproperties of the browser environments under which tests are performed.
.
threats to validity external validity the web applications that we study are widely used and popular open source projects regression tested across morethan10browserenvironments.theircodesizesrangefrom 1214to5893lines whichplacesthemassmallandmid sizeapplications.
as such the applications cannot represent all classes of webapplications especiallylarge scalewebapplicationssuchas on line shopping and game applications.
however we were unable tofindotherwebapplicationsthatpubliclyprovidetestfailuredata relative to cross browser testing.internal validity we have implemented the recent failure based methods and failure frequency based method by ourselves and therefore there is a possibility that our implementations do not work as intended.
to reduce this threat we conducted a manual verification process with a small number of builds to determine whetherresultsinthosecaseswerecorrect.anotherthreattointernalvalidityisthatweconductedourrunsonacloudserver and this may allow jobs to be interrupted causing measures of time to differ.
however sauce labs creates a new virtual machine having the same computing resources for each job and each job performs regression testing on an application with each browser environment.inthiscontext aregressiontestruncannotbeinterrupted by another regression test run within the cloud server.construct validity we used apfd cto measure prioritization techniquecost effectiveness.weassumethattheseverityofeach failureisthesame butincasesinwhichitdifferscost effectiveness resultscoulddiffer.weconsider everydetectionofafailurewhen calculating apfd c even when a failure may have previously been detected thisisnecessary however becauseitisdifficulttoaccurately assess in this experiment or in practice whether a given failureis thesame asapreviouslyencounteredfailure.wedonot accountforpotentialchangesinapfd cvaluesthatmayoccurif faultsarefixedduringatestingprocess.weusejobtimetorepresent the cost of regression testing within a testing environment.
jobtimeincludesexecutiontimeoftestsuitesandtimeforgeneratinglogsandmetadatawhichhelpadevelopertodebugfailures.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
prioritizing browser environments for web application test execution icse may june gothenburg sweden figure apfd cfor each web application and prioritization technique we considerjobtimealoneas ourmeasurefortesting cost butin practice testingcostmayincludeadditionalfactorssuchastime spent by humans interpreting results.
.
results and analysis figure provides data on the apfd cvalues obtained using the prioritizationtechniquesandbaselinetechniquesweconsidered per web application.
the y axes indicate apfd cvalues and the x axes denote techniques using their abbreviated names.
for each application for each technique the box represents the distribu tion of apfd cvalues obtained across all the runs involving that technique and application.
inspection of the boxplots suggests that for all four applications thefourbasicprioritizationtechniques outperformedthebaseline approaches intermsofmedianperformance inalmostallcases.
theexceptionsinvolvem3comparedwithm2onbootstrap and m3 compared to m1 on lodash.
across all techniques and web applications themedianimprovementratesrangedfrom .
to .
for no ordering and from .
to .
for random ordering.
the techniques that achieved the highest median apfd cvalue are the exact matching based prioritization technique m3 on backbone and the ml based technique m6 on bootstrap and the hybrid technique using the ml based technique m8 on theother applications.
the lowest median apfd cvalues among the heuristicsoccurredwhenusingtheml basedtechnique m6 for backbone the exactmatching based prioritizationtechnique m3 forbootstrap andlodash and the similarity matching based prioritizationtechnique m4 for underscore .forboth backbone and underscore alltheheuristicstendedtoachievehighapfd cvalues morethan88 medianapfd c .for bootstrap andlodash the exact matching based prioritization technique m3 displays the lowest median apfd camong the heuristics but after combiningitwiththetechniquesthatconsiderfrequentlyfailedbrowser environments m7 m8 the median apfd cvalues increased by .
and .
on average respectively.
todeterminewhethertheapfd cdifferencesweobservedare statisticallysignificant weperformedaone wayanovatestusing pythonscipy .ournullhypothesiswas h0 alltheprioritization techniques have the same apfd cmeans and because resultstable pairwise tests on technique pairs backbone bootstrap grouping meantechnique grouping meantechnique a93.
m3 a68.
m7 a93.
m8 a68.
m6 a93.
m7 a67.
m8 a93.
m4 a66.
m5 b91.
m5 a62.
m3 c85.
m6 a59.
m4 d69.
m1 b50.
m2 e50.
m2 b42.
m1 lodash underscore grouping meantechnique grouping meantechnique a79.
m8 a89.
m8 a b .
m6 b88.
m6 a b .
m7 b88.
m7 b72.
m4 c86.
m3 c70.
m5 c86.
m4 d67.
m3 c85.
m5 d66.
m1 d73.
m1 e49.
m2 e49.
m2 vary somewhat widely across web applications we applied the test to the results on a per application basis.
the anova test showsthat the p value for each application was less than .
so we wereabletorejectthenullhypothesisforeachoftheexperiment objects.theresultsofthisanovatestindicatethatatleastone of the prioritization techniques produced statistically significantly different mean apfd cvalues.
todeterminewhichpairsoftechniquesdiffer weappliedpairwisetests.becausetherateoftype1errors incorrectlyrejecting the null hypothesis increases when conducting two sample t tests multiple times we corrected the significance level using bonfer roni correction .
after the pairwise testing we grouped the techniquesthat arenotstatisticallysignificant intothesamegroup.
table shows the relationships between techniques obtained by the foregoing process per program.
techniques are listed in terms of descending order of apfd cmeans and shared grouping letters indicate cases in which techniques do not statistically significantly authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden jung hyun kwon in young ko and gregg rothermel figure distribution of ratio of failed environments to all environments in a build differ.
for all objects the grouping of random ordering m2 differs from that of all prioritization heuristics.
for all objects except lodash the grouping of no ordering m1 differs from that of all prioritization heuristics.
the exact match based technique m3 in lodashbelongs to the same group as no ordering.
basedontheresultofthepairwisetesting iftheseresultsgeneralize we would suggest that techniques belonging to the first grouping a be the first choices for use in practice however these differ across web applications and practitioners would need to use results from initial runs to select techniques to use longer term.
onbackbone techniques that consider recently failed browser environments m3andm4 andhybridtechniques m7andm8 are such techniques.for bootstrap any prioritization heuristics can beused.for lodash theml based m6 andhybridtechniquesare suchtechniques andfor underscore thehybridapproachusing themltechnique m8 istheonlysuchtechnique.thatsaid acrossalltheobjects thehybridapproachusingthemltechniquealways belongs to the first group so this technique might be the one toconsider first when a new web application must be tested across many browser environments.
discussion we now present the results of additional analyses of our results.we analyze the ratio of failed browser environments and details about test cases and causes of failures.
we also consider several other issues such as parallel test execution and prioritization time.
ratio of failed browser environments in a build.
we analyzed the ratio of failed browser environments to all browser environments tested for each build of each experiment object.
figure shows the result.
we found that the ratio of failed builds to total buildswaslessthan20 inallcases .
for backbone .
for bootstrap .
for lodashand .
for underscore .
this resultshowsthatthere isroomforprioritizationofbrowserenvironments to provide faster feedback to developers.
details on test cases and causes of failures.
toidentifythe causes of test failures in browser environments and determine whetherfailuresareenvironment specific weanalyzedthenumber of failed unit test cases for each failed browser environment and the overlap among failed test cases between environments.
in thisanalysis wefocusontestcasefailures notfaults.thisisbecauseit is difficult and error prone in many cases to trace failures to faults inpartbecausedevelopersseldomexplicitlyreportenvironment failures in issue reports.
from the test logs for backbone andunderscore we extracted the number of total and failed test cases and the names of failedtest cases for each browser environment.
for bootstrap failed test cases are not described in the logs but we were able to obtain information about six builds from unit test reports in the sauce labswebsite.for lodash thenumberoftotaltestcasesisnotin the logs so we extracted the number including other information on builds from the unit test reports.
thenumberoffailedtestcasesissmallforallobjects.thepercentage of failed test cases to total test cases ranged from .
to .
.theaveragenumberoftotalandfailedtestcaseswere399.
and .
for backbone .
and .
for bootstrap .
and .
for lodash and .
and .
for underscore.
to calculate overlap among failed test cases between environments wecalculatedjaccardsimilaritycoefficients betweenpairs ofbrowserenvironments andaveragedthecoefficientsofallthe pairs.acoefficientiscalculatedbythesizeoftheintersectionof thetwofailedtest casenamesetsdividedbythesizeoftheirunion.
forbackbone bootstrap andunderscore the overlaps are low and4 respectively .meanwhile for lodash theoverlapis high .
it is possible that faults in lodashmanifest themselves in more browser environments than faults in other objects.
we were able to find several environment specific failure causes by reading conversations among developers and commit messages.
table4providesexamples.fourcausesinvolvedimplementation in these cases browsers did not fully implement certain features.one cause involved a test case even when an application does not contain environment specific failures its test code can contain anenvironment specificfault.twoothercausesinvolvedmobile devices.
mobile browsers have different levels of feature support thandesktopbrowsers andtendtoperformworsethanbrowsers on desktop devices.
finally there were flaky failures.
reducedfeedbacktimeusingtheproposedtechniques.
to determine why our prioritization techniques apfd cvalues are higher than those for the baseline techniques we investigated how feedbacktimeisactuallyreducedforeachfailure.theboxploton theleftsideoffigure5showsthereductioninfeedbacktimefor failuresforeachoftheexperimentobjectsotherthan lodash when usingtheml basedtechnique.
lodashisomittedbecausewedo nothavetimeinformationforit .theml basedtechniqueprovidesfeedbackforeachfailurebetween0.86and1.19minutesfasterthan whennoorderingisused andbetween0.93and3.12minutesfaster than when random ordering is used.
forlodash we estimated job time as the job time for accessible builds ninebuilds andmeasuredreductioninfeedbacktimebased on that.
we estimated the job time for a browser environmentas the average job time for the browser environment with the same module name and browser type.
as a result the reduction in feedbacktimewasestimatedat10.77minutes.giventhat lodash s median number of browser environments is we expect thatthe larger the number of browser environments the greater the reduction in feedback time that can be achieved.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
prioritizing browser environments for web application test execution icse may june gothenburg sweden table causes of failures cause cause detail a feature is not fully implementedie11 does not implement string description for map setand weakmap objects a feature that is not implemented is usedin test codea test case usesarray.prototype.indexoffunction but the function is notimplemented in ie8 a feature isimplementeddifferently hasownproperty is a non enumerable property in ie8 while it is enumerablein other browsers a feature is notimplementedin a mobile browsertypedarray.prototype.subarray is supported from safari on ios .
device performanceis differentperformance of safari on ios isslower than desktop browser so thebrowser cannot finish rendering anupdated page in a given time in test code flaky failure ios .
flakiness figure5 left reducedfeedbacktimeforfailures.right anexample showing different feedback times in underscore the right side of figure presents an example showing how faster feedback is achieved on failures for one particular buildof underscore .
the ml based technique detected all four failed browserenvironmentsbytestingtheapplicationonthefirstfive browser environments whereas the use of no ordering did not detect all failures.
prioritization time analysis.
each prioritization technique usescomputationtimetoprioritizebrowserenvironments.recentlyfailed based techniques spend time comparing browser environmentsinabuildwithcachedbrowserenvironments andthemlbased technique spends time learning its prediction model and predicting the probability of failure on a browser environment.
table5showsthetimerequiredtoapplyeachtechniqueotherthan the hybrid techniques to browser environments.
each technique wasabletoprioritizebrowserenvironmentsinlessthan0.03seconds for each of the applications.
naturally the more sophisticatedsimilarity basedmatchingandml basedtechniquesrequiredmore timethanthelesssophisticatedexactmatching basedandfailure frequency based techniques but the times were still small.
wheretable runtime costs of prioritization techniques backbone bootstrap lodash underscore m30.
.
.
.
m40.
.
.
.
m50.
.
.
.
m60.
.
.
.
hybrid techniques are concerned the time required to prioritizebrowser environments can be estimated as the sum of the time taken to run the two techniques the hybrid is composed of.
parallel test execution.
test cases can be run in parallel in different browser environments and prioritizing browser environments is still necessary.
in fact sauce labs allows developers toset priorities for browser environments.
when sauce labs runs out of available virtual machines or a concurrency limit given to adeveloper is exceeded browser environments with higher priority take precedence over browser environments with lower priority.
to quantifythe benefitsof scheduling testenvironments in the presence of parallel test execution we modified the existing apfd equation as follows ap f d p c1 p c2 p ... cm p nm 2n thevariablesusedherearethesameasthoseusedearlier withthe exceptionof p.here prepresentsthenumberofparallelexecutions.
when p themodifiedequationbecomesthesameastheoriginal equation.whenweassumethatthetestingtimeundereachbrowser environment is the same all the cmvalues are reduced by ptimes because psessionscanberuninparallel andtestfailurescanbe detected ptimes faster.
wecompareoneofthehybridapproaches m8 tonoordering andrandomorderinginaparallelenvironment.figure6showsthe apfdpresults as the number of parallel executions increases.
each greenlinewithsquaresindicatesourapproach eachblueline with points denotedby stars indicates no ordering each redline with points denoted by plus signs indicates random ordering and each green line with points denoted by squares indicates our hybrid approach.
for all the approaches the apfd pvalues increase as the degree of concurrency increases.
in addition the apfd pvalues for both approaches convergeas the degree ofconcurrency increases.
althoughthereisadifferenceindegree ourapproachoutperforms the baseline approach even in the presence of parallel executions.
exact matching based prioritization technique in lodash.
m3 exact matching has an apfd cvalue that is percentage pointslowerthanm1 noordering .therearetwopossiblereasons forthisresult.first thenumberofpropertiesfor lodashbrowser environments is more than that for other applications so it canbe more difficult to obtain exact matchings for lodash.
second forlodash the browser version in the list of environments was updated more often than in the other applications.
according tolodash s maintainer their builds consider current and previous versions of most browsers .
consequently m3 often fails to assign higher priority values to environments that partially match previously failed environments.
ho wever other techniques assign higher priority scores regarding the partial matching.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
icse may june gothenburg sweden jung hyun kwon in young ko and gregg rothermel figure apfd paccording to concurrency related work detecting regression failures under multiple web browser environments is closely related to detecting cross browser incompatibility xbi problems.
many researchers have attempted to automatically detectcross browserincompatibilityproblems or match compatible features across different web browser environments .
to date however this work has not specifically considered regression testing.
in addition existing research considersxbisatthedomleveltoensureconsistentappearanceofhtml pages but does not consider xbis at the unit function level.
it is importanttocheckxbisattheunit functionlevelbecauseunittesting of functionscan isolate root causesof errors more easilythan other types of tests moreover for open source web applications functions are usually called by various other web applications.
testing of web browser environments is also similar to the testing of configurable software.
configurable software is softwarethat can be customized by users through selections of options suchsoftwaremustbetestedonvariousconfigurations.cohenet al.
present a study in which a single version of a configurable web browser firefox is tested.
this study quantifies the effects of changing configurations on fault detection effectiveness andthe code coverage achieved by test suites.
qu et al.
present aregressiontestingtechniquethatprioritizesconfigurationsofa configurable software system and show that scheduling configurationscanresultinearlierfaultdetection.thesestudies however donotconsiderfaultsinclient sidewebapplications.inaddition these approaches require code coverage information which can be expensive to obtain in a rapid development environment.
the issues fortesting of software productlines spls are also closelyrelatedtoissuesfortestingwebapplicationsacrossbrowser environments.
to reduce the costs of testing spls researchers haveconsideredtechniquesthatselectsubsetsofvariation point combinations.oneapproachutilizescombinatorialtestdesignsthatselectonlypair wisecombinations therebyreducingthenumberof combinations .
asecond approachextends combinatorialtest designs to apply a specific testing method to spl models such as afeaturemodelororthogonalvariability model .similartoour work theseapproachesattempttoreducetestingcostsinsituations where many combinations of variation points are present.
our approach however focuses on scheduling browser environments.
our prioritization techniques are based on considering recently orfrequentlyfailingtestingenvironments.thisideaiscloselyrelatedtoadefectpredictionworkbykimetal.
thatconsiders recentfaults.kimetal.utilizeacachetostorefault pronesoftware entities locations of fixed faults and other locations including the locations of recently added or changed code.
they show that when of the source code files are in a cache of faults occur in these files thus demonstrating the usefulness of cached informationinfaultfinding.engstrometal.
alsostudytheuseof cached information in regression testing.
their approach however focuses on regression test selection rather than test case priori tization and they do not consider the xbi problem.
some defect predictiontechniquesusemachinelearningalgorithms thesebuild machine learning classifiers that automatically learn patterns in buggyfilesorapis andpredictdefectivefilesorapis .this work does not however consider regression testing.
several regression testing techniques have made use of previous build history information .
these approaches are based ontheideathatsometestsuitesaremorelikelytorevealfailures than others.
in particular elbaum et al.
use time windows whichare closelyrelatedto caches totrack recenttestsuites that revealedfailures.inthatwork timewindowsareusedfortestsuiteselectionandprioritization.incontrast ourworkconsidersrecently and frequently failed web browser environments.
conclusion wehavepresentedcost effectivetechniquesforprioritizingbrowser environments when regression testing web applications that functionbyconsideringrecentlyandfrequentlyfailedbrowserenviron ments.wehaveempiricallycomparedourapproacheswithbaseline approaches including the original ordering and random orderings of browser environments for several web applications.
we show thattherateoffaultdetection apfd c achievedbyourtechniques isbetterthanthatofthebaselineapproaches andtheimprovement is statistically significant.
we intend to conduct additional studies on more web applications.
in addition if we can obtain information about failure severities wecanincorporatethisintotechniques.todothiswe canconvertfailureseveritytoaweightvalue andmultipliedthe priority score of each browser environment generated by each approach by that weight.
we also plan to use other software artifacts toschedulebrowserenvironments suchasinformationonupdated code.bycombiningsuchdatawithbuildhistorydata wehopeto be able to create more cost effective prioritization techniques.