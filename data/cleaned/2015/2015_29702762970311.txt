testing advanced driver assistance systems using multi objective search and neural networks raja ben abdessalem shiva nejati lionel c. briand snt university of luxembourg luxembourg raja.benabdessalem shiva.nejati lionel.briand uni.luthomas stifter iee s.a. contern luxembourg thomas.stifter iee.lu abstract recent years have seen a proliferation of complex advanced driver assistance systems adas in particular for use in autonomous cars.
these systems consist of sensors and cameras as well as image processing and decision support software components.
they are meant to help drivers by providing proper warnings or by preventing dangerous situations.
in this paper we focus on the problem of design time testing of adas in a simulated environment.
we provide a testing approach for adas by combining multiobjective search with surrogate models developed based on neural networks.
we use multi objective search to guide testing towards the most critical behaviors of adas.
surrogate modeling enables our testing approach to explore a larger part of the input search space within limited computational resources.
we characterize the condition under which the multi objective search algorithm behaves the same with and without surrogate modeling thus showing the accuracy of our approach.
we evaluate our approach by applying it to an industrial adas system.
our experiment shows that our approach automatically identifies test cases indicating critical adas behaviors.
further we show that combining our search algorithm with surrogate modeling improves the quality of the generated test cases especially under tight and realistic computational resources.
ccs concepts software and its engineering !software testing and debugging keywords advanced driver assistance systems multi objective search optimization simulation surrogate modeling neural networks .
introduction with the challenge of developing software for autonomous vehicles comes the challenge of testing this software to ensure vehicles safety and reliability.
simulation i.e.
design permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore acm.
isbn .
.
.
.
testing of system models is arguably the most practical and e ective way of testing software systems used for autonomous driving.
rich simulation environments are able to replicate various real world tra c situations.
they enable engineers to execute scenarios describing di erent ways in which pedestrians interact with the road tra co rv e hicles interact with one another.
recent years have seen major improvements in accuracy and usability of simulation tools.
their full exploitation however is hampered by two main factors existing simulation tools lack the intelligence and automation necessary to guide the simulation scenarios in a direction that would be likely to uncover faulty behaviors.
hence engineers have to identify critical system behaviors manually with simulation being used only to execute re play these behaviors.
executing simulation scenarios is computationally expensive.
hence given a limited time budget for testing only a small fraction of system behaviors can be simulated and explored.
in this paper we provide solutions to specifically address these two limitations when testing is performed via simulation.
we show how our proposed solutions enable e ective testing of software systems used to assist drivers.
motivation.
we motivate our work using an advanced driver assistance system adas case study.
adass e.g.
collision avoidance systems are developed to assist drivers to properly react to risky situations .
these systems constitute one of the fastest growing segments in the automotive industry.
adass are typically based on vision systems and sensor technologies.
our case study is a pedestrian detection vision based pevi system.
its main function is to improve the driver s view by providing proper warnings to the driver when pedestrians people or animals appear to be located in front of a vehicle in particular when the visibility is low due to poor weather conditions or due to low ambient light.
pevi consists of a ccd camera and software components implementing image processing and object recognition algorithms as well as algorithms that determine when and which warning message should be shown to the driver.
testing pevi with real hardware and in the real environment e.g.
by making a person or an animal cross a road while a car is approaching is obviously dangerous timeconsuming costly and to a great extent infeasible.
hence a large part of testing for adas has to be carried out using what is known as physics based simulation platforms .
these simulation platforms are able to replicate a wide range of virtual tra c and road environments.
this includes simulating various weather conditions road types and topologies permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
intersections infrastructures vehicle types and pedestrians.
further such platforms are able to simulate sensor technologies such as radar camera and gps .
figure shows a snapshot of a simulation environment called prescan that is used to test pevi.
the physics based simulation function of prescan is enabled via a network of connected matlab simulink models implementing dynamic behavior of vehicles pedestrians and sensors.
the software under test pevi in our work is also developed in simulink and is integrated into prescan using inter block connections between the simulink models of the car and pevi.
figure a snapshot of the simulation platform used to test the pedestrian detection vision based pevi system.
challenges.
provided with a physics based simulation platform test case execution is framed as executing models of the system under test and its environment.
existing simulation platforms are able to simulate adas behaviors with reasonable accuracy when provided with a set of input test data.
however they have two important limitations the first limitation is that simulation platforms provide no guidance to engineers as to which test scenarios should be selected for simulation.
since test inputs are specified manually in current platforms simulation is limited to a small number of scenarios hand picked by engineers.
manual test generation is expensive and time consuming and further manually picked test cases are unlikely to uncover faults that the engineers are not aware of a priori.
hence it is important to augment these simulation platforms with some automated test strategy technique i.e.
a sampling strategy in the space of all possible simulation scenarios which attempts to build a su cient level of confidence about correctness of the system under analysis through exercising only a small fraction of that space.
the key question is how to choose an e ective test strategy for testing adas.
we note that the space of all possible test scenarios for adas is very large.
traditional test coverage measures which are common for small scale white box testing are infeasible and impractical for testing applications with large test spaces.
following the intuitive and common practice of system test engineers we develop a test strategy that focuses on identifying high risk test scenarios that is scenarios that are more likely to reveal critical failures .
in particular we rely on search techniques to devise a test strategy that focuses testing e ort on an e ective and minimal set of scenarios.
the search is guided by heuristics that characterize high risk scenarios on which testing should focus.
we develop meta heuristics based on system requirements and critical environment conditions and system behaviors.the second limitation is that physics based simulations are computationally expensive because they often involve executing high fidelity mathematical models capturing continuous dynamic behaviors of vehicles and their environment.
to address this limitation we rely on surrogate models built based on machine learning techniques.
surrogate models are mathematical relations and aim to reduce computational cost by approximating high fidelity but computationally expensive models of physical phenomena .
they are able to predict simulation outputs within some confidence interval allowing us under certain conditions to bypass the execution of expensive simulations.
contributions.
we propose an automated technique to test complex adass based on physics based executable models of these systems and their environments.
our technique builds on the intuitions illustrated on the motivating example described above utilizing meta heuristic search techniques guided by quantitative fitness functions capturing relevant and critical aspects of the system under test and its environment.
we make three contributions in this paper.
the first contribution is that we formulate our testing approach as a multi objective search technique .
we use multiobjective search to obtain test scenarios that stress several critical aspects of the system and the environment at the same time.
for example pevi test scenarios should exercise behaviors during which a pedestrian appears in front of a car in such a way that the possibility of a collision is high and the chance of detecting the pedestrian is low because the pedestrian is very close to the car or because the camera s field of view is blocked.
we compute such desired test scenarios by minimizing the following three fitness functions a function measuring the distance between the pedestrian and the pevi warning areas in front of a car a function estimating the time to collision and a function measuring the distance between the car and the pedestrian see section for detailed information about pevi .
note that combining fitness functions into one function and using single objective search is less desired in this situation because first our three fitness functions are about di erent concepts i.e.
time and distance .
second engineers are typically interested in exploring interactions among critical factors of the system and the environment.
for example they might be particularly interested to inspect if pevi is able to detect pedestrians when they are located on the borders of the camera s field view.
for this purpose a multi objective search algorithm that produces several test cases that exercise di erent and equally critical interactions of the system and the environment is preferred to a single objective search algorithm that generates a single test case.
oursecond contribution is concerned with the large execution time of our testing approach.
the execution time of our search based testing technique is large because physicsbased simulations are computationally expensive.
we reduce the execution time of our search algorithm by proposing anew combination of multi objective search with surrogate models built based on supervised learning techniques .
surrogate models are able to predict the fitness function values within some confidence intervals.
the time required for surrogate models to predict values is significantly less than the time required to run simulations of physical models.
the combination of multi objective search with surrogate modeling proposed in this paper is not tied to our particular search based testing algorithm and is applicable to 64any multi objective search algorithm that computes a set ofpareto optimal solutions .
a solution is called pareto optimal if none of the fitness functions used by the search can be improved in value without degrading some of the other fitness values .
in our work we use optimistic and pessimistic fitness function predictions computed based on surrogate models and a given confidence level to rank pareto fronts during search.
we identify and prune from the search space the candidate solutions that have a low probability to be selected in the best ranked pareto front.
we show that when actual fitness values are not better than their respective optimistic predictions the search algorithm with surrogate modeling behaves the same as the original search algorithm without surrogate modeling.
specifically under this condition and provided with the same set of candidate solutions at each iteration search with and without surrogate modeling select the same solutions but the search with surrogate modeling is likely to call less simulations per iteration than the search without surrogate modeling.
note that our proposed combination of multi objective search with surrogate modeling is more accurate than existing alternatives as it eventually uses the actual simulations instead of the predictions to compute pareto optimal fronts.
ourthird contribution is focused on demonstrating the effectiveness of our search based testing technique by applying it to an industrial cases study i.e.
the pevi system.
our results show that our search based testing technique for adas outperforms a random test case generation strategy baseline .
combining multi objective search with surrogate modeling improves the quality of the generated test cases within a limited time budget for test generation.
our search based testing technique is able to produce several test scenarios indicating potential errors in the pevi system.
these test scenarios had not been previously found by manual testing based on domain expertise.
structure.
section describes the pevi system.
section outlines our approach to developing surrogate models.
section provides our multi objective search algorithm that uses surrogate modeling.
section tailors our search algorithm to the pevi system.
section presents our empirical evaluation.
section compares our work with the related work.
section concludes the paper.
.
the pevi system in this section we provide some further background on the pevi system its important inputs and outputs and the fitness functions that we design to guide our test strategy to exercise pevi s most critical behaviors.
pevi requirements.
based on the pedestrian detection vision based pevi specification the cone shaped space in front of a car that is scanned by the pevi camera is divided into three warning areas illustrated in figure .
the size of the cone vertex is a feature of the camera and is called camera s field of view .
the warning areas are described as follows.
the acute warning area awa is the red rectangle in figure .
the warning area wa is the orange area in figure .
the cross warning area cwa refers to the two yellow right angled rectangles on the two sides of wa in figure .
the size and the position of the above three areas depend on the type of the car on which pevi is deployed the type waawa cwa cwa dawawawalwawwa lcwalawa waawa cwa cwa lawafigure pevi s warning areas.
of the camera used for pevi and the oem i.e.
car maker preferences.
for example in our case study the field of view is set to and the length of awa lawa ranges between 60m to 168m depending on the car speed.
the main requirement of pevi is stated as follows r the pevi system shall generate a red orange or yellow alert when it detects an object in awa wa and cwa warning areas respectively.
further pevi shall fulfill this requirement under di erent weather conditions and while the car runs on di erent types of roads with di erent speeds .
the requirement r although summing up the main function of pevi is still very broad.
there are several scenarios where a pedestrian may end up being in one of the dangerous area in front of a car when crossing a road.
to focus testing on the most high risk test scenarios among the numerous possibilities that requirement rcharacterizes we identified the following specific situations after discussions with engineers at our partner company it is more critical for pevi to detect pedestrians in the warning areas when pedestrians are closer to the car and when the chance of collision is higher.
.
we use these specific situations to define fitness functions for our multi objective search algorithm.
pevi input and output.
in general pevi s function is impacted by several physical phenomena and environment factors.
for example road friction or wind may a ect vehicle speed which in turn influences pevi s behavior.
however given that the testing budget both in terms of manual and computational e ort is limited we identified through our discussions with the domain expert the most essential elements impacting the pevi system.
we developed a domain model to precisely capture these elements.
this domain model essentially specifies a restricted simulation environment that is su cient for testing pevi.
further this domain model characterizes the pevi inputs and the outputs generated after simulating pevi.
the domain model is shown in figure .
based on this model a test scenario for pevi contains the following input the value of the scene light intensity the weather condition that can be normal foggy rainy or snowy the road type that can be straight curved or ramped the roadside objects namely trees and cars parked next to the road the camera s field of view the initial speed of the vehicle and the initial position the orientation and the speed of the pedestrian.
all these input elements except for the vehicle and the pedestrian properties are static i.e.
immobile during the execution of a test scenario.
the vehicle and the pedestrian human or animal are dynamic i.e.
mobile .
our domain model makes some simplifying assumptions about pevi s test scenarios.
for example we assume that the test scenarios contain only one pedestrian and one vehicle and the vehicle and the pedestrian speeds are constant.
these assumptions are meant to reduce the complexity of intensity realscenelight dynamicobject1 weathertype conditionweather fog rain snow normal enumeration condition output trajectory field of view realcamera sensorroadside object roadtype rtroad1 curved straight ramped enumeration rt vc realvehicle x0 real y0 real real vp realpedestrian x real y realposition1 state booleancollisionparked carstrees simulationtime real timestep realtest scenario pevi state booleandetection11111111 positioned uses 11figure a fragment of the pevi domain model.
test scenarios and were suggested by the domain expert.
however we note that our search based test generation approach is general and is not restricted by these assumptions.
each of the input elements in figure i.e.
the elements related by a composition relation to the test scenario element in figure impacts pevi s behavior.
for example the weather condition and the scene light intensity impact the quality of images and the accuracy of pevi in detecting pedestrians.
the camera field of view and the road shape e.g.
straight curved and ramped impact the topological positions of the three warning areas which in turn impact the pedestrian detection function of pevi.
roadside objects may block the camera s field of view hence leaving pevi with little time to detect a pedestrian and to react with a proper warning message.
finally pevi s function should be tested for various scenarios by varying the speed of the vehicle and the pedestrian and the position and orientation of the pedestrian.
each test scenario is associated with a simulation time denoted by tand a time step denoted by t. the simulation time tindicates the time we let a test scenario run.
the simulation interval is divided into small equal time steps of size t. in order to execute a test scenario we need to provide the simulator with the values of the input elements described in figure as well as tand t. ideally pevi should be tested by varying properties of all the input elements.
however due to technical limitations of prescan the simulation platform used for testing pevi only properties of the pedestrian and the vehicle can be directly manipulated by dynamically modifying the simulink models implementing the vehicle and the pedestrian.
other elements have to be configured manually through the tool user interface.
hence in this paper we limit the pevi test input to the properties of the vehicle and the pedestrian and leave the problem of testing pevi for other input properties to the future.
specifically we define the pevi s test input as av e c t o r vc x0 y0 vp where vcis the car speed x0and y0specify the position of the pedestrian is the orientation of the pedestrian and vpis the speed of the pedestrian.
we assume that the initial position of the vehicle xc yc is fixed in test scenarios i.e.
xc 0mandyc m .
we denote the value ranges for vc x0 y0 andvp respectively by rvc rx0 ry0 r andrvpsuch that rvc rvp r rx0 xc 20m xc 85m and ry0 yc 15m yc 2m .
note that variables vc x0 y0 andvpare all of type float.
having provided the input prescan simulates the behavior of pevi the vehicle and the pedestrian and generates the following output elements output trajectory vectors each dynamic object i.e.
the vehicle and the pedestrian is associated with an output trajectory vector that stores the position of that object at each individual time step.
the size of the trajectory vectors denoted by k is equal to the number of time steps within the simulation time i.e.
k t t. we denote the trajectory output of the pedestrian by the following two functions xp yp t t .
.
.
k t !r.
we write xp t and yp t to denote the pedestrian position on x and y axes at time t respectively.
similarly we define functions xc yc t t .
.
.
k t !rfor the trajectory output of the car collision this is a boolean property indicating whether there has been a collision between the vehicle and the pedestrian during the simulation and detection this is a boolean property generated by pevi indicating whether pevi has been able to detect the pedestrian or not.
fitness functions.
our goal is to define fitness functions that can guide the search into generating test scenarios that break or are close to breaking the requirement rmentioned earlier.
based on our discussions with the domain expert we identify the following three fitness functions.
.
minimum distance between the car and the pedestrian.
the first function denoted by dmin p car computes the minimum distance between the car and the pedestrian during the simulation time.
we denote the euclidean distance between the pedestrian and the car at time tbyd p car t .
the function dmin p car is then defined as follows dmin p car min d p car t t t. the test scenarios during which the pedestrian gets closer to the car are more critical.
hence our search strategy attempts to minimize the fitness function dmin p car .
.
minimum distance between the pedestrian and awa.
the second function denoted by dmin p awa computes the minimum distance between the pedestrian and awa during the simulation time.
we denote the distance between the pedestrian and awa at time tbyd p awa t .
this function depends on the shape of the road the orientation of the pedestrian and her position at time t. the value ofd p awa for time steps in which the pedestrian is inside awa is zero.
the function dmin p awa is then defined as follows dmin p awa min d p awa t t t the goal of our search strategy is to minimize the second fitness function as well.
this is because in order to test pevi s function for awa we need to generate scenarios during which the pedestrian crosses awa or gets close to it.
to test pevi for the two other warning areas wa and cwa we modify dmin p awa to compute the distances between the pedestrian and wa and cwa respectively.
.
minimum time to collision.
the third fitness function is referred to as the minimum time to collision and is denoted byttcmin.
the time to collision at time t ttc t is the time required for the car to hit the pedestrian if both the car and the pedestrian continue at their speed at time tand do not change their paths.
the function ttcminis then defined as the minimum value of ttc t when tranges from to t.ttcminhas proven to be an e ective measure to estimate the collision risk and to identify critical tra c situations .
we are interested to generate scenarios that yield a small ttcminsince these scenarios are more risky.
.
surrogate models we use surrogate models in our work to mitigate the computation cost of executing physics based adas simulations.
66specifically in order to compute the three fitness functions described in section we have to execute expensive physicsbased simulations.
we create a surrogate model for each fitness function to predict the fitness values without running the actual simulations.
such surrogate models are often developed using machine learning techniques such as classification regression or neural networks .
given that we are dealing with real valued functions regression orneural network techniques are more suitable for our purpose because classification techniques are geared towards functions with categorical outputs.
many studies have shown that neural networks perform better than regression techniques in particular when the input space under analysis is large and when the relationship between inputs and outputs is complex .
hence we use neural networks to build surrogate models.
neural networks can be used with supervised or unsupervised training algorithms .
in our work we are able to obtain output values for training input data by running simulations.
hence we use neural networks in a supervised training mode.
neural networks consist of a number of artificial nodes called neurons connected via weighted links forming a network of neurons.
the neurons are organized in several layers.
the first layer is the input layer followed by one or more hidden layers.
the last layer is the output layer.
given a network with a predefined number of neurons and layers the training process aims to synthesize a network by learning the weights on links connecting the neurons.
learning is carried out in a number of iterations known as epochs .
in this paper we consider the following well known training algorithms to develop our surrogate models bayesian regularization backpropagation br levenberg marquardt lm and scaled conjugate gradient backpropagation scg .
given a fitness function f we build a surrogate model offby training a neural network.
to do so we use a set of observations containing input values and known output values .
we divide the observation set into a training set and a testset.
the training set is used to infer a predictive function f. this is done by training a neural network of f such that ffits the training data as well as possible i.e.
for the points in the training set the di erences between the output of fand that of fare minimized.
the test set is then used to evaluate the accuracy of the predictions produced by fwhen applied to points outside the training set.
training neural networks requires tuning a number of parameters particularly the number of hidden layers the number of neurons in each hidden layer and the number of epochs.
further we need to choose among the three training algorithms i..e br lm and scg .
finding the best values for these parameters and selecting the best performing algorithm in our case is addressed in our empirical evaluation section .
in addition to building function fto predict the values of a fitness function f we develop an error function fcl that estimates the prediction error based on a given confidence level cl.
the value of clis a percentage value between and .
for example let fcl be the error function computed for fwith respect to cl .
this implies that with a probability of the actual value of f p lies in the interval of f p fcl .
we compute fcl based on the distribution of prediction errors obtained based on the test sets.
.
search with surrogate model we cast the problem of test case generation for adas as a multi objective search optimization problem .
specifically we identified three fitness functions in section to characterize critical behaviors of the pevi system and its environment.
the solutions to our problem are obtained by minimizing these three fitness functions using a multiobjective pareto optimal approach that states that a solution pis said to dominate another solution p0 i fpis not worse than p0in all fitness values and pis strictly better than p0in at least one fitness value .
the solutions on a pareto optimal front are non dominating representing best found test scenarios that stress the system under analysis with respect to the three identified fitness functions.
in our work we rely on population based and multi objective search optimization algorithms .
in this class of algorithms the dominance relation over chromosome populations is used to guide the search towards paretooptimal fronts.
in our work we choose the non dominated sorting genetic algorithm version nsgaii algorithm which has been applied to several application domains and has shown to be e ective in particular when the number of objectives is small .
figure illustrates the nsgaii algorithm.
the algorithm works as follows initially a random population pis generated line .
after computing fitness functions f1 ... f k for each individual in p line the individuals in pas well as those in the archive afrom the previous iteration are sorted based on the non domination relation line .
in particular a partial order relation rank is computed to sort elements in q p abased on the fitness functions f1 ... f k. assuming that the goal of optimization is to minimize the fitness functions f1tofk the partial order rank q qis defined as follows 8p p02q rank p p0 i2 ... k fi p fi p0 v 9i2 ... k fi p f i p0 specifically rank p p0 if and only if pdominates p0at least in one fitness value i.e.
pis not worse higher than p0 in all the fitness values and pis strictly better less than p0 in at least one fitness value.
note that it might happen for a given pair of individuals that neither of them dominates the other.
for these individuals we use the notion of crowding distance denoted by cd to be able to partially rank them.
we write non dominating p p0 when pandp0are non dominating and say that non dominating p p0 holds i 8i2 ... k fi p fi p0 w 9i i02 ... k fi p f i p0 fi0 p f i0 p0 letr qbe such that for all p p02r we have non dominating p p0 .
for the elements in r a crowding distance function cdis defined that assigns a value to p2r based on the distance between pand other p02r.
the definition of the relation rank q qis then extended as follows for every p p02q we have rank p p0 i 8i2 ... k fi p fi p0 9i2 ... k fi p fi p0 w non dominating p p0 cd p0 cd p having computed the rank partial order the nsgaii algorithm then creates a new archive aof the best solutions found so far by selecting the best individuals from qbased onrank lines .
note that bestranked q rank returns an element p2qsuch that no other p02qdominates p i .
e .
rank p0 p for every p02q p .
when there are 67algorithm.nsgaiiinput m population and archive size a p m g maximum number of search iterationsoutput bestsolution the best solutions found ingiterations.
.a empty archive .p p1 ... pm initial population randomly selected .forgiterationsdo4.computefitness p for allp2p fitness valuesf1 p ...fk p are computed .q p a q 2m .rank computeranks q .a .while a mdo8.p bestranked q rank .a a p .bestsolution a12.p breed a breeding a new populationpfrom theparent archivea .returnbestsolutionfoundfigure nsgaii algorithm more than one individual that are not dominated by any element in q bestranked q rank returns the one that appears first in the lexicographic ordering.
after computing the archive a the algorithm breeds a new children population pfrom the parent archive aby calling the breed procedure line .
this is done by selecting m individuals as parents from ausing the tournament selection technique and then creating mo springs from them selected parents using the crossover andmutation genetic operators.
the details of the breed procedure can be found in .
we will describe the genetic operators used in our approach in section .
the algorithm terminates by returning the best solutions found within ggenerations.
in this paper we change the nsgaii algorithm in figure to use instead of the actual fitness values the predicted fitness values obtained from surrogate models to compute the partial order rank and to select the best individuals a. we refer to our algorithm as nsgaii sm.
the main goal of nsgaii sm is to speed up the search by selecting an archive of best individuals afrom the set qwithout the need to run costly simulations for every element in the newly bred children population p. specifically we bypass execution of the simulation for any individual p2p if we can conclude using predicted fitness values that phas a low probability to be included in a. recall that the surrogate model for any fitness function fcomprises a prediction function fand an error function fcl that estimates the prediction errors of fwithin the confidence level cl.
since our optimization problem aims to minimize fitness values for any individual p we have a most optimistic fitness value f p fcl p and a most pessimistic fitness value f p fcl p .
the gap between these two values widens by increasing the confidence level cl a n d decreases by lowering cl.
our nsgaii sm algorithm is shown in figure .
the algorithm computes predicted fitness values for every individual p2p line .
the algorithm further uses a set predicted to keep track of elements for which only predicted fitness values are known i.e.
the elements for which the actual simulation has not yet been executed.
the set predicted is initially set to psince for the elements in p actual fitness values have not yet been computed.
then the algorithm computes two partial order relations rank andrank .
the relation rank is computed based on optimistic fitness values f p fcl p for individuals in predicted and actualalgorithm.
nsgaii sm input m population and archive size a p m g maximum number of search iterations output bestsolution the best solutions found in giterations.
.a archive .p p1 ... p m initial population randomly selected .forgiterations do .
predictfitness p for every p2pand every fitness function fis.t.
i2 ... k compute fi p fcl i p and fi p fcl i p .
predicted p .
q p a q 2m .
rank rank computeranks q .
a .
while a m do .
p bestranked q rank .
while p62predicted a m do .
a a p .
q q p .
p bestranked q rank .
if a mthen break .
p bestranked predicted rank .
computefitness p run simulation and compute actual fitness values f1 .
.
.
fkforp .
predicted predicted p .
rank rank computeranks q re rank the remaining elements in qafter computing the actual fitness values for p. .
bestsolution a .
p breed a breeding a new population from the parent archive .return bestsolutionfound figure nsgaii sm algorithm fitness values f for other individuals.
dually the relation rank is computed based on pessimistic fitness values f p fcl p for individuals in predicted and actual fitness values f for other individuals.
letrank be the partial order over qcomputed based on the actual fitness values for every element in q assuming that the actual fitness values are known for elements in q .
then we show the follow lemma.
lemma.
letbestranked q rank 62predicted .
suppose for every p2predicted and every fitness function fi2 f1 ... f k we have fi p fcl i p fi p .
then bestranked q rank bestranked q rank .
proof.
by our assumption the actual fitness value for any element p2predicted is higher than their optimistic fitness value fi p fcl i p which is the value used to create rank .
hence none of the elements in predicted could be ranked higher than bestranked q rank when we use the partial order rank.
the above lemma states that assuming that actual fitness values are not better than the optimistic predictions and if bestranked q rank 62predicted then bestranked q rank is equal to the best ranked element computed in nsgaii where we do not use predictions.
given the above lemma in nsgaii sm we first add the elements of qthat are ranked best by rank and are not in predicted to the archive of best elements a lines .
after that if ais still short of elements i.e.
a m we compute actual fitness values for the predicted element that is ranked highest by rank i.e.
the partial order based on pessimistic fitness values of predicted elements lines .
we then recompute rank andrank line and continue until we select mbest elements from qintoa.f o r all the elements in a the actual fitness values are already computed i.e.
a predicted .
upon termination of the while loop lines in figure the set predicted qcontains those elements that nsgaii sm was able to discard without the need to com68pute the actual fitness values for them.
hence at each iteration the size of predicted indicates the number of simulation calls that our algorithm has been able to save.
this is in contrast to the original nsgaii algorithm figure where at each iteration simulation is called for mtimes.
according to the above lemma if actual fitness values are not better than the optimistic predictions then nsgaii and nsgaiism behave the same.
that is assuming that nsgaii and nsgaii sm are provided with the same set pat each iteration they select the same candidate solutions set a but nsgaii sm is likely to perform less simulations per iteration than nsgaii.
the probability of actual fitness values being better than the optimistic predictions depends on the confidence level cl.
for example for cl with a probability of .
the actual fitness values are better than their optimistic predictions and hence nsgaii sm might select less optimal solutions compared to nsgaii given the same setp.
in section we empirically compare the quality of the solutions generated by nsgaii sm and nsgaii in particular by accounting for the randomness factor in generatingpand by executing the two algorithms within a limited and realistic time budget.
.
tailoring search to pevi the algorithms nsgaii and nsgaii sm described in section are generic.
we tailor them to our search based test generation problem by specifying the search input representation the fitness functions and the genetic operators.
input representation.
the input space of our search problem consists of vectors vc x0 y0 vp .
the variables vc x0 y0 andvpand their ranges are defined in section .
each value assignment to the vector vc x0 y0 vp represents a chromosome and each value assignment to the variables of this vector represents a gene.
fitness functions.
we use the three fitness functions dmin p car dmin p awa and ttcmin defined in section for our search algorithm.
a desired solution is expected to minimize these three fitness functions.
genetic operators.
thebreed procedure in the nsgaii and nsgaii sm algorithms is implemented based on the following operators s e l e c t i o n .
we use a binary tournament selection with replacement that has been used in the original implementation of nsgaii algorithm .
c r o s s o v e r .
we use the simulated binary crossover operator sbx .
sbx creates two o springs from two selected parent individuals.
the di erence between o springs and parents is controlled by a distribution index the o springs are closer to the parents when is large while with a small the di erence between o springs and parents will be larger .
in this paper we chose a high value for i.e.
based on the guidelines given in .
m u t a t i o n .
mutation is applied after crossover to the genes of the children chromosomes with a certain probability mutation rate .
given a gene x i.e.
any of the variables vc x0 y0 andvp our mutation operator shifts xby a value x0selected from a normal distribution with mean and variance .t o a v o i d i n v a l i d o springs if the result of a crossover or a mutation is greater than the maximum it is set to the maximum.
if the result is below the minimum it is clamped to the minimum.
.
evaluation in this section we investigate the following research questions rqs through our empirical evaluation applied to the pevi case study.
rq1.
comparing random search nsgaii and nsgaii sm how do nsgaii nsgaii sm and random search perform compared to one another?
we start by comparing the time performance and the quality of solutions obtained by our test generation strategy when we use nsgaii and nsgaii sm.
our goal is to determine whether nsgaii sm is able to generate results with higher quality than those obtained by nsgaii within the same time period.
we then compare the algorithm that performs better between nsgaii and nsgaii sm with a random test generation algorithm the baseline of comparison typically adopted in sbse research .
rq2.
usefulness does our approach help identify test scenarios that are useful in practice?
this question investigates whether the test scenarios generated by our approach were useful for the domain experts and how they compared with the test scenarios that have been previously devised by manual testing based on domain expertise.
metrics.
we evaluate the prediction accuracy of surrogate models using the coe cient of determination r2 that measures the predictive power of a surrogate model by identifying how well a test set fits the model.
specifically r2 measures the proportion of the total variance of fexplained by ffor the observations in the test set where fis a fitness function and fis its corresponding predictive function.
the value of r2ranges between and .
the higher the value ofr2 the more accurate the surrogate model is.
to assess and compare the quality of pareto fronts obtained by our alternative search algorithms we use two wellknown quality indicators hypervolume hv and generational distance gd .
hv measures the volume in the solution space that is covered by members of a nondominated set of solutions.
the larger the volume i.e.
the higher the value of hv the better the results of the algorithm.
gd compares the pareto front solutions computed by an algorithm with an optimal pareto front ortrue pareto front i.e.
the best non dominated solutions that exist in a given space of solutions for a given problem.
in particular gd is the average distance between each point in a computed pareto front and the closest optimal pareto front solution to that point.
a value of for gd indicates that all the obtained solutions by a search algorithm are optimal.
the lower gd the better the results of the algorithm.
computing an optimal pareto front is usually not feasible.
as suggested in the literature instead we use a reference pareto front that is a union of all the non dominated solutions computed by our search algorithms i.e.
nsgaii nsgaii sm and random search .
the hv and gd are selected from the combination and convergence quality indicator categories respectively.
as discussed in to assess the quality of computed pareto fronts with respect to combination and convergence indicators it is su cient to choose only one indicator from each of these two categories.
experiment design.
we implemented the nsgaii and nsgaii sm algorithms and the neural network surrogate models in matlab.
in addition we implemented a test generation strategy based on random search.
random search and our nsgaii based search algorithms require to interact 69with prescan to execute simulations of the simulink models of the pedestrian the car and the pevi system embedded into the car see section .
the nsgaii sm algorithm in addition to calling the prescan simulator calls neural networks that are developed to serve as surrogate models.
we ran all the experiments on a laptop with a .
ghz cpu and 16gb of memory.
based on our experiments each pevi simulation i.e.
each call to prescan on average takes min with a min value of .
min and a max value of .
min.
the simulation time variations are due to the variations in the car and the pedestrian speeds and positions.
further we may stop simulations before completion at a point where we can conclusively determine the fitness function values.
to answer our research questions we designed and performed the following experiment.
first we identified the training algorithm and the configuration values that lead to the most accurate neural network based surrogate models for the pevi case study.
to do so for each of our three pevi fitness functions we compared di erent neural network configurations.
the comparison is based on a k fold cross validation with k .
specifically we first selected using adaptive random search observation points from the input search space of the pevi system.
adaptive random search was used to maximize diversity in our training and test sets.
it is an extension of the naive random search that attempts to maximize the euclidean distance between the points selected in the input space.
recall from section that each pevi input point is a vector vc x0 y0 vp selected from a five dimensional space.
we simulated each point to obtain the actual values for each fitness function.
in the experiment we refer to fitness function dmin p car b yf1 t odmin p awa b yf2 and to ttcmin byf3.
the observation points are randomly partitioned into five disjoint subsets with points in each.
we then randomly selected four subsets to create a training set with points and the remaining subset is used as the test set.
this process is repeated for five times so that for each fold cross validation the r2values are computed on a test set containing the entire points.
to account for randomness we repeated the fold cross validation ten times.
to develop neural networks with high prediction accuracy we considered three training algorithms br lm and scg and we set di erent values to the following parameters the number of hidden layers nl the number of neurons in each hidden layer nn and the number of epochs ne .
specifically we set nl as is common in the literature .
there are various recommendations for setting nn.
in particular nnis recommended to be less than twice or equal to 3of the size of the input vector or to be a number between the input and the output size .
we considered the values and for nn.
in addition we considered the value for nnbecause in some cases the accuracy may improve when nnis set to values considerably larger than the input size .
finally we set neto and .
in total we developed and trained di erent neural network configurations.
we computed r2for di erent repetitions of fold cross validations of the neural network configurations related to our three fitness functions.
we selected the following neural network configurations with highest predictive accuracy highest r2 for our three fitness functions for f1 we selected the configuration that was developed by the br algorithm with nn and ne r2 .
.
f o r f2 we selected the configurationthat was developed by the br algorithm with nn and ne r2 .
and for f3 we selected the configuration that was developed by the lm algorithm with nn and ne r2 .
.
note that r2 .
indicates that of the variance in the test set is explained by the predictive model.
the high r2values of the selected configurations indicate their high predictive accuracy.
having obtained the most accurate surrogate models to be used by the nsgaii sm algorithm we now discuss value selection for the search algorithms parameters.
since our experiments which involve running physics based simulations are very time intensive we were not able to systematically tune the search parameters e.g.
based on the guidelines provided in .
instead we selected the parameters based on some small scale preliminary experimentations as well as existing experiences with multi objective search algorithms.
specifically we set the crossover rate to .
the mutation rate to .
and the population size to .
our choice for the crossover rate is within the suggested range of .
our preliminary experimentations showed that more explorative search may lead to better results.
hence we set the mutation rate to .
which is higher than the suggest value of lwhere lis the length of chromosome .
finally we chose a relatively small population size to allow for more search iterations generations within a fixed amount of time.
results.
next we discuss our rqs rq1 comparing random search nsgaii and nsgaii sm .
to answer rq1 we ran random search nsgaii and nsgaii sm with cl .
times for min.
we computed the hv and the gd values for the pareto front solutions obtained by these alternative search algorithms at every min interval from to min.
we used the resulting hv and gd values and the changes in these values over time to first compare nsgaii and nsgaii sm by focusing on their performance when the algorithms are executed within a practical execution time budget i.e.
min .
second we compare the better algorithm between nsgaii and nsgaii sm with random search.
to statistically compare the hv values we performed the non parametric pairwise wilcoxon paired signed ranks test and calculated the e ect size using cohen s d .
the level of significance was set to .
and following standard practice dwas labeled small for .
d .
medium for .
d .
and high for d .
.
comparing nsgaii and nsgaii sm.
figure a shows the hv values obtained by runs of nsgaii and nsgaiism up to min.
as shown in the figure at the beginning both nsgaii and nsgaii sm are highly random.
after executing these two algorithms for min the degree of variance in hv values across nsgaii sm runs reduces faster than the degree of variance in hv values across nsgaii runs.
further the average hv values obtained by nsgaiism grows faster than the average hv values obtained by nsgaii.
after executing the algorithms for min both search algorithms converge towards their pareto optimal solutions and the di erence in average hv values between the two algorithms tends to narrow.
we note that the di erences between the hv distributions of nsgaii and nsgaii sm are not statistically significant.
this is likely because the number of runs is rather small thus yielding low statistical power.
however as shown in figure a the medians and averages of the hv values ob700.
.
.
.
.
time min hv5010015010 a comparing hv values obtained by nsgaii and nsgaii sm nsgaii mean nsgaii sm mean .
.
.
.
.
time min b comparing hv values obtained by rs and nsgaii sm hvrs mean nsgaii sm mean .
.
.
.
.
time min hv5010015010 c hv values for worst runs of nsgaii nsgaii sm and rs rs nsgaii sm nsgaii figure comparing hv values obtained by a runs of nsgaii and nsgaii sm cl .
b runs of random search and nsgaii sm cl .
and c the worst runs of nsgaii nsgaii sm cl .
and random search.
tained by nsgaii sm are higher than the medians and averages of the hv values obtained by nsgaii.
given the large execution time of our test generation algorithm in practice testers will likely have the time to run the algorithm only once.
with nsgaii certain runs really fare poorly even after the initial min of execution.
figure c shows thehv results over time for the worst run of nsgaii nsgaiism and random search among our runs.
as shown in the figure the worst run of nsgaii yields remarkably lower hv values compared to the worst run of nsgaii sm.
with nsgaii the tester might be unlucky and by running the algorithm once obtain a run similar to the worst nsgaii run in figure c .
since the worst run of nsgaii sm fares much better than the worst run of nsgaii we can consider nsgaii sm to be a safer algorithm to use especially under tight time budget constraints.
we note that as shown in figure c the hv values do not necessarily increase monotonically over time.
this is because hv may decrease when due to the crowding distance factor solutions in sparse areas but slightly away from the reference pareto front are favored over other solutions in the same pareto front rank .
we further compared the gd values obtained by nsgaii and nsgaii sm for runs of these algorithms up to min.
similar to the hv results after min executing these algorithms the average gd values obtained by nsgaii sm is better than the average gd values obtained by nsgaii.
in addition we compared the average number of simulations per generation performed by nsgaii and nsgaiism.
as expected the average number of simulations per generation for nsgaii is equal to the population size i.e.
ten .
for nsgaii sm this average is equal to .
.
hence nsgaii sm is able to perform more generations iterations than nsgaii within the same execution time.
as discussed in section for cl at a given iteration and provided with the same set p nsgaii sm behaves the same as nsgaii with a probability of .
and with a probability of .
nsgaii sm produces less accurate results compared to nsgaii.
therefore given a fixed execution time nsgaii sm is able to perform more iterations than nsgaii and with a high probability .
the solutions generated by nsgaii sm at each iteration are likely to be as accurate as those would have been generated by nsgaii.
as a result and as shown in figure a given the same time budget nsgaii sm is able to produce more optimal solutions compared to nsgaii.
finally we compared nsgaii sm with three di erent confidence levels i.e.
cl .
.
and .
.
the hv and gd results indicated that nsgaii sm performs best and better than nsgaii when clis set to .
.
comparing with random search.
figure b shows the hv values obtained by random search and nsgaii sm.
as shown in the figure after min execution nsgaiism yields better hv results compared to random search.
the hv distributions obtained by running nsgaii sm after min and until min are significantly better with a large e ect size than those obtained by random search.
similarly we compared the gd values obtained by nsgaiism and random search.
the gd distributions obtained by nsgaii sm after min and until min are significantly better than those obtained by random search with a large e ect size at min and min and otherwise a medium e ect size at other times.
to summarize when the search execution time is larger than min nsgaii sm outperforms nsgaii.
with less than min execution time both algorithms show a high degree of randomness.
when engineers cannot a ord to run the test generation algorithm for a long time for example because they make a change to the pevi system and need to rerun the test execution procedure frequently nsgaii sm 71is more likely to provide close to optimal solutions compared to nsgaii.
further as shown in figure c the worst run of nsgaii sm performs considerably better than the worst run of nsgaii.
finally nsgaii sm is able to find significantly better solutions compared to random search.
rq2 usefulness .
to demonstrate practical usefulness of our approach we have made available at some test scenario examples obtained by our nsgaii based test generation algorithms.
we presented these test scenarios as well as other scenarios to domain experts at our partner company.
the scenarios were generated for various stressful weather conditions e.g.
fog snow and rain and for situations where roadside objects block the camera s field of view or when ramped and curved roads may interfere with the pedestrian detection function of pevi.
in all the example scenarios at either pevi fails to detect a pedestrian that appears in the red warning area awa in front of a car or the detection happens very late and very close to the collision time.
as confirmed by our domain expert such scenarios had not been previously developed by manual testing based on domain expertise.
these scenarios particularly helped engineers identify particular car speed and pedestrian speed ranges and pedestrian orientations for which the pevi s detection function is more likely to fail.
in addition the light scene intensity the light orientation and reflection may impact the detection capabilities of pedestrian detection algorithms.
however due to the current imitations of prescan the pevi simulation tool discussed earlier we were not able to define fitness functions related to the scene light intensity.
to summarize our nsgaii based test generation algorithms are able to identify several critical behaviors of the pevi systems that have not been previously identified based on manual and expertise based simulations.
.
related work search based testing has largely focused on unit testing and has rarely been used for system testing.
exceptions include gui testing and the generation of system test cases to exercise non functional behaviors such as quality ofservice constraints computational resources consumption and deadline misses .
embedded software systems and their environments are prevalently captured and simulated using physics based models such as those captured by matlab simulink.
some of the test automation techniques for matlab simulink use metaheuristic search to guide testing towards the maximisation of coverage criteria for example path coverage or towards the generation of input signals that satisfy certain signal shape patterns or temporal constraints .
these testing strategies have mostly focused on unit function level testing aiming to maximize coverage or diversity of test inputs.
these strategies however are inadequate for testing complex physics based dynamic models such as those used in our case study.
our testing approach in contrast is driven by system level requirements as well as critical and stressful situations of the system and its environment.
similar to our work surrogate modeling has been previously used to approximate expensive fitness computations and simulations in the context of evolutionary and metaheuristic search algorithms.
surrogate modeling has been applied to scale up search based solutions to optimization problems in avionics chemical systems and themedical domain .
in particular combination of surrogate modeling and multi objective population based search algorithms has been applied to optimization problems in mobile ad hoc networks manufacturing and optimizing energy consumption in buildings .
these techniques however solely rely on surrogate model predictions to select best candidate solutions without using the prediction errors and confidence levels.
this may lead to a significant deviation between the best solutions selected based on surrogate model predictions and those solutions that would have been selected based on actual fitness computations.
in contrast in our work we use the prediction errors to decide whether we should compute actual fitness values for candidate solutions or not.
further we show that when actual fitness values are not better than their respective optimistic predictions nsgaii and nsgaii sm behave the same but nsgaii sm is likely to call less simulations per iteration than nsgaii.
in surrogate modeling has been used in conjunction with single objective local search such that prediction errors and actual fitness values are used to ensure the search algorithm accuracy.
our work di ers from the work of as we combine surrogate modeling with multiobjective population search algorithms.
.
conclusion physics based simulation tools provide feasible and practical test platforms for control and perception software components developed for self driving cars.
we identified the following two key challenges that hinder systematic testing based on these simulation tools these tools lack the guidance and automation required to generate test cases that would be likely to uncover faulty behaviors and executing individual test cases is very time consuming.
we proposed an approach based on combination of multi objective search and neural networks.
we developed meta heuristics capturing critical aspects of the system and its environment to guide the search towards exercising behaviors that are likely to reveal faults.
our proposed search algorithm relies on neural network predictions to bypass actual costly simulations when predictions are su cient to conclusively prune certain solutions from the search space.
our evaluation performed on an industrial system shows that our search based algorithm outperforms random test generation combining our search algorithm with neural networks improves the quality of the generated test cases under a limited and realistic time budget and our approach is able to identify critical system and environment behaviors.
due to the current technical limitations of the simulation tool that we used in our study we had to rely on a subset of the pevi input elements i.e.
the properties of the vehicle and the pedestrian to generate test cases.
our approach however is general and can account for various critical properties of the environment once the technical limitations of our current simulation tool are resolved.
in future we plan to perform more experiments and to further improve our search algorithm by dynamically refining the neural network models using simulations performed during the search.
.
acknowledgement we gratefully acknowledge funding from iee s.a. luxembourg and from fonds national de la recherche luxembourg under grant fnr p10 verification and validation lab.
.