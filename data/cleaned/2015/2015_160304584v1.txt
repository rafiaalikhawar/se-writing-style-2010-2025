arxiv .04584v1 mar 2016semi supervised verifiedfeedback generation shalini kaleeswaran anirudh santhiar aditya kanade indian institute of science bangalore shalinik anirudh s kanade csa.iisc.ernet.insumit gulwani microsoftresearch redmond sumitg microsoft.com abstract studentshaveenthusiasticallytakentoonlineprogrammin glessons and contests.
unfortunately they tend to struggle due to la ck of personalized feedback when they make mistakes.
the overwhe lming number of submissions precludes manual evaluation.
the re is an urgent need of program analysis and repair techniques cap able of handling both the scale and variations in student submiss ions while ensuring qualityof feedback.
towards this goal we present a novel methodology called semisupervised verified feedback generation .
we cluster submissions by solution strategy and ask the instructor to identify or ad d a correct submission in each cluster.
we then verify every submis sion in a cluster against the instructor validated submission i n the same cluster.
if faults are detected in the submission then feedb ack suggestingfixestothemisgenerated.
clusteringreducesthebu rdenon theinstructorandalsothevariationsthathavetobehandle dduring feedbackgeneration.
theverifiedfeedbackgenerationensu resthat only correct feedback is generated.
wehaveappliedthismethodologytoiterativedynamicprogr amming dp assignments.
our clustering technique uses featu res of dp solutions.
we have designed a novel counter example guided feedback generation algorithm capable of suggesting fixes to all faults in a submission.
in an evaluation on 2226submissions to 4problems we could generate verifiedfeedback for submissionsin .6seachonanaverage.
ourtechniquedoesagood job of reducing the burden on the instructor.
only one submis sion had tobe manually validatedor added for every 16submissions.
.
introduction programming has become a much sought after skill for superi or employment intoday stechnology driven world .
studen tshave enthusiastically taken toonline programming lessons andc ontests in the hope of learning and improving programming skills.
un fortunately theytendtostruggle due tolackofpersonalized f eedback when they make mistakes.
the overwhelming number of student submissions precludes manual evaluation.
there is an urgen t need of automated program analysis and repair techniques capabl e of handlingboththescaleandvariationsinstudentsubmissio ns while ensuring quality of feedback.
a promising direction is to cluster submissions so that the instructor provides feedback for a representative from each c luster which is then propagated automatically to other submission s in the same cluster .
this provides scalability while k eeping the instructor efforts manageable.
many novel solution s have been proposed in recent times to enable clustering of progra ms. theseincludesyntacticortest basedsimilarity co occurrence of code phrases and vector representati ons obtained by deep learning .
however clustering ca n bestudent submissions ?
?
?
?
?
?
?
?
?clustersof submissions ?
?
??
?
????
clusterswith validatedsubmissions???
?
?
?
check check check submissions with feedback check check check check check check ?clustering by solution strategy instructorvalidated submissions verified feedback generation figure semi supervisedverified feedback generation is a submission verified to be correct is a faulty submission for which feedback is generated and ?
is an unlabeled submission for whichfeedbackis notgenerated.
correctonlyina probabilistic sense.
thus thesetechniquescannot guarantee that the feedback provided manually by the instru ctor by looking only at some submissions ina cluster would indee d be suitable to all the submissions in that cluster.
as a result some submissions may receive incorrect feedback.
further if su bmissions that have similar mistakes end up in different cluster s some of them may not receive the suitable feedback.
instead of hel ping these drawbacks can cause confusion among the students.
toovercome thesedrawbacks wepropose anovelmethodology in which clustering of submissions is followed by an automat ed feedback generation phase grounded in formal verification.
figure1showsourmethodology called semi supervisedverifiedfeedback generation .
given a set of unlabeled student submissions we first cluster them by similarity of solution strategies an d ask the instructor to identify1a correct submission in each cluster.
if none exists the instructor adds a correct solution similar to the submissions in the cluster after which we do clustering aga in.
in the next phase each submission in a cluster is verified against the instructor validated submission in the same cluster.
if any faults are detected in the submission then feedback suggesting fixe s to themisgenerated.
becauseprogramequivalencecheckingis anundecidable problem it maynot be possible togenerate feedba ck for every submission.
we let the instructor evaluate such submi ssions manually.
this is better than propagating unverified or inco rrect feedback indiscriminately.
theproposed methodology has severaladvantages.
first it uses unsupervised clusteringtoreduce theburdenonthe instruc tor.
the supervision from the instructor comes in the form of identif ying a correct submission per cluster.
second the verification p hase 1to minimize instructor s efforts further we discuss strat egies to suggest potentiallycorrect submissions tothe instructor .1void main inti j n max 3scanf d n input intm dp dp is the dp array for i i n i for j j i j 7scanf d m input dp m initialization for i i n i for j j i j if j dp dp m update else if j i dp dp m update else if dp dp dp dp m update elsedp dp m update max dp for i i n i if dp max max dp 23printf d max output figure2 acorrect submissionfor thematrix pathproblem.
complements clustering by providing certainty about correctness of feedback.
clustering helps by reducing the variations re quired to be handled during feedback generation.
it would be difficu lt to check equivalence of very dissimilar submissions e.g.
an iterative solution against a recursive solution.
since only submissi ons that are similar i.e.
belong to the same cluster are compared there is a higher chance of making equivalence checking work in pract ice.
third feedback is generated for each submission separatel y so it is personalized.
this is superior topropagating a manually written feedback indiscriminatelytoallthe submissions ina clust er.
to demonstrate the effectiveness of this methodology we ap ply it to iterative dynamic programming assignments.
dynamic p rogramming dp is a standard technique taught in algorith ms courses.
shortest path and subset sum are among the many pr oblems that can be solved efficiently by dp.
we design features t hat characterizethedpstrategyandextractthemfromstudents ubmissions by static analysis and pattern matching.
the features include the types of the dp arrays used for memoizing solutions to sub problems and how the sub problems are solved and reused ite ratively.
the set of features we need is small and all features are discrete valued.
therefore our clustering approach is ver y simple and works directlyby checking equality of feature values.
we also propose a novel feedback generation algorithm calle d counter example guided feedback generation .
the equivalence between two submissions is checked using syntactic simplifica tions and satisfiability modulo theories smt based constrain t solving.
ifanequivalencecheckfails ouralgorithmusesthecounte r example generated by the smt solver to refine the equivalence query.
t his process terminates when our algorithm proves the equivalen ce or is unable to refine the query.
a trace of refinements leading to a logically validequivalence queryconstitutes the feedbac k. as an example consider the matrix path problem2taken from a popular programming contest site codechef.
a lower triang ular matrixof nrows is given.
startingata cell apathcan be traversed in the matrix by moving either directly below or diagonally b elow totheright.
theobjectiveistofindthemaximumweightamong the pathsthatstartatthecellinthefirstrowandfirstcolumn an dendin any cell in the last row.
the weight of a path is the sum of all ce lls inta intb return a b ?
a b 4intmax arr intarr inti max max arr for i i i if arr max max arr return max 11intmain intn i j a d d is the dp array 13scanf d n input for i i n i for j j i j scanf d a input d a initialization for i i n i for j j i j d a max d d update intans max arr d 22printf d ans output return figure a faulty submission belonging to the same cluster a s thecorrect submissioninfigure2.
inthedeclaration types of aanddshould be int intheupdate under guard j compute d d a instead of d a d d ?
d d .
under guard j !
j i compute d d a instead of d a d d ?
d d .
intheoutput under guard true compute maximum overd ... d instead of d ... d .
figure the auto generated feedback for the submission in figure3byverifyingitagainst the submissioninfigure2.
along that path.
figure shows an example correct submissio n to this problem and figure shows a faulty submission.
the tw o programs are syntactically and structurally quite differe nt but both use 2d integer arrays for memoization and iterate over them i nthe samemanner.
ourclusteringtechniquethereforeputsthemi ntothe same cluster.
this avoids unnecessarily creating many clus ters but requires a more powerful feedback generation algorithm tha t can handle stylisticvariations betweensubmissions e.g.
figure 3uses multiple procedures whereas figure 2uses only one.
our algorithm automatically generates the feedback in figu re forthefaultysubmissionbyverifyingitagainstthecorrec tsubmission.
the first correction suggests that the submission shou ld use array sizes as int instead of hardcoded value of int .
theupdatetothedparray datline20infigure3missessomecorner cases for which our algorithm generates corrections a nd above.
the computation of output at line should use the cor rect array bounds as indicated by correction .
this is a compreh ensive listof changes tocorrect the faulty submission.
we have implemented our technique for c programs and evaluatediton 2226studentsubmissions to 4problems fromcodechef.
on1911 ofthem wecouldgenerate feedbackbyeitherverifying them to be correct or identifying faults and fixes for them.
in addition to faults in wrong answers we also found faults i n 265submissions accepted by codechef as correct answers!
like mostonlinecontestsites codechefusestest basedevalua tion.
ourstatic verification technique has a qualitative advantage o ver the test based approach of online judges.
the submissions come from 1860students from over 250different institutes and are therefore representativeofdiversebackgroundsandcodingstyles.
e venthen thenumber ofclustersrangedonlyfrom 80acrosstheproblems.
our technique does a good job of reducing the burden on the instructor.
on an average using one manually validated or add ed submission we generated verified feedback on 16other submissions.
we had to add only 7correct solutions manually.
while our technique generated feedback automatically for 1911submissions the remaining submissions require manual evaluation.
ourtechnique isfastandonanaverage took .6stogeneratefeedback for eachsubmission.
workonfeedback generation sofar has focused on introductory programming assignments .
in comparison we address the challenging class of algorithmic assignments in particular that of dynamic programming.
the program repair approa ches fordevelopers dealwithoneprogram atat ime.
weworkwithallstudentsubmissionssimultaneously.
todos o we proposeamethodologyinspiredbybothmachinelearningand verification.
unlikethedevelopersetting wehavetheluxuryof calling upon the instructor to identify or add correct solutions.
we exploit this to give complete and correct feedback but then our techn ique must solve the challenging and in general undecidable pr oblem of checking semantic equivalence of programs.
the salient contributions of this workare as follows wepresentanovelmethodologyofclusteringofsubmissions followedbyprogramequivalence checkingwithineachcluster.
this methodology can pave way for practical feedback generation tools capable of handling both the scale and vari ations in student submissions while minimizing the instru ctor s effortsand ensuring qualityof feedback.
we demonstrate that this methodology is effective by applying it to the challenging class of iterative dp solutions.
we design a clustering technique and a counter example guided feedback generation algorithm for dpsolutions.
we experimentally evaluate the technique on 2226submissions to 4problems and generate verifiedfeedback for of them.
we show that our technique does not require many inputs from the instructor and runs efficiently.
.
detailed example we now explain in details how our technique handles the motivating example from the previous section.
.
clustering phase the two submissions in figure and figure are syntacticall y and structurally quite different.
our technique extracts f eatures of the solution strategy in a submission.
these features are mo re abstract than low level syntactic or structural features and put the superficiallydissimilarsubmissions intothe same cluster.
the solution strategy of a dp program is characterized by the dp recurrence being solved .
the dp recurrence for the cor rect submission infigure 2isas follows dp m ifi j dp m ifi ne ationslash j dp m ifj i ne ationslash max dp dp m otherwise where dpisthedparray mistheinputmatrixof nrows and igoes from 0ton jgoes from 0toiandmaxreturns the maximum ofthe twonumbers.
the dprecurrence of the submissioninfigur e3 is similarbut missesthe second andthirdcases above.
comparingtherecurrencesdirectlywouldbeidealbutextra cting them is not easy.
students can implement a recurrence formul a in different imperative styles.
they may use multiple procedu res as infigure and arbitrary temporary variables tohold inter mediate results.
rather than attempting to extract the precise recu rrence we extract some features of the solution to find submissions t hat use similar solution strategies.
for this our analysis ide ntifies and labels the dparrays used ineach submission.
italso identifi es and labels key statements that read inputs initialize the dp array update the dp array elements using previously computed ar ray elements and generate the output.
the comments in figure and figure3identify the dparrays and keystatements.
wecallaloopwhichisnotcontainedwithinanyotherstateme nt as atop level loop .
forexample the loopat lines infigure is a top level loop but the loop at lines is not.
more gen erally a statement which is not contained withinany other sta tement is atop level statement .
thefeatures extracted by our technique and theirvalues for the submission infigure2are as follows .
type andthe number ofdimensions of the dparray an bracketle tint an bracketri ht .
whether the input arrayis reusedas the dp array no .
the number of top level loops which contain update statements for the dparray .
foreach top level loop containing updates tothe dparray a the loop nestingdepth b the direction of loop indices an bracketle t an bracketri ht indicating that the respective indices are incremented by one in each iterationof the corresponding loops c the dparrayelement updated inside the loop dp extracting these features requires static analysis and syn tactic pattern matching.
the most challenging part is identifying which array serves as a dp array.
an array which is defined in terms of itself is identified as a dp array since in the dp recurrence t he dp array appears on both sides.
however a student may use som e temporaryvariablestostoreintermediateresultsofdpcom putation andpass valuesacrossprocedures.
asexplainedinsection3 .
we track data dependences inter procedurally.
in figure th e array elements d andd are passed to the procedure max.
through inter procedural analysis our technique infers t hat the return value of maxis indeed defined in terms of these arguments and hence dis defined in terms of itself at line .
thereby it discovers that dis a dparray.
the submission in figure yields similar feature values and is clustered along with the submission in figure .
note that ou r objective is not to use clustering to distinguish between corr ect and incorrect submissions.
we therefore do not encode the exact nature of the initialization or update of the dp array in the fea tures.
analyzing these is lefttothe verificationandfeedback phas e. .
verification andfeedback phase afterclustering suppose theinstructor identifiesthesub mission in figure as correct.
our objective is to verify the submiss ion in figure against it and suggest fixes if any faults are found.
f or brevity wewillrefertothe submissioninfigure 2as the reference and the submission infigure 3as the candidate .
by analyzing the sequence in which inputs are read our technique infers that the candidate uses two input variables an integer variable nand a 2d integer array a where nis read first line andasecond line .
their types respectively match the types of the input variables nandmof the reference except that the candidate uses a hardcoded arraysize for a. bothsubmissions use 2dis 1 valid?
is 2 valid?
is 3 valid?
a successive equivalence queries and resultscounter example e1 counter example e2 counter example yesyes yesusing the counter example e1 the algorithm discovers that when j 2computes statement s1 line in figure but according to 1 it should compute s2 d d a .
let j ne ationslash s1 j s2 .
using the counter example e2 the algorithm discovers that when j i j ne ationslash 2computes s1but according to 1 it should compute s3 d d a .
let j ne ationslash j ne ationslash i s1 j ne ationslash j i s3 j s2 .correction correction 3 1 pre 1 2 post 2 pre 1 post 3 pre 1 post b refinementstepsandcorrections suggested figure5 stepsof theverified feedbackgeneration algorith m for the dpupdateinthefaultysubmissioninfigure3.
integer dp arrays but the candidate hardcodes array size of t he dp array d also.
while the reference can handle arbitrarily lar ge input matrices the candidate can handle input matrices only u p to size101 .
for smaller input matrices the reference is more space efficient than the candidate.
our technique therefore emits correction 1infigure 4suggesting size declarations for aandd.
we check equivalence of matching code fragments of the two submissions one by one.
the matchingcode fragments are ea syto identifygiventhestatementlabelscomputed duringfeatur e extraction.
forourexample line20ofthecandidateisan update statement and lines12 and17of the reference arealso upda te statements.
therefore thetop level loop say l2 atlines of the candidate matches the top level loop say l1 at lines of the reference.
the question iswhether theyare equivalent.
we check equivalence of the loop headers first.
the input vari ables nin both submissions correspond to each other and are not re assigned before they are used in the respective loop head ers.
therefore the loop headers of l1andl2are equivalent.
thus the corresponding loop indices are equal ineachiteration.
to check equivalence of loop bodies our algorithm formulat es anequivalence query 1which asserts that inan i j thiteration if the two dp arrays are equal at the beginning then they are equa l at the end of the iteration.
the equivalence query is of the form 1 pre 1 2 post where preencodes the equality of dp arrays loop indices and input variables at the beginning of the iteration and the lo wer and upper bounds on the loop index variables in the reference post encodes the equality of dp arrays at the end of the iteration we syntacticallycheck thatinput variables arenot changed 1isa formula encoding the statements inthe loop body of the refer ence and 2is a formula encoding the statements inthe loopbody of the candidate.
converting a loop free sequence of statemen ts into a formula is straightforward.
for example an if statement such asif p x e is converted to a guarded equality constraint p x ewhere x is a fresh variable.
the predicates in an if else statementarepropagatedsoastomaketheguardsmutuallydi sjoint and finally the conjunction of all guarded equality constra ints is taken.
we defer other technical details tosection3.
.
as shown in figure .a the algorithm checks whether 1is a logically valid formula.
the smt solver finds the followingcounter example e1whichshows that the formula isnot valid j dp d m a dp d dp d following the usual convention we use as equality in formulae and use as equality in code.
similarly ne ationslash and !
denote disequalitysymbols informulae versus code.
our algorithm called counter example guided feedback generationalgorithm uses e1tolocalizethe faultinthecandidate.
itfirst identifies which guards are satisfied by the counter example in the candidate and the reference and whether they are equivalen t. the guard j 0issatisfiedin 1andtheimplicitguard trueforline20 issatisfiedin 2. sincetheyarenotequivalent thealgorithminfers thatthefaultysubmissionismissingacondition.
onthecon trary if the guards turnout tobe equivalent the fault is localized t othe assignmentstatement.
itthenderivesaformula 2giveninfigure5.b whichletsthecandidate compute line20under theguard j!
0and makes it compute d d a under j .
this assignment statement is obtained from the assignment at lin e under the guard j 0of the reference in figure by substituting thevariablesfromthecandidate.
thealgorithmrecordsthi srefinement inthe form of correction of figure .
as shown in figure .a it checks validity of 2 pre 1 postobtained by replacing 2 the encoding of candidate s loop body by defined in figure .b .
this results in a counter example e2using which the algorithm discovers the missing case of j iand generates correction of figure .
for brevity we do not show the counter example e2.
a refined equivalence query 3shown in figure .b is computed.
as shown in figure5.a thisformulaisvalidandestablishesthatthefa ultsinthe candidate canbe fixedusingthe synthesized feedback.
theinputandinitializationpartsofthetwosubmissionsar efound to be equivalent.
in our experiments we observed certain re peating iterative patterns.
the computation of a maximum over an array in lines in figure is one such example.
we encode syntactic patterns to lift these to certain predefined funct ions.
we define maxwhich takes the first and the last elements of a contiguous arraysegmentasarguments andreturnsthemaximum o ver the array segment.
in figure the output expression in term s of maxis max d d and in figure the output is max dp dp .
asyntacticcomparison between the twoleads tocorrection 4infigure .
.
technicaldetails wenowexplainourapproachforclusteringsubmissions and the algorithm forverifiedfeedback generation.
.
clustering by solutionstrategy the first phase of our technique is to cluster submissions by t he solution strategysothat eachcluster can be analyzed separ ately.
.
.
featuredesign section .
has already introduced the features of a submiss ion that we use.
typically inmachine learning a large number o f features are obtained and then the learning algorithm finds the i mportant ones called feature selection .
in our case since the domain is well understood we design a small number of suitable fea tures that provide enough informationabout the solutionstrateg y. in particular we cluster two submissions together if the y use the same type and dimensions for the dp arrays either both use dp arrays distinct from the input arrays or not and there i s a one to one correspondence between top level loops which c ontain dpupdate statements the loops should have thesame depth d irection and the dp array element being updated.
twosubmissi ons inthe same cluster can differinall other aspects.
the rationale behind these features is simple checking equ ivalence of two submissions which use the same types of dp array s andsimilardpupdateloopsiseasierthaniftheydonotshare these properties.
for example the subset sum problem can be solv ed by using either a boolean dparrayor an integer dparray but the two implementations are hard to compare algorithmically.
reca ll the matrix path problem stated in the introduction.
consider a s ubmissionwhichtraversesthematrixfromtop to bottomanda nother which traverses it from bottom to top.
using one to validat e the other isdifficultandperhaps evenundesirable.
thefeatur esofdp update loops will prevent these submissions from being part of the same cluster.
imposing further restrictions by adding mor e features canmakeverificationsimplerbutwillincreasethebu rdenon the instructor by creatingadditional clusters.
the feature .c described in section .
requires a bit more e xplanation.
we want to get the dp array element being updated inside each loop containing a dp update statement.
if two sub missions use different names for dp arrays and loop indices we cannot compare them.
to compare them across submissions ou r technique uses canonical names for them dpfor the dp array and loop indices i j k etc.
from the outer to inner loops.
if a submission uses multiple dparrays thenwe assign subscripts to dp.
.
.
featureextraction identifying input statements and variables is simple.
we lo ok for the common c library functions like scanf.
the case of output statements is similar.
a variable xis identified as a loop index variable if xis a scalar variable xis initialized before the loop is entered xupdated inside the loop and xis used in the loop guard.
identifying dp arrays requires more subtle a nalysis discussed below.
we call dp arrays input variables and l oop indices in a submission as dp variables .
all other variables are calledtemporary variables.
to eliminate the use of a temporary variable xat a control locationl wecompute a set of guarded expressions g1 e1 .
.
.
gn en where the guards and expressions are defined only over dp vari ables and the guards are mutually disjoint.
we denote this s et by l x and call thesubstitution store .
semantically if gk ek l x then xandekevaluate to the same value at lwhenever gkevaluates to true at l. the substitution store is lifted in a natural manner to expressions and statements.
for instance fo r an assignment statement s x e l s g1 x e .
.
.
gn x e n where g1 e1 .
.
.
gn en l e .
gulwani and juvekar developed an inter procedural backward symbolic execution algorithm to compute symbolic boun ds onvaluesofexpressions.
whilewearenotinterestedintheb ounds the equality mode of their algorithm suffices to compute subs titutionstores.
we referthe reader to forthe details.
to determine whether a statement sat location lis an initialization or an update statement we perform pattern matching over l s .
if the same array appears on both sides of an assignment statementthenthearrayisidentifiedasadparrayandthesta tement is labeled as an update statement.
a statement where the lhs i s a dp array and rhs is an input variable or a constant is labeled a s an initialization statement.
in l s the temporary variables in s arereplacedbytheguardedexpressionsfromthesubstituti onstore.
this makes the labeling part of our tool robust even in presen ce of temporaries and procedure calls.
for example suppose we ha vet x x t .
thesecondstatementcanbeidentifiedasanupdatestatementthroughpatternmatchingonlyifwesubstitu tex in place of ton the rhs of the statement.
in general l s may containmultipleguardedstatements.
if l s s1 .
.
.
sn we require that all of s1 .
.
.
snsatisfy the same pattern and get the same label.
extractingthe feature values is now straightfo rward.
.
.
clusteringandidentifyingcorrectsubmissions allourfeaturesarediscretevalued.
therefore ourcluste ringalgorithm is very simple and works directly by checking equali ty of featurevalues.
once theclusteringisdone weasktheinstr uctorto identify a correct submission from each cluster.
to reduce i nstructor sefforts wecanemploysome heuristicstorankcandida tes ina clusterandpresentthemone by one totheinstructor.
fore xample we can use a small set of tests or majority voting on some other features of submissions like the loop bounds of update loops .
the instructor can accept a submission as correct or add a mod ified version of an existing submission.
if none of this is pos sible the instructor can write a correct solution similar to the so lutions in the cluster.
if a new submission is added we perform clust ering again.
the instructor may have correct solutions from a prev ious offeringofthecourseiftheassignment isrepeatedfromapr evious offering.
theinstructorcanaddthemtothedatasetevenbef ore we apply clusteringtothe submissions.
.
verified feedback generation once thesubmissions areclusteredandtheinstructor has id entified a valid submission for each cluster we proceed to the ver ified feedback generation phase.
we check semantic equivalence o f a submissionfromacluster calledthe candidate withtheinstructorvalidated submission from the same cluster calledthe reference .
.
.
variableandcontrolcorrespondence program equivalence checking is an undecidable problem.
in practice a major difficultyis establishing correspondenc e between variables and control locations of the two programs .
we exploit the analysis information computed during feature ext raction tosolve this problem efficiently.
let be a one to one function called a variable map .
maps the input variables and dp arrays of the reference to the corr esponding ones of the candidate.
to obtain a variable map the input variables of the two submissions are matched by consider ing the order in which they are read and their types.
the dp arrays are matched based on their types.
if there are multiple dp arr ayswith the same type in both submissions then all type compati ble pairsareconsidered.
thisgeneratesasetofpotentialvari ablemaps andequivalence checkingisperformedforeachvariablemap separately.
theonewhichsucceedsandproducestheminimumnumb er of corrections is used for communicating feedback to the stu dent.
in equivalence checking we eliminate the occurrences of te mporary variables using the substitution store computed durin g feature extraction.
we therefore do not need to derive corresponden ce betweentemporary variables whichsimplifiesthe problem gre atly.
the feature extraction algorithm labels the input initial ization update and output statements of a submission.
we refer to the se statements as labeled statements .
the labeled statements give an easy way to establish control correspondence between the su bmissions.
we now use the notion of top level statements defined i n section .
.
let r s1 .
.
.
sk be the list of all top level statements of the reference such that each statement in rcontains at least one labeled statement and the order of statements in ris consistent withtheirorder inthereferencesubmission.
it iseasyto seethatthetop levelstatementsinasubmissionaretotall yordered.
let c s1 .
.
.
sn be the similar list for the candidate submission.
withoutlossofgenerality fromnowon weassumethat there is only one dp array in a submission and the top level stateme nts are possibly nested loops.
a top level loop in ror cmay contain multiple statements which have different labels.
for example a loop may read the input and also update the dp array.
we call it a heterogeneous loop.
if a loop reads two different input variables then also we cal l it a heterogeneous loop.
heterogeneous loops make it difficult to establishcontrolcorrespondencebetweenthestatementlist s rand c. fortunately it is not difficult tocanonicalize the stateme nt lists usingsemantics preserving loop transformations well known in the compilers literature .
our algorithm first does loop spli tting to split a heterogeneous loop into different homogeneous loop s. it thendoesloopmergingtocoalescedifferentloopsoperatin gonthe same variable.
specifically it merges two loops reading the same input array.
it also merges loops performing initializatio n to the same dp array.
during merging we ensure that there is no loop in between the merged loops such that it reads from or writes t o the same variable or array as the merged loops.
in our experie nce in most cases these transformations work because loops rea ding inputs or performing initialization of dp arrays do nothave loopcarrieddependences or ad hoc dependences between loops.
in contrast by definition loops performing dp updates do ha ve loop carried dependences.
we therefore do not attempt loop merging for such loops.
the feature 3in section .
tracks the number of loops containing dpupdates.
therefore twosubmissions inthe sameclusteralreadyhavethesamenumber ofloops containin gdp updates.
thus clustering helps in reducing the variants th at need tobe considered duringfeedback generation.
letrandcbetheresultingstatementlistsforthereferenceand candidate submissions respectively.
if they have the same l ength and at each index i the ith loops in the two lists operate on the variables related by a variable map the statements operating on the variables carry the same labels and the loops have th e same nesting depth and directions then we get the control correspondence r c. if our algorithm fails to compute variable or control correspondence for the candidate then it exits wi thout generating feedback implicitlydelegatingittothe instr uctor.
.
.
equivalencequeries lets 1ands 2be the top level loops from the reference and the candidate such that s s .
we first use the substitution map computed during feature extraction to eliminate temporary vari ables and procedure calls in s 1ands 2by equivalent guarded expressions over only dp arrays loop indices and input variab les.
lets1 l1 s ands2 l2 s where l1andl2are control locations of s 1ands .
we formulate an equivalence query for the iteration spaces ofs1ands2.
let corrbe the correspondence between the input variables dp arrays and loop indices of s1ands2at the matching nesting depths.
we define iter 1to be the range of the loop indices ins1andguards 1to be the disjunction of all guards present inthe loop body of s1.
similarly we have iter 2andguards 2fors2.
the equivalence query is definedas follows corr iter guards iter guards this query provides more flexibility than using direct synta ctic checking between the loop headers.
for example suppose s1is for i i n i true s ands2isfor i i n i i s .s1executes sfor1 i nands2alsoexecutes s for1 i n. a syntactic check will end up concluding that s2executes one additional iteration when i is0.
but our equivalence query establishes equivalence between the iterationspaces as de sired.
theformulationofthequery toestablishequivalence between loopbodiesof s1ands2isasdiscussedinsection2.
.
eventhough the submissions use arrays we eliminate them from the queri es.
a loop body makes use of onlya finite number of symbolic arrayex pressions.
wesubstituteeachuniquearrayexpressioninaq ueryby ascalarvariablewhileencodingcorrespondencebetweenth escalar variables in accordance with the variable map .
we overcome some stylistic variations when the order of operands of a com mutativeoperationdiffersbetweenthetwosubmissions.
fore xample says1uses x ands2uses y such that x y i a and j b. theexpressions i jandb aare notidenticalunder renaming but are equivalent due to commutativity.
to take ca re of this we force a fixed ordering among variables in the two subm issions for commutative operators.
sometimes the instructo r may include some constraints over input variables as part of the problem statement.
in the equivalence queries our algorithm ta kes inputconstraintsintoaccountandalsoaddsarrayboundschec ks.
we omit these details due tospace limit.
.
.
counter exampleguidedfeedbackgeneration algorithm 1is our counter example guided feedback generat ion algorithm.
its input is a list qof equivalence queries where each query i i correspondstothe ithstatementsinthetwosubmissions.
iencodes theequivalence ofiterationspaces and iofthe loop bodies.
if the ith statements are not loops iistrueand i just checks equivalence of the loop free statements.
the ou tput of the algorithm is a listof corrections tothe candidate submi ssion.
algorithm iterates over the query list line .
for a query i i it first checks whether iis logically valid or not.
if it isnotthenthealgorithmsuggests acorrectiontomaketheit eration spaces of the ith statements loops of the two submissions equal lines .
itthen enters a refinement loop for iat lines7 .
during each iteration of the refinement loop it checks wheth er iis valid.
if yes it exits the loop line .
otherwise it get s a counter example from the smt solver and finds the guarded statements that are satisfied by .
let g1 s1 1and g2 s2 2be those statements line .
the formulae 1and 2 correspondtotheencodings oftheloopbodiesofthereferen ceand the candidate respectively.
note that the conversion of sta tements toguardedequalityconstraints section2.
ensuresthat theguards within 1and within 2are pairwise disjoint.
let be the variable map which is same as the variable correspondence butaugmentedwiththecorrespondence betweenloopalgorithm1 algorithm g enfeedback input alist q of equivalence queries output alist of corrections to the candidate submission 1foreach i i qdo 2if ne ationslash ithen 3suggest corrections to make theiteration spaces of the ith statements of the two submissions equal 4end 5let i pre 1 2 post k 7repeat k k 9if ithen break else let ne ationslash ibeacounter example letg1 s1 1andg2 s2 2s.t.
g1and g2 if pre g1 g2 then 2 i i 2 suggest computation of s1 instead of s2under g2 else h2 g2 g1 h g2 g1 2 g2 s2 h2 s1 h s2 i i 2 suggest computation of s1 instead of s2under h2 end 22end 23until k 24ifk thensuggest acorrection to replace 2by 1 25end indices atthe samenestingdepths for the ithstatements.
thefunction is lifted in a straightforward manner to expressions and assignments.
thealgorithmchecks whethertheguards g1andg2are equivalent line .
if they are then the fault must be in the assignmentstatement s2.
itthereforedefines 2bysubstituting s2by s1 in 2 line13 andrefines ibyreplacing 2by line14 .
it suggests an appropriate correction for the candidate sub mission line .
the other case when the guards are not equivalent l eads to the other branch lines .
the algorithm now splits t he guarded assignment g2 s2to make it conform to the reference under h2 g2 g1 whereas for h g2 g1 the candidate can continue to perform s2 line .
it computes 2by replacing g2 s2byh2 s1 andh s2 line .
it then refines ibyreplacing 2by line19 andsuggestsanappropriatecorrectionforthecandidatesubmission line20 .
therefineme ntloop terminates when no more counter examples can be found line and thus progressively finds allsemantic differences between ith statements of the twosubmissions.
each iteration of the refinement loop eliminates a semantic d ifference betweenapairofstatementsfromthetwosubmission s and the loop terminates after a finite number of iterations.
in pr actice givingalonglistofcorrectionsmightnotbeusefultothest udentif there are toomanymistakes inthe submission.
a betteralter native mightbetostopgeneratingcorrectionsafterathresholdis reached.
we use a constant to control how many refinements should be attempted line .
if this threshold is reached then the al gorithm suggests a total substitution of 1 in place of 2 line .
in our experiments we used .
due to the explicit verification of equivalence queries our algorithm only generates correct feedback.
the feedback for the declarations of the candidate are obtained by checking dimensi ons of the corresponding variables according to .
.
implementationtable summaryof submissionsandclusteringresults.
problem total clusters with clusters with subs.
correct sub.
manuallyadded correct sub.
sumtrian mgcrnk marcha1 pptest total we consider c programs for experimental evaluation.
we have implementedthe source code analysis usingthe clangfront endof the llvm framework and use z3 for smt solving.
we presently donot support pointer arithmetic.
inthepre processingstep ourtoolperformssomesyntacti ctransformations.
it rewrites compound assignments into regular assignments.
for example x yis rewritten to x x y .
a code snippet of the form scanf d a for i i n i scanf d a where the input array is read inmultiple statements is transformed to use a single read statement.
the abov e snippet will be rewritten to for i i n i scanf d a .
sometimes students read a scalar variable and then assign ittoanarrayelement.
ourtooleliminatestheuseofthescal arvariable and rewrites the submission so that the input is read dir ectly into the array element.
another common pattern is to read a se quence of input values into a scalar one by one and then use i t in thedpcomputation.
forexample consider thecode snippet for i i n i for j j n j scanf d x dp dp x .
it does not use an array to store the sequence of input values.
we declare an array and rewrite the snippet t o use it.
when feedback is generated for the submission an explan atory note about the input array is added.
in each of the syntactic t ransformations we ensure thatthe program semantics isnot alte red.
many students especially beginners write programs with c onvoluted conditional control flow and unnecessarily comple x expressions.
inaddition therefinementsteps ofourcounter example guidedfeedbackgenerationalgorithmmaygeneratecomplex guards.
topresentclearandconcisefeedbackeveninthefaceofthes epossibilities in the post processing step our tool simplifie s guards in the feedback using the smt solver.
we use z3 s tactics to remo ve redundant clauses evaluate sub expressions to boolean co nstants and simplifysystems of inequalities.
.
experimental evaluation to assess the effectiveness of our technique we collected s ubmissions tothe following 4dpproblems3oncodechef .sumtrian described inthe introduction section.
.mgcrnk find a path from to n n in an n n matrix sothattheaverageofallintegersincellsonthepat h excluding the end points is maximized.
from each cell the pathcan extendtocells tothe rightor below.
.marcha1 the subset sum problem.
.pptest theknapsack problem.
weselected submissions tothese problems that implemented an iterative dp strategy in the c language.
a user can submit sol utions any number of times.
we picked the latest submissions f rom individual users.
these represent their best efforts and ca n benefit results of feedbackgeneration.
problem verified as corrections average unlabeled correct check suggested corrections ?
sumtrian .
mgcrnk .
marcha1 .
pptest .
total .
from feedback.
we do not consider submissions that either do not compile or crash on codechef s tests.
to enable automated te sting oncodechef the submissions had an outermost loop toite rate over testcases weidentifiedandremovedthisloopautomati cally before further analysis.
table shows the number of submissions for each problem.
sumtrian had the maximum number of submissions and pptest had the minimum .
there were a total of 2226submissions from 1860students representing over 250institutions.
thesesubmissionsemployawiderangeofcodingidiomsandma ny possible solution approaches both correct and incorrect.
this is a fairlylarge diverse and challenging set ofsubmissions.
.
effectiveness ofclustering our features were quite effective in clustering submission s by their solution strategies.
since we do not include features representinglow levelsyntacticorstructuralaspectsofsubmi ssions the clusteringresultedinonlyafewclustersforeachproblem without compromising our ability to generate verified feedback.
tab le gives the number of clusters.
the number of clusters increas ed gracefully from the smallest problem by the number of submi ssions to the largest one.
the smallest problem pptest yielded only 2clusters for 41submissions whereas the largest problem sumtrian yielded 80clustersfor 1983submissions.
ourmanual evaluation revealed that ineachcluster the solutions wer e actually following the same dpstrategy.
the small number of clusters reduces the burden on the instru ctorsignificantly.
insteadofevaluating 2226submissionsseparately the instructor is required to look at representatives from o nly114 clusters.
codechef uses test suites to classify problems in to correct and incorrect.
as a simple heuristic we randomly picke d one of the submissions marked as correct by codechef in each clus ter and manually validated it.
as shown in table this gave us co rrect representatives for clusters across the problems.
the remaining 7clusters seemed tofollow some esoteric strategies and we manually added a correct solutiontoeach of them.
clustering also helps the instructor get a bird s eye view of the multitude of solution strategies.
for example it can be use d to findthemost orleastpopular strategyusedinstudent submis sions.
insumtrian the most popular strategy with 677submissions was the one that traverses the matrixrows bottom up travers es the columns left toright and updates the element i j .
.
effectiveness offeedback generation our tool verifies a submission from a cluster against the manu ally validated or added correct submission from the same clu ster.
table shows the number of submissions verified as correct check submissions for which faults were identified and correc tions suggested and submissions which our algorithm could not handle ?
.
across the problems 1122submissions amounting to50 were verified to be correct with the maximum at for sumtrian and the minimum at for pptest.
for a total of 789submissions amounting to some corrections were sug i only0.
o only .
correct .
i u .
i u o .
u only .
u o26.
others30 figure distribution of submissions in a cluster of sumtrianbythetype of feedback.
gested by our tool.
the maximum percentage of submissions wi th corrections were for pptest at71 and the minimum was forsumtrian .
many submissions had multiple faults.
table shows the average number of corrections over faulty submiss ions foreach problem.
pptest requiredthemaximum number of corrections of .7on average.
in all our tool succeeded in either verifying or generating verifiedfeedback for submissions.
fortheremaining submissions ourtoolcouldneither generate feedback nor verify correctness.
these submissio ns need manual evaluation.
marcha1 had the maximum percentage of unlabeled submissions at and mgcrnk had the minimum at12 .
these arise either because the smt solver times out we kept the timeout of 3s for each equivalence query or due to the limitationsof the verificationalgorithm orthe implementa tion.
these results on the challenging set of dp submissions are en couraging and demonstrate effectiveness of our methodolog y and technique.
even if we assume that all 315unhandled submissions are faulty wecould generate verifiedfeedback for faultysubmissions.
incomparison onasetof introductory programming assignments singh et al.
report that of faulty submissions couldbefixedusing manuallyprovidederrormodels .
ourcounterexample guided feedback generation technique guarantees c orrectness of the feedback.
in addition we would have liked to communicate the feedback to the students and assess their respo nses.
unfortunately theircontact details were not available to us.
diversityoffeedbackandpersonalization.
the feedback propagation approaches suggest that t he same feedback text written by the instructor can be propagat ed to all submissions withina cluster.
we found that this is not pr actical and the submissions withinthe same cluster require heterog eneous feedback.
figure 6shows the distributionof submissions in aclusterof sumtrian bythetypeoffeedback.
weonlyhighlightfeedbackoverthelogicalcomponentsofasubmission initializ ation i update u and output o .
feedback related to type declarat ions andinputstatements possibly inconjunctionwithfeedba ckonthe logical components issummarized under the category othe rs .
while only .
submissions were verified to be correct submissions had faults in one of the logical components of th e dp strategy initialization .
update .
and output .
.
asshown infigure a largepercentage ofsubmissions hadfa ults intwological components and .
had them inall three components.
ofthesubmissionswereintheotherscategory.
clearly itwould be difficult forthe instructor topredict faults ino ther submissions in a cluster by looking only at some submissions in t he cluster and write feedback applicable to all.
we do admit tha t figure6isbasedonourclusteringapproachandotherapproache smay yield different clusters.
even then the clusters would be c orrecttable submissionsbyfaultycomponents.
faulty comp.
sumtrian mgcrnk marcha1 pptest i only u only o only i u i o u o i u o others total submissionidsfeedback sizebefore simplification aftersimplification figure7 effectofsimplificationonfeedbacksizefor mgcrnk onlyinaprobabilistic senseandtheverificationphase wesuggest would add certainty about correctness of feedback.
our technique generated personalized feedback depending o n which components of a submission were faulty.
table shows the number of submissions by the faulty components.
across t he problems pptest had the maximum percentage .
of submissions requiring corrections to multiple logical compon ents and sumtrian had the minimum percentage .
.
the most common faultycomponents variedacross problems.
typesoffaults foundandcorrected.
our tool found a wide range of faults and suggested appropria te correctionsforthem.
thisismadepossiblebyavailability ofacorrect submission to verify against and the ability of our veri fication algorithm to refine the equivalence queries to find all faults .
the faults found and corrected include incorrect loop headers initialization mistakes including missing or spurious initializa tion missing cases in the dp recurrence errors in expressions and gua rds incorrect dimensions etc.
concisenessoffeedback.
to reduce the size of formulae in the generated feedback we perform simplifications outlined in section .
we measure th e effectiveness of the simplifications by disabling them and usi ng the sum of ast sizes nodes in the ast of the guards in our feedback text as feedback size .
figure shows the impact of the simplifications on feedback size in the case of mgcrnk by plotting submission ids versus feedback size.
the figure excludes cas es where simplification had no impact on feedback size.
simplifi cations ensured that the feedback size was at most and .1on average.
without simplifications the maximum feedback siz e was599.
simplifications where applicable reduced feedback size by .
on anaverage across the problems.
.
comparisonwithcodechef our tool was able to verify 12submissions as correct that were tagged by codechef as incorrect.
this was surprising becaus e codechefusestestswhichshouldnotproduce such falsepositives .
on investigation we found that the program logic was indeed correct as verified by our tool.
the faults were localized to out put formatting or in custom input output functions.
understa ndably black box testing used by codechef cannot distinguish betw een formatting and logical errors.
however being able to disti nguish betweenthesetypesoffaultswouldsavetimeforthestudent s. our tool finds logical faults but not formattingerrors.
due to the incompleteness of testing codechef did not identify all faulty submissions false negatives .
this can hurt students since they may not realize their mistakes.
we checked the cas es when codechef tagged a submission as correct but our tool issued some corrections.
for 64submissions our tool identified that the submissions were making spurious initializations to th e dp array.
for 112submissions our tool identified that the dp udpate wasperformedforadditionaliterationsthanrequiredandg enerated feedback to fix the bounds of loops containing update stateme nts.
importantly our tool detected out of bounds array accesses in99 submissions andsuggestedappropriatecorrections.
in 265distinct submissions ourtool was abletoidentifyone ormore of thef aults described above whereas codechef tagged themas correct!
t hus our static technique has a qualitative advantage over the te st based approach of online judges.
.
performance weranourexperimentsonanintelxeone5 .60ghzmachine with8 cores and 24gb ram.
out tool runs only on a single core.
on an average our tool generated feedback in .6s including the time for clustering and excluding the time for identi fying correct submissions manually.
.
limitations andthreats to validity our technique fails for submissions that have loop carried dependencies over scalar variables apart from the loop index v ariables submissions that use auxiliary arrays and submissio ns for whichpatternmatchingfailstolabelstatements.
weinheri tthelimitations of smt solvers in reasoning about non linear const raints and program expressions with undefined semantics such as di visionby0.
mostoftheunhandledcasesarisefromtheselimita tions.
our approach cannot suggest feedback for errors in custom in put output functions output formatting typecasting et c. our approach may provide spurious feedback enforcing stylistic c onformance with the instructor validated submission.
for examp le if a submission starts indexing into arrays from position 1but the instructor validated submission indexes from position our tool generates feedback requiring the submission to follow 0based indexing.
this may correct some misconception about array ind exing that the student may have.
nevertheless these differen ces can be either compiled away during pre processing or through sm t solving with additional annotations.
we will investigate t hese in future.
finally our implementation currently handles onl y a frequently used subset of cconstructs and libraryfunctions.
there can be faults in our implementation that might have affected our results.
to address this threat we manually chec ked the feature values and feedback obtained and did not encounter any error.
threatstoexternal validityarisebecause our resul tsmaynot generalize to other problems and submissions.
we mitigated thisthreat bydrawing upon submissions from more than 1860students on4differentproblems.
whileourtechniqueisabletohandlemo st constructs that introductory dp coursework employs furth er studies are required to validate our findings in the case of other p roblems.
in section .
we compared our tool with the classifica tion available on codechef.
the tests usedby codechef are not pub lic and hence we cannot ascertaintheir quality.
byimprovingt he test suites some false negatives of codechef maydisappear butb lackbox testing will not be able to distinguish between logical f aults and formattingerrors discussed insection5.
.
.
related work programrepresentationsandclustering.
in order to cluster submissions effectively we need strate gies to represent both the syntax and semantics of programs.
many cl ustering approaches use only edit distance between submissio ns while others use edit distance along with test based si milarity .
we use neither of these.
glassman et al.
advocate a hierarchical technique where the submissions ar e first clustered using high level abstract features and then us ing lowlevel concrete features.
an interesting recent directio n is to use deep learning to compute and use vector representations of p rograms .
peng et al.
propose a pre training te chnique to automatically compute vector representations of d ifferent astnodeswhichisthenfedtoatree basedconvolutionneura lnetwork foraclassificationtask.
pieceetal.
propose arecursive neural network to capture both the structure and functi onality ofprograms.
thefunctionalityislearnedusinginput outp ut examples.
but the class of programs considered in is very sim ple.
it onlyhandles programs whichdo not have anyvariables.
since our experiments were focused on iterative dp solution s we designed features that capture the dp strategy.
the above approaches are more general but unlike us they maynot put the s ubmissions in figure and in the same cluster.
our algorithm e xtracts features in the presence of temporary variables and p rocedures andmight be useful inother contexts as well.
feedbackgenerationandpropagation.
theideaofcomparinginstructorprovidedsolutionswithst udent submissions appears in .
it uses graph representation an d transformations for comparison of fortranprograms.
xuand chee userichergraphrepresentationsforobject orientedprog rams.
rivers andkoedinger useeditdistance asametrictocompare gr aphs and generate feedback.
gross et al.
cluster student sol utions by structural similarity and perform syntactic comparison s with a known correct solution to provide feedback.
feedback gener ated by pattern matching may not always be correct.
in contrast w e generateverifiedfeedback but for the restricteddomain of dp.
alur et al.
develop a technique to automatically grade au tomata constructions using a pre defined set of corrections .
singh et al.
apply sketching based synthesis to provide feedb ack for introductory programming assignments.
in addition to a ref erence implementation the tool takes as input an error model in the form ofcorrectionrules.
theirerrormodelistoorestrictiveto beadapted to our setting that requires more sophisticated repairs and that too for a more challenging class of programs.
gulwani et al.
address the orthogonal issue ofproviding feedback toaddress performance issues while srikantandaggarwal use machine le arning to assess coding quality of prospective employees and do not provide feedback on incorrect solutions.
the idea of exploiting the common patterns in dp programshas been used by pu et al.
but for synthesis of dp programs .
the clustering based approaches propagate the ins tructorprovided feedback to all submissions in the same cluster wh ereas we generate personalized and verified feedback for each subm issioninaclusterseparately.
overcode alsoperformscl ustering of submissions and provides a visualization technique to as sist the instructor inmanually evaluating the submissions.
programrepairandequivalencechecking.
genetic programming has been used to automatically generat e program repairs .
these approaches are not direct ly applicable in our setting as the search space of mutants is very large.
further genprog relies on redundancy present in other parts of the code for fixing faults.
this condition is not met in our s etting.
software transplantation transfers function ality from one program to another through genetic programming and slic ing.
prophet learns a probabilistic application independ ent model of correct code from existing patches and uses it to rank rep air candidates from a search space.
these are generate and val idate approaches which rely on a test suite to validate the changes .
in comparison we derive corrections for a faulty submission b y program equivalence checking witha correct submission.
konighopher et.
al.
present a repair technique using re ference implementations.
their fault model is restrictive a nd only considers faulty rhs.
many approaches rely on program speci fications for repair including contracts ltl assertions and pre post conditions .
recent appr oaches that use tests to infer specifications and propose repairs in clude semfix minthint directfix andangelix .
these approaches use synthesis symbolic execution and p artial maxsat respectively.
bothdirectfixand angelixuse pa rtial maxsat but angelix extracts more lightweight repair constr aints toachieve scalability.
spr uses parameterized transf ormation schemas to search over the space of program repairs.
in contr ast we use instructor validated submissions and a combination of patternmatching static analysis and smtsolving.
automated equivalence checking betweena program and itsop timized version has been studied in translation validation .
partush and yahav design an abstract interpretatio n based technique to check equivalence of a program and its patched v ersion.
in comparison our technique performs equivalence ch eck between programs writtenbydifferent individuals indepen dently.
all these approaches are designed for developers and deal wi th only one program at a time.
our tool targets iterative dp solu tions written by students and works on a large number of submission s simultaneously.
it combines clustering and verification to handle both the scale andvariations instudent submissions.
.
conclusionsand futurework we presented semi supervised verified feedback generation to deal with both scale and variations in student submissions while minimizing the instructor s efforts and ensuring feedback quality.
we alsodesigned a novel counter example guided feedback ge nerationalgorithm.
wesuccessfullydemonstratedtheeffecti veness of our technique on 2226submissions to 4dpproblems.
our results are encouraging and suggest that the combinatio n of clustering and verification can pave way for practical fee dback generation tools.
there are many possible directions to imp rove clustering and verification by designing sophisticated alg orithms.
we plantoinvestigate these for more problem domains.
.