transfer learning for performance modeling of configurable systems an exploratory analysis pooyan jamshidi carnegie mellon university usanorbert siegmund bauhaus university weimar germanymiguel velez christian k astner akshay patel yuvraj agarwal carnegie mellon university usa abstract modern software systems provide many configuration options which significantly influence their non functional properties.
to understand and predict the effect of configurationoptions several sampling and learning strategies have beenproposed albeit often with significant cost to cover the highlydimensional configuration space.
recently transfer learning hasbeen applied to reduce the effort of constructing performancemodels by transferring knowledge about performance behavioracross environments.
while this line of research is promising tolearn more accurate models at a lower cost it is unclear whyand when transfer learning works for performance modeling.
toshed light on when it is beneficial to apply transfer learning weconducted an empirical study on four popular software systems varying software configurations and environmental conditions such as hardware workload and software versions to identifythe key knowledge pieces that can be exploited for transfer learning.
our results show that in small environmental changes e.g.
homogeneous workload change by applying a lineartransformation to the performance model we can understandthe performance behavior of the target environment while forsevere environmental changes e.g.
drastic workload change wecan transfer only knowledge that makes sampling more efficient e.g.
by reducing the dimensionality of the configuration space.
index t erms performance analysis transfer learning.
i. i ntroduction highly configurable software systems such as mobile apps compilers and big data engines are increasingly exposed to end users and developers on a daily basis for varying use cases.users are interested not only in the fastest configuration butalso in whether the fastest configuration for their applicationsalso remains the fastest when the environmental situation hasbeen changed.
for instance a mobile developer might beinterested to know if the software that she has configuredto consume minimal energy on a testing platform will alsoremain energy efficient on the users mobile platform or ingeneral whether the configuration will remain optimal whenthe software is used in a different environment e.g.
with a different workload on different hardware .
performance models have been extensively used to learn and describe the performance behavior of configurable sys tems .however the exponentially growing configuration space com plex interactions and unknown constraints among configura tion options often make it costly and difficult to learn an accurate and reliable performance model.
even worse existing techniques usually consider only a fixed environment e.g.
fixed workload fixed hardware fixed versions of the dependent libraries should that environment change a newperformance model may need to be learned from scratch.this strong assumption limits the reusability of performancemodels across environments.
reusing performance models or fig.
transfer learning is a form of machine learning that takes advantage of transferable knowledge from source to learn an accurate reliable and less costly model for the target environment.
their byproducts across environments is demanded by many application scenarios here we mention two common scenarios scenario hardware change the developers of a soft ware system performed a performance benchmarking of thesystem in its staging environment and built a performancemodel.
the model may not be able to provide accuratepredictions for the performance of the system in the actualproduction environment though e.g.
due to the instability of measurements in its staging environment .
scenario workload change the developers of a databasesystem built a performance model using a read heavyworkload however the model may not be able to provideaccurate predictions once the workload changes to a write heavy one.
the reason is that if the workload changes different functions of the software might get activated moreoften and so the non functional behavior changes too.
in such scenarios not every user wants to repeat the costly process of building a new performance model to find asuitable configuration for the new environment.
recently theuse of transfer learning cf.
figure has been suggestedto decrease the cost of learning by transferring knowledgeabout performance behavior across environments .
similar to humans that learn from previous experienceand transfer the learning to accomplish new tasks easier here knowledge about performance behavior gained in oneenvironment can be reused effectively to learn models forthe changed environments with a lower cost.
despite itssuccess it is unclear why and when transfer learning works for performance analysis in highly configurable systems.
.
c circlecopyrt2017 ieeease urbana champaign il usa t echnical research497 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
to understand the why and when in this paper we conduct an exploratory empirical study comparing performance behavior of highly configurable systems across environmentalconditions changing workload hardware and software versions to explore what forms of knowledge can be commonly exploited for performance modeling and analysis.
specifically we explore how performance measures and modelsacross the source and target of an environmental change arerelated.
the notion of relatedness across environments gives us insights to consolidate common knowledge that is shared implicitly between the two environments from knowing entire performance distributions to knowing about the best or invalid configurations or knowing influential configuration options or knowing about important interactions.
the various forms of shared knowledge that we discovered in this empirical study provide opportunities to develop novel transferlearning that is not only based on correlation concept but alsomore diverse forms of similarities across environments.
more specifically we explore several hypotheses about the notion of common knowledge across environments.
ourhypotheses start with very obvious relationships e.g.
correlation that can be easily exploited but range toward moresubtle relationships e.g.
influential options or invalid regions remain stable that can be explored with more advanced transfer learning techniques yet to be developed.
we tested ourhypotheses across environmental changes in4 configurable systems that have been selected purposefully covering different severities and varieties.
for instance we selected simple hardware changes by changing computing capacity as well assevere changes by changing hardware from desktop to cloud .
our results indicate that some knowledge about performance behavior can be transferred even in the most severe changes we explored and that transfer learning is actually easy for manyenvironmental changes.
we observed that for small changes we can frequently transfer performance models linearly acrossenvironments while for severe environmental changes we canstill transfer partial knowledge e.g.
information about influential options or regions with invalid configurations that can stillbe exploited in transfer learning for example to avoid certainregions when exploring a configuration space.
overall ourresults are encouraging to explore transfer learning further forbuilding performance models showing broad possibilities ofapplying transfer learning beyond the relatively small changesexplored in existing work e.g.
small hardware changes low fidelity simulations similar systems .
overall our contributions are the following we formulate a series of hypotheses to explore the presenceand nature of common transferable knowledge betweena source and a target environment ranging from easilyexploitable relationships to more subtle ones.
we empirically investigate performance models of config urable systems before and after environmental changes.we performed a thorough exploratory analysis to understandwhy and when transfer learning works.
we discuss general implications of our results for perfor mance modeling of configurable software systems.
we release the supplementary material including data ofseveral months of performance measurements and scriptsfor replication i ntuition understanding the performance behavior of configurable software systems can enable i performance debugging ii performance tuning iii design time evolution or iv runtime adaptation .
a commonstrategy to build performance models is to use some formof sensitivity analysis in which the system is executedrepeatedly in different configurations and machine learningtechniques are used to generalize a model that explains theinfluence of individual options or interactions .
in this paper we are interested in how a performance model for a configurable system changes when we deploy the systemin a different environment.
to this end we distinguish be tween configuration options parameters that users can tweak the system to select functionality or make tradeoffs among performance quality and other attributes and environment changes differences in how the system is deployed and used in terms of workload hardware and version.
if a performancemodel remains relatively stable across environments e.g.
the top configurations remain the top configurations the most influential options and interactions remain most influential we can exploit this stability when learning performance modelsfor new environments.
instead of building the model fromscratch as often exhaustively measuring the same config urations on a new environment we can reuse knowledgegathered previously for other environments in a form oftransfer learning .
that is we can develop cheaper faster and more accurate performance models that allow us to make predictions and optimizations of performancein changing environments .
for example consider an update to faster hardware.
we would often expect that the system will get faster but willdo so in a nearly uniform fashion.
however we may expectthat options that cause a lot of i o operations e.g.
a backup feature may benefit less from a faster cpu than other options so not all environment changes will cause uniform changes.
iftransfer across hardware is indeed usually easy this encour ages for example scenarios in which we learn performancemodels offline on cheap hardware and transfer it to the realsystem with few expensive measurements for adjustment.
thequestion is what kind of knowledge can be exploited acrossenvironments in practice with simple or more advanced formsof transfer learning.
specifically we ask whether there existscommon information i.e.
transferable reusable knowledge c.f.
figure that applies to both source and target environments and therefore can be carried over across environments.
a. environmental changes let us first introduce what we mean by an environment the key concept that is used throughout this paper.
an environmental condition for a configurable system is determined byits hardware workload and software version.
i hardware the deployment configuration in which the software systemis running.
ii workload the input of the system on which it operates on.
iii v ersion the state of the code base at a certain point in time.
of course other environmental changesmight be possible e.g.
jvm upgrade .
but we limit this study to this selection as we consider the most common changes inpractice that affect performance behavior of systems.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
b. preliminary concepts in this section we provide definitions of concepts that we use throughout this study.
the formal notations enable us to concisely convey concepts throughout the paper.
configuration and environment space letciindicate thei th configuration option of a system a which is either enabled or disabled the definitions easily generalize to nonboolean options with finite domains .
the configuration spaceis a cartesian product of all options c dom c dom cd wheredom ci anddis the number of options.
a configuration is then a member of the configuration space where all the options are either enabled or disabled.
we describe an environmental condition eby variables e drawn from a given environment space e h w v where each member represents a set of possible values for the hardware h workload w and system version v. we use notation ec as shorthand for an environment change from workload w1to workload w2where hardware and version remain stable.
performance model given a software system awith configuration space cand environment space e aperformance model is a black box function f c e rthat maps each configuration c c ofain an environment e e to the performance of the system.
to construct a performance model we run ain a fixed environmental condition e e on various configurations ci c and record the resulting performance values yi f ci e epsilon1iwhere epsilon1i n i is the measurement noise corresponding to a normal distributionwith zero mean and variance i. the training data for learning a performance model for system ain environment eis then dtr ci yi n i wherenis the number of measurements.
performance distribution we can and will compare the performance models but a more relax representation that allows us to assess the potentials for transfer learningis the empirical performance distribution.
the performance distribution is a stochastic process pd e r that defines a probability distribution over performance measures for environmental conditions of a system.
to construct a per formance distribution for a system awith configuration space c we fit a probability distribution to the set of performance values d e yi e e using kernel density estimation in the same way as histograms are constructed in statistics .
influential option at the level of individual configuration options we will be interested in exploring whetheroptions have an influence on the performance of the system in either environment not all options will have an impact onperformance in all environments.
we introduce the notion of aninfluential option to describe a configuration option that has a statistically significant influence on performance.
options interaction the performance influence of individual configuration options may not compose linearly.
forexample while encryption will slow down the system due toextra computations and compression can speed up transfer overa network combining both options may lead to surprising ef fects because encrypted data is less compressible.
in this work we will look for interactions of options as nonlinear effects where the influence of two options combined is different fromthe sum of their individual influences .
invalid configuration we consider a configuration as invalid if it causes a failure or a timeout.c.
transferable knowledge as depicted in figure any sort of knowledge that can beextracted from the source environment and can contribute to the learning of a better model i.e.
faster cheaper more accurate or more reliable in the target environment is con sidered as transferable knowledge or reusable knowledge .there are several pieces of knowledge we can transfer such as i classification or regression models ii dependency graphsthat represent the dependencies among configurations and iii option interactions in order to prioritize certain regions in theconfiguration space.
for transferring the extracted knowledge we need a transfer function that transforms the source model to the target model tf f e s f et .
in its simplest form it can be a linear mapping that transforms the source model to the target f et f es where are learned using observations from both environments .more sophisticated transfer learning exists that reuses source data using learners such as gaussian processes gp .
iii.
r esearch questions and methodology a. research questions the overall question that we explore in this paper is why and when does transfer learning work for configurable software systems?
our hypothesis is that performance models in source and target environments are usually somehow related.
to understand the notion of relatedness that we commonlyfind for environmental changes in practice we explore severalresearch questions each with several hypotheses from strongnotions of relatedness e.g.
linear shift toward weaker ones e.g.
the stability of influential options rq1 does the performance behavior stay consistent across environments?
section iv if we can establish with rq1 that linear changes across environments are common this would be promising for trans fer learning because even simple linear transformations canbe applied.
even if not all environment changes may beamendable to this easy transfer learning we explore what kind of environment changes are more amendable to transferlearning than others.
rq2 is the influence of configuration options on performance consistent across environments?
section v for cases in which easy transfer learning are not possible rq2 concerns information that can be exploited for trans fer learning at the level of individual configuration options.specifically we explore how commonly the influential optionsremain stable across environment changes.
rq3 are the interactions among configuration options preserved across environments?
section vi in addition to individual options in rq2 rq3 concerns interactions among options that as described above can oftenbe important for explaining the effect of performance varia tions across configurations.
again we explore how commonlyinteractions are related across environment changes.
rq4 are the configurations that are invalid in the source environment with respect to non functional constraints alsoinvalid in the target environment?
section vii finally rq4 explores an important facet of invalid configurations how commonly can we transfer knowledge about invalid configurations across environments?
even if we cannot authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
transfer much structure for the performance model otherwise transferring knowledge about configurations can guide learn ing in the target environment on the relevant regions.
b. methodology design we investigate changes of performance models across environments.
therefore we need to establish the performance of a system and how it is affected by configurationoptions in multiple environments.
to this end we measure theperformance of each system using standard benchmarks andrepeated the measurements across a large number of configu rations.
we then repeat this process for several changes to theenvironment using different hardware different workloads and different versions of the system.
finally we perform theanalysis of relatedness by comparing the performance and howit is affected by options across environments.
we performcomparison of a total of environment changes.
analysis for answering the research questions we formulate different assumptions about the relatedness of the source and target environments as hypotheses from stronger to more relaxed assumptions.
for each hypothesis we defineone or more metrics and analyze environment changes infour subject systems described below.
for each hypothesis wediscuss how commonly we identify this kind of relatednessand whether we can identify classes of changes for whichthis relatedness is characteristic.
if we find out that for anenvironmental change a stronger assumption holds it meansthat a more informative knowledge is available to transfer.
severity of environmental changes we purposefully select environment changes for each subject system with the goal of exploring many different kinds of changes with differentexpected severity of change.
with a diverse set of changes we hope to detect patterns of environment changes that havesimilar characteristics with regard to relatedness of perfor mance models.
we expect that less severe changes lead to more related performance models that are easier to exploit in transfer learning than more severe ones.
for transparency werecorded the expected severity of the change when selecting environments as listed in table ii on a scale from small change to very large change.
for example we expect a smallvariation where we change the processor of the hardware toa slightly faster version but expect a large change when wereplace a local desktop computer by a virtual machine in thecloud.
since we are neither domain experts nor developers ofour subject systems recording the expected severity allows usto estimate how well intuitive judgments can eventually bemade about suitability for transfer learning and it allows us tofocus our discussion on surprising observations.
c. subject systems in this study we selected four configurable software systems from different domains with different functionalities and written in different programming languages cf.
table i .
spear is an industrial strength bit vector arithmetic decision procedure and a boolean satisfiability sat solver.
it is designed for proving software verification conditions andit is used for bug hunting.
we considered a configurationspace with options that represent heuristics for solving the problems and therefore affect the solving time.
we measuredtable i o verview of the real world subject systems system domain d c h w v spear sat solver x264 video encoder sqlite database sac compiler d configuration options c configurations h hardware environments w analyzed workload v analyzed versions.
how long it takes to solve a sat problem in all configurations in multiple environments four different sat problemswith different difficulty serve as workload measured on threehardware system with two versions of the solver as listed intable ii.
the difficulty of the workload is characterized by thesat problem s number of variables and clauses.
x264 is a video encoder that compresses video files with a configuration space of options to adjust output quality encoder types and encoding heuristics.
due to the size of theconfiguration space we measured a subset of sampledrandomly configurations.
we measured the time needed toencode three different benchmark videos on two differenthardware systems and for three versions as listed in table ii.each benchmark consists of a raw video with different qualityand size and we expect that options related to optimizingencoding affect the encoding time differently.
we judgedexpected severity of environmental changes based on thedifference between quality and size of benchmark videos.
sqlite is a lightweight relational database management system embedded in several browsers and operating systems with configuration options that change indexing and fea tures for size compression useful in embedded systems buthave performance impact.
we expect that some options affectcertain kinds of workload e.g.
read heavy rather than writeheavy workloads more than others.
we have measured 1000randomly selected configurations on two hardware platformsfor two versions of the database system as workload we haveconsidered four variations of queries that focus on sequentialreads random reads sequential write and batch writes.
sac is a compiler for high performance computing .
the sac compiler implements a large number of high level and low level optimizations to tune programs for efficientparallel executions configurable with options controllingoptimizations such as function inlining constant folding andarray elimination.
we measure the execution time of a programcompiled in randomly selected configurations to assessthe performance impact of sac s options.
as workloads we select different demo programs shipped with sac each computationally intensive but with different characteristics.workloads include monte carlo algorithms such as pfilter with multiple optimizable loops as well as programs heavily based on matrix operations like srad.
to account for measurement noise we have measured each configuration of each system and environment times and used the mean for the analyses.
while many performance andquality measures can be analyzed our primary performancemetric is wall clock execution time which is captured differ ently for each systems in table i execution time encodingtime query time and analysis time.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
iv .
p erformance beha vior consistency rq1 here we investigate the relatedness of environments in the entire configuration space.
we start by testing the strongest assumption i.e.
linear shift which would enable an easytransfer learning h1.
.
we expect that the first hypothesisholds only for simple environmental changes.
therefore wesubsequently relax the hypothesis to test whether and whenthe performance distributions are similar h1.
whether theranking of configurations h1.
and the top bottom configu rations h1.
stay consistent.
table ii summarizes the results.
h1.
the relation of the source response to the target is a constant or proportional shift.
importance.
if the target response is related to the source by a constant or proportional shift it is trivial to understandthe performance behavior for the target environment using themodel that has already been learned in the source environment we need to linearly transform the source model to get the target model.
we expect a linear shift if a central hardwaredevice affecting the functionality of all configuration optionshomogeneously changes such as the cpu or homogeneousworkload change.
previous studies demonstrated the existenceof such cases where they trained a linear transformation toderive a target model for hardware changes .
metric.
we investigate whether f c e t f c es c c. we use metric m1 pearson linear correlation between f c es andf c et to evaluate the hypothesis.
if the correlation is we can linearly transform performance models.
due to measurement noise we do not expect perfectcorrelation but we expect for correlations higher than .
simple transfer learning can produce good predictions.
results.
the result in table ii show very high correlations for about a third of all studied environmental changes.
inparticular we observe high correlations for hardware changesand for many workload changes of low expected severity.
hardware change hardware changes often result in nearperfect correlations except for severe changes where we haveused unstable hardware e.g.
amazon cloud in ec .
we investigated why using cloud hardware resulted in weaklinear correlations.
we analyzed the variance of the mea surement noise and we observed that the proportion of thevariance of the noise in the source to the target in ec 2is 2 ecs 2 ect2 .
which is an order of magnitude larger than the corresponding one in ec1 2 ecs1 2 ect1 .
.
this suggests that we can expect a linear transformation across environments when hardware resources execute in a stableenvironment.
for transfer learning this means that we couldreuse measurements from cheaper or testing servers in orderto predict the performance behavior .
moreover it alsosuggests that virtualization may hinder transfer learning.
workload change f o r spear we observed very strong correlations across environments where we have consideredsat problems of different sizes and difficulties.
also whenthe difference among the problem size and difficulty is closeracross environments e.g.
ec 3vs.ec4 the correlation is slightly higher.
this observation has also been confirmed for other systems.
for instance in environmental instance ec3 insqlite where the workload change is write heavy from sequential to batch we have observed an almost perfect correlation .
while in the read heavy workload ec4 randomto sequential read the correlation is only medium at .
first the underlying hardware contains an ssd which has different performance properties for reading and writing.
second adatabase performs different internal functions when insertingor retrieving data.
this implies that some environmental conditions may provide a better means for transfer learning.
v ersion change f o r spear ec5 and x264 ec5 the correlations are extremely weak or non existence while for sqlite ec5 the correlation is almost perfect.
we speculate that the optimization features that are determined by the configuration options for spear and x264 may undergo a substantial revision from version to version because algorithmic changes may significantly improve the way howthe optimization features work.
the implication for transferlearning is that code changes that substantially influence theinternal logic controlled by configuration options may requirea non linear form of transformation or a complete set of newmeasurements in the target environment for those options only.
insight.
for non severe hardware changes we can linearly transfer performance models across environments.
h1.
the performance distribution of the source is similar to the performance distribution of the target environment.
importance.
in the previous hypothesis we investigated the situation whether the response functions in the source andtarget are linearly correlated.
in this hypothesis we considera relaxed version of h1.
by investigating if the performancedistributions are similar.
when the performance distributionsare similar it does not imply that there exists a linear mappingbetween the two responses but there might be a moresophisticated relationship between the two environments thatcan be captured by a non linear transfer function.
metric.
we measure m2 kullback leibler kl divergence to compare the similarity between the performance distributions d ec kl pds pd t ipds ci logpds ci pdt ci where pds t are performance distributions of the source and target.
as an example we show the performance distributions of ec1 andec13and compare them using kl divergence in figure the lower the value of kl divergence is the more similar are the distributions.
we consider two distributions as similar ifd ec kl pds pd t and dissimilar otherwise.
results.
here we are interested to find environmental changesfor which we did not observe a strong correlation but forwhich there might be similarities between the performancedistributions of the environment.
for ec 6inspear ec3 inx264 ec4 6insqlite and ec5 8insac the performance distributions are similar across environments.
this implies that there exist a possibly non linear transfer function that wecan map performance models across environments.
previousstudies demonstrated the feasibility of highly non linear kernel functions for transfer learning in configurable systems .
insight.
even for some severe environmental changes with no linear correlation across performance models the performance distributions are similar showing thepotential for learning a non linear transfer function.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
performance distributions of environments depending on the severity of change may be dissimilar dec1 kl .
a b or very similar dec13 kl .
c d .
h1.
the ranking of configurations stays stable.
importance.
if the ranking of the configurations stays similar the response function is then stable across environments.
wecan use this knowledge to prioritize certain regions in theconfiguration space for optimizations.
metric.
here we use rank correlation by measuring the m3 spearman correlation coefficient between response variables.
intuitively the spearman correlation will be high whenobservations have a similar rank.
we consider rank correlationshigher than .9as strong and suitable for transfer learning.
results.
the results in table ii show that the rank correlationsare high across hardware changes and small workload changes.this metric does not provide additional insights from whatwe have observed in h1.
.
however in one environmentalchange where due to excessive measurement noise the linearcorrelation was low ec 2for spear the rank correlation is high.
this might hint that when unstable hardware conditionsexist the overall ranking may stay stable.
insight.
the configurations retain their relative perfor mance profile across hardware platforms.
h1.
the top bottom performer configurations are similar.
importance.
if the top configurations are similar across environments we can extract their characteristics and use that inthe transfer learning process.
for instance we can identify thetop configurations from the source and inform the optimizationin the target environment .
the bottom configurations canbe used to avoid corresponding regions during sampling.
notethat this is a relaxed hypothesis comparing to h1.
.
metric.
we measure m4 m5 the percentage of thpercentile top bottom configurations in the source that are also top bottom performers in the target.
results.
the results in table ii show that top bottom configurations are common across hardware and small workloadchanges therefore this metric does not provide additionalinsights from what we have observed in h1.
.insight.
only hardware changes preserve top configura tions across environments.
v. s imilarity of influential options rq2 here we investigate whether the influence of individual configuration options on performance stays consistent acrossenvironments.
we investigate two hypotheses about the influ ence strength h2.
and the importance of options h2.
.
h2.
the influential options on performance stay consistent.
importance.
in highly dimensional spaces not all configuration options affect the response significantly.
if we observea high percentage of common influential options across envi ronments we can exploit this for learning performance modelsby sampling across only a subset of all configuration options because we already know that these are the key optionsinfluencing performance.
metric.
in order to investigate the option specific effects we use a paired t test to test if an option leads to anysignificant performance change and whether this change issimilar across environments.
that is when comparing the pairsof configuration in which this option is enabled and disabledrespectively an influential option has a consistent effect tospeed up or slow down the program beyond random chance.if the test shows that an option makes a difference we thenconsider it as an influential option.
we measure m6 m7 the number of influential options in source and target we alsomeasure m8 m9 the number of options that are influential in both one environment.
results.
the results in table ii show that slightly more than half of the options for all subject systems are influentialeither in the source or target environments.
from the influentialoptions a very high percentage are common in both.
this canlead to a substantial reduction for performance measurements we can fix the non influential options and sample only alongoptions which we found influential from the source.
insight.
only a subset of options is influential which is largely preserved across all environment changes.
h2.
the importance of options stays consistent.
importance.
in machine learning each decision variable here option has a relative importance to predict the response andimportance of the variables play a key role for in the featureselection process .
here we use this concept to determine the relative importance of configuration options because inconfigurable systems we face many options that if prioritized properly it can be exploited for performance predictions .
metric.
we use regression trees for determining the relative importance of configuration options because i they havebeen used widely for performance prediction of configurablesystems and ii the tree structure can provideinsights into the most essential options for prediction becausea tree splits into those options first that provide the highestinformation gain .
we derive estimates of the importance of options for the trained trees on the source and target by examining how the prediction error will change as a result of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
options.
we measure m10 correlation between importance of options for comparing the consistency across environments.
results.
from table ii the correlation coefficient between the importance of options for different environmental changes is high and the less severe a change the higher the correlation coefficients.
this confirms our intuition that small changesin the environment do not affect the influence strength of anoption.
some environmental changes where the correlationwas low according to m1 show a high correlation betweenoption importance according to m10 ec 7inspear ec3 inx264 ec1 insac.
this observation gives further evidence that even though we did not observe a linear correlation there might exist a non linear relationship betweenperformance measures.
for instance the influence of optionsstay the same but interactions might change.
insight.
the strength of the influence of configuration options is typically preserved across environments.
vi.
p reserv ation of option interactions rq3 we state two hypotheses about the preservation of option interactions h3.
and their importance h3.
.
h3.
the interactions between configuration options are preserved across environments.
importance.
in highly dimensional configuration spaces the possible number of interactions among options is exponentialin the number of options and it is computationally infeasibleto get measurements aiming at learning an exhaustive numberof interactions.
prior work has shown that a very large portionof potential interactions has no influence .
metric.
one key objective here is to evaluate to what extent influential interactions will be preserved from source to target.here we learn step wise linear regression models a techniquethat has been used for creating performance influence modelfor configurable systems .
we learn all pairwise interac tions independently in the source and target environments.
wethen calculate the percentage of common pairwise interactions from the model by comparing the coefficients of the pairwiseinteraction terms of the regression models.
we concentratedon pairwise interactions as they are the most common formof interactions .
similar to h2.
we measure m11 m12 the number of interactions in the source target m13 the number of interactions that agree on the directionof effects in the source and the target.
results.
the results in table ii show three important observations i only a small proportion of possible interactions havean effect on performance and so are relevant confirming priorwork ii for the large environmental changes the differencein the proportion of relevant interactions across environmentsis not similar while for smaller environmental changes theproportion is almost equal iii a very large proportion ofinteractions is common across environments.
the mean percentage of interactions averaged over all changes are for spear x264 sqlite sac respectively where would mean that all pairwise combination of options have a distinct effect on performance.also the percentage of common interactions across environments is high for spear x264 sqlite sac respectively.
this result points to an important transferable knowledge interactions often stay consistent across changes.
this insight can substantially reduce measurementefforts to purposefully measure specific configurations.
insight.
a low percentage of potential interactions are influential for performance model learning.
h3.
the effects of interacting options stay similar.
importance.
if the effects of interacting options are similar across environments we can prioritize regions in the configu ration space based on the importance of the interactions.
metric.
we measure m14 the correlation between the coefficients of the pairwise interaction terms in the linear model learned independently on the source and target environmentsusing step wise linear regression .
results the results in table ii reveal a very high and in several cases perfect correlations between interactions acrossenvironments.
for several environmental changes where wepreviously could not find a strong evidence of transferableknowledge by previous metrics ec 8inx264 ec4 insqlite andec14insac we observed very strong correlations for the interactions.
the implication for transfer learning is that a linear transfer function see h1.
may not applicable forsevere changes while a complex transfer function may exist.
insight.
the importance of interactions is typically preserved across environments.
vii.
i nv alid configurations similarity rq4 for investigating similarity between invalid configurations across environments we formulate two hypotheses about the percentage of invalid configurations and their commonalitiesacross environments h4.
and the existence of reusableknowledge that can distinguish invalid configurations h4.
.
h4.
the percentage of invalid configurations is similar across environments and this percentage is considerable.
importance.
if the percentage of invalid configurations is considerable in the source and target environments this pro vides a motivation to carry any information about the invalidconfigurations across environments to avoid exploration ofinvalid regions and reduce measurement effort.
metric.
we measure m15 m16 percentage of invalid configurations in the source and target m17 percentage of invalid configurations which are common between environments.
results.
the results in table ii show that for spear and x264 a considerable percentage of configurations are invalid and all of them are common across environments.
for sac approximately of the sampled configurations are invalid.
for some workload changes the percentage of common invalid configuration is low .
the reason is that some options in sac may have severe effects for some programs to be compiled but have lower effects for others.
insight.
a moderate percentage of configurations are typically invalid in both source and target environments.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
h4.
classifiers for distinguishing invalid from valid configurations are reusable across environments.
importance.
if there are common characteristics among the invalid configurations we can learn a classifier in the source to identify the invalid configurations and transfer the knowledge classifier model to the target environment to predict invalidconfigurations before measuring them thus decrease cost.
metric.
we learn a classifier using multinomial logistic regression .
it is a model that is used to predict the probabilitiesof being invalid given a set of configuration options.
wemeasure m18 the correlation between the coefficients i.e.
the probability of the configuration being invalid of the classification models that has been leaned independently.
results.
the results in table ii show that for spear and x264 the correlations between the coefficients are almost perfect.
for sac in environmental changes where the common invalid configurations are high the correlations between coefficients are also very high.
for two cases ec6 7inspear we could not find any reusable knowledge previously with other metrics.here we can observe that even when the influence of optionschange the region of invalid configurations may stay the same.
insight.
information for identifying invalid regions can be transferred with a high confidence across environments.
viii.
l essons learned and discussion based on our analyses of environmental changes we can discuss lessons learned implications and threats to validity.
a. lessons learned based on the empirical results we have learned that there is always some similarities that relate the source and target in different forms depending on the severity of the change simple changes we observed strong correlations between response functions interpolating performance measures and therefore there is a potential for constructing simplelinear transfer functions across environments rq1 .
large changes we observed very similar performance dis tributions e.g.
version changes .
in these cases we found evidence of high correlations between either options rq2 or interactions rq3 for which a non linear transfer maybe applicable.
therefore the key elements in a performancemodel that has been learned from the source will notchange but the coefficients corresponding to options andtheir interactions might need to be relearned for the target.
severe changes we have learned that a considerable part of configuration space is invalid across environmental changesthat could be considered for sampling configurations rq4 .
b. implications for transfer learning research we provide explanations of why and when transfer learning works for performance modeling and analysis of highly configurable systems.
while all research questions have positiveanswers for some environmental changes and negative answersfor others as discussed above in section iv section vii theresults align well with our expectations regarding the severityof change and their correspondence to the type of transferable knowledge i for small environmental changes the overallperformance behavior was consistent across environments and a linear transformation of performance models provides a goodapproximation for the target performance behavior.
ii forlarge environmental changes we found evidence that individ ual influences of configuration options and interactions maystay consistent providing opportunities for a non linear mapping between performance behavior across environments.
iii even for severe environmental changes we found evidence of transferable knowledge in terms of reusability of detectinginvalid from valid configurations providing opportunities foravoiding a large part of configuration space for sampling.
the fact that we could largely predict the severity of change without deep knowledge about the configuration spaces orimplementations of the subject systems is encouraging in thesense that others will likely also be able to make intuitivejudgments about transferability of knowledge.
for example auser of a performance analysis approach estimating low sever ity of an environment change can test this hypothesis quicklywith a few measurements and select the right transfer learningstrategy.
transfer learning approaches for easy environmentalchanges are readily available .
for more severe environmental changes more research is needed to exploit transferable knowledge.
our results showthat even with severe environmental change there always issome transferable knowledge that can contribute to perfor mance understanding of configurable systems.
while somelearning strategies can take existing domain knowledge intoaccount and could benefit from knowledge about influen tial options and interactions it is less obvious how to effectively incorporate such knowledge into sampling strategies and how to build more effective learners based onlimited transferable knowledge.
while we strongly suspect thatsuitable transfer learning techniques can provide significantbenefits even for severe environmental changes more researchis needed to design and evaluate such techniques and compareto state of the art sampling and learning strategies.
specifically we expect research opportunities regarding sampling strategies to exploit the relatedness of environments to select informative samples using the importance of specific regions or avoiding invalid configurations.
learning mechanisms to exploit the relatedness across environments and learn either a linear or non linear asso ciations e.g.
active learning domain adaptation fine tuning a pre trained model feature transfer or knowledge distillation in deep neural networkarchitectures .
however efforts need to be made to makethe learning less expensive.
performance testing and debugging of configurable systems to benefit from our findings by transferring interesting test cases covering interactions between options ordetecting invalid configurations .
performance tuning and optimization benefit from the findings by identifying the interacting options and to perform importance sampling exploiting the importancecoefficients of options and their interactions.
performance modeling benefit from the findings by developing techniques that exploits the shared knowledge in the modeling process e.g.
tuning the parameters of a queuing network model using transfer learning.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
c. threats to v alidity external v alidity we selected a diverse set of subject systems and a large number of purposefully selected environmental changes but as usual one has to be careful when gen eralizing to other subject systems and environmental changes.
we actually performed experiments with more environmental changes and with additional measurements on the same subjectsystems e.g.
for sac we also measured the time it takes to compile the program not only its execution but we excludedthose results because they were consistent with the presenteddata and did not provide additional insights.
internal and construct v alidity due to the size of configuration spaces we could only measure configurationsexhaustively in one subject system and had to rely on sampling with substantial sampling size for the others which may miss effects in parts of the configuration space that we did not sample.
we did not encounter any surprisingly differentobservation in our exhaustively measured spear dataset.
we operationalized a large number of different measures through metrics.
for each measure we considered multiple alternative metrics e.g.
different ways to establish influential options but settled usually on the simplest and most reliable metric we could identify to keep the paper accessibleand within reasonable length.
in addition we only partiallyused statistical tests as needed and often compared metricsdirectly using more informal comparisons and some ad hocthreshold for detecting common patterns across environments.a different operationalization may lead to different results but since our results are consistent across a large number ofmeasures we do not expect any changes to the big picture.
for building the performance models calculating importance of configuration options and classifying the invalid configurations we elected to use different machine learn ing models step wise linear regression regression trees andmultinomial logistic regression.
we chose these learner mainlybecause they are successful models that have been used inprevious work for performance predictions of configurablesystems.
however these are only few learning mechanismsout of many that may provide different accuracy and cost.
measurement noise in benchmarks can be reduced but not avoided.
we performed benchmarks on dedicated systems andrepeated each measurement times.
we repeated experimentswhen we encountered unusually large deviations.
ix.
r elated work a. performance analysis of configurable software performance modeling and analysis is a highly researched topic .
researchers investigate what models are moresuitable for predicting the performance which sampling andoptimization strategies can be used for tuning these models and how to minimize the amount of measurement efforts.
sampling strategies based on experimental design such as plackett burman have been applied in the domain of con figurable systems .
the aim of these samplingapproaches is to ensure that we gain a high level of informationfrom sparse sampling in high dimensional spaces.
optimization algorithms have also been applied to find optimal configurations for configurable systems recursiverandom sampling hill climbing direct search optimization via guessing bayesian optimization and multi objective optimization .
the aim of optimizationapproaches is to find the optimal configuration in a highlydimensional space using only a limited sampling budget.
machine learning techniques such as support vector machines decision trees fourier sparse functions active learning and search based optimization and evolu tionary algorithms have also been used.
our work is related to the performance analysis research mentioned above.
however we do not perform a comparison of different models configuration optimization or samplingstrategies.
instead we concentrate on transferring performancemodels across hardware workload and software version.transfer learning is orthogonal to these approaches and can contribute to making them efficient for performance analysis.
b. performance analysis across environmental change environmental changes have been studied before.
for example in the context of mapreduce applications performance anomaly detection micro benchmarking ondifferent hardware parameter dependencies andperformance prediction based on similarity search .
recently transfer learning is used in systems and software engineering.
for example in the context of perfor mance predictions in self adaptive systems configurationdependency transfer across software systems co designexploration for embedded systems model transfer acrosshardware and configuration optimization .
althoughprevious work has analyzed transfer learning in the contextof select hardware changes we more broadlyempirically investigate why and when transfer learning works.that is we provide evidence why and when other techniquesare applicable for which environmental changes.
transfer learning has also been applied in software engineering in very different contexts including defect predictions and effort estimation .
x. c onclusions we investigated when and why transfer learning works for performance modeling and analysis of highly configurable systems.
our results suggest that performance models are frequently related across environments regarding overall performance response performance distributions influentialconfiguration options and their interactions as well as invalidconfigurations.
while some environment changes allow simplelinear forms of transfer learning others have less obviousrelationships but can still be exploited by transferring morenuanced aspects of the performance model e.g.
usable for guided sampling.
our empirical study demonstrates the existence of diverse forms of transferable knowledge acrossenvironments that can contribute to learning faster better reliable and more important less costly performance models.
a cknowledgment this work has been supported by afrl and darpa fa8750 .
kaestner s work is also supported bynsf awards and and the science of securitylablet h9823014c0140 .
siegmund s work is supported bythe dfg under the contracts si and si .
wewould like to thank tim menzies vivek nair wei fu andgabriel ferreira for their feedback.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table ii e xperimental results rq1 rq2 rq3 rq4 h1.
h1.
h1.
h1.
h2.
h2.
h3.
h3.
h4.
h4.
environment es m1 m2 m3 m4 m5 m6 m7 m8 m9 m10 m11 m12 m13 m14 m15 m16 m17 m18 spear workload variables clauses w1 w w w version v1 .
v .
ec1 s .
.
.
.
.
.
.
.
.
ec2 l .
.
.
.
.
.
.
.
.
.
ec3 l .
.
.
.
.
.
.
.
.
ec4 m .
.
.
.
.
.
.
.
.
.
ec5 s .
.
.
.
.
.
.
.
.
.
ec6 l .
.
.
.
.
.
.
.
.
.
ec7 vl .
.
.
.
.
.
.
.
.
.
x264 workload pictures size w1 w w version v1 r2389 v r2744 v r2744 ec1 sm .
.
.
.
.
.
.
.
.
ec2 s .
.
.
.
.
.
.
.
.
ec3 m .
.
.
.
.
.
.
.
.
ec4 m .
.
.
.
.
.
.
.
.
ec5 l .
.
.
.
.
.
.
.
.
ec6 l .
.
.
.
.
.
.
.
.
ec7 l .
.
.
.
.
.
.
.
.
ec8 vl .
.
.
.
.
.
.
.
.
sqlite workload w1 write seq w2 write batch w read rand w read seq version v1 .
.
.
v .
.
ec1 s .
.
.
.
.
.
n a n a n a n a ec2 m .
.
.
.
.
.
n a n a n a n a ec3 s .
.
.
.
.
.
n a n a n a n a ec4 m .
.
.
.
.
.
n a n a n a n a ec5 m .
.
.
.
.
.
n a n a n a n a ec6 l .
.
.
.
.
.
.
n a n a n a n a ec7 vl .
.
.
.
.
.
.
n a n a n a n a sac workload w1 srad w pfilter w kmeans w hotspot w nw w nbody100 w nbody150 w nbody750 w gc w cg ec1 l .
.
.
.
.
.
.
.
.
.
.
ec2 l .
.
.
.
.
.
.
.
.
.
.
ec3 s .
.
.
.
.
.
.
.
.
.
.
ec4 l .
.
.
.
.
.
.
.
.
.
.
ec5 m .
.
.
.
.
.
.
.
.
.
.
ec6 s .
.
.
.
.
.
.
.
.
.
.
ec7 l .
.
.
.
.
.
.
.
.
.
.
ec8 l .
.
.
.
.
.
.
.
.
.
.
ec9 vl .
.
.
.
.
.
.
.
.
.
.
ec10 l .
.
.
.
.
.
.
.
.
.
.
ec11 s .
.
.
.
.
.
.
n a n a n a n a ec12 s .
.
.
.
.
.
.
n a n a n a n a ec13 s .
.
.
.
.
.
.
n a n a n a n a ec14 l .
.
.
.
.
.
.
n a n a n a n a es expected severity of change sec.
iii b s small change sm small medium change m medium change l large change vl very large change.
sac workload descriptions srad random matrix generator pfilter particle filtering hotspot heat transfer differential equations k means clustering nw optimal matching nbody simulation of dynamic systems cg conjugate gradient gc garbage collector.
hardware descriptions id type cpus clock ghz ram gib disk h1 nuc .
ssd h2 nuc .
scsi h3 station .
scsi h4 amazon .
ssd h5 amazon .
.
ssd h6 azure .
scsi metrics m1 pearson correlation m2 kullback leibler kl divergence m3 spearman correlation m4 m5 perc.
of top bottom conf.
m6 m7 number of influential options m8 m9 number of options agree disagree m10 correlation btw importance of options m11 m12 number of interactions m13 number of interactions agree on effects m14 correlation btw the coeffs m15 m16 perc.
of invalid conf.
in source target m17 perc.
of invalid conf.
common btw environments m18 correlation btw coeffs.
506authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.