python probabilistic type inference with natural language support zhaogui xu xiangyu zhangyz lin chen kexin peiy and baowen xu z state key laboratory of novel software technologyydepartment of computer science nanjing university chinaypurdue university usa zgxu smail.nju.edu.cn xyzhang kpei cs.purdue.edu lchen bwxu nju.edu.cn abstract we propose a novel type inference technique for python programs.
type inference is di cult for python programs due to their heavy dependence on external apis and the dynamic language features.
we observe that python source code often contains a lot of type hints such as attribute accesses and variable names.
however such type hints are not reliable.
we hence propose to use probabilistic inference to allow the beliefs of individual type hints to be propagated aggregated and eventually converge on probabilities of variable types.
our results show that our technique substantially outperforms a state of the art python type inference engine based on abstract interpretation.
ccs concepts software and its engineering !automated static analysis mathematics of computing !max marginal computation keywords python dynamic languages type inference probabilistic inference .
introduction python is one of the most popular programming languages nowadays.
however since python is a dynamic language developers often su er from the lack of type information during development.
in fact type errors are very commonly encountered bugs in python.
however python type inference is highly challenging.
many python projects heavily utilize external api functions that are often not in python.
objects are created and mutated in those api functions.
they may also become correlated by these functions e.g.
through aliasing .
such creations mutations and correlations are critical for correct type inference.
however these zcorresponding authors.api functions are usually di cult to analyze due to the different programming languages used and the lack of source code.
the large number of api functions also makes manual mocking prohibitively expensive.
in addition python variable types are path sensitive.
a variable may have various types depending on the program paths.
types and attribute sets of objects can be dynamically mutated substantially adding to the di culty of static typing.
traditional uni cation based type inference is not applicable to python type inference due to path sensitivity.
researchers have proposed various solutions for python type inference .
most of them work by leveraging data ow between untyped variables and variables of known types e.g.
variables assigned with constants .
their e ectiveness hinges on the manual mocking of the large number of external api functions.
according to our experiment section a state of the art system pysonar2 that was used by google inc. can only type .
of variables in real world programs.
there have been a lot of works on type inference for dynamic languages in general .
many of them have the similar idea of leveraging variables with known types.
some of them are dynamic analysis requiring good test coverage.
we propose a novel python type inference technique based on probabilistic inference.
we observe that python programs have a lot of type hints such as attributes that are accessed variable names and explicit type checks.
however many of these type hints are uncertain meaning that they are not fully reliable.
for example attribute sets may be dynamically mutated so that the observed attribute accesses may not match the prototype of the type.
developers may not respect the naming conventions.
our idea is to correlate all these uncertain type hints through probabilistic inference which is a technique that allows beliefs that is the initial con dences of type hints to be propagated and aggregated through the correlations among program artifacts e.g.
data ow between variables .
eventually the computation converges on the probabilities of variable types.
our contributions are summarized as follows.
we identify four kinds of type hints that can be used to infer types in python including data ow between variables attribute accesses variable names and explicit type checks.
some of these hints are uncertain.
we propose the novel idea of using probabilistic inference for python type inference which allows us to naturally model the uncertainty of type hints and easily handle dynamic features.
we develop a machine learning based approach to expermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
fse november seattle wa usa c acm.
... .
artifact evaluated by fse def gzip f args kwargs resp f args kwargs url resp.url mthd resp.method data compress resp ... result resp return result figure a motivating example.
tract variable type hints from their names.
we develop a prototype and evaluate it on realworld python projects.
our results show that our technique can type .
of the variables that cannot be typed by a state of the art system with estimated .
precision.
.
challenges python program type inference has the following prominent challenges.
incomplete data flow and lack of interface definition.
most existing static type inferencing techniques work by observing data ow starting from values with known types .
unfortunately python programs heavily rely on external functions that may be implemented in other languages and have undocumented interfaces side e ects.
moreover many python projects are indeed libraries that provide services to other downstream projects that may not be available during type analysis.
all these lead to incomplete data ow and di culties in type inference.
consider a python program snippet shown in fig.
.
it is extracted and adopted from a popular python library httpbin.
as we can see from the example function f is a parameter of function gzip .
in the library gzip is a top level function that is not invoked by any other functions.
as such we cannot get any type information for the parameters of gzip from function invocations.
hence we do not know the interface of f including the type of return value.
in addition an external function compress is invoked at line which does not have any python source code.
therefore it is di cult to know if the external function has any side e ects on the object referenced by resp.
most existing techniques will fail to infer the types of resp data andresult due to the incompleteness of data ow.
the overarching idea of our technique is to leverage the incomplete uncertain type hints in a program and conduct probabilistic inference.
for each variable our tool will produce a list of types that is ordered by their likelihood.
in particular we leverage two kinds of type hints.
we call them hints because they are uncertain incomplete by nature.
the rst kind is extracted from program semantics describing how a variable is being used and what attributes are being accessed.
note that such accesses are incomplete in most cases i.e.
only a subset of attributes of an object are accessed .
the second kind is variable names.
we assume the developers for a project follow some naming conventions which can be extracted by data mining the names of variables that can be typed by existing type inference tools.
note that these naming hints are uncertain.
we can only say this variable may likely be x type according to its name .
in our example according to lines and we know that the object referenced by resp must contain attributes url and method .
however from these two hints we cannot determine the type of resp because both the instances of type response and the instances of type request have these two1 def deflate f args kwargs r f args kwargs if not args data r.data else req r.request ... return r figure an example for path sensitivity.
attributes.
in addition we do not know if other attributes ofresp are accessed in the function compress .
interestingly from the naming convention variable resp may possibly be an instance of response as it is lexically similar to response .
in other words when we consider both kinds of hints the likelihood of resp being of response type becomes much higher.
path sensitivity and dynamic updates.
as python is a dynamic language a variable may have di erent types at runtime depending on the program paths.
these types may not even have sub type relations.
in such cases static type inference should not produce a speci c type for xat location l but rather a type set.
path sensitivity makes probabilistic inference particularly challenging as contradicting type hints may be collected along di erent paths.
if their probabilities are simply aggregated they may nullify each other leading to type inference failures or incorrect types.
consider an example in fig.
.
variable rhas di erent types along the two branches.
we observe two attribute accesses of r i.e.
accessing data and request along the two respective branches.
we cannot simply assert that the type of rcontain both attributes.
otherwise the probabilistic inference engine would fail to infer the type of ras there does not exist any type in the type domain that have both attributes.
the set of attributes of an object can also be dynamically modi ed at runtime so that the type hints collected along a path may not match with the original prototype of an object.
probabilistic inference needs to take this into consideration.
we discuss the probabilistic inference engine in section .
we then further discuss how to handle path sensitivity and dynamic updates in section .
.
.
system overview in this section we give an overview of our system.
framework.
fig.
presents the architecture of our system.
it consists of three components the variable name classi er the probabilistic constraint generator and the probabilistic type inference engine.
basically our technique takes the whole project source code and then outputs a ranked list of types for each variable each type annotated with its probability.
the high level workow of our approach contains the following steps.
first it extracts the naming conventions of the project by performing machine learning on the variables that can be typed with existing static type inference techniques.
second it generates a set of constraints from the data ow attribute accesses type check predicates and variable names.
third it transforms these generated constraints to a probabilistic inference network called factor graph .
fourth it resolves the graph using belief propagation to get the probabilities of individual types for each variable.
finally it ranks the computed type probabilities for each variable and lters out the unlikely types according to some given thresholds.
variable name classi er.
the classi er takes the source code of a python project and applies an existing static type 608figure system framework.
inference technique .
the variables that can be typed are fed to a machine learning engine that takes both the variable names and the corresponding types and outputs a trained type classi cation model based on the naming conventions of the project.
probabilistic constraint generator.
we generate four kinds of probabilistic constraints namely data ow constraints attribute constraints subtype constraints and naming constraints .
let us consider the motivating example.
given the de nitions presented in fig.
assume our goal is to probabilistically infer the types of variable result .
our approach computes the probability of result having each type in the type domain typedomain .
as part of the procedure given type response2typedomain our technique aims to infer the probability of result being a variable of the response type.
fig.
shows the corresponding constraints.
first it generates constraints from program data ow.
according to line we can generate an equivalence constraint a between variables resp andresult with .
probability.
intuitively it means the types of resp and result must be equivalent according to the data ow.
second we generate probabilistic constraints from the attribute accesses called attribute constraints .
the constraints shown in b represent the attribute constraints between resp and type response .
the rst constraint in b denotes that if resp holds an instance of response the attribute set of type response must contain the attributes url and method i.e.
with a probability high .
the constraint is constructed from lines and .
it is a standard in probabilistic inference to use a close to .
value to denote high probability when there is uncertainty .
in this case the uncertainty comes from the dynamic attribute updates of an object.
hence we can only say an instance of response has a high probability containing attributes url and method .
the second constraint in b represents if resp contains the two attributes url and method we have a probability .
that resp is an instance of response .
we use probability because there are other types in the domain that also have the two attributes e.g.
request .
note that we do not use probability .
which has the meaning of there is no evidence that resp is an instance of response .
in contrast a probability less than .
means that there is evidence that the statement is not true.
third python programs often use explicit runtime type check functions such as isinstance to test the dynamic type of an object.
a path sensitive analysis that can distinguish the type of an object according to the branch taken can also provide strong type hints.
we model such hints as subtype constraints see details in section .
.
fourth we generate probabilistic constraints according to the variable naming conventions called naming constraints .
we assume that each variable in the program follows certainx2variableset 2typedomain a represents the attribute set of the original prototype of .
p x represents a predicate denoting the type of variable xcan be type .
n x represents a predicate denoting the type of variable x can be type from the naming conventions of type .
c ap !brepresents a probabilistic constraint denoting predicate ahas a probability pimplying predicate b. figure basic de nitions.
a p resp response !p result response b p resp response !
f url method g a response f url method g a response !p resp response c n resp response !p resp response n result response !p result response figure generated constraints of the example.
naming conventions speci c to the project.
the constraints in fig c state if resp result is an instance of response according to the naming conventions there is .
probability that resp result is of type response .
the probability is to model the uncertainty that the developers may not always obey the naming conventions.
probabilistic inference engine.
we conjoin the four kinds of constraints into a probabilistic graphical model called factor graph and perform belief propagation using thesum product algorithm.
belief propagation is an iterative procedure that will eventually produce a satisfying marginal probability of p result response which indicates the probability of variable result having type response .
we provide the details of the probabilistic model in next section.
such probability is computed for each type in the type domain.
for example the probability of result being of string type is very low as the probabilities of constraints in fig.
b become close to .
eventually a ranked list of types is generated for each variable at each program point.
.
probabilistic inference in this section we illustrate how probabilistic inference is performed via the motivating example in fig.
.
assume our goal is to infer the probability of the type of variable result being response .
let boolean variables x1andx2denote the predicatesp result response andp resp response in fig.
respectively.
we then can denote the original constraint a as follows.
x11 !x2 x21 !x1 let boolean variable x3represent the predicate f url method g a response in fig.
b we have the following formula.
x20 !x3 x30 !x2 with the equations and given the observation x3 we want to infer the probability for x1 .
intuitively we aim to compute the likelihood of result being an instance ofresponse when we observe resp contains both url and method attributes.
in addition let boolean variables x4andx5represent the predicatesn resp response andn result response in fig.
respectively.
we thus have the following formula.
x40 !x2 x50 !x1 formally we represent each of the aforementioned probabilistic constraints cias a probabilistic function fias follows.
609c1 x11 !x2c2 x21 !x1c3 x20 !x3 c4 x30 !x2c5 x40 !x2c6 x50 !x1 c7 x3 c8 x4 c9 x5 f1 x1 x2 if x1!x2 otherwisef6 x1 x5 if x5!x1 otherwise f2 x1 x2 if x2!x1 otherwisef7 x3 ifx3 otherwise f3 x2 x3 if x2!x3 otherwisef8 x4 ifx4 otherwise f4 x2 x3 if x3!x2 otherwisef9 x5 ifx5 otherwise f5 x2 x4 if x4!x2 otherwise figure constraints and valuation functions.
table boolean constraints with probabilities.
x2x3x4f3 x2 x3 f4 x2 x3 f5 x2 x4 f7 x3 f8 x4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
fi x1 xm p if the constraint is true potherwise wherex1 x2 xmrepresent the boolean variables associated with the constraint ciandprepresents the probability of the constraint being true.
for our motivating example the probabilistic constraints ci i and the corresponding probabilistic functions fi i are listed in fig.
.
note that according to lines and of the motivating example the attribute set ofresp must contain url and method .
therefore the probability of x3 c7 is .
.
the probabilities of c8and c9are predicted by the variable name classi er according to the naming conventions.
we will explain how they are computed in section .
in general assume there are kprobabilistic constraints c1 c2 ... ckonnboolean variables x1 x2 ... xn.
functions f1 f2 ... fkcorrespond to these constraints.
since all the constraints ought to be satis ed the function representing the conjunction of the constraints is hence the product of the corresponding probabilistic functions.
therefore we have the following equation.
f x1 x2 xn f1 f fk the joint probability function is de ned as follows which is essentially the normalized version of f x1 xn .
p x1 x2 xn f1 f fkp x1 xn f1 f fk according to the above equation we can further compute the marginal probability pi xi as follows.
p xi x x1x x2 x xi 1x xi x xnp x1 x2 xn in other words the marginal probability is the sum over all the variables other than xi .
for a type i variable xiasserts that the type of a given program variable be i. hence in order to discover the possible types of the variable we compute xi s probability for all the i s in the type domain and order them by their marginal probabilities.
consider the motivating example.
for simplicity we ignore the program variable result and only consider variable resp.
the related boolean variables are thus x2 x3andx4 figure factor graph example.
.and the corresponding probabilistic functions are f3 f4 f5 f7andf8.
table presents the values of the probabilistic functions.
assume we want to compute the marginal probability p x2 that is the probability of the type of variable resp being response .
the computation procedure is shown by the following equation.
p x2 p x3 x4f3 x3 f x3 f x4 f x3 f x4 p x2 x3 x4f3 x2 x3 f x2 x3 f x2 x4 f x3 f x4 similarly when program variable result is also considered we can compute the marginal probability of p x1 representing that the type of result has a probability .
being response .
on the other hand let us consider the probability of the type of result being request .
since the variable resp is less likely the request type following the naming conventions the name classi er computes a low probability .
for n resp request .
with other constraints we can hence infer the probability of result being a request type variable p0 x1 which is lower than the probability of the response type.
factor graph.
the computation of marginal probabilities via equation could be very expensive as it has to enumerate all the possible combinations of the variable valuations.
factor graph is a probabilistic graphical model allowing e cient computation.
we present a part of the factor graph of our previous example in fig.
.
a factor graph consists of two kinds of nodes factor nodes represented by squares andvariable nodes represented by circles.
a factor node represents a probabilistic function e.g.
fiin equation .
a variable node represents a variable in the function e.g.
xiin equation .
edges are directed from a factor node to each variable of the function.
sum product algorithm.
the sum product algorithm is an e cient algorithm computing marginal probabilities based on factor graph .
in this algorithm probabilities are propagated only between adjacent nodes through message passing.
the probability of a node is updated by integrating all the messages it receives.
then the node will further propagate the probability to its receivers.
the algorithm is iterative and terminates when the probabilities converge.
probabilistic inference has been successfully applied to many areas such as debugging and arti cial intelligence .
in this paper our prototype tool is built upon pgmpy an open source probabilistic graphical modeling library in python.
.
constraint generation in this section we discuss how to generate the probabilistic constraints used by the type inference engine.
as discussed in section the probabilistic constraints fall into four categories data ow constraints generated from data ow of the program attribute constraints extracted according to attribute accesses subtype constraints that 610code constraint y1 x1 ifa1 z1 y1 else z2 b1 z3 z1 z2 w1 z3 s1 w1p x1 !p y1 p y1 !p z1 p b1 !p z2 p z3 !p z1 p z2 p z3 !p w1 p s1 !p w1 figure an illustrative example.
denote the explicit runtime type checking conditions naming constraints derived from variable names and the naming conventions of the project.
note that all these constraints are transformed into factor graphs from which the type probabilities are computed for each variable.
.
data flow constraints data ow constraints denote the de ne use relations across variables.
it is particularly useful for inferring types of a variable when its attributes are never or rarely accessed but it has def use relations with other variables whose attribute accesses provide lots of type hints.
in many cases data ow constraints also allow our engine to aggregate the attribute access type hints across multiple correlated variables.
in our motivating example fig.
there is no direct evidence that indicates the type s of the variable result .
however we can still compute its type s due to its data ow correlation with variable resp.
we analyze each module of the given python project and build the module dependence graph.
according to the module dependence graph we compute a topological order for all the involved modules.
for each module we rst transform the source code into the single static assignment ssa form in which each variable is de ned exactly once.
we leverage functions to merge analysis results from di erent branches.
for each assignment y op x1 x2 in the program we generate a data ow constraint as follows.
p y !
xip xi sampleopincludes copy operations and operations.
the disjunction allows high con dence from any right hand side variable to be propagated to the left hand side variable and vice versa.
in our implementation we generate di erent dataow constraints depending on the di erent operand result types e.g.
an addition of a oating point value and an integer value yields a oating point value .
we omit the details as they are standard.
our analysis is piggy backed on a standard points to analysis.
to avoid bogus data ow we only consider must alias relations.
note that missing data ow is less critical to our analysis compared to others as it only means the corresponding type hints of the disconnected variables cannot be linked to acquire higher con dence.
in many cases the disconnected variables already have strong enough local type hints as suggested by our results in section .
example.
we use an example in fig.
to illustrate how to construct data ow constraints.
the left side of the gure presents the program in ssa form and the right side presents the corresponding constraints.
in the ssa form we use the superscripts to denote the di erent de nitions of a variable.
for example the three de nitions of zare denoted byz1 z2andz3 respectively.
observe at line the type predicates for z1andz2are disjointed.during the analysis we also collect all the observable types involved in the project which constitute the type domain .
later for each type in the domain our engine infers the probabilities of each variable having the type.
.
attribute constraints attribute constraints are extracted from attribute accesses which provide type hints for the objects.
intuitively the more unique attribute accesses we observe about an object the better chance we can infer its type.
generating attribute constraints entails addressing the following challenges path sensitivity.
the type of a python program variable may be path sensitive meaning that the variable may have di erent types along di erent paths.
as discussed in section if we do not carefully handle path sensitivity the attribute constraints may become contradictory and lead to inference failure.
this suggests that our attribute constraint construction shall be path sensitive.
dynamic update of attribute set.
in python the attribute set of an object can be dynamically updated.
consequently the relation between the type of an object and its attribute set may become uncertain.
here we consider that dynamic updates do not change the type of an object which is determined upon the object creation.
fortunately probabilistic analysis allows us to naturally model such uncertainty.
more details will be provided later in this section.
incompleteness in attribute accesses.
attribute accesses provide a lot of type hints.
however in most cases we can only observe a subset of the attributes of an object.
in other words our observation is incomplete.
as a result there is uncertainty in such type hints even though they do provide useful information towards resolving the type.
in our motivating example both types response and request have the attributes url and method .
the attribute accesses allow us to narrow down the set of possible types.
but there is still uncertainty in deciding the type.
handling path sensitivity challenge .
to address the rst challenge we collect the attribute accesses of each variablexpath sensitively.
the collection procedure consists of two steps traverse the individual paths in the control ow graph of the procedure in which xresides and collect the attribute accesses of xfor each path.
cycles are broken by unrolling each loop once.
note that the number of unrolling for a loop is irrelevant in our context.
merge paths with the same set of attribute accesses and remove those that have an empty set.
note that if multiple paths have the same set of attribute accesses they do not provide additional con dence of the inferred type as the condence is derived from the uniqueness of the set of attribute accesses i.e.
how many types contain these attributes instead of the number of paths along which these accesses are observed.
consider the example in fig.
.
since there are two paths in the program we collect two sets of attribute accesses f data gandf request g for variable r. after collecting the attribute accesses of xalong each path we then construct the attribute constraints of x. for each pathi we introduce a predicate p xi to assert that the type of variable xin pathibe .
these constraints often have the following form.
p xi p !fattributes on ig a fattributes on ig a p0 fattributes on ig a p0 !p xi 611intuitively the rst constraint asserts that if xiis of type the attributes observed on iare a subset of the attributes of type with probability p high i.e.
.
.
note that using a close to .
value instead of .
to denote a very high probability is standard in probabilistic inference .
handling dynamic attribute updates challenge .ideally if the observed attributes are indeed a subset of the attribute set of denoted asa the boolean variable denotingfattributes on ig a has the true value false otherwise.
however in practice if the attributes are not a subset of s attributes according to s de nition original prototype we cannot simply set the boolean variable to false as this may be caused by dynamic attribute updates.
for instance assume a new attribute a0is added to xand then accessed.
the observed attributes are hence not a subset of s attribute set although the type of xis still .
to model dynamic updates ideally we would identify all attribute updates especially attribute additions and the variables that are a ected by these updates and then change the attribute constraints according to the updates.
in the aforementioned example a0should be precluded from the observed attribute set of the constraint.
however achieving soundness in such analysis requires a fulledged pathsensitive analysis.
our solution is hence to use probability to model the uncertainty caused by dynamic updates.
speci cally when the observed attribute accesses are not a subset of s attribute set we assign a probability p0to the predicate equation depending on the number of observed attributes that are part of s attribute set.
the computation ofp0is represented by the following formula.
p0 low jfattributes on ig a j jfattributes on igj high low note that if all the observed attributes are part of s attribute set p0 high .
if none of the observed attributes are part of s attribute set p0 low i.e.
.
.
an example will be presented later in this section.
handling incompleteness in attribute accesses challenge .
equation asserts given that the attributes observed on iare a subset of the attributes of type xiis of type with probability p0.
the probability p0 is determined by the uniqueness of the observed attributes that is how many other types also contain these attributes.
the computation is represented by the following formula.
p0 high n nrepresents the number of types in the observable domain which contain the attributes that we observed.
when nis very large p0is close to .
meaning that we are completely uncertain if xiis of type or not.
note that p0 means that we are positive about the assertion and p0 means that we are negative i.e.
there are evidence for xi not having type .
whenn p0 high meaning that we are highly con dent that xiis of type when the observed attributes are unique.
computing the probability at the de nition point.
intuitively if xhas type at pathi it also has type at its de nition point.
formally if the attribute accesses of xcan be collected from multiple paths we use the maximal probability of all paths to represent the one at its de nition point.
the computation is represented as the following formula.
p x max p x1 p x2 p xn type has attributes a1 a2 a3 and a4in its de nition.
types 1and 2have attributes a1anda2in its de nition.
xis of type .
def foo ... x ... if... ... x.a1 else ... x.a2 gee x if... ... x.a310 def gee t if... ... t.a2 else ... t.a4 t.a5 ... attr.
add if... ... t.a5 figure example code for attribute constraints.
path constraint tt in foo a p x1 !fa1 a3g a b fa1 a3g a c fa1 a3g a !p x1 tf in foo d p x2 !fa1g a e fa1g a f fa1g a !p x2 ft in foo ... ff in foo ... tt in gee g p t1 !fa2 a5g a h fa2 a5g a i fa2 a5g a !p t1 ... ... figure attribute constraints for individual paths of the example in fig.
.
tt in foo means a path in foo in which both predicates take the true branch.
p x represents the probability of xhaving type at its de nition point.
note that the probability p xi of each pathican be computed by formulas stated in section .
the essence of path sensitivity in our technique.
we cannot a ord fulledged path sensitivity in general due to the exponential number of possible paths.
we only consider path sensitivity of attribute accesses for the same variable within the same procedure .
speci cally for each variable xin a procedure we collect its attribute accesses pathsensitively within the procedure.
note that xmay be related to other variables.
the attribute accesses for those variables are collected independently and also in a path sensitive fashion.
the correlations between xand these variables are modeled by the data ow constraints discussed in section .
that are path insensitive.
from the results in section .
such a design allows us to achieve a balance between overhead and e ectiveness.
example.
consider the example in fig.
.
variable xis of type that has attributes a1 a2 a3 and a4.
however this is unknown and we want to infer x s type from the code.
variablexis de ned at line and its attributes a1 a2 and a3are accessed at lines and respectively.
it is also passed to function gee in which its attributes are accessed.
a new attribute a5is added to the object at line .
ideally we would explore the individual whole program paths to construct attribute constraints.
for example a path !
!
!
!
!
!
!
!
!
!
contains accesses to attributes a1 a4 a5 and a3.
such whole program path sensitive analysis incurs prohibitive overhead.
instead we collect the attributes for variable xinfoo along the intra procedural path !
!
!
!
!
and the attributes for tin gee along !
!
!
!
.
the former has attributes a1anda3whereas the latter has a4anda5.
the likelihoods of xhaving type andthaving type are rst inferred independently based on the intra procedural attribute s f args kwargs if isinstance s basestring y s.lower else y str s figure an example for subtype constraints sets.
they are further aggregated through the dataow constraint between xandt.
fig.
shows the attribute constraints for individual paths.
the rst four rows denote the attribute constraints for the four paths in foo .
in particular constraint a means that if xis of type in path both predicates take the true branch it is highly likely that the observed attributes a1and a3are part of the attributes of .
constraint b means that a1and a3are highly likely to be in the attribute set of .
constraint c asserts the reversion of a .
note that since the observed attributes a1and a3are unique to the probability on top of the arrow is high .
constraints d f are similar except that the probability in f is .
due to the fact that the observed attribute a1 along path is not unique to .
note that 1and 2also have a1.
the probability .
is computed by equation .
constraints g i are for a path in gee with both predicates taking the true branch.
observe that constraint h denotes that even though a2and a5are not part of the attribute set of due to the dynamic attribute addition at line we do not set the predicate fa2 a5g a to false.
instead it has a probability .
of holding a true value according equation .
according to equations in section we can infer the probability ofxhaving type at paths and is .
.
and .
respectively.
with equation we further infer the probabilities of xandthaving at their de nition points are both .
.
combining the dataow constraints p x !p t we have the nal likelihood of xbeing of is .
.
similarly the likelihood of xhaving 1is .
.
.
subtype constraints subtype constraints are extracted from the type checking statement isinstance x which implies the possible types of a variable.
we also generate the subtype constraints path sensitively which is similar to the generation of attribute constraints.
we use a simple example in fig.
to illustrate how we generate subtype constraints.
in this example line is an external function call so we do not know the type of its return value and hence the type of variable s. line represents a subtype checking of s. intuitively we know that when the execution goes into the true branch line the type of smust be a subtype of basestring .
on the other hand when the false branch line is taken it must not be a subtype of basestring .
accordingly assume we are computing the likelihood of the type of variable s being we can generate the subtype constraints as follows.
a p s1 !
4basestring b p s2 !
4basestring here s1ands2represent variable sin the two branches and4represents a subtype relation.
intuitively a denotes that when observing is a subtype of basestring e.g.
str we know the type of variable sat line is very likely to be and vice versa.
constraint b is similarly interpreted.
.
naming constraintsour technique is based on probabilistic inference which allows us to take into consideration uncertain type hints even from the natural language perspective.
the intuition is that when programmers name a variable they often follow some conventions implying its type.
in our motivating example in fig.
we observe that lots of variables are lexically similar to response e.g.
r resp and most of them are indeed of the response type.
besides we can also infer the type of a variable according to other natural language features such as part of speech and singular plural form of nouns.
for instance a variable with a name starting with a verb e.g.
has connected very likely represents a boolean variable.
when a variable s name is plural e.g.
connections it probably represents a collection e.g.
list .
such type hints are uncertain and hence cannot be leveraged by most existing dataow based type inference techniques.
to model uncertainty we use probabilistic constraints to represent the implicit relations between variables and naming conventions.
in the following we rst illustrate how we use machine learning to mine the implicit naming conventions and then discuss how to generate the naming constraints.
naming convention learning.
the learning component takes three inputs the variable set the observed type domain and the partial mapping from variables to their types and nally produces a variable name classi er and a variable type classi er.
the process consists of three main steps first it lexically clusters variable names using k means .
then it extracts natural language features from each variable.
finally it trains the classi cation model for each type in the domain.
clustering variable names.
the procedure of clustering variable names consists of three steps normalize the variables compute variable name similarities cluster the variables using k means.
normalization of a variable x entails the following.
first we remove all the tail digits ofx s name if any.
second we identify all the terms of x s name via some commonly used separators e.g.
and capital letters .
third we transform each term into its lower case.
finally we concatenate them again using separator .
for instance variable hasconnected2 is normalized to has connected .
for the computation of lexical similarity ofx1andx2 we use the following formula.
sim x1 x2 jlcs x1 x2 j jlas x1 x2 j min j x1j j x2j here jlcs x1 x2 jdenotes the length of the longest common substring between x1and x2 which denote the normalizedx1andx2 respectively.jlas x1 x2 jrepresents the length of the longest common abbreviation between x1and x2.
we use the longest common abbreviation in addition to the longest common substring because we observe that abbreviations are widely used.
for example programmers may use a abbreviation pgm to represent the word program .
however if we only use the longest common substring the similarity between the two is low.
in contrast when the common abbreviation is considered the similarity becomes much higher.
for clustering variable names we use k means which produces a classi er that can identify the variable name cluster of a given variable.
the classi er is then used in the feature extraction for variables.
extracting natural language features.
we extract four kinds of features from each variable.
given a variable x we rst compute the cluster id of this variable using the 613aforementioned classi er.
second we extract the part ofspeech features of x. we rst split the normalized name of xinto terms and then count the frequencies of the verbs and nouns as features.
third we extract the singular plural form feature of x s name.
we split the normalized xinto terms.
if the terms contain a plural word we set the feature to otherwise .
finally we extract the lexical similarity between the normalized xand each normalized name of type in the domain.
we extract this feature because we observe that a lot of variables are lexically similar to its type name.
training classi cation models.
we train a model for each type in the observed domain.
it is a standard supervised machine learning procedure.
speci cally given type we rst extract the aforementioned four kinds of features for each variable xand set the target label as when the inferred types of xcontain otherwise .
we put all the data into a svm classi er and train a model for denoted asm .
with the trained model m if we have a new variablex0 we can predict the probability of the type of x0being .
the predicted probability will be used in our naming constraints.
constructing naming constraints.
as we will show in section due to the uncertainty of naming conventions the probabilities produced by the models alone cannot e ectively predict correct types.
instead we represent them as naming constraints that are used together with other constraints to infer types.
to construct naming constraints we introduce a threshold to represent the weight of the naming conventions in type inference.
basically for a given type and a variable x we have the following constraint n x !p x n x represents that the type of xcan be according to the naming model m .
in our motivating example we set the threshold seec5andc6in fig.
.
note that we set the initial probability of n x to the predicted probability m .
for example we assume that the initial probability as .
in our motivating example see c8in fig.
.
due to the space limitations we move the details to our technical report .
.
evaluation we have implemented a prototype in python which consists of three components namely the naming convention learning component the constraint generator and the inference engine.
the naming convention component relies on a widely used type inference engine pysonar2 to infer variable types.
the machine learning subcomponents are implemented using sklearn .
the probabilistic inference engine is built on pgmpy .
we make our tool along with the evaluation environment publicly available at .
our evaluation aims to address four research questions rq1 how e ective is our approach in inferring variable types compared to existing static analysis?
rq2 how e cient is our approach?
can it scale to realworld python programs?
rq3 what is the impact of the thresholds?
rq4 what is the impact of each kind of constraints?
.
experiment setup we use pysonar2 a static python type inference engine as both the baseline for comparison and a building block for the naming convention component.
pysonar2 is based onabstract interpretation an advanced static analysis technique.
it was previously used by google inc. and its underlying analysis engine is currently used by sourcegraph a widely used analysis system for code repositories.
we could not compare with other research prototypes as they are not publicly available or cannot be properly installed.
preparation.
we extract variables whose types cannot be inferred by pysonar2 .
to collect the ground truth for these variables we developed a dynamic analysis collecting the runtime types of variables by executing all the test cases of the benchmarks.
the variables that are typed dynamically but not statically are the targets of our experiment.
in other words we want to observe how many of them can be typed by our technique and how precise the inferred types are compared to the observed types.
note that the dynamic analysis is only for the experiment our technique is static.
benchmarks.
we select a set of real world python projects as shown in table columns .
most of them are from github and widely used in practice.
when choosing these projects we also considered diversity.
observe that some of them are large with the largest over 54k lines of python code.
these projects mainly fall into the following domains httpbin httpie requests andurllib3 popular http libraries and applications.
paramiko a widely used sshv2 library.
fabric a well known remote deployment and system administration application.
bs4 pyquery and simplejson popular html xml parsing libraries.
bottle cherrypy flask tornado and web2py widely used web application and development frameworks.
pyspider a famous web crawling toolkit.
click a well known command line utility.
gitpython a popular library used to interact with git.
computation of recall and precision.
before describing the results we rst explain how we evaluate the e ectiveness.
as described in section our system essentially computes an ordered list of types with probabilities.
when the computed probability of a type is less than .
we say the variable unlikely has that type.
for types whose probabilities are over .
we cluster them by a threshold gap .
intuitively we traverse the ranked type list of a variable and compute the gap gof probabilities between any two adjacent types 1and .
ifg gap we introduce a new cluster starting with .
eventually we partition the list to clusters with the gap larger than gap .
our tool will only report the rst cluster.
in practice the rst clusters may also be large.
to avoid overloading the users we introduce another threshold topn which de nes the maximum number of types the system reports.
we leverage the dynamically collected types to compute recall for variable x denoted as r x in the following.
rx jd x c1 x topn j jd x j d x represents the dynamically collected type set of variablex.c1 x topn represents the reported type cluster of x whose size is bounded by topn .
for precision p x we use the following.
px jfeasible c1 x topn j jc1 x topn j 614table summary of the experiment results.
benchmarksvar positives recall precision type set size avg.
time s name sloc top top top top top top top top top httpbin .
.
.
.
.
.
.
.
.
httpie .
.
.
.
.
.
.
.
.
paramiko .
.
.
.
.
.
.
.
.
urllib3 .
.
.
.
.
.
.
.
.
requests .
.
.
.
.
.
.
.
.
fabric .
.
.
.
.
.
.
.
.
bs4 .
.
.
.
.
.
.
.
.
simplejson .
.
.
.
.
.
.
.
.
pyquery .
.
.
.
.
.
.
.
.
bottle .
.
.
.
.
.
.
.
.
cherrypy .
.
.
.
.
.
.
.
.
ask .
.
.
.
.
.
.
.
.
tornado .
.
.
.
.
.
.
.
.
web2py .
.
.
.
.
.
.
.
.
pyspider .
.
.
.
.
.
.
.
.
werkzeug .
.
.
.
.
.
.
.
.
click .
.
.
.
.
.
.
.
.
gitpython .
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
feasible c1 x topn represents the reported types that are feasible at runtime.
since we do not have the ground truth for each possible cluster size ranging from to topn we randomly select variables to estimate the overall average precision.
for each variable we use the following procedure to determine if a type in the reported type set is feasible.
when the variable is a parameter to an external api function we rst check the documentation of the api to determine if is correct.
if there is no documentation we randomly create a value of and pass it as an input to the function leveraging existing unit tests.
if the execution does not crash we consider it feasible.
if a variable is assigned the return value of an api call we rst check documentation.
if the documentation is not available we replace the function call with a random value of type and observe whether the execution will induce a type exception.
eventually we take the average over the precision of all the sampled variables.
our experiments were on an intel i7 3770u machine with 16gb ram ubuntu .
and python .
.
.
.
experimental results table shows the summary of results.
column var presents the number and percentage of the variables whose types are observed during dynamic runs but cannot be completely inferred by pysonar2 .
observe that pysonar2 fails to infer types for a large number of variables ranging from hundreds to thousands.
note that we do not consider variables whose types are not observed during execution as we cannot acquire any ground truth for those variables.
e ectiveness evaluation rq1 .
for the evaluation of e ectiveness we use the naming convention threshold described in section .
and the probability gap gap described in section .
.
columns present the results.
column positives presents the percentage of variables having at least one type whose probability is over .
.
observe that most variables have at least one positive type.
columns present the results of recall.
we choose three topn con gurations namely and .
observe that we have a high recall with an average .
at top .
at top and .
at top .
also observe that the recall becomes higher along with the increase of topn .
columns present the precision.
observe that we also have a high precision with an average .
at top .
at top and .
at top .
the precision de creases with the increase of topn .
columns describe the average size of the reported rst cluster.
observe that the size is reasonably small.
analysis time rq2 .
column presents the average time for inferring the type of a variable.
observe that most variables can be inferred in one second with an average .
second.
this suggests our technique is su ciently fast to be used in ide for real world python programs .
impact of thresholds rq3 .
we study the impact of three thresholds namely gap high and .
we use the thresholdtopn .
fig.
a shows the impact of gap .
we evaluate four settings namely .
.
.
and .
.
observe that the recall has a small increase with the increase ofgap while the precision decreases a little bit.
fig.
b shows the impact of high .
observe that the impact on both recall and precision is negligible.
fig.
c presents the impact of .
observe that it has an impact similar to gap but in the other direction.
impact of constraints rq4 .
we evaluate the impact of each kind of constraints by adding one kind at a time with the order of attribute constraints subtype constraints dataow constraints andnaming constraints .
for precision we only focus on variables having at least one positive type.
fig.
d shows the results.
observe that each kind has positive contribution to the recall.
especially the impact of naming constraints is signi cant.
the precision overall has a marginal decrease with the additions of constraints.
threats to validity.
our evaluation is performed on a limited set of programs.
it is possible that the performance of our technique may vary for other programs.
the evaluation of precision is sample based and requires substantial manual e orts.
sampling errors and human errors may skew the results.
our technique depends on a set of parameters.
although we have studied a number of parameter settings the study is incomplete due to the large parameter space.
.
related work probabilistic modeling.
recently probabilistic graph models were applied to type inference of javascript.
raychev et al.
predicted javascript types by learning a probabilistic model from a large repository of programs big code .
our technique is di erent from theirs.
first assumes a large number of programs from which variable names and types can be learned.
in contrast our technique a b c d figure impact of the thresholds a c and each kind of constraints d .
works on a single project.
we do not assume a large code repository.
note that many python types are project speci c by our experiment only used in a project.
second the model in is monolithic and encodes properties of the entire program all together.
a solution is global optimal which may sacri ce local optimality.
in contrast we analyze types of individual variables one by one using separate models.
hence our approach achieves local optimality potentially having better precision and recall for individual variables.
note that during development programmers likely query about types of individual variables instead of all variables.
third we leverage project speci c naming conventions which have signi cant contribution to the performance according to our evaluation section .
probabilistic inference has also been widely used in many other areas such as software debugging security and speci cation extraction .
although we rely on a similar underlying inference engine we address a set of unique challenges because of the di erent application domain.
type inference for python.
several analyses have been developed for type inference of a subset of python.
slib et al.
infer ow insensitive types based on the cartesian product algorithm cpa .
cannon et al.
analyze atomic types from a local view of procedures.
aycock et al.
infer types for a subset of python aggressively according to type consistency.
rigo et al.
and gorbovitski et al.
optimize python programs using abstract interpretation based type inference.
these approaches use forward analyses to infer variable types and hence when encountering external function calls they heavily rely on manual mocking.
they may also fail to infer lots of types due to the lack of test drivers when analyzing libraries.
our approach is based on collecting type hints according to how variables are used which requires much less mocking and test drivers.
in addition it leverages uncertain type hints from variable names and the reasoning engine is probabilistic inference.
type analysis for other dynamic languages.
manytype analyses have been proposed on other dynamic languages such as javascript ruby and php .
typedevil detects type inconsistencies in javascript according to dynamic observations of types.
their method is dynamic analysis based and depends on test drivers.
although their approach removes false positives using belief analysis their belief is not a notion in probabilistic inference.
druby statically infers types for a subset of ruby.
it also starts from variables with known types.
rubydust dynamically infers types in ruby and hence heavily depends on test coverage.
zhao et al.
developed a type inference technique for php optimizations.
ours is mainly for programming support and maintenance.
.
conclusion we propose a probabilistic inference based python type inference technique.
it allows us to leverage various type hints such as those derived from data ow attribute accesses and variable names.
some of them are uncertain.
our results show that our technique substantially outperforms a state of the art type inference engine based on abstract interpretation.
our technique can type .
of the variables that cannot be typed by abstract interpretation with estimated .
precision.
.