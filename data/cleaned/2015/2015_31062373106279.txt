discovering relational specifications calvin smith university of wisconsin madison usagabriel ferns university of wisconsin madison usaaws albarghouthi university of wisconsin madison usa abstract formal specifications of library functions play a critical role in a number of program analysis and development tasks.
we present bach a technique for discovering likely relational specifications from data describing input output behavior of a set of functions comprising a library or a program.
relational specifications correlate different executions of different functions for instance commutativity transitivity equivalence of two functions etc.
bach combines novel insights from program synthesis and databases to discover a rich array of specifications.
we apply bach to learn specifications from data generated for a number of standard libraries.
our experimental evaluation demonstrates bach s ability to learn useful and deep specifications in a small amount of time.
ccs concepts theory of computation logic and verification logic and databases software and its engineering dynamic analysis security and privacy logic and verification keywords hyperproperties datalog specification mining acm reference format calvin smith gabriel ferns and aws albarghouthi.
.
discovering relational specifications.
in proceedings of 11th joint meeting of the european software engineering conference and the acm sigsoft symposium on the foundations of software engineering paderborn germany september esec fse pages.
introduction formal specifications of library functions play a critical role in a number of settings in program analysis and verification specifications are essential to efficiently and precisely analyzing applications that use libraries whose code is unavailable or too complex to analyze.
in software engineering formal specifications can be used to unambiguously document libraries and apis as well as aid developers in program evolution.
we address the problem of discovering a rich class of specifications defining behaviors of a set of functions.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse september paderborn germany copyright held by the owner author s .
publication rights licensed to association for computing machinery.
acm isbn .
.
.
.
setting imagine we are given a set of functions f1 .
.
.
fn along with a dataset drepresenting a partial picture of the input output behavior of each function fi perhaps collected through random testing or instrumentation.
we ask the following question what can we learn about the set of functions f1 .
.
.
fn by simply analyzing the dataset d?
we present a novel and expressive algorithm called bach that is able to discover likely relational specifications that correlate i different executions of a single function or ii different executions within collections of functions.
in other words bach learns hyperproperties this is in contrast to traditional techniques that discover properties of single executions invariants .
for instance bach may learn the following specifications from some input output data where all variables are implicitly universally quantified add x y z add y x z gt x y t gt y z t gt x z t trim uppercase x y uppercase trim x y x abs x y abs y x specification is a bi implication specifying that addis a commutative function.
specification on the other hand is an implication specifying transitivity of gt greater than .
bach may also discover specifications that correlate different functions e.g.
specification specifies that the composition trim uppercase is equivalent to uppercase trim where trim removes whitespace from a string and uppercase turns all characters to uppercase .
further bach may discover sophisticated specifications that are refined with additional constraints for instance specification specifies that the function abs absoulte value is invertible on positive integers.
primary challenges there are three primary challenges that arise in learning relational specifications from a dataset i what does it mean for a specification to explain partial input output behavior?
ii how do we efficiently handle large amounts of input output data?
iii the space of possible relational specifications is vast so how do we efficiently traverse the search space?
we now describe how bach tackles these challenges.
figure provides a high level overview of bach.
consistency verifier the first piece of the puzzle is defining what it means for a specification to explain a dataset.
we view a specification as a first order formula f and the given dataset as a partial interpretation d. we formalize what it means for dto be a model of f. bach employs a notion of evidence to rank specifications.
if there exists any negative evidence e.g.
a counterexample to transitivity of a function then the specification is considered inconsistent with the data and discarded.
otherwise a specification is considered more likely to be true depending on a measure of the positive evidence that is available for it.
the specification consistency verifier checks specifications on a given dataset.
since we are potentially dealing with thousands of esec fse september paderborn germany calvin smith gabriel ferns and aws albarghouthi input output examples per function we could easily incur a prohibitive polynomial blowup when evaluating a specification e.g.
evaluating f x h y requires us to evaluate fon the cartesian product of the available outputs of andhin the dataset.
to efficiently handle large amounts of examples we exploit the insight that we can encode the specification consistency checking problem as a set of non recursive relational horn clauses aunion of conjunctive queries which is a subset of sql allowing us to delegate the problem to efficient scalable databases or datalog solvers.
induction and abduction engines the second piece of the puzzle is how to automatically discover specifications.
our first insight is that we are searching for a specification a logical formula that is comprised of a setof programs compositions of functions and connections between them.
consider the transitivity formula f x y t z p1 f y z t z p2 f x z t z p3 here we have programs two on the left of the implication p1 p2 and one on the right p3 .
the programs are connected by sharing their inputs and outputs through quantified variables.
following this observation to discover specifications we utilize aspecification induction engine that traverses the space of sets of programs and connections between them.
this is analogous to how an inductive synthesis algorithm searches for a single program satisfying some property here we search for a set of programs.
if the induction engine discovers a specification sthat is too strong to hold on the given dataset the guard abduction engine asks the question what do we need to know in order to make the specification hold?
viewed logically the abduction engine weakens the specification sby qualifying it with some guard g resulting ing s. in practice we exploit the insight that the guard abduction problem can be reduced to a classification problem by splitting data into positive and negative sets those that satisfy the specification and those that do not.
by alternating between induction and abduction bach is able to learn a rich array of specifications.
implementation we implemented bach and applied it to learn specifications of a range of python libraries including a geometry module and an smt solver s api.
our results demonstrate bach s ability to discover useful and elegant specifications explaining interactions between functions.
while bach learned a number of expected specifications we were pleasantly surprised by some of the non obvious specifications it managed to infer see section .
most related work the most closely related work to ours is claessen et al.
s which discovers equational specifications through random testing.
the class of specifications learnable by bach is richer in a number of dimensions in addition to learning equivalences as bi implications between pairs of programs bach is able to i learn implications between pairs of programs ii learn equivalences and implications over sets of programs e.g.
for properties like transitivity which correlate executions between more than two copies of a program and iii abduce guards on specifications.
further bach operates in a black box setting we do not assume access to library code or a random test generator.
instead we directly operate on data making our approach general perhaps even beyond software specifications e.g.
hardware components and networks.
inoutinout...f1fndatasetinput output data specification induction guard abductionspecification consistency verification learning enginethe specification induction engineattempts to learn specifications ofthe formy fory f whereyandfare conjunctions offunction applications.
guard abduction enginespecification consistency verifierthe specification consistency verifierchecks whether a given specificationis consistent with the given data set that is the data set does not falsifythe specification and that there issomeevidence of it holding.
8x y.j y8x y.j y8x y.......learned specificationsoutput streamthe output of bach is a stream ofspecifications learned from the givendata set.the abduction engine refines speci fications produced by induction en gine by augmenting them with ad ditional constraints e.g.
y fbecomesg y f .i1o1............i1o1 sg sspecificationrefinedspecification ve ve evidencespecification induction engine the input to bach is a relational datasetdwhere each relation describesa subset of the input output rela tion induced by some functionfi.figure main components of the bach algorithm in comparison with likely invariant discovery techniques e.g.
daikon our problem is more general and theoretically more complex.
checking whether an invariant holds on a dataset representing program states requires a linear traversal of the set of observed states while ensuring that the invariant holds on each state.
in our relational setting however we need to simultaneously consider multiple executions which is why we delegate specification checking to a database engine.
for instance to falsify a loop invariant e.g.
x all we need is a single execution where xis negative however to falsify transitivity of a function we need a set of executions that together demonstrate that a function is not transitive.
section makes a detailed comparison with other works.
summary of contributions to summarize the primary contributions of this paper are as follows we formally define the relational specification learning problem as that of discovering likely specifications with respect to a dataset of input output behaviors.
we present bach an automated tool that learns relational specifications from input output data of a library of functions.
bach utilizes a novel inductive and abductive synthesis technique to learn a rich class of specifications.
617discovering relational specifications esec fse september paderborn germany we show that checking if a specification is consistent with the given data can be reduced to conjunctive queries allowing us to use efficient databases or datalog solvers to check specifications.
we describe our implementation of bach and present a thorough experimental evaluation on a range of libraries demonstrating bach s ability to learn a spectrum of useful specifications.
illustrative examples in this section we demonstrate the operation of bach through a set of simple illustrative examples.
each function discussed has an associated set of observations in figure .
e1 properties of addition suppose we have a function addthat takes two numbers and returns their sum and we have observed the input output relation of add in figure where i1andi2are the inputs and ris the return value.
induction phase the induction engine of bach searches the space of specifications and proposes candidate specifications.
suppose that the induction phase proposes the following candidate add x y z add y x z where x y zare implicitly universally quantified variables.
consistency verification now the specification goes to the consistency verifier which checks if the specification is consistent with the provided dataset.
the consistency verifier asks two questions ve evidence is there evidence that the specification holds?
ve evidence is there evidence that the specification does not hold?
to find positive evidence the verifier attempts to find three constant values a b and c such that add a b cand add b a c as per the given dataset.
the set of all possible values of a b c is considered the set of positive evidence.
to characterize positive evidence we view the set of input output examples of addas a ternary relation radd x y z where xandyare the inputs and zis the corresponding output.
now every possible tuple a b c that is evidence that the candidate specification holds is in the relation radd x y z radd y x z where is the standard joinoperation from relational algebra.
in the rest of the paper we will use the formalism of horn clauses in non recursive datalog to define positive evidence.
specifically we will say that the set of positive evidence pis defined as follows p x y z radd x y z radd y x z .
if the relation pis empty then there is no positive evidence.
semantically the above horn clause defines pas the smallest relation such that if a b c raddand b a c radd then a b c p. in our example we see that there is at least one tuple in p .
we now try to find negative evidence i.e.
tuples that falsify the specification.
since the specification is a bi implication we need to find evidence that holds for one side but not the other.
let us try to find a tuple that satisfies the left hand side but not the right hand side.
we do this as follows n x y z radd x y z radd y x z z z if the relation nis not empty then we know that there exists a tuple a b c such that add a b add b a .
in our example the relation nis empty and therefore bach infers the specification stating that addis a commutative function.add i1i2r .........gt i1i2r f t f t t .........abs i1 r ...... concat i1i2r a b ab a a .........len i1 r a b ab ...... figure example observed function executions.
intuitively our goal is to discover specifications with i no negative evidence associated with them we say they are consistent with the data and that ii have some positive evidence which we use as a proxy to the likelihood of a specification.
e2 transitivity of comparison consider gt the function implementing the greater than operation for integers with data in figure .
the specification induction phase will propose the following specification gt x y w gt y z w gt x z w note that this is an implication bach is able to infer both implications and bi implications as we shall describe in detail in section .
the consistency verifier will be able to find positive evidence and no negative evidence for this specification thus declaring it a possible specification for gt.
specifically it will solve the following two horn clauses on the given dataset and discover that the set pis non empty while nis empty.
p .
.
.
rgt x y w rgt y z w rgt x z w n .
.
.
rgt x y w rgt y z w rgt x z w w w e3 identity of absolute value consider the function abs absolute value with data in figure .
induction phase suppose that the induction phase proposes the following candidate specification abs x x which can be viewed as the implication true abs x x consistency verification the consistency verifier will solve the following set of horn clauses to discover positive and negative evidence and store them in two relations p x andn x p x rabs x x n x rabs x x x x in this example both relations will not be empty.
specifically p .
.
.
andn .
.
.
.
guard abduction the guard abduction phase attempts to weaken the specification by finding a formula gsuch that g abs x x 618esec fse september paderborn germany calvin smith gabriel ferns and aws albarghouthi has no negative evidence and has all or most of the positive evidence.
in other words we can view the guard abduction phase as solving a classification problem that of finding a classifier a formula g that labels elements of the set pwith trueand elements of the set nwith false.
to find such a g we assume we are given a set of predicates and functions with which we can construct g. for instance if we are learning specifications of an apithat operates over integers we might instantiate our algorithm with standard operations over integers e.g.
.
in this example the abduction phase might return the formula x resulting in the correct specification x abs x x there are many ways to approach such a classification task e.g.
using decision tree learning.
in practice we employ a simple algorithm for learning a conjunction of predicates that separates positive and negative evidence.
e4 string operations let us now consider an example with multiple functions.
suppose we are given a dataset provided in figure describing the input output relations of concat which concatenates two strings and len which returns the length of a string.
here is the empty string.
suppose that the induction phase proposes the specification len concat x y z len x z obviously this is not true.
however using the positive and negative evidence the guard abduction phase will discover that the specification holds when y resulting in the following specification y len concat x y z len x z additionally bach will infer other properties of concat and len e.g.
that the order of concatenation does not change the length of the resulting string.
len concat x y z len concat y x z we have illustrated the operation of bach through a series of simple examples.
in sections we formalize bach.
in section we thoroughly evaluate bach on a range of python libraries.
specifications and evidence we now formalize the core definitions needed for our algorithm.
formulas we assume formulas are over an interpreted theory where we have a finite set of uninterpreted functions f1 .
.
.
fn where each fihas arity ar fi .
aformulafis of the form v.g or v.g where i vis a set of variables.
ii g the guard is a formula over an interpreted set of predicate and function symbols.
iii analogously is defined asv i i where each iis an atom of the form t o where ois a variable in vandtis anested function application over the functions and variables v. we assume thatf has no free variables.
for simplicity and w.l.o.g.
we assume that all variables and functions are over the same domain d e.g.
integers .
example .
.
consider the following formula with f x y.x z g .. f x y z f x y z observe that gis a subformula using an interpreted predicate over integers and each of and are composed of a single atom containing nested uninterpreted function applications.
interpretations and models aninterpretation igives a definition to each function f i.e.
for each fandi dar f iassigns a value o d such that f i o. for each f we use ifto denote the definition of fini if i7 o f i o we will consider the interpretation iasi s f if a union of sets indexable by functions in .
given a formulaf an interpretation iis amodel off denoted i f ifisatisfiesf using the standard definition of first order satisfiability.
datasets intuitively a dataset din our setting is a partial interpretation that is an interpretation that defines each function f on a finite subset of the domain dar f .
given a dataset dand interpretation i we say iis acompletion ofd denoted d i if for allf df if.
consistency our goal is to define what it means for a formula f to explain a dataset d. we thus define a notion of consistency .
a formulafisinconsistent with d denoted d cf if i d.i f otherwise we say that disconsistent withf ord cf.
in other words if no matter how we complete a dataset it results in an interpretation that falsifies f then we say that fis inconsistent with d. otherwise we say it is consistent.
positive and negative evidence note that while a formula fcan be consistent with d this could happen vacuously.
the simplest case is the empty dataset which is consistent with any satisfiable formulaf.
our goal is not only to find a consistent formula but one that explains the data well.
we therefore define the notions of positive andnegative evidence.
first we define d restricted assignments.
definition .
d restricted assignment .
given quantifier free formula with variables v ad restricted assignment dis a map from eachv vto a constant that appears in the dataset d. additionally for every term f i d i7 o df for some o d where d is with all variables replaced by their assignment in d. example .
.
letdf .
d x7 is ad restricted assignment to f x x. this is because d f x x isf and1is in the domain of df.
on the other hand d x7 is not a valid assignment because fis not defined on 2indf.
in case of nested terms e.g.
f x ad restricted assignment needs to set xto a value cin the domain of d such that d c is in the domain of f. definition .
positive evidence .
given dandf wherefis of the form v.g or v.g we define positive evidence as pos d f d i d.i d g informally positive evidence is the set of instantiations of variables vthat non vacuously satisfy the body of f i.e.
g .
619discovering relational specifications esec fse september paderborn germany definition .
negative evidence .
we define negative evidence as the set of d restricted assignments such that neg d f d i d.i d g or with d g in the casefis an implication.
informally we can view negative evidence as the set of witnesses to the fact thatfis inconsistent with d. example .
.
consider formulaf x y.f x y x y and dataset d where df andd .
here there is no negative evidence.
however there is no positive evidence either i.e.
there is no witness to the fact that fand are equivalent.
if we update d to then pos d f will be the singleton set with the assignment that sets xto andyto .
alternatively if we update d to then negative evidence will be the set of two assignments x7 y7 x7 y7 .
the following lemma captures the fact that existence of negative evidence implies that the formula is inconsistent with the dataset.
lemma .
.
neg d f d cf the proof follows immediately from the definitions of consistency and negative evidence.
we will see the utility of this lemma in our problem specification.
specification learning problem given a dataset d fis alikely specification ifpos d f 0and neg d f .
in other words d supports the specification f and by the above lemma we cannot show that d cf.
anoptimal specification f is a likely specification that maximizes some function hofdandf.
formally f arg max fstneg d f h d f an immediate choice for his given by h d f pos d f .
intuitively this uses the amount of positive evidence as a proxy for how wellfexplains d. of course hcan also be adjusted to bias our search further if necessary e.g.
to formulas of smaller size.
specification learning algorithm we are now ready to formalize bach.
the algorithm is shown in figure as a set of non deterministic rules if the premise above the horizontal line is true then the instruction below the line is executed.
at a high level the operation of bach is simple it i iteratively constructs specifications and ii checks whether they are consistent with the data.
the state maintained by bach consists of two sets i j a set of conjunctions of atoms which are used to construct specifications e.g.
given j we can construct v. .
ii s a set of discovered likely specifications.
both sets grow monotonically.
search we assume there is a fixed signature dataset d and set of variables v. recall that an atom is of the form f t1 .
.
.
tn v where each tiis a function application or a variable.
the rules add expv and expfconstruct new atoms and conjoin them to formulas in the setj.
the symbol is used to denote a wildcard a hole that can be filled to complete an atom.
a conjunction j iscomplete if it contains no wildcards denoted cmp in figure .
induction and abduction the rules ind andabd form the core of the algorithm.
they apply when a specification is learned which they add to the set of likely specifications s. the analogous rules ind andabd learn specifications with implications not shown in the figure due to similarity .
let us walk through ind .
it picks two conjunctions of complete atoms and from the setj.
it then constructs a formula f v. .
iffhas positive but no negative evidence then it is added to the set of specifications s. for now we use posand neg declaratively in section we present an algorithm that constructs the sets of evidences.
the rule abd applies when i fhas non empty sets of negative and positive evidence and ii the two sets can be separated .
we assume we have an oracle classify that returns a formula g that separates the positive and negative evidence.
formally classify returns a formula gwithout uninterpreted functions and with free variables in v such that neg d f .
g is unsatisfiable.
x pos d f .x x. g is satisfiable.
in other words geliminates all negative evidence point and maintains some of the positive evidence point .
observe that we do not need gto maintain allpositive evidence we only need a non empty set and that gives us a likely specification.
soundness we view the soundness of bach as only adding likely specifications to the set s. this is maintained by construction through i the rules ind andabd ii and the definition of classify which ensures that all negative evidence is excised and some positive evidence is preserved.
rule application schedule our presentation of the algorithm as a set of rules allows us to dictate the search order by varying the scheduling of rule application.
for instance if we are interested in learning relations between pairs of programs we can restrict applications of the rule add to formulas that are true.
this ensures that there is only a single conjunct on either side of the bi implication.
we must also decide when to apply ind andabd .
in practice we apply abd right after a failed application of an induction rule.
specifically if a failed application of ind results in positive evidence andnegative evidence then we apply abd with the hope that we can find a guard that eliminates the negative evidence.
consistency verification we now describe our technique for verifying consistency of a formulafwith respect to a dataset d. .
background and overview the principal idea underlying our technique is that positive and negative evidence of a formula fand dataset dcan be characterized using a union of conjunctive queries ucq .
a conjunctive query cq is a first order logic query that can model a subset of database queries written in sql specifically a conjunctive query corresponds to a non recursive horn clause.
therefore a ucqcorresponds to a non recursive datalog program a set of horn clauses whose evaluation results in the positive and negative evidence.
our formulation of consistency verification as database query evaluation allows us to leverage efficient highly engineered database engines and datalog solvers.
620esec fse september paderborn germany calvin smith gabriel ferns and aws albarghouthi init j true s f j f .
.
.
ar f add j j j v v expv j j j f expf j j cmp j f v. pos d f neg d f ind s s f cmp j f v. p pos d f n neg d f g classify p n abd s s v.g notes i s xis short for s x ii .
.
.
ar f inadd andexpfare fresh and iii inexpfis assumed to be an argument to a function figure bach s main algorithm the rule init is only applied at initialization we provide a brief description of datalog and refer the reader to abiteboul et al.
s textbook for a formal presentation of datalog semantics .
a horn clause is of the form h x0 b1 x1 .
.
.
bn xn .
where h b1 .
.
.
bnare relation symbols each xiis a vector of variables of size equal to the arity of the corresponding relation the atom h x0 is the head of the clause and the set of atoms bi xi i is the body of the clause.
a datalog program cis a set of horn clauses.
semantically a solution of a datalog program is the least interpretation of the relations that satisfies all the clauses.
for our purposes we will enrich our language with inequalities of the form x y where xandyare variables which can appear in the bodies of clauses.
we will use underscores e.g.
r x to denote that the second argument of risunbound i.e.
can take any value.
.
detailed description figure describes the algorithm used to construct a set of horn clauses encoding the positive negative evidence of fwith respect tod.
we assumefis of the form x. where v i i v j j each i and j is an atom of the form f t1 .
.
.
tn x andxis the vector of universally quantified variables.
we assume that input output data of each n ary function fis stored in a n ary relation rf.
the horn clause construction decomposes into three steps is encoded in a relation ah xa and inbh xb by flattening the atoms positive evidence is encoded in the relation p x by collecting variable assignments satisfying and and negative evidence is encoded in the relation n x by satisfying while negating .
procedure encode aggregates all generated horn clauses into a single datalog program c. note that we expect the formula fto be a bi implication.
if fis of the form x. we construct negative evidence by only considering data satisfying and .
given a vector of variables xappearing in a formula f we will construct a vector of datalog variables xindexed by x x i.e.
x ximplies xx x .
we use h x s where sis the set of terms ri xi n i to denote the horn clause h x r1 x1 r2 x2 .
.
.
rn xn .in addition we use for adding a single element to a set.
for example x y z x y z .encoding specifications let us walk through the construction of horn clauses encoding and .
we focus on as the encoding for is symmetric.
by assumption the conjunction consists of atoms ta i xa i where ta iis a term of the form f t1 .
.
.
tn we extract those atoms using the atoms subroutine.
in the first for loop of encode we iterate over every atom and encode it as a horn clause.
the atom ta i xa iis encoded in the relation ai xa i xa i where xa irepresents the inputs to the term ta iandxa irepresents the output in this case the value of the formula variable xa i .
because each atom can have nested function applications this procedure is recursive and so we make use of the subroutine flatten .
finally we encode as the conjunction of each atom which translates into the horn clause ah xa ai xa i xa i n i .
example .
.
we now demonstrate horn clause construction on a simple example.
consider the following formula x y.f x y h x y which states that f is equivalent to h. encoding the atom f x yrequires three recursive calls to flatten once for x x and f x .
working from the inside out we see flatten x converts the term xto the pair xx.
the call to flatten x uses this pair to construct the pair r xx o o. finally flatten f x expands on this value to return the pair rf o o r xx o o .
the first for loop in encode uses these results to tie the formula variable yto the output of the term f x by constructing the clause a1 xx xy o xy rf o o r xx o .
the right hand side of is encoded similarly.
positive evidence positive evidence consists exactly of those variable assignments that non trivially satisfy both and .
as and are already fully encoded in the relations ah xa andbh xb this requirement is immediately encodable as the horn clause p x ah xa bh xb .
negative evidence let us now describe the construction of the horn clauses encoding negative evidence.
intuitively negative evidence occurs when we satisfy the left side but falsify the right side .
in other words we want to falsify at least one of the atoms 1 .
.
.
m. we thus construct mclauses each one encoding assignments that falsify one of the j s. for instance assignments that 621discovering relational specifications esec fse september paderborn germany defflatten t term case tisx where x x return xx case tisf t1 .
.
.
tn fori .
.
.
n ri oi flatten ti o fresh variable r rf o1 .
.
.
on o sn i 1ri return r o defencode spec c ta i xa i n i atoms tb j xb j m j atoms encode fori .
.
.
n ra i oa i flatten ta i xa i vars ra i c c ai xa i xxa i ra i oa i xxa i c c ah xa ai xa i xa i n i encode forj .
.
.
m ...omitted... c c bh xb bj xb j xb j m j encode positive evidence c c p x ah xa bh xb encode left negative evidence forj .
.
.
m o fresh variable bad bj xb j o o xb j c c n x ah xa bad bi xb i i j encode right negative evidence fori .
.
.
n ...omitted... returnc figure encoding formulas as horn clauses.
omitted for loops are symmetric by exchanging aandb to those immediately preceding.
we use for assignment and for datalog implication.
falsify 1are encoded by the clause n x ah xa b1 xb o o xb b2 xb .
.
.
bm xb m the fresh variable ois used to encode the fact that b1should output a value that is not equal to xb thus falsifying the right hand side of the bi implication.
recall that b1encodes a term of the form f .
.
.
x. effectively the above clause states that f .
.
.
x. example .
.
recall example .
.
negative evidence as constructed by encode is written as follows n xx xy ah xx xy b1 xx o o xy n xx xy bh xx xy a1 xx o o xy the first clause encodes the requirement that f x yis satisfied but h x yis not the second clause encodes the opposite fact.
correctness once we have constructed the horn clauses we evaluate them on the given dataset to construct the relations pandn.
the following theorem states correctness of the construction theorem .
.
given a dataset dand specificationfof the form x. or x. let n x andp x be the relations computed using the horn clauses cfrom figure .
then p x pos d f andn x neg d f .
complexity it is important to note that the decision problem of solving a conjunctive query is np complete combined complexity .
if the size of the query is fixed and the only variable is the size of the data the problem is in ptime data complexity .
this is why database engines are efficient queries are typically small but data is large.
these classic results shed light on the difficulty of the problem of finding positive negative evidence one could easily reduce conjunctive query solving to finding positive evidence in our setting thus our consistency verification problem is np hard.
implementation and evaluation in this section we i describe our implementation of bach ii present an exploratory study in which we apply bach to a number of libraries and iii present an empirical evaluation to investigate the performance and precision characteristics of bach.
.
implementation bach is implemented in ocaml.
it takes as input i a signature of simply typed functions ii input output data for each function and iii a set of predicates to compute the guards.
bach uses the souffl datalog engine to compute positive negative evidence.
ordering the search the search rules add expv and expfare scheduled to implement a frontier search with respect to the size of specifications.
that is we visit specifications in order from smallest to largest.
the search rules are augmented with types ensuring that only well typed specifications are explored.
pruning the search top down enumerative synthesis tools typically have exponential branching of the search space and bach is no exception.
to combat this explosion of the search space bach employs a series of search space pruning techniques first bach uses a representation of specifications that guarantees that each explored specification is unique with respect to conjunct reordering by commutativity of conjunction and variable renaming.
second whenever bach proves a specification f correct it records one of and the larger with respect to number and size of atoms if it is obvious .
during the search bach will never apply the search rules to generate the recorded term.
specification preference bach combines induction and abduction rule application as follows given two sets of conjunctions j it first attempts to learn the bi implication using theind rule.
if ind fails to apply due to existence of negative evidence then bach examines the negative evidence to determine if it is only one sided .
if so then bach learns an implication using the rule ind .
if no implication can be learned bach resorts to abduction.
specifically it solves a number of abduction problems to learn guards that make the following specifications likely ones g1 g2 g3 622esec fse september paderborn germany calvin smith gabriel ferns and aws albarghouthi table list of benchmarks number of functions is in parentheses.
benchmark description list standard list operations including hd tl cons etc.
matrix matrix operations from python s sympy library.
trig trig.
functions sin cos etc.
in python s math module.
z3 apito python s z3library including satandvalid .
geometry manipulations of shapes from python s sympy library.
sets functions from python s setmodule.
dict functions from python s dict dictionary module.
fp199 arithmetic on f199 the finite field of order .
strings string operations from python s string module.
bach then picks the specification with the highest positive evidence.
abduction guard abduction is done by a simple classification algorithm that finds a small conjunction of the provided predicates.
each predicate is instantiated with every combination of variables.
for instance if the predicate a bis provided andfcontains the variables xandy abduction will use x yandy x. bach learns a conjunction that separates the positive and negative evidence of fwhile retaining as much positive evidence as possible.
.
exploratory evaluation setup in order to test the efficacy of bach we targeted a set of python libraries .
each benchmark consists of i a finite signature ii a set of predicates and iii a dataset of randomly sampled executions for each function.
these random samples are generated by uniformly sampling function inputs from a subdomain of the relevant type and then evaluating the function.
we are interested in examining a variety of specifications.
to cover as much of the search space as possible we run many independent executions of bach in parallel.
each execution is configured to search over a different subset of functions from the signature or at a different initial depth.
this gives a mix of large and small likely specifications with a variety of combinations of functions.
after letting each execution run for a short amount of time minutes all the resultant likely specifications are collected and presented together.
a partial list of specifications found is provided in figure .
the output of bach contains many specifications that are possibly of interest a few of which are discussed below.
z3specifications z3is a high performance smtsolver with apis for many programming languages.
the z3benchmark contains functions from a subset of python s z3api.
bach finds the expected specifications relating and or and negthrough demorgan s laws distributivity etc.
however the benchmark also contains valid andsat which check for the validity or satisfiability of a formula.
consequently bach discovers the specification p true valid x p sat x p which states that valid formulas are always satisfiable but not the opposite .
bach also finds interactions between valid and the logical connectives.
for example valid x p valid y p valid and x y p which encodes the fact that validity is preserved by and.
strings specifications thestrings benchmark contains the typical set of functions for manipulating strings.
bach finds likely specifications which encode idempotence properties such as lstrip x y lstrip y y learned specifications for list sorting a list preserves length length x a length sort x a hdis the destructor of cons cons a y x hd x a revis an involution true rev rev x x learned specifications for matrix identity matrix is upper triangular identity x p true upper x p true transpose preserves symmetric ness symmetric x p symmetric transpose x p transpose is an involution transpose transpose x x learned specifications for trig sinis the inverse of arcsin arcsin z x sin x z sinhas period 2 k.x 2 k y sin x z sin y z sinandcosare shifted by x y sin x z cos y z learned specifications for strings a string is a prefix of itself p true prefix x x p stripping whitespace is idempotent lstrip x y lstrip y y palindromes are preserved by reverse concat y reverse y x reverse x x learned specifications for z3 validity implies satisfiability p true valid x p sat x p andis commutative and x y z and y x z andpreserves validity valid x p valid y p valid and x y p learned specifications for sets the empty set contains nothing p false clear x y contains y a p the empty set is contained in every set p true clear x y subset y z p the subset relation is inclusive p true subset x x p learned specifications for geometry if enclosed shape contains point then encloser also contains it b true encl x y b encl pt y p b encl pt x p b rotating a shape by a multiple of 2 results in same shape k.x 2 k rotate y x y figure a sample of learned specifications on our benchmark suite.
formulas k.x 2 k y k.x 2 k x y and p true false are guards.
623discovering relational specifications esec fse september paderborn germany table average correctness results t1is the type i error t2is the type ii error and size is the number of specifications produced observations observations observations observations benchmark t1 t2 size t1 t2 size t1 t2 size t1t2 size ff199 .
.
.
.
.
.
.
.
.
.
.
.
trig .
.
.
.
dict .
.
.
.
geometry .
.
lists .
.
.
.
.
.
.
.
.
matrices .
.
.
.
.
.
.
.
.
.
sets .
.
.
.
.
.
.
.
strings .
.
.
.
.
.
.
.
.
.
where lstrip x removes all whitespace on the left of x as well as useful facts like p true prefix x x p which states that string prefix is a reflexive relation.
amusingly bach also learns that we can construct palindromes by concatenating a string and its reverse concat y reverse y x reverse x x. trig specifications the trig benchmark contains trigonometric functions from python s math module which have a rich set of semantics.
bach has no problem finding many of these properties as likely specifications.
these include the fact that trigonometric functions are periodic k.x 2 k y sin x z sin y z where k.x 2 k yis provided as a predicate on xandy.
bach also discovers that sinand arcsin arealmost inverses of each other arcsin z x sin x z. note the implication.
this is because arcsin is sometimes undefined and so sinand arcsin are only inverses on the range of arcsin .
geometry specifications thegeometry benchmark contains functions from sympy s a popular python library geometry module which supplies operations over 2d shapes on a plane.
bach learns the following specification b true encl x y b encl pt y p b encl pt x p b which states that if i 2d shape xencloses shape y and ii point pis in shape y then pis in shape x. another insightful property that bach detects is that rotating a shape by a multiple of 2 results in the shape itself k.x 2 k rotate y x y finding interesting specifications in order to extract the previous specifications and those in figure we rank the output of bach in decreasing order by a function h d f f pos d f where comparison is done lexicographically and is computed by counting astnodes.
optimal specifications in this context are those that are small yet have large amounts of positive evidence.
while this ranking function worked to produce a variety of interesting specifications it also obscured a few that we expected to see ranked more highly associativity of matrix multiplication did not show up until late in the list.
finding improved ranking functions for various domains and tasks is an area of future research.
.
empirical evaluation we now investigate i the scalability of bach and ii the significance of bach s learned specifications.
scalability the horn clauses for negative evidence can in some cases result in a polynomial increase in the size of the relations.
to evaluate the impact of this behavior we measure the number of checked specifications per second i.e.
calls to datalog solver .
search performance is dependent more on the structure of the formula and the amount of data than any inherent semantic meaning of the library functions.
as such we test scalability on a representative benchmark in this case ff199 .
for k and1000 we sample kobservations for each function to construct the dataset dk.
we run bach for minutes on dk and report the number of checked specifications at every point in time.
the results are presented in figure a .
the results are as anticipated with more data bach checks less specifications in the same amount of time.
the best performing benchmark k checks 9x more specifications than the worstperforming benchmark k .
of note are the plateaus in the k 10results which indicate souffl getting slowed down with queries with large output.
these plateaus suggest that too much data can overwhelm the external datalog solver to the point of losing performance.
in the future we would like to experiment with approximate queries where we sample a subset of the data with the goal of falsifying a query before trying the full dataset.
error analysis to evaluate correctness of bach we need to determine how often bach is wrong.
we proceed by fixing a notion of ground truth and computing type i ii error.
type i error is bach presenting an incorrect specification false positive while type ii error is bach failing to present a correct specification false negative .
accurately determining ground truth for the domains bach operates on requires enumerating all possible hypotheses and asking a human expert or an automated verifier to label them as true or false.
this is an infeasible i there are infinitely many possible hypotheses as our formulas are not size bounded and ii even if we bound the size of formulas there are exponentially many specifications to consider.
using an automated verifier is a possibility but state of the art checking of relational specifications is limited to simple programs and properties .
in our setting we are dealing with non trivial dynamic python code.
to evaluate error rates we conducted an experiment where weapproximate ground truth by running bach on a large observed executions per function dataset per benchmark and ensured bach checked every formula up to size measured by ast leaves .
we chose observations to generate ground truth because i we observed that the number of discovered specifications 624esec fse september paderborn germany calvin smith gabriel ferns and aws albarghouthi a b c figure a of specifications per unit time for ff199 .
b error analysis for ff199 band is confidence interval .
c error analysis for sets.
stabilizes at and ii at observations the number of type i errors is very low.
for point ii we corroborated the accuracy of our approximate ground truth by randomly sampling specifications from those discovered for each benchmark with observations and manually classifying them as correct incorrect specifications.
manual inspection showed that of the sampled specifications were correct.
to understand how well bach performs on varying amounts of data for k we randomly sample kobservations per function to construct a dataset dkthat is independent from the approximate ground truth.
we run bach on the same search space ground truth was generated on formulas up to size and compute type i and type ii with respect to ground truth.
for each k we repeat this process multiple times each time randomly sampling a different dataset.
see table for the results.
for every benchmark type i ii error tends to decrease as we increase the amount of data.
by observations type ii errors have nearly disappeared for every benchmark except ff199 .
by observations the number of type i errors has also fallen dramatically.
for example in strings which has over average specifications produced bach generates on average .
type i errors.
to provide a clearer picture for representative best and worstcase benchmarks ff199 andsets respectively we also compute error rates at each k .
.
.
.
the results are presented in figure b c .
here false positive error rate is fp tp fp where fpandtpare the number of false and true positives.
negative error rate is defined symmetrically.
by observations our worst performing benchmark ff199 has a positive error rate of .
meaning we expect bach to discover an incorrect specification of the time.
conversely our best performing benchmark sets has converged to ground truth by observations.
these results indicate that for most benchmarks bach can achieve reasonable results before the decrease in performance found in our scalability experiments becomes prohibitive.
the exceptions to are ff199 andmatrices which are both numeric benchmarks.
this is due to the difficulty of sampling corner cases e.g.
or non invertible matrices with low numbers of observations.
related work specification inference in the introduction we compared bach with quickspec and daikon .
the work of henkel et al.
for documenting java container classes is also very closely related to quickspec and has the same comparison with bach.a number of specification learning techniques use positive and negative examples to learn safe preconditions for calling a function .
for example padhi et al.
s pie and sankaranarayanan et al.
s work use input output data to learn preconditions that ensure a given postcondition is satisfied for a single function.
these approaches synthesize a boolean formula over a fixed set of predicates piecan additionally infer new predicates by searching over a given grammar.
gehr et al.
use positive and negative examples to synthesize a precondition that ensures that two function calls commute with the goal of discovering safe parallel execution contexts.
bach discovers specifications that correlate executions of multiple functions and does not require annotated positive negative examples.
bach s abduction engine solves a boolean classification task like the aforementioned works.
thus it can technically be instrumented for inferring safe preconditions.
a number of other techniques aim to learn temporal specifications e.g.
which specify acceptable sequences of events e.g.
calls to an api.
ilp inductive logic programming ilp is a machine learning technique that infers horn clauses to logically classify a set of positive and negative examples.
more than twenty years ago cohen used ilpto infer specifications by observing behaviors of a switching system.
sankaranarayanan et al.
used ilp to infer horn clauses that explain when exceptions are thrown in various data structure implementations.
ilptechniques like foil and progol are optimized for learning horn clauses and tend to sacrifice correctness full classification precision for scalability.
bach on the other hand does not require annotated examples i.e.
it is unsupervised and can in principle discover horn clause specifications in addition to arbitrary bi implications.
conclusion we presented bach an automated technique for learning relational specifications from input output data.
our evaluation demonstrated bach s ability to learn interesting specifications of realworld libraries.
there are many potential uses of bach which we plan on investigating in the future it could be used to detect axioms useful for verification of applications using library code for lemma discovery e.g.
in interactive theorem provers like coq or to automatically annotate library documentation with specifications.