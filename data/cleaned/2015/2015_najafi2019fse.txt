bisecting commits and modeling commit risk during testing armin najafi department of computer science and software engineering concordia university montr al qu bec canada a ajaf encs.concordia.capeter c. rigby department of computer science and software engineering concordia university montr al qu bec canada peter.rigby concordia.caweiyi shang department of computer science and software engineering concordia university montr al qu bec canada shang encs.concordia.ca abstract software testing is one of the costliest stages in the software development life cycle.
one approach to reducing the test execution cost is to group changes and test them as a batch i.e.batch testing .
however when tests fail in a batch commits in the batch need to be re tested to identify the cause of the failure i.e.the culprit commit.
the re testing is typically done through bisection i.e.a binary search through the commits in a batch .
intuitively the effectiveness of batch testing highly depends on the size of the batch.
larger batches require fewer initial test runs but have a higher chance of a test failure that can lead to expensive test re runs to find the culprit.
we are unaware of research that investigates and simulates the impact of batch sizes on the cost of testing in industry.
in this work we first conduct empirical studies on the effectiveness of batch testing in three large scale industrial software systems at ericsson.
using months of testing data we simulate batch sizes from to and find the most cost effective batchsize for each project.
our results show that batch testing saves of test executions compared to testing each commit individually.
in a second simulation we incorporate flaky tests that pass and fail on the same commit as they are a significant source of additional test executions on large projects.
we model the degree of flakiness for each project and find that test flakiness reduces the cost savings to .
in a third simulation we guide bisection to reduce the likelihood of batch testing failures.
we model the riskiness of each commit in a batch using a bug model and a test execution history model.
the risky commits are tested individually while the less risky commits are tested in a single larger batch.
culprit predictions with our approach reduce test executions up to compared to ericsson s current bisection approach.
the results have been adopted by developers at ericsson and a tool to guide bisection is in the process of being added to ericsson s continuous integration pipeline.
ccs concepts software and its engineering software testing and debugging .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august tallinn estonia association for computing machinery.
acm isbn .
.
.
.
culprit risk models batching and biseciton software testing empirical software engineering acm reference format armin najafi peter c. rigby and weiyi shang.
.
bisecting commits and modeling commit risk during testing.
in proceedings of the 27th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august tallinn estonia.
acm new york ny usa pages.
introduction software testing is one of the costliest stages of the software development process.
prior research estimates that testing consumes between to of the time in software development life cycle .
to isolate test failures some companies have adopted the devops strategy of testing each individual commit.
while effective at isolation there are substantial computation requirements.
to limit the resource requirements some software companies including ericsson have adopted batching to reduce the cost of testing.
batch testing groups commits and allows all of them to be tested at once.
when the batch passes all of the commits can proceed in the continuous integration pipeline at once and save resources.
although batch testing can reduce test executions it introduces a new challenge.
when a batch fails the culprit commit causing the batch failure needs to be identified.
one of the common approaches used for finding a culprit in a group of failing commits is bisection.
in the first part of our work we study the impact of batch testing on reducing the test executions in environments with various test failure rates.
in practice test failure rates in test environments tend to be very low.
for example on chrome only .
of tests fail .
batch testing offers the highest savings in test environments with low failure rates.
in the second part we examine flaky tests which can pass and fail on the same commit.
google reports that in tests are flaky and that newly failing tests are actually flaky failures .
flaky tests are exacerbated by batching as the batch size grows the probability that one or more commits will have a flaky test failure also grows.
in the last part of our work we propose more efficient approaches for finding the culprits when a batch failure happens.
we propose risk based approaches for calculating risk values for commits of a batch.
then we propose a testtopk approach for testing the riskier commits individually and the rest of the less risky commits together in a separate batch.
we study how this approach can reduce the test executions compared to the bisection approach used in ericsson.
we propose two risk calculation approaches.
the first approach is based on well studied bug model literature .
our second approach is based on using 279esec fse august tallinn estonia armin najafi peter c. rigby and weiyi shang test execution history and the file changes of the commits.
more specifically we answer the following three research questions.
rq1 what is the most cost effective batchsize for the number of culprits discovered during testing?
batching commits allows a set of commits to be tested as a single unit.
however if a batch fails a bisection process must be used to isolate the commit responsible for the failure i.e.the culprit.
a bisection involves a binary search leading to an addition of lo 2 n executions to isolate a single culprit.
using three projects at ericsson we calculate the culpritrate for each project and simulate varying batchsizes to determine the most cost effective batchsize for each project.
we find that the higher the culpritrate the smaller the most costeffective batchsize .
for example project a has a culpritrate .
times higher than project c and with a batchsize of the savings are .
in contrast project c can have a batchsize up to and saves of test executions when compared with testing all commits.
the culpritrate can be trivially calculated from historical test results to determine the most cost effective batchsize for any project.
rq2 what is the most cost effective batchsize when some bisections are done as a result of flaky failures?
test flakiness is an inevitable part of any test environment.
a flaky test failure is defined as a test that passes and fails on the same commit.
we study the impact of test flakiness in finding the most cost effective batchsize .
when commits are tested individually a flaky failure does not affect other commits and the number of executions remains constant.
in contrast the larger the batchsize the higher the probability that at least one of the commits in the batch will be flaky.
any flaky batch failure incurs the penalty of an unnecessary bisection.
we find that the higher the flakerate the smaller the batchsize and smaller the savings in executions.
for example project b has aflakerate .
times higher than project c and a batchsize of saves of the executions compared to respectively.
with flaky failures project c s most cost effective batchsize and savings are reduced from a batchsize of and execution savings of to and respectively.
rq3 can risk models predict the culprit commit and reduce the number of executions to find the culprits on failing batches?
batch testing is effective in reducing the test executions however it introduces a new challenge.
when a batch fails the root cause of the failure i.e.culprit needs to be found among the failed commits.
we use commit risk models to predict the culprit commit when a batch fails.
we use two types of models bugmodels and testexecutionhistory models.
bugmodels have been effective at identifying the commits that are most likely to lead to future bugs i.e.bug introducing changes.
.
we use these techniques to identify which of the commits is the most likely culprit.
we then test the riskiest commits individually and batch the remaining commits.
our second approach is based on using test execution history.
test executions history has been largely studied for performing test selection and prioritization .
in contrast we use test execution history to predict a culprit commit given a batch test failure.
particularly we use the relationship between file changes and test failures extracted from the test execution history.we find that both culprit risk prediction models are effective buttestexecutionhistory outperforms the bugmodel .testexecutionhistory is able to predict the culprits using the top2 predictions with a sufficientandcorrectat2 of and for projects b and c with batchsizes .
we note that if the prediction is wrong we will simply run more test executions to find the correct culprit.
compared to bisection testexecutionhistory saves .
and .
executions respectively the results we present here have convinced ericsson developers to implement our culprit risk predictions in the culpred tool that will make their continuous integration pipeline more efficient.
this paper is structured as follows.
in section we discuss the batching and bisection process used at ericsson.
in section we explain how we guide the bisection process to reduce the number of test executions to find culprits with risk models.
in section we introduce our simulations methodology and data used in the study.
in section we present results for each of our research questions.
in section we describe the threats to validity and how we mitigate them.
in section we discuss related work.
in section we present our contributions and conclude the work.
background on batching and bisection to reduce test execution costs instead of testing each new commit submitted by developers individually commits can be collected in groups called batches and tested together.
ericsson uses this technique to reduce test executions as part of its continuous integration processes.
in this context every batch is a group of one or more commits that require specific tests.
as developers submit new changes commits enter the test queue.
the batching process consists of periodically collecting commits from the top of the queue and running the required tests.
as tests are combined into a single build and tested together batching can reduce test executions.
however when a test fails on a batch we need additional test runs to isolate the commit s that is causing the failure i.e.the culprit commit.
one approach to isolating the culprit commit is to run bisection which is a binary search on the commits contained in a batch.
the bisection process used at ericsson involves splitting the commits of the batch in half as illustrated in figure .
this produces two new batches that are each half the size of the original batch.
if the tests pass on a batch we know the culprit is not among these commits.
if a batch fails we continue the bisection process.
the stopping condition is when the remaining batches contain a single commit and the tests on that commit fail.
a single commit with a failing test is the isolated culprit.
the culprit will be subject to further investigations by testers or developers.
there can be multiple culprits and the search continues with each split until all the culprits are found.
for example in figure the process starts with batch .
this batch fails because it includes one culprit commit commit .
the commits in batch are split into batch and batch .
batch passes because it includes no culprits.
therefore all of its commits get delivered.
batch however fails because it includes the culprit commit.
batch is split into batch and .
batch passes.
280bisecting commits and modeling commit risk during testing esec fse august tallinn estonia batch however fails and because it consists of one commit it is the culprit.
mathematically we know that a binary search always requires lo 2 n executions.
however as can be seen in the example the tests must be run on both sides of the split binary tree.
as a result we must run lo 2 n executions.
since we must determine if the starting batch passes we need an additional execution.
with n commits the number of executions required to find a single culprit is lo 2 n guiding bisection based on culprit risk models commits are batched and tested in the order in which they arrive fifo queue .
however some commits contain more risky changes than others .
our goal is to model the risk and group changes such that risky commits are tested individually while less risky commits are grouped into batches that are more likely to pass without requiring bisection.
in this section we describe how we guide bisection by risk in the subsequent sections we show how we calculate the risk using two models testexecutionhistory and bugmodel .
the bisection process is inefficient because when any batch fails there are at least lo n additional executions where n is the number of commits in the failing batch.
we introduce the testtopk approach to isolate the top n riskiest commits and test them individually while batching the remaining commits into a single large batch.
figure provides an illustration of top1.
the process starts with batch which fails because it includes a culprit.
after the failure we calculate risk values for the commits and test the topnindividually.
for top1 we individually test the riskiest commit.
the remaining commits are tested as a single batch of size three.
to find the culprit and deliver the commits we need three executions instead of the five used for normal bisection see figure .
bugmodels have been used to suggest risky files and changes that are more likely to contain future bugs.
however it has been difficult for developers to act upon these predictions as they do not indicate specific problems in the source code.
testexecutionhistory has been used to determine which tests should be run for a set of files as well as to prioritize tests in a queue but has not been used to create batches of commits.
in this work we modify these approaches to assign risk to each of the changes under test and to determine which commit is most likely to be the true test failure i.e.culprit.
we guide the bisection processes by testing high risk commits individually.
.
bugmodel bugmodels have a long history in the software engineering literature.
research has predicted which files and modules are most likely to contain defects i.e.which are riskiest .
in contrast kamei et al.
quantified the risk of a commit instead of an individual file or module.
in this way the authors were able to alert developers to the changes that may need additionalreview.
however the measures are difficult to act upon because they simply indicate that a change is large or that a developer has less experience instead of indicating specific problems in the source code.
since our goal is to simply identify the riskiest commit and the action is to simply run a test bugmodels provide adequate information.
instead of training on the likelihood that a change will introduce a bug we are interested in how likely a commit is to fail tests i.e.
is a culprit indicating a system fault.
we train a logistic regression model to distinguish the commits that are most likely culprits so our unit is the commit.
we use the simplest measures proposed by kamei et al.
and leave more advanced bugmodels to future work.
as each commit may have multiple file changes if a measure is related to specific files the average of the measure over all of the file changes of the commit is considered.
the measures are explained below number of line changes total number of lines deleted and inserted in the commit.
number of file changes total number of file changes in the commit.
number of modified subsystems kamei et al.
define a subsystem as the root directory of a file path in a project tree.
for this metric we simply count the number of file changes that have different root directories.
commit message a boolean value based on availability of bug fix or defect in the commit message .
developers number of developers that were involved in change history of the changed files of the commit averaged over the files.
experience experience of the author of the commit on each of the changed files of the commit averaged over the files.
change time interval time interval between the current change and the previous change of each of the changed files of the commit averaged over the files.
.
testexecutionhistory prior work has shown that tests that have failed in the past are likely to continue failing .
preliminary work at ericsson has shown a relationship between failing tests and the files in the change under test .
while these works use test history to select and prioritize the tests that should be run for a change we are the first to use this relationship to determine which change in a failing batch is the likely culprit.
for each historical culprit we record the tests that fail and the files that were changed so that we can calculate how likely a test is to fail for a given file.
the following process is used to calculate a culprit score for each commit in a batch.
.
given the frequency of historical file and test failures we calculate the probability that the culprit is related to a file and test prob filen testx filen fails testx total fails testx .
we normalize this probability by the number of lines changed in the file over the total lines changed in the commit 281esec fse august tallinn estonia armin najafi peter c. rigby and weiyi shang rpplw rpplw rpplw rpplw dwfk rpplw rpplw dwfk rpplw rpplw dwfk rpplw dwfk rpplw dwfk edwfk h hfxwlrqv lq wrwdo wr lvrodwh wkh fxosulw frpplwv figure bisection .
when a batch of commits fails a bisection is performed to find the culprit commit.
since an execution is required for each binary split there are lo 2 n 1executions required to find a culprit.
to bisect commits we must run executions.
however if the batch passes we would need execution to test the commits.
rpplw rpplw rpplw rpplw dwfk xosulw suhglfwlrqv rpplw rpplw rpplw rpplw rpplw rpplw dwfk edwfk h hfxwlrqv lq wrwdo wr lvrodwh wkh fxosulw frpplwv dwfk rpplw q hyhu vwhs wkh wrs frpplw iurp wkh udqnhg olvw zloo eh fkrvhq wr eh whvwhg vhsdudwho rpplw figure testtopk.
the riskiest n commits are tested individually with the remaining commits combined in a single batch.
in this case top1reduces the number of required executions to three compared to the five in figure .
prob norm filen testx line changes in filen line changes in commit prob filen testx .
we sum across all files in the commit to calculate the culprit score for the commit culprit score commit n files x testsprob norm filen testx figure shows an example.
let us assume batch is a new failure and we want to calculate culprit risk scores for its commits commit and .
commit has two file changes file a and file b. to calculate the culprit probability for file b and test we see that in the past test has failed times and of those times file b was under change prob f ileb test .
as there are lines changed in commit and file b has line changes prob norm f ileb test is calculated as .
we similarly calculate prob norm f ilea test and sum the two values to get the final risk score for commit .
simulation methodology and data we evaluate three projects at ericsson which we name a b and c over the period of january to september .
the test practices and bisection techniques have been described in section .
project a is the smallest with .3k commits.
project b is larger with .5k commits.
project c is the largest with .5k commits.the goal of this work is to find the most cost effective batchsize given the culpritrate and the flakerate .
to perform a simulation we need to know the culpritrate and the flakerate .
when a test fails on a commit and an issue with the software is discovered we consider this commit to be the root cause or culprit for the test failure.
during batching commits are grouped together and bisection must be used to identify which commit is the cause or culprit of the failing batch.
we define the culpritrate to be the number of culprit commits divided by the total number of commits for the project.
culpritrate culpritcommits totalcommits the respective culpritrate for projects a b and c is .
.
and .
.
project c has the fewest culprits.
project a and b have .
and .
times as many culprits as project c. we must quantify the flakerate for our simulations because flaky test failures require additional bisections and executions.
a flaky test failure is defined as a test that passes and fails on the same commit.
we define a flakybatch as a batch that initially fails but does not lead to an individual failing commit i.e.no culprits are identified.
we define the flakerate as the number of flaky batches divided by the total number of batches.
flakerate flakybatches totalbatches the respective flakerates for projects a b and c are .
.
and .
.
projects a and b have a similar flakerate with project a being slightly higher.
projects a and b both have .
times more flaky failures than project c. 282bisecting commits and modeling commit risk during testing esec fse august tallinn estonia 7hvw loh idlov wrwdobidlov loh idlov wrwdobidlov loh idlov wrwdobidlov wrwdobidlovbryhudoo rpplw rpplw dwfk dloxuh rq 7hvw rpplw loh olqh fkdqjhv loh olqh fkdqjhv 7rwdo olqh fkdqjhv dloxuh 2ffxuuhqfh uhtxhqf 0ds iru 7hvw figure calculating the commit culprit score based on file and test failure frequency.
.
the impact of batchsize onflakerate we calculated the overall flakerate for all batchsizes .
however the larger the batchsize the higher the probability that at least one of the commits in the batch will be flaky.
we do not know theflakerate for individual commits or tests.
instead we know the size of the batch and whether or not it was a flakybatch .
any flaky batch failure incurs the penalty of unnecessary bisection and executions that must be accounted for in a simulation.
we create a logistic regression model to determine the probability that a batch of size n will result in a flaky failure.
in figure we plot the logistic regression line indicating the probability of failure for batch sizes to .
ericsson requested that we do not show the actual flakerate for batches so the y axis is unlabeled.
however it is clear that as the batchsize grows the probability that a batch is flaky increases dramatically.
using this model we correct the number of executions to include flaky failures.
in equation we multiply the number of batches by theflakerate to give us the expected number of flaky batches.
each flaky batch requires an additional bisection the cost in executions is defined in equation .
flakyexecutions batches flakerate log2 n we then add these additional flakyexecutions to the executions required to find all the true culprits total executions culrpitexecutions flakyexecutions for example for project c a batchsize of is about two times more likely to have a flaky failure than batchsize of .
we add these extra flaky executions for batchsize .
.
simulation methodology ericsson testers evaluate batch test failures on a daily basis.
we run daily simulations using a simple incremental framework that has been commonly used in the research literature .
figure probability of a flaky failure for each batchsize .
the probability is estimated with a logistic regression model for each project.
we anonymized the y axis at ericsson s request.
our simulation period runs for months and covers 14k commits which lead to hundreds of culprits.
we use the first months as an initial training period.
after this period we test the approaches on the commits that are available for the test each day t 90to t .
to predict whether a failure on day d twill lead to a culprit we train on the historical data from d 0tod t 1and test on d t. we repeat this training and testing cycle for each day until we reach d .
we also run a simulation using a sliding training window of three months.
in this case to predict whether a batch failure on dayd twill lead to a culprit we train on the historical data from 283esec fse august tallinn estonia armin najafi peter c. rigby and weiyi shang d t 90tod t 1and test on d t. we repeat this training and testing cycle for each day until we reach d .
while we simulate batching on ericsson data our method and measures are not tied in any way to ericsson data.
to run this simulation on other projects one simply needs the test outcomes for each change.
the test outcome will allow one to calculate the culpritrate andflakerate .
results in this section we present the results by answering three research questions.
.
rq1 what is the most cost effective batchsize for the number of culprits discovered during testing?
batching commits for testing is more efficient with a low test failure rate i.e.culpritrate .
the higher the culpritrate the larger the number of bisections resulting in more executions.
in the extreme case where there are no test failures all commits could be placed in a single massive batch requiring a single passing execution and saving n executions where n is the total number of commits.
in practice the culpritrate tends to be very low.
for example on chrome .
of tests fail .
since the vast majority of tests do not fail testing all commits individually wastes resources.
theoretically the lower the culpritrate the higher the batchsize .
we calculated the culpritrate for each project and found .
.
and .
culprits for a b and c respectively.
with these variable culpritrates we simulate the savings relative to testing all commits individually testallcommits for batchsizes through .
figure shows the simulation results.
the savings are substantial even for the smallest batchsize commits.
the figure shows that this batch size requires and fewer executions for projects a b and c respectively.
we see that the savings are logarithmic with the majority of the savings occurring with batchsizes up to .
for project c with the lowest culpritrate we note that the savings plateau with batchsizes greater than providing little additional savings.
the maximum saving is and for the projects respectively.
these savings and batchsizes validate our conjecture.
project a and b have high culpritrates and see similar cost effective batchsizes and savings in executions.
project a and b have more than two times as many culprits as project c. project c has the highest batchsize and the greatest savings.
the higher the culpritrate the smaller the most costeffective batchsize .
for example project a has a culpritrate .
times higher than project c and with a batchsize of the savings are .
in contrast project c can have a batchsize up to and saves of test executions when compared with testing all commits.
figure improvement in test executions for different batchsizes .
in an ideal environment we see a logarithmic increase with most of the savings in executions being realized before batches of size four.
.
rq2 what is the most cost effective batchsize when some bisections are done as a result of flaky failures?
a flaky test failure is defined as a test that passes and fails on the same commit.
we define a flakybatch as a batch that initially fails but does not lead to an individual failing commit i.e.no culprits are identified.
flaky tests are a significant problem with google reporting that in tests are flaky and that newly failing tests are actually flaky failures .
when commits are tested individually a flaky failure does not affect other commits and the number of executions remains constant.
in contrast the larger the batchsize the higher the probability that at least one of the commits in the batch will be flaky.
any flaky batch failure incurs the penalty of an unnecessary bisection.
as the batchsize grows the probability of a flaky failure increases according to the models in figure .
the flakerate will limit the most cost effective batchsize .
in section .
we modeled the flakerate for batches of size to for each project.
in this section we adjust the simulation for the varying flakerate of our studied ericsson projects.
we found that the respective flakerates for projects a b and c are .
.
and .
.
figure shows the simulation results after correction for the flakerate of the projects for different batch sizes.
at batchsize we see a reduction in executions of and respectively for projects a b and c. we see that the savings in executions are logarithmic up to batchsize and respectively.
after batchsize we see a decrease in the savings with an increase in the number of executions as flaky failures become more frequent in larger batch sizes.
284bisecting commits and modeling commit risk during testing esec fse august tallinn estonia figure improvements in test executions considering the flakerates .
the flakerate controls the batchsize and the project with the highest flake rate does not see any advantage above batchsize .
project c still attains high execution savings at with a batchsize of .
the maximum saving is and for projects a b and c at batchsize for projects b and c and at batchsize for project a. these savings and flakerates validate our conjecture.
acknowledging flaky failures reduces the most cost effective batchsize.
project a has the highest flakerate and the smallest most cost effective batchsize of and lowest savings of .
project b has slightly lower flakerate and has more commits than project a its most cost effective batchsize is with savings of .
project c has fewer flakes than the other projects and has a most cost effective batchsize of with savings of .
without considering the flakerate project c had a batchsize of and a savings of .
however in figure we can see by a batchsize of project c already saves .
creating larger batches of commits leads to a higher probability that any one of them will be a flaky failure requiring additional wasted executions.
the trade off between savings and additional executions is most cost effective at a batchsize of for project c. clearly the flakerate must be taken into account when performing simulations to find the most cost effective batchsize for a software project.
the higher the flakerate the smaller the batchsize and smaller the savings in executions.
for example project b has a flakerate .
times higher than project c and abatchsize of saves of the executions compared to respectively.
with flaky failures project c s most cost effective batchsize and savings are reduced from a batchsize of and execution savings of to and respectively.
.
rq3 can risk models predict the culprit commit and reduce the number of executions to find the culprits on failing batches?
in this section we use bugmodels andtestexecutionhistory models to predict which commit in a batch is the true culprit.
our goal is to reduce the number of executions to find the culprit by testing highrisk commits in isolation.
we isolate the top k riskiest commits and test these individually while combining the remaining less risky commits in a single large batch.
in the background on bisection in section we use figure to illustrate how the riskiest commit top1 is tested in isolation while the remaining commits are tested in a batch.
however if the risk prediction is incorrect we would need a maximum of executions to find the culprit.
in contrast figure shows a bisection of a failing batch will always require executions to find a single culprit.
an accurate risk model will reduce the number of executions while an inaccurate model can even increase the number of executions to find culprits.
we evaluate the bugmodels andtestexecutionhistory models on two evaluation measures sufficientandcorrectatk and percentexecutiondifferencewithbisection.
sufficientandcorrectatk determines how many of the total culprits in each batch are correctly predicted in the topk suggested commits of the algorithm.
sufficientandcorrectatk numcorrectculpritpredictionsatk totalculprits for example a batch with two culprits using k 1has a maximum sufficientandcorrectat1 of 2or as no single prediction can find two culprits.
in contrast the maximum sufficientandcorrectat2 is 2or .
our ultimate goal is to reduce the number of total executions.
we calculate the difference in the number of executions for the risk models relative to the current process at ericsson.
percentexecutiondifferencewithbisection numexecutionsbisection numexecrisk a negative percent difference indicates a saving in executions when compared with bisection while a positive percentage indicates that the risk based approach does not outperform bisection and requires more executions.
.
.
batchsize for culprit prediction.
in the previous section we found that project a has an most cost effective batchsize of which means that when there is a test failure there will always be two executions regardless of commit risk i.e.both commits need to be tested individually.
we exclude project a from this analysis.
in contrast we found that the optimal batchsize for bisection for projects b and c is four commits.
we use batchsize 4to evaluate our risk based algorithms.
we also experimented with batchsize .
.
.16and found that size produced the best result.
we evaluate top1 and top2 only because with a batchsize testtop3 testtop4 and testall are equivalent requiring all commits to be tested individually.
for example if we test the top ranked commits in isolation the remaining batch has only one 285esec fse august tallinn estonia armin najafi peter c. rigby and weiyi shang commit so all commits are tested effectively in isolation which is equivalent to testall.
.
.
results for culprit risk prediction.
the results of our analysis are shown in table .
for testtop1 the top ranked commit will be tested in isolation while the remaining three commits will be tested as a batch.
if the prediction is incorrect we re run the process on the next highly ranked commit.
the bugmodel with testtop1 has sufficientandcorrectat1 of and for projects b and c respectively.
however it requires .
and .
more total executions than bisection for projects b and c. testexecutionhistory with testtop1 has a sufficientandcorrectat1 of and for projects b and c respectively.
for project b testexecutionhistory with testtop1 requires .
more executions.
however for project c we see fewer total executions are needed .
when compared to bisection .
when there is more than one culprit a model that only predicts one culprit i.e.testtop1 will not be able to find all culprits and will require additional executions.
project b has batches with two or more culprits of the time and clearly requires at least testtop2.
in contrast project c has two or more culprits only of the time.
the bugmodel s predictions are not accurate enough at testtop1 and require more executions than bisection due to these inaccurate predictions.
in contrast testexecutionhistory s top1 prediction is accurate enough to reduce the number of execution .
for project c. fortesttop2 the commits ranked and by the commit risk model are tested individually while the other commits are tested in a single batch.
the bugmodel with testtop2 has a sufficientandcorrectat2 of for both project b and c. the corresponding values fortestexecutionhistory are and .
testexecutionhistory with top2 is the most effective technique improving on the bugmodel by and percentage points for the projects respectively.
both commit risk models are more effective than bisection for project b and c with .
and .
executions for bugmodel and .
and .
executions for testexecutionhistory respectively.
the model accuracy at top2 are sufficient to reduce the number of executions when compared with bisection .
we combined the bugmodel and thetestexecutionhistory model but noted a reduction in accuracy and savings.
both culprit risk prediction models are effective but testexecutionhistory outperforms the bugmodel .testexecutionhistory is able to predict the culprits using the top2 predictions with a sufficientandcorrectat2 of and for projects b and c with batchsizes .
compared tobisection this results in .
and .
fewer executions respectively.
threats to validity in this section we discuss the threats to the validity of our findings.
.
external validity our study only considers three projects in the software development environment of ericsson.
although we believe that theseprojects can be good representatives of generic projects in industry our results may not generalize to other projects.
they also have varying size project a is the smallest and variations in culpritrate andflakerate project a has twice as many culprits as c and project b has 3more flaky failures.
a recent report from facebook shows that in practice testing approaches should start with the assumption that all tests are flaky.
since our methodology and simulation only requires the test outcomes on commits and can easily be applied to other projects to determine the most cost effective batchsize for a project replicating our study on other system may help further understand the generalizability of our findings.
.
construct validity our simulations include simplifications of some of the ericsson processes and may not exactly match the reality of the development environment of ericsson.
in order to verify our results we suggest practitioners implement our approaches in production workflows and evaluate the results in real environments after determining the most cost effective batchsize .
as explained in section .
we have experimented two training models a sliding window training model and also using all previous data.
our results show that the savings using a sliding window training model is slightly lower than using all previous data.
particularly for our best approaches i.e.testtop2 bugmodel and testexecutionhistory savings are .
and .
instead of .
and .
respectively for project b and .
and .
instead of .
and .
respectively for project c. hence the diagrams and distributions explained in this paper are based on using all previous data at each iteration day.
this parameter can be easily changed based on the results attained for other projects.
moreover our experiments show that some of our extracted features for creating bug models lead to deterioration of the results.
notably features regarding average developer experience number of file changes and average time interval among file changes have been excluded because of the reduced the quality of the culprit risk predictions.
.
internal validity finally we assume that a flakybatch will result in the same number of executions as is required to find a single culprit i.e.
lo 2 n .
however given that the test is flaky all tests may pass on the first split in the bisection.
in the case of batches of size finding a single culprit requires executions.
however if all commits pass after the first split there are only additional executions required.
our approach is conservative when treating bisections adding the number of executions required to find a culprit even if one does not exist i.e.a flaky batch.
related work in this section we discuss the prior research that is related to this paper.
.
batch testing and bisection there are two reasons why commits are grouped test efficiency and integration.
when resources are scarce e.g.
expensive specialized test hardware individual commits must be grouped together as 286bisecting commits and modeling commit risk during testing esec fse august tallinn estonia table we are able to reduce the number of executions relative to normal bisection by and .
for projects b and c. note since there can be multiple culprits in a batch testtop1 is often not sufficient to find all the culprits.
note project a is excluded because its most cost effective batchsize is so top1 top2 testall.
note we only display top1 and top2 because with a batchsize of top3 top4 testall.
t opk t opk project b sufficient and correct difference in executions sufficient and correct difference in executions bugmodel .
.
testfile .
.
project c sufficient and correct difference in executions sufficient and correct difference in executions bugmodel .
.
testfile .
.
there are not enough resources to test each commit individually.
while unit tests can determine that each individual commit is working we must test to ensure that when the changes are combined i.e.integrated there are no new faults.
regardless of the reason for a batch once it fails the commit or commits that are causing the problem must be identified and fixed i.e.the root cause or culprit must be found .
one of the common approaches used for finding a culprit in a group of failing commits is bisection.
when commits are ordered gitbisection can use an ordered binary search to identify the culprit in log n time.
at google integration tests can run on the order of hours and can cover thousands of commits making gitbisection too computationally expensive.
instead google developers use the static build dependencies to determine which tests must be run when a file is changed.
when a group of changes fails during integration testing google developers can immediately eliminate all changes that do not individually relate to the failing test.
since there can be thousands of changes in an integration test google also scores the remaining commits on the basis of the number of files in a change more files more likely to be the culprit and the distance to the root of the build test dependency dag closer to the root safer as more developers have assessed it by now .
at ericsson each commit under test should be independent of the other commits.
as a result gitbisection is not possible because the commits are combined into an unordered group of changes.
a bisection on an unordered set of commits is more expensive than an ordered gitbisection requiring lo 2 n 1executions see section .
furthermore at ericsson it is complicated to extract the static build dependency graph of which tests will be run.
in our work we evaluate the optimal batch size given the culpritrate and theflakerate .
we then guide bisection by using historical models instead of statistical dependencies.
.
test selection the goal of test selection is to choose the most appropriate tests to be run for a given change.
in our work we use these ideas to determine which change is the most likely culprit given the tests that have failed.
early work on test selection used static analysis and code coverage .
the google culprit finder uses similar information in the form of build test file dependencies to select the most likely culprit.in contrast our work builds on the history of test failures.
to prioritize tests previous works have used the recency of the test failures to determine which test is most likely to fail .
building on these ideas researchers have developed sliding windowed predictions used association rule mining and test co failure distributions .
these only consider the tests and do not consider the unit under test.
in contrast a preliminary work at ericsson determined which tests to run on the basis of which tests have failed with commits containing similar files .
we reverse this idea and instead of predicting which tests to run given the files in a change we determine which is the most likely culprit given the failing test and the files under change.
.
bug models recent works have extensively studied bug predictions and bug models.
the focus of earlier work has been on predicting defective software modules or evaluating the impact of different software metrics related to that .
however other studies focus on predicting defects on the change level.
predicting bugs on the change level makes it easier for developer address the issues and act on the predictions.
for example kim et al.
propose an approach for classifying the developer changes as buggy or clean.
they extract features like the lines modified in each change author and time of the change and complexity metrics from software revision history.
they train a support vector machine classifier to predict the changes as buggy or clean.
this approach also examines the risk associated with each submitted change without connecting them to a concrete fault localization context of a test failure.
our approach on the other hand does so using an empirical approach that points to the culprit change that is involved in a test failure.
kamei et al.
propose a risk analysis approach in change level.
they construct a logistic regression model for analyzing changes using different factors under six high level categories of diffusion size purpose history and developer experience to calculate the risk values.
the number of modified subsystems lines of code added the average time interval between the last and the current change and recent developer experience are among the utilized metrics.
we adopt this study as one of our risk prediction approaches.
yang et al.
propose a deep learning based technique for predicting the faulty changes.
they use an advanced deep learning 287esec fse august tallinn estonia armin najafi peter c. rigby and weiyi shang algorithm named deep belief network for extracting a set features for measuring the changes.
then they train a logistic regression classifier for predicting the risk values of the changes.
similar to this approach also just predicts the risks associated with different changes but does not associate them with any concrete test failure.
our approach however starts from a concrete test failure and attempts to locate the change associated with that test failure.
yang et al.
propose another similar study for predicting defective changes using a two layer ensemble learning approach.
young et al.
propose a replication of this study.
what all these studies have in common is predicting the buggy commits as early as possible i.e.a process called just in time defect prediction.
a problem with these bug models is that there is no concrete evidence that a suggested change is actually problematic and needs to be investigated as soon as possible.
our study however focuses on predicting culprit commits.
a culprit commit is one of the multiple changes that have actually failed a test and needs to be found and addressed right away.
recent study by ananthanarayanan et al.
aims to build prediction models for prevent a commit from breaking the build of software.
such prediction models may be adopted in our approach to further improve the our prediction of culprit commits.
conclusion the resources required for testing large scale modern software systems has grown dramatically.
each change must be tested and integrated.
to save resources commits are grouped into batches for testing.
this paper is the first work to study the most cost effective batchsize based on the number of true test failures culpritrate .
flaky tests are a known problem on all large systems we factor flakerate into our simulations.
the flakerate is more damaging with large batch sizes as the number of commits in a batch grows so does the probability of the batch failing due to a flaky test.
we also use risk prediction models to more quickly isolate commits that are the likely culprits using bugmodels andtestexecutionhistory models.
we provide a fundamental insight into the cost of batching and an actionable use for commit risk prediction models.
we make three major contributions we find the higher the culpritrate the smaller the most costeffective batchsize .
we see a logarithmic increase with most of the savings in executions being realized before batches of size four.
our results show that project c with the lowestculpritrate can optimally use batchsize 9and have savings above of executions over testing all commits individually.
we model the flakerate .
with moderate levels of flakiness the savings seen above a batchsize 4do not outweigh the additional executions required to identify a flaky failure.
the flakerate controls the batchsize and the project with the highest flake rate does not see any advantage above batchsize .
project c still attains high executions savings at with a batchsize of .
using risk predictions from bugmodels andtestexecutionhistory models we are able to rank the commits by how likelythey are to contain the culprit.
we find that the testexecutionhistory model achieves an average sufficientandcorrectat2 of .
and outperforms the bugmodel .
by using these risk predictions compared to bisection we need .
to .
fewer executions.
our work opens a new area of research into culprit finding and prediction.
while we have examined preliminary bugmodels and modified the work on test selection to identify potential culprits in the testexecutionhistory model we feel that there is much further work to be accomplished.
the results we present here have convinced ericsson developers to implement our culprit risk predictions in the culpred tool that will make their continuous integration pipeline more efficient.
acknowledgement we would like to thank ericsson for providing access to their system used in our study.
the findings and opinions expressed in this paper are those of the authors and do not necessarily represent or reflect those of ericsson and or its subsidiaries and affiliates.
moreover our results do not reflect the quality of ericsson s products.