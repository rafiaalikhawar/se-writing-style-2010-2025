size and accuracy in model inference nimrod busany shahar maoz yehonatan y ulazari tel aviv university tel aviv israel abstract many works infer finite state models from execution logs.
large models are more accurate but also more difficult to present and understand.
small models are easier to present and understand but are less accurate.
in this work we investigate the tradeoff between model size and accuracy in the context of the classic k tails model inference algorithm.
first we define mk tails a generalization of k tails from one to many parameters which enables fine grained control over the tradeoff.
second we extend mk tails with a reduction based on past equivalence which effectively reduces the size of the model without decreasing its accuracy.
we implemented our work and evaluated its performance and effectiveness on real world logs as well as on models and generated logs from the literature.
i. i ntroduction many works infer finite state models from execution logs.
the inferred models have different potential uses from program comprehension and malware detection to finding problems in service levels to list a few.
large models with many states and transitions are more accurate but also more difficult to present analyze and understand.
small models are easier to present analyze and understand but are less accurate.
in this work we investigate the tradeoff between model size and accuracy in the context of the classic k tails model inference algorithm.
k tails introduced in takes an execution log and a positive integer kas input and constructs a finite state machine fsm whose states correspond to unique sub sequences of length kor less from the log and which accepts a language that over approximates the set of traces in the log.
roughly the higher the parameter k the more accurate the produced model.
at the same time however the higher the parameter k the larger the produced model.
over the last two decades k tails was very popular and has been used in many variants by many different authors.
we investigate the tradeoff between model size and accuracy in two ways.
first we present mk tails a generalization of k tails from a single parameter kto a set of parameters k1 ... k t .
second we present an efficient reduction in the size of the inferred model that does not affect accuracy.
specifically first we observe that some elements in the logs e.g.
some events or sequences of events may be of more interest than others.
for example based on domainknowledge in security the engineer may know that some events or sequences of events are more sensitive than others and require more accurate inspection.
as another example in the context of checking that a bug was fixed the engineer may be more interested in events that were involved in the bug at hand than in other events that appear in the log.
however thek tails algorithm provides a single fixed level of abstraction over approximation for the entire log.
in other words the engineer cannot get a model that has less abstraction around some events and more around others her control over the tradeoff between abstraction and model size is very limited.
our new algorithm mk tails extends the k tails algorithm to accept a set of input parameters k1 k2 ...kt each of which applies to a subset of events in the log s alphabet as selected by the engineer.
accordingly mk tails produces a model whose level of abstraction varies it is more accurate around events whose corresponding kis high and is less accurate around events whose corresponding kis low.
this enables fine grained control over the tradeoff.
as we later show increasing the value ofkfor the entire alphabet as was done in all previous works may result in a major increase in the model size.
in contrast our extension mk tails allows the engineer to make the model accurate where necessary and less accurate where it is not necessary gaining more accuracy while paying less in model size and thus better control the tradeoff.
second we observe that the model produced by k tails or mk tails may include many redundant states and transitions the model is often larger than it needs to be.
thus based on merging of states with equivalent past we show a reduction in the model s states and transitions that preserves the model s language and thus reduces its size without reducing its accuracy.
importantly unlike existing reduction algorithms for nondeterministic finite automata our reduction is efficient as it takes advantage of the information in the logs and the unique properties of the k tails algorithm even before the model itself is constructed.
it is important to note that our present work does not make any new claims regarding the usefulness of the results of ktails for tasks for which it has been used in the past program comprehension test generation log differencing etc.
based on claims and evidence provided in previous works by others as we cite below we assume that k tails results could be useful for software engineers and focus solely on the challenges that are common to all applications of k tails enabling better control over the tradeoff between model size and accuracy.
it is also important to note two additional assumptions underlying our present work.
first in this work we assume that smaller finite state models are indeed easier to present and understand than larger finite state models.
evaluating this assumption with engineers is outside the scope of our work.
second the application of mk tails assumes that the engineer has domain knowledge or task related knowledge that leads her to care a lot about accuracy around some events and care 34th ieee acm international conference on automated software engineering ase .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
less about accuracy around other events.
providing criteria for selecting these events of interest is domain specific and task specific and is outside the scope of our present work.
we implemented our ideas and evaluated them over realworld logs as well as over models from the literature and logs that we generated from them.
the results show that mktails performs well and is able to produce models with high accuracy arround selected events while not paying much in model size.
further they show that our reduction scales well and effectively reduces model sizes by .
see sect.
vi.
over the last two decades k tails has been used in many variants in many works .
to the best of our knowledge no work has considered a generalization of k tails to many parameters.
in addition no work has investigated applying reductions over the k tails model while taking advantage of its unique structure and exploiting the data structures from which it is inferred.
see related work in sect.
vii b. ii.
e xample we use a small example to motivate and demonstrate our work presented semi formally for illustration purposes.
formal definitions appear later in the paper.
consider a very small log which consists of traces see fig.
.
an engineer wants to produce a finite state model that represents the behaviors that appear in the log.
ideally the model should be informative small and accurate.
for a start the engineer runs the classic k tails with k .
fig.
shows the resulting model which is a compact but a rather general one perhaps too general.
for example the model accepts traces that contain the sequence cd lf mkdr although this sequence is not part of any trace in the log.
the engineer may be happy with the size of the model but less happy with its spurious traces and the relatively high abstraction.
hence she increases the value of kto reduce the abstraction in the model.
fig.
shows the model produced by running k tails with k .
the new model no longer allows the spurious sequence cd lf mkdr .
indeed the abstraction in this model has decreased dramatically.
however the size of the new model when counting both states and transitions grew by .
from to making it more difficult to comprehend.
tr1 tr2 tr3 tr4 tr5 init init init init init conn conn conn conn conn i n i n i n i n i n s f c d c d l n l n l f l f l f r f a f mkdr sft sft del rn out out out af rf dis dis dis rn del clr rm clr out out dis reset dis d i s c l r fig.
a log of traces inspired by the cvs.net model from to address this the engineer uses our new algorithm mktails as based on her knowledge of the domain some events in the logs are more important than others she uses mk tails to assign different ks to different subsets of the log s alphabet.
specifically as an example she uses k for the event lf which based on her knowledge of the domain is more important than other events and k for all other events.
fig.
shows the output of mk tails in this case.
now the model size counting all states and transitions has only grown from to yet it excludes the spurious sequence above.
this demonstrates the ability of mk tails to deal with the tradeoff between model size and accuracy by fine grained control using different abstraction levels around different events.
finally the engineer applies our past equivalence reduction algorithm over the mk tails model.
this reduces the size of the model without changing the language it accepts and thus without affecting its accuracy and as we later show is done efficiently over mk tails models .
fig.
shows the result of applying the reduction to the model of fig.
.
note that states and from fig.
have been merged into state from fig.
.
further states and from fig.
have been merged into state from fig.
.
the model now has states and transitions a reduction of .
importantly this final model accepts the same language as the mk tails model over which it was applied.
iii.
p reliminaries we present basic definitions and background on k tails and nfa reductions required for the later parts of the paper.
a. basic definitions a trace over an alphabet is a finite word w angbracketleft 1 2 ... m angbracketright where 1 ... m .f o rj 1we use wjto denote the jth element in w.w eu s e w to denote the length of w. for a positive integer k let kdenote the set of all sequences of length kor less.
a loglover an alphabet is a set of traces l w1 ... w n .
we denote by l l ethe number of traces and events in the log resp.
example .
for the log in fig.
sft init cd conn ln in af lf del out dis sf rf rn rm mkdr clr reset l l e .
definition finite state machine fsm .a finite state machine is a structure m angbracketleftq i f angbracketright whereqis a set of states i qis a set of initial states f qis a set of terminal accepting states is an alphabet and is a transition relation q q. we use subscript notation to refer to the elements of an fsm modelm.
for example mrefers to the transition relation of m.f o rt q q prime ts t tedenoteq andq primeresp.
definition a run .a run over an fsm model m i sa finite sequence of transitions that starts on an initial state and maps to transitions in m angbracketleftt1 t2 ... tn angbracketright s.t.
t 1s authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
the result of running k tails with k on our example log.
the model has states and transitions and contains the spurious sequence cd lf mkdr red edges .
fig.
the result of running k tails with k on our example log.
the model has states and transitions.
fig.
the result of running mk tails with k forlf and with k for all the other events in the log.
the model has states and transitions.
the spurious sequence cd lf mkdr is excluded.
fig.
the result of applying past equivalence reduction over the mk tails model in fig.
.
the model has states and transitions.
green states correspond to past equivalent states from fig.
.
i i n t i m tie ti 1s.
each run defines a word w angbracketleftt1 t2 ...tn angbracketright.
the word wis inl m ifftne f. letmbe an fsm over an alphabet .w eu s e l m to denote the set of all words accepted by m. an fsm is deterministic iff every word has a single corresponding run and non deterministic otherwise.
we refer to a deterministic non deterministic fsm by the common acronyms dfa and nfa resp.
b. k tails k tails first introduced in is a classic model inference algorithm.
over the last two decades k tails has been presented in several variants and implemented in many works e.g.
.
k tails takes a log and a parameter kas input.
it starts by representing the log as an fsm mlincomposed of linear sub fsms one per trace which are joined by adding a singleinitial state qinit transitioning to the start of each trace via a unique label and a single terminal state qaccto which all traces transition to at the end via a unique label.
the language of mlinequals to the language of the log given that each trace is encapsulated by and events.
we refer to this version of the log as the encapsulated version denoted by len.
next k tails iteratively merges states in the mlinfsm two states are merged iff they are k equivalent i.e.
if their future of length kor less is identical.
the algorithm terminates and outputs the resulting fsm when no two remaining states are k equivalent.
more formally we define a function futurek qmlin 2 k mapping states in mlinto sets of k sequences consecutive sequences of kevents or less.
the k equivalence relation induces a partition of the states of the initial fsm mlininto equivalence classes e e1 e2 ... e m where each ei e is uniquely defined by its future sequences of length kor less.
two states q1 q2 eiifffuturek q1 futurek q2 .
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
when lifted from qmlintoe the function futurekbecomes the injective function id e p k .
for all q ei futurek q id ei .
definition k fsm .k fsm the fsm computed by ktails for a log land a positive integer k i sa nf s m ml angbracketleftq i f angbracketrightwhere q e whereeis the set of equivalence classes defined above is the alphabet of the log len e e a e a uniontext braceleftbig e prime q q prime mlins.t.q prime mlin q a q e q prime e prime bracerightbig i qinit is an artificial initial state and f qacc is an artificial terminal state.
among other properties the correctness of the k tails algorithm implies that the k fsm mmay over approximate but not under approximate the set of traces in the log l i.e.
every w lis accepted by ml l l ml .
additional useful properties of the k fsm are that all its states are reachable from the initial state qinit and that the accepting state qacc i s reachable from all states.
alg.
presents pseudo code for k tails implementation.
the input of alg.
is a linear representation of an encapsulated log lenand a natural number k and its output is a k fsm.
the main procedure calls alg.
a. the algorithm initializes an empty dictionary that maps future to futures.
then it iterates over each state of mlin computes its future and the future of its consecutive state and updates the dictionary.
afterwards the main procedure calls alg.
b which infers the model.
the algorithm starts by initializing an fsm model m. it then iterates over the future dictionary lines alg.
b .
the algorithm adds a state every time a new future is encountered and adds a transition between every pair of states with consecutive futures.
the label over the transition is the first event over the sequences that correspond to the equivalence class of the source state.
note that all event sequences in a future equivalence class start with the same event due to the nature of mlinover which the future equivalence is applied.
thus the procedure is unambiguous and well defined.
finally the algorithm unifies all states that are followed by the artificial initial and terminal symbols resp.
to a dedicated start and terminal states and returns the k fsm model.
complexity.
since a trace of a total of nevents is represented bynunique states in mlin and the artificial initial and terminal states constructing and storing mlinrequires o len e .
iterating over all states of mlinand computing the futures dictionary requires o len e k .
iterating over the futures dictionary requires also o m where m is the number of transitions in the k fsm.
since for any k m o len e the time complexity of the algorithm is o len e .
as for the space complexity storing mlin the model and the futures dictionary is bounded by o len e k .
c. nf a reductions for a given nfa the problem of finding its minimal language equivalent nfa is pspace complete .
therefore to reduce the size of an nfa while preserving its language algorithm ktails function algorithm ktails input m mlin int k output fsm set angbracketleftset angbracketleftstr angbracketright angbracketright futures dict kfuturemapping m lin k return infermodel futures dict function algorithm a kfuture mapping input fsm m lin int k output set angbracketleftstr angbracketright set angbracketleftset angbracketleftstr angbracketright angbracketright futures dict set angbracketleftstr angbracketright set angbracketleftset angbracketleftstr angbracketright angbracketright futures dict init for state q qmlindo state q prime getnxtstate q m lin set angbracketleftstr angbracketright future q computekfuture m lin k q set angbracketleftstr angbracketright future prime q computekfuture m lin k q prime futures dict .add future prime q return futures dict function algorithm b i nfer model input set angbracketleftstr angbracketright set angbracketleftset angbracketleftstr angbracketright angbracketright futures dict output fsm fsm m emptyfsm for set angbracketleftstr angbracketright ftrsrc futures dict.keys do for set angbracketleftstr angbracketright ftrtrg futures dict do addeqstate m ftr src addeqstate m ftr trg addtransition m ftr src ftrtrg setinitialandterminalstates m return m past research has focused on heuristics.
ilie and y u present an algorithm for nfa reduction using invariant equivalences.
an equivalence relation over qdefines a partition over q so the terms are used interchangeably.
formally a partition over qis denoted by b1 b2 ... b n where biis a block of states from q uniontext i nbi q and i j bi bi .
one may define a partial order between partitions.
let primedenote two partitions of q. then primeiff b b prime prime b b prime.
definition .
an equivalence relation overqis rightinvariant w.r .t.
miff q f f2 terminal and non terminal states are not equivalent q q prime q q q prime q prime q equivalent states lead to equivalent states on any letter ilie and y u show that merging states that are equivalent by any right invariant relation does not change the language of an nfa and that there exists a unique largest right invariant partition over the states of an nfa.
they present a polynomial time algorithm that computes the largest right invariant equivalence.
the algorithm starts from a partition that separates non terminal and terminal states.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
then it iteratively searches for none equivalent states w.r.t.
to def.
and refines the partition until reaching a fixed point.
finally equivalent states are merged to obtain a smaller nfa.
in a later work ilie et al.
show that the largest rightinvariant relation ris the coarsest stable refinement of the partition f q f w.r.t.
.
hence it can be computed using well known existing partition refinement algorithms by kanellakis and smolka .
importantly in our context the algorithm can be dually applied for left invariant equivalences which are symmetrically defined over the reversed automaton.
both reductions preserve the language of the nfa while empirically reducing it by .
our past equivalence reduction tailors the left invariant reduction for k tails.
iv .
c ontribution m any parameters we now present the first contribution of our work mk tails a generalization of k tails from a single parameter k to many parameters k1tokt.
based on domain knowledge or on the task at hand some events may be considered by the engineer as more important or more sensitive than others.
for example when analyzing android apps sensitive events may be api calls that access sensitive resources .
our generalized version allows the engineer to vary the strength of the abstraction around different subsets of the alphabet making it less accurate around some and more accurate around others.
a. defining k tails with many parameters the mk tails algorithm generalizes the original k tails.
as input it takes a log a set of distinct positive integer parameters k k1 ... k t and a corresponding partition of the alphabet into disjoint subsets s s1 ... s t .
the output of mk tails is a k fsm model that over approximates the log.
roughly the algorithm ensures that for every si s every event j siis associated with equivalence classes with a future length of at least ki.
to formalize we define the intersection of set of sequences seqs 2 kwith a set of eventss p using a boolean function as follows seqs s braceleftbiggtrue .
s seq seqs.
j.seq j false otherwise in other words the set sintersects with seqs iff one of the events in sappears in a sequence in seqs .
we now lift the k tails equivalence relation by replacing the single k future function futurek see sect.
iii with a generalized many kfunction.
definition gen future function .denote the maximal value ofkinkbykmax .
the function gen future qmlin k s 2 kmax maps states in mlintok sequences of length at most kmax where each state q qmlinis mapped to sequences f utureki q of length ki s.t.si futureki q true and j negationslash i ki kj sj futurekj q false .
intuitively when using the gen future function equivalence classes that are followed by sequences see def.
of idinalgorithm computegenfutures function compute genfutures m ksetsarr q input mlin m array angbracketleftk s angbracketright k sets arr sorted desc.
by k state q output futureq set angbracketleftstr angbracketright future of length ki k forki si ksetsarr do future ki q computekfuture m ki q ifsi future ki qthen return future ki q sect.
iii that include an event from si are based on k futures of at least ki.
similar to the original future function gen future induces a partition of the states of mlininto equivalence classes e e1 e2 ... e m where each of the equivalence classes in eis uniquely defined by its future sequences.
two states q1 q2 ei iffgen future q1 gen future q2 .
definition mk fsm .the mk fsm is a generalization of the classical k fsm which is obtained when replacing the classical k future function sect.
iii with the gen f uture function.
example .
consider the k fsm in fig.
where k s are defined as follows k1 s1 lf and k2 s2 s1 .
consider trace tr3 in fig.
and its corresponding branch in mlin q3 i q3 ... q3 t .
let us denote the state preceding the fourth event on this branch cd b yq3 .
clearly future1 q3 and future2 q3 .
to compute gen future q3 we compute s1 futurek1 q3 ands2 futurek2 q3 and take the maximal kamong the pairs ki si that return true.
since both return true we get that gen f uture q3 .
remark .
the mk fsm defined by mk tails m has similar properties to the classical k tails model k fsm.
specifically again for a log l everyw lis accepted by m i.e.
l l ml all the states of mare reachable from the initial stateqinit and the accepting state qacc is reachable from all states.
in particular when s andk k the output of mk tails is the same as the output of the original k tails.
in this sense mk tails is a conservative extension of k tails.
b. computing the mk fsm to compute the generalized k fsm mk fsm we use alg.
but replace the calls to computekfuture with the function computegenfutures shown in pseudocode in alg.
.
similar to computekfuture as input computegenfutures receives a model and a state q. however instead of a single k the function receives an array of pairs.
each pair angbracketleftki si angbracketrightconsists of an integer ki k1 ... k t and a corresponding si .
the different siare disjoint and their union equals formallyt uniontext i 1si .
the function assumes a descending order of the pairs on the first component.
this ensures that for each state q alg.
avoids redundant computations of futures with length less than the actual k required for q. authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
complexity.
denote the size of the pairs array by t.t o sort it the algorithm requires o tlog t .
this sorting is performed once.
then the only difference from alg.
are the calls to alg.
.
we denote by kmax the maximal k ink.
the algorithm makes at most tloop iterations in which it makes at most kmax steps computes k future by thecomputekfuture func.
and performs an intersection check.
the intersects si future ki q function requires ki si steps.
therefore the extended algorithm adds a factor that is bounded by t max k smax permlin state where max k andsmax denote the largest k and the largest subset of events resp.
therefore the overall complexity iso t max k smax len e tlog t .
since alg.
complexity is o len e k see sect.
iii we get that the increase in the complexity is only the multiplication by a constant k t max k smax i.e.
o len e k .
v. c ontribution k fsm r eduction we now present the second contribution of our work an effective and efficient size reduction technique that preserves the language of the k fsm model.
a. right and left invariant equivalence for k fsm to reduce the size of the k fsm one may consider using the right invariant equivalence see def.
and its symmetrical leftinvariant equivalence.
these may be useful since merging states by these relations is language preserving ilie and yu .
below we will show that in the context of k fsm the rightequivalence reduction has no effect.
then in contrast we will show that in the context of k fsm the left equivalence reduction is effective and can be efficiently computed.
theorem .
consider a k fsm m and a pair of states e e prime qm.
let rdenote a right invariant equivalence relation over q. then e negationslash re prime.
proof.
assume towards a contradiction two different states e ande primes.t.e re prime.
condition of def.
implies that any transition from qcan be matched by an equivalent transition fromq prime.
therefore by inductively applying r we get that for any sequence seqk angbracketlefte e1 ... e k angbracketrightthat belongs to id e there exists a sequence seq prime k angbracketlefte prime e prime ... e prime k angbracketrights.t.ei re prime i. since ris an equivalence relation the same argument holds from anyseq prime k id e prime .
hence eande primehave similar futures i.e id e id e prime e e prime which contradicts the assumptiona thateande primeare two different states.
corollary .
as an immediate corollary any k fsm model is minimal w.r .t.
any right invariant relation i.e.
no two states can be merged .
the implication of corollary is that using any rightinvariant relation for reducing a k fsm mis ineffective.
in contrast to the right invariant relations merging states by left invariant relations is a very effective method for reducing the size of a k fsm.
as an example consider a log with the following three traces t1 angbracketlefta b c angbracketright t2 angbracketlefta b d angbracketright t3 angbracketlefta b e angbracketright.
fig.
top shows the output of running k tails with fig.
a k fsm top obtained by running k tails over l angbracketlefta b c angbracketright angbracketlefta b d angbracketright angbracketlefta b e angbracketright with k and its left invariant reduction bottom k over the log.
as can be seen states are leftequivalent as they all have a single incoming transition from state1with the label a .
therefore we can merge them without changing the language of the model.
note that k tails splits those states due to the future divergence in states .
such redundant splits are common in k fsms.
after applying left equivalence once the succeeding states become left equivalent and can be merged which results in the model in fig.
bottom .
in its special application to k tails we call the left invariant equivalence past equivalence .
b. computing past equivalence reduction of a k fsm we now present an efficient algorithm for computing past equivalence reduction of k fsm.
most importantly our algorithm exploits the structure of the k fsm.
in contrast to the algorithm presented by ilie and y u which starts from a coarse partition and employs refinement operations until convergence our algorithm starts from a refined partition and employs an iterative coarsening operation until convergence.
while our procedure is general it converges on any k fsm after at most k 1iterations yielding an improved complexity over the state of the art.
alg.
presents the pseudo code for past equivalence reduction implementation.
the algorithms input is the futures dict constructed by alg.
a which maps future to futures.
recall that each future corresponds to a state in the k fsm and is represented by a set of strings of length kmax or less.
the algorithm uses the futures dict to construct line from which it computes the transitions that are incoming each state using the computeincomingtransitions function.
then the algorithm initializes an initial partition of the states where every state is mapped to a single state block with a unique block id lines .
the algorithm performs a series of block merging iterations alg.
lines until reaching a fixed point no more blocks can be merged .
at the beginning of each iteration the algorithm computes the pst eqv states dictionary which maps the sets of incoming transitions at the block level to states that succeed them lines .
to this end the algorithm iterates over the incm dict dictionary and computes the set of incoming transitions per state using the state2block dictionary.
then it calls addpastset which adds the state qto the past equivalence class of block intrns line .
importantly after iterating all states this dictionary holds pastinvariant equivalence classes w.r.t.
the current partition.
then authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
algorithm computepastequivalence function algorithm compute pastequiv alence input set angbracketleftstr angbracketright set angbracketleftset angbracketleftstr angbracketright angbracketright futures dict output state int state2block set angbracketlefttransition angbracketright getkfsmtransitions futures dict state set angbracketlefttransition angbracketright incm dict computeincomingtransitions int bid state int state2block for q incm dict.keys dostate2block b id while true do compute past equivalence by current partition set angbracketlefttransition angbracketright set angbracketleftstate angbracketright pst eqv states for state q incm dict.keys do set angbracketleft int angbracketright block intrns for transition t incm dict do int inc block id state2block block intrns.add inc block id t addpastset pst eqv states block intrns q check if new past equivalence found bool fixed point true for set angbracketlefttransition angbracketright pst eqv pst eqv states do set angbracketleftstate angbracketright states pst eqv states blocks for q states doblocks.add state2block if blocks 1then fixed point false coarsen partition bid for q states dostate2block b id iffixed point then return state2block the algorithm iterates over each past equivalence class and checks if it includes states that belong to more than a single block according to the current partition lines .
if so it merges such states by mapping them to a new block id and updates the state2block dictionary lines .
finally the algorithm reaches a fixed point when no new block is added and returns the last partition state2block .
the correctness of our algorithm is based on the fact that it finds the unique coarsest past invariant equivalence.
its complexity of o k is based on the fact that it reaches the fixed point after at most k 1iterations and in each iteration lines it iterates over twice.
example .
consider our example k fsm in fig.
which was generated by running k tails with k .
we focus on the states that are merged.
initially the algorithm computes the incoming transition dictionary incm dict and maps each state to a singleton block line .
let us denote the initial block of each state by the state id.
at the start of the first iteration the algorithm computes the incoming transitions at the block level lines and updates the past equivalence dictionary pst eqv states .
states all have a single incomingtransition from block b1with an a label.
therefore they are added to a single equivalence class line .
then the algorithm loops over the past equivalence sets in pst eqv states and maps states that are grouped together and are associated with different blocks to a new block id lines .
states which do not share a block are mapped to a new block b10 line .
since a new block was added the algorithm makes another iteration.
on the second iteration the algorithm recomputes the pastequivalence dictionary.
this time states all have a single incoming transition from b10with a b label.
therefore they are added to a single equivalence class and are later merged to a new block b11.
finally the algorithm makes one last iteration.
since no two other states are past equivalent the algorithm reaches a fixedpoint after 3iterations and returns the following mapping b0 b1 b10 b11 b4 b5.
we list three theorems by which we state the correctness and complexity of alg.
.
the theorems are stated w.r.t.
a k fsm but trivially hold for a mk fsm.
formal proofs are in supporting materials .
theorem .
algorithm terminates after at most k 1iterations on any k fsm with a maximal future of k. theorem .
let i i 1denote the partitions obtained by the iteration i iteration i 1of alg.
resp.
then i i see sect.
iii and i 1unifies any blocks in ithat are pastequivalent.
theorem .
algorithm terminates on the coarsest pastequivalence partition max where max is the partition such that i f is a past equivalence partition then max .
vi.
i mplementa tion valida tion and ev alua tion a. implementation and v alidation we implemented k tails mk tails and the past equivalence reduction in java.
the end to end implementation allows the engineer to set up a mapping between subsets of the alphabet and their corresponding ks and to apply k fsm past equivalence reduction.
it computes and visually presents a k fsm similar to the one in fig.
.
we made it available as a prototype web application for review and experiments.
we encourage the interested reader to check it out in supporting materials .
further to test the scalability of past equivalence reduction alg.
we compared it against kanellakis and smolka see sect.
iii c .
we followed the description of the algorithm and implemented it in java.
we validated our implementation as follows.
first we run small examples like the ones used in sect.
ii where we were able to manually inspect and validate the correctness of the output.
second we added tests for the following assertions each trace from the log is accepted by the model the past equivalence reduction does not change the language of the k fsm the past equivalence reduction yields identical authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
model to that of kanellakis and smolka .
we used brics a well known automaton java library to validate assertions and and compared the number of states and transitions in the reduced models to validate assertion .
the validations increased our confidence in the correctness of our ideas and implementation.
b. research questions we consider the following research questions rq1 can we efficiently compute mk tails?
rq2 can we reduce model size and increase conditional accuracy around sensitive events using mk tails?
rq3 can we efficiently compute past equivalence reduction over mk tails?
rq4 can past equivalence reduction over mk tails effectively reduce the size of the model?
c. logs setup and measures set1 .
we used finite state machine models in our evaluation all taken from the literature .
the models vary in size and complexity i.e.
the alphabet size ranges from to the number of states ranges from to and the number of transitions ranges from to .
from these models we generated logs using a publicly available trace generator from lo et al.
configured to provide state coverage of four visits per state and a minimum of traces.
these yielded logs of roughly traces each.
the complete list of models and their statistics and the generated logs are available in supporting materials .
set2 .
in addition we used six real world logs we have obtained from a large telecommunication company.
these logs have an alphabet size that ranges from to number of traces from to number of events from to and average trace length from .
to .
.
the logs anonymized are available in supporting materials .
to evaluate mk tails we selected a subset of events from the log.
we defined a random set of events to be sensitive events and assigned them with a higher kthan the rest of the events.
to measure the ability of mk tails to reduce noise around sensitive events we follow a similar procedure to the one suggested by lo and khoo .
first we define the notion of precision.
let us consider a log land the mk tails model minferred from it.
let us consider a sample of the traces from m and denote it by l prime.
we define the precision of the model as the fraction of traces from l prime that appear in the log l i.e.
p l prime l l prime .
since we focus on the sensitive events we extend the notion of precision to conditional precision which only accounts for traces containing at least one sensitive event.
more formally let sbe a set of sensitive events.
further let us denote by l prime s the set of traces from l primethat include events froms i.e.
l prime s t t l prime e ss.t.eappears in t .
then the conditional precision w.r.t.
sis defined as follows cp l prime s ls l prime s .remark .
since k tails models are an over approximation of the log their recall w.r .t.
the log equals .
hence we did not compute it.
remark .
we chose to compare precision w.r .t.
the log and not w.r .t.
the model as was done by lo and khoo .
we do so since our underlying assumption that larger ks yield more accurate models holds w.r .t.
the log and not w.r .t.
the model.
we do not make any claims about the generalization capabilities of k tails to learn new behaviors from the observed ones but merely focus on its ability to compactly capture behaviors that appear in the log without introducing much noise.
to measure the ability of our algorithm to reduce the model size we define the model size reduction.
let m1andm2 denote two models.
we denote by m the total number of states and transitions in m. then the size reduction of m1 w.r.t.m2is defined as m1 m2 .
finally we measure the running times.
in measuring the running time we include the time of parsing the logs and computing the models.
in rq1 and rq2 by mk tails we refer to alg.
combined with alg.
.
in rq3 and rq4 we refer to alg.
.
we executed all experiments on an ordinary laptop computer intel i7 cpu .6ghz 16gb ram with windows bit os java .
.
bit.
we executed all runs at least times to average out measurement noise from the java execution.
we report average and median running times.
d. experiments and results rq1 to answer rq1 we conducted the following experiment.
we run k tails and mk tails on the logs of set1 andset2 .
for k tails we used k .
for mk tails we randomly selected of the log alphabet as sensitive events.
for mk tails for the none sensitive events we used k and for the sensitive events we used k prime in three different runs .
we measured the running times.
in this experiment we did not include the log reading time which is common to both algorithms.
fig.
shows the average running times of k tails and mk tails for each of the logs set1 followed by set2 in milliseconds.
for each log the blue and orange columns depict k tails and mk tails alg.
and alg.
average running times resp.
in all the running time for mk tails was higher than the one for k tails.
overall mk tails adds an overhead that ranges from .
to .
with an average increase of .
finally in absolute terms fig.
shows that the total average running times of mk tails in all logs is below milliseconds.
this demonstrates the applicability of mk tails to different logs of realistic sizes.
to answer computing mk tails does not come for free however the overhead above k tails is limited.
running mk tails with of its alphabet defined as sensitive for a variety of different logs with thousands of traces and tens of thousands of events did not take more than a second.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
cvs.net datagram.
formatter multicast.
rapidminer socket ssh.net stringtok.
tcpip url log log log log log log 6avg.
running times ms log fig.
average running times ms of k tails blue mk tails orange fork k prime and10 of alphabet as sensitive events.
table i mk tails conditional precision and size reduction logk k k k cp cp size red.
cp size red.
cp size red.
rapidminer .
.
.
.
.
.
.
cvs .
.
.
.
.
.
.
datagramsock et0.
.
.
.
.
.
.
multicastsock et0.
.
.
.
.
.
.
sock et .
.
.
.
.
.
.
url .
.
.
.
.
.
.
formatter .
.
.
.
.
.
.
stringt okenizer .
.
.
.
.
.
.
ssh .
.
.
.
.
.
.
tcp ip .
.
.
.
.
.
.
l1 .
.
.
.
.
.
.
l2 .
.
.
.
.
.
.
l3 .
.
.
.
.
.
.
l4 .
.
.
.
.
.
.
l5 .
.
.
.
.
.
.
l6 .
.
.
.
.
.
.
rq2 to answer rq2 we compared the models produced with mk tails against models produced with a single kin terms of size and conditional precision cp by conducting the following experiment.
we run k tails setting k as a baseline.
then we randomly selected a single event from the alphabet of each log as sensitive event.
to measure the effectiveness of mk tails in improving the conditional precision we first computed the precision of the baseline model i.e.
k .
we run a random trace generator lo et al.
over it and produced traces that included the sensitive event.
then we measured its conditional precision table i second column from the left w.r.t.
the original log.
to measure the ability of mk tails to filter noise around sensitive events we measured its conditional precision w.r.t.
the log produced by the baseline model k table i columns from the left .
that is we only accounted for traces that were accepted by both the small k tails model and the more refined mk tails model.
to minimize the potential bias of the random choice of the sensitive event we repeated the selection of the sensitive event times per log.
as can be seen the conditional precision varies across models and is much greater in comparison to k .
further as expected it increases with k prime we measured an average cp of75.
.
.
fork prime resp.
table i also shows the size reduction in comparison to running k tails with an increased k prime.
that is we compared thetable ii median running times in milliseconds of alg.
and kanellakis and smolka when running with k over log sets set1 andset2 .
k alg.
.
.
.
.
.
kanellakis and smolka .
.
.
.
.
size of mk tails models with k k prime to that of k tails models withk prime.
for example the value .
in the row for model rapidminer column k prime means that m1 m2 .
wherem1is the model inferred using mk tails with k andk prime andm2is the model inferred using k tails withk .
as can be seen mk tails is able to dramatically reduce the size of the model across different logs.
further the reduction gains increase with k prime with a median of .
.
.
fork prime resp.
to answer we have evidence that mk tails can yield significant reduction in the model size while increasing the conditional precision.
rq3 to answer rq3 we compared the running time of the general nfa reduction algorithm kanellakis and smolka with the past equivalence reduction alg.
fork over log sets set1 andset2 .
table ii reports the median absolute running times of applying past equivalence reduction with alg.
and kanellakis and smolka in milliseconds as function of kacross the logs.
as can be seen alg.
requires an order of magnitude less time than the competing algorithm.
figure shows a boxplot of the running time as function ofkacross different logs where boxes labeled by pm k and sm k correspond to the application of alg.
and kanellakis and smolka over k fsms resp.
note that the boxplot does not include outliers due to the long running times of the socket logs when running with kanellakis and smolka .
the boxplot shows that the running times of kanellakis and smolka are higher than those of alg.
for all logs and all values of k. to explain the large difference in running times we investigated the number of iterations performed by each of the algorithms.
since both algorithms move between partitions via iterations that require o until reaching a fixed point this factor is key in analyzing the complexity of the algorithms.
by theorem our algorithm converges in at most k iterations.
table iii shows the average number of iterations until kanellakis and smolka reaches a fixed point for different values of kacross the logs.
as can be seen the number of iterations in kanellakis and smolka rapidly increases with k in particular when moving between small values of k. this explains the superior running times of alg.
.
to answer we have evidence that alg.
scales well and improves over the state of the art in terms of computation time.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii average number of iterations made by kanellakis and smolka when running with k over log sets set1 andset2 .
k iterations .
.
.
.
.
fig.
average running times ms log scale of alg.
pm and kanellakis and smolka sm as function of k when run over set1 andset2 .
rq4 to answer rq4 we report the model size before and after applying past equivalence reduction.
we run the reduction alg.
over models produced with k o v e r log sets set1 andset2 .
table iv shows the median size of the k tails models the median size of the reduced k tails models and the median size reduction per model for different values of k. as can be seen the algorithm effectively reduces the size of the models across all kvalues.
the median size reduction reduces with k but is above across all values of k. further the measured size reductions are consistent with the ones reported by ilie and y u and are in the range .
to answer we have evidence that alg.
effectively reduces the size of k tails models.
the algorithm achieves significant size reduction across different logs and for different values of k. threats to validity.
we consider the following threats to the validity of our results.
first our implementation may have bugs.
to mitigate this we validated our implementation extensively see sect.
vi a .
second the logs used may not be representative of real world logs.
to address this we used a sample of real world logs from a large telecommunication company and synthetic logs we generated from real world models borrowed from prior works with high variability in number of events trace length etc.
see sect.
vi c. vii.
d iscussion and related work we discuss alternatives to our solutions as well as limitations and implications of our work.
we then continue to discuss related work.table iv size reduction over log sets set1 andset2 .
k k tails model size median .
.
.
.
.
reduced k tails model size median .
.
.
.
.
size reduction per model median .
.
.
.
.
a. discussion alternatives to the many parameters of mk tails.
one may suggest other methods to focus on events of interest.
first for example filtering on the log e.g.
ignoring infrequent events or traces or ignoring events that are considered uninteresting .
second slicing on the finite state machine model .
we view these methods as complementary to mk tails.
mktails allows fine grained control over the accuracy around the selected more sensitive events while not giving up the context that is often lost when using filtering or slicing techniques.
alternative reduction algorithm.
ilie et al.
note that one may use the well known paige tarjan pt algorithm to apply the past equivalence reduction.
this algorithm has time complexity of o log q which is better than the o q of kanellakis and smolka .
both algorithms however use the same approach of starting from a coarse partition and apply a series of split iterations each with a cost ofo .
in contrast our algorithm uses an opposite approach.
it starts from a refined partition and importantly applies only at mostk 1state merging iterations each with the same cost ofo .
the large number of iterations made by kanellakis and smolka see table iii indicates that reducing the number of iterations to a constant rather than to a log of the number of states is expected to be competitive.
we leave a direct comparison with pt to future work.
is precision monotonic with k?one may expect that increasing the value of kwould increase the precision of the inferred model relative to the log.
however this is not necessarily the case.
nevertheless for every log there is a large enough ksuch that precision equals .
this applies to k tails as well.
as for conditional precision when one only increases k prime a value of 1may never be obtained due to possible imprecision entailed by other non sensitive events.
under which conditions does mk tails provide size reduction?
mk tails is a heuristics.
specifically assigning a higher value ofkto a small subset of the events rather than to all of them will never increase the model size but does not guarantee to reduce it.
for example when the sensitive events for which we assign the higher kimmediately succeed all non sensitive events in one or more of the traces.
on the other hand when the sensitive events appear in relative isolation as part of a small number of unique k sequences we will obtain a significant reducution in the model size.
in practice as our experiments demonstrate mk tails is able to dramatically reduce the size of the model across different logs see rq2 in sect.
vi.
implications to anyone who uses k tails.
mk tails allows engineers to control the detailedness of the model in different authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
areas based on domain knowledge or the specific task at hand.
instead of a single level of abstraction increasing kimproves model accuracy around selected events while reducing the model size in comparison to a single global k. although mktails allows much flexibility in choosing different ks to different subsets of the alphabet in practice choosing these ks may not be easy.
we believe that in practice one would be satisfied with defining only a simple binary partition between interesting and less interesting events and assigning a higher value of kto the former and a lower value of kto the latter.
b. related work we discuss model inference works that deal with the notion of model size and accuracy.
some authors have used precision recall and f measure to quantitatively evaluate the quality of the inferred models .
lo and khoo have also used co emission and ps.
in all of these the inferred models are compared against the ground truth models from which the logs were produced.
the comparison is done by sampling traces from both models and performing acceptance testing.
thus all these require ground truth models which are available in experiments but not available in real world setup.
in our evaluation of mk tails we followed a different approach to evaluate the accuracy of the generated models.
we compared the inferred model against the log not against the ground truth model from which the log was generated see rem.
.
we demonstrated how increasing the value ofkincreases accuracy but comes with the price of larger models.
we have also defined and used conditional precision specifically in order to show that mk tails is able to increase conditional precision around events of interest while paying a rather low cost in additional model size.
many works have used implemented or extended k tails e.g.
.
we cite them here as evidence for the popularity of k tails in the literature which motivated our choice of this algorithm as a baseline for our work on size vs. accuracy.
to our knowledge none of these works has considered a generalization from one to many parameters and thus all are limited to the single level of abstraction defined by the choice of k. in addition none has applied post processing size reductions.
our size reduction is different than the general nfa reduction as its better theoretical complexity and better empirical performance relies on the specific context of the k tails algorithm.
viii.
c onclusion and future research to deal with the tradeoff between size and accuracy in model inference we presented mk tails a generalization of k tails from single to many parameters which enables fine grained control over the abstraction on different subsets of the event alphabet.
we have further presented an efficient algorithm based on past equivalence to reduce the size of the resulting model without affecting its accuracy.
we implemented our ideas in a prototype web based application which we have made available for experiments.our evaluation over logs generated from models from the literature and additional logs provided to us by our industrial partner shows that mk tails can be computed efficiently with only little overhead above the classical k tails.
moreover it shows that the use of mk tails can dramatically reduce model size while maintaining high conditional accuracy for events of interest.
finally it shows that our past equivalence reduction when applied to the models generated by mk tails is efficient and effective in reducing model size.
the important implications for any work that uses k tails is to consider finer control over the abstraction by assigning different values of kto different subsets of the alphabet based on domain knowledge.
we consider the following future research directions.
first to improve the practicality and applicability of our work it may be interesting to look at semi automated means to translate domain knowledge or data on the task at hand into the selection of the subsets of events to consider sensitive .
second once the subsets of events are given how should the values of the ks be selected?
one may suggest to automate the choice of different ks based on target conditional precision given by the engineer.
note that this may indeed be possible since in our framework computing precision does not require a ground truth model.
third one could extend our own statistical approach from k tails to mk tails.
following this work it may be useful to strengthen accuracy computations with statistical guarantees.
intuitively when sampling from a large log we may want to stop sampling when we have enough confidence that the estimated accuracy we have obtained is close enough to the accuracy of the model that one would have inferred from the complete log.
fourth note that mk tails like the classic k tails deals with the tradeoff between size and accuracy in a way that abstracts away the frequencies of the different traces or k sequences in the log.
in some domains and for some applications however these frequencies are important and should be represented in the inferred model.
there it would be necessary to extend mk tails and the notion of accuracy to account for frequencies.
finally we consider an interactive application inspired by mk tails where the engineer can dynamically increase and decrease the local value of kon selected states or events.
this will result in a dynamic details on demand approach to model inference.
we leave all these for future work.