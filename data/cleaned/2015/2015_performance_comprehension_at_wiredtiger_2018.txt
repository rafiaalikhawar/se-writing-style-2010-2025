performance comprehension at wiredtiger alexandra fedorova university of british columbia canadacraig mustard university of british columbia canadaivan beschastnikh university of british columbia canadajulia rubin university of british columbia canada augustine wong university of british columbia canadasvetozar miucin university of british columbia canadalouis ye university of british columbia canada abstract software debugging is a time consuming and challenging process.
supporting debugging has been a focus of the software engineering field since its inception with numerous empirical studies theories and tools to support developers in this task.
performance bugs and performance debugging is a sub genre of debugging that has received less attention.
in this paper we contribute an empirical case study of performance bug diagnosis in the wiredtiger project the default database engine behind mongodb.
we perform an in depth analysis of jira tickets documenting wiredtiger performance related issues.
we investigate how developers diagnose performance bugs what information they collect what tools they use and what processes they follow.
our findings show that developers spend the majority of their performance debugging time chasing outlier events such as latency spikes and throughput drops.
yet they are not properly supported by existing performance debugging tools in this task.
we also observe that developers often use tools without knowing in advance whether the obtained information will be relevant to debugging the problem.
therefore we believe developers can benefit from tools that can be used for unstructured exploration of performance data rather than for answering specific questions.
ccs concepts software and its engineering software performance software development methods programming teams keywords software performance comprehension performance debugging acm reference format alexandra fedorova craig mustard ivan beschastnikh julia rubin augustine wong svetozar miucin and louis ye.
.
performance comprehension at wiredtiger.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
introduction software is becoming increasingly complex and spans mobile devices data centers diverse architectures multi core gpus etc.
in this world performance is an important software engineering consideration .
in particular many software products are routinely compared rated and bought or not bought based on their performance.
this includes databases key value stores distributed computing systems machine learning frameworks and more.
achieving high performance requires performance debugging identifying bottlenecks and mitigating them.
however without understanding the underlying cause without proper information and experiments to reproduce the bug developers are at a loss of how to fix a performance bug.
this process which we term performance comprehension has not been previously detailed.
existing work on performance debugging develops new tools to complement mainstream commercial and open source performance debugging tools such as perf.
instead this paper asks what information do developers collect?
do they formulate hypothesis or are they driven by the tool at hand?
and how do they collaborate to understand system performance?
in other words we are interested in answering the broad question of how do developers approach performance comprehension?
we explore how developers diagnose performance issues with a case study of wiredtiger wt an open source high performance storage engine written in c. as of wt is part of mongodb2 a popular nosql database.
wt developers collaborate in a decentralized manner and use jira for tracking and for thoroughly discussing and resolving all performance issues.
in our study we analyzed performance related tickets in wt s jira repository.
we focused on the information that developers do and do not collect tools they use and do not use and processes they follow.
we also considered the relationship between information and tools do information needs drive tool usage or do developers prefer and use certain tools first and then sift through the resulting information?
here are a few of our findings from the study finding .
wt developers spend most of their performance debugging time chasing latency spikes and throughput drops.
despite being outliers these events represent worst case system behaviour and are thus of critical importance for performance oriented groups such as wt.
finding .
when looking for root causes of latency spikes developers consider correlated behavior between threads and events that happen over time.
however developers are not properly supported by existing performance debugging comprehension tools.
november lake buena vista fl usa fedorova et al.
most program profilers aggregate samples and cannot identify occasional latency spikes.
the wt developers thus rely on manually correlated log messages to diagnose tail latencies.
finding .
developers typically do not formulate initial hypotheses about the root cause of a performance issue.
we found that developers did not know in advance if the performance data they plan to collect from running a tool will be relevant to debugging the problem.
they often used the tools in an exploratory fashion.
study design for our case study we extracted performance related tickets from wt s jira repository and analyzed them to answer three research questions rq1 what information do developers collect during performance comprehension?
rq2 what tools do developers use during performance comprehension?
rq3 what processes do developers follow during performance comprehension?
next we give the necessary background on the wiredtiger project and its software practices.
we then describe our data collection analysis and validation techniques and outline threats to the validity of our study.
.
wiredtiger wiredtiger wt is a multi threaded open source storage engine designed for high performance and scalability.
started in it has a team of around developers3 with the core six developers contributing over commits each.
at the time of writing the most active developer contributed over commits and the most active six developers contributed over commits in total.
three of these developers have years of experience one has around years of experience and the remaining two developers have around years of experience.
wt is comprised of over lines of c code4and runs on posix compatible systems including linux and freebsd as well as windows.
wt clients include several popular web service providers such as amazon web services.
in wt was acquired by mongodb the provider of one of the most popular open source document oriented nosql databases.
wt achieves a 10x performance increase over mongodb s previous storage engine .
why study wt.
we use wt to study performance comprehension because the main goal of wt is to develop a high performance storage engine .
wt is carefully designed to efficiently use all available cpu cores large amounts of ram and advanced synchronization techniques to enable more efficient parallelism.
developers thus spend a significant amount of effort understanding and resolving performance related issues.
moreover wt presents a unique opportunity for our study because its open development makes it possible to investigate performance comprehension practices of a successful performance oriented software team.
3reported by github s contributors list graphs contributors 4calculated using cloc on .c .cc .h and .i files in the src and api directories of the wt repository.development process.
wt developers work remotely from around the world and communicate with each other online.
at the time of writing there is a significant number of commits from seven distinct time zones.
developers communicate primarily through jira tickets for issue tracking over saying that they used jira as a notebook for debugging sessions .
other less significant communication channels are github pull requests for code reviews and merges over commits and over pull requests and mailing lists.
yet the project conventions are to maintain all conversations about issues in jira as opposed to other means such as email.
the wt team does not use irc or slack.
wt developers use the jenkins continuous integration ci system to monitor the health and performance of the latest product version.
for each new commit jenkins automatically runs tests to evaluate the current quality and performance of the system.
jira performance issues are created following failures listed in jenkins reports directly by customers or by developers who encountered performance related issues while working on the project.
.
data collection for wt jira is a sufficiently complete source of performance debugging activity which motivated us to perform our study with this data source.
to collect performance related tickets for our analysis we navigated to the mongodb jira issue tracker repository5and selected project wiredtiger .
we searched for tickets that satisfy the following criteria tickets of any standard type i.e.
bug documentation improvement new feature task technical debt or workload.
tickets in any state i.e.
open in progress in code review resolved closed waiting for user input needs reviewer or needs merge.
tickets containing the keyword performance in either the title or the body of the ticket.
we sampled the repository twice first in august and then in february .
both times we analyzed the tickets one by one using the method described in section .
.
we stopped when we reached conceptual saturation no new information was obtained from analyzing further data.
as the result of this process we analyzed tickets updated between april and december in the first round and in the second .
the analyzed tickets contained between and comments made by developers median .
mean .
with to distinct developers contributing to each ticket median mean .
initial issue descriptions were between and words median .
mean .
.
developer comments were between and words median .
mean .
.
in total developers contributed to the majority of tickets with each of the top four developers contributing to more than issues.
.
data analysis we used open coding a technique from grounded theory for deriving theoretical constructs from qualitative analysis .
two authors of this paper independently read each selected jira ticket and coded ideas contained in the data.
we focused on information comprehension at wiredtiger esec fse november lake buena vista fl usa tools and processes that developers use to diagnose performance issues.
we did not code the description of the problem or its resolution.
on a weekly basis all authors of the paper met to discuss quotes identified by the two coders review their grounding in jira tickets merge and refine codes and concepts as necessary identify emerging categories and also refine coding rules and processes.
following strauss s version of grounded theory where data sampling and data analysis are seen as interleaved steps that are repeated until one can describe the researched phenomenon we continued coding the tickets until all authors agreed that new data does not change our understanding of performance diagnosis process.
at this stage we coded tickets originally sampled in august .
to further validate our results and ensure a stable coding process and set of concepts one author of the paper sampled the repository one more time in february and coded another eight tickets.
the complete set of codes is available online6.
as a result of this process we analyzed tickets.
we marked between and quotes per ticket median mean .
identifying codes related to information needs codes related to tools and codes related to process for information and tools inter relationships for reproducibility and for experiments .
the four most senior developers were responsible for comments corresponding to .
of the codes.
these are discussed in detail in sections .
.
member check to validate our findings we shared an earlier version of this paper with the wiredtiger developers whose comments we studied.
three developers responded together the respondents account for of all comments written across of the tickets we studied.
two of the three developers said they read the entire paper.
comments on findings.
we asked if the developers agreed with our findings.
the respondents found our findings accurate they did not dispute the findings in any of the sections offering feedback on typos and grammatical fixes.
i m frankly surprised how accurately you managed to capture the in the paper it did an excellent job of defining the categories.
completeness of studying jira tickets.
we asked developers whether their comments on jira tickets accurately reflect the challenges they faced and the tasks they performed in diagnosing and fixing performance issues.
two of the three developers responded in the affirmative the other developer did not comment on this question .
personally i comment in tickets for a few purposes.
some of it is so that we have a history and can revisit why did we do that?
at a future time after we ve or i ve forgotten the details.
some is so that others can reproduce what i m and see or not the same results.
some is to validate or simply communicate how i m attacking a problem.
.
threats to validity internal validity.
we might have misinterpreted developers intentions or misidentified concepts introducing researcher bias into analysis.
we attempted to mitigate this threat by performing independent coding and cross validating each code identified in our analysis by at least two authors.
we also discussed and validated the results during weekly meetings with all authors of this paper.
moreover one member of our team worked as a consultant for wt and is thoroughly familiar with the wt development processes.
s he confirmed that our understanding of the debugging traces in the tickets we studied are representative of performance debugging sessions.
in addition to the person who consulted for wt our team had other members who developed database software for other companies so we were well positioned to understand the technical details in the tickets.
we thus believe our analysis is reliable.
in organizations where developers communicate through a variety of different channels especially in person bug reports or issue trackers provide incomplete views of a bug s history .
however for wt the jira database is the main communication channel we are thus confident that jira is a sufficiently complete source of performance related debugging information for our study.
external validity.
as in other software engineering case studies our research might not generalize beyond our subject of study the wt system and the sample of tickets we consider.
it is possible that the practices used by other companies and teams may differ from those used at wt.
our tickets sample includes patterns and practices followed by experienced software developers giving us confidence that our study captures at least some of the best practices for performance diagnosis.
rq1 information used the information developers collect during performance comprehension rq1 is captured by codes extracted from jira tickets.
table shows codes that appeared in at least of the tickets together with their descriptions the number of tickets containing each code and the total number of occurrences of each code across all documents.
we categorized the codes along three dimensions temporal view execution view and system stack location as shown in figure .
the next section describes these categories followed by a detailed description of findings derived from the categorization.
.
categorizing information codes temporal view tells us whether the information about program execution typically metrics like throughput latency cpu time breakdown by function were used as aggregates e.g.
averaged or summed up across an execution or in the form of time series.
an example of time series presentation is where a metric of interest e.g.
the latency of a function is presented as a set of measures so that the developer has a chance to see how the metric changes during the execution and correlates with other metrics or events.
execution view distinguishes between information aggregated across all execution contexts or threads vs. when it is presented for individual threads.
for example latency of a critical function could be averaged across all threads or could be examined separately for each thread or thread type allowing the developer to learn whether some threads ran more slowly than others.
system stack location describes the origin of the information is the metric related to the behavior of the hardware such as cpu utilization or disk throughput?
does the information originateesec fse november lake buena vista fl usa fedorova et al.
temporal viewaggregated across execu.ontime seriesexecu1on viewaggregated across all threads processesper threadsystem stack loca1onhardwareopera.ng systemcompiler librariesapplica.on figure dimensions of classification for information codes.
from measuring events in the operating system?
does it come from measurements related to the compiler or libraries?
or does it originate solely from measuring the events in the application such as the duration of functions or properties of data structures?
codes can be labeled along each dimension a sequence of labels produces a category for a given code.
for example a code that describes information in the form of time series that is presented for each individual thread would fall into time series per thread category.
table organizes our codes into categories using the first two dimensions temporal and execution views .
in the third dimension system stack location the application label dominates so we do not further split the categories along this dimension codes in the table correspond to application unless noted otherwise .
only codes that appeared in at least of the tickets are listed in the table.
most of the codes are deterministically labeled along each dimension with two exceptions at the bottom of table where we did not have sufficient data to uniquely label the codes.
for example the code info code behavior which indicates that the developer referred to the application source code in order to reason about a performance anomaly was difficult to label as either per thread orall threads along the execution view dimension source code is static and it is not always clear if a particular code sequence would be executed by all running threads or by a certain subset.
.
time series per thread category developers need to understand thread activity over time.
time series per thread is the most prominent category including out of most frequently occurring codes.
that is developers mostly looked for information across time and across threads as opposed to the aggregated form.
this is despite the fact that aggregated information is much easier to obtain via tools like cpu profilers as we will see in the next section getting time series per thread information required rudimentary and manual methods such as manual code instrumentation or stopping the debugger at just the right time to observe specific events.
we believe that the need for time series per thread information is not unique to developers at wiredtiger.
google s and ibm s inhouse performance tools display the execution trace as time series broken down by execution context .
the most frequently occurring code is info functions or activityover time by threads .
it represents a situation where a developer learned what threads were during different periods in the execution e.g.
what functions or groups of functions they wereexecuting when and for how long.
in one example the developer wrote there are two checkpoints the first starts at about total secs and it finishes quickly before the next second stat report.
the next checkpoint starts at about total secs and continues for the rest of the test.
in this case a checkpoint is identified by a group of functions and is executed by a specially designated checkpoint thread.
the developer observed when the checkpoint started and ended.
in another case a developer wanted to learn which wt functions the thread is executing if any and when running pmp i almost never catch any thread in wt during the thread run.
with threads i always see out of yielding in wt page in func .
another group of codes dominating the time series per thread category relates to threads being blocked delayed active or inactive.
information about blocked or delayed threads was especially difficult to piece together and required developers to analyze series of callstacks obtained via a debugger or by sifting through log files.
below are the codes in this category and the example quotes describing them info threads blocked delayed due to certain functions or activity .
.
.
the vast majority of the time is spent in application threads in cond wait waiting for cache evict current to have something.
this means app threads are stuck in the loop in wt cache eviction worker .
info threads waiting over time there are often periods where most threads are waiting on cache for a full second.
info threads being active or inactive affects performance this drop is due to the async threads that are created for the compact.
the rest of the test is not async only the compact operation is done using async.
therefore the async threads spin there looking for work.
latency spikes and throughput variations are a key concern.
much of performance debugging is centered around latency spikes and intermittent drops of throughput.
these are rare events that occur when the duration of certain functions or operations or the rate of their completion falls far below average.
info latency spikes i got over instances of slow operations on and with maximum reported latencies over 1s.
in running the evict btree stress multi.wtperf workload i see a number of multi second latencies.
info intermittent throughput drop the load versus read phases are very distinct and the load phase shows a drop in performance taking longer to populate.
this observation is again not endemic to wiredtiger.
google s dean and barroso wrote about how these rare but important events can wreak havoc in large systems .
latency spikes and drops of throughput can only be observed when the performance metrics are presented as time series any aggregated metric such as overallperformance comprehension at wiredtiger esec fse november lake buena vista fl usa category and code code description tickets total time series per thread info functions or activity overtime by threadsdeveloper obtains the functions or groups of functions i.e.
activity that were executed over time by different threads21 info threads blocked delayed dueto certain functions or activitydeveloper finds when and for how long threads are blocked or delayed when executing certain functions or performing certain activity14 info latency spikes developer observes unusually long latencies seen in the program.
info intermittent throughput drop developer observes that throughput occasionally drops during execution.
info threads waiting over time developer finds that threads are waiting on something e.g.
a lock or another condition during particular portions of the execution.
info intermittent throughputdrop tied to certain functions oroperationsdeveloper finds that throughput drops occur when a certain operation e.g.
checkpoint cache eviction is happening or when a certain function is executing.
info latency spikes tied to certainfunctions or operationsdeveloper observed that a latency spike was observed at the same time as a certain function or group of functions was executing.
info threads being active orinactive affects performancedeveloper observes that performance varies depending on which threads were running or not during a particular time in the execution.
time series all threads info correlation of data structurestate with execution progressdeveloper observes that the state of a data structure correlates with how quickly the code runs.
info data structure state over time developer observes the data structure state e.g.
its size or the presence of specific elements during various periods in the execution.
aggregated all threads info where time is spent within afunctiondeveloper observed where time is spent in a certain function.
info use of synchronizationprimitives affects performancedeveloper observes that using synchronization primitives e.g.
fine vs. coarse or using one synchronization primitive over the other or using synchronization primitives too often affects performance.
info function cpu hotspot os lib app developer obtained a cpu profile of the running program.
aggregated per thread info work accomplished bydifferent threadsdeveloper compared different threads or different thread types in terms of how much work they accomplished.
work is a loosely defined as anything that had to be done in the program.
aggregated per thread all threads info code behavior developer relied on their knowledge of the application code or specifically examined the code.
time series aggregated per thread all threads info performance tied to kernelactivity or state os developer learned whether the performance correlates with some activity in the kernel e.g.
paging or a particular kernel state.
info correlation of performanceand io os developer learned whether performance either aggregate or during specific times in the execution correlates with reading or writing files.
table information codes that occurred in at least of the issues.
codes relate to the application level of the stack unless indicated otherwise.
issues refers to number of issues with code while total records the total number of code occurrences.esec fse november lake buena vista fl usa fedorova et al.
throughput or average latency will conceal them.
profilers do not sample at high enough resolution to detect rare latency spikes so developers at wiredtiger google and ibm alike built their own tools to observe latency spikes in per thread or per process timelines.
we were surprised that there was no off the shelf tool that the wt developers found suitable for their needs.
the remaining codes in this category describe developers trying to understand what caused those latency spikes or throughput drops to occur.
info intermittent throughput drop tied to certain functions or operations .
.
.
what i see in a typical run is that performance drops when a checkpoint runs.
info latency spikes tied to certain functions or operations .
.
.
when i investigated it for benchmark at the beginning of this issue the high latencies always were at the times of merges.
we observed that the wt process of correlating performance anomalies with various program events was entirely manual.
.
time series all threads category developers need to observe data structure state over time.
both codes in this category deal with data structures specifically how the state of the data structure changes over time and how it can explain sudden changes in performance info correlation of data structure state with execution progress when cache utilization hits performance falls off a cliff.
info data structure state over time the cache is regularly empty as per the empty score .
this information was obtained entirely via manually inserted log messages.
while there are tools that automatically track function calls tracking data structures and especially understanding the semantics of what is being tracked is more difficult .
.
aggregated all threads category this category contains three codes for two of them the information developers need can be trivially delivered by the profiler infofunction cpu hotspot where developers obtain the cpu profile of the running program and info where time is spent within a function where developers observe time spent in a certain function.
we noticed that some developers always ran the profiler as the first step in debugging a performance issue.
understanding the use of synchronization primitives was more important than obtaining the cpu profile but this information was typically collected from multiple sources and often required understanding of the synchronization protocol.
the following quote for the code info use of synchronization primitives affects performance demonstrates that the developer had to understand a complex synchronization primitive and what it was used for using a read write lock to account for direct writes is the dog .
.
remaining information categories application behavior and not os or hardware is the focus.
it is notable that the remaining categories just like the first three are dominated by codes where the information was obtained at the application level.
only the codes in the time series agg.
per thread all threads refer to the information obtained at the os level.
this finding is surprising as wiredtiger is a key value store so we expected the i o and virtual memory management sub systems to be important for performance.
os level information was mostly used to explain latency spikes or throughput drops info correlation of performance and io more runs of this and i ve confirmed that there isn t a relationship between read or write operations and the delays.
i have instrumented every read and write operation over 250ms and i can see that there are periods when we report operational latency over 700ms with no reported slow read and write operations.
understanding the code base is important but is no substitute for high quality performance data.
in several cases information was obtained by analyzing the source code or by relying on existing knowledge of the code rather than by using a performance tool info code behavior .
.
.
with the way the code is currently structured wt verbose is always called even if it is an empty function.
the reason is that the is much more cautious at creating additional eviction workers and as a result we have less contention and a higher throughput.
the higher latencies for app threads tend to come when eviction is needed but there is no work available.
anecdotally we noticed that some developers tended to be rationalists in that they more often tried to explain performance anomalies using their innate knowledge of the code while others were empiricists they always gathered experimental data before forming hypotheses.
perhaps the key observation is that even very experienced developers could not debug tough performance issues relying on their knowledge of code alone having high quality performance information was crucial.
rq2 tools used to understand which tools developers use during performance comprehension rq2 we identified codes related to tools.
we categorized the tools according to the same dimensions as the information codes in section .
table shows the codes that appeared in at least of the tickets along with their descriptions and categories.
the remainder of this section summarizes our main findings regarding how wt developers use tools.
a manually generated application log is the most often used performance debugging tool.
the top category time series per thread all threads includes the codes related to using the application log.
tool manual log existing was used when the previously instrumented log messages were sufficient to get insight into the problem.
tool manual log new was used when the developer had to manually add new log messages to get to the bottom of the issue.
the relative frequency of this code it occurred in of all tickets indicates that obtaining the needed information is largely a manual process even in software with established logging developers often had to manually add new instrumentation.
this is consistent with prior work that noted the high frequency with which logging code is modified .performance comprehension at wiredtiger esec fse november lake buena vista fl usa category and code code description tickets total time series per thread all threads tool manual log existing the software was previously instrumented to output log messages.
the developer turned logging on to collect information for performance debugging.
the log included messages generated over the course of the execution hence the time series temporal view .
depending on the context each message identifies the thread that logged it per thread execution view or can reflect information aggregated over all threads.
tool manual log new this code was used when the developer relied on the same kind of application level logging as covered by the tool manual log existing code but the existing instrumentation was not sufficient and the developer had to add new log messages to the code to obtain the data they wanted.
tool show statistics over time this code was used for two in house tools that visualized statistics collected via an application log as time series.
time series per thread tool gdb callstacks lib app developer used a debugger gdb in our case not for program errors crashes but to understand where and when threads spend their time.
tool pmp lib app pmp stands for poor man s profiler .
this is a script that periodically collects gdb callstacks across all threads and pre processes them with awk.
aggregated all threads tool profiler os lib app this is a typical cpu profiler a tool that automatically provides per function breakdown of cpu cycles and other information.
in our case profilers included perf zoom and instruments.
developers used profilers to only look at information aggregated across threads.
table tool codes in our study.
codes relate to the application level of the stack unless indicated otherwise.
tickets is the number of tickets with code total is the total number of code occurrences.
developers have yet to find the right tool for visualizing logs.
to analyze the log files developers either manually sifted through them or used visualizations.
in our case study developers used two different tools both covered by the code tool show statistics overtime.
over time one tool was deprecated and replaced.
recently wiredtiger has built another in house tool that specifically tracks long running operations .
this churn in tools suggests that developers remain on the lookout for the right log visualization tool.
when the log does not provide per thread information developers seek it by other means.
it was surprising to see that even though developers usually required per thread information see section the application log provided it in the aggregated form.
in some cases it was possible to distinguish between thread types such as application threads vs. internal housekeeping threads based on the contents of the log messages.
but when this level of detail was insufficient the developers resorted to debugger based tools to fill in the gaps.
two codes in the time series per thread category illustrate these situations tool gdb callstacks was used when the developer manually stopped the debugger and examined the callstacks to infer where threads spent time and in particular where they blocked.
tool pmp refers to poor man s profiler which is a set of scripts around gdb that obtain callstacks periodically and parse them with awk.
here are examples illustrating how these tools were used to obtain per thread detail looking at the pmp traces we could see that there were threads sleeping at all times and one thread working.
running pmp i almost never catch any thread in wt during the thread run.
with threads i always see out of yielding in wt page in func .
when running pmp on mongod with wt and running a single thread any time i saw a thread actually in the wt library it was inpwrite via the logging subsystem.
since these tools were not built specifically for obtaining perthread performance information they appeared awkward to use and often required orchestration so that the stack traces could be collected at just the right time.
for example i do have pmp scheduled to run around the time of the very large dropoff.
profilers are not as prevalent as one might expect.
the category aggregated aggregated contains the code generated whenever a profiler was used.
we observed three profilers in our data linux s perf zoom and macos s instruments.
profilers are easy to use and impose low overhead because they use sampling.
unfortunately sampling is unable to capture rare latency spikes so profilers are used to obtain time aggregated information.
although profilers can provide per thread breakdown we did not see them being used in this way.
we were at first surprised that profilers were not used more often.
as we realized that developers spent the majority of their timeesec fse november lake buena vista fl usa fedorova et al.
chasing latency spikes and throughput drops it became clear why profilers were not helpful.
the following quote from a developer captures the sentiment i would only expect perf to be of limited use here.
it is useful in giving an indication of where time is spent over the entire run which is good for general performance tuning but i don t know how to use the results to find what is causing outlier operations7.
rq3 process besides collecting information and using tools performance comprehension involves many processes.
for example developers share and discuss information formulate hypothesis perform and report on experiments etc.
we seek to understand the process that wt developers follow during performance comprehension rq3 .
the three most dominant types of processes that we observed are the relationship between tools used and information gathered the process of reproducing a performance issue and experimentation to gain new insights into an issue.
.
tools and information inter relationships tools generate information but information may also lead to tool usage.
we found substantial evidence that the two are closely linked.
we created a mapping between tools and information using process codes.
a process code was created whenever the text of the document made it clear that a developer used a specific tool to obtain a specific piece of information.
this produced a large number of codes because every such process code reflects a relationship between some tool and some information.
we created process codes by concatenating the tool code with the code for the information obtained using the tool.
if the developer indicated that s he sought a specific type of information and then used the tool to obtain it the info string appears first in the process code and the tool string appears second as in process info functions or activity over time by threads tool pmp .
on the other hand if the developer appeared to use the tool in an exploratory fashion that is without deciding in advance which information produced by the tool might end up being useful in performance debugging the tool string appears first and the information string appears second process tool pmp info functions or activity over time by threads .
the following quote is an example of the information first process i added new stats and turned on statistics logging to see how many times wt lsm manager pop entry is called and how many times that call results in a manager operation getting executed.
and this quote exemplifies the tool first process i collected wt statistics for all runs too.
maybe there is some trend we can see that can help skew the conditional.
here are the ages of how often the server thread helped evict pages.
in section we described the categories we used to group information and tool codes.
the category for a process code is a concatenation of the categories corresponding to the tool and to the information.
we further grouped these categories into concepts reflecting whether the tool or the information was time series aggregate per thread or across threads.
since both the tool and the 7this refers to unusually long running function invocationstool category information category tickets time series per thread time series per thread time series per thread time series all threads time series all threads time series all threads time series all threads time series per thread aggregated all threads aggregated all threads table process concepts accounting for at least of all tickets.
in each concept the developers used a tool first without looking for specific information.
information could correspond to these four concepts there was a total of 4x4 possible concepts for process codes.
of these only five concepts listed in table appeared in at least of all tickets a time series per thread tool was used to obtain the time series per thread information a time series per thread tool was used to obtain time series information aggregated across all threads a time series tool that aggregates data across threads was used to obtain time series information aggregated across threads a time series tool that aggregates data across threads was shoehorned into obtaining time series per thread information and a tool that produced data aggregated across time and across threads was used to obtain the same kind of information.
using these concepts we identified three key patterns in how developers relate tools and information developers use tools first without looking for specific information.
all concepts reflected developers using a tool first.
out of process codes that we created only seven codes in four tickets matched the information first scenario indicating that the process of performance debugging is for the most part exploratory.
we found that even experienced developers often do not know where to begin in resolving a performance problem.
developers use tools to observe time series information.
four of these process categories were related to the way developers acquired time series information either per thread or across all threads.
developers mainly used manual logs and gdb based tools to collect time series information.
manual logging was used to collect time series behaviour for specific threads in tickets of all tickets and used to collect time series behaviour of all threads in tickets of all tickets .
gdb based tools gdb gdbmon pmp were used to collect per thread time series behaviour in tickets of all tickets .
classic cpu profilers are not frequently used.
only one of our process categories reflected developers using standard cpu profilers like perf.
these tools were used to observe aggregate program behaviour in tickets of all tickets .
overall this suggests that the process of performance comprehension is mostly about obtaining time series per thread information using simple tools such as application logging or gdb in an exploratory fashion without knowing first what to look for.performance comprehension at wiredtiger esec fse november lake buena vista fl usa .
reproducibility being able to reproduce a performance bug on a machine that has tools to diagnose and trace the issue is critical to root cause analysis and debugging.
as we studied performance debugging processes when answering rq3 we learned that reproducability was a concern.
overall we created codes relating to reproducibility with at least one code appearing in tickets .
of these reproducibility tickets had instances where a developer tried to reproduce an issue but failed.
this resulted in of tickets coded with reproducibility containing directions or guidance on how to generally reproduce the performance problem.
a common solution to reproducibility problems is to share a configuration file with other developers.
however only tickets coded with reproducibility problems required sharing configuration files.
instead most reproducibility discussion centered around the right hardware software versions to reproduce the issue.
the importance and difficulty of reproducing performance issues in heterogeneous environments has been also noted in prior work .
difficulty reproducing the software version.
in of the tickets that had reproducibility challenges developers struggled to find the code exhibiting the problem.
developers often had several concurrent versions of their own or supporting software i.e.
mongodb and ensuring a developer had the right versions of all software that exhibited the performance problem was not trivial.
for example could you confirm whether the version of mongodb you are using includes that change?
the version of master develop was wt on top of mongo .
difficulty reproducing the hardware.
tickets with reproducibility issues dealt with hardware conditions.
wt developers primarily use cloud based instances for development.
some hardware challenges were related to cloud use.
for example i would appreciate any tips on how to reproduce that behaviour.
perhaps i should run on the same aws instance?
what workloads and under what circumstances like hardware have you seen performance increases?
my aws box and the jenkins have cores and 32gb.
developers seemed reticent to re use instances unless absolutely necessary.
the reasons behind this is not clear to us though the long time to setup a new environment might be a cause.
often a developer lives in a single development environment for a long time because they have added a number of customizations and it would be time consuming to transfer to a new instance.
instead developers asked each other to run tests on instances they could not access for example could you please retest on your instance again?
difficulty reproducing the configuration.
in tickets developers manually shared configuration settings with each other.
this involved developers copy pasting details of their configurations and can be an error prone process because developers may not understand the necessary configuration settings required to reproduce an issue.
however we hesitate to say that this is a problem as discovering that a little known configuration detail has an effect on performance can be a significant step in diagnosing theproblem.
another form of manual sharing was developers sharing code between each other by including it as a comment on a ticket.
dependency on cloud based tools such as ci systems.
one complex reproducibility ticket in wt stood out to us.
the issue was a hang during a performance benchmark run by jenkins on a cloud instance.
reproduction was challenging in two ways the jenkins server itself was not set up to easily debug benchmarks in situ developers instead tried to set it up on their own machines.
the benchmark was challenging to configure and set up on the developer s machines which caused spurious conclusions partway through debugging.
this ticket was eventually solved through substantial effort by reproducing the issue on the jenkins cloud instance where it was discovered to be a race condition.
.
experiments developers run experiments to help identify the cause and potential solutions to a performance issue.
as we were analyzing performance debugging processes we found that much of them revolved around experimental details.
we created experimentrelated codes with at least one appearing in tickets .
experimental comparisons.
comparing code workloads hardware instances or configurations to identify a performance issue was a common activity.
of tickets with some experiment contained a comparison.
we classified experiments as comparisons when we saw at least two different versions of some artifact being compared in contrast to e.g.
experiments in which a program was tested with a given input see workloads .
the most common experiment was a comparison between two different versions of code that already exists such as a comparison between two branches or commits in git.
this appeared in tickets of all tickets where an experiment was performed .
this type of experiment helps with performance regressions in which the developer wants to narrow down what affected performance.
when i was looking at differences between versions i found a good starting place.
i compared changeset good performance to commit y post merge bad performance .
the next most common experiment was a comparison between existing code and a modified version of this code that the developer wrote.
this appeared in tickets of all tickets that contained experiments .
developers used these experiment to determine if the bug was present after the change.
the code change was sometimes a solution attempt but it was also used to disable an often necessary component to help isolate the cause.
for example i made a small experimental change to the develop code to start all eviction threads in wt evict create.
i ran with sweeps turned off comparing it to an identical run yesterday with sweeps on.
comparisons between different software configurations including changing the number of threads appeared in tickets of tickets with experiments .
changing the number of threads was a common experiment appearing in tickets while changing all other kinds of configurations except threads appeared in tickets some tickets used both kinds of experiments .
other kinds ofesec fse november lake buena vista fl usa fedorova et al.
comparisons occurred less frequently.
comparisons between different workloads test cases appeared in tickets of all tickets with experiments comparisons between hardware configurations appeared in tickets .
experiments with workloads.
experiments involving running workloads to evaluate programs appeared in tickets of all tickets with experiments .
previously established workloads were the most popular appearing in tickets of all tickets with experiments .
a wide collection of workloads are often kept to help test the program in different ways such as correctness or performance.
these workloads are often automatically run by the ci system but were also run by wt developers manually to investigate an issue.
these were used as well understood input and helped developers to validate theories about less understood program behaviour.
for example i ve run the medium btree.wtperf test on several changesets both diagnostic and production builds and do not see any change in performance in case it is the other side of the same coin the multi btree zipfian workload read throughput shows a similar performance increase with those changes.
i have started a and run on the ssd with mmap turned back on but no block compressor specified.
we ll see what that looks like tonight.
developers only developed new workloads in tickets drawing on the ycsb benchmarks in of these and re used workloads introduce by another ticket in tickets.
for example constructing a workload that inserts 16mb values from threads in parallel configures a memory page max of 5mb does not do checkpoints does not use explicit transactions i am going to run this branch with mongodb with some of the recent workloads that were having eviction issues.
in particular the standalone version of the insert test from server the final experiment that we noted was a measurement of how performance of a given benchmark varies.
this appeared in tickets of all tickets with experiments .
developers used this to determine how consistently the program behaves across repeated runs on the same machine.
frequently this was used to dismiss or accept as valid an observed performance drop or increase in the context of the program s historical behaviour often this information was not known to all developers .
for example developer a is fruit pop usually stable?
developer b i would say fruit pop is generally stable.
i d say variation may be normal which on a run of seconds is seconds on either side but over feels outside that.
when i started testing i realized that this workload has a lot of variance.
discussion and implications profilers do not do the job.
our case study uncovered that wt developers spent most effort on understanding how latency spikes and intermittent throughput drops relate to other events in the program.
off the shelf performance tools namely profilers were not helpful because developers relied on time series per thread view of the execution which profilers do not provide.
although the sampledata collected by profilers could in theory be displayed across time threads the profiler sampling resolution is not fine enough to capture rare latency spikes.
to obtain the needed information developers used rudimentary tools such as a debugger outfitted with shell scripts but mostly application level logging a battery of log messages manually inserted in the code.
log analysis is often manual and exploratory.
for the most part developers manually sifted through raw log files.
manual log analysis works as long as the logs are not too big or if the developer knows what to search for.
unfortunately as our analysis revealed developers seldom know what to look for.
instead of starting with a concrete hypothesis and looking for a specific piece of information developers usually just throw a tool at the problem to see what information turns up.
tools should support free exploration of execution traces.
future performance tools must support the exploratory process of performance debugging.
requiring a developer to decide a priori which information to log is not productive because developers do not know in advance what information they need.
that is why almost a quarter of issues in our case study necessitated new log messages instead of using existing ones.
ideally a performance tool would log in a non discriminating fashion a wealth of information about functions data structures and their timing so that developers can freely explore the execution trace and discover new phenomena.
designing such exploratory tools poses two challenges large runtime overhead and an overwhelming amount of information to process.
overhead can be addressed with hardware support such as intel processor trace pt or software techniques such as log20 or google s efficient tracing tools .
examining massive amounts of information that tracing can produce is a challenge in and of itself.
from conversations with google engineers we learned that their performance traces are petabytes in size!
there are many visual analysis tools for large execution traces such as extravis synctrace revel and tr mper s thread overview sequence view tracediff and lvis .
ibm s zinsight and google s in house tools are performance debugging tools used in practice that explicitly track outlier events.
however they do not support completely free exploration requiring the developer to specify an execution window to look at.
this concern was related to us by one wt developer identifying interesting time periods is a definite issue with the abc tool8.
we generally rely on users to indicate a relatively small time window to facilitate investigation.
i dream of being able to do some automated pattern matching on the data in order to isolate interesting time periods in the future.
developers could take better advantage of cloud resources.
despite running a cloud based ci system and working primarily on cloud instances wt developers still only used a few personal instances for development and testing.
when an issue appeared on a specific hardware configuration it was often left up to a particular user to re run the experiment after others have pushed changes.
essentially developers treated their cloud instances as personal development machines.
even the wiredtiger ci system in this case was apparently only able to to test one branch of code at a time.
8the developer referred to a specific in house tool that shows statistics over time.performance comprehension at wiredtiger esec fse november lake buena vista fl usa we think developers could take better advantage of the immediate availability of cloud resources assuming they have a sufficient budget.
for example to dissuade developers from local and personal customizations developers could maintain environment configuration as part of version control.
to reduce hardware related reproducibility challenges developers should be able to easily start new instances with the standardized environment.
and to enable easier instance sharing developers can maintain common authentication between all of their instances.
supporting performance experimentation.
experiments yield valuable information but only when the workloads are well understood.
in wt s case a common catalog of workloads with welldocumented properties and behaviors seemed to improve reproducibility and accuracy of interpretation.
but developers often manually re ran and cataloged experimental output.
we believe that existing ci systems can provide better support potentially running experiments pre emptively and along different dimensions to establish baselines for interpreting performance degradations similar to what was described by nguyen et al.
.
we hope that our findings on how developers work to address performance issues will spur new ideas for tools in this area.
related work several studies characterize performance bugs.
for example zaman et al.
compare the qualitative difference between performance bugs and non performance bugs in mozilla and google chrome along four dimensions impact context fix and fix validation.
the study finds that developers and users face problems in reproducing performance bugs and have to spend more time discussing performance bugs than other kinds of bugs.
likewise nistor et al.
study how performance bugs are discovered reported to and fixed by developers and compare the results with those for non performance bugs.
the authors consider bugs from eclipse jdt eclipse swt and mozilla.
they find that fixing performance bugs is more difficult than fixing non performance bugs and that compared with non performance bugs a larger percentage of performance bugs are discovered through code reasoning than by users observing the negative effects of the bugs.
song et al.
follow up on these findings and find that userreported performance problems are observed through comparison including comparisons within one code base with different inputs and comparisons across multiple code bases.
performance problems are also often reported as comparisons including both good and bad inputs.
in addition like nistor et al.
this study also finds that performance problems take a long time to diagnose.
to better understand how developers locate performance bugs navigate through the code understand the program and communicate the detected issues baltes et al.
perform an observational study with twelve developers who are trying to fix performance bugs in two open source projects.
the developers worked with a specific profiling and analysis tool that visually depicts runtime information in a list representation and is embedded into the source code view.
the study concludes that fixing a performance bug is a code comprehension and navigation problem and that flexible navigation features are needed.
program comprehension implicates several of our information codes .
however our studyconsiders performance information more broadly e.g.
relation of program activity and os activity and also reports on tools and processes that are implicated in performance comprehension.
jin et al.
study performance bugs that are randomly sampled from five large software projects.
they discover that more than one quarter of the bugs are caused by developers misunderstanding the workload or performance related characteristics of certain functions and almost half of the bugs require large scale input to be reproduced.
we also observed the importance of reproducibility in our dataset and we characterized several corresponding difficulties section .
.
they also found that developers employed one of three bug tracing strategies changing a function call sequence conditionally skipping code units which do not always perform useful work and changing program parameters.
similarly liu et al.
study performance bugs from eight popular android apps focusing on bug characteristics such as gui lagging and memory bloat.
they found that over a third of these bugs manifest themselves only after special user interactions and that performance bug detection requires manual effort.
this is consistent with our findings section we found that wt developers expended substantial manual effort on performance comprehension frequently using custom and manual tools such as logging.
our work complements these prior studies instead of contrasting performance and non performance debugging how performance bugs are identified and reported and what are the code patterns that can help identify performance bugs we investigate what kind of information developers need to locate performance bugs what tools they use in practice and what processes they follow.
an extensive body of prior work considers developers information needs adoption of certain tools and software processes more generally.
by contrast our focus is specifically onperformance comprehension.
to the best of our knowledge this is the first study of this kind.
conclusion performance debugging is a task with its own processes tools and information requirements .
we presented an empirical case study of performance related issues in the wiredtiger project a high performance database engine.
we studied developer discussions threads in issues and used grounded theory to characterize the concepts related to information collected tools used and the process the developers followed in their investigations.
overall we found that these performance focused developers spend a lot of their time chasing latency spikes and throughput drops but are not supported by existing tools.
instead they examine manual logs or the raw output of rudimentary call stack samplers.
developers keep a number of workloads to help monitor the performance of their system but often have difficulty reproducing performance issues in a cloud environment.
to diagnose performance issues developers perform exploratory differential experiments to understand what recent changes to code could have caused the problem.
they also frequently use tools without knowing what information might be relevant to the problem.esec fse november lake buena vista fl usa fedorova et al.