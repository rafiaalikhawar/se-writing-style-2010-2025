clami defect prediction on unlabeled datasets jaechang nam and sunghun kim department of computer science and engineering the hong kong university of science and technology hong kong china email fjcnam hunkimg cse.ust.hk abstract defect prediction on new projects or projects with limited historical data is an interesting problem in software engineering.
this is largely because it is difficult to collect defect information to label a dataset for training a prediction model.
cross project defect prediction cpdp has tried to address this problem by reusing prediction models built by other projects that have enough historical data.
however cpdp does not always build a strong prediction model because of the different distributions among datasets.
approaches for defect prediction on unlabeled datasets have also tried to address the problem by adopting unsupervised learning but it has one major limitation the necessity for manual effort.
in this study we propose novel approaches cla and clami that show the potential for defect prediction on unlabeled datasets in an automated manner without need for manual effort.
the key idea of the cla and clami approaches is to label an unlabeled dataset by using the magnitude of metric values.
in our empirical study on seven open source projects the clami approach led to the promising prediction performances .
and .
in average f measure and auc that are comparable to those of defect prediction based on supervised learning.
i. i ntroduction defect prediction plays an important role in software quality .
defect prediction techniques provide a list of defectprone source code so that quality assurance qa teams can focus on the most defective parts of their products in advance.
in this way qa teams can effectively allocate limited resources on which to review and test their software products before releasing them.
in industry defect prediction techniques have been actively adopted for software quality assurance .
researchers have proposed and facilitated various defect prediction algorithms and metrics .
most defect prediction models are based on supervised learning for classification e.g.
predicting defect proneness of a source code file or regression e.g.
predicting the number of defects in a source code file .
rather than using a machine learning technique kim et al.
proposed bugcache which manages defect prone entities in source code by adapting the cache concept used in operating system .
metrics for defect prediction can be divided into code and process metrics .
code metrics represent how the source code is complex while process metrics represent how the development process is complex .
in particular studies on defect prediction metrics have been actively conducted as the use of software archives such as version control systems and issue trackers has become popular .
most metrics proposed over the last decade such as change code metric churn change code entropy popularity and developer interaction have been collected from various software archives .
however typical defect prediction techniques based on supervised learning are designed for a single software project and are difficult to apply to new projects or projects that have limited historical data in software archives.
defect prediction models based on supervised learning can be constructed by using a dataset with actual defect information that is a labeled dataset .
defect information usually accumulates in the software archives thus new projects or projects with a short development history do not have enough defect information.
this is a major limitation of the typical defect prediction techniques based on supervised learning.
to address this limitation researchers have proposed various approaches to enable defect prediction on projects with limited historical data.
cross project defect prediction that builds a prediction model using data from other projects has been studied by many researchers .
defect prediction techniques on unlabeled datasets were proposed as well .
recently an approach to build a universal defect prediction model by using multiple project datasets was introduced .
however there is still an issue of different distributions among datasets in existing approaches for cross project defect prediction cpdp and universal defect prediction udp .
in cpdp and udp the major task is to make the different distributions of datasets similar since prediction models work well when the datasets for training and testing a model have the same distributions .
however the approaches based on making the different distributions similar between training and test datasets may not always be effective .
compared to cpdp and udp the existing approaches for defect prediction on unlabeled datasets are relatively less affected by the issue of different distributions among datasets but will always require manual effort by human experts .
zhong et al.
proposed the expert based defect prediction on unlabeled datasets where a human expert would label clusters from an unlabeled dataset after clustering .
catal et al.
proposed the threshold based approach where labeling datasets is conducted based on a certain threshold value of a metric .
a proper metric threshold for the threshold based approach is decided by the intervention of human experts .
since these approaches on unlabeled datasets are conducted on the test dataset itself the issue of the different distributions among datasets is not affected .
however these approaches require manual effort by human experts .
the goal of this study is to propose novel approaches cla and clami which can conduct defect prediction onclassification software archives b c c b ... instances with metrics and labels b c b ... training instances preprocessing model ?
new instance generate instances build a model c b fig.
the typical defect prediction process based on supervised learning.
unlabeled datasets in an automated manner.
the key idea of the cla clami approaches is to label an unlabeled dataset by using the magnitude of metric values.
in our empirical study the cla clami approach led to promising prediction performances of .
average fmeasure and .
average auc while the typical defect prediction based on supervised learning showed .
and .
in average f measure and auc respectively.
in addition the cla clami approaches outperformed or were comparable to the existing approaches for defect prediction on unlabeled datasets with statistical significance.
these promising results show that our cla clami approaches have the potential for defect prediction on projects that have limited historical data.
the contributions of this study are as follows proposing novel approaches cla and clami for defect prediction on unlabeled datasets in an automated manner.
an empirical study to evaluate the cla clami approaches against existing defect prediction approaches.
ii.
b ackground and related work a. typical defect prediction process fig.
shows the typical defect prediction process for a single software project based on supervised machine learning classification .
since this whole process is conducted within the single software project it is called within project defect prediction wpdp .
in this process we first collect various data such as development artifacts and historical information from software archives such as the version control system and the issue tracker of the software project.
using the data from software archives we can measure the complexity of the software project and its development process.
the complexity measurement can be conducted at different granularities such as the function method file class or subsystem package levels.
in addition we can collect defect information for the software project from the issue tracker and commit messages in the version control system.
with the collected data we can generate instances that consist of metrics features in a machine learning sense and labels.
in fig.
an entity with numeric values and labels cross prediction model target project t est set source project training set model build predict fig.
cross project defect prediction b and c is called an instance.
each instance represents a function a file or a subsystem according to their granularity.
metrics measure the complexity of the software or its development process and each metric has a numeric value in the instance.
there are various kinds of metrics such as lines of code the number of functions and the number of authors touching a source code file .
the instance is also labeled as either buggy b or clean c by using the collected defect information as in fig.
.
with the set of labeled instances we can apply some preprocessing techniques used in machine learning.
the representative preprocessing techniques are normalization and feature selection and are widely used in defect prediction studies .
however preprocessing may not be necessary depending on the defect prediction models .
we can then generate a dataset of training instances to build a prediction model.
since the prediction model is built by using supervised learning we can use various machine learners to build a classification model such as logistic regression decision tree naive bayes and random forest .
using the prediction model built by the dataset of training instances we can predict new instances as buggy or clean.
for example as shown in fig.
the new instance marked with ?
is classified as buggy by the prediction model.
this typical defect prediction for a single project wpdp has a major limitation.
for any new projects or projects with limited historical data it is difficult to build defect prediction models since we cannot generate the labeled dataset for training a model without defect information .
in section ii b we list related studies addressing this limitation.
b. defect prediction on projects with limited historical data researchers have tried to address the issue on new projects or projects with limited historical data to build defect prediction models by proposing techniques for cross project defect prediction and defect prediction on unlabeled datasets .
cross project defect prediction cpdp fig.
shows the typical cpdp process.
in fig.
the small shaded circles insource project represent labeled instances and the small rectangles in target project represent unlabeled instances.
we first build a prediction model using the source project with labeled instances.
then using the model we can predictwhether an instance in the target project is defect prone or not.
however cpdp has a challenging issue that prediction performance was not practical .
zimmermann et al.
conducted cross predictions but only predictions were successful in their experimental setting .
watanabe et al.
proposed the metric compensation approach to improve cpdp models.
the approach by watanabe et al.
makes a target project similar to a source project by normalizing metric values using the average metric values .
the metric compensation improved the cpdp models but the prediction performance of cpdp with the metric compensation is worse than that of wpdp .
turhan et al.
proposed the nearest neighbour nn filter for cpdp .
the nn filter selects the nearest source instances for each target instance .
in other words when building a prediction model the nn filter approach uses the most similar source instances to the target instances .
however its prediction performance is still worse than that of wpdp .
to resolve the cpdp issue ma et al.
and nam et al.
facilitated transfer learning techniques from the machine learning community .
ma et al.
proposed transfer naive bayes tnb which provides more weight to the source instances that are similar to target instances when building a naive bayes model .
the tnb led to better prediction performance than the approach based on the nn filter .
nam et al.
adopted transfer component analysis tca which is a stateof the art transfer learning technique and proposed tca for cpdp .
in the empirical study of nam et al.
the performance of cpdp was comparable to that of wpdp .
recently cpdp models have been evaluated with a view to its cost effectiveness .
rahman et al.
confirmed that cpdp models can outperform wpdp models in terms of cost effectiveness.
canfora et al.
proposed multi objective approach for cpdp .
multi objective models built using a genetic algorithm help software engineers choose prediction models having different objectives such as high recall or low cost .
in their empirical study multi objective models achieved better prediction results than a wpdp model in terms of cost effectiveness .
panichella et al.
proposed a combined defect predictor codep for cpdp .
codep combines defect prediction results from different machine learning models and led to better prediction performance than a single prediction model in auc and cost effectiveness .
zhang et al.
addressed the cpdp issue by proposing the universal defect prediction model .
since the individual project may have its specific defect characteristic the universal model may not work for all projects .
to resolve this limitation zhang et al.
proposed context aware rank transformations that change metric values ranging from to across all projects .
in this way the universal model could be built using projects from sourceforge and google code.
in their experimental setting it showed a comparable prediction performance to wpdp .
the related studies about cpdp actually addressed the same issue we address in this study that is defect prediction on unlabeled dataset.
however in contrast to our cla clami approaches the cpdp approaches always require abundant source project datasets and the prediction model can be con structed when both source and target projects have the same metric set.
most studies for cpdp try to make the different distributions of source and target datasets similar by using techniques such as transforming metric values selecting similar instances and using transfer learning .
however those techniques cannot always effectively make different distributions similar when compared to the case where the same project dataset is used.
therefore they can still suffer from the dataset shift problem .
recently this problem has also been observed even in different releases of the same project .
cla and clami do not need any techniques to make different distributions of datasets similar since we just use the same project dataset where we want to predict defects.
in other words the cla clami approaches do not need to consider any source projects and the limitation of different metric sets in project datasets to build a prediction model.
defect prediction on unlabeled datasets there are a couple of studies for defect prediction on unlabeled datasets .
zhong et al.
proposed the expert based approach .
the expert based approach first clusters unlabeled instances using a clustering algorithm such as k means then asks a humanexpert whether a cluster is defect prone or not after providing average metric values of the cluster that is centroid .
using the expert based approach zhong et al.
achieved a .
false positive rate and a .
false negative rate in the best cases.
the expert based approach always requires human experts to decide whether a cluster is defect prone or not.
thus this approach cannot fully automate defect prediction on unlabeled datasets.
however our approaches cla and clami need no human experts and can automate the defect prediction process on unlabeled datasets.
catal et al.
proposed a one stage threshold based approach .
after initially proposing a two stage approach based on both clustering and threshold they concluded that the one stage threshold based approach is easier than the two stage approach and still effective enough .
the threshold based approach predicts an instance as buggy when any metric value is greater than the given metric threshold values .
the threshold values were decided based on experience and hints from the literature past defect prone modules and analysis of past versions of a project .
the one stage threshold based approach achieved a .
false postive rate and a false negative rate .
the threshold based approach needs to decide metric threshold values in advance.
in other words additional effort is required for metric threshold values.
however cla and clami do not need additional effort and can build a prediction model using only unlabeled datasets.
iii.
a pproach fig.
shows the overall process of our cla and clami approaches for defect prediction on an unlabeled dataset.
the key idea of our approaches is to label unlabeled instances by using the magnitude of metric values.clami approach overview unlabeled dataset clustering labeling metric selection instance selection metric selection clami model build predict training dataset test dataset cla model clustering labeling fig.
the overview of cla and clami for defect prediction on an unlabeled dataset.
we name our approaches cla or clami based on the two or four steps respectively.
the first two steps for cla and clami are clustering instances and labeling instances in clusters.
with these two steps we can label predict all instances in the unlabeled dataset.
cla consists of only these two steps.
based on the two steps we name our first approach cla .
clami has two additional steps to generate a training dataset metric selection and instance selection.
based on all four steps we name our second approach clami .
in the machine learning community metric feature selection and instance selection have been widely used to improve prediction models by removing noisy metrics and instances .
in addition metric selection and instance selection have also been applied to improving various prediction models in the software engineering community .
for clami we also select metrics from the unlabeled dataset to generate a test dataset with the same set of metrics as in the training dataset generated by clami since both training and test datasets should have the same metric set.
the test dataset includes all the instances from the original unlabeled dataset but with only selected metrics.
when the training and test datasets are ready for clami we can build a prediction model using various machine learning classifiers and conduct defect prediction for the unlabeled test dataset.
the following subsection describes the four steps in detail.
a. steps for cla and or clami clustering instances fig.
shows how to cluster instances in a dataset.
in fig.
x1 x7represent metrics of the unlabeled dataset while inst.
a grepresent instances of the dataset.
we first identify higher metric values that are greater than a specific cutoff threshold.
in the example of fig.
we use a median value for each metric as the threshold to decide higher metric values.1for example the median of the metric x1 is1fromf0 3g.
then we can identify the higher metric values of x1 i.e.f2 3g.
in the same way we compute the higher metric values for other metrics as well.
in fig.
the higher metric values that are greater than a corresponding median value are in bold font.
after identifying the higher 1in section vi we apply various cutoff thresholds to decide higher metric values and compare prediction performances on the various thresholds.
x1 x4 cluster k unlabeled dataset x1 x2 x3 x4 x5 x6 x7 label ?
?
?
?
?
?
?
median inst.
a inst.
b inst.
c inst.
d inst.
e inst.
f inst.
g instances k the number of metrics whose values are greater than median.
c cluster k a e b d f cluster k g cluster k clustering instance selection x1 x2 x3 x4 x5 x6 x7 label buggy clean buggy clean buggy clean clean inst.
a inst.
b inst.
c inst.
d inst.
e inst.
f inst.
g x1 x5 x7 label buggy buggy clean clean buggy clean x1 x5 x7 label buggy buggy clean final training dataset labeling clusters instance conflicts metric conflict scores selected metrics x1 x4 label buggy clean buggy clean buggy clean clean inst.
a inst.
b inst.
c inst.
d inst.
e inst.
f inst.
g x1 x4 label clean buggy clean clean clean inst.
b inst.
c inst.
d inst.
f inst.
g final training dataset fig.
clustering and labeling.
any metric values greater than the median values are in bold font.
metric values we can count the number of the higher metric values in each instance.
for example in the instance a the number k of the higher metric values is three in x1 in x3 and in x7 .
after computing kvalues of all instances we can group instances that have the same kvalue.
as shown in fig.
we can form four clusters with k and .
labeling by considering kvalues we divide clusters into two groups a top half and a bottom half and label the instances in the top half of the clusters as buggy and the others as clean.
in fig.
the instances a c and ein the clusters k and k are labeled as buggy and other instances are labeled as clean.
the intuition of this labeling process is based on the defectproneness tendency of typical defect prediction datasets that is higher complexity causes more defect proneness .
in other words since typical defect prediction metrics measure the complexity of the source code and development process there is a tendency that buggy instances have higher metric values than clean instances .
in this sense we label instances in the top half of the clusters as buggy since the number of the higher metric values in the instances of the top half of the clusters is more than that of the bottom half of the clusters.
metric selection after labeling the instances in the top and bottom clusters we conduct metric selection based on the metric violation scores mvs for clami.
since the quality of defect prediction models is highly dependent on the quality of the metrics metric selection to choose the most informative metrics for prediction models has been widely adopted in defect prediction .
metric selection in clami is based on removing metrics that can minimize violations in the defect proneness tendency of defect datasets .
not all metrics follow the defect proneness tendency so that metric selection of clami can be helpful to build a better prediction model.
a violation is a metric value that does not follow the defectproneness tendency.
the metric value in x1of the instance elabeled as buggy is not greater than the median so this case is counted as a violation.
the metric value ofx2in the instance flabeled as clean is greater than its median value so this case is also considered to be a violation.
by counting all these violations we can define the metric violation score x1 x4 cluster k unlabeled dataset x1 x2 x3 x4 x5 x6 x7 label ?
?
?
?
?
?
?
median inst.
a inst.
b inst.
c inst.
d inst.
e inst.
f inst.
g instances k the number of metrics whose values are greater than median.
c cluster k a e b d f cluster k g cluster k clustering instance selection x1 x2 x3 x4 x5 x6 x7 label buggy clean buggy clean buggy clean clean inst.
a inst.
b inst.
c inst.
d inst.
e inst.
f inst.
g x1 x5 x7 label buggy buggy clean clean buggy clean x1 x5 x7 label buggy buggy clean final training dataset labeling clusters instance conflicts metric violation scores selected metrics x1 x4 label buggy clean buggy clean buggy clean clean inst.
a inst.
b inst.
c inst.
d inst.
e inst.
f inst.
g x1 x4 label clean buggy clean clean clean inst.
b inst.
c inst.
d inst.
f inst.
g final training dataset fig.
computing metric violation scores mvs and metric selection.
violated metric values are shaded in dark gray.
the metrics with the minimum mvs are selected.
x1 x4 cluster k unlabeled dataset x1 x2 x3 x4 x5 x6 x7 label ?
?
?
?
?
?
?
median inst.
a inst.
b inst.
c inst.
d inst.
e inst.
f inst.
g instances k the number of metrics whose values are greater than median.
c cluster k a e b d f cluster k g cluster k clustering instance selection x1 x2 x3 x4 x5 x6 x7 label buggy clean buggy clean buggy clean clean inst.
a inst.
b inst.
c inst.
d inst.
e inst.
f inst.
g x1 x5 x7 label buggy buggy clean clean buggy clean x1 x5 x7 label buggy buggy clean final training dataset labeling clusters instance conflicts metric violation scores selected metrics x1 x4 label buggy clean buggy clean buggy clean clean inst.
a inst.
b inst.
c inst.
d inst.
e inst.
f inst.
g x1 x4 label clean buggy clean clean clean inst.
b inst.
c inst.
d inst.
f inst.
g final training dataset fig.
instance selection and the final training dataset.
violated metric values are shaded in dark gray.
instances without violations are selected.
of an i th metric mv s i as follows mv s i ci fi where ciis the number of violations in the i th metric and fiis the number of metric values in the i th metric.
fig.
shows how to compute mvs and how to select metrics by mvs.
in fig.
shaded metric values represent violations.
for example in x3 the number of violations is .
by equation the violation score of x3 mv s is .
.
in this way we can compute the violation scores of all the metrics.
then we can select metrics whose violation scores are minimum.
in fig.
metrics that have the minimum mvs that is arex1andx4.
thus we finally select x1 andx4as metrics for the training dataset.
instance selection fig.
shows the final step to generating a training dataset for clami that is instance selection.
after the metric selection there may still be violations in the metric values.
we remove instances that have any violated metric value.
as shown in fig.
the instances aandehave violations in their values thus we remove instances aand eto generate the final training dataset.
instance selection is widely used in various prediction models as well .
after instance selection it might be that there are no buggy and or clean instances because of many violations.
it is not possible to build defect prediction models based on supervised learning when both buggy and clean instances do not exist together.
in this case we can use the next minimum mvs to select metrics until we can generate a training dataset with both buggy and clean instances together.table i the seven defect datasets from two groups.
group dataset of instances of metricsprediction granularity all buggy netgene httpclient .
filejackrabbit .
lucene .
rhino .
relink apache .
file safe .
zxing .
the reason we propose clami models that are built by a machine learner is to get advantage from metric and instance selection.
cla can label all instances.
however cla may have violated metrics and instances that do not follow the defect proneness tendency of typical defect prediction metrics.
in this reason cla may label instances incorrectly.
to minimize the instances that might be incorrectly labeled we apply metric and instance selection by removing the violations.
then we predict defects of the unlabeled instances by using the machine learner built using a training set that consists of most representative metrics and instances selected by clami.
iv.
e xperimental setup a. research questions to evaluate the cla clami approaches we set the following research questions.
rq1 are the prediction performances of cla clami comparable to those of typical defect prediction based on supervised learning for a single software project?
rq2 are the prediction performances of cla clami comparable to those of existing approaches for defect prediction on unlabeled datasets?
in rq1 we first compare cla clami to the typical defect prediction using labeled data that is supervised learning.
if cla clami shows comparable prediction results to the typical defect prediction then in practice it can be used for projects without labeled data.
we also compare cla clami to the existing defect prediction approaches on unlabeled datasets.
in rq2 cla clami is compared with two baselines that is threshold based and expert based approaches .
b. benchmark datasets table i lists seven datasets from two groups netgene and relink used in our empirical study.2the prediction performances of defect prediction models are significantly affected by noisy defect data .
the noisy defect data are usually caused when the defects are collected by automatic tools for mining software archives .
for this reason we choose these seven datasets as our experimental subjects since the defect data in the datasets are manually verified or linked to code changes .
2herzig et al.
used the combined metrics of net work and gene alogy in their empirical study.
we call this dataset group netgene after several letters of two names.experimental settings rq1 supervised learning model t est set training set supervised model baseline1 training predict cla clami model training predict predict threshold based baseline2 expert based baseline3 training predict training predict fig.
experimental setup for baselines and cla clami.
the netgene datasets from the study by herzig et al.
were generated by using more than issue reports manually classified.
herzig et al.
discussed the problem of misclassified issue reports .
since the misclassified issue reports provide wrong labels to defect datasets it is critical for defect prediction models .
thus for our empirical study we used all the netgene datasets used in the study by herzig et al.
.
the netgene datasets include network and change genealogy metrics .
herzig et al.
proposed metrics based on change genealogies and validated the metrics with existing metrics such as complexity metrics and network metrics .
in their empirical study the combined metrics using network and change genealogy metrics led to the best prediction performance in their empirical study .
we also used the golden datasets of relink as experimental subjects .
bird et al.
discussed the problem about missing links between issue reports and change logs that impact on defect prediction performance .
wu et al.
addressed the missing link problem by relink and used golden datasets based on manually inspecting links in their empirical study .
each relink dataset consists of code complexity metrics extracted by the understand tool .
c. experimental design with baselines to evaluate cla clami we set three baselines supervised learning threshold based and expert based approach.
baseline1 we use supervised learning models trained by labeled data as a baseline to validate rq1 since the typical defect prediction model for a single software project is based on supervised learning .
fig.
shows how to build a supervised learning model as well as a cla clami model for a dataset.
we compare the prediction performance of a typical defect prediction model based on the supervised learning model baseline1 and our cla clami model.
in supervised learning we need labeled data thus to build a supervised learning model we divided the datasets into two splits as in fig.
.
in the first prediction round we build a prediction model using the first split as a training set and test the model on the second split as atest set.
then for the second round the first split is used as a test set and the second split is used as a training set.
as shown in fig.
the supervised learning model is built using the training set with labeled instances small shaded circles and predicts defects on the test set with unlabeled instances small unshaded circles .
since our approaches cla and clami are for an unlabeled dataset we build cla clami models using only the test set with unlabeled instances as in fig.
.
from both the supervised learning model and the cla clami model we can measure the prediction performance and evaluate whether the prediction performance of our cla clami model is comparable to that of the supervised learning model or not rq1 .
since there is randomness to split a dataset into training and test sets we repeat the first and second rounds i.e.
two fold cross validation times to conduct predictions and report the averaged results .
as a machine learning classifier for both the supervised learning model and clami we use logistic regression that has been widely used and shown good prediction performance in defect prediction studies .
in addition we use other classifiers such as bayesian network j48 decision tree logistic model tree naive bayesian random forest and support vector machine .
we discuss prediction performances on various classifiers in section vi.
baseline2 and baseline3 fig.
also shows how to predict defects in an unlabeled dataset using existing approaches based on threshold baseline2 and expert baseline3 and the cla clami models.
since the two baseline approaches and the cla clami models are for the unlabeled dataset each prediction is conducted on the same test set with unlabeled instances.
the threshold based approach predicts a source code file as buggy when any metric value of the file is greater than the designated values of a threshold vector .
catal et al.
set the threshold vector for metrics including lines of code cyclomatic complexity unique operator unique operand total operator and total operand as in their study.
with this threshold vector if the cyclomatic complexity of a source code file is greater than the threshold value of the file is predicted to be buggy.
catal et al.
actually proposed threshold based and clustering based approaches together but opted to use the threshold based approach since it is easier and more effective than the clustering based approach .
thus we use the threshold based approach as one of baselines from existing approaches for defect prediction on unlabeled datasets.
we generate threshold vectors for each dataset in table i by using the tuning machine technique as did catal et al.
.
catal et al.
used three techniques to determine a threshold vector such as experience and hints from the literature tuning machine and analysis of multiple versions as proposed by marinescu .
however we only apply the tuning machine technique that decides a threshold value maximizing the number of correctly predicted instances by considering past defects since experience and hints from the literature and analysis of multiple versions are not available for our subject datasets.
past defects on each subject dataset are not available either therefore we compute threshold values maximizing the number of correctly predicted instances in thetable ii comparison of results between baselines and cla clami in precision recall f measure and auc.
sl supervised learning thd threshold based exp expert based the better cla clami results that sl thd and exp with statistical significance wilcoxon signed rank test p .
are in bold font in gray and with an asterisk respectively.
the better results between cla and clami with statistical significance are underlined.
projectprecision recall f measure auc sl thd exp cla clami sl thd exp cla clami sl thd exp cla clami sl clami httpclient .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
jackrabbit .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
lucene .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rhino .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
apache .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
safe .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
zxing .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
average rank3.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rank sum rank sum same test dataset for this experiment.
this means the threshold values computed by the tuning machine should work better on our subject datasets than those computed with the past defect although we only use the tuning machine.
as the second baseline we use the expert based approach proposed by zhong et al.
.
the expert based approach is based on clustering and human experts.
in other words this approach first clusters instances in a dataset by using clustering algorithms such as k means and then human experts decide whether a cluster is buggy or clean .
this approach requires human experts therefore it is impossible to automate the whole process.
for our empirical study we cluster instances using the k means clustering algorithm and then decide upon the label of each cluster as human experts know the actual label of the cluster if the number of buggy instances in the cluster is greater than that of clean instances we label the cluster as buggy otherwise as clean.
for the number of clusters we used as zhong et al.
suggest .
d. measures to measure the prediction performance of baselines and cla clami we use precision recall f measure and or the area under the receiver operating characteristic curve auc .
f measure is a widely used prediction measure in defect prediction studies since it represents the harmonic mean of precision and recall .
precision represents the rate of correctly predicted buggy instances among all instances predicted as buggy.
recall measures the rate of correctly predicted buggy instances among all actual buggy instances.
auc is known to be useful for comparing different prediction models .
auc is plotted using true positive rate recall and false positive rate by changing different prediction thresholds .
thus auc is a proper measure for comparing the overall prediction performances of different models.
since f measure is highly affected by class imbalance and prediction threshold it can make it difficult to fairly compare prediction models .
for this reason auc is also widely used in the defect prediction literature .
the auc of .
is considered as promising prediction performance .
when comparing cla clami models to baselines we report precision recall and f measure.
when comparing asupervised learning model and a clami model we additionally report auc.
both the supervised learning model and the clami model are based on statistical models so that auc can be computed by prediction threshold values.
for f measure we use .
as a prediction threshold as have most defect prediction studies reporting f measure .
for statistical tests we conduct the friedman test p .
with the nemenyi test as a post hoc test when comparing multiple models over the seven benchmark datasets.
after the friedman test with the nemenyi test we report the visual representation of our results following dem sar guidelines .
dem sar also suggested using the wilcoxon signed rank test when comparing two models .
thus to compare the supervised learning model and the clami model in auc we conduct the wilcoxon signed rank test p .
.
in addition when comparing two models a baseline vs cla clami for each dataset we conduct the wilcoxon signed rank test p .
to test if the performances from predictions between the baseline and the cla clami models are different with statistical significance.
v. r esults table ii shows the prediction performances between baselines and the cla clami in various measures such as precision recall f measure and auc.
for supervised learning and clami we use logistic regression as a classifier.
as a threshold to decide higher metric values in cla clami we use a median value of each metric.
for precision recall or fmeasure we conduct the friedman test and report the average ranks for the five approaches such as supervised learning sl the threshold based approach thd the expert based approach exp cla and clami as in the last row of table ii .
the friedman test ranks the five approaches in each dataset .
for example in terms of precision the ranks of the five approaches in apache are sl thd exp cla and clami the approach with the best precision is ranked in .
after computing the ranks for the seven datasets we can compute the average rank for each approach.
the friedman test compares whether the average ranks are statistically significant or not .
table ii for each dataset if the results of cla clami are better than those of sl thd and exp with statistical significance the wilcoxon signed rank test p .
the results of cla clami are in bold font in gray and asterisked1 thd cla clami exp sl fig.
comparison of average ranks of baselines and cla clami in f measure.
approaches that are not significantly different the nemenyi test at p .
are connected.
respectively.
the better results between cla and clami with statistical significance are underlined.
in terms of auc the clami results that better sl with statistical significance the wilcoxon signed rank test are in bold font.
cla and clami outperform sl and thd in most datasets in terms of f measure with statistical significance.
cla and clami outperform sl in and datasets respectively and outperform thd in all datasets.
in auc clami outperforms sl in most datasets except lucene with statistical significance.
fig.
visualises the results of post hoc tests by the nemenyi test after the friedman test in terms of f measure .
the friedman test p .
computes the p value as .
for f measure results in table ii.
this represents that there are statistical difference between the average ranks of five approaches in f measure.
then we conduct the nemenyi test as a post hoc test for each pair of the approaches.
the top line in fig.
represents the axis where average ranks of five approaches are plotted.
approaches that are not statistically significant are connected.
in fig.
there are two groups thd sl and sl clami cla exp based on the connected approaches.
the lower average rank the right side in the axis represents the better prediction performance.
from fig.
we could observe cla clami and exp outperform thd.
cla clami and exp seem to have an equivalent performance as they are in the same group.
sl is on the border between two groups so that it is difficult to conclude whether sl performs the same as other approaches because of insufficient resulting data.
in terms of auc clami shows comparable results to sl after conducting the wilcoxon signed rank test p .
between sl and clami for all seven datasets .
the wilcoxon signed rank test compares the sums of the positive and negative ranks between sl and clami in table ii and the computed p value is .
.
thus the difference of the rank sums between sl and clami is not statistically significant.
table ii also shows the comparison results between baselines and cla clami in precision.
for each dataset cla and clami outperform sl in four datasets.
however cla and clami do not outperform thd and exp in most datasets with statistical significance only one gray cell against thd and no asterisk against exp .
fig.
shows the results of the post hoc tests after the friedman test the computed p value is .
in terms of precision.
the average ranks of cla and clami are worse than exp with statistical significance.
compared to sl and thd cla and clami do not show critical difference as they are grouped together.
thd cla clami exp sl fig.
comparison of average ranks of baselines and cla clami in precision.
approaches that are not significantly different the nemenyi test at p .
are connected.
thd cla clami exp sl fig.
comparison of average ranks of baselines and cla clami in recall.
approaches that are not significantly different the nemenyi test at p .
are connected.
table ii shows the comparison results between baselines and cla clami in recall.
for each dataset cla and clami outperform sl and thd in most datasets results in six datasets are in bold font and all results in cla and clami are shaded in gray.
cla and clami also outperform exp in four datasets.
fig.
shows results of the post hoc tests after the friedman test the computed p value is .
in terms of recall.
the average ranks of cla and clami are better than those of thd with statistical significance.
compared to sl and exp cla and clami do not show critical difference as they are grouped together.
the prediction performances between cla and clami do not show a significant difference as shown in fig.
and .
the difference in average ranks between cla and clami is marginal e.g.
.
vs .
in recall.
the average rank .
of cla in precision is slightly better than that .
of clami but its difference is marginal as well.
since cla does not require any machine learning classifier cla is a simpler approach compared to clami.
thus we suggest to use cla.
however in some datasets such as rhino and zxing clami outperforms cla in precision recall and fmeasure as in table ii.
in this sense it would be interesting to investigate when clami works better than cla.
we remain this as future work.
overall cla and clami show comparable results to sl rq1 and exp rq2 and outperform thd rq2 in recall f measure and or auc.
however in terms of precision cla and clami show the worst ranks although they are not statistically significant against sl and thd.
in terms of recall cla and clami show the best ranks compared to other approaches although there are no statistical significances against sl and exp.
menzies et al.
already discussed that prediction models with low precision and high recall are useful in many industrial situations .
in this sense cla and clami that only use a little knowledge about the defectproneness tendency of metric values show the potential for defect prediction on unlabeled datasets.
note that cla and clami do not need initially labeled instances and manual1 lmt nb lr bn rf j48 svm fig.
comparison of all classifiers against each other in auc.
approaches that are not significantly different the nemenyi test at p .
are connected.
lmt nb lr bn rf j48 svm fig.
comparison of all classifiers against each other in f measure.
approaches that are not significantly different the nemenyi test at p .
are connected.
effort but achieve comparable prediction performances to most baselines in terms of recall f measure and auc.
vi.
d iscussion a. performance on various classifiers we evaluate whether clami models work with other machine learning classifiers.
to build clami models we use bayesian network bn j48 decision tree j48 logistic model tree lmt logistic regression lr naive bayesian nb random forest rf and support vector machine svm which are widely used in defect prediction .
since we compare multiple classifiers we conduct the friedman test with the nemenyi test.
fig.
visualises the results of post hoc tests by the nemenyi test after the friedman test the p value was .
in auc .
nb and lmt show better average ranks than svm and j48 in terms of auc.
however for rf bn and lr it is difficult to conclude that their average ranks are different from other classifiers with statistical significance.
the average aucs are .
bn .
j48 .
lmt .
lr .
nb .
rf and .
svm .
most aucs are around .
except svm.
fig.
shows the results of post hoc tests by the nemenyi test after the friedman test the p value was .
in fmeasure .
the average f measures are .
bn .
j48 .
lmt .
lr .
nb .
rf and .
svm .
most f measures are around .
except svm.
ghotra et al.
compared various classifiers for defect prediction .
svm was one of the lowest ranked classifiers in their empirical study.
in this sense the low ranks of clami models built by svm confirm their study .
b. performance on various cutoffs to decide the higher metric values we apply various cutoff values n th percentiles where nis .
.
.
and as well as the first and third quartiles 25th and 75th percentile .
in total we use percentiles p10 for the 10th percentile p20 p25 the first quartile p30 p40 p50 median p60 p70 p75 the third quartile p80 and p90.
p25 p70 p80 p50 p60 p30 p20 p90 p75 p40 p10 fig.
comparison of clami models using various cutoffs in fmeasure.
approaches that are not significantly different the nemenyi test at p .
are connected.
p25 p70 p80 p50 p60 p30 p20 p90 p75 p40 p10 fig.
comparison of clami models using various cutoffs in auc.
approaches that are not significantly different the nemenyi test at p .
are connected.
as shown in fig.
and the median cutoff threshold p50 shows the best ranks in f measure and auc although clami with p50 does not outperform that with most other cutoff thresholds no statistical significance .
cla shows the similar results on various cutoffs the p40 .
and p50 .
show the best ranks in f measure.
in this sense we suggest using a median metric value as the threshold for cla clami in the very early stage of the software development phases when there is no information about the best threshold for cla clami.
however as exp results show human effort is helpful to achieve better prediction performance.
in this sense cla clami with additional human effort to decide a proper threshold might lead to better prediction performance as well.
then the cutoff thresholds for cla clami can be properly set by software engineers by using related projects that have similar distributions.
thus we have a plan to extend cla clami models with human effort as future work.
c. metric distribution analysis of datasets we investigate whether each metric is correlated with defect proneness by observing the distributions of metric values of buggy and clean instances.
in fig.
the box plots compare the distributions of the metric values of the safe dataset.
since the safe dataset has metrics there are pairs of plots in fig.
.
a pair of plots shows two distributions of buggy or clean instances for one metric respectively.
the distributions of metric values of buggy instances are plotted in gray while those of clean instances are plotted in white.
the solid horizontal line in a box represents the median value in each distribution.
the top and bottom of boxes represent the third and first quartiles respectively.
individual points in fig.
are outliers.
we normalized all metric values to compare the distributions of metrics in the same scale normalized metric values in fig.
.
the distributions of individual metrics in fig.
show different tendencies of defect proneness.
for example the metric m18 shows a high degree of discrimination between buggy and clean instances.
if we classify instances as buggy0.
.
.
.
.
m01m02m03m04m05m06m07m08m09m10m11m12m13m14m15m16m17m18m19m20m21m22m23m24m25m26metricsnormalized metric valueslabelbuggycleanfig.
distributions of metric values of buggy and clean instances in safe.
when the normalized metric value of m18 is greater than .
about of buggy and clean instances can be classified correctly.
the higher metric value of m18 implies more defect proneness.
however metrics such as m06 have less discriminative power between buggy and clean instances as shown in fig.
.
thus m06 may be a less effective predictor for the safe dataset when building a prediction model.
the defect proneness tendency of the metric m22 is reversed its higher metric values show less defect proneness.
in clami metrics such as m22 are automatically ignored since a majority of values in m22 are considered as violations by clami.
the supervised models wpdp for the safe dataset are built using all metrics so that some metrics that do not have discriminative power in terms of defect proneness can degrade the prediction performance of the models.
the metrics m06 m08 and m22 have relatively low discriminative power as there is little difference in the defect proneness tendencies.
the clami models are built using around six metrics on average of the safe dataset since clami applies metric selection to generate a training dataset.
the most frequently selected six metrics from the safe dataset are m13 m15 m16 m18 m23 and m25.
in fig.
these six metrics show a clear tendency of defect proneness in safe.
in other words the clami models can be constructed with metrics that have more discriminative power than using all metrics that also include several metrics with relatively low defect proneness tendencies.
this can be a major reason clami models could outperform supervised learning models in many project datasets as shown in table ii.
there are previous studies where defect prediction performance can be improved further by using a small subset of selected metrics .
our study also confirms these studies on the impact of metric selection.
we observe similar trends with other datasets except for lucene after investigating the distributions of the metrics of each dataset.
in the case of lucene the selected metrics by clami do not follow the defect proneness tendency of the typical defect prediction metrics.
for this reason clami does not outperform the supervised learning models in f measure and auc as shown in table ii.
d. threats to validity we carefully chose publicly available defect datasets such as netgene and relink dataset groups that were generatedusing manually verified issue reports and links between issue reports and code changes respectively.
however netgene datasets may have the quality issue of links between the manually verified issue reports and code changes since linking issue reports and code changes was still conducted automatically .
in the case of relink the issue reports were not manually verified although linking the issue reports to code changes was conducted manually .
validating cla clami with more reliable datasets generated with manual effort may be needed.
however to the best of our knowledge these two defect dataset groups were generated with manual verification compared to other available datasets.
the cla clami approaches are evaluated on defect datasets from seven open source projects.
thus the generalization of our results may be an external threat.
however we observe the potential of cla clami as they can work well on datasets that follow the design rationale of defect prediction metrics.
however cla clami may not work on the datasets that do not follow the defect proneness tendency of typical defect prediction metrics.
since this can be a limitation of cla clami we have a plan to conduct additional experiments on various defect datasets as future work.
we implement thd and exp approaches as real experts know the correct knowledge about threshold values and cluster labels.
however our implementation may not be the same as the real experts work so there could be a bias to compare cla and clami to thd and exp.
vii.
c onclusion enabling defect prediction for new projects or projects with limited historical information has long been a challenging issue.
to address this limitation we proposed cla clami which can build a prediction model on unlabeled datasets in an automated manner.
in our empirical study on seven open source projects cla clami models led to better or comparable results to typical defect prediction models and other baseline approaches in most projects in terms of recall f measure and or auc.
in addition we observed clami models work well on project datasets just using a small subset of selected metrics that follow the typical defect proneness tendencies of the datasets.
this result implies that in practice cla and clami have the potential for defect prediction on unlabeled datasets without need for manual effort.
to evaluate the applicability of our approach in industry we plan to apply cla and clami to proprietary software projects.