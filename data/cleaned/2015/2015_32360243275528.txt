building lean continuous integration and de livery pipeline s by applying devops principles a case study at varidesk vidroha debroy varidesk inc. coppell texas usa vidroha.debroy varidesk.com senecca miller varidesk inc. coppell texas usa senecca .miller varidesk.com lance brimble varidesk inc. coppell texas usa lance.brimble varidesk.com abstract continuous integration ci and continuous delivery cd are widely considered to be best pra ctices in software development.
studies have shown however that adopting these practices can be challenging and there are many barriers that engineers may face such as overly long build times lack of support for desired workflows issues with configuration etc.
at varidesk we recently began shifting our primary web application from a monolithic to a micro services based architecture and also adapted our software development practices to a im for more effective ci cd.
in so we also ran into some of the same afore mentioned barriers .
in this paper we focus on two specific challenges that we faced long wait times for builds release s to be queued and completed and the lack of support for tooling especially from a cross cloud perspective.
we then present the solutions that we came up with which involved re thinking devops as it applied to us and re building our own ci cd pipelines based on devops supporting approaches such as containerization infrastructure as code and orchestration.
our re designed pipelines have led us to see speed increases in terms of total build release time in the range of 330x x and have enabled us t o seamlessly move from a single cloud to a multi cloud environment with no architectural changes to any apps .
ccs concepts software and its engineering agile software development software configuration manageme nt and version control systems keywords continuous integratio n continuous deployment continuous delivery devops software build software release acm reference format vidroha debroy senecca miller and lance brimble.
.
building lean continuous integration and delivery pipelines by applying devops principles a case study at varidesk.
in proceedings of the 26th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november lake buena vista fl usa.
acm new york ny usa pages.
introduction contin uous integration ci which automates software build tasks such as compilation and testing is rapidly increasing in popularity.
when c ombined with continuous delivery cd1 which can automate the deployment of software the overall time between when co de is completed and made available to end users customers is greatly reduced.
reports show that ci cd can help companies for example flickr to deploy software more than times per day and facebook to increase the frequency of mobile deployments .
there are also additional benefits such as shorter feedback loops and in the case of hp s laserjet firmware division development costs per program went down .
having ci cd pipelines is thus considered to be a best practice and in fact an essential practice from the perspective of devops .
the state of devops report also finds these practices to be consistently indicative of high it performance .
varidesk offers a multitude of active workspace solutions for home and office spaces and our customers include organizations such as microsoft verizon state farm and walmart to name just a few .
varidesk has a multi national presence and offers businesses and general consumers localized products and several purchase options via our website .
the current complete web application is built on top of a commercial off the shelf cots content management system cms which also handles many responsibilities pertaining to ecommerce and communication with our enterprise resource planning erp system and therefore our cur rent web application is somewhat monolithic in terms of its design .
recognizing the value of ci cd we have had a pipeline for this web application in place right from the start but given the application s monolithic nature the pipeline itself is also simple because even though we have multiple environments such as development staging and production in the end it has always been just one app and this has made builds releases and the supporting infrast ructure easy to change update and maintain.
we recently decided to move towards a micro services based architecture decomposing our web application into many smaller and more focused services .
this however meant that each service should be build able and deliverable independent of cd is often also used to denote continuous deployment.
we direct readers interested in learning more about the subtle distinction between continuous delivery and continuous deployment to the post at .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cit ation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permiss ion and or a fee.
request permissions from permissions acm.org.
esec fse november lake buena vista fl usa association for computing machinery.
acm isbn ... .
esec fse november lake buena vista fl usa vidroha debroy senecca miller and lance brimble the other which meant in dividual integration and delivery pipelines while also retaining the ability to build and release everything en masse .
consequently t he n umber of build and release definitions would increase significantly and the infrastructure that was utilized may no longer be sufficient.
at the same time we also decided to have our new system on a multi cloud environment which wasn t the case before meaning that our pipeline s had to support this and not make any cloud specific assumptions .
this created a number of challenges as we were not able to build and release speedily and at scale across multiple clouds f urther discussed in section using the existing infrastructure and tooling we had in place.
a literature survey also revealed that ci itself and cd too has received little attention from the research community and that despite the general adoption of these practices many unanswered questions still exist along with barriers that software engineers have to face when using ci and associated tools .
in fact a number of the barriers discovered by studies such as via survey are precisely the ones that we faced.
ultimately we were able to get past these barriers and build out lightweight and flexible ci cd pipelines by the application of devops prin ciples to the pipelines themselves .
more concretely we leveraged ideas such as c ontainerization and orchestration to create the infrastructure that supports these pipelines.
this was novel from our perspective because our initial goals had only been to apply these ideas to manage the apps themselves at runtime.
given the successes that we have seen that we directly owe to this strategy we wanted to share our findings with the larger community with the hope that they will be beneficial to academia and industry alike.
specifically the contributions made by this paper are we independently confirm the findings of last year s research study while we were not part of the authors original survey we acknowledge that we faced many of the same barriers identified in the study when using ci.
we focus on barriers long wait times for builds releases to be queued completed and the lack of support for tooling2 and share solutions that we applied providing as much transparency and detail as we can.
we issue a call for a closer collaboration between industry and academia on researching ci and cd and increasing the knowledge base.
as pointed out in with no dat a and many unanswered questions decisions are often made based on folklore.
by sharing our experiences we want to do our part to address this.
the remainder of this paper is organized as follows.
section presents background information followed by section which provides technical details on the changes we made to our ci cd pipelines per devops principles .
section details our results and observations followed by our conclusions3 in section .
the study in identified several barriers some distinct but related.
for example lack of support for the desired workflow and lack of tool integration .
we generalize these together as lack of tool support it allows for brevity without sacrificing content or clarity for our paper .
as shown in studies such as and all within the last years there has been very little work done in this area.
furthermore there has been background herein we detail our existing c i setup and infrastructure that worked for our monolithic app clarify the goals with respect to the new micro services and discuss the reasons why the existing setup would be problematic toward achieving our goals.
.
existing prior setup build release definitions visual studio team services vsts an offering by microsoft has been our one stop shop for source version control task planning and bug tracking as well as the repository for our build and release definitions .
each definition consists of one or more tasks each task in turn corresponding to a step in the ci or cd process which are made available in vsts when creating editing the definitions.
the intent of the tasks is to encapsulate the description for each desired step in terms of not just the action to be taken but also setting up pre requisites making sure that a minimum version of dependencies was available and such.
a reference list of common build release tasks available for use with vsts is available at us vsts pipel ines tasks ?view vsts.
tasks that an organization develops on its own can also be uploaded to the vsts marketplace to share with others or make available to just the organization s vsts account.
for our monolithic web app build release definitions using only standard vsts tasks had been sufficient.
build release infrastructure build steps compilat ion testing etc.
and release steps deployment app restart etc.
need to take place on some infrastructure be it a physical machine or a virtual machine somewhere and microsoft abstracts this away to the concept of an agent installable software that runs one build or deployment job at a time .
microsoft then offers the ability to use their hosted agents which is where they provide the machine power and take care of maintenance and upgrades for a certain price point or the ability to self host agents where organizations can install the agent sof tware on their own linux macos or windows machines.
from a pricing perspective running on one s own machines was most appropriate for us and thus we opted to use self hosted agents instead of the microsoft hosted agent.
note that each vsts account does come with the ability to build and release jobs for free for up to hours per month and so we did leverage this but only to the extent that it was free .
organizations such as ours that use vsts can therefore support their ci cd by using agents and since each agent runs one job at a time the more agents there are the more builds releases can be run simultaneously.
each agent can be grouped into a pool and whenever builds releases are queued an available agent from the pool can pick it up to process.
microsoft does support installing multiple agents on the same machine but they state that there are cases where much efficiency is not gained and that users may run into problems if concurrent build processes are using common dependencies such as npm packages for example one build might update a dependency while another build is in the middle of using it which could cause unreliable almost no industry academia collaboration and thus we have no dedicated section on related work.
852building lean continuous integration and delivery pipelines by ... esec fse november lake buena vista fl usa results and errors .
this applies to us as our current web app uses and new micro services will use npm packages.
from an infrastructure perspective this equated to one agent per machine for us and the setup we chose to go with was to install agents on virtual m achines vms that were hosted in the cloud in our case microsoft azure4 .
this would mean we could manage our vms easily via the azure user interfa ce ui and could also disable enable them at will without having to physically maintain any infrastructure.
we were also able to leverage other azure features such as virtual networks and policies to allow limit access to from these machines.
in the interests of transparency we reveal that we used vms with one agent each and each vm was sized at standard a2 with vpus and .
gb of memory running windows server .
for our monolithic web app a build infrastructure of vms sized at a2 running an agent each had been sufficient.
figure visualizes our setup and shows the various processes.
we show a simplified flow with the understanding that this varies from environment to environment for example there are more quality gates in upper environments as we get closer to production .
we also maintain focus on the apps pointing out that other aspects a lso come into play for example creating load balancing rules virtual networks etc.
.
in our case and we assume it would be for any organization it was cost effective to use the same infrastructure to execute both builds and releases though we point out that this was not a necessity.
.
desired goals herein we outline the desired goals that accompanied those we had for our updated micro service based system .
the reasons why we decided to switch to micro service s are beyond the scope of this paper thou gh it is worth mentioning that our old monolithic system was getting harder to extend and we were attracted by the micro service architecture in terms of the ability to scale and deploy units of the architecture independently.
when making such a big architectural change however there is a corresponding desire to also update augment in other areas simultaneously and we focus on those areas that are pertinent to ci cd and app run time.
specifically we wanted to .
build scale and deploy each service independently.
.
define the above in terms of code i.e.
having the services define what their run time requirements were in a prescriptive fashion addressing dependencies along the way .
.
adopt devops principles such as using containers and orchestration to maintain fluidity of scale automatic app re launch in case of a crash injection of secret values on app launch via environment variables and so on .
for brevity we do not explain these concepts herein assuming that the r eader is fa miliar with them .
in the interests of transparency however we share that our goal was to have every build produce a docker5 image which wou ld then run as a container.
the various containers wou ld be managed using kubernetes6 which is an open source container orchestration system .
.
allow our apps to run cross cloud in the interests of reliability and cost effectiveness.
for example while we strive d for cross region support within cloud providers we also needed to be prepared for outages across a n entire cloud provider.
while we had only been running in azure earlier we now also wanted to be running in google cloud7 with amazon web services aws under possible future consideration.
.
problems faced long wait times for builds releases at least one of the problems becomes evident when we directly apply the existing ci cd setup as is to support the desired goals as shown in figure .
since every micro service has its own build and release definition and per the desired app decomposition each service can be worked on by different developers completely independent of on e another the number of builds and releases increases greatly and the build infrastructure the vms highlighted in the red box now becomes a bottle neck.
if both agents were busy with a current build or release then any more requests would just get queued and the queue itself docker figure the existing ci cd pipeline and infrastructure used to support the monolithic web app 853esec fse november lake buena vista fl usa vidroha debroy senecca miller and lance brimble would start growing.
even though the microsoft hosted agent could be leveraged as hours of time is available free of charge every month it did very little to alleviate how much time was spent just waiting for builds an d releases to start and then finish.
it was especially frustrating for the team all around when builds would be scheduled but would end up failing because of a unit test failure or because a file had not been checked in as such a build was not just bloc king others till it had finished but also would need to be re built itself once the error had been fixed.
we note that we had accounted for the potential need to scale our build release infrastructure both vertically adding more resources such as vcpus o r ram to each agent and horizontally adding more agents themselves during our initial process planning.
but given what we were now seeing in terms of the build release frequency should we scale to keep up some basic calculations already put us an order of magnitude over the budget we had set for the build r elease infrastructure.
it was not uncommon to see builds and releases queued for more than a couple of hours .
and scaling to try and keep up was going to put us way beyond our budget.
it was not plausible for us to spend that much additional money just on ci cd when all we were was trying to re architect our web app.
at the same time the problem was severe in terms of wall clock time lost just waiting for a build release .
we even tried to maximize our time on the hosted agent in one special case our development manager at the time reported waiting up to hours for a build to be queued on the hosted agent!
lack of tool support when we began architecting our new solution and developing the ci cd pipelines for it we did some basic proofs of concept including building out container images instead of just compiled binaries and pushing them to a container registry.
unfor tunately because we had not acquired the license for google cloud yet we did our initial assessment completely ba sed off the integration between vsts and azure.
as discussed in section .
tasks encapsulate steps in the build and release definitions and there was very conveniently a ta sk already available for use as shown in figure .
unfortunately it only became evident later that this task was only meant to support azure or docker container registries .
the task itself was non configurable with alternatives which meant we had a problem pushing to google container registry i.e.
we could not directly achieve the flow shown in figure .
figure adding a docker compose task in a build while searching for other options we discovered work being done by google to develop a vsts extension for deploying to google cloud but it is not available on the vsts marketplace as yet.
in fact the current official release is only at version .
.
which suggests that it is far from being available for general consumption.
a github repository9 exists for the project that is available which means we could contribute some code ourselves but that would also take time and effort that we had not budgeted for.
thus we fully concur with claim that lack of tool support presents a barrier to the adoption of ci cd .
at this point it looked like due to lack of tool support we were either going to have to give up google cloud as an option or create entirely new ci cd pipelines to support it.
cloud tfs figure applying the existing ci cd setup directly to support the micro service based architecture 854building lean continuous integration and delivery pipelines by ... esec fse november lake buena vista fl usa devops t o the rescue it has been acknowledged that there is no uniformly agreed upon definition of devops and that it can mean different albeit related ideas in different contexts.
there are some common principles however that seem to generally apply for example embracing ideas such as cont ainerization at a technical level and working toward cross collaboration among teams from a process perspective .
it had always been our intent to align ourselves with the devops rhythm while we were building out the micro services but the application of these very same principles also to the build release pipelines was very beneficial to us.
.
agents as orchestrated c ontainers recall from the discussion in section .
that while we could concurrently perform multiple builds on the same vm by installing multiple agents on it it could cause unreliable results and errors .
this has to do with the fact that each agent is really just an app that when running on the same kernel lowest level of the operating system along with another agent has the ability to interfere with it.
but containerization solves precisely this problem!
what if instead of just installing agents on vms we had our agents run in containers on a virtualized operating system?
the standard setup is contrasted with this approach in figure and it completely isolates one agent from another.
agent libraries kernelagent libraries kernelagent agent agentagent libraries agent librariesagent libraries a non containerize d b containerized figure .
non containerized vs containerized agents continuing to think along the lines of these devops principles allowed us to realize additional benefits as well.
we created our own custom docker image hosted in our own container registry that extended the standard microsoft agent image with other useful tools for example both the azure cli tools and the google cloud cli tools .
furthermore any time we wanted to update the image with more tools we would just update our own custom image and apply it to simply create new containers.
this is very different from the traditional setup where if new tools or dependencies needed to be brought into the vms then they would have to be done on a vm by vm basis.
additionally if microsoft ever updated the agent software then the currently running agents would have to be un installed one by one and then re installed whereas in our containerized case we would just update the common docker image once.
finally we used kubernetes to manage our running agent containers and thought of our build vms simply as a node clust er.
kubernetes allowed us to automatically bring up a new container should one of the existing containers fail.
it also allowed us to secret ize sensitive data such as access tokens used to communicate with vsts and the azure and google cloud container reg istries such that values were injected at container initialization and never stored in the base image in keeping with security best practices.
.
breaking down a vsts task addressing the problem of pushing images to google cloud s container registry was truly a cross collaborative effort between teams at varidesk another embodiment of devops principles.
at the time due to the limitations of the docker compose task offered on vsts discussed in section .
we either had to develop a separate pipeline independent of vsts which would lead to redundancy or try to add code to the task that had been started on by google which was far from ready or in the worst case give up on the idea of pushing to google cloud maybe switching to aws .
based on brain storming sessions between the development cloud and automation teams at varidesk considering the various alternatives we decided to focus most on the vsts docker compose task.
the underlying idea here was that this task already had the ability to push to docker and azure container registries fundamentally pushing to any container registry should follow the same steps and if we could mimic those steps and then adjust to support google cloud then we could create the missing logic ourselves.
and a n interesting decision was made at this time to not mimic the vsts dock er compose task as another custom vsts task on our side but rather to bring all the logic into a script that could be executed on the build release agents.
this directly plugged into the idea of using our own custom docker image for the container ized ver sion of the agents because by using this approach with kubernetes we could mount volumes with the necessary script files to load them into the container file system at runtime making our vsts task to push to google cloud a very simple one just invoke the script passing in the necessary parameters .
using the script which mimics the operations performed in the docker compose task we could now easily push the image to google cloud and there were additional benefits described in section .
our custom script was then exposed as a task in our own build definitions and only needed one input the name of the base docker image passed in via a parameter named docker image name at build time as shown in figure .
the same task is used across all our service builds just passing in a different image name for every build definition .
figure custom task as a script to p ush to google cloud results and observations the ability to push to google cloud via vsts is binary i n nature and thus we can simply say that we were able to do so with our changes to the pipeline whereas we were unable to do so without it.
we explored an alternative where we could listen in on check ins made to source code by registering for webhooks 855esec fse november lake buena vista fl usa vidroha debroy senecca miller and lance brimble but there was considerabl e lag and duplication of logic and so re thinking the vsts task as basic scripting was very beneficial.
it is also noteworthy that by decomposing the vsts docker task to a script we de coupled ourselves from a dependency on vsts tasks.
the same script that we used to push our images to google container registry can in fact be used to push to the azure container regi stry the only part that would change would be the authentication information which is injected into the containers and thus we don t need to rely on the vsts docker task at all.
using this approach we can also expand to aws irrespective of whether th ey offer support for vsts tasks or not.
interestingly enough this current setup ports over nicely to other build tools such as jenkins10 which means that in a way we are able to de couple ourselves from vsts for the purposes of ci cd entirely.
this makes our current ci cd pipeline very robust .
to assess whether containerized agents were helpful or not we looked at data from builds and releases of historical data decomposition into micro services had translated into different build and release definitions at the time of writing of this paper .
a reason for the difference between the number of builds and releases is that not every build completes successfully which prevents it from making to the release phase and to fix the is sue results in at least one other build typically more .
furthermore check ins into branches always results in builds but typically only incorporation into the mas ter branch trunk results in a release.
table comparing queue times for builds releases agents hosted basic containeriz ed avg.
queue time 1hr min .
min .
seconds table presents the average time spent by a build or release in the queue i.e.
just waiting to be processed where the hosted column represents using the vsts hosted agent the basic column r epresents using non containerized agents and the containerized column represents our approach with the same infrastructure capacity as basic described in section .
with containerized agents in all managed by kubernetes on just vms .
we cannot speak to the size capacity of the hosted agent as that information is not provided by vsts.
the time spent in the queue for the basic approach is about times that of the containerized approach and similarly the queue time using the hosted a gent is times that of the containerized approach which translates to significant time saved.
since all of the infrastructure is managed without any new cost incurred yet the throughput is high our ci cd pipeline is very lean.
ci cd pipelines are an essential devops practice.
but by applying devops principles to the construction of the ci cd pipeline itself we were able to make it very lean and robust.
conclusions continuous integration ci and continuous delivery cd are widely considered to be best practices .
yet little research has been done in this area and very recent studies have shown that there are gaps in the knowledge base and barriers that prevent adop tion of ci cd at organizations.
we ran into some of these barriers such as long build release wait times and lack of tool support at varidesk while migrating from a monolithic web app to a more micro services based architecture and trying to go cross cloud.
thus we ratify the results from earlier studies in that such barriers do indeed exist.
we were able to get around these barriers by applying devops principles such as containerization orchestration and cross collaboration among teams to ultimately develop a lean and robust ci cd pipeline which has shown to be very performant and suitable f or our needs.
in so we also achieved de coupling and portability which are good software engineering principles .
we describe our solutions in as much detail as we can to support the development of the knowledge base in the areas of ci cd and enco urage further collaboration between academia and industry.
future work includes researching how to automatically scale the number of build release agents up and down based on the current build release load.
while kubernetes makes the scaling easy we do no t as yet have a way to query the load and respond in real time to add more or even reduce the number of agents.
our current customized docker image for our build release agent has proprietary information which we are removing and the same applies to th e current scripts that were developed in house.
once done we will make these freely available to the larger community.