syntactic and semantic differencing for combinatorial models of test designs rachel tzoref brill school of computer science tel aviv university and ibm research israelshahar maoz school of computer science tel aviv university abstract combinatorial test design ctd is an effective test design technique considered to be a testing best practice.
ctd provides automatic test plan generation but it requires a manual definition of the test space in the form of a combinatorial model.
as the system under test evolves e.g.
due to iterative development processes and bug fixing so does the test space and thus in the context of ctd evolution translates into frequent manual model definition updates.
manually reasoning about the differences between versions of real world models following such updates is infeasible due to their complexity and size.
moreover representing the differences is challenging.
in this work we propose a first syntactic and semantic differencing technique for combinatorial models of test designs.
we define a concise and canonical representation for differences between two models and suggest a scalable algorithm for automatically computing and presenting it.
we use our differencing technique to analyze the evolution of real world industrial models demonstrating its applicability and scalability.
further a user study with ctd practitioners shows that comprehension of differences between real world combinatorial model versions is challenging and that our differencing tool significantly improves the performance of less experienced practitioners.
the analysis and user study provide evidence for the potential usefulness of our differencing approach.
our work advances the state of the art in ctd with better capabilities for change comprehension and management.
i. i ntroduction one of the effective techniques for coping with the verification challenge of increasingly complex software systems is combinatorial test design ctd a.k.a.
combinatorial testing .
ctd requires a manual definition of the test space in the form of a combinatorial model consisting of a set of parameters their respective values and constraints on the value combinations.
a valid test in the test space is defined to be an assignment of one value to each parameter that satisfies the constraints.
a ctd algorithm automatically constructs a subset of the set of valid tests so that it covers all valid value combinations of every tparameters where tis usually a user input.
this systematic selection of tests is based on empirical data that shows that in most cases the appearance of a bug depends on the interaction between a small number of features of the system under test .
an under explored challenge for wide deployment of ctd in industry is the manual process for modeling and maintaining the test space.
in practice creating a ctd model is not a one time effort when the system under test evolves so shouldthe models.
in face of the move to agile methodology and to continuous delivery mode where software development cycles are getting ever shorter test design needs to frequently adjust to changes which in the context of ctd means frequent model definition updates.
however although in these settings technologies for handling model changes are increasingly necessary we are unaware of any work that reasons about the evolution process of combinatorial models or provides tool support for it.
a recent survey by nie et al.
reveals that only around of the publications on ctd explore the crucial modeling process and the topic of model maintenance is not even mentioned in .
close to ctd tools are listed in e.g.
pict acts jenny and aetg but to the best of our knowledge none of these existing tools provides indication on the effect of change operations on the model i.e.
what is the relation between the original model and the new one and how they differ.
without such tool support the practitioner is left in the dark as to whether the performed change will result in the intended effect and what other changes may be required.
when a series of such change operations is performed as is typically the case this problem exacerbates.
in this work we propose a first approach for syntactic and semantic differencing of combinatorial models.
computing and representing the differences between models are challenging tasks.
first some parts of a model can implicitly change due to explicit changes to other parts and thus the resulting differences will not be revealed by a purely syntactic comparison.
for example adding a new value to a parameter that appears in the constraints can result in new tests being defined as invalid even without any explicit changes to the constraints.
second there can be different syntactic representations to the same model i.e.
representations that result in the same set of valid tests .
specifically it is challenging to clearly and concisely represent differences between constraints which are propositional logic formulas over multi valued parameters.
finally as our evidence from the field shows real world models can be huge thus computing a semantic differencing must scale well in order to be used in practice.
our work addresses these challenges by proposing a canonical representation of a combinatorial model in terms of the value combinations that are excluded from its set of valid tests.
the importance of this representation is in its use as a basis for a comparison between models.
other non2017 ieee acm 39th international conference on software engineering ieee acm 39th international conference on software engineering .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
canonical representations cannot be used for comparison.
our differencing approach consists of two parts.
the syntactic differencing shows the additions and removals of parameters values and constraints.
the semantic differencing concisely computes and presents the differences in the set of valid tests based on the canonical representation of the models.
to scale the computation to real world model sizes we use an efficient representation of the sets of valid tests and their differences which is based on binary decision diagrams a compact data structure for representing and manipulating boolean functions.
it is important to note that differencing of ctd models isnot a mere theoretical exercise.
first the model and not the test suite that is directly and automatically derived from it is the main artifact that ctd test designers create read revise and maintain.
second the cost of an incorrect model update can be very high if the model change is incomplete then new tests generated from the model will not adequately cover the software change if the model change is incorrect it may result in generating redundant or erroneous tests.
finally as we will demonstrate in section iii ctd model updates can be quite tricky due to the implicit dependencies between the different parts of the model.
thus in this work we focus on differencing at the model level.
our differencing technique serves as a review tool for practitioners to verify that their model updates are complete and correct.
we implemented the differencing technique within the industrial strength commercial ctd tool ibm functional coverage unified solution ibm focus .
ibm focus has been in use already for several years by hundreds of ctd practitioners inside and outside of ibm.
to evaluate the applicability of our ideas and implementation to real world models and their versions we applied our differencing technique to real world industrial models with a total of versions versions per model commits .
our analysis reveals that while real world commits of combinatorial models of test designs tend to be large and complex in practice our approach provides acceptable performance times in almost all cases.
we further evaluated the effect of our differencing technique on users by conducting a user study with ctd practitioners on two real world models each consisting of two real world versions.
the results show that comprehension of differences between real world combinatorial model versions is challenging and that our differencing tool improves the performance of practitioners.
this was most evident for the less experienced practitioners whose score improved by over percent when using our differencing tool.
to conclude our contributions are as follows a first approach for syntactic and semantic differencing of combinatorial models of test designs a scalable implementation for our differencing technique implemented in an industrial strength commercial ctd tool the application of the new differencing technique to real world industrial models with a total of versions and a user study with ctd practitioners that shows that our differencing technique significantly improvesthe performance of less experienced practitioners in correct comprehension of differences between real world model versions.
the proposed differencing technique advances the stateof the art in ctd with new tools for change comprehension and management.
ii.
b ackground we provide background on combinatorial models and their semantics and on the use of binary decision diagrams to represent them.
combinatorial models and their semantics .
a combinatorial model is defined as follows.
let p p1 ... p n be a labelled set of parameters v v1 ... v n a labelled set of finite value sets where viis the set of values for pi and ca set of boolean propositional constraints over p. a test v1 ... v m where i vi vi is a tuple of assignments to the parameters in p. the semantics used in practice by ctd tools is boolean semantics.
in this semantics a valid test is a test that satisfies all constraints in c. the semantics of the model is the set of all its valid tests denoted by s p v c .
using binary decision diagrams to represent combinatorial models .
in a compact representation of combinatorial models using binary decision diagrams bdds was presented.
bdds are a compact data structure for representing and manipulating boolean functions commonly used in formal verification and in logic synthesis .
utilizes the efficient computation of boolean operations on bdds such as negation conjunction and disjunction to compute the bdd representing the set of valid tests from the user specified constraints.
the set of invalid tests in the model is represented using the conjunction of the bdds for each of the constraints.
multi valued parameters are handled using standard boolean encoding and reduction techniques to bdds .
the set of valid tests is represented by the negation of the bdd for the invalid tests conjunct with a bdd that represents the legal multi valued to boolean encodings of the parameter values.
this bdd based representation of the combinatorial model is the basis for the implementation of our semantic differencing.
iii.
r unning example and overview we start off with an example and overview of our work.
the presentation in this section is semi formal.
formal definitions appear in section iv.
table i depicts the parameters values and constraints of a combinatorial model for an on line shopping system which we use as a running example.
the model defines the test space and which tests in it are valid.
for example the test is instock os air dt immediate is valid while the test is instock os ground dt immediate is invalid.
below we follow a series of updates to the model inspired by similar updates we have seen in the evolution of real world models adding a value updating a constraint splitting a parameter etc.
we describe the updates discuss their semantics authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table i example on line shopping model parameter values itemstatus is instock outofstock nosuchproduct ordershipping os air ground deliverytimeframe dt immediate oneweek onemonth constraints dt immediate os air dt onemonth os ground and demonstrate how our differencing solution handles them.
the updates are relatively small and local.
we use them to demonstrate the basic principles of our analysis as well as the challenges associated with differencing even with such seemingly simple updates.
from v1 to v2.
following the addition of a new feature to the system a practitioner added the value sea to the parameter ordershipping and committed a second version of the model.
one may view this form of change as an extension where parts are added to the model to describe the test space in more detail.
figure depicts the result of our differencing analysis between the first two versions of the model.
the differencing consists of two parts syntactic and semantic.
the syntactic part reports the changes that were made to the parameters constraints and values.
as expected in our example it reports solely on the addition of the sea value to the os parameter.
the second semantic part reports the changes in the set of valid tests in terms of its strongest exclusions .
a strongest exclusion is a combination that is excluded by the constraints in the model and thus does not appear in any valid test but whose every strict subset is included in the model.
the motivation for showing the changes to the strongest exclusions is twofold.
first they constitute a canonical representation of the test space as we will show in section iv and therefore can be used as a basis for comparison between different test spaces.
second they represent the tests that were excluded from the model in a concise form and thus can be used to provide information to the practitioner about the differences between the test spaces.
note that while the constraints were not explicitly updated following the addition of the sea value i.e.
syntactically the constraints are identical in the two versions our analysis reveals that two new strongest exclusions were added to the model os sea dt immediate and os sea dt onemonth .
from v2 to v3.
when reviewing these added exclusions the practitioner may have realized that the constraints need to be updated as well in order to reflect the intended use of the new feature.
thus she deleted the second constraint and wrote a new one instead dt onemonth os ground os sea and committed a third version of the model.
when the differencing analysis is performed between the third and the second versions the strongest exclusion os sea dt onemonth will be marked as removed since it is no longer an exclusion in the third version.
one may view this form of change where a strongest exclusion becomes included in thetest space without being part of a new strongest exclusion as acorrection .
from v3 to v4.
after further inquiries the practitioner realized that delivery time frame of one month actually consists of two different values 6to10workingdays and over10workingdays which represent two separate logical paths of the application under test.
thus she replaced theonemonth value with these two values deleted the second constraint and wrote a new one instead dt 6to10workingdays dt over10workingdays os ground os sea and committed a fourth version of the model.
figure depicts the result of our differencing analysis between the third and fourth versions of the model.
one may view this form of change where a parameter or a value is divided into several different cases as a split .
the split of the value is displayed on the left side and the split of the strongest exclusion is displayed on the right side where the original one is removed and replaced with new strongest exclusions one for each case.
the split pattern is easy to detect when viewing the changes to the strongest exclusions whereas it becomes less obvious when viewing the removed and added constraints.
furthermore a constraint can have different syntactic representations that are semantically equivalent.
for example the second constraint in the fourth version could have been dt negationslash immediate dt negationslash oneweek os ground os sea resulting in a syntactically different yet semantically equivalent model.
the model with this representation of the second constraint would completely hide the fact that it splits onemonth into two new values as these values do not even appear in the constraint.
moreover this version of the constraint can be used also in the third version of the model in which case no explicit changes would have been made to the constraints when moving to the fourth version though the split of the strongest exclusions would still occur.
iv .
f ormal solution for model differencing we now formally present our proposed differencing technique for combinatorial models followed by a description of our algorithm for computing it.
throughout this section we will use the notations defined in section ii and demonstrate the ideas on the running example presented in section iii.
a. model differencing definitions given a combinatorial model s p v c for every parameterp p we define p.name to be its identifier.
similarly for every vi v v.name denotes the identifier of every value v vi.
we define corresponding sets for the identifiers of the parameters and their values pn p.name p p and vi v vni v.name v vi .
for every constraint c c we define c.expr to be its boolean expression.
we define the set ce c.expr c c for the expressions of the constraints.
given two models s1 p1 v1 c1 and s2 p2 v2 c2 we define a syntactic differencing diffsyn s1 s2 and a authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
differencing between the first and second versions.
on the left the syntactic differencing of parameters constraints and values.
on the right the semantic differencing of strongest exclusions.
fig.
differencing between the third and fourth versions.
on the left the syntactic differencing of parameters constraints and values.
on the right the semantic differencing of strongest exclusions.
complete semantic differencing diffcsem s1 s2 between the two models as follows.
syntactic differencing diffsyn s1 s2 consists of the following parts parameter additions p p.name pn1 pn2 parameter removals p p.name pn2 pn1 constraint additions c c.expr ce1 ce2 constraint removals c c.expr ce2 ce1 v alue additions uniontext pi p1 pj p2 v v v1i v.name vn1 i vn2 j pi.name pj.name v alue removals uniontext pi p1 pj p2 v v v2j v.name vn2 j vn1 i pi.name pj.name for example the left side of figure presents the syntactic differencing between v3and v4of the on line shopping model which consists of the addition of two values to the common parameter dt the removal of one value from it no parameter additions or removals the addition of one constraint and the removal of another.
semantic differencing while syntactic differencing reasons about the syntactic changes in the parameters and values complete semantic differencing reasons about all the differences in the test space of s1ands2.
to enable a comparison of the two test spaces we first define a concise and canonical representation of a test space.
the representation we propose is based on the notion of strongest exclusions .
a strongest exclusion is a combination that is excluded by the constraints in the model and thus does not appear in any valid test but whose every strict subset is included in the model and thus appears in at least one valid test.
the set of strongest exclusions of a model together with the set of parameters and their values uniquely define the test space of the model.
theorem .
letse be the set of strongest exclusions of a combinatorial model s p v c .
then p v se is a canonical representation of s p v c .
proof we will show that for two models s1 p v c1 ands2 p v c2 i fs1 s2 they have the same set of valid tests then se1 se2.
assume to the contrary i.e.
that there exists a strongest exclusion e se1such that e negationslash se2.
then it follows from the definition of a strongest exclusion that one of the following holds estrictly contains a strongest exclusion e prime se2.
since s1 s2 it must hold that e prime authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
is excluded from s1 contrary to our initial assumption that e se1.
eis not an exclusion in s2 which means there is a test t s2that contains e. since s1 s2 it holds that t s1 contrary to our initial assumption that e se1.
for example for the on line shopping model from section iii sev1 os air dt onemonth os ground dt immediate .
note that each strongest exclusion represents multiple complete tests that are excluded from the model.
note also that each constraint defines one or more strongest exclusions.
diffcsem s1 s2 consists of the following parts strongest exclusion additions e e se1 se2 strongest exclusion removals e e se2 se1 .
for example for the on line shopping model sev2 sev1 os sea dt immediate os sea dt onemonth .
hence diffcsem v2 v1 contains the latter two strongest exclusions as depicted on the right side of figure .
the canonical representation of a model using strongest exclusions allows us to define the completion level cl of a version of a model as well as of a commit differencing two model versions .
the cl of a model version is the maximal arity of a single strongest exclusion.
it indicates the maximal depth of exclusions that needs to be explored in order to reach a complete canonical representation of the model.
in our running example the cl of v5is v5 s canonical representation includes no strongest exclusions of arity larger than .
cl is extended naturally from a single model version to the comparison of a commit consisting of two model versions.
specifically the cl of a commit is the maximum between the cls of the two model versions before and after the commit.
it represents an upper bound on the maximal number of parameter values appearing in all strongest exclusions required for presenting all the commit differences.
the cl of a commit is only an upper bound on the largest strongest exclusion that will appear in the diff because the diff contains only strongest exclusions that are not common to both model versions.
as in the single model case we use the cl of a commit as an indicator for the maximal depth that needs to be explored in order to reach a complete differencing between two model versions.
in section iv b we present a way to compute the cl of a model and of a commit .
while presenting the differences in terms of strongest exclusions is purely semantic and independent of the syntactic representation of the model it is also beneficial to link semantic information to syntactic information.
as shown in figures and we achieve this information linking by mapping the strongest exclusions that appear in the differencing back to their excluding constraints as specified by the ctd practitioner.
each strongest exclusion in diffcsem s1 s2 serves as a witness for the test space change induced by its excluding constraint under the changes to the model parameters and values.
for example in figure the strongest exclusion os sea dt immediate is mapped to its excludingconstraint dt immediate os air .
the information that this constraint is responsible for the added strongest exclusion is particularly valuable in this case since no explicit changes were made to the constraints in version v2to exclude this combination following the addition of the sea value.
discussion of alternatives.
a rather na ve approach for partial semantic differencing could have been to present complete tests that are valid in one version of the model and not in the other.
one problem with this approach is that since the versions may have different parameters and values the question whether a test in one version is valid in another version may not be well defined i.e.
the constraints in one version may refer to parameters and values that do not exist in the other version and vice versa.
another problem with this na ve approach is that it must be partial since the number of such complete tests may be huge.
our solution avoids these two problems.
first by computing the strongest exclusions in each model and then comparing the two resulting sets of value combinations our differencing is well defined.
second since each strongest exclusion represents numerous complete tests that are excluded from the model our solution concisely represents all the test space differences between the two models and is orders of magnitude smaller than one that relies on complete tests.
another alternative approach to the use of strongest exclusions for semantic differencing could be to use partial inclusions or strongest inclusions.
partial inclusions are value combinations that are contained in at least one valid test.
strongest inclusions are value combinations whose every extension to a complete test forms a valid test.
however both options of using inclusions are significantly less informative than using strongest exclusions.
typically all parameter values of a model are valid in combination with at least one assignment to the other parameters.
hence all model values are partial inclusions.
a diff based on partial inclusions would simply result in the list of added and removed values making the semantic differencing identical to our syntactic one.
moreover partial inclusions do not form a canonical representation of a model.
strongest inclusions form a canonical representation but are usually complete tests.
thus a diff based on strongest inclusions will result in a significantly larger and less concise diffcsem s1 s2 than when using strongest exclusions.
reducing the size of the semantic differencing while a complete semantic differencing is desirable it is also beneficial to reduce the amount of strongest exclusions that the practitioner needs to review.
two techniques to achieve this are filtering out redundant information and dividing the presented information into categories.
derived exclusions.
redundant information may exist in diffcsem s1 s2 due to derived exclusions .
a derived exclusion is an invalid value combination i.e.
it is excluded from the model that is excluded not due to any single constraint but rather due to the interaction between different constraints.
for example consider the first constraint in table i and assume we add a third constraint to the model os air is instock .
the interaction of the two constraints yields a de623 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
rived exclusion dt immediate is outofstock which is not directly excluded by any of the three constraints.
some of the strongest exclusions that appear in the diff may be derived exclusions and thus contain redundant information the fact that they are excluded can be concluded from other excluded combinations in the diff.
to eliminate this redundancy and reduce the amount of information in diffcsem s1 s2 we remove derived exclusions from the result of the comparison between se1andse2.
note that due the removal of derived exclusions diffcsem s1 s2 is no longer canonical since the same exclusion can be derived in one model and not in another semanticallyequivalent model i.e.
that has the same set of valid tests depending on the syntactic representation of the constraints.
we note however that the importance of strongest exclusions as a canonical representation of a model is for comparison purposes other non canonical representations cannot be used for comparison and reduction to a non canonical form is done only on the diff results after the comparison for presentation purposes.
removing derived exclusions has two important advantages.
first it enables tighter linkage of the semantic information to syntactic information because each strongest exclusion presented in diffcsem s1 s2 can be linked to a single userspecified constraint that excludes it as presented in figures and .
second it filters out redundant information from the presentation and reduces the number of exclusions that the practitioner needs to review.
categorization.
another technique that helps with the analysis of the diff is to divide the computed information into categories.
we observe that most of the analysis effort concentrates on the strongest exclusions that contain common parameters because they indicate changes in the parts of the test space that belong to both model versions.
when a set of parameters is removed from the model it is natural that all the constraints on their inter relations are also removed from the model and similarly for the case of parameter additions.
to this end and for clarity of presentation we divide the semantic diff view into three separate views additions and or removals that are defined on at least one common parameter which is the main diff view additions that are defined only on added parameters and removals that are defined only on removed parameters.
in all our examples from section iii all exclusions differences are of the first type hence only the main view is shown.
note that an added or removed strongest exclusion may be defined both on common parameters and on parameters unique to one model in which case it will be only presented in the main view.
b. computing the differencing while computing diffsyn s1 s2 is straightforward and can be achieved by a simple traversal over the parameters values and constraints of the two versions computing diffcsem s1 s2 is much more challenging.
specifically it requires computing the set of strongest exclusions se of ainput the bdd va l i ds1of all valid tests of s1.
the bdd va l i ds2of all valid tests of s2.
the set of constraints c1ofs1.
the set of constraints c2ofs2.
output the bdd seadded of strongest exclusions that are in s1 and not in s2.
the bdd seremoved of strongest exclusions that are in s2and not in s1.
1init completed fa l se 2completed fa l se 3se1 newlist 4se2 newlist 5level 6while completed completed do fori 2do if completedi then computese va l i dsi level sei if logicalortext sei va l i dsithen completedi tr ue end end end level 16end 17seadded se1 se2 18seremoved se2 se1 19removederived seadded c1 20removederived seremoved c2 algorithm semantic diff of combinatorial models model version.
to achieve efficient computation of se we use a symbolic computation based on binary decision diagrams bdds as our primary data structure for the set of valid tests as well as for all other computed artifacts.
as explained in section ii to handle domains of discrete values rather than only boolean ones we use a bdd that represents the legal multi valued to boolean encoding of the parameter values.
for clarity of presentation we omit the handling of this standard encoding from the following pseudo code and accompanying algorithm description.
in algorithm we introduce a novel algorithm for computing the semantic differencing of two model versions.
the algorithm is iterative where in each step it computes the strongest exclusions up to arity level line until reaching the cl.
once reached it compares the two sets of strongest exclusions to identify their differences l. and removes derived exclusions from these differences l. .
bdds are used to represent the set of valid tests of each model version va l i d si to compute and represent the strongest exclusions of each version se1andse2 and their differences seadded andseremoved to check whether the cl has been reached l. and to remove the derived exclusions from the differencing results.
the algorithm relies on the efficiency of negation conjunction disjunction and existential quantification of bdds as explained in section ii.
algorithm presents the method computese which computes in each iteration the strongest exclusions of arity level for a given model.
for every set of parameters tof size level it computes the bdd excluded tof all exclusions on t. this is achieved by first computing the bdd of valid assignments tot which is the projection of the va l i d bdd on t and then negating the resulting bdd l. .
a projection on a set of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
parameters tis computed by existentially quantifying out the parameters not appearing in t. next to filter out exclusions on tthat are not strongest exclusions the method conjuncts the bdd excluded twith the negation of every strongest exclusion bdd from a level smaller than level whose parameters are contained in t l. .
thus all strongest exclusions for each set of parameters tare symbolically computed at once using bdd operations.
at the end of each call to computese the main algorithm checks whether the cl has been reached by checking whether the strongest exclusions collected so far represent the entire valid test space.
this is achieved by checking whether the disjunction of the strongest exclusions collected so far is identical to the bdd of invalid tests l. .
finally once the cl of both models is reached and hence the cl of the commit the iteration in algorithm terminates and the set of differences is computed.
the derived exclusions are removed from the result in the removederived method.
this is also computed symbolically for every set of parameters tof size up to cl by projecting each of the constraints on t and removing from the bdd of strongest exclusions for tall tuples that are not included in one of these projections using bdd conjunction and disjunction operations.
for additional details about the computation of derived exclusions see .
note that the removal of the derived exclusions cannot precede the identification of the exclusion differences because the same strongest exclusion can be derived in one version and not the other.
had the two operations been swapped such an exclusion would be falsely identified as a difference between the two versions.
in practice for test spaces with a huge number of parameters and high cls computese as described so far might require large memory or long computation time for computing the projections of the legal space bdd on all parameter tuples of size level .
this step line of computese is the main contributor to the complexity of our algorithm since for each model it requires parenleftbign k parenrightbig project operations on the bdd of the valid test space where nis the number of parameters and k is the cl.
thus we choose to limit the size of the bdds used during this computation.
if the intermediate bdds exceeds a given size threshold the iteration on the parameter tuples is interrupted and the result achieved so far from the previous level is used instead.
in such cases the user will be notified that the differencing is incomplete and that differences are shown only for strongest exclusions up to the last fully computed level.
in section v we will show that most realworld models we analyzed reach the cl.
of course a higher threshold could be used to allow the remaining models reach their cl.
v. e v alua tion we present an evaluation of our work in terms of the results of our differencing technique when applied to realworld model evolution.
we then continue with a user study evaluating the effectiveness of our differencing technique in helping ctd practitioners comprehend model changes.input the bdd va l i d sof all valid tests of s. the arity level of strongest exclusions to compute.
the list of bdds se of strongest exclusions for levels smaller than level .
output the list of bdds se of strongest exclusions for levels up to and including level .
1init t all parameter tuples of size level 2fort tdo excluded t project va l i d s t sizese size se fori ... s i z e s e 1do ift param se i then excluded t excluded t se i end end se se excluded t 11end algorithm computese a. real world evidence the research questions guiding our first evaluation are rq1a how do combinatorial models evolve in practice?
in particular what are the common kinds and sizes of changes?
rq1b how does our differencing computation perform on real model versions in practice?
to answer these questions we applied our differencing tool to the evolution of a large corpus of real world models.
models used and setup we applied our differencing technique to the evolution of real world industrial models with versions and version commits1.
the models were written by different ctd practitioners over a period of years and originate from different domains firmware paas iaas file system operating system database storage analytics banking telecom networking and software applications email document management finance etc.
.
the models also capture different levels of testing such as function test system test etc.
we did not select the models according to any criteria and they represent all data available to us.
the versions result from user defined commits the time difference between two consecutive versions ranges between a day and months.
all runs of our differencing computation were performed on a linux machine with .
ghz cores and gb ram.
only one core was used in our experiments.
the bdd package used was jdd .
in the following we provide observations on the collected model data and on our differencing results.
complete per model data as well as all versions data and differencing results are available from .
results and observations variability in size and complexity.
the data shows high variability in size and complexity of both versions and commits.
the size of models ranges from to parameters with median .
and standard deviation sd of .
median and sd are computed on average across commits per model .
there were to 1unfortunately all models are confidential since they were created for ibm or its clients.
we are in the process of checking the option of sharing most of them after obfuscation.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
values per parameter median .
sd .
and to constraints median .
sd .
.
the resulting test space sizes are ranging from to valid tests median .
sd .
and cls ranging from to median .
sd .
.
the size of the changes resulting from commits ranges from the addition or removal of a single parameter constraint and or value to parameters added median sd .
and removed median sd .
constraints added median sd .
and removed median .
sd .
values added to a common parameter median .
sd .
and removed median .
sd .
and from no changes in the strongest exclusions to strongest exclusions added median .
sd .
and removed median .
sd .
.
while of the models had rather small or less parameter additions on average across commits of the models had .
or more parameter additions on average and while of the models had or less parameter removals on average had .
or more such removals on average.
similarly while of the models had less than value additions to common parameters on average had .
or more such additions on average and while of the models had less than value removals on average had or more such removals on average.
syntactic diff.
based on the observed data changes in constraints additions in of the commits removals in of the commits are more likely to occur than the other syntactic changes which are more or less equally common parameter value additions as well as value removals in of the commits parameter removals in of the commits .
additions and removals.
when considering both syntactic and semantic diff of the commits included both additions to the model and removals from it.
included only removals and only additions.
we conclude that a commit tends to contain complex changes rather than simpler changes of only additions to the model or only removals from it.
to answer the results show high variability in size and complexity of both versions and commits.
commits tend to contain large and complex changes that typically involve a combination of additions removals and modifications to the constraints.
the large size and complexity of the models and changes involved as evident above require an efficient and scalable differencing solution to handle real world model evolution.
performance.
figure presents the runtime results of our semantic differencing computation.
out of a total of commits computing differencing in the tool to display the differences as shown in the screenshots from our running example took less than a second for of the models less than seconds for and only required over a minute of the commits from only two models .
these two latter models involved an exceptionally high number of parameters large test space bdds and a cl greater than all contributing to the exceptionally long running time.
fig.
performance of our semantic differencing on real world model commits to answer the results show that our differencing computation performs in acceptable times on almost all real model versions in practice.
b. user study the research questions guiding our user study are rq2a is comprehension of changes in models challenging and does our presentation of syntactic and semantic differencing help practitioners in better understanding the changes done and their consequences?
rq2b does the expertise level influence the performance and confidence of practitioners when attempting to understand such changes?
rq2c is there a difference in difficulty of comprehension between syntactic and semantic changes?
setup and participants our study included two realworld models each with two real world versions.
the first model model a describes a system test space for features of ibm r circlecopyrtpower7 r circlecopyrt .
its first version contains parameters and constraints and its second version contains parameters and constraints.
the second model model b describes an end to end test space for a paas.
its first version contains parameters and constraints and its second version contains parameters and constraints.
the sizes and changes in the two models are comparable to realworld model sizes and changes as reflected by the evidence shown in section v a. we asked questions about each model related to the changes between the two versions.
one question about the syntactic differences between the two versions and questions about the semantic differences.
the syntactic questions asked about the number of parameters or values that were added or removed from the second version.
we presented two types of semantic questions.
the first type referred to the effect of adding values or parameters to the model e.g.
by adding the value vto parameter xin the model how many new combinations of vand a value u of parameter yare added to the space of valid tests?
.
the second type of semantic question referred to specific value combinations and whether there was a change in their authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
validity status e.g.
the value combination cis excluded from the valid test space of version is it also excluded from version ?
.
before running the study we reviewed these questions with the two ctd practitioners who created the two models we used in our study and verified that they reflect real world comprehension tasks of model changes.
each study participant filled an online questionnaire in an ibm corporate network containing the questions about the two models questions in total .
for one model only the model version definitions were provided and for the other model the syntactic and semantic diff report was provided as well as shown in section iii .
the decision on the order of models in the questionnaire as well as on the identity of the model that included the differencing report was randomly made per participant.
in addition after each question the participants were asked to rate their confidence in the correctness of their answer from least to most .
practitioners participated in the study all of whom are ctd practitioners regularly involved in creating and comprehending combinatorial models for test designs as part of their work at ibm.
all participants were previously unfamiliar with the two study models.
results we separate the question scores and confidence into two groups questions answered without using our differencing report score confidence without diff and questions answered while using our differencing report score confidence with diff .
note that due to the setup explained above some participants had the differencing report available for model a while others had it for model b. similarly some participants had the differencing report available for the first model they were asked about be it aorb while others had it for the second one.
figure summarizes the score results of our study.
the average score without diff was .
out of and with diff .
.
the average confidence without diff was .
out of and with diff .
.
these numbers indicate an overall improvement of in performance and a slight improvement in confidence when using the differencing report.
to answer the results show that comprehension of changes in models is challenging and that our presentation of differencing improves practitioners comprehension of the changes done and their consequences.
a deeper analysis is required in order to understand the impact of the differencing report on different practitioners.
to this end we explore the differences between the practitioners based on their level of expertise as follows.
the overall average score was .
.
the participants are composed of two distinct groups.
participants are expert ctd practitioners with a high level of expertise.
all these participants scored between and much above the overall average score.
participants are less experienced ones with a relatively low level of expertise in ctd.
these participants scored between and all below the overall average score.
figure summarizes also the score results of the expert practitioners and less experienced practitioners separately.
forthe expert practitioners the average score was without the diff report and .
with it.
out of questions per model collectively the experts answered correctly with diff and .
we consider this difference negligible.
thus the expert practitioners performed very well regardless of the diff.
their average confidence was .
without the diff report and .
with it indicating only a very slight increase in confidence.
in contrast for the less experienced practitioners the average score was .
without the diff report and with it indicating a significant improvement in performance.
their average confidence was .
without the diff report and .
with it relatively low in both cases and rightfully so.
fig.
average score for the total practitioners as well as separately for the experts and less experienced practitioners.
to answer for less experienced practitioners the differencing report significantly improves performance while not impacting confidence for expert practitioners the diff report does not impact performance and confidence.
all participants answered all the syntactic questions correctly regardless of their level of experience and regardless of whether the diff report was available.
for the semantic questions alone the average score was .
without the diff report and .
with it.
the average confidence for a syntactic question was .
above the overall average confidence of .
.
to answer the results show as one might expect that syntactic questions are much easier to answer than semantic ones hence our differencing technique is more useful for understanding semantic differences.
the complete study results are available from .
c. threats to v alidity we discuss threats to the validity of our results starting with internal validity.
first our implementation of differencing computation may not be free of bugs.
to mitigate this we manually verified the results of the diff computation on small synthetic models and on many of the real world models.
second there could have been a bias in the results of our user study due to the order of models as well as the order between questions for which the diff report was provided and those for which it was not provided.
to mitigate this as explained above we randomized the order of models in the user study as authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
well as the decision whether the first or second model would be accompanied with the diff report.
there are also threats to external validity.
first the models and versions used in the user study and in the analysis might not be representative of real world model evolution.
to eliminate this threat we chose real world models and versions all that were available to us.
second our study participants might not be representative of real world practitioners.
to mitigate this we chose real world ctd practitioners who are regularly involved in creating and comprehending combinatorial models for test designs.
however a larger group of practitioners and more models and questions could strengthen the generalizability of our results.
vi.
r ela ted work much work has been published on syntactic and semantic differencing of programs and models within the more general field of software evolution see e.g.
.
most relevant to our present work is the work of maoz et al.
on semantic model differencing for class diagrams using sa t and activity diagrams using bdd based symbolic algorithm .
other recent work considered semantic differencing for feature models in the context of software product lines.
these works address similar challenges to the ones we deal with including the efficient computation of the semantic differences and their effective presentation to the engineer.
a framework for relating syntactic and semantic model differences has been presented in .
to the best of our knowledge our work is the first to consider syntactic and semantic differencing for combinatorial models.
in addition the existing model differencing work provided prototype implementations and evaluated performance but none has evaluated the usefulness of diff representations to real world practitioners.
while there is a large body of work on various aspects of combinatorial testing to the best of our knowledge none of it addresses the problem of differencing of combinatorial models in the context of their evolution.
the survey by nie et al.
considers academic papers on combinatorial testing but does not mention model evolution.
many ctd tools exist but to the best of our knowledge they provide no support for model differencing.
our practical experience in contrast shows that managing and comprehending changes in models is a challenge encountered frequently by practitioners.
one notable exception is the work of qu et al.
which examines the effectiveness of combinatorial testing prioritization and re generation strategies on regression testing in evolving programs with multiple versions.
however the work does not address evolution at the model level.
derived exclusions in combinatorial models a.k.a.
implicit constraints have been discussed in previous works and were shown to complicate the solving of the ctd problem and the modeling process .
uses derived exclusions to review and debug the model constraints.
while it suggests to present only minimal derived exclusions the concept of strongest exclusions is not discussed independently.
moreover the work does not suggest the use of strongest exclusions for a canonical representation of a model.
finally all these works do not consider derived exclusions in the context of evolution.
in a recent work we presented a lattice based semantics for interpreting the evolution of combinatorial models .
the new semantics replaces the inadequate boolean semantics which is currently in use by ctd tools for interpreting model changes.
it provides a theoretical base for a consistent interpretation of atomic changes to the model and exposes which additional parts of the model must change following an atomic change in order to restore validity.
the differencing approach we present and evaluate in this paper fits well with this new semantics since strongest exclusions can be precisely defined using our lattice based semantics.
finally in a research roadmap presented in a recent review on combinatorial testing yilmaz et al.
suggest the challenge to handle evolving models as they change over time .
our work starts going in this direction.
vii.
c onclusion and future work in this work we propose a first syntactic and semantic differencing technique for combinatorial models of test designs and present an efficient algorithm for computing it.
to enable semantic differencing we suggest a concise and canonical representation of a model that is used as the basis for differencing.
we implemented our technique in our ctd tool ibm focus and evaluated it on real world models with versions demonstrating its acceptable performance times.
our analysis reveals that real world commits of combinatorial models tend to be large and complex in practice.
we further conducted a user study with ctd practitioners on real world combinatorial model versions.
the study showed a significant improvement of over percent in the performance of less experienced practitioners when using our differencing techniques and an improvement in confidence of expert practitioners in their comprehension of model changes.
the present paper is part of our larger research project on comprehension and evolution of combinatorial models for test designs.
in a recent paper we presented the use of visualization for comprehension of combinatorial models and test plans .
as part of this project we plan to further analyze model differences and identify change patterns that commonly occur in real world combinatorial model evolution such as abstraction refinement and refactoring.
finally we plan to investigate co evolution of models and the test plans derived from them and find practical and efficient ways to update a test plan following changes to the model from which it was derived.
viii.
a cknowledgments this research was done under the terms of a joint study agreement between ibm corporation inc via the ibm research lab haifa and tel aviv university.
additionally part of the research leading to these results has received funding from the european community s seventh framework programme fp7 under grant agreement no.
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.