optimizing test placement for module level regression testing august shi suresh thummalapenta shuvendu k. lahiri nikolaj bjorner jacek czerwonka university of illinois at urbana champaign usa microsoft corporation usa microsoft research usa abstract modern build systems help increase developer productivity by performing incremental building and testing.
these build systems view a software project as a group of interdependent modules and perform regression test selection at the module level.
however many large software projects have imprecise dependency graphs that lead to wasteful test executions.
if a test belongs to a module that has more dependencies than the actual dependencies of the test then it is executed unnecessarily whenever a code change impacts those additional dependencies.
in this paper we formulate the problem of wasteful test executions due to suboptimal placement of tests in modules.
we propose a greedy algorithm to reduce the number of test executions by suggesting test movements while considering historical build information and actual dependencies of tests.
we have implemented our technique called testoptimizer on top of cloudbuild the build system developed within microsoft over the last few years.
we have evaluated the technique on five large proprietary projects.
our results show that the suggested test movements can lead to a reduction of .
million test executions .
across all our subject projects.
we received encouraging feedback from the developers of these projects they accepted and intend to implement of our reported suggestions.
i. i ntroduction large scale software development projects use build systems to manage the process of building source code applying static analyzers and executing tests.
any inefficiencies in the underlying build system directly impact developer productivity .
given the significance of build systems major companies such as microsoft and google have made huge investments in developing efficient incremental parallel and distributed build systems.
example build systems include cloudbuild bazel and fastbuild .
these build systems view a software project as a group of inter dependent modules and inherently perform safe regression test selection at the module level.
given a change they leverage the build dependency graph to identify modules that are impacted by the change and perform activities such as building or applying static analyzers only on those modules.
more specifically whenever any dependency of a test module is impacted by the given change all tests in that module are executed otherwise the module is skipped and no tests in the module are executed.
the major advantage of module level test selection is that it does not require any additional metadata such as fine grained dependencies like exercised statements for each test beyond what is available in the build specification.
especially in the case of largesoftware projects that execute millions of tests each day storage and maintenance of this additional metadata adds nontrivial overhead.
therefore module level test selection is the most practical option for performing test selection.
despite the increasing sophistication of build systems large software projects with thousands of modules often have dependency graphs that make inefficient use of these build systems.
such dependency graphs not only increase the buildactivity time but also make module level test selection less efficient by executing tests that are not affected by the given change.
v akilian et al.
studied the impact of monolithic build modules on the performance of distributed builds.
their work focuses on underutilized modules that include files not needed by some of its dependents and attempts to split them into smaller modules.
our work highlights another source of inefficiency in the form of wasteful test executions due to test placement in test modules that have more dependencies than the tests actually need.
therefore many irrelevant tests that are not affected by a change often get executed due to changes in developer specified dependencies of the module and such changes cannot alter the behavior of these tests.
execution of irrelevant tests can not only waste machine resources but can also severely affect developer productivity if those irrelevant tests are flaky .
flaky tests are tests that can pass or fail even without any changes to code.
in practice each test failure requires developers to manually triage the failures and identify the root cause.
therefore when an irrelevant flaky test fails developers end up spending unnecessary effort debugging the failure which has nothing to do with their change only to identify that the failure is due to a flaky test.
from our experience with cloudbuild the reasons for such incorrect placement of tests include a lack of comprehensive knowledge of all test modules in the project and developers making large refactorings to the code base more details in section v d .
the goal of our work is to provide a practical solution to reduce the number of wasteful test executions under the constraints imposed by the underlying build systems.
our solution for finding better placement for tests is applicable beyond just the cloudbuild build system as it can be applied to any build system that builds at the level of modules such as the build system that is used at google bazel .
ieee acm 39th international conference on software engineering ieee acm 39th international conference on software engineering .
ieee authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a c b d z x y t8 c t9 at1 a b c t2 b t3 ct4 b d t5 b t6 d t7 a build count a b c b c b d b c d fig.
.
an example dependency graph with build nodes a b c and d test nodes x y and z actual dependencies for tests t1through t9 and the build count number of times built for sets of build nodes a. running example to illustrate how a build system performs regression test selection at the module level and the problem we address in this paper consider an example dependency graph shown in figure for a hypothetical project.
the example project includes four application modules a b c and d and three test modules x y and z .
in the rest of this paper we use the notation build node to represent an application module and test node to represent a test module.
when a change happens to the code in a build node the build node is built and all build nodes and test nodes transitively dependent on that build node are built as well.
whenever a test node is built all the tests inside it are also executed.
for example when test node zis built tests t8andt9are executed.
the directed edge from ctoaindicates that cis dependent on a. given this background we next explain how this dependency graph can lead to wasteful test executions.
suppose that a developer makes a change to code in build node d. as such dis built and test node y which depends on d i s built as well and all the tests in yare executed.
however as shown in figure tests t5andt7do not actually depend on d and they need not be executed when dchanges.
these two tests are executed because they happen to be placed in y. this inefficiency is even worse if the dependencies change more frequently.
the table build count in figure shows the number of times a set of build nodes are built together e.g.
the set of build nodes a b c are built times together.
we see that in this example build node dby itself is built times.
as ydepends on d it is built at least times hence all tests in test node yare executed at least times.
to be more precise the tests are executed times because these tests also need to be executed when dis built in combination with other build nodes and when bis built as well resulting in an additional times when sets a b c b c b d and b are built .b.
overview of our technique in this paper we focus on reducing wasteful test executions due to incorrect placement of tests within test nodes.
an ideal placement of tests to avoid wasteful test executions is to place each test in a test node that shares exactly the same set of dependencies that are exercised by the test during its execution.
this placement ensures that only relevant tests get executed for a given change.
however in large projects moving thousands of tests to achieve the ideal placement requires huge effort.
given that developers are always under pressure to meet customer requirements they often have limited time to perform these activities.
therefore it is not practical to invest huge effort although one time in achieving the ideal placement.
furthermore introducing a large number of modules can significantly increase the time to build all modules in the project.
our technique addresses these issues by suggesting movements that reduce the number of test executions while minimizing the number of suggestions to give to developers.
our technique called testoptimizer accepts the following inputs dependency graph actual dependencies for tests and number of times build nodes have been built over a given range of time.
testoptimizer formulates the problem as a decision problem asking if there is a way to split test nodes to reduce the overall number of wasteful test executions.
testoptimizer uses a heuristic called affinity which is the set of dependencies the tests in a test node should be testing section iv b .
affinity helps determine the tests that need not be moved i.e.
tests that execute the affinity of its test node are not amenable to movement.
testoptimizer uses a greedy algorithm to iteratively suggest test movements that lead to reductions in number of test executions.
these suggestions involve moving tests into a newly created test node or to an existing test node that shares the exact same dependencies as the tests.
furthermore testoptimizer suggests the test movements ranked in the order of highest reduction in the number of test executions first.
we envision that developers can use testoptimizer once to get the correct placement of all tests.
once the recommendations are incorporated testoptimizer need to be executed only on added or modified tests.
for the example shown in figure testoptimizer produces two suggestions moving tests t5into a new test node and moving tests t7andt9into a new test node .
these suggestions reduce the number of test executions by test executions a42 reduction more details in section iii for this example.
c. contributions this paper makes the following contributions we formalize the problem of wasteful test executions due to incorrect placement of tests in test nodes.
we define a cost metric that models the expected number of test executions in a given range of time based on historical build count.
we propose a technique called testoptimizer that uses a greedy algorithm to suggest test movements between test nodes.
our technique produces a ranked list of authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
suggestions prioritizing the ones that give the highest reductions in the expected number of test executions.
we implement testoptimizer in a tool on top of cloudbuild a widely used build system at microsoft.
to evaluate our tool we apply it on five large proprietary projects.
our results show that the suggested movements can result in a reduction of .
million test executions .
across all our subjects.
we received positive feedback from the developers who also accepted and intend to implement of our suggestions.
ii.
p roblem sta tement letbbe the set of all build nodes for the project.
let n be the set of all test nodes for the project.
let tbe the set of all the tests for the project contained in the set of test nodes.
let be a partitioning of t and for a test node n let n t be the set of tests contained in n. given that is a partitioning of t n1 n2 for distinct test nodes n1andn2.
for each test t lettdeps t b be all the build nodes that tdepends on these are the build nodes tneeds to both compile and to execute.
for the purpose of this paper we require that build nodes do not depend on test nodes.
further whenever a test node nis built all the tests in n are executed.
consider a sequence of builds during a given range of time r say over a six month period .
there are two factors that influence the total number of test executions for a project duringr i the count of the number of times test nodes are built over r given the incremental nature of a build not every test node in a project is built in a given build and ii the number of tests executed when building a test node.
our solution changes the partitioning of tests by moving tests from existing test nodes to other possibly new test nodes.
to compute the reduction in the number of test executions we first need to compute the number of times test nodes are built the test nodes build counts after a change in partitioning without actually replaying the builds during r. a. computing build count for a test node let us assume we have collected the build count for all the build nodes and test nodes during a past range of time r. to determine the build count of a test node nafter moving tests across test nodes one possibility is to simply reuse the build count of ncollected in that range of time r. however simply reusing the build count is inaccurate for two main reasons.
first after moving tests between test nodes there can be changes to the dependencies of each existing test node.
second new test nodes can be added which do not have any existing build count information.
recall the example in figure .
if test t8is moved out of z that movement removes the dependency that test node zhas on c. this modified test node cannot be expected to have the same build count as before.
similarly if t7is moved out of yand into a brand new test node that depends only on a we would not have any collected build count for this new test node.however in most cases we can accurately compute the build count of a given test node entirely in terms of the build count of the build nodes in band the dependencies of the tests within the given test node.
we make the following assumptions during the range of time r the dependencies of individual tests do not change.
tests are changed added or removed to the test nodes only in conjunction with another change to a dependent build node i.e.
a change does not affect test nodes only.
from our experience with cloudbuild we believe these assumptions hold for most test nodes.
for the first assumption given that the dependencies in question are modules in the project as opposed to finer grained dependencies such as lines or files it is unlikely that there are drastic changes that lead to changes in dependencies at the level of modules.
for the second assumption we find that it is very rare for developers to be only changing test code they change code in build nodes much more frequently and changes made to test nodes are in response to those build node changes.
as we explain below given these assumptions we can compute precisely when a test node nwill be built namely whenever at least one dependency intdeps n is built.
although these assumptions may not hold for a very small fraction of test nodes during the time framer their effect is negligible when ris sufficiently large e.g.
a few months .
we define the dependencies of a test node nas the union of the dependencies of the tests contained in ngiven .
definition ndeps n for a given test node n the set of build nodes that ndepends on under is defined as ndeps n .
b b t n such that b tdeps t given our assumptions about test nodes a test node nis built if and only if a build node in ndeps n is built.
for a subset b prime b letbcr b prime denote the number of builds where only the build nodes in b primeare built together.
the box in figure titled build count shows the number of times each subset of build nodes is built e.g.
the exact subset a b c was built times in the range of time.
given our assumptions of test nodes not changing a test node nis built in a build iff any of its dependencies is built.
we can thus compute the number of times a test node nis built by summing up the build counts of bcr b prime whereb primeintersects withndeps n .
definition nodecount r n for a given test node n the computed number of times it is built during a range of timeris defined as nodecount r n .
summationdisplay b prime b b prime ndeps n negationslash bcr b prime since the set of all builds in rcan be partitioned by using the distinct subsets b primeas identifiers we count each build exactly once in the above equation.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
b. number of test executions in a project our metric for the cost of testing is the number of tests that are executed over r. as such our definition of the testing cost for a test node is related to the number of tests in the test node and the number of times the test node would be built definition nodecost r n the number of test executions for a test node nwithin a range of time ris the product of the number of tests in nand the build count of n withinr nodecost r n .
n nodecount r n the total number of test executions for testing the entire project would then be the sum of the number of test executions for each test node.
definition costr n the number of test executions costr n needed for building all test nodes nin a project within a range of time ris defined as costr n .
summationdisplay n nnodecost r n c. reducing test executions our goal is to reduce the number of test executions in a project.
while we could formulate the problem to allow placement of tests to any test node irrespective of the initial placement the resulting ideal placement of tests could be very different from the original placement of tests.
such a placement would then result in too many suggested test movements for the developers to implement and it is not practical to invest huge effort although one time in achieving this ideal placement of tests.
in this paper we consider the option of splitting a test node ninto two test nodes n n prime n prime negationslash n where a subset of tests from nare moved to n prime.
as such we can constrain the number of suggested test movements to report to developers.
we define n overn n prime to be identical to atn n n prime and the disjoint union n n unionmulti n n prime n meaning that n only moves a subset of tests possibly empty from nton prime.
our test placement problem can now be restated as the following decision problem does there exist a n n such that costr n costr n n prime n c?
where cis a constant value representing a threshold for reducing at least a certain number of test executions.
the threshold cacts as a knob for controlling suggestions where a split is suggested only when the reduction in number of test executions is worth the overhead of the developer implementing the suggestion.
essentially crepresents the minimal return on investment that can be expected by a developer to implement the suggested split.
with multiple such nthat reduce the number of test executions by at least c the one that provides the highest reduction can be chosen first.
iii.
t est optimizer our technique called testoptimizer provides a practical solution to reduce the number of wasteful test executions.algorithm splitting a test node procedure split test node n r groups newmap for each t n do deps tdeps t if groups .containskey deps then groups end if groups groups t end for n prime newnode n newp artition n prime repeat tests maxcost costr n n prime n for eachdeps groups .keys do prime n removet ests prime n gr o u p s addt ests prime n prime g r o u p s newcost costr n n prime prime ifnewcost maxcost then tests groups maxcost newcost end if end for iftests negationslash then removet ests n n t e s t s addt ests n n prime t e s t s end if until tests return n n prime end procedure more specifically testoptimizer produces a ranked list of suggestions that help reduce a large number of wasteful test executions with minimal test movements.
our technique also allows developers to specify threshold cin terms of the number of test executions that should be reduced for a suggestion.
the suggestions also include additional recommendations where tests can be moved into existing test nodes in case such test nodes already exist in the dependency graph.
we first present how testoptimizer splits a single test node into two test nodes and then explain how to use the single split algorithm to deal with all test nodes in a given project.
a. splitting a test node given a test node we seek to find a subset of tests that can be moved to a new test node resulting in the highest reduction in number of test executions.
we use an iterative greedy algorithm to find such subset of tests that can be moved from the given test node.
algorithm shows the individual steps.
given a test node n the loop in lines to iterates over the individual tests in that test node.
for each test t our algorithm gets the build nodes the test depends on using tdeps t .
if an existing authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
iteration node y node y1 test execs g1 t4 g2 t5 g3 t6 g4 t7 g1 t4 g4 t7 g2 t5 g3 t6 g1 t4 g4 t7 g3 t6 g2 t5 table i steps in splitting test node y group of tests already shares the same set of build nodes as dependencies the test is added to that group otherwise a new group is created.
next the algorithm simulates moving groups of tests into another initially empty test node so as to identify the group that results in the highest reduction.
at line the algorithm makes a new test node n prime and at line the algorithm makes a new partitioning nthat includes n prime.
the loop to iterates through the groups and simulates moving each group of tests from nton prime.
the simulation is done by using a temporary partitioning primethat starts as nbut is modified by using removet ests to remove a group of tests from n and then using addt ests to add the same group of tests to n prime.
the algorithm chooses the group of tests whose movement results in the highest reduction in number of test executions.
the outer loop lines to greedily chooses to move that group of tests from ninton prime lines to by modifying that new partitioning n. the outer loop iterates until there are no groups that help reduce the number of test executions.
when the loop terminates the algorithm returns the new partitioning nand the new test node n prime.
in case ncannot be split n will map n primeto an empty set.
our algorithm moves groups of tests instead of individual tests in each iteration due to two reasons.
first moving groups of tests instead of individual tests can make the search faster.
second since the group of tests share the same dependencies the tests are likely related to each other and should be placed in the same test node.
in the example dependency graph shown in figure consider the given test node nasy.
table i shows the result after each iteration of the outer loop lines to .
initially loop to identifies four groups of tests shown as g1to g4.
the initial number of test executions computed is .
at the end of the first iteration of the outer loop group g4is selected as the group that results in the highest reduction in the number of test executions resulting in test executions.
note that g4includes the test t7that depends only on a which is the root node of the dependency graph.
therefore it is clearly evident that a large number of wasteful test executions is due to the test t7.
after one more iteration the algorithm returns the partitioning that maps test node nto tests in groups g1andg3and maps new test node n primeto tests in groups g2and g4 this results in a final test executions.
b. handling all test nodes we next explain how we use the previous algorithm to deal with all test nodes in the given project.
given a set ofa c b d a1 x t7 a t9 at1 a b c t2 b t3 ct4 b d t6 dz t8 cy1 t5 by fig.
.
final dependency graph after applying our suggestions test nodes testoptimizer applies algorithm to each test node to find a split that results in the highest reduction.
in case the algorithm returns a partitioning where the new test node is mapped to no tests testoptimizer considers that the test node under analysis cannot be split further.
in each iteration testoptimizer finds the test node that has the best split i.e.
producing the highest reduction in the number of test executions.
testoptimizer then updates the overall partitioning with the returned partitioning for that best split and adds the new test node into the set of all test nodes.
testoptimizer repeats these steps until there exist no test node that can be split any further.
at the end testoptimizer returns all suggestions ranked in descending order based on their reductions in number of test executions.
if there are any new test nodes that share the exact same dependencies as another test node new or existing testoptimizer also makes the suggestion to combine the two test nodes into one.
testoptimizer also allows developers to specify thresholds for a split.
from section ii c this is the threshold value c representing the minimal number of test executions a split must reduce by.
the threshold is implemented by adding an additional condition to line in algorithm .
these criteria can help eliminate trivial suggestions that may not be worthwhile of the effort required to implement the suggestion.
returning to our running example consider the threshold c as100 test executions.
our technique first splits y creating new test node y1 as shown in table i. it next splits z creating new test node z1 where znow only includes t8 andz1includes t9.
our technique finally splits y1 creating new test node y2 y1now includes t5andy2includes t7.
testoptimizer terminates since no further split can achieve the given criterion.
testoptimizer also suggests the two test nodes y2andz1including tests t7andt9 respectively can be combined into one test node a1 since both the tests share exactly the same dependency.
figure shows the final dependency graph after applying our suggestions.
overall for this example our technique reduces the number of test executions by .
c. properties of testoptimizer using our technique we can guarantee that every suggestion does indeed ensure a reduction in number of test executions.
however our technique may fail to find a split that reduces the cost even if there exists one.
this is due to the greedy nature of the algorithm that only moves one group of tests authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
that share the same dependency from nto the split node n prime one that provides the maximum reduction in number of test executions .
for lack of space we omit such an example there exists one that shows the limitation of our algorithm.
iv .
i mplementa tion we implemented testoptimizer as a prototype on top of cloudbuild a widely used build system at microsoft.
currently cloudbuild handles thousands of builds and executes millions of tests each day.
a. code coverage for our implementation we targeted the tests written using the visual studio team test vstest framework as the majority of tests in cloudbuild are executed using vstest.
since testoptimizer requires actual dependencies of each individual test we use the magellan code coverage tool to collect those dependencies.
in particular we first instrument the binaries in a project where a binary corresponds to a build node using magellan.
we then execute the tests on the instrumented binaries and save a coverage trace for each test.
the coverage trace includes all blocks that are executed by the test.
we map these blocks to the build nodes to construct the set of actual dependencies for each test.
our implementation also handles the special constructs such as assemblyinitialize and classinitialize that have specific semantics in vstest.
for example assemblyinitialize is executed only once when running all the tests in a test node but the binaries exercised should be dependencies to all the tests in that test node.
b. test node affinity we use affinity to refer to the set of build nodes that are intended to be tested by a test node.
affinity helps avoid suggesting moving tests that are already in the right test node and also helps improve the scalability of our technique.
ideally one would ask the developers to provide the set of build nodes each test node is intended to test.
however as it is infeasible to ask the developers to spend time labelling every single test node we instead develop a heuristic to automatically compute the affinity for any given test node n. first we compute the dependencies ndeps n of the test node.
next we compute b n of each b ndeps n as the number of tests in nthat covers bduring their execution.
b n t n b tdeps t finally let max n m a x b prime ndeps n b prime n the maximum value among all build nodes in n. we compute the affinity as affinity n b ndeps n b n max n once the affinity is computed any test that does not exercise all dependencies in affinity n is considered to be amenable for movement called an amenable test .
in the context of algorithm affinity can be implemented by modifying line to skip tests that exercise all dependencies in affinity n .
our experimental results show that affinity helped exclude a large build test build count subject kloc nodes nodes tests entries proja projb projc projd proje overall average table ii sta tistics of subjects used in our ev alua tion number of tests from our analysis and it also provided more logical suggestions.
furthermore we found that developers tend to agree with the affinity computed for each test node section v d .
as per the developer feedback our technique is able to identify the affinity correctly for of the test nodes in our subjects.
c. output testoptimizer generates an html report with all suggestions that can help reduce the number of test executions.
the report displays metrics concerning the number of test executions with the current placement of tests and the build count for the current test nodes.
for each test node with tests amenable for movement the report suggests how to split the test node to reduce the number of test executions.
furthermore the report also suggests any existing test nodes where the tests can be moved into instead of making a new test node.
existing test nodes are suggested only when the test node shares the exact same dependencies as the tests to be moved.
the suggestions are shown in a ranked order starting with the test nodes that achieve the highest reductions.
this ranked list can help developers in prioritizing their effort.
for completeness the report also includes the test nodes that were found to not contain any tests amenable for movements as to give the developer a more complete picture.
v. e v alua tion in our evaluation we address the following three research questions rq1.
how many test executions can be saved by applying testoptimizer suggestions?
rq2.
how scalable is testoptimizer in handling large real world projects?
rq3.
what is the developer feedback for the suggestions of testoptimizer?
a. experimental setup we applied our technique on five medium to large proprietary projects that use cloudbuild as the underlying build system.
table ii shows the statistics of our subjects.
for confidentiality reasons we refer to our subjects as proja projb projc projd and proje.
the row overall is the sum of all the values in each column.
the row average is the arithmetic average of all the values in each column.
all subjects primarily use c as the main programming language but also include some code in other languages such as c authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
or powershell.
column shows the size of c code in each subject.
as shown in the table our subjects range from medium scale kloc to large scale projects kloc .
column shows the number of build nodes and column shows the number of test nodes.
column shows the number of manually written tests in each subject.
for each subject we collect historical data about the number of times sets of build nodes are built in a previous range of time.
cloudbuild maintains this information about each build in a sql server database.
using this database we computed the build count information for sets of build nodes during the time period of days starting from feb 1st .
column shows the number of table entries in the database for each subject where each entry is a set of build nodes for the subject along with the number of times those build nodes were built together.
in our running example from figure this number would correspond to the number of entries in the box titled build count for each subject.
the historical data is over a period of days for all subjects except proje.
we could not collect days worth of historical data for proje our largest subject due to out of memory errors since our tool performs all in memory computations.
therefore for proje we used only 30days worth of historical data from the same start date .
the split criterion we used as a configuration option to our technique is to split a test node only if the reduction is at least2 test executions setting the threshold value cto be .
we used this criteria based on our discussions with the developers of our subjects.
finally although we compute the reduction based on the historical data over a range of time rin the past we present to developers the results as potential savings for a future range of time r as future savings is what matters more to developers under the assumption that the past is a good indicator for the future.
b. results regarding rq1 table iii presents the results showing the reduction in terms of number of test executions after applying testoptimizer.
column shows the number of test nodes where testoptimizer found amenable tests based on affinity section iv b .
test nodes with amenable tests are amenable test nodes .
column shows the number of amenable tests.
the percentage of amenable test nodes range from .
in projb to .
in proje .
these results indicate that developers often ensure that tests are placed in the correct test node.
however the fact that there are tests in incorrect test nodes suggests that developers can still make mistakes as to where the tests belong to especially if they lack a global view of the project so there is still room for improvement.
up to tests across all subjects can be moved as to reduce the number of test executions.
column shows the number of tests that were actually suggested to be moved by testoptimizer meaning their movement can provide a substantial reduction in the number of test executions.
column shows the number of new test nodes that need to be created for the tests to be moved into notincluding any existing test nodes that already exactly share the dependencies of the tests to be moved .
column shows the number of test executions in millions for the original placement of tests based on historical data.
columns show the reductions if developers were to implement the suggestions.
column shows the reduction in terms of number of test executions while column shows the percentage of reduction when compared against the original number of test executions column .
however these columns show the reduction relative to the number of test executions of all test nodes in the project there are many test nodes that testoptimizer found as non amenable.
column also shows percentages but compared against the number of test executions concerning only the amenable test nodes essentially compared against only the test nodes that testoptimizer can actually suggest movements from.
when considering only the amenable test nodes we see the percentage of reduction in number of test executions is higher compared with the reductions based on all the test nodes.
for the percentages in the table the row overall is computed as the total reduction in number of test executions across all subjects over the total number of original test executions.
average is the arithmetic average of all the percentages.
to address rq1 testoptimizer can help reduce millions of test executions up to among our subjects.
by considering only the amenable test nodes these reductions range from .
to .
among our subjects.
regarding rq2 column shows the time in minutes taken by testoptimizer to analyze the historical data and to run through its algorithm to suggest movements for each subject.
our results show that the analysis time ranges from .
minutes up to .
minutes hours .
the time seems to be a factor of the number of build nodes test nodes tests and the amount of historical data available.
we find this time can still be reasonable as we envision our technique to be run infrequently.
we also envision testoptimizer can be run incrementally by only analyzing newly added tests as to suggest to developers the best test node to place new tests testoptimizer can work quickly when analyzing only a small number of tests.
furthermore testoptimizer is still a prototype and it can be improve by implementing lazy loading of data and caching previous computations to avoid repeated calculations of build count.
c. spurious dependencies in our experience we found that some of the test nodes have additional developer specified dependencies that are not required by any tests inside that test node.
we refer to such dependencies as spurious dependencies .
the primary reason for spurious dependencies could be the evolution of the application code.
more specifically developers could have originally specified a test node in the build system to have some dependencies but after code changes the tests were moved around so that the test node no longer depends on some of those developer specified dependencies.
as such when a spurious dependency is built the dependent test node is built unnecessarily causing a number of wasteful test executions.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
orig reduced test execs amenable amenable moved new test execs all amenable time to subject test nodes tests tests test nodes millions analyze min proja .
.
.
.
projb .
.
.
.
projc .
.
.
.
projd .
.
.
.
proje .
.
.
.
overall .
.
.
.
average .
.
.
.
table iii results of applying test optimizer on our subjects spurious deps test reduced execs test execs subject test nodes deps millions proja .
.
projb .
.
projc .
.
projd .
.
proje .
.
overall .
.
average .
.
table iv results when considering spurious dependencies a spurious dependency can be simply removed and all the tests within the test node will still run properly.
given testoptimizer spurious dependencies are the build nodes declared as dependencies in the build specification for a test node but are not covered by the test node s tests.
table iv shows the number of test nodes that have spurious dependencies in each subject column along with the total number of spurious dependencies those test nodes depended on column .
for some subjects such as projb and proje we see that almost of all test nodes have spurious dependencies.
although detecting spurious dependencies is not a core contribution it is an additional advantage of our technique.
the reduction in number of test executions after both moving tests and removing spurious dependencies is the reduction developers would actually obtain.
table iv shows the effects of having spurious dependencies and the reduction in the number of test executions for each subject.
column shows the number of test executions in millions for each subject using developerspecified dependencies for each test node.
the number of test executions is higher than the values shown in table iii as they include the effects of these spurious dependencies.
columns and show the reduction in number of test executions for each subject after the suggested test movements from testoptimizer relative to the number of test executions obtained using the developer specified dependencies.
compared to the reductions seen in table iii the reduction is higher.
we also see cases where spurious dependencies seem to be a big problem.
to give an estimate of how the reduction in the number of test executions map to savings in terms of machine time we calculated the average time ms across all the tests of the five subjects.
using this average test execution time and the reduction of number of test executions our results show that testoptimizer can help save .
days of machine time.accepted valid rejected invalid subject sugg.
sugg.
sugg.
sugg.
proja projb projc projd overall average table v feedback from developers of four subjects d. developer feedback regarding rq3 we approached the developers of our subjects to receive their feedback on suggested test movements.
we received feedback from the developers of four of our subjects proja projb projc and projd and yet to receive the feedback for the remaining subject.
table v presents the results for each of these four subjects.
column shows the number of test movement suggestions reported by our tool which is counted by the number of test nodes where our tool found tests amenable for movement.
column shows the number of suggestions accepted by developers and they intend to implement the suggestions.
column shows the number of suggestions that the developers considered as valid but they do not intend to implement the suggestions.
finally column shows the number of suggestions that the developers considered as invalid.
as shown in our results .
of the suggestions were indeed accepted by the developers.
furthermore among the accepted suggestions in proja developers already implemented sixof the suggestions.
overall we received highly encouraging feedback from the developers.
the feedback also helped us understand how code evolution resulted in wasteful test executions.
for example a developer informed us that some application code was moved from one build node to a new one as a part of a major refactoring.
however the related test code was not moved into the relevant test node.
since the original test node still had a dependency on the new build node tests continued to execute properly but the tests would also execute when any other dependency the original test node had was built.
after analyzing our suggestions the developer felt that our technique could also be quite helpful finding a better organization of the tests.
this response is encouraging since it demonstrates testoptimizer s ability in addressing issues beyond wasteful test executions.
another feedback was that developers may not be aware of an existing test node better suited for their authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
a b c x t1 b .... t99 b t100 cbuild count a b c b c y t1 c .... t99 c t100 b fig.
.
an example dependency graph illustrating the scenario where developers tend to accept our suggested movements tests especially with many developers and many nodes in the project.
therefore developers tend to place tests in some test node they are familiar with they later add more dependencies to that test node eventually leading to wasteful test executions.
developers also appreciated the idea that our suggestions can help break edges from test nodes to build nodes in the dependency graph thereby reducing the build time along with the test execution time breaking edges prevents test nodes from being built which itself takes some time in the build system .
the developers we approached also asked that we provide these reports at regular intervals e.g.
once per week so they can monitor and improve the health of their project.
accepted we present the two common scenarios under which developers tend to accept our suggestions column in table v .
we use the dependency graph shown in figure as an illustrative example.
in the graph test node xthat is dependent on build nodes bandc.
the figure also shows the build counts of bandc where the build count of cis much greater than the build count of b. the figure shows that the xincludes tests where most of them are dependent on band only a few just one in this example depends on c. the primary issue is that most of the tests in xare wastefully executed due to the high build count of c. in this scenario our technique typically suggests to move the tests in xinto a test node that is dependent only on b. we noticed that developers tend to accept our suggestions in these scenarios since they help reduce a large number of test executions in test nodes such as x. another common scenario where developers often accepted our suggestion is when there already exist a test node with the exact same set of dependencies where the tests can be moved.
in this scenario the effort involved in implementing the suggestion is minimal i.e.
developers just need to copypaste the tests into the existing test node.
valid rejected we present the common scenario under which developers considered that the suggestions are valid but not willing to implement those suggestions column of table v .
we use the same dependency graph in figure as an illustrative example this time focusing on test node y instead.
in y the majority of the tests have a dependency on cinstead of b. although this scenario could result in wasteful test executions with respect to tests such as t100 it is not ascomponent1 component2 b x ycomponent1 component2 b1 x yb1 fig.
.
an example scenario where our suggestion leads to a higher level refactoring of a build node significant since only a few tests are impacted compared to the dependency graph shown in figure .
due to the lower benefit in implementing the suggestion developers seemed reluctant in moving such tests especially when there is no existing test node the tests can simply be moved into.
invalid regarding the suggestions that are considered as invalid column of table v we found that they are primarily due to implementation issues.
testoptimizer relies on code coverage to collect test traces in this case using magellan.
in case a test exercises a binary via only a constant or refers to a class using constructs such as typeof we noticed that magellan does not collect the necessary dependency a limitation in the code coverage tool.
due to such missing dependencies our tool suggested invalid movements that were rejected by the developers.
in the future we plan to explore further on how to collect dependencies in these scenarios.
interesting scenario we finally present an interesting scenario where our suggestion led to a higher level refactoring of a build node.
figure shows two major components in the project where each component contains several build nodes.
there are two test nodes xandy which are intended to test build nodes in component1 and component2 respectively.
however due to the build node b changes in component1 can trigger tests in y and similarly changes in component2 can trigger tests in x. in an attempt to implement our suggestion inx instead of splitting x the developer split the build node binto two build nodes b1andb2.
the resulting dependency graph is shown in the figure where the split helped remove dependencies between the build nodes and test nodes.
this is very encouraging feedback since our suggestions can help developers make insightful decisions in removing major unnecessary dependencies in the project.
in our future work we plan to explore how to automatically suggest these higher level refactorings for build nodes.
e. v erifiability due to legal issues we can neither reveal the names of our subjects nor release the testoptimizer code.
we can share anonymized output of testoptimizer for each subject1.
vi.
r ela ted work our work is related to existing work on regression test selection .
given a change these techniques aim to select a subset of affected tests.
the key idea of these techniques is to maintain some metadata such as statements covered by tests on the previous version authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
and leverage this metadata to select a subset of tests that are affected by the change.
recent work by gligoric et al.
found regression test selection that tracks dependencies at the coarse granularity level of files to be effective.
the build system our work focus on performs regression test selection at the even coarser granularity level of modules.
harrold et al.
specify that selection can be based on coarser grained entities such as methods classes or modules.
a major advantage of module level regression test selection is that it is extremely light weight and highly scalable which is why it is widely used in practice.
testoptimizer improves this regression test selection that naturally comes with the build system.
in contrast to previous finer grained regression test selection techniques our technique does not require any additional information beyond what is needed for building the modules.
although previous techniques can further reduce the number of test executions by the selection on each test node we argue that these previous techniques do not help as much due to the following reasons.
first existing techniques assume that tests are executed at the end of the build.
therefore even if there exists a single test in a test node that is impacted by the change the entire test node still needs to be built.
in contrast to that our technique can help skip the build of that test node as well.
second existing techniques require storage and maintenance of metadata which needs to be updated along with the changes in the underlying modules.
this aspect adds non trivial overhead in the case of millions of test executions.
instead our technique does not require any additional information beyond what build system already saves.
finally our technique can also be extended to refactor application code as well to further fine tune the dependency graph we plan to focus on this in our future work.
there has been some previous work on improving regression testing in the industrial environment.
elbaum et al.
proposed improving regression testing at google through a combination of test selection in a pre submit phase and test prioritization in a post submit phase.
instead of collecting coverage for test selection and prioritization they considered historical test failures to determine what tests to run and what order to run them.
herzig et al.
proposed a test selection technique based on cost model.
their technique dynamically skips tests when the expected cost of running a test exceeds the expected cost of not running the test.
all tests are still run at least once before the code is released so defect detection is merely delayed but can potentially be more costly to fix .
these previous techniques complement our technique.
in contrast to their techniques we can guarantee that some tests can be ignored by moving to a different test node since they are not relevant for the given change.
therefore we envision that developers can use our technique to avoid running tests that are not relevant for a change and then use their techniques to further reduce the number of test executions.
there exists work on techniques to improve the efficiency of builds .
telea and v oinea developed a tool that decomposes header files in c c code to remove performance bottlenecks.
morgenthaler et al.
developeda tool that ranks libraries based on their utilization rank to identify under utilized nodes that can be refactored.
our work is closely related to v akilian et al.
that attempts to decompose build nodes into two or more nodes to improve build times.
their work shares a similar goal with our work i.e.
avoid unnecessary building of nodes when dependencies change.
testoptimizer significantly differs from their technique due to the following reasons.
first we focus on moving individual tests in test nodes into either existing or new test nodes.
we allow movements to existing test nodes as opposed to their technique that only creates new nodes.
second since test nodes are essentially the leaf nodes in the dependency graph we do not have to update dependencies for any child nodes.
third their technique does not take historical information into consideration i.e.
they assume all nodes are built in every build.
in contrast to that we take historical information into consideration to ensure that tests are moved away from test nodes that have a high historical build count.
finally their techniques identifies dependencies statically whereas our technique uses dynamic analysis code coverage which is more precise.
although it would be ideal to evaluate both techniques on common subjects it is not practical since both are proprietary applications and are targeted for different technologies.
vii.
c onclusions in this paper we present testoptimizer a technique that helps reduce wasteful test executions due to suboptimal placement of tests.
we formulate the problem of wasteful test executions and develop an algorithm for reducing the number of wasteful test executions.
we have implemented our technique in a prototype tool on top of microsoft s cloudbuild.
we show the effectiveness of testoptimizer by applying it on five proprietary projects.
our results show that our technique can reduce .
million test executions .
across all our subjects.
furthermore developers of four of our subjects accepted and intend to implement of our suggestions developers have already implemented some of these suggestions as well.
beyond saving machine resources the reduction in test executions can also help reduce the developer effort in triaging the test failures if these irrelevant tests are flaky.
in future work we plan to use testoptimizer incrementally to suggest ideal placements for newly added tests soon after they are added.
we also plan to extend testoptimizer to refactoring build nodes as well to improve build times.
finally we plan on exploring large scale automated refactoring techniques for applying our suggestions.