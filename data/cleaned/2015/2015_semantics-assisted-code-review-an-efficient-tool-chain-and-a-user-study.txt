semantics assisted code review an efficient toolchain and a user study massimiliano menarini yan yan and william g. griswold department of computer science and engineering university of california at san diego la jolla ca usa mmenarini ucsd.edu yayan cs.ucsd.edu wgg cs.ucsd.edu abstract code changes are often reviewed before they are deployed.
popular source control systems aid code review by prese nting textual differences between old and new versions of the cod e leaving developers with the difficult task of determining wheth er the differences actually produced the desired behavior.
fortunately we can mine such information from code repositories.
we propose aiding code review with inter version semantic differen tial analysis.
during review of a new commit a developer is pr esented with summaries of both code differences and behavioral d ifferences which are expressed as diffs of likely invariants ext racted by running the system s test cases.
as a result developers can more easily determine that the code changes produced the desired effect.
we created an invariant mining tool chain getty to support ou r concept of semantically assisted code review.
to validate our a pproach we applied getty to the commits of popular open source projects we assessed the performance and cost of run ning getty in different configurations and we performed a c omparative user study with developers.
our results demonstrate that semantically assisted code review is feasible effective and that real programm ers can leverage it to im prove the quality of their reviews.
index terms software behavior mining software repository code review likely invariants dynamic impact analysis scalability software testing i. introduction to aid code review today s popular version control systems vcs s e.g.
git integrate with textual differencing tools e.g.
git diff .
a reviewer can start with a summary of changed code between two versions then navigate to related code snippets at her own discretion.
however the textual pro gram level differences from current differencing tools provide only indirect information about the behavioral impact of code changes.
the results of testing provide only a pass fail view o f that behavior perhaps disguising subtle bugs.
a reviewer shoul d read further into the source code to understand both the syntac tic and semantic changes and examine the related tests to verify that the changes are being properly tested.
we propose that reviewers would benefit from summaries of the behavioral effects of those changes.
the summaries should be concise comprehensive presented in a familiar notation an d complement existing review information.
likewise the produc tion of summaries should require little effort on the part of t he developer or reviewer just like continuous integration today supports effortless regression testing.
we call the resulting i nfrastructure and process semantic assisted code review scr .
the idea of differential assertion checking is a gesture in the right direction but these works require manual eff ort and do not provide a comprehensive view of semantic effects.
however applying this concept to behavioral summaries mined from code repositories could provide the best of both worlds.
existing tools like daikon can infer likely invariants and repo rt them in the terminology of the program itself .
likely invar iants are properties that summarize the traces of runs fed to da ikon.
following the terminology used by daikon we use the term likely invariant or invariant to refer to these properties .
we propose that performing inter version differencing of the volu minous invariants extracted by daikon will provide a concise behavioral diff .
because daikon invariants are reported at th e method level it would be easy to attach them to the code diffe rence summaries provided by a tool like git.
our approach realized in a tool called g etty builds on daikon and provides an infrastructure for automatically pulling building testing and analyzing multiple commits of a program from its code repository.
making this approach practical requir es solving three critical problems.
because several data and compute intensive steps are required we need to address perfor m a n c e a n d c o s t i s s u e s .
b e c a u s e t h e r e a r e t w o p o t e n t i a l causes of change to daikon s dynamically inferred invariants changes to source code and changes to tests we must identify how to extract and compare invariants to highlight the contributions of each of these changes.
and because of the novelty and possible complexity of running code reviews based on be havioral summaries we need to verify that real programmers can effectively use them to improve the quality of their code revie ws.
the following contributions address these problems .
we introduce three complementary techniques for effectively mining behavioral information in our case daikon invariants for code review section iii.b .
the techniques applied in g etty include a using impact analysis to scope invariant mining to the methods related to the current c o m m i t b m i t i g a t i n g m e m o r y p r e s s u r e b y t r a c i n g o n e class at a time and c paralle lizing trace gathering and invariant extraction.
our experiments on open source pro jects demonstrate that these techniques enable nearly arbi trary reductions in running time by parallelizing analyses.
.
c circlecopyrt2017 ieeease urbana champaign il usa t echnical research554 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
moreover renting processors in the cloud makes running getty for scr acceptable both in terms of time and cost.
.
to help the reviewer understand the effects of changed source code versus changed test cases our approach ex tracts likely invariants and presents their differences for various combinations of source code versions and test suite versions.
we show how scr behavior change summaries can reveal bugs and other problems earlier than they were actually discovered by the original reviewers.
to this end we applied g etty on a portion of google s gson revision history sections ii and iv .
we discovered two bugs that were previously unreported.
moreover in a retrospective analysis of gson and five other open source projects we show that invariant differentials helped find testing gaps in of selected commits section v .
we also analyzed six randomly chosen previously known bugs one in each project g etty s invariant differentials made the bug evident at its point of introduction in out of cases.
.
to ensure that the results we achieved in our experiments with g etty can be achieved by programmers in general and to assess the benefits of scr compared to traditional lightweight review processes we ran a comparative user study with participants.
we divided the participants in teams of and assigned review tasks on real commits of the gson open source project.
six teams used the g etty toolchain for their review while three teams used the tools available in github.
the results of this study demonstrate that scr as implemented by g etty changes how reviewers perform their task and can improve the quality of the feedback they produce.
before diving into g etty s design and evaluation we motivate and define semantic assisted code review.
before clos ing we discuss related work in section vii.
ii.
s emantics assisted code review with getty we illustrate how to perform scr with g etty by following the review of a real commit taken from the gson project.
the review follows the work of real reviewers that participated in a user study further discussed in section vi.
gson is a googlesponsored open source java library for performing conversions between java objects and their json representations .
when we ran our study gson had undergone commits by contributors with software releases since the project s sta rt in .
all changes were peer reviewed using what s called lightweight code review which attempts to achieve the benefits of formal code review with lower overhead and delay .
we support our scenario and the analysis of how the review is carried out by quoting study participants.
the case study pr esented in this section does not follow the review of a single s tudy participant instead it summarizes the process followed by dif ferent teams and is simplified to demonstrate the key elements of a continuous semantic review with getty e.g.
we show the review of a single method .
we review gson commit e450822 which modified the class lazilyparsednumber .
this class parses strings into numeric values it does this lazily meaning that the class maintains a string representation of the number and converts it to a numeri c value only when a method returning the number is invoked.
the patch changes two methods of this class intvalue and longvalue which return the number represented as an int and long respectively.
the changes implemented in the patch use the bigdecimal class to parse the string even when an integer value is requested.
therefore no exception is thrown when the string has decimals or if the number is very large.
the patch adds one test.
the reviewer first looks at the commit message and possibly at the referenced issues that the commit addresses.
in this cas e the message is use bigdecimal to parse number string when requesting it as integer.
lazilyparsednumber has the value of a string that can be interpreted as a number.
use bigdecimal to parse the number string to avoid precision loss in general.
how ever when requesting as an integer it ignores all digits after decimal point if any and ignores all bits that overflow the range of requested integer type .
this description is not clear unfortunately unclear commit messages and issue reports are commonplace.
one of our reviewers commented ... ignores all bits that overflow the range of requested integer type.
... that is not a good way of saying that!
see table v for decoding c1la .
next the reviewer opens the g etty and peruses the upper box in the page which lists top to bottom a the sou rce code methods that have changed highlighted in blue b the testing methods that have been updated also highlighted in blu e and c the methods whose invariants have changed highlighted in red .
the reviewer notices that both the code of method longvalue and its invariants changed boxed in the picture .
the reviewer clicks the lazilyparsednumber longvalue method link boxed in fig.
causing it to be listed in the middle o f the next section of dotted boxes as well as listing its changed invariants below .
the dotted boxes summarize the invocations closely related to longvalue .
above longvalue the reviewer sees that longvalue has two direct callers.
being shown in red means they have changed invariants underlined means changed source.
in this case a te st case calling longvalue has changed.
to the left of longvalue are methods that one of longvalue s callers called immediately before calling longvalue and to the right methods called immediately fig.
.
g etty methods and classes with changed invariants fig.
.
g etty navigation of the call graph authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
after longvalue returned.
the gray text color indicates no changes to the method s source or invariants.
the reviewer does not see anything on the left suggesting they would affect longvalue s invariants so she is not motivated to click any of them.
the box below shows the methods that longvalue itself called in this case no method is called.
the reviewer now turns her attention to the invariants displayed below the invocation summary .
removed invariants would be highlighted in red added invariants in green a nd changed invariants in yellow the figure shows only added invariants just as text changes are highlighted in git.
the gray header above the invariants indicates that they are for the exi t point of the method in line the return inside the catch blo ck in yellow .
the reviewer observes immediately that it appears that only one value is parsed using bigdecimal in the modified code.
in the reviewer s own words so i think what this means is that the exception case is tested with only one test... and that also seems not necessarily great right?
you want at least a few tests for all branches.
a couple of tests at least for all branches.
this is a working hypothesis the reviewers can then validate by browsing through the code.
she then clicks on testnumberdeserialization in the top box of fig.
opens the source diff view and confirms that indeed only one test was added a simple test parsing the strin g .
thus confirming the hypothesis.
before firing off a comment to the developer however she uses impact analysis to confirm that the existing test works as expected.
likely invariants are computed by running each project s test cases.
in this example both the main source code a nd the test code have been modified there are two potential cause s for daikon s likely invariants to change.
g etty isolates the impact of these changes by computing the invariants for different combinations of source and test code versions.
for example.
th e getty invariant diff for longvalue in the older commit code running the old and new test suites shows a throwscombined section on the right side new test suite .
this shows that the old code throws an exception when tested with the value .
.
one reviewer in our user study stated okay so the old source is throwing an exception presumably because it might have been called as something it wasn t supposed to be called .
this scenario highlights three features of scr with g etty invariant differentials impact isolation and invocation flows .
the display of just the changes in invariants provided the reviewer a concise behavioral view of the commit enabling quick creation of working hypotheses about the commit despite the dozens of underlying invariants.
her ability to explore differe nt combinations of old and new tests helped her isolate the behav ioral effects due to the source changes.
finally the summary o f the application s call structure around longvalue helped her quickly focus on a particular part of the program.
we next elab orate on these three elements.
a. diff ing behavior scr depends on having summaries of the input output behavior of methods.
these can naturally be phrased in terms of observed invariants.
for a dynamic tool like daikon these in variants are not absolute but depend on executions which we discuss more in the next subsection.
the number of likely invariants for a method before and after a commit can be numerous and reasoning about their differences can be mentally challenging.
however because the behavioral changes between program commits can be quite small so could the differences in their invariants.
this motivates the creatio n of invariant difference sets between program commits to suppress the common invariants and help the reviewer focus her attention on just what s changed since the last commit.
for example for the longvalue method in example of the previous section with no isolation of effects there were invariants before c ommit and after.
as shown in the figure for just the changed i nvariants there were just added a reduction.
other examples we encountered in our study showed reductions of more than .
for each kind of program point of a method m entry exit and exceptional exit g etty calculates the change in invariants between an older commit and a newer commit as two sets the removed invariants and the added invariants.
following git s style of code differencing g etty displays added removed and changed invariants.
a changed invariant is just a presentation of an added invariant paired with a removed invariant based on their overall similarity.
for example a removed invariant x would be paired with the added invariant x because they contain the same variable operator and value type.
taking advantage of daikon s consistent invariant formatting g etty is able to use the minimality of text differences to infer changed invariants.
comparing the logical formulae would achieve better results in some cases .
b. impact isolation as seen in the gson scenario when developers modify sources they often add test cases as well meaning that invaria nts can change due to either or both source and test changes.
getty must support a reviewer in isolating behavioral impacts to one or the other.
by running the same test cases on both the old and the new source any resulting invariant differences can be confidently attributed to the changes to the source code.
likewise by runnin g the old and new test cases on the same source any resulting in variant differences can be attributed to the changes to the tes t fig.
.
g etty invariants diff for longvalue fig.
.
g etty right side of diff for testnumberdeserialization added code authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
cases.
because the separation of source code and tests is standardized it is possible for g etty to automatically extract and show the invariants for a commit under four different combinations of source and tests shown in table i. the first variant provides no isolation showing the full effects of a new commit by simply running the older commit s tests on the old source and the newer commit s tests on the ne w source.
the second isolates source effects by running each of the old and new source on a common set of tests the union of the old and new tests.
we motivate this variant by first considering an alternative the intersection of the two commits test suites.
the resulting test suite would be guaranteed to compile and run on both source code bases.
however this excludes test cases that were intentionally written to demonstrate the behavior of a par ticular commit so it would often produce less useful invariant s than desired.
the union of all tests on the other hand will i nclude those unique tests but oftentimes some won t run or com pile on the commit for which it wasn t written.
as a simple example if a new method is introduced in the new commit and some tests are added to test this method then these new tests will not compile with the old source.
this creates an asymmetry in which test cases run on which commit which seems to defeat the isolation of effects to the source.
however the failure of compilation is really just an early indicator of a failure to r un.
the fact that a test case runs on one commit but not the other reveals a behavioral property of the source code.
thus the default condition for isolating source effects is to run the unio n of the test cases modulo compilability.
when the reviewer is interested in effects due to changes in the test cases g etty executes the test suites defined in the two commits on the same source code.
since the source code bases cannot be unioned like the tests there are two included varian ts last two rows of table i running the two test suites on the old source and running the two test suites on the new source.
full y understanding the effects of the changes to the test cases migh t require spending time looking at the results of both variants.
as discussed in section ii regarding gson commit e450822 the test impact isolation running new tests on old code reports an exception showing that the new test is not simply passing all the time but is also capable of revealing incorrect behavior.
c. invocation flows a change of one method in the source code can have widespread effects on the behavior of numerous methods.
this is a motivation for providing behavior change summaries that directly articulate those wide spread effects.
still a reviewer needs help in finding her way around.
semantic effects are propagated directly by the application s control flow a field is set in o ne method and then its value is passed to another where it is us ed set returned and so forth.
thus a natural way for a reviewer to explore a source code base is to navigate its call graph from caller to callee from callee to caller and so forth .
g etty provides a local area call graph as seen in the dotted boxes in fig.
.
only callers immediate siblings and callees whose invariants have changed are necessarily displayed.
as screen space allows more neighbors are displayed in gray to indicate their invariants were not affected by the commit .
cli cking any method in the displayed local call graph puts that method in the center and displays its callers immediate siblin gs and callees around it.
in this way it is possible to explore a ll the invariant changes through the program s control flow.
g etty computes the invocation flows from execution traces during testing.
because the flows actually occurred a reviewer can compare them with the expected flows to identify problem atic or unexpected results .
iii.
i mplementing and scaling scr like continuous integration ci scr depends on heavy lifting in the back end to support developers and reviewers work.
scr requires not only the same compilation and testing support of ci but also adds the often m assive cost of m ining likely invariants with daikon.
both human costs and computa tional costs must be minimized.
we discuss each in turn.
a. automated invariant differential extraction similar to yan et al.
we leverage existing open source tools commonly used in continuous integration like the build tool maven and the testing framework junit .
specifi cally we implement the g etty tool chain in four main components static source diff analyzer villa this component takes the textual code differentials from git and determines which methods and test cases have changed.
these constitute the change set .
dynamic callgraph analyzer agent next the old and new source are run on their test suites to extract their dynamic call graphs and acquire the change set s local invocation flows .
agent supports multi threading and exceptions.
the resulting impact set will be an expanded change set containing all methods and test cases for which we calculate invariants on the two commits.
invariant detector center this component checks out two commits and using daikon infers invariants for all meth ods in the impact set in all combinations of tests and source specified in table i. center uses a version of daikon that supports exceptional invariant detection .
since test methods can depend on each other center always executes the whole test suite.
to minimize disk i o we pipe traces directly to daikon.
semantic differential viewer gallery next gallery assigns invariant changes to methods and creates the user inter face described in section ii.
in our use of g etty on gson center takes over of the total execution time with agent next at under even when behavior isolation strategies effects of old src test combo new src test combo src tests src tests entire commit old old new new source only old old new new old new tests for old src old old old new tests for new src new old new new authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
just focusing on the impact set.
even for the small gson app center requires over hours to run.
the next subsection focuse s on center s performance.
b. scalable invariant differential extraction ideally invariant extraction times would be commensurate with build and testing costs enabling a timely repair compile review feedback loop.
to achieve such performance we have three advantages to exploit.
x we only need to trace and extract invariants for the methods that are relevant to the current pair of commits the impact set.
thus we apply dynamic impact analysis to restrict tracing and invariant inference to relevant methods.
x much of the performance cost is due to memory pressure.
as long as the full test suite is run the daikon trace for a class will be the same regardless of which other classes are being traced.
to this end we trace one class at a time rather than all relevant methods at once.
x invariant detection is embarrassingly parallel allowing us to parallelize inference in the cloud.
we partition each project s class granularity inference processes into as many groups as there are cores on the machine and distribute them to all the cores.
each group for a project is executed on a separate core concurrent with the other groups.
after all the groups finish center merges all the invariants collected a trivial step.
because of the class granularity the limit to parallelization is the number of classes in the impact set.
to evaluate these techniques we used gson and five other randomly selected projects from apache commons .
we to ok ran dom c om m its of e ach p r o je ct ex e cu te d th e center analysis on them and took the truncated mean for each project throwing out the low and high times .
their characteristics ar e summarized in table ii.
the total time taken by the daikon inference process running on all methods in the impact set shown in the second to last column for reference we call this the alltests all methods mode or atam increases with the running time of the test suite.
the cost of inference has no evident re lationship with the other factors.
the size of a commit will infl uence the size of the change set and hence the inference time b ut we controlled for this by averaging over the commits.
while limiting inference to the impact set makes invariant extraction tractable the cost is far too high for the last three projects requiring a day or more to run the last was killed after more th an three days .
reducing memory pressure by tracing one class at a time all tests single class or atsc cuts run times by about a factor of two as shown in table ii atsc .
we evaluated the parallel mode called parallel atsc or patsc on a cluster with eight processor nodes typical of what can be found in a cloud deployment today.
each node has .66ghz cpus and 16gb of ram.
fig.
plots running time against the number of processors used.
the single processor condition runs plain atsc the rest patsc.
patsc running on the same single node using both processors achieved .
speedup and efficiency.
its suboptimal speed up is due to memory contention.
running on all processors eight nodes provides a total speedup is .
with efficiency with a nearly linear speedup across the range implying high scalabili ty of invariant inference for scr.
related is the cost of computing g etty s invariants in the cloud say as part of an existing continuous integration proces s. fig.
plots the estimated additional cpu cost on amazon ec2.
the costs are modest tracking project size.
the high efficienc y of parallelization modestly increases the baseline average .
the scalability of patsc is bounded by the number of classes in the impact set.
for the commits examined here the av erage number of classes is with a standard deviation of suggesting generally ample parallelism.
at the low end there ar e a few commits that contain just classes for example commit c241318 in collections.
its times for invariant inference are .
.
.
and .
hours on and processors resp ectively.
the overall speedup from to processors is .7x wi th no discernable speedup from to processors since the maximum expected speedup is and the net time is bounded by the longest running class.
however all the small commits have shor t running times that don t demand high levels of parallelization.
project characteristics projects commits kloc methods tests tte tie atam tie atsc gson .
.3s .22h .70h cli .
.0s .98h .10h codec .
.3s .64h .67h crypto .
.5s .20h .43h collections .
.9s .51h .36h configuration .
.5s .81h tte total time for testing tie total time for inference environment intel .
ghz dual core 4gb ddr3 ram mac os x y osemite fig.
.
average running time versus number of cpus.
the single processor mode is atsc others are patsc.
environment intel .
ghz dua l processor quad core 16gb ram ubuntu server .
.
fig.
.
cloud cost of getty s invariant extraction plotted agai nst the number o f cpus used.
each quad core processor is priced as one independen t amazon ec2 t2.xlarge instance vcpu up to 30ghz 16gb ram .
hour for north california feb .
cpu cpus cpus cpus cpuscost usd gson cli codec crypto collections configuration authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
iv.
how to identify problems with getty in section ii we saw that scr with g etty can both quickly reveal inadequate testing and help users to successfully review a gson commit .
in this section we show how g etty supports scr in finding bugs.
we highlight commit 903769e in fig.
which introduced a test comparing two json primitive integers and .
the equals method should return false because the two values are different.
however the test failed revealing a bug.
the b ug was never fixed in the last commit we analyzed 423d18f the developers assessed that the price is too much to pay to fix the bug and overrode the test s failure by changing its assertfalse to asserttrue .
it is interesting to note that the bug was not revealed until commit despite all previous commits passing their code reviews.
the question then is whether scr could have aided in finding this bug sooner preferably at the point of introductio n were it in use by this project at the time.
in commit e89c949 developers introduced new features such that integers of different types byte short integer long and biginteger are comparable to each other in equals and floating point numbers of different types float double bigdecimal are comparable to each other.
fig.
shows the two new if branches added in equals to compare integers and floating points.
the predicate method isintegral checks whether a jsonprimitive object represents an integer i.e.
the type of value attribute is byte short integer long or biginteger .
if both this and other represent integers the first branch is executed and both value attributes are converted to long to compare for equality.
similarly the predicate isfloatingpoint c h e c k s whether a jsonprimitive object represents a decimal i.e.
the type of value attribute is float double or bigdecimal .
if both this and other store decimals the second branch is executed and value attributes are converted to double to compare for equality.
the two branches are independent from each other since the two predicates separate all input numbers into two disjoint sets.
we first study the branch from line to line for integers.
we expect that the branch returns true when both value attributes store the same integer values regardless of the speci fic integer types.
the invariant differentials are shown in fig.
a. at the exit point of line where the new integer comparison branch returns this.value must be long if the return value is false.
this is surely an incorrect invariant because the type of this.value being long is not a necessary condition for equals to return false.
the new branch should return false if value attributes are quantitatively unequal even when one is or both are not long.
in this regard we suspect a bug was introduced that created an incorre ct dependency between the result of comparison and the types of value attributes.
to verify if the bug actually exists we need to add a test.
th e incorrect invariant is informative.
we challenge the incorrect long type dependency by creating and comparing biginteger objects that cannot be precisely converted to long.
java s long type is a bit signed integer so any integer representation over bits will be masked when converting to long.
for example takes bits and is converted to after conversion.
we there fore create four jsonprimitive objects one for lp1 for lp1c for and lp2 f o r and assert that none of them are equal except for the pair of lp1 and lp1c.
our test is stronger than fig.
s because it only considers the case wher e other.value is biginteger but ours considers this.value being biginteger as well.
since our test fails we conclude that the bug was introduced in commit .
this is a typical case where a bug was introduced after the developers added a new feature.
notice that commit is dated sep but commit discovered the bug on sep nearly years after the bug was introduced.
a reviewer could have found the bug much earlier if they had been able to examine invariant differentials.
additionally consider the branch from line to line that deals with floating point comparisons.
similar to the prev ious integer branch we expect this branch to return true w hen both represent the same decimal values regardless of the specific decimal type of value attribute.
in fig.
10a the dynamic invariant at exit point says the return value is always true i.e.
for all tests so far this branch has only returned true.
this indicates that either there is a lack of testing for unequal decimals or unequal decimals are compared but there is a bug.
consequently we add a test case to compare unequal decimals.
building on previous experience we consider not only unequal double numbers but also unequal bigdecimal numbers.
our test case passes the first assertion but fails the se cond one.
passing the first assertion implies that equals behaves corfig.
.
new feature to compare between integers in commit e89c949 public void testequalsintegerandbiginteger jsonprimitive a new jsonprimitive 5l jsonprimitive b new jsonprimitive new biginteger assertfalse a equals b a.equals b fig.
.
the failing test case in commit public void testequalsforbigintegers biginteger limit new biginteger jsonprimitive one new jsonprimitive l jsonprimitive lp1 limit new jsonprimitive limit.add new biginteger jsonprimitive lp1c limit a different object new jsonprimitive limit.add new biginteger jsonprimitive lp2 limit new jsonprimitive limit.add new biginteger compare limit limit etc.
assertfalse limit lp1.equals one assertfalse limit one.equals lp1 assertfalse limit limit lp1.equals lp2 asserttrue limit limit lp1.equals lp1c fig.
.
test case to confirm the integer equality bug if isintegral this isintegral other return getasnumber .longvalue other.getasnumber .longvalue if isfloatingpoint this isfloatingpoint other return getasnumber .doublevalue other.getasnumber .doublevalue authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
rectly given two small unequal decimals confirming our hypothesis that the wrong invariant was due to lack of testing.
the f ailure of the second assertion reveals a new bug when comparing large unequal numbers.
this bug was never found or discussed in the gson project.
in commit commit hash a263a3f right after developers discovered the integer comparison bug in commit the developers further modified the same decimal comparison branch but the invariant differential in fig.
10b shows that similar to exit point in fig.
10a at the exit point of the decimal branch the return value is incorrectly correlate d to the specific type of value attribute.
this is a new bug discovered with g etty .
we submitted a bug report to the gson project along with the test case in fig.
and it awaits action .
many of the problems identified here could in principle have been identified through test coverage reports.
however although standard test coverage tools confirm that the conditions of a branch were tested they don t reveal coverage of the doma in and range of methods.
invariant differentials directly state th e anomalous properties of the input e.g.
other is never null or output pointing to what kinds of tests need to be added inputs t hat include null .
and coverage tools do not help in identifying bugs just areas of the code that are insufficiently tested.
for exam ple in commit there were executions of equals for numeric equalities.
among them tested integer equalities and tested decimal equalities.
we did not have to examine all test executions to identify the missing test cases we only examined dynamic invariant differentials.
we concluded not only were they insufficiently tested but also both branches have bugs.
we also notice that most of the information on test quality is a result of the impact isolation analysis section iib that review ers need to perform to understand the source of behavioral changes when looking for bugs.
therefore while other tools exist that can provide similar information on test quality detec ting test problems is a convenient side effect of looking for bugs w ith semantics assisted code review.
v. e ffectiveness of invariant differentials i n s e c t i o n s i i a n d i v w e r e p l a y e d p a r t o f t h e h i s t o r y o f gson to demonstrate that scr with g etty can help reveal insufficient testing and find bugs.
using the six open source pro jects studied in section iii table ii this section addresses the question of whether scr is effective on a regular basis.
a. identifying test insufficiency using g etty we applied scr on test only commits randomly selected from the projects.
the number of commits for each project varies according to the project size and histo ry length see table iii .
the maven emma plugin reports branch test coverage for the chosen testing commits.
we inspected the invariant differences of each commit to identify inadequacies in the testing of the methods under test in the co mmit.
the question is whether or not the invariant differences were able to expose insufficiency and why.
as a simple metric we consider tests sufficient for a method if they cover all combinations of types that result in different behaviors of the method.
for example when testing equals for two integers see section iv we want to see test cases for all co mbinations of regular and big integers each with equal and unequal values.
although this overlooks corner cases i.e.
it low ers the experiment s success rate it is a straightforward repeat able metric.
we determined ground truth by exhaustively inspecting the tests and source after making the first determination with getty .
we summarize our code review results in table iii.
cumulatively of the testing commits were identified as bei ng insufficient.
g etty led us to incorrectly classify commits as results of test sufficiency inspection projects commits insuff false insuff little data gson cli codec crypto collections configuration cumulative insuff test insufficiency evident false insuff.
incorrectly classified as insufficient little data too few invariants results of buggy commit inspection projects introduction commit discovery commit time between commits y n gson b634804 60ef777 day y cli e366a69 085a153 months n codec b9cab09 days n crypto f3c5416 a983a2c days y collections eced882 59c6e94 days y configuration f59158e d6c3900 days y y n whether csr lended insights to discover the bug at the time o f its introduction participant labels e1 e2 e3 e4 e5 e6 c1 c2 c3 left la la ln le le le la la le right ra ra rn re re re ra ra re label group number seat position l r experience l evel n e a n novice under two years e experienced under years and a advanced years .
exit points point return false o this.value is long point return true a commit the invariant differential clearly indicates a bug.
exit points point return false o this.value is lazilyparsednumber b ommit invariant differential points to bug like commit s. fig.
.
selected dynamic invariant differentials across commit s. for presentation purposes fully qualified names have been shortene d. fig.
.
test case to confirm the decimal equality bug.
public void testunequaldecimals jsonprimitive smaller new jsonprimitive .
jsonprimitive larger new jsonprimitive .
assertfalse smaller larger smaller.equals larger bigdecimal dmax bigdecimal.valueof double.
max value jsonprimitive smallbd dmax .
new jsonprimitive dmax.add new bigdecimal .
jsonprimitive largebd dmax .
new jsonprimitive dmax.add new bigdecimal .
assertfalse smallbd largebd smallbd.equals largebd authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
insufficiently tested due to invariants derived from coinciden tal correlations in the data.
of the remaining commits had too few invariants generated to support a judgment.
reviewer experience might aid in improving these numbers.
one fifth of the commits did not produce sufficient invariant differentials for the task.
given the simple nature of our insufficiency metric these results are quite positive.
these results corroborate res ults that popular test coverage tools are not always a good indicato r of test suite effectiveness .
during this study we observed two simple invariants that often exposed inadequate testing both seen in the scenario in se ction ii.
one is the failure to test for null as an input value e.g.
other z null .
the other is a boolean return value always being true or false.
additionally invariants on exception exits were helpful in confirming behavior after a failure was intentionally induced by a test case.
b. finding bugs we randomly selected one confirmed bug from each project including a new one from gson found the commit that introduced the bug and applied scr to check whether the bugs could have been found when introduced.
as ground truth we checked out the commit confirming the existence of the bug executed th e failed test s traced the buggy method s behaviors using the eclipse jdt remote debugger and studied the root cause of the bug.
then we used git blame to trace back through the editing history to find the commit that made the buggy edit.
fi nally we used g etty to review the semantic changes of the commit that introduced the bug.
see table iv.
the differentials aided discovery of four of the six bugs between and days before the bug was reported.
for the two that failed daikon s lack of invariants over the contents of strings was the cause.
for the bug in issue cli the command line parser threw an exception when parsing an option that is the prefix of another.
the bug was introduced when developers added prefix matching to the parser.
the bug in codec concerns a string encoding using the k lner phonetik algorithm that contains sequentially repeated digits.
vi.
a comparative user study i n t h i s s e c t i o n w e d e s c r i b e a c o m p a r a t i v e u s e r s t u d y t h a t demonstrates that real reviewers can effectively use g etty to improve the quality of their code reviews.
a. study set up we arranged the study to simulate a lightweight code review on commits of the gson project.
we identified three roles developer reviewer and internet helper.
the developer is the programmer who implements the functionality or fixes the bug and updates the issue played by investigators .
a reviewer is a separate engineer who reviews the code changes and provides comments to the developer played by our study subjects .
the internet helper plays the role of search engines e.g.
google online q a communities e.g.
stackoverflow and more for g etty since the tool being a prototype has no online presence role played by the investigators .
we enrolled anonymous participants with to years of programming experience in academia or industry table v .
all our participants fill out a questionnaire and we rated their programming experience as novice under two years experienced under years and advanced years .
we grouped our participants into two member review teams of comparable experience.
six performed reviews with g etty experimental groups .
the other three used the code diff tools available in github control groups .
we set up pair programming style reviews to avoid the negative impact of approaches like the think aloud protocol where researchers may unintentionally influence what participants say and do by asking them about their work .
we simulated a real code review environment.
all teams performed their reviews in the same quiet lab.
we used two apple inch imac s one used for the code review the other used by the experimental investigator to reply to reviewers comments through the issue tracker reset issue states and perform rela ted tasks.
we video recorded all sessions and interviewed all participants afterwards.
we set up a two phase review process once reviewers gave us feedback we would provide the fixes suggested and ask for a second review.
we bound the experiment s duration by limiting the number of iterations to two and by preparing a comprehensive set of fixes that would fix all issues reported by previou s reviewers plus all the ones we had found using g etty .
we chose three issues and commits from the gson project.
all commits passed their original code review process but were later found to be insufficiently tested or suffered from undisc overed bugs.
issue was described in section ii.
issue implements two new methods equals and hashcode in class lazilyparsednumber .
these methods are part of the jav a o b jec t in t erf ace an d n ee d to c o rr ec tly com pa re th e cl as s values.
lazilyparsednumber is complex because it keeps a string representation of numbers.
so just comparing strings can give the wrong results for different representations of the same num ber.
in this patch the developers implemented both methods and tested them.
however most tests are trivial.
for example both equals and hashcode are tested comparing two objects whose string value is .
reviewers should find these tests inadequa te.
in addition there is a subtle bug.
according to the java speci fication developers should ensure that equals and hashcode are consistent i.e.
two lazilyparsednumber objects where equals returns true must have identical hashcode values.
this is not the case in this patch.
issue contains patches to jsonparser .
developers updated the application logic of jsonnumber a helper method used by the parser to return a jsonprimitive object if the value string can be parsed as a number.
to avoid overflow and precision information loss during parsing they used biginteger to parse all integers and bigdecimal to parse decimal numbers.
later it was decided that this change unintentionally masked overflow errors i n getasbiginteger and getasint in class jsonprimitive .
b. initial observations overall the experimental teams liked using g etty .
they acknowledged that inspecting semantic changes is necessary and helpful to code review and that they would like to incorporate authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
similar procedures into their daily code review process.
they provided useful suggestions for improving g etty e.g.
linking a method s invariants to the test cases that contributed to its t race.
we also obtained anecdotical evidence that g etty helps reviewers produce higher quality reviews.
for example all the ex perimental teams discovered missing tests in the first two issu es and suggest good tests.
on the other hand two of the teams tha t did not have access to g etty struggled with it.
in issue for example the reviewers using g etty found that tests were missing and gave pointed suggestion on how to improve testing.
some of these suggestions are test for big numbers that might overflow negative numbers examples such as .
or .
that cannot be represented as integer values and invalid inputs e.g.
a123 .
reviewers using getty noted that only longvalue is tested with decimals but that bigdecimal is not being tested in intvalue .
on the other hand only one of the three control teams correctly identified the missing tests.
one of the control teams missed the problems completely.
they reported functionality looks good .
c3e misinterpreted the code and suggested that the existing tes t for value .
was wrong and to replace string .
with .
we also validated our implicit hypothesis that a good understanding of invariants is required to successfully use g etty .
our team of novices struggled with issue because they could not decipher what some of the invariants meant.
none of the other teams experienced such problems.
c. effects of tools on review process the main finding from our study is that g etty substantially modified the review process of our participants.
as exhibited i n section ii g etty led to a hypothesis driven process in which reviewers used the invariant diffs to develop an initial un derstanding of the effects of code changes and then used g etty to navigate the code and validate their hypotheses.
in simple cases they sometimes skipped the validation step and just adde d a comment to their review.
while there are small variations in how the experimental teams performed their reviews scr using g etty followed the general process highlighted in fig.
.
reviewers started b y looking at the goal of the commit expressed as an issue in the bug tracking system or in the commit message .
next re viewers focused on the methods where invariants had changed.
for each such method they analyzed the invariant diffs presented by g etty as described in section ii for fig.
and by looking at these invariants they generated hypotheses on wha t had been changed or not .
these hypotheses ranged from sus pecting missing tests or potential bugs to clarifications on t he meaning of the goal described for the commit e.g.
in section i i e2ra hypothesizes that there was a single test for the new code .
once the reviewer has some working hypothesis for what is going on he or she read or searched the code to verify the hy pothesis.
this activity is specifically directed at confirming or disproving the hypothesis and the code was efficiently navi gated using g etty s navigation features e.g.
the call graph in fig.
.
next the reviewers usually confirmed that require ments were correctly understood by the reviewer considering the code and invariant changes observed.
finally once all the methods with changed invariants have been studied the reviewers decided to either accept the patch or suggest changes.
this process contrasts with the control teams.
their primary modality was wide scale code reading in git diff in an effort to understand the behavior of the old and new code.
this process consistently induced fatigue leading to loss of focus mistake s and sometimes giving up when the semantic changes were not evident from the code diffs.
in issue c1la gave up and as sumed that the behavior of getasbiginteger and getasint must be correct as their code was unchanged it s totally plausible to be the getasbiginteger getasint currently implement that func tionality already given a number of n. but i don t know.
d. limitations and threats to validity limitations .
g etty uses daikon to extract invariants from code.
this means that it inherits some limitation that affect i ts ability to discover some issues.
for example the second issue in the user study of section vi contains a subtle bug.
this bug arises from the incorrect interplay of the equals and hashcode methods of the lasilyparsednumber class.
unfortunately to expose this bug g etty should provide class invariants using these two query methods.
however daikon is not able to detect which methods are queries do not modify the state and to use them i n invariants.
another limitation arises from the optimizations needed to make g etty s performance acceptable.
instead of extracting invariants for each method we compute an impact set as described in section iii.a.
while this works well in practice there could be cases where some important invariant is not computed and presented to reviewers.
threats to internal validity .
we identified multiple factors in our experiments that could potentially affect their validity .
our experiment participants were not familiar with g etty nor with the projects they reviewed.
yet the control participants were familiar with their tools perhaps pessimizing our results .
in code review it s subjective whether one test or another is required and so our bar for a successful review was subjective on this count.
we addressed this by analyzing how g etty was used instead of fixing an arbitrary bar for successful review.
threats to external validity .
while we endeavored to maximize external validity there are a few limitations in our exp eriments.
we had a limited number of participants in our study and although they are all members of population of reviewers they may not be representative of the population.
we mitigated this risk by selecting students with distinct experience levels including several with industry experience.
still our participant s may not be representative because they are mostly graduate stu dents and four have a background in programming languages.
a second threat is the representativeness of the issues we used in our experiments.
we randomly chose problems from open source projects.
still there may be a bias because we had to choose a projects that were relatively well tested and b co mmits that were not too complex to bound the length of the study .
because g etty is a prototype and does not have manuals or fig.
.
review process with g etty authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
forums that can be googled for answers on its use the investigator first trained the users and then answered questions about the tool and its use during the experiment.
this could have in fluenced the result either positively or negatively.
finally our study was conducted in a simulated code review environment with an accelerated timeline reviewers would get feedback on their reviews immediately .
consequently in the second review phase the issue was still fresh in the minds of the reviewers.
vii.
r elated work many researchers have appreciated the value of differencing static semantic information.
lahiri vaswani and hoare from microsoft research discuss differential static analysis .
s everal promising applications are highlighted including semantic differencing and differential contract checking.
person et al.
proposes differential symbolic execution to detect and characteriz e the effects of program changes in terms of behavioral differences then use a theorem prover to compare the symbolic summaries for such differences .
iproperty hits the similar idea .
s ymdiff presents differential assertion checking for comparing different versions of a program with respect to a set of assertions.
the approach defines relative correctness the s econd program version does not violate assertions the first one s atisfies.
although it provides a weaker guarantee than outright c orrectness it is more tractable than traditional assertion check ing and is still powerful the authors of s ymdiff were able to soundly verify null pointer dereferencing bugs.
however the practical use of differential static analysis is limited becaus e the approach generally requires users to write assertions intermed iate contracts or worse proof scripts all of which impose ver y high overhead to programmers .
moreover static analysis can be overly conservative limiting the value of its inference s. directly related is dynamic impact analysis.
chianti executes tests on two versions of the code and differentiates their runt ime behaviors then it decomposes the difference into a set of pred efined atomic changes like add a new class remove a method change definition of static initializer etc.
and t hen relates those changes to affected tests .
chianti is partic ularly helpful in isolating changes that lead to a test failure.
idise considers dynamic calling context information from inter proce dural analysis to categorizes impact behaviors and extends no tions of test coverage by change impact information .
alt hough chianti and idise are debugging tools their underlying technologies could be applied to code review.
our work follows the spirit of software change contracts which define a formal language to summarize how patches change code behavior.
change contracts are used to specify the intent of code changes and can be verified at runtime .
preliminary work explored extracting these contracts by tracing the execution of test cas es .
instead of specifying expected changes getty helps reviewers discover how code changes affect program behavior we use daikon to extract generalized likely invariants provide im pact analysis and support navigation via the call graph.
holmes and notkin take a hybrid static dynamic approach to semantic differencing .
their approach analyzes invocation dependencies based on their presence in each of four graphs th e static call graph and the dynamic call graph from each of the two versions given.
a visualization of the differences in the cross product of the graphs can reveal anomalies that motivate further inspection.
for example a developer updating a thirdparty library and expecting the system to behave the same would be surprised to find the control flow has changed at runtime y et not in the static call graph.
differences in static and dynamic call graphs are less detailed and more removed from the code than invariant differences.
for example commit discussed in section iv would have fallen into the consistent s d partition because the changes to equals produced consonant static and dynamic call graph changes.
however g etty s invariant differentials told the reviewer that the commit actually contains a bug .
randoop employs invariant differencing to determine when random test case generation can halt that is when adding tests stops improving the invariants .
their differencing method that supports some understanding of logic is more sophisticated than our current textual approach.
randoop and other approaches such as palikareva et al.
s method of performing concolic testing on a unified version of two versions of the code to automatically generate test inputs for new program paths introduced in the code can be used to automatically generate the tests needed to run g etty .
gerrit supports distributed code review by providing a staging area for changes where they can be reviewed prior to committing to the repository .
phabricator is a platform integ rating many tools including git diff based code review .
sinc e our g etty tool generates results as html files they could be integrated into these existing platforms.
viii.
conclusion traditional code review is semantically poor.
a summary of behavioral differences can enable the reviewer to see what behaviors have changed since the last commit.
the daikon dy namic invariant extractor can be used to mine behavioral differ entials.
to isolate effects due to changes in source versus tes ts we must run daikon on different combinations of sources and tests placing extreme demands on performance.
running dai kon on just the methods of interest reduces run time.
also par titioning daikon s inference process on a per class basis reduc es memory pressure and parallelizes well.
our studies of six open source projects shows that g etty consistently aided in uncovering insufficient testing and findi ng bugs at their point of introduction.
our case study further sho ws that g etty changes reviewers process from reading all the code to generating hypotheses that focus their analysis ultimately producing better review comments.
acknowledgment this research was supported in part by nsf grant ccf1719155.
we thank sorin lerner for his advice on static program analysis michael ernst and his team for their help with daikon philipp hirch for his contribution to implementing exception handling in chicory the participants of our user study and th e anonymous reviewers for their insightful comments.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.