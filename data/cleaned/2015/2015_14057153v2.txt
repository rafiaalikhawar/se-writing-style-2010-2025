efficient and reasonable object oriented concurrency scott west google inc. switzerland scottgw google.comsebastian nanz bertrand meyer department of computer science eth z rich switzerland firstname.lastname inf.ethz.ch abstract making threaded programs safe and easy to reason about is one of the chief di culties in modern programming.
this work provides an e cient execution model for scoop a concurrency approach that provides not only data race freedom but also pre postcondition reasoning guarantees between threads.
the extensions we propose in uence both the underlying semantics to increase the amount of concurrent execution that is possible exclude certain classes of deadlocks and enable greater performance.
these extensions are used as the basis of an e cient runtime and optimization pass that improve performance over a baseline implementation.
this new implementation of scoop is on average also faster than other well known safe concurrent languages.
the measurements are based on both coordination intensive and data manipulation intensive benchmarks designed to o er a mixture of workloads.
categories and subject descriptors d. .
language constructs and features concurrent programming structures d. .
processors code generation optimization run time environments keywords concurrency object oriented performance optimization .
introduction programming languages and libraries that help programmers write concurrent programs are the subject of intensive research.
increasingly special attention is paid to developing approaches that provide certain execution guarantees they support the programmer in avoiding delicate concurrency errors such as data races or deadlocks.
for example all research was done while employed at eth z urich opinions in this paper do not necessarily re ect those of google inc.languages such as erlang and others based on the actor model avoid data races by a pure message passing approach languages such as haskell are based on software transactional memory avoiding some of the pitfalls associated with traditional locks.
providing these guarantees can however be at odds with attaining good performance.
pure message passing approaches face the di culty of how to transfer data e ciently between actors and optimistic approaches to shared memory access such as transactional memory have to deal with recording committing and rolling back changes to memory.
for this reason execution strategies have to be developed that preserve the performance of the language while maintaining the strong execution guarantees of the model.
this work focuses on scoop an object oriented approach to concurrency that aims to make concurrent programming simpler by providing higher level primitives that are more amenable to standard programming techniques such as pre postcondition reasoning.
to achieve this goal scoop places restrictions on the way concurrent programs execute thereby gaining more reasoning capabilities but also introducing performance bottlenecks.
to improve the performance of scoop programs while maintaining the core of the execution guarantees this paper introduces a new execution model called scoop qs1.
we rst give a formulation of the scoop semantics which admits more concurrent behaviour than the existing formalizations while still providing the reasoning guarantees.
on this basis lower level implementation techniques are developed to make the scheduling and interactions between threads e cient.
these techniques are applied in an advanced prototype implementation .
the design and implementation choices are evaluated on a benchmark suite that includes computation intensive and coordination intensive workloads showing the advantages of the scoop qs execution strategies.
the overall performance is compared to a broad variety of other paradigms for parallel and concurrent programming c tbb go haskell and erlang demonstrating scoop s competitiveness.
the remainder of this paper is structured as follows.
section introduces scoop and formally speci es executions.
section describes the implementation techniques for this model.
section evaluates the e ectiveness of the di erent optimizations.
section compares scoop qs against a variety of other paradigms.
an analysis of related work is performed in section and conclusions are drawn in section .
1qs is pronounced queues as queues feature prominently in our new approach the runtime and compiler associated with qs is called quicksilver available from .arxiv .7153v2 jul .
execution model the key motivation behind scoop is providing a concurrent programming model that allows the same kinds of reasoning techniques that sequential models enjoy.
in particular scoop aims to provide areas of code where pre postcondition reasoning exists between independent threads.
to do this scoop allows one to mark sections of code where although threads are operating concurrently data races are excluded entirely.
.
a brief overview in fig.
one can see two programs that are running in parallel.
supposing that xis the same object in each thread separate x do x.foo a long comp x.bar end thread 1separate x do x.bar b short comp c x.baz end thread figure a simple scoop program there are only two possible interleavings x.foo x.bar x.bar x.baz or x.bar x.baz x.foo x.bar however in contrast to synchronized blocks in java these separate blocks not only protect access to shared memory but also initiate concurrent actions for both threads the calls on xare performed asynchronously thus for thread x.foo can execute in parallel with long comp .
however itcannot be executed in parallel with x.bar as they have the same target x. scoop has another basic operation the query that provides synchronous calls.
it is so called because the sender expects an answer from the other thread this is the case with the c x.baz operation where thread waits for x.baz to complete before storing the result in c. the scoop model associates every object with a thread of execution its handler .
there can be many objects associated to a single handler but every object has exactly one handler.
in fig.
xhas a handler that takes requests from threads and .
the threads that wish to send requests to xmust register this desire which is expressed in the code by separate x.the threads are deregistered at the end of the separate block.
this model is similar to other message passing models such as the actor model .
what distinguishes scoop from languages like erlang is that the threads have more control over the order in which the receiver will process the messages.
when multiple processes each send multiple messages to a single receiver in erlang the sending processes do not know the order of processing of their messages as they may be interleaved with messages from other processes .
in scoop since each thread registers with the receiver the messages from a single separate block to its handler will be processed in order without any interleaving.
this ordering gives the programmer the ability to reason about concurrent programs in a sequential way within the separate blocks.
to be precise pre postcondition reasoning can be applied to a separate object protected by a separate block even though the actions are being executed in parallel.
aseparate object is marked as such by the type system h c0 c1 c2 c3 figure normal handler implementation and methods may only be called on a separate object if it is protected by a separate block.
maintaining reasoning among multiple independent separate objects is also possible but requires all separate objects concerned be protected by the same separate block.
the original scoop operational semantics mandated the use of a lock to ensure that pre postcondition reasoning could be applied by a client on its calls to a handler.
one can visualize this as the client c0placing the calls in a queue for the handler hto dequeue and process as in fig.
.
the other clients c1 c2 c3 that may want to access the handler s queue must wait until the current client is nished.
.
reasoning guarantees there are a few key reasoning guarantees that an implementation of scoop must provide .regular non separate calls and primitive instructions assignment etc.
execute immediately and are synchronous.
.calls to another handler h on which object xresides within the body of a separate xblock will be executed in the order they are logged and there will be no intervening calls logged from other clients.
the e ect of rule is that normal sequential reasoning is applied to calls that are issued by the client to the client.
rule implies that calls that are made from the client to the handler are applied in order thus the client can apply pre postcondition reasoning from one call it has made to the next.
.
the scoop qs execution model the rst scoop guarantee is easy to achieve it is simply how sequential programs operate.
to understand how to implement scoop e ciently it is important to concentrate on the the second guarantee.
this guarantee states that the requests from a particular client are processed by the handler in the order they are sent disallowing interleaving requests from other clients.
to prevent clients from interfering with one another on a particular handler can be achieved by giving each client their own private area a queue in which to place their requests.
each client then just shares this private queue with the handler to which it wants to send requests.
syntax.
the following syntax of statements sis used to describe the execution model.
s separate x sjcall x f jquery x f j waithjrelease hjendjskip note that separate blocks and call and query requests model instructions of scoop programs whereas the statements wait release end and skipare only used to model theseparate h qh separate x s jj x qx t h qh s call x end jj x qx t call h qh call x f jj x qx t h qh skip jj x qx t query h qh query x f jj x qx t h qh waitx jj x qx t sync h qh waitx jj x qx release h h qh skip jj x qx skip run h ss ys skip h ys s end h ys end h ys skip seq h xs s1 h xs s0 h xs s1 s2 h xs s0 s2 seqskip h xs skip s2 h xs s2 parstepq q0 pjjq pjjq0 onestepp q p qmanystepp p0p0 q p q figure inference rules of scoop qs runtime behaviour.
in particular statements waitandrelease describe the synchronization to wait for the result after a query statement endmodels the end of a group of requests and skipmodels no behaviour.
operational semantics.
in fig.
an operational semantics that conforms to the scoop guarantees is given.
it is described using inference rules for transitions of the form p q where pandqare parallel compositions of handlers.
the jjoperator is commutative and associative to facilitate appropriate reordering of handlers.
the basic representation of a handler is a triple h qh s of its identity h request queue qh and the current program it is executing s. a request queue is a list of handler tagged private queues and is thus really a queue of queues.
private queues of a client handler ccan be looked up qh and can be updated qh where lis the new list to associate with the handler h. both lookup and updating work on the lastoccurrence of c which is important as this is the one that the client modi es.
the queue can also be decomposed structurally with ysmeaning that the head of the queue is from client xwith private queue s and ysis the rest of the structure possibly empty .
so although the private queues in the queue of queues can be accessed and modi ed in any order they are inserted and removed in rst in rst out order.
we describe the unique operations of fig.
separate blocks the rule separate the two di erent kinds of requests call query sync rules and how these requests are processed by the handlers run andend rules .
the sequential and parallel composition rules are de ned in the standard way.
in the rule separate clients insert their private queue at the end of the handler s request queue.
this operation occurs at the beginning of a separate block.
this registers them with the handler who will eventually process the requests.
the fact that a handler only processes one private queue at a time ensures that the reasoning guarantees are maintained.
it is also a completely asynchronous operation as the supplier s handler triple only consists of variables i.e.
there are no restrictions on what state the supplier has to be in for this rule to apply.
additionally the client appends a call x end action before the end of the separate block to signal thatthe supplier xcan take requests from other clients.
the scoop qs semantics in contrast to the original lock based scoop semantics uses multiple queues that can all be accessed and enqueued into simultaneously by clients.
this behaviour is visualized in fig.
where the outer gray c0 h c1 c2 figure handler implementation based on queue of queues boxes are nodes in the queue of queues and the inner green boxes are nodes in the private queues.
this nested queueing maintains the reasoning guarantees while still allowing all clients to enqueue asynchronous calls without waiting.
in rule call the call action is non blocking it asynchronously appends the requested method fto the end of the appropriate client s private queue.
rule query requesting a query execution however does require blocking as it must wait for the result of the function application.
this is modeled by sending the query request and introducing a pair of actions which can only step forward together the wait release pair.
there is only one rule sync that can rewrite these into skip and it can only do so when both processes are executing each of the pair.
each handler processes its request queue in the following way in rule run if the handler is idle executing skip then it will examine the request queue.
if the request queue s rst entry a private queue is non empty then the rst action is taken out of that private queue and placed in the program part of the handler to execute.
if the request queue is empty or it contains an empty private queue as its rst entry then the thread does nothing.
in rule end the thread nishes one private queue and switches to the next when it encounters the endstatement which was placed by the owner of the outgoing private queue when it nished executing its separate block rule separate .
.
multiple handler reservations the separate block as shown so far only reserves a single handler and this block provides race freedom guaranteesbetween a single client and a single handler.
however a client may want to ensure consistency among multiple handlers or objects.
to provide guarantees about multiple handlers a multiple handler separate block must be used as in fig.
.
in this example this has the e ect that whenever a client separate x y do x.set red y.set red end a thread 1separate x y do x.set blue y.set blue end b thread figure multiple reservations reserves both xand y the colours of each object will be the same either both red or both blue.
when written in this way and executed under either scoop or scoop qs any client that comes after the execution of thread or thread or both and reserves xand ytogether will always seex.colour y.colour .
if using nested reservation this may not be the case due to a possible race enqueueing the private queue into the queue of queues.
the modi cation to the separate rule to support this is straight forward.
first one de nes an update function that updates a handler if it is in the set x. resone x h x qx t x qx t if x2x x qx t if x 2x then this is applied over the parallel composition of all handlers.
resmany x h pjjq resmany x h p jj resmany x h q resmany x h x qx t resone x h x qx t lastly a function describes that each handler in the set represented here by a list so it can be traversed is sent an endmessage.
endmany x xs call x end endmany xs endmany skip these functions combine to de ne a generalized separate rule that can reserve multiple handlers atomically.
separatep0 resmany x h p ends endmany x h qh separate x s jjp h qh s ends jjp0 .
deadlock under the original handler implementation of scoop the program in fig.
will deadlock under some schedules.
this is due to the inconsistent locking order of xand y. however in the scoop qs execution model this example cannot deadlock because there are no longer any blocking operations both clients can simultaneously reserve the handlers xand y and log asynchronous calls on them.
deadlock is still possible in scoop qs however one must also use queries which block to achieve the same e ect.
if x.query and y.query are added to the innermost separate blocks of client and client respectively the program may deadlock.separate x do separate y do x.foo y.bar end end client 1separate y do separate x do x.foo y.bar end end client figure possible deadlock situation .
implementation the semantics described in section are used to implement a compiler and runtime for scoop programs.
the operational semantics gives rise to notable runtime performance and implementation properties.
we pay particular attention to how to move the implementation from a synchronizationheavy model to one which reduces the amount of blocking.
the runtime for scoop qs is written in c the compiler is written in haskell and targets the llvm framework to take advantage of the lower level optimizations that are available.
using llvm is a necessary choice for this work because it is important to compare with other more mature languages and the comparison should not focus on obvious shortcomings such as a lack of standard optimizations.
llvm is also built to be extended this work extends llvm by adding a custom optimization pass.
the scoop qs compiler runtime and benchmarks are available from github .
the runtime is broken into layers task switching lightweight threads and handlers.
some of the optimizations described in this section take place at the handler layer but there are also some that use the other two layers as well to optimize scheduling.
.
request processing therun andendrules describe all of the queue management facilities that a handler has to perform.
this correspondence is shown in the high level implementation of the main handler loop given in fig.
.
run rule when there is a private queue available while qoq.dequeue private queue if dequeue returns true run rule process calls from this queue.
otherwise end rule switch to the next private queue while private queue.dequeue call execute call call figure main handler loop the structure of the handler s loop directly corresponds to the data structure implementation a queue of queues .
one can see that private queues are continually taken from the outer queue where the dequeue operation returns a boolean result.
false corresponds to no more work indicating theprocessor can shut down not that the queue is empty as may be in a non blocking queue implementation.
for each private queue that is received calls are repeatedly dequeued out of it and executed until false is returned from the dequeue operation indicating that the endrule has been triggered and the client presently does not wish to log more requests.
note that the arrangement of clients and handlers follows a particular pattern when the queue of queues pattern is used.
namely that each handler rst reserves a position in the queue of queues each queue of queues has many clients trying to gain access but only one handler removing the private queues.
this is a typical multiple producer single consumer arrangement so an e cient lock free queue specialized for this case can be used to implement the queue of queues.
similarly once the private queue has been dequeued by the handler the communication is then single producer single consumer the client enqueues calls the handler dequeues and executes them.
again an e cient queue can be constructed to especially handle this case.
these optimizations are important as they are involved in all communication between clients and handlers.
.
client requests the handler loop implementation above resides in the runtime library.
the client side is where the compilation and runtime system meet.
in particular the compiler emits the code allowing the client to package and enqueue requests for the handler and handle waiting for the results of separate queries.
private queue h p client.queue for h separate rule adding an empty queue to the queue of queues h.qoq.enqueue h p compiled body separate rule compiler adds the code to enqueue the end marker h p.enqueue end figure a compiled separate block when a client reserves a handler with separate hdo body end this corresponds to the code shown in fig.
.
the client receives a private queue h pfor the desired handler h represented in the separate rule by the private queue appearing on the handler s queue of queues.
this private queue can either be freshly created or taken from a cache of queues to improve execution speed.
the client then enqueues this new private queue on the queue of queues for the handler which means the private queue is now ready to log calls in the body.
finally corresponding to the end of the separate block the constant denoting the end of requests is placed in the private queue allowing the handler to move on to the next client.
there will typically be calls to the handler in the body of a separate block.
the asynchronous calls are packaged using the lib library which abstracts away the details of various calling conventions.
this packaged call is then put into the proper private queue for the desired handler.
this can be seen in fig.
the enqueue operation relating directly to the call rule.
packaging the call entails setting up the call interface cif with the appropriate argument andarg types ffi type pointer arg values arg ffi prep cif ffi call ffi default abi ffi type void arg types call rule showing the setup via libffi.
h p.enqueue call new ffi call arg values figure enqueueing an asynchronous call return types with ffi prep cif and then storing the actual arguments for later application by the handler.
note that the allocation of arguments and argument types for the call cannot be done on the client s stack because the call may be processed by the handler after the client s stack frame has been popped.
for e ciency reasons a di erent strategy is used for synchronous calls queries .
this is because packaging a call involves allocating memory populating structures and the handler must later unpack it.
in short this takes longer than a regular function call.
in the asynchronous case these steps are unavoidable because the execution of the call must be done in parallel with the client s operations.
however for synchronous calls this is not the case the client will be waiting for a reply from the supplier when the supplier nishes executing the query.
to make use of this optimization opportunity for shared memory systems we can change the query rule to the following h qh query x f jj x qx t h qh waitx f jj x qx t note that the execution of the call fis shifted to the client after the synchronization with the handler has occurred.
this does not change the execution behaviour because as in the original rule all calls on the handler are processed before the query and the client does not proceed to log more calls until the query has nished executing.
as can be seen from fig.
the old rule rst generates the call sends it to the handler packing same as async ffi call ffi call f result query rule h p.enqueue ffi call sync rule h p.sync a generated code for initial sync rule.
new query rule h p.enqueue sync sync rule h p.sync new query rule result f b generated code for modi ed sync rule.
figure executing a query f and then synchronizes these actions come from the combination of the query andsync rule.
the new rule just performs the call after synchronization occurs .
this approach o ers three main bene ts there is no memory allocation required no encoding decoding of the call is required and which call is being made is known statically.
the last item is important as now the underlying optimizer knows which call is being made statically.
this allows optimizations such as inlining.one last optimization uses the knowledge that when the handler nishes synchronizing with a client it will have no more work to do.
therefore it control passes directly from the handler to the client using the scheduling layer of the lightweight threads to avoid global scheduler.
this optimization is safe because the handler will otherwise just be idle and avoids unnecessary context switching.
.
multi reservation separate blocks the code generation for the multi reservation separate block di ers slightly from the single reservation case which is optimized due to it being a simpler operation.
one can see in fig.
that some of the complexity is pushed into the client client.new reservations client.add handler h1 client.add handler h2 client.reserve handlers private queue h1 p client.queue for h1 private queue h2 p client.queue for h2 compiled body h1 p.enqueue end h2 p.enqueue end figure a compiled reservation separate block run time library.
the run time maintains structures that allow the multiple handlers to be stored.
the interface between the compiled code and run time consists of marking the start of a new set of reservations with new reservations adding a handler with add handler and nally safely reserving all handlers with reserve handlers .
the client can now retrieve the private queues that were just reserved they do not need to be inserted into the handler s queue of queues because the reservation mechanism has already done that.
signalling the end of the private queue is done as before.
currently the multiple reservation implementation uses one spinlock for every handler to maintain the ordering guarantees.
however since the number of memory accesses to enqueue in the queue of queues is quite small a more sophisticated implementation could use transactional memory to implement the same behaviour.
these spinlocks were not found to decrease performance.
.
removing redundant synchronization the scoop model essentially prevents data races by mandating that one must access read and write separate areas of memory through their respective handlers.
due to this a common scoop idiom is that memory is often copied back and forth between processors when a local copy is desired for speed reasons.
one example of this is sending data to a worker for further asynchronous processing.
when copying data in scoop there are essentially two options push or pull.
either the data is copied via routines that asynchronously push data to a separate target or the data is synchronously pulled by the client that wants it using queries.
even though the rst option appears to enable more concurrency because it is asynchronous it often isn t the case.
consider sending an array one integer at a time this involves reading the integer from the client packaging the call that will set the integer on the handler sending the call then applying the call.
the speed advantage of utilizing more than one core is dwarfedby the huge cost of issuing the call.
also the second option synchronous pull tends to be more natural as the client knows how and where to reconstruct the data.
therefore it is natural to make queries as e cient as possible.
this was partially addressed using the approach in the previous section using sync operations and executing the query on the client.
there is a further enhancement that can be made to this approach which is eliding unnecessary sync calls.
a sync call is not necessary if the previous call to the desired handler was also a sync call basically if the handler is already synced it doesn t need to be re synced .
we perform this elision in two ways either by dynamically recording the synced status in the runtime and ignoring sync operations on handlers that have already been synced or statically by performing a static analysis.
.
.
dynamic avoidance the dynamic method keeps the synced status in the private queue structure.
when a sync call is made on a private queue nothing happens if the queue is already synchronized the call merely returns and the synced status is una ected.
if the queue is not currently synced the sync message is sent to the handler as usual and when it returns the synced ag is set in the handler re ecting that the handler is processing this private queue but the queue is empty.
.
.
static removal the static analysis starts by traversing the control ow graph cfg .
it annotates every basic block basic blocks being sequences of basic instructions with a set of handlers that are synchronized by the end of the block.
this set of handlers is called the sync set .
the traversal of a function s basic blocks can be seen in fig.
.
each block acts as a while changed6 b2changed changed changed fbg common tb.predecessors.sync set ifb.sync set6 updatesync b common b.sync set updatesync b common changed changed b.successors figure sync set calculation for a function sync set transformer adding and removing handlers from the set.
as an initial input the intersection of the sync sets of all the block s predecessors is used.
the traversal continues until every basic block s sync set has stopped changing.
of course this only says how the blocks are traversed not how a given block s sync set is calculated given the instruction in that block.
this is described in the updatesync function shown in fig.
.
each type of instruction is hanupdatesync b synced forinst2b h handlerof inst synced synced fhgifinstis sync.
synced fhgifinstis async.
ifinsthas side e ects synced otherwise return synced figure sync set calculation for a block dled di erently synchronization calls add the target handlerto the sync set asynchronous calls remove those handlers and anything they may be aliased to and arbitrary calls clear the sync set entirely.
obviously this nal case is quite severe as it has to be because a call could subsequently issue asynchronous calls on all the handlers currently in the syncset.
this can be mitigated by not clearing the sync set for functions which are marked with the readonly and readnone ags.
llvm will automatically add these ags when it can determine that they hold.
the static analysis operates on llvm bitcode and is implemented as a standard llvm pass outside of the base compiler .
keeping the pass outside of the base scoop qscompiler has the advantage that it separates the generation of code from the analysis and transformation of the generated control ow graph.
.
.
example the e ect of the sync coalescing pass can be seen in fig.
.
this program has three blocks with sync operations in each h p.sync x a h p.sync fg true falsefg h p.sync fgb1 b2 b3 a a simple loop before the synccoalescing pass.
x a h p.sync fh pg true falsefh pg fh pgb1 b2 b3 b after sync coalescing syncsets label edges.
figure sync coalescing pass one.
before the sync coalescing pass in fig.
14a the client is reading values out of a handler s array for which a na ve code generator will produce a sync before every array read.
fig.
14b shows the results of the sync coalescing pass in such a situation.
the sync sets are shown explicitly on the edges out of each block.
in this case there are no calls that may invalidate a sync set so the handler h pappears on all edges.
the result of this is that the sync calls in blocks b2 and b3 can be removed.
removing sync calls in the body of a loop can greatly increase performance.
note that even though the sync call in the body of b2 was removed h pstill appears on b2 s outgoing edges because b2 doesn t invalidate the synchronization on h pby issuing an asynchronous call.
it is not always possible however to remove the sync operations even when the processor is apparently una ected.
consider fig.
where there is an additional call to i p. enqueue r in fig.
15a.
enqueueing a call is an asynchronous activity but it occurs on a di erent handler variable.
this is not enough though to conclude the handler h pis una ected as these are only variables and could be aliased to one another.
meaning they e ectively be the same handler.
this means h p.sync x a i p.enqueue r h p.sync fg true falsefg h p.sync fgb1 b2 b3 a a simple loop with an extra asynchronous call on i. h p.sync x a i p.enqueue r h p.sync fhpg true falsefg h p.sync fgb1 b2 b3 b handlers h pand i pmay be aliased no coalescing.
figure ine ective sync coalescing pass that at the end of the b2 block the outgoing edges are labeled visible in in fig.
15b with neither h pori p. if more aliasing information is given to the compiler then it is possible that this ambiguity can be resolved and h pcan be added to the sync set for the block.
the static analysis is important as it goes further towards getting scoop out of the way of the optimization passes.
in the end our implementation uses both the static and dynamic approaches.
the static analysis is used when it can be but it is necessarily conservative.
for the cases where the static analysis keeps an unnecessary sync operation around the dynamic check will eliminate the round trip to the handler.
.
optimization evaluation here we examine the impact of the following optimizations also outlined in section applying no optimizations none .
dynamically coalescing sync operations by recording and checking the synchronization status in the runtime dynamic as in section .
.
.
statically determining unnecessary sync operations and removing them in a compiler pass static as in section .
.
.
usage of the queue of queues and private queues as a handler client communication abstraction qoq as seen in the semantic model given in section .
applying all optimizations all .
for the comparison a variety of workloads are used.
.
computation and coordination based workloads properly evaluating a core runtime mechanism such as scoop qs requires that it be used in a diverse assortment of situations.
with this in mind we categorize the benchmarks that will be used into two main groups parallel problems where concurrency is not part of the functional speci cation but can be used to speed up the execution.
concurrent problems which are de ned by their concurrent behaviour.
the rst work type parallel is often a data processing task where multiple threads each process part of a large data set to decrease the total running time.
the second type of work concurrent is more about the coordination between the threads of control.
here the concurrency is part of the system s speci cation such as a server handling multiple clients simultaneously.
.
.
parallel workloads the benchmark programs we selected for the parallel problems are a selection from the cowichan problem set .
they focus on numerical processing and working over large arrays and matrices.
the programs include randmat randomly generate a matrix of size nr.
thresh pick the top p of a matrix of size nrand construct a mask.
winnow apply a mask to a matrix of size nr sorting the elements that passed the mask based on their value and position and taking only nwfrom that sorted list.
outer constructing a matrix and vector based o a list of points.
product matrix vector product.
these benchmarks can be sequentially composed together the output of one becoming the input to the next to form achain .
this chain is more complex and sizable than the individual and gives a more diverse picture of a language s parallel performance.
.
.
concurrent workloads the concurrent problems focus on the interaction of di erent independent threads with each other.
we have created three benchmarks that represent di erent interaction patterns mutex nthreads all compete for access to a single resource the threads do not depend on each other.
prodcons nproducers and nconsumers each operate on a shared queue the queue has no upper limit so producers do not depend on consumers but consumers must wait until the queue is non empty to make progress.
condition n odd and n even workers increment a variable from an odd even to an even odd number.
each group depends on the other to make progress.
all of the above are repeated for miterations.
finally to this we add two concurrency benchmarks from the computer language benchmarks game threadring threads pass a token around a ring in sequence until the token has been passed nttimes.
chameneos colour changing chameneos mate and change their colours depending on who they mate with.
this is done nctimes.
the combination of these parallel and concurrent benchmarks gives us a balanced view of the performance characteristics of the approach.table normalized to fastest comparison of optimizations on parallel tasks task none dyn.
static qoq all chain .
.
.
.
.
outer .
.
.
.
.
product .
.
.
.
.
randmat .
.
.
.
.
thresh .
.
.
.
.
winnow .
.
.
.
.
setup.
all benchmarks were performed times on a intel xeon processor e7 server .
ghz each with cores physical cores total with gb of ram running red hat enterprise linux server release .
.
language and compiler versions used were gcc .
.
go .
.
ghc .
.
erlangr16b01.
for the parallel benchmarks the problem sizes used arenr p 1andnw for the concurrent benchmarks n m nt and nc .
.
parallel benchmarks as mentioned in the previous section the idiomatic way to transfer data in scoop qs is to have the client pull data from the handler.
this happens in the cowichan problems often as the underlying data structures are almost exclusively large arrays and they must be distributed to and from workers.
this is important for interpreting the results of the comparison of optimizations when they are applied to the cowichan problems.
fig.
displays the communication time as this is by far the factor dominating the execution time normalized to the fastest version and in log scale.
from this it is clear that there is a marked improvement in communication time for the parallel benchmarks over the programs that had no optimizations that reduce the number of sync operations that are performed.
the di erence between having no reduction in the number of sync calls none qoq and employing some reduction technique dynamic static all is that the latter is at least times faster chain near times faster thresh winnow outer product or over times faster randmat .
reducing the number of sync calls is a great bene t for programs that require large numbers of synchronous calls.
there is an additional property of these benchmarks that they access the arrays in a very regular way namely they copy from one array to another generally calling sync many times in a tight loop.
in these cases it is bene cial to fully lift this call right out of the loop body as the static optimization does.
the di erence between the static anddynamic synccoalescing optimizations can be seen in more detail in table .
we can see for example that randmat is 3x faster when using the static sync coalescing optimization compared to thedynamic and others using static sync coalescing are between .1x and .4x faster than dynamic .
.
concurrent benchmarks while the qoq optimization has little e ect on the cowichan problems it has a stronger in uence on the concurrency problems which can be seen in fig.
.
the full results are visible in table .
using qoq is faster chamenos condition prodcons or about the same mutex threadring .
the increase in performance can be attributed to the better utilization of availablechain outer product randmat thresh winnow nonedyn.staticqoqall nonedyn.staticqoqall nonedyn.staticqoqall nonedyn.staticqoqall nonedyn.staticqoqall nonedyn.staticqoqall optimizationtime normalized figure communication times for di erent optimization techniques evaluated on parallel tasks chameneos condition mutex prodcons threadring .
.
.
.
.
nonedyn.staticqoqall nonedyn.staticqoqall nonedyn.staticqoqall nonedyn.staticqoqall nonedyn.staticqoqall optimizationtime s figure comparison of scoop qs optimizations on concurrent benchmarks table times in seconds for optimizations applied on concurrent benchmarks task none dyn.
static qoq all chameneos .
.
.
.
.
condition .
.
.
.
.
mutex .
.
.
.
.
prodcons .
.
.
.
.
threadring .
.
.
.
.
processing capabilities resulting from less blocking in the underlying semantics.
when using qoq there are also fewer context switches since the private queues require only one context switch to wait for a query to return.
when not using the queue of queues a client must wait three times rst for the lock on the handler then the handler must wait for the client to log its query then the client must wait again for the handler to return the query.
however the queue ofqueues approach does introduce some overhead in cases where there are few calls issued in the separate block because it must enqueue twice once putting the private queue into the queue of queues then putting the call into the private queue.
however mostly the qoq optimization is bene cial.
one can also see here the importance of the dynamic sync coalescing optimization.
in particular it bene ts the chameneos condition prodcons and threadring benchmarks.
this underscores the bene t of limiting the roundtrips to the handler when they are not necessary.
when it applies this increases the performance between .
and .
it is important to note here that because the workloads are irregular the static sync coalescing is not as e ective.
its main bene t comes in cases of regular access patterns where it can be applied more readily.
.
summary each optimization has particular situations in which itbrings the most bene t qoq is best on coordination tasks but is not as useful for query heavy workloads.
dynamic sync coalescing is useful on both coordination tasks and tasks with many queries.
static sync coalescing is primarily e ective on very structured query usages beating even dynamic in such situations.
the geometric mean of all benchmarks is .70s for no optimizations .99s for dynamic sync coalescing .24s for static sync coalescing .21s for qoq and .36s with all optimizations.
the net e ect is that the nal scoop qs runtime is faster than the basic runtime.
.
application outside the prototype to provide further evidence of the e ectiveness of the execution techniques presented here a new runtime was constructed for the research branch eve of the ei elstudio ide.
the new runtime which we ve named eve qs incorporates the qoq anddynamic optimizations.
the static optimization was not implemented due to the lack of robust static code analysis and transformation facilities in ei elstudio.
the results are promising the speedup compared to the existing production scoop runtime has a geometric mean of .
on the concurrency benchmarks .
on the parallel benchmarks and a .
across all benchmarks.
the absolute performance of eve qs is lower compared to scoop qs because eve qs inherits several implementation decisions from ei elstudio.
for example the use of a shadow stack for garbage collection inhibiting e cient tight loop optimizations important for the parallel benchmarks.
also storing handler ids in the object header requiring maintenance of a secondary thread safe data structure to lookup the handler data.
since the handlers are accessed incredibly often thisalso is a detriment to performance.
lastly since scoop qs uses lightweight threads situations with high contention such as in the concurrency benchmarks bene t. .
language comparison it is di cult to gauge the appropriateness of a concurrency model without comparing it against its contemporaries.
with that in mind this section presents a comparison of scoop qs with four well established languages.
.
a variety of languages the comparison languages should be modern well known and represent a variety of di erent underlying design choices.
for the purposes of the evaluation this means that we should select from di erent programming paradigms di erent approaches to shared memory di erent concurrency safety guarantees and di erent threading implementations.
we have chosen a selection of languages c tbb threading building blocks erlang go and haskell .
this selection attempts to combine a reasonable number of the facets outlined above to give a complete picture.
to make the diversity clear we present this in table .
the memory column refers to how memory is shared between threads.
erlang has no sharing between di erent processes when data is sent between processes it is copied in its entirety.
in scoop qs the programmer is only able ti access shared memory through a handler.
in haskell it is perfectly possible to construct data races if one uses mutable