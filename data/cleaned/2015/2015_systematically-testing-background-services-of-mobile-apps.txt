systematically testing background services of mobile apps li lyna zhang chieh jan mike liang y unxin liu enhong chen university of science and technology of china china microsoft research china abstract contrary to popular belief mobile apps can spend a large fraction of time running hidden as background services.
and bugs in services can translate into crashes energy depletion device slow down etc.
unfortunately without necessary testingtools developers can only resort to telemetries from user devices in the wild.
to this end snowdrop is a testing framework that systematically identifies and automates background services inandroid apps.
snowdrop realizes a service oriented approach that does not assume all inter component communication messages are explicitly coded in the app bytecode.
furthermore to improve the completeness of test inputs generated snowdrop infers field values by exploiting the similarity in how devel opers name variables.
we evaluate snowdrop by testing commercially available mobile apps.
empirical results show that snowdrop can achieve .
more code path coverage thanpathwise test input generators and .
more coverage than random test input generators.
index t erms app background services test input generation android intents i. i ntroduction while significant efforts have been invested in profiling the mobile app foreground activities we argue that proactively gaining visibility into app background services is equally important.
surprisingly mobile apps can spend asignificant fraction of time running hidden as backgroundservices .
services can continue to run even after theassociated app exits e.g.
pressing the home button on the mobile device or turning off the screen.
services typically handle three types of operations on behalf of the app i periodic app state refresh ii server notifications and iii long running tasks that do not need userinteractions e.g.
music playing and geo location tracking .surprisingly chen et al.
reported that apps can consumea significant amount of energy in the background .
of device battery drain are during the screen off time and28.
are due to apps with frequent background services.rosen et al.
showed that .
of network traffic are due to background services.
furthermore background services canexhibit sensitive behaviors such as collecting location data .
in this work we argue that proactively testing background services can benefit from systematically automating through the service life cycle.
and such testing can complement the foreground ui testing that many app developers are already this is different from passive measures against misbe having services many user centric tools focus on limitingbackground services at run time and otherapproaches wait and collect telemetries from user devicesin the wild .
these tools are passive measures againstmisbehaving services and should not replace app testing.
current app testing tools are mostly for ui testing and they do not consider background service s unique executionmodel.
first services are hidden and not user interactive so there is no guarantee that they are reachable throughany means of ui automation.
instead developers typicallyrely on passing system level inter component communication icc messages or intents to start services.
while our testingscenario seems applicable to related efforts in inferring intentpayloads they strongly assume all intents are explicitlycoded by the developer.
furthermore many efforts cannotreasonably infer valid values for arbitrary developer specifiedfields in intents .
to this end we present snowdrop a developer facing fuzzing framework that systematically discovers an app shidden services and automates their lifecycle to cover all codepaths.
conceptually snowdrop generates a more completeset of test inputs by considering both trigger inputs andexecution inputs.
taking advantage that most app packages are in bytecode that can be decompiled into intermediate representations snowdrop can perform static analysis withoutany developer overhead.
snowdrop addresses the followingchallenges in achieving the testing coverage.
first to test all services of an app snowdrop realizes the service oriented approach that generates test inputs by firstlocalizing all services.
this is different from the icc orientedapproach that simply identifies intents coded in the appbytecode.
in fact if developers opt implicit intents in theirapps the icc oriented approach can have a lower testingcoverage as the target service of an implicit intent is decidedby the os at run time.
furthermore empirical results showthat ui monkey testing has limitations in that .
of services cannot be triggered by ui invocations.
second for the completeness of test inputs generated we aim to infer developer specified fields in intents e.g.
extras key value pairs.
since these fields can syntactically take arbi trary values challenges arise from selecting a value that hasthe appropriate semantics from the vast solution space.
whilerelated efforts generate random test inputs or assume thatall relevant value assignments are in the bytecode snowdropexploits the similarity in how developers name variables.
ourheuristic extends text classification with feature vector from the natural language processing nlp community.
third to exercise all code paths of each identified service snowdrop generates execution inputs that include return values .
c circlecopyrt2017 ieeease urbana champaign il usa t echnical research4 authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
of system apis.
we argue that the low complexity of typical app services suggests that the proven pathwise test datageneration is feasible.
furthermore since each hardwareand system component can have a set of corresponding apis snowdrop maintains finite state models to ensure consistencyacross generated execution inputs.
we summarize our contributions as follows.
this paper motivates the need to proactively test app background services and lays the necessary foundation a whitebox testingframework for background services.
we address challenges inbuilding a such framework and implement an up and runningsystem.
empirical results and case studies from popularandroid apps support snowdrop s potential and practicality.without human inputs snowdrop can achieve .
moreservice code path coverage than existing ic3 based pathwisetest input generators and .
more coverage than randomtest input generators.
finally snowdrop reported crashes on12.
of background services in our app pool and we shareour investigation findings in case studies.
ii.
b ackground this section highlights hidden impacts of services on user experience and describes unique properties of services thattesting needs to consider.
a. hidden impacts of services several measurement studies have tried to quantify previously unaware impacts of services on user experience.
chen et al.
studied the telemetry of energy consumption of android apps on galaxy s3 and s4 devices.
they found that .
of device battery drain are duringthe screen off time i.e.
only background services are run ning .
for .
of apps the background energy expenditureaccounts for more than of the total energy.
lee et al.
reported app processes can spend more time in the background than being in the foreground.in fact many apps periodically generate network usage in the background regardless of user activities.
for example facebook was observed to initiate network traffic every 20minutes.
furthermore rosen et al.
looked at apps and noticed that up to of the total network related energyconsumption happen in the background.
b. android background services services are suitable for operations that do not need user interactions.
depending on the scenario developers can choose one of the three interfaces broadcastreceiver handles short running tasks less than seconds alarm implements time based triggers for short operations and service is suitable forlong running non interactive tasks.
since service has a largerimpact on the app performance and resource consumption thispaper mainly focuses on this type of background services.
lifecycle.
since it is not easy to visually observe the lifecycle of services their behavior might not be as expected.
in android service can be further categorized into beingunbounded and bounded.
while both can be started at anytable i many types of service triggering system events cannot be accomplished with only ui automa tion .these cases account for .
of instances in our pool of apps .
a reachable through ui event trigger event callbacks .
app launch .74ui invocation .
app pause .
b unreachable through ui event trigger broadcasts .
icc from services .43app resume .
timer .
time unbounded service can actually continue running beyond the app s visually perceivable lifecycle rather than until theapp exits .
we elaborate on the lifecycle of unbounded servicesnext.
while the code to start services depends on the category of service used the general idea is similar inter componentcommunication icc or intents.
for unbounded services theintent is sent through the startservice method.
as we describe in the next subsection intents can have arbitrarypayloads as defined by the developer.
therefore determiningthe proper payload for fuzzing is crucial.
in addition to runningto completion e.g.
stopservice orstopself method service can also be forced to terminate by the operatingsystem s low memory killer lmk .
the developer needs toproperly handle the premature termination to avoid any state loss e.g.
ondestroy method .
icc messages intent .
the intent contains six fields specifying the delivery of an inter component communication icc message component the target component name which can be explicitly specified by a string of full qualifiedclass name or can be an abstract action whose target component is implicitly decided by the os at run time.
therefore intents can be explicit and implicit by distinguishing whetherspecifying the component.
action a string to implicitly indicate the action to perform uridata a uri object expresses the data to operates on for the target component datatype a string indicates the type of the uridata category a string represents the additional information about the action to execute extras consists of pairs of extras.key andextras.value i.e.
key value pairs for storing any additional information.
extras.key is of string type and extras.value can take arbitrary type.
iii.
g aps of existing testing tools a. limitations of ui monkeys ui monkeys automate an app by invoking one ui element at each step.
while ui event handlers can send intents to startservices not all services are programmed to be triggered in thisway.
fig.
shows an example of this case where service1 is indirectly triggered by a timer.
to quantify this limitation ofui monkeys we looked at our pool of popular apps andwe counted the number of services whose startservice methods are detected in ui handlers.
table i suggests that .
of services are not reachable through ui invocation.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
not all intents are explicitly coded.
ui starts service1 with an alarm and service2 is started by an implicit intent.
being serviceoriented snowdrop ensures the testing coverage without worrying any implicit redirection by the os.
b. limitations of intent inference tools as the icc message is the direct way to start android background services icc oriented approaches try to localize all intents explicitly coded in the app bytecode and infer fieldvalues for these identified intents based on some heuristics.
asthis subsection discusses with respect to automating services the testing coverage of these icc oriented approaches dependson two factors the number of intents identified and the correctness of field values inferred especially for developer specified intent fields e.g.
extras .
first localizing all intents explicitly coded in the app bytecode can have low testing coverage.
as fig.
illustrates android services can be triggered by implicit intents .
and implicit intents introduce the uncertainty of which ser vices are actually called by the os at runtime.
we quantifythis coverage of automating services with ic3 .
since allrunnable background services need to be declared in the appmanifest file we use the manifest file as the ground truth and remove dummy declarations not implemented .
empirical results show that intents ic3 can find correspond to only83.
of services in our app pool of popular apps.
second many intent field inference heuristics do not consider the field semantics e.g.
url city name geo location coordinates .
since developer specified fields such as extras can syntactically take arbitrary values challenges arise from selecting the appropriate value from the vast candidate space.some tools simply rely on developers to provide most testinputs .
to reduce the manual effort random test input generators generate random intent field values .
while random generators have a low complexity it does not guarantee test input quality.
fig.
.
architectural overview of snowdrop analyzing decompiled app bytecode to generate test inputs with codeminer and automating serviceswith servicefuzzer.
iv .
s ystem overview and architecture snowdrop tackles the challenges of generating test input data necessary to exercise all code paths of every service in an app.
this section discusses two major aspects in the design of snowdrop what test inputs are necessary and how these testinputs can be automatically generated.
these two questions impact the overall testing coverage.
each test case c.f.
def iv .
maps to one code path of a service and it contains service trigger inputs c.f.
def iv .
and service execution inputs c.f.
def iv .
.
the former is necessary to properly start a service and it includes i servicename and ii control dependencies among services.
the latter dictates the control flow of service execution and it includes i intent fields e.g.
extras and ii return values of system apis called.
each code path of every service would have one set of trigger inputs and execution inputs.
definition iv .
.
atest case contains one instance of service trigger input and service execution input.
each case maps to one code path of a service.
definition iv .
.
we define a service trigger input as c cden ... cden n .cis the name of background service.
cden s are services that can send intents to start this service.
definition iv .
.
we define a service execution input as intent ap i ap i ... .
intent is p p.v where pand p.vrepresent a pair of intent field and value respectively.
let ap i be a a.v where aanda.v represent a pair of system api and return value respectively.
fig.
illustrates the system architecture.
the developer submits compiled app packages.
since mobile app packages are typically in intermediate representations e.g.
java byte code snowdrop decompiles app bytecode to get readable bytecode manifest and configuration files.
then as thissection elaborates next the codeminer module uses staticanalysis to generate both trigger inputs and execution inputs.
servicefuzzer takes the generated inputs to dynamically testindividual background services.
codeminer.
unlike the icc oriented approach that assumes all intents are explicitly coded in app bytecode snowdrop opts a service oriented approach to generate test inputs it localizes the code block of individual services and thenstatically analyzes how these services access intent fields.
codeminer takes three main considerations.
first there can be control dependencies among services i.e.
one service starts authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
another service and may pass data as intent values c.f.
v .
second generating execution inputs requires codeminer to look at each code control path in the service and codeminergrounds on the proven pathwise approach and constraintsolving c.f.
vi .
although the pathwise approach has been shown to scale poorly with respect to the code complexity it is practical for testing app background services as they have a much lower complexity an average of .
code paths per service.
third values of intent fields can be arbitrarystrings e.g.
uridata andextras which complicates value inference without knowing the field semantics.
forinstance the developer can store the video url in extras.
therefore codeminer adopts a heuristic that leverages the similarity in how developers name variables c.f.
vi a .
servicefuzzer.
for individual instances of test inputs generated servicefuzzer injects crafted explicit intents with offthe shelf tools such as the android debugging bridge and itmanipulates system api call return values with off the shelftools such as android xposed .
if the service dependencygraph indicates that a service can be started by another service then servicefuzzer does not try to directly automate it.
servicefuzzer takes two additional considerations c.f.
vii .
first a testing session terminates when either the ser vice finishes execution or the developer specified terminationcondition is met.
second since android does not provide per formance counters at per thread level disaggregating counters is necessary to extract per service information.
v. s ervice trigger inputs genera tion this section discusses techniques that codeminer uses to generate service trigger inputs.
a. v ariable assignment analysis v ariable assignment analysis is an information flow analysis to keep track of how a value is passed from variables to variables.
in the case of snowdrop it identifies variables inthe code that originate from either intent fields or systemapi return values.
specifically given a variable in the code codeminer starts backward analysis on each line of codefrom that point.
and it marks variables that have assignmentsleading to the target variable.
for android apps variable assignments can happen in four main places the variable is assigned somewhere at the caller method and passed as an method argument thencodeminer would search all the call methods for the value.
the variable is assigned as a return value of a callee method then codeminer would search the callee method for thereturned value the variable is a class field variable which is assigned somewhere within the class scope.
codemineranalyzes the dfg data flow graph to search the field forthe last time assigned the variable is assigned within the method scope.
b. service dependency graph construction in the service dependency graph each node represents a service and directed edges indicate the caller callee relationshiptable ii percentage of service dependencies tha t can successfully start background services .
service caller num services success rate app on launch .
.
broadcastreceiver .
.
service .
.
i.e.
the source node can send intent to start the destination node .
if a service can be started by another snowdrop cansave time by not directly automating it.
we note that falsenegatives in capturing the dependency do not impact the testing coverage as snowdrop will automate any service withno known dependency.
discover background services c .the number of background services that codeminer can find for subsequent test input generation impacts the overall testing coverage.
sinceall runnable background services need to be declared in theapp manifest file codeminer parses the android manifestfile to list all background services of an app.
however it is possible that the developer declares services that arenever implemented.
to filter out such cases codeminer thensearches each declared service in the decompiled intermedi ate representations.
specifically each android service shouldextend the service class.
locate caller component cden .given a target service name we now identify its caller service.
by definition thecaller service would have code to construct the correspondingintent.
therefore for each intent in the code we performvariable assignment analysis to find the target componentname.
if the name matches our target service we look at the component that sends this intent.
if the component is another service e.g.
service or broadcastreceiver then we add adirected edge in the service dependency graph.
we also noteintents that reside in oncreate of the main activity which means our target service would be automatically started afterthe app launch.
c. microbenchmarks this subsection characterizes service dependency graphs in our pool of apps c.f.
viii a .
table ii shows the distribution of the three components that can send intents to start services indirectly app on launch service and broadcastreceiver.
interestingly there are .
of services that havea dependency with another service and .
can be startedsuccessfully.
in other words there is no need for snowdrop tostart services that can be indirectly started.
table ii suggests that a benefit of leveraging service dependency graph is the reduction in the number of test cases by .
.
vi.
s ervice execution inputs genera tion this section discusses techniques that codeminer uses to generate service execution inputs.
codeminer follows the standard practice of representing a code path by its constraints that need to be solved.
for authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
fig.
.
pathwise constraints of a code path for service1.
satisfying constraints requires fuzzing intent fields and system api return values.
instance fig.
shows one set of pathwise constraints for service1 in fig.
.
constraints on a path are first solved individually to obtain a set of possible values.
then for constraints that have several sets e.g.
network info in ourexample sets are merged by the operation of union or intersection according to the logical operators in the path.
the rest of this section discusses how individual constraints are solved and this requires inferring both intent field valuesand system api return values.
a. intent field inference as mentioned before an android intent object has six fields component action category datatype extras extras.key extras.value uridata.
this component outputs a set of p p.v .
we note that extras.key and extras.value make up a keyvalue pair.
thus when p action component uridata category datatype prepresents the extras.key.
for example in the set action android.intent.action.view username david username represents the extras.key anddavid is the corresponding extras.value.
since android confines what values the first four fields can take developers typically hardcode their values which makes it easy to run variable assignment analysis c.f.
v a .
both uridata andextras.value are more challenging to infer than other fields because they can syntactically take on arbitrary values.
while a naive way is to randomly generatedata for these two fields codeminer improves the semantical correctness of value inference with nlp based heuristic.
next we elaborate on this process.
inferring uridata field.
uridata stores uri address.
while some developers hardcode the uri string uridata can also be dynamically assigned at runtime as we discuss later table vi suggests only .
uridata can be inferred from searching for hardcoded strings.
since datatype specifies uridata s mime type we can assign a uri that matches the mime type.
if the datatype field is not specified in the intent we can use another intent field action to infer the mime type.
this is because the mime type canhave a connection with the intent s action.
for instance if action isaction edit uridata most likely contains the uri of the document to edit.
then we classify uri mime types into the following commonly used groups text plain image application mul tipart video x token audio message.
these are realized by matching datatype andaction.
each group contains a list of candidate uris for the address of objects pre loaded on the emulator or for popular webpages.
inferring extras field.
extras.value can take on arbitrary types and values which depend on the usage scenario ranging from storing web urls to city names.
from analyzingour app pool we made an observation that developers typicallyuse similar key names for the same purpose.
an example is music url and songurl which both reference to some web urls of audio files.
we note that in this example while it is practically infeasible to guess the exact urlstring providing a compatible string is sufficient to carry onthe service execution.
therefore inferring the semantics ofextras.value is crucial for inferring the value.
building on efforts from the natural language community codeminer combines the word2v ec tool and the supportv ector machine svm classification algorithm.
given aword word2v ec computes the feature vector for it.
snowdropuses a popular word2v ec model already trained to recog nize word semantics.
so words that are semantically similarshould have similar feature vectors.
we note that feature vectors are in mathematical representation and they can beclustered by data classification algorithms such as svm.
to train svm to cluster similar feature vectors we first get vectors of extras.keys in our app pool.
for key name composing of multiple words we use the standardpractice of averaging over individual vectors.
then we label eachextras.key s vectors with groups name id text title number action video type folder user audio file version message error date time alarm url image location password json widget.
these groups are the top groups after we apply k means clustering on all extras.keys.
each group has a set of candidate values such as a string that represents the current city name.
candidate sets are mainlyfrom four sources app based data include widget id package name local storage path etc environment based data include current date time with different format strings file based data cover specifications and formats e.g.
the format and size of pictures and url based data include url addresses for web pages images audio videos etc.
with the trained word2v ec model and the svm model codeminer first classifies each new extras.key and then assigns a candidate value to the correspondingextras.value field.
b. system api return v alue inference system apis belong to the android namespace and their usage has an android.
or java.
prefix.
this observationsimplifies the task of determining which variables hold valuesderived from some system api return values.
resource relatedsystem apis considered are listed in table iii.
however since a group of system apis can change one mobile device component e.g.
gps the challenge lies ingenerating test data that are consistent to the hardware state.to this end codeminer maintains a finite state machine fsm authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iii popular android system api s called by background services .
system api log .
network info .
wakelock .
calendar info .
database info .
file .04network .22location info .
unique identifier .67system api file info .
account info .
audio .94bluetooth info .
system settings .
sms mms .
account settings .
synchronization data .
contact info .
for typical hardware components on mobile devices.
fortunately most mobile devices exhibit a similar set of hardwarecapabilities geo localization data i o motion related sensors etc.
in addition android provides standard abstractions tohide individual hardware components intrinsic properties.
forexample sensors basically have three reachable states on off sampling.
for each system api that codeminer needs to generate test data codeminer keeps track of the cur rent state of hardware components.
in the sensor exampleabove if the sensor has been activated by registering withsensormanager then its status should be on.
c. microbenchmarks we evaluate the use of the svm based classification algorithm by comparing with the human baseline.
specifically with extras.key strings from our app pool we manually label them into groups c.f.
vi a .
then we randomly allocate and of these strings as thetraining set validation set and testing set respectively.
we usecross validation to choose the parameter c which trades off misclassification against simplicity of the decision surface from the set .
.
.
.
.
.
.the accuracy measured by the testing set is .
over the24 groups as compared to the random guess of an accuracy of .
with a recall of .
and a precision of .
.
vii.
a dditional practical considera tions this section discusses practical considerations for servicefuzzer to test individual background services with test inputsgenerated by codeminer.
a. automating background services before injecting intents to start services servicefuzzer launches the app to allow the opportunity to properly initialize.
for example many apps check for environment dependencies e.g.
the database tables.
then servicefuzzer pauses the appforeground activity by simulating the user pressing the devicehome button.
servicefuzzer then selects nodes without any incoming edges in the service dependency graph as these nodes rep resent services c without any dependency cden s .
for each of these services servicefuzzer turns correspondingsets of test data into individual explicit intents.
specifically c in the service trigger inputs becomes the intent s targetcomponent and pairs of pandp.v in service execution inputs become intent fields.
the execution of service code paths is guided by both intent fields and any system api return values.
the for mer is in the intent crafted in the previous step.
the lat ter requires servicefuzzer to intercept system api calls bythe service and this can be done with off the shelf toolssuch as xposed.
for each system api call intercepted ser vicefuzzer searches in the execution inputs for a pair ofaand a.v that matches the system api.
the return value is then manipulated to be a.v.
for example we can intercept the return value of activenetwork.gettype with an integer connectivitymanager.type wifi or connectivitymanager.type mobile in fig.
.
b. terminating a testing session to simulate different termination conditions snowdrop allows developers to select from the following two terminationstrategies for each testing session.
graceful termination.
this termination strategy allows the service to run to completion.
servicefuzzer passively infers the lifecycle by intercepting onstartcommand onbind andondestroy callback events.
forced termination.
this termination strategy kills the service when the developer specified timer fires.
in addition the developer can instrument calls to our debugging api snowdrop kill in the app bytecode.
upon interceptingthis api call at run time servicefuzzer would kill the appmain process and spawned processes to simulate the termina tion forced by the os.
c. disaggregating logs snowdrop provides resource utilization logs including cpu and memory.
providing per service logs can be challenging as the android platform provides performance counters at applevel not component level.
snowdrop exercises log disaggregation as below.
we record the service s start and end time and attribute counters in this lifetime period to it accordingly.
the service lifetime typically can be identified with the three callbacks onstartcommand onbind ondestroy.
we note that there are cases of concurrent threads a service is created as athread within the app main process or even multiple services.to handle these cases we attribute the resource utilizationequally and this is a standard technique used by app dataanalysis tools .
viii.
e v alua tion this section is organized by the following major results without human inputs snowdrop can achieve .
moreservice code path coverage than existing pathwise test inputgenerators and .
more coverage than random test inputgenerators.
.
of intents generated by codeminer are not malformed or invalid more than .
higherthan comparison baselines.
test input generation takes an authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
average of .
sec for one android service.
this makes snowdrop feasible for testing background services in practice.
a. methodology and implementation we implemented snowdrop in python and java codeminer has lines of code loc servicefuzzer has loc and logging modules have loc.
codeminer dis assembles and decompiles android app bytecode with androguard .
servicefuzzer runs inside genymotion oneof the fastest emulators available.
servicefuzzer exercises the standard practice of injecting intents and killing services withandroid debugging bridge adb .
and it uses xposed to manipulate system api return values.
to calculate code path coverage we modify bboxtester with the populartool jacoco to first list all code paths in services.
then we cross reference this full list with our own list of executedbranches to calculate the number of exercised paths.
evaluation metrics and comparison baselines.
to evaluate the effectiveness we adopt the metric of path coverage or the number of branched control paths in a service that a testingtool executes.
we also use the test input generation time asanother evaluation metric.
with these metrics evaluations are based on the following comparison baselines.
first for representing random test inputgenerators one baseline is intent fuzzer an icc basedfuzzing for android apps.
it relies on static analysis to identify all intents and their mandatory fields and then randomly gen erates field values.
another baseline is nullintentfuzzer a fuzzing tool that focuses on crafting intents with onlythe service component name.
second as there is no offthe shelf pathwise test input generators for app services we implemented one with ic3 .
ic3 is the current state of the art solution for intent fields inference.
methodology.
our app pool consists of popular apps on the google play store.
to minimize any test bias towards certain app categories we take top apps from each ofthe popular app categories i.e.
news music and social .collectively these apps have a total of services.
to run experiments we generate test inputs for all apps with snowdrop and comparison baselines.
for snowdrop andic3 each code path in identified services has a test case andwe let a test run for five minutes after injecting the intent viaadb.
since it is difficult to force random test input generatorsto target a particular code path we let intent fuzzer and nullintentfuzzer run as many rounds as possible within thesame time window for fairness.
verifiability.
factors that might influence the reproducibility of results include the version of apps in our pool the version of open sourced tools that snowdrop uses and the hardware spec of our testing environment.
for the first two factors we have archived the downloaded packages.
for thelast factor we are running windows on intel i7 cpuand gb of ram.
fig.
.
service code path coverage for different test input generators.
each tool tries to generate test inputs to execute as many code paths in services of our app pool as possible.
by inferring all intent fields and system api return values snowdrop can achieve better coverage than ic3 and intent fuzzer.
b. what is the service code path coverage achieved?
we start by empirically measuring and comparing snowdrop s service code path coverage.
as mentioned above the ground truth is computed by bboxtester and ja coco .
we generate the test input for each code path with snowdrop and two other baselines intent fuzzer and ic3.these two baselines provide visibility into existing random andpathwise test input generators.
a test case represents one codepath and it successfully completes if all branches on the codepath are executed.
as fig.
shows excluding services in third party libraries snowdrop successfully executes .
of all16 service code paths in our app pool.
this is .
more coverage than ic3 and .
more coverage than intentfuzzer.
there are two cases where a test case can not completely execute a code path a branch cannot be satisfied and execution exceptions due to malformed or invalid intents.
thefollowing two subsections delve into these two cases.
c. how well do test inputs satisfy branch conditions?
one factor affecting the service code path coverage is whether the test input can satisfy all branches on its intended code path.
there are two main reasons behind unsatisfiablebranching conditions branching conditions are hard coded to be false or branching arguments cannot be externally fuzzed.
to quantify these reasons we classify branchingarguments by their types constant non sys api e.g.
developers own functions sys api and intent field.
the last two classes can be externally fuzzed.
while servicefuzzer fuzzessys api return values through interception c.f.
viii a we do not intercept non sys apis as they also contain developer code that needs to be tested.
table iv shows the distribution of possible variations of branching conditions.
each variation has one of the two tags fuzzable i.e the branching condition arguments can be fuzzed to satisfy and unfuzzable i.e otherwise .
given that only sys api and intent field can be external fuzzed there are some branching statements out of scope for snowdrop.
table v shows that snowdrop can successfully fuzz .
of branching conditions.
we note that cases involving sys api tend to have a relatively lower percentage.
and this is a knownproblem in the community as certain data types of return authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table iv distribution of different branching conditions by the type of their arguments .given tha t only sys api and intent field can be externally fuzzed there are some branching sta tements out of scope for snowdrop .
tag argument argument unfuzzable constant constant .
fuzzable sys api constant .
fuzzable sys api sys api .
fuzzable sys api intent field .
fuzzable intent field intent field .
fuzzable intent field constant .
unfuzzable sys api non sys api .
unfuzzable non sys api intent field .
unfuzzable non sys api constant .
unfuzzable non sys api non sys api .
table v number of fuzzable branching conditions tha t snowdrop can satisfy .
argument argument satisfiable sys api constant .
sys api sys api .
sys api intent field .
intent field intent field .
intent field constant .
values are difficult to fuzz which include arrays iterators the pair in a map a set i o operations content providers or evensome complex data objects e.g.
bitmap .
finally we note the use of non sys apis varies among app categories.
fig.
organizes snowdrop s code coverageby the app category.
interestingly it suggests that music andphotography apps generally achieve lower coverage.
photog raphy apps tend to use specialized library functions such ascalculating the rescaling size for bitmap files.
similarly musicapps contain many developer defined objects such as a singerprofile object that is generally difficult to solve for test inputgenerators.
d. what is the correctness of intent inference heuristics?
another factor affecting the service code path coverage is the correctness of intents generated.
specifically malformed and invalid intents can cause run time exceptions and these exceptions force tests to terminate unexpectedly.for instance a tool can assign arbitrary string values toextras key value pairs that expect well formed strings such as web urls or media file paths.
in other cases numeric values can have a meaning such as gps longitudeand latitude.
fig.
shows that .
of intents generatedby snowdrop can successfully run to the test completion.interestingly this is .
and .
more successfulcases than ic3 and intent fuzzer respectively.
since ic3and intent fuzzer do not consider the field semantics mostexceptions caught e.g.
numberformatexception and andnullpointerexception are due to malformed and invalid test inputs.
next we discuss malformed intents generated by codeminer.
first .
of these cases belong to services in fig.
.
depending on the type of branching conditions and data structures in the code some app categories are more difficult to generate test inputs for.
fig.
.
malformed or invalid intents can cause run time exceptions andforce a test to terminate unexpectedly.
of all intents generated snowdrop generates the highest percentage of correct intents.
for comparison baselines many exceptions include numberformatexception and nullpointerexception.
third party libraries e.g.
advertisement and app analytics tracking.
one observation is that these third party librarieshave intent fields with uncommon naming convention e.g.
extra situa tion and frontia user aipkey .
second .
of unsuccessful cases are due to widget update services which require the right widget id.
third .
of unsuccessful cases are services for licensing or signinginformation which require the right licensing information.while being out of scope for this paper it is possible toimplement a human feedback loop to address these limitations.
finally we benchmark the two inference heuristics of codeminer to understand how they contribute to the cor rectness of intents.
as we mentioned before codeminerfirst tries variable assignment analysis v aa to find fieldvalue assignments in app binaries.
since developer specifiedfields can be difficult to infer codeminer then infers theirvalues with nlp based heuristic nlph .
table vi showsthe effectiveness of the two strategies variable assignmentanalysis v aa and nlp based heuristic nlph .
first we observe that variable assignment analysis can handle mostcases of component action datatype extras.key andcategory.
the reason is that these fields have a predefined set of values and most apps explicitly assign their values.
second table vi suggests that the situation is different foruridata andextras.value where the developer can assign arbitrary values.
in these cases the inference relies authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
table vi percentage of intent fields whose v alues can be inferred by v ariable assignment analysis v aa and nlp based heuristic nlph .
r esults are grouped by the intent field type .
intent inferred inferred unresolved field by v aa by nlph component .
n a .
action .
n a .
uridata .
.
n a datatype .
n a .
extras.key .
n a .
extras.value .
.
n acategory .
n a .
fig.
.
time taken to generate test inputs for each of our apps.
thecomputation cost of snowdrop is practical for real world usage.
on applying classification techniques to the variable name.
empirical results show that .
of uridata and .
ofextras.value can be inferred this way.
table vi also suggests that there are instances codeminer fails to infer.
one reason is that codeminer ignores data typesthat cannot be analyzed by static analysis or sent over adb parcelable serializable and bundle.
in our app pool whilemost extras.values are string and integer .
of them are of these unsupported types.
e. is the computation cost feasible in practice?
measurements show that snowdrop has a reasonable cost for real world usage.
fig.
shows the average time to generate test cases for an app is .
sec which translates to .
sec per android app service.
while being out of scope for this paper optimizations on constraint solving will reduce this cost.
forreference we note that ic3 takes an average of .
sec togenerate the icc message per service.
ix.
c ase studies with snowdrop we tested our pool of apps and found that .
of background services have bugs.
for example .
of apps have problems of poor exception handling.
foranalysis we logged adb runtime exceptions and various per formance counters including cpu utilization network tx rx power consumption etc.
we confirmed snowdrop s findingsby manually examining decompiled the bytecode of reported apps.
this section highlights real world bugs that snowdropreported on apps released in the wild.a.
crashes and poor exception handling ted.
snowdrop detected java.lang.nullpointerexception powermanager wakelock.release on a null object reference.
by manually examining the decompiled app bytecode we found that the com.pushio.manager.pushioengagementserviceservice does not initialize a wakelock object before acquiring and releasing the lock in some code paths.
huffington post.
snowdrop detected a crash due to java.lang.runtimeexception wakelock under locked com.commonsware.cwac.wakeful.wakefulintentservice in downloadall.autoofflineservice.
this errorsuggests that the wakelock was released more timesthan it was acquired.
we confirmed by decompilingthe huffington post app.
and we found that the appdoes not check whether the wakelock is acquired inwakeful.wakefulintentservice which is called by downloadall.autoofflineservice.
sophos mobile control.
snowdrop detected java.lang.noclassdeff ounderror com.sonymobile.enterprise.configuration in installcertificateservice.
we manually inspected the decompiled bytecode the installcertificateservice tried to instantiate the com.sonymobile.enterprise.configuration class which does not exist in the expected third party library sonymobile.enterprise.
one possible explanation is thatthe developer might have mistaken the library version.
b. resource usage bugs fox tv turkiye.
logs indicate that this app has many periods of extremely low cpu utilization.
this observation suggests that the app might have prevented the cpu frombeing suspended to reduce energy consumption.
we manuallyexamined the decompiled app bytecode and we found that thepushadservice was trying to hold pushad.wakelock until all async jobs completed.
however this wakelock was not properly released in two of the code paths.
airplay dlna receiver.
airplay dlna receiver wirelessly receives videos pictures and music from nearby devices.
without any active streaming sessions we noticed that the app had an unexpectedly high network traffic mb s when waxplayservice was running.
one potential impact of this behavior is the high energy consumption.
we used wireshark to investigate possible causes with network level traffic traces and we found that the app constantly sentout byte mdns bonjour requests.
in addition the appperiodically sends a group of byte tcp packets to thenetwork gateway.
x. r ela ted work a. mobile app testing frameworks most existing testing frameworks rely on ui automation for workloads of touch screen interactions and gestures.
ap pium and uiautomator are popular frameworks authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.
with api to build ui driven tests.
puma explores a flexible programming paradigm to ui driven app analysis and reran addresses touch sensitive record and replay.testflight is a framework for ios developers to invitereal world users to exercise the app.
amc reduces theamount of work that a human expert must perform to evaluatethe conformance of vehicular apps.
smartdroid identifiesui based trigger conditions in android apps to help privacyexperts to quickly find the sensitive behaviors.
raindrops envisions a split execution model for building automated andcontextual testing services to bring real world contexts intomobile app testing.
unfortunately background services are hidden from users and empirical data show .
of services are not reachable through ui invocations.
this observation motivates the need of snowdrop.
b. intent inference tools many testing scenarios rely on the icc oriented approach or the ability to extract icc messages coded in the app bytecode.
for instance comdroid and iccta analyzeicc messages to detect privacy leaks.
as iii b discusses there are also generic tools to facilitate the icc oriented approach.
the first category of tools issimply a fuzzer that assumes most of the intent structurewould be given by the user .
these tools can imposea significant burden due to manually creating test inputs toachieve full coverage.
to this end random test input generatorscan generate random intent field values .
however random data generators do not guarantee test input quality nor the time to achieve full testing coverage .
finally some tools exploit app code knowledge to make informeddecisions on intent inference and generation.
intentfuzzer intercepts api calls to learn keys in extras and then randomly generates value for each key.
ic3 improves onepicc and it analyzes the code to infer constraints given by a code path on values that each field can take on.
while these icc oriented tools try to identify as many intents as possible in the app bytecode implicit intents can be challenging for static analysis tools.
furthermore sincesome intent fields can syntactically take on arbitrary values snowdrop improves the inference correctness with nlp basedheuristic.
c. visibility into background services the research community has recently started to explore problems related to background services.
chen et al.
conducts a large scale study and telemetries from user devicesreveal services can consume unexpected amount of energy.then they propose hush as a way to dynamically suppress background services on the device.
tamer is an os mechanism that monitors events and rate limits the wake upof app services on the device.
there are related efforts on code analysis.
entrack aims to finely trace energy consumption of system services and addresses challenges in accurate per thread accounting.
pathaket al.
proposes a compile time solution based on the classic reaching data flow analysis problem to automaticallyinfer the possibility of a no sleep bug in a given app.
snowdrop can complement these efforts and it fills the gap of enabling developers to systematically test all services before the app release.
xi.
d iscussion and future work system limitations.
as with any system snowdrop has limitations.
first dynamically loaded code can be challenging for snowdrop to analyze as the code might not be availableuntil run time.
fortunately most background services are notimplemented in this fashion.
second while the nlp basedheuristic improves the correctness of intent value inferencefor snowdrop there are still cases where developers use meaningless variable names that complicate semantic under standing.
as a future work we are exploring additional meansof inferring the semantics of variables.
overhead of pathwise test input generation.
the community has raised the concern that pathwise test input generation does not scale well with the software complexity.
fortunately mobile app services generally have relatively low complexity and empirical results suggest an average of .
code pathsper service.
complementing ui automation testing.
many tools are available for developers to automate ui testing.
we believe that ensuring app experience can benefit from testing both ui andbackground services.
since both aspects do not conflict witheach other snowdrop can complement existing ui automationtesting.
applicability to other mobile platforms.
background services that consume too much resources are also present in ios and windows apps.
while concepts of snowdrop are applicable to other mobile platforms our current investigationsuggests that there does not seem to be an easy way to triggerbackground services to run.
we are exploring workarounds.
xii.
c onclusion snowdrop fills the gap towards systematically testing app background services.
it automatically generates test inputs tomaximize service code path coverage.
empirical results showthat snowdrop can achieve higher coverage over existing ran dom and pathwise test input generators.
furthermore snow drop has uncovered service problems ranging from crashesto energy bugs and it can well complement the existing uiautomation based testing.
a cknowledgment we thank the anonymous reviewers for their insightful comments and suggestions.
this research was partially supportedby grants from the nature science foundation of china grantno.
u1605251 and the national science foundation fordistinguished young scholars of china grant no.
.
authorized licensed use limited to lahore univ of management sciences.
downloaded on august at utc from ieee xplore.
restrictions apply.